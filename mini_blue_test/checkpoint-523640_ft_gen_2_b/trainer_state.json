{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 40.0,
  "eval_steps": 500,
  "global_step": 523640,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007638835841417768,
      "grad_norm": 10.657843589782715,
      "learning_rate": 4.999045145519823e-05,
      "loss": 3.3775,
      "step": 100
    },
    {
      "epoch": 0.015277671682835536,
      "grad_norm": 8.593043327331543,
      "learning_rate": 4.998090291039646e-05,
      "loss": 2.7733,
      "step": 200
    },
    {
      "epoch": 0.022916507524253303,
      "grad_norm": 6.839597702026367,
      "learning_rate": 4.997135436559469e-05,
      "loss": 2.5876,
      "step": 300
    },
    {
      "epoch": 0.030555343365671072,
      "grad_norm": 6.752279758453369,
      "learning_rate": 4.996180582079291e-05,
      "loss": 2.5862,
      "step": 400
    },
    {
      "epoch": 0.03819417920708884,
      "grad_norm": 7.681690216064453,
      "learning_rate": 4.9952257275991146e-05,
      "loss": 2.4859,
      "step": 500
    },
    {
      "epoch": 0.045833015048506606,
      "grad_norm": 8.203631401062012,
      "learning_rate": 4.994270873118937e-05,
      "loss": 2.5746,
      "step": 600
    },
    {
      "epoch": 0.05347185088992438,
      "grad_norm": 8.797572135925293,
      "learning_rate": 4.9933160186387596e-05,
      "loss": 2.5264,
      "step": 700
    },
    {
      "epoch": 0.061110686731342144,
      "grad_norm": 8.149998664855957,
      "learning_rate": 4.9923611641585825e-05,
      "loss": 2.4843,
      "step": 800
    },
    {
      "epoch": 0.06874952257275992,
      "grad_norm": 6.0527191162109375,
      "learning_rate": 4.9914063096784054e-05,
      "loss": 2.352,
      "step": 900
    },
    {
      "epoch": 0.07638835841417768,
      "grad_norm": 8.628549575805664,
      "learning_rate": 4.9904514551982276e-05,
      "loss": 2.4528,
      "step": 1000
    },
    {
      "epoch": 0.08402719425559545,
      "grad_norm": 6.464604377746582,
      "learning_rate": 4.989496600718051e-05,
      "loss": 2.5456,
      "step": 1100
    },
    {
      "epoch": 0.09166603009701321,
      "grad_norm": 5.797004699707031,
      "learning_rate": 4.988541746237873e-05,
      "loss": 2.4371,
      "step": 1200
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 6.0449748039245605,
      "learning_rate": 4.987586891757696e-05,
      "loss": 2.4681,
      "step": 1300
    },
    {
      "epoch": 0.10694370177984876,
      "grad_norm": 6.874170780181885,
      "learning_rate": 4.986632037277519e-05,
      "loss": 2.4058,
      "step": 1400
    },
    {
      "epoch": 0.11458253762126652,
      "grad_norm": 5.592609882354736,
      "learning_rate": 4.985677182797342e-05,
      "loss": 2.3223,
      "step": 1500
    },
    {
      "epoch": 0.12222137346268429,
      "grad_norm": 9.184085845947266,
      "learning_rate": 4.984722328317165e-05,
      "loss": 2.4646,
      "step": 1600
    },
    {
      "epoch": 0.12986020930410205,
      "grad_norm": 6.0286712646484375,
      "learning_rate": 4.983767473836988e-05,
      "loss": 2.3133,
      "step": 1700
    },
    {
      "epoch": 0.13749904514551983,
      "grad_norm": 6.738003253936768,
      "learning_rate": 4.9828126193568105e-05,
      "loss": 2.3394,
      "step": 1800
    },
    {
      "epoch": 0.14513788098693758,
      "grad_norm": 8.912747383117676,
      "learning_rate": 4.981857764876633e-05,
      "loss": 2.3457,
      "step": 1900
    },
    {
      "epoch": 0.15277671682835536,
      "grad_norm": 5.332336902618408,
      "learning_rate": 4.980902910396456e-05,
      "loss": 2.45,
      "step": 2000
    },
    {
      "epoch": 0.16041555266977311,
      "grad_norm": 7.68809700012207,
      "learning_rate": 4.9799480559162785e-05,
      "loss": 2.2977,
      "step": 2100
    },
    {
      "epoch": 0.1680543885111909,
      "grad_norm": 7.897040843963623,
      "learning_rate": 4.978993201436101e-05,
      "loss": 2.2737,
      "step": 2200
    },
    {
      "epoch": 0.17569322435260867,
      "grad_norm": 8.165068626403809,
      "learning_rate": 4.978038346955924e-05,
      "loss": 2.2572,
      "step": 2300
    },
    {
      "epoch": 0.18333206019402642,
      "grad_norm": 6.318943023681641,
      "learning_rate": 4.977083492475747e-05,
      "loss": 2.3903,
      "step": 2400
    },
    {
      "epoch": 0.1909708960354442,
      "grad_norm": 8.665945053100586,
      "learning_rate": 4.97612863799557e-05,
      "loss": 2.338,
      "step": 2500
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 7.775863170623779,
      "learning_rate": 4.975173783515393e-05,
      "loss": 2.3601,
      "step": 2600
    },
    {
      "epoch": 0.20624856771827974,
      "grad_norm": 7.405876636505127,
      "learning_rate": 4.974218929035216e-05,
      "loss": 2.2724,
      "step": 2700
    },
    {
      "epoch": 0.21388740355969751,
      "grad_norm": 6.559520721435547,
      "learning_rate": 4.973264074555038e-05,
      "loss": 2.2707,
      "step": 2800
    },
    {
      "epoch": 0.22152623940111527,
      "grad_norm": 6.22802734375,
      "learning_rate": 4.972309220074861e-05,
      "loss": 2.2552,
      "step": 2900
    },
    {
      "epoch": 0.22916507524253305,
      "grad_norm": 6.395657062530518,
      "learning_rate": 4.9713543655946836e-05,
      "loss": 2.3393,
      "step": 3000
    },
    {
      "epoch": 0.2368039110839508,
      "grad_norm": 5.595874309539795,
      "learning_rate": 4.9703995111145065e-05,
      "loss": 2.2991,
      "step": 3100
    },
    {
      "epoch": 0.24444274692536858,
      "grad_norm": 5.636528968811035,
      "learning_rate": 4.969444656634329e-05,
      "loss": 2.2773,
      "step": 3200
    },
    {
      "epoch": 0.2520815827667863,
      "grad_norm": 7.409404277801514,
      "learning_rate": 4.968489802154152e-05,
      "loss": 2.3463,
      "step": 3300
    },
    {
      "epoch": 0.2597204186082041,
      "grad_norm": 5.971039295196533,
      "learning_rate": 4.9675349476739744e-05,
      "loss": 2.28,
      "step": 3400
    },
    {
      "epoch": 0.2673592544496219,
      "grad_norm": 6.9893107414245605,
      "learning_rate": 4.966580093193797e-05,
      "loss": 2.2475,
      "step": 3500
    },
    {
      "epoch": 0.27499809029103967,
      "grad_norm": 7.263493537902832,
      "learning_rate": 4.96562523871362e-05,
      "loss": 2.2545,
      "step": 3600
    },
    {
      "epoch": 0.2826369261324574,
      "grad_norm": 5.20176887512207,
      "learning_rate": 4.964670384233443e-05,
      "loss": 2.2369,
      "step": 3700
    },
    {
      "epoch": 0.29027576197387517,
      "grad_norm": 7.313406944274902,
      "learning_rate": 4.963715529753266e-05,
      "loss": 2.3171,
      "step": 3800
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 7.786764144897461,
      "learning_rate": 4.962760675273089e-05,
      "loss": 2.2247,
      "step": 3900
    },
    {
      "epoch": 0.3055534336567107,
      "grad_norm": 7.441811561584473,
      "learning_rate": 4.9618058207929116e-05,
      "loss": 2.2796,
      "step": 4000
    },
    {
      "epoch": 0.3131922694981285,
      "grad_norm": 8.033016204833984,
      "learning_rate": 4.960850966312734e-05,
      "loss": 2.2316,
      "step": 4100
    },
    {
      "epoch": 0.32083110533954623,
      "grad_norm": 6.029402732849121,
      "learning_rate": 4.9598961118325574e-05,
      "loss": 2.3599,
      "step": 4200
    },
    {
      "epoch": 0.328469941180964,
      "grad_norm": 5.907043933868408,
      "learning_rate": 4.9589412573523796e-05,
      "loss": 2.1529,
      "step": 4300
    },
    {
      "epoch": 0.3361087770223818,
      "grad_norm": 8.1845064163208,
      "learning_rate": 4.9579864028722024e-05,
      "loss": 2.2844,
      "step": 4400
    },
    {
      "epoch": 0.34374761286379957,
      "grad_norm": 6.590937614440918,
      "learning_rate": 4.957031548392025e-05,
      "loss": 2.21,
      "step": 4500
    },
    {
      "epoch": 0.35138644870521735,
      "grad_norm": 6.501490592956543,
      "learning_rate": 4.956076693911848e-05,
      "loss": 2.2644,
      "step": 4600
    },
    {
      "epoch": 0.35902528454663507,
      "grad_norm": 7.2427659034729,
      "learning_rate": 4.9551218394316704e-05,
      "loss": 2.2885,
      "step": 4700
    },
    {
      "epoch": 0.36666412038805285,
      "grad_norm": 6.897184371948242,
      "learning_rate": 4.954166984951494e-05,
      "loss": 2.3156,
      "step": 4800
    },
    {
      "epoch": 0.37430295622947063,
      "grad_norm": 8.671448707580566,
      "learning_rate": 4.953212130471316e-05,
      "loss": 2.3517,
      "step": 4900
    },
    {
      "epoch": 0.3819417920708884,
      "grad_norm": 6.977689266204834,
      "learning_rate": 4.952257275991139e-05,
      "loss": 2.2134,
      "step": 5000
    },
    {
      "epoch": 0.3895806279123062,
      "grad_norm": 7.203551769256592,
      "learning_rate": 4.951302421510962e-05,
      "loss": 2.3118,
      "step": 5100
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 8.980148315429688,
      "learning_rate": 4.950347567030785e-05,
      "loss": 2.2231,
      "step": 5200
    },
    {
      "epoch": 0.4048582995951417,
      "grad_norm": 5.946340084075928,
      "learning_rate": 4.9493927125506076e-05,
      "loss": 2.2185,
      "step": 5300
    },
    {
      "epoch": 0.41249713543655947,
      "grad_norm": 5.594563961029053,
      "learning_rate": 4.9484378580704304e-05,
      "loss": 2.1813,
      "step": 5400
    },
    {
      "epoch": 0.42013597127797725,
      "grad_norm": 7.857141971588135,
      "learning_rate": 4.947483003590253e-05,
      "loss": 2.364,
      "step": 5500
    },
    {
      "epoch": 0.42777480711939503,
      "grad_norm": 7.803111553192139,
      "learning_rate": 4.9465281491100755e-05,
      "loss": 2.1521,
      "step": 5600
    },
    {
      "epoch": 0.43541364296081275,
      "grad_norm": 6.6732635498046875,
      "learning_rate": 4.945573294629899e-05,
      "loss": 2.1603,
      "step": 5700
    },
    {
      "epoch": 0.44305247880223053,
      "grad_norm": 5.010250091552734,
      "learning_rate": 4.944618440149721e-05,
      "loss": 2.2466,
      "step": 5800
    },
    {
      "epoch": 0.4506913146436483,
      "grad_norm": 6.27398681640625,
      "learning_rate": 4.943663585669544e-05,
      "loss": 2.2057,
      "step": 5900
    },
    {
      "epoch": 0.4583301504850661,
      "grad_norm": 6.938957214355469,
      "learning_rate": 4.942708731189367e-05,
      "loss": 2.295,
      "step": 6000
    },
    {
      "epoch": 0.46596898632648387,
      "grad_norm": 5.0990190505981445,
      "learning_rate": 4.94175387670919e-05,
      "loss": 2.2928,
      "step": 6100
    },
    {
      "epoch": 0.4736078221679016,
      "grad_norm": 6.596705913543701,
      "learning_rate": 4.940799022229012e-05,
      "loss": 2.2548,
      "step": 6200
    },
    {
      "epoch": 0.4812466580093194,
      "grad_norm": 7.948777198791504,
      "learning_rate": 4.9398441677488356e-05,
      "loss": 2.2261,
      "step": 6300
    },
    {
      "epoch": 0.48888549385073715,
      "grad_norm": 7.873510360717773,
      "learning_rate": 4.938889313268658e-05,
      "loss": 2.2501,
      "step": 6400
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 6.329504013061523,
      "learning_rate": 4.9379344587884806e-05,
      "loss": 2.1904,
      "step": 6500
    },
    {
      "epoch": 0.5041631655335727,
      "grad_norm": 7.776589870452881,
      "learning_rate": 4.936979604308304e-05,
      "loss": 2.0766,
      "step": 6600
    },
    {
      "epoch": 0.5118020013749904,
      "grad_norm": 4.324591636657715,
      "learning_rate": 4.9360247498281264e-05,
      "loss": 2.1708,
      "step": 6700
    },
    {
      "epoch": 0.5194408372164082,
      "grad_norm": 8.112269401550293,
      "learning_rate": 4.935069895347949e-05,
      "loss": 2.1964,
      "step": 6800
    },
    {
      "epoch": 0.527079673057826,
      "grad_norm": 6.158720970153809,
      "learning_rate": 4.934115040867772e-05,
      "loss": 2.1896,
      "step": 6900
    },
    {
      "epoch": 0.5347185088992438,
      "grad_norm": 7.265646457672119,
      "learning_rate": 4.933160186387595e-05,
      "loss": 2.1536,
      "step": 7000
    },
    {
      "epoch": 0.5423573447406616,
      "grad_norm": 6.98304557800293,
      "learning_rate": 4.932205331907417e-05,
      "loss": 2.2335,
      "step": 7100
    },
    {
      "epoch": 0.5499961805820793,
      "grad_norm": 7.57611608505249,
      "learning_rate": 4.931250477427241e-05,
      "loss": 2.1411,
      "step": 7200
    },
    {
      "epoch": 0.5576350164234971,
      "grad_norm": 6.316906452178955,
      "learning_rate": 4.930295622947063e-05,
      "loss": 2.1838,
      "step": 7300
    },
    {
      "epoch": 0.5652738522649148,
      "grad_norm": 7.626104354858398,
      "learning_rate": 4.929340768466886e-05,
      "loss": 2.2287,
      "step": 7400
    },
    {
      "epoch": 0.5729126881063326,
      "grad_norm": 5.8879194259643555,
      "learning_rate": 4.928385913986709e-05,
      "loss": 2.2305,
      "step": 7500
    },
    {
      "epoch": 0.5805515239477503,
      "grad_norm": 6.38561487197876,
      "learning_rate": 4.9274310595065315e-05,
      "loss": 2.1803,
      "step": 7600
    },
    {
      "epoch": 0.5881903597891681,
      "grad_norm": 5.906652927398682,
      "learning_rate": 4.9264762050263544e-05,
      "loss": 2.2461,
      "step": 7700
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 6.4330735206604,
      "learning_rate": 4.925521350546177e-05,
      "loss": 2.1703,
      "step": 7800
    },
    {
      "epoch": 0.6034680314720037,
      "grad_norm": 7.464908123016357,
      "learning_rate": 4.924566496066e-05,
      "loss": 2.3294,
      "step": 7900
    },
    {
      "epoch": 0.6111068673134215,
      "grad_norm": 6.857761383056641,
      "learning_rate": 4.923611641585822e-05,
      "loss": 2.1936,
      "step": 8000
    },
    {
      "epoch": 0.6187457031548392,
      "grad_norm": 9.278703689575195,
      "learning_rate": 4.922656787105646e-05,
      "loss": 2.2414,
      "step": 8100
    },
    {
      "epoch": 0.626384538996257,
      "grad_norm": 6.639617443084717,
      "learning_rate": 4.921701932625468e-05,
      "loss": 2.3605,
      "step": 8200
    },
    {
      "epoch": 0.6340233748376748,
      "grad_norm": 7.915894031524658,
      "learning_rate": 4.920747078145291e-05,
      "loss": 2.184,
      "step": 8300
    },
    {
      "epoch": 0.6416622106790925,
      "grad_norm": 6.4702277183532715,
      "learning_rate": 4.919792223665114e-05,
      "loss": 2.3064,
      "step": 8400
    },
    {
      "epoch": 0.6493010465205102,
      "grad_norm": 5.8580193519592285,
      "learning_rate": 4.918837369184937e-05,
      "loss": 2.2587,
      "step": 8500
    },
    {
      "epoch": 0.656939882361928,
      "grad_norm": 6.946641445159912,
      "learning_rate": 4.917882514704759e-05,
      "loss": 2.3173,
      "step": 8600
    },
    {
      "epoch": 0.6645787182033458,
      "grad_norm": 6.735767364501953,
      "learning_rate": 4.916927660224582e-05,
      "loss": 2.2043,
      "step": 8700
    },
    {
      "epoch": 0.6722175540447636,
      "grad_norm": 6.466943740844727,
      "learning_rate": 4.9159728057444046e-05,
      "loss": 2.3036,
      "step": 8800
    },
    {
      "epoch": 0.6798563898861814,
      "grad_norm": 7.8700947761535645,
      "learning_rate": 4.9150179512642275e-05,
      "loss": 2.1279,
      "step": 8900
    },
    {
      "epoch": 0.6874952257275991,
      "grad_norm": 6.589061260223389,
      "learning_rate": 4.9140630967840503e-05,
      "loss": 2.1611,
      "step": 9000
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 7.277041912078857,
      "learning_rate": 4.913108242303873e-05,
      "loss": 2.2382,
      "step": 9100
    },
    {
      "epoch": 0.7027728974104347,
      "grad_norm": 7.649940490722656,
      "learning_rate": 4.912153387823696e-05,
      "loss": 2.1537,
      "step": 9200
    },
    {
      "epoch": 0.7104117332518525,
      "grad_norm": 8.28800106048584,
      "learning_rate": 4.911198533343518e-05,
      "loss": 2.123,
      "step": 9300
    },
    {
      "epoch": 0.7180505690932701,
      "grad_norm": 6.725391387939453,
      "learning_rate": 4.910243678863342e-05,
      "loss": 2.0346,
      "step": 9400
    },
    {
      "epoch": 0.7256894049346879,
      "grad_norm": 7.296143054962158,
      "learning_rate": 4.909288824383164e-05,
      "loss": 2.1828,
      "step": 9500
    },
    {
      "epoch": 0.7333282407761057,
      "grad_norm": 6.575263500213623,
      "learning_rate": 4.908333969902987e-05,
      "loss": 2.2687,
      "step": 9600
    },
    {
      "epoch": 0.7409670766175235,
      "grad_norm": 8.261496543884277,
      "learning_rate": 4.90737911542281e-05,
      "loss": 2.1705,
      "step": 9700
    },
    {
      "epoch": 0.7486059124589413,
      "grad_norm": 6.935733795166016,
      "learning_rate": 4.9064242609426326e-05,
      "loss": 2.0446,
      "step": 9800
    },
    {
      "epoch": 0.756244748300359,
      "grad_norm": 6.254842758178711,
      "learning_rate": 4.905469406462455e-05,
      "loss": 2.1122,
      "step": 9900
    },
    {
      "epoch": 0.7638835841417768,
      "grad_norm": 5.258333206176758,
      "learning_rate": 4.9045145519822784e-05,
      "loss": 2.1235,
      "step": 10000
    },
    {
      "epoch": 0.7715224199831946,
      "grad_norm": 6.896238327026367,
      "learning_rate": 4.9035596975021006e-05,
      "loss": 2.1002,
      "step": 10100
    },
    {
      "epoch": 0.7791612558246124,
      "grad_norm": 7.804656505584717,
      "learning_rate": 4.9026048430219234e-05,
      "loss": 2.1662,
      "step": 10200
    },
    {
      "epoch": 0.78680009166603,
      "grad_norm": 6.888854026794434,
      "learning_rate": 4.901649988541746e-05,
      "loss": 2.0903,
      "step": 10300
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 6.146206855773926,
      "learning_rate": 4.900695134061569e-05,
      "loss": 2.2039,
      "step": 10400
    },
    {
      "epoch": 0.8020777633488656,
      "grad_norm": 7.039429187774658,
      "learning_rate": 4.899740279581392e-05,
      "loss": 2.2645,
      "step": 10500
    },
    {
      "epoch": 0.8097165991902834,
      "grad_norm": 5.6176276206970215,
      "learning_rate": 4.898785425101215e-05,
      "loss": 2.1505,
      "step": 10600
    },
    {
      "epoch": 0.8173554350317012,
      "grad_norm": 7.000094890594482,
      "learning_rate": 4.897830570621038e-05,
      "loss": 2.2589,
      "step": 10700
    },
    {
      "epoch": 0.8249942708731189,
      "grad_norm": 5.843156814575195,
      "learning_rate": 4.89687571614086e-05,
      "loss": 2.1184,
      "step": 10800
    },
    {
      "epoch": 0.8326331067145367,
      "grad_norm": 6.515773773193359,
      "learning_rate": 4.8959208616606835e-05,
      "loss": 2.2086,
      "step": 10900
    },
    {
      "epoch": 0.8402719425559545,
      "grad_norm": 7.739624500274658,
      "learning_rate": 4.894966007180506e-05,
      "loss": 2.1981,
      "step": 11000
    },
    {
      "epoch": 0.8479107783973723,
      "grad_norm": 10.633212089538574,
      "learning_rate": 4.8940111527003286e-05,
      "loss": 2.1193,
      "step": 11100
    },
    {
      "epoch": 0.8555496142387901,
      "grad_norm": 5.956071376800537,
      "learning_rate": 4.8930562982201514e-05,
      "loss": 2.1799,
      "step": 11200
    },
    {
      "epoch": 0.8631884500802077,
      "grad_norm": 6.470799446105957,
      "learning_rate": 4.892101443739974e-05,
      "loss": 2.1821,
      "step": 11300
    },
    {
      "epoch": 0.8708272859216255,
      "grad_norm": 7.282951831817627,
      "learning_rate": 4.891146589259797e-05,
      "loss": 2.2075,
      "step": 11400
    },
    {
      "epoch": 0.8784661217630433,
      "grad_norm": 7.042235374450684,
      "learning_rate": 4.89019173477962e-05,
      "loss": 2.1455,
      "step": 11500
    },
    {
      "epoch": 0.8861049576044611,
      "grad_norm": 6.879594802856445,
      "learning_rate": 4.889236880299443e-05,
      "loss": 2.2092,
      "step": 11600
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 6.679892063140869,
      "learning_rate": 4.888282025819265e-05,
      "loss": 2.1634,
      "step": 11700
    },
    {
      "epoch": 0.9013826292872966,
      "grad_norm": 7.85321569442749,
      "learning_rate": 4.8873271713390887e-05,
      "loss": 2.2462,
      "step": 11800
    },
    {
      "epoch": 0.9090214651287144,
      "grad_norm": 7.814871788024902,
      "learning_rate": 4.886372316858911e-05,
      "loss": 2.2701,
      "step": 11900
    },
    {
      "epoch": 0.9166603009701322,
      "grad_norm": 6.750171184539795,
      "learning_rate": 4.885417462378734e-05,
      "loss": 2.1487,
      "step": 12000
    },
    {
      "epoch": 0.92429913681155,
      "grad_norm": 6.45812463760376,
      "learning_rate": 4.8844626078985566e-05,
      "loss": 2.1453,
      "step": 12100
    },
    {
      "epoch": 0.9319379726529677,
      "grad_norm": 6.799169063568115,
      "learning_rate": 4.8835077534183795e-05,
      "loss": 2.2721,
      "step": 12200
    },
    {
      "epoch": 0.9395768084943854,
      "grad_norm": 6.565350532531738,
      "learning_rate": 4.8825528989382016e-05,
      "loss": 2.1543,
      "step": 12300
    },
    {
      "epoch": 0.9472156443358032,
      "grad_norm": 8.927435874938965,
      "learning_rate": 4.881598044458025e-05,
      "loss": 2.1455,
      "step": 12400
    },
    {
      "epoch": 0.954854480177221,
      "grad_norm": 6.2788896560668945,
      "learning_rate": 4.8806431899778474e-05,
      "loss": 2.1331,
      "step": 12500
    },
    {
      "epoch": 0.9624933160186387,
      "grad_norm": 8.168878555297852,
      "learning_rate": 4.87968833549767e-05,
      "loss": 2.0968,
      "step": 12600
    },
    {
      "epoch": 0.9701321518600565,
      "grad_norm": 6.079451560974121,
      "learning_rate": 4.878733481017493e-05,
      "loss": 2.0883,
      "step": 12700
    },
    {
      "epoch": 0.9777709877014743,
      "grad_norm": 5.483613014221191,
      "learning_rate": 4.877778626537316e-05,
      "loss": 2.1369,
      "step": 12800
    },
    {
      "epoch": 0.9854098235428921,
      "grad_norm": 7.280965328216553,
      "learning_rate": 4.876823772057139e-05,
      "loss": 2.1447,
      "step": 12900
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 6.0614705085754395,
      "learning_rate": 4.875868917576962e-05,
      "loss": 2.1192,
      "step": 13000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.0605130195617676,
      "eval_runtime": 2.9739,
      "eval_samples_per_second": 232.022,
      "eval_steps_per_second": 232.022,
      "step": 13091
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.964215874671936,
      "eval_runtime": 36.4766,
      "eval_samples_per_second": 358.887,
      "eval_steps_per_second": 358.887,
      "step": 13091
    },
    {
      "epoch": 1.0006874952257276,
      "grad_norm": 5.976796627044678,
      "learning_rate": 4.8749140630967846e-05,
      "loss": 2.2336,
      "step": 13100
    },
    {
      "epoch": 1.0083263310671453,
      "grad_norm": 5.739107608795166,
      "learning_rate": 4.873959208616607e-05,
      "loss": 2.1473,
      "step": 13200
    },
    {
      "epoch": 1.0159651669085632,
      "grad_norm": 6.680917263031006,
      "learning_rate": 4.87300435413643e-05,
      "loss": 2.1207,
      "step": 13300
    },
    {
      "epoch": 1.0236040027499809,
      "grad_norm": 7.165970325469971,
      "learning_rate": 4.8720494996562525e-05,
      "loss": 2.1709,
      "step": 13400
    },
    {
      "epoch": 1.0312428385913988,
      "grad_norm": 9.24274730682373,
      "learning_rate": 4.8710946451760754e-05,
      "loss": 2.1452,
      "step": 13500
    },
    {
      "epoch": 1.0388816744328164,
      "grad_norm": 6.563086032867432,
      "learning_rate": 4.870139790695898e-05,
      "loss": 2.0929,
      "step": 13600
    },
    {
      "epoch": 1.046520510274234,
      "grad_norm": 5.805669784545898,
      "learning_rate": 4.869184936215721e-05,
      "loss": 2.0913,
      "step": 13700
    },
    {
      "epoch": 1.054159346115652,
      "grad_norm": 5.48603630065918,
      "learning_rate": 4.868230081735543e-05,
      "loss": 2.0956,
      "step": 13800
    },
    {
      "epoch": 1.0617981819570697,
      "grad_norm": 5.809831619262695,
      "learning_rate": 4.867275227255367e-05,
      "loss": 2.1545,
      "step": 13900
    },
    {
      "epoch": 1.0694370177984875,
      "grad_norm": 5.466857433319092,
      "learning_rate": 4.866320372775189e-05,
      "loss": 2.1173,
      "step": 14000
    },
    {
      "epoch": 1.0770758536399052,
      "grad_norm": 5.6993794441223145,
      "learning_rate": 4.865365518295012e-05,
      "loss": 2.1158,
      "step": 14100
    },
    {
      "epoch": 1.084714689481323,
      "grad_norm": 7.3900017738342285,
      "learning_rate": 4.864410663814835e-05,
      "loss": 2.1329,
      "step": 14200
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 7.998657703399658,
      "learning_rate": 4.863455809334658e-05,
      "loss": 2.0912,
      "step": 14300
    },
    {
      "epoch": 1.0999923611641587,
      "grad_norm": 7.3166680335998535,
      "learning_rate": 4.8625009548544805e-05,
      "loss": 2.1136,
      "step": 14400
    },
    {
      "epoch": 1.1076311970055763,
      "grad_norm": 6.926626682281494,
      "learning_rate": 4.861546100374303e-05,
      "loss": 2.06,
      "step": 14500
    },
    {
      "epoch": 1.1152700328469942,
      "grad_norm": 6.549658298492432,
      "learning_rate": 4.860591245894126e-05,
      "loss": 2.0724,
      "step": 14600
    },
    {
      "epoch": 1.1229088686884119,
      "grad_norm": 6.694014072418213,
      "learning_rate": 4.8596363914139485e-05,
      "loss": 2.0763,
      "step": 14700
    },
    {
      "epoch": 1.1305477045298296,
      "grad_norm": 5.7263946533203125,
      "learning_rate": 4.8586815369337713e-05,
      "loss": 2.0898,
      "step": 14800
    },
    {
      "epoch": 1.1381865403712474,
      "grad_norm": 5.81451940536499,
      "learning_rate": 4.857726682453594e-05,
      "loss": 2.1451,
      "step": 14900
    },
    {
      "epoch": 1.1458253762126651,
      "grad_norm": 9.874438285827637,
      "learning_rate": 4.856771827973417e-05,
      "loss": 2.2135,
      "step": 15000
    },
    {
      "epoch": 1.153464212054083,
      "grad_norm": 4.557074069976807,
      "learning_rate": 4.855816973493239e-05,
      "loss": 2.1432,
      "step": 15100
    },
    {
      "epoch": 1.1611030478955007,
      "grad_norm": 4.87415075302124,
      "learning_rate": 4.854862119013063e-05,
      "loss": 2.0759,
      "step": 15200
    },
    {
      "epoch": 1.1687418837369186,
      "grad_norm": 7.1597723960876465,
      "learning_rate": 4.853907264532886e-05,
      "loss": 2.1188,
      "step": 15300
    },
    {
      "epoch": 1.1763807195783362,
      "grad_norm": 6.168932914733887,
      "learning_rate": 4.852952410052708e-05,
      "loss": 2.1498,
      "step": 15400
    },
    {
      "epoch": 1.1840195554197541,
      "grad_norm": 6.645564079284668,
      "learning_rate": 4.8519975555725314e-05,
      "loss": 2.1152,
      "step": 15500
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 6.894056797027588,
      "learning_rate": 4.8510427010923536e-05,
      "loss": 2.1489,
      "step": 15600
    },
    {
      "epoch": 1.1992972271025897,
      "grad_norm": 6.829993724822998,
      "learning_rate": 4.8500878466121765e-05,
      "loss": 2.0378,
      "step": 15700
    },
    {
      "epoch": 1.2069360629440073,
      "grad_norm": 6.725090980529785,
      "learning_rate": 4.8491329921319994e-05,
      "loss": 2.1796,
      "step": 15800
    },
    {
      "epoch": 1.214574898785425,
      "grad_norm": 6.777667999267578,
      "learning_rate": 4.848178137651822e-05,
      "loss": 2.0818,
      "step": 15900
    },
    {
      "epoch": 1.222213734626843,
      "grad_norm": 7.45310640335083,
      "learning_rate": 4.8472232831716444e-05,
      "loss": 2.1842,
      "step": 16000
    },
    {
      "epoch": 1.2298525704682606,
      "grad_norm": 6.648919105529785,
      "learning_rate": 4.846268428691468e-05,
      "loss": 2.1156,
      "step": 16100
    },
    {
      "epoch": 1.2374914063096785,
      "grad_norm": 7.586540222167969,
      "learning_rate": 4.84531357421129e-05,
      "loss": 2.1148,
      "step": 16200
    },
    {
      "epoch": 1.2451302421510961,
      "grad_norm": 6.660101413726807,
      "learning_rate": 4.844358719731113e-05,
      "loss": 2.2371,
      "step": 16300
    },
    {
      "epoch": 1.252769077992514,
      "grad_norm": 7.958584785461426,
      "learning_rate": 4.843403865250936e-05,
      "loss": 2.2351,
      "step": 16400
    },
    {
      "epoch": 1.2604079138339317,
      "grad_norm": 8.549927711486816,
      "learning_rate": 4.842449010770759e-05,
      "loss": 2.1212,
      "step": 16500
    },
    {
      "epoch": 1.2680467496753494,
      "grad_norm": 7.2710089683532715,
      "learning_rate": 4.8414941562905816e-05,
      "loss": 2.1221,
      "step": 16600
    },
    {
      "epoch": 1.2756855855167673,
      "grad_norm": 5.3004231452941895,
      "learning_rate": 4.8405393018104045e-05,
      "loss": 2.1152,
      "step": 16700
    },
    {
      "epoch": 1.2833244213581851,
      "grad_norm": 6.430453300476074,
      "learning_rate": 4.8395844473302274e-05,
      "loss": 2.1212,
      "step": 16800
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 7.614453315734863,
      "learning_rate": 4.8386295928500496e-05,
      "loss": 2.1312,
      "step": 16900
    },
    {
      "epoch": 1.2986020930410205,
      "grad_norm": 7.870596885681152,
      "learning_rate": 4.837674738369873e-05,
      "loss": 2.0513,
      "step": 17000
    },
    {
      "epoch": 1.3062409288824384,
      "grad_norm": 6.774415016174316,
      "learning_rate": 4.836719883889695e-05,
      "loss": 2.1048,
      "step": 17100
    },
    {
      "epoch": 1.313879764723856,
      "grad_norm": 4.5582499504089355,
      "learning_rate": 4.835765029409518e-05,
      "loss": 2.0919,
      "step": 17200
    },
    {
      "epoch": 1.321518600565274,
      "grad_norm": 5.6097731590271,
      "learning_rate": 4.834810174929341e-05,
      "loss": 2.1085,
      "step": 17300
    },
    {
      "epoch": 1.3291574364066916,
      "grad_norm": 5.653775215148926,
      "learning_rate": 4.833855320449164e-05,
      "loss": 2.1336,
      "step": 17400
    },
    {
      "epoch": 1.3367962722481095,
      "grad_norm": 5.48486852645874,
      "learning_rate": 4.832900465968986e-05,
      "loss": 2.039,
      "step": 17500
    },
    {
      "epoch": 1.3444351080895272,
      "grad_norm": 4.896012783050537,
      "learning_rate": 4.8319456114888097e-05,
      "loss": 2.0115,
      "step": 17600
    },
    {
      "epoch": 1.3520739439309448,
      "grad_norm": 4.84825325012207,
      "learning_rate": 4.830990757008632e-05,
      "loss": 2.1481,
      "step": 17700
    },
    {
      "epoch": 1.3597127797723627,
      "grad_norm": 6.462705612182617,
      "learning_rate": 4.830035902528455e-05,
      "loss": 2.0996,
      "step": 17800
    },
    {
      "epoch": 1.3673516156137804,
      "grad_norm": 5.775611877441406,
      "learning_rate": 4.8290810480482776e-05,
      "loss": 2.1751,
      "step": 17900
    },
    {
      "epoch": 1.3749904514551983,
      "grad_norm": 6.040096282958984,
      "learning_rate": 4.8281261935681005e-05,
      "loss": 2.0795,
      "step": 18000
    },
    {
      "epoch": 1.382629287296616,
      "grad_norm": 14.455910682678223,
      "learning_rate": 4.827171339087923e-05,
      "loss": 2.2635,
      "step": 18100
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 7.291331768035889,
      "learning_rate": 4.826216484607746e-05,
      "loss": 2.1056,
      "step": 18200
    },
    {
      "epoch": 1.3979069589794515,
      "grad_norm": 6.325390815734863,
      "learning_rate": 4.825261630127569e-05,
      "loss": 2.1085,
      "step": 18300
    },
    {
      "epoch": 1.4055457948208692,
      "grad_norm": 6.30556058883667,
      "learning_rate": 4.824306775647391e-05,
      "loss": 2.0872,
      "step": 18400
    },
    {
      "epoch": 1.413184630662287,
      "grad_norm": 6.4961371421813965,
      "learning_rate": 4.823351921167215e-05,
      "loss": 2.1252,
      "step": 18500
    },
    {
      "epoch": 1.420823466503705,
      "grad_norm": 7.309135437011719,
      "learning_rate": 4.822397066687037e-05,
      "loss": 2.0696,
      "step": 18600
    },
    {
      "epoch": 1.4284623023451226,
      "grad_norm": 5.6706862449646,
      "learning_rate": 4.82144221220686e-05,
      "loss": 2.0141,
      "step": 18700
    },
    {
      "epoch": 1.4361011381865403,
      "grad_norm": 8.420165061950684,
      "learning_rate": 4.820487357726683e-05,
      "loss": 2.0232,
      "step": 18800
    },
    {
      "epoch": 1.4437399740279582,
      "grad_norm": 6.647212505340576,
      "learning_rate": 4.8195325032465056e-05,
      "loss": 2.0762,
      "step": 18900
    },
    {
      "epoch": 1.4513788098693758,
      "grad_norm": 6.73564338684082,
      "learning_rate": 4.818577648766328e-05,
      "loss": 2.0803,
      "step": 19000
    },
    {
      "epoch": 1.4590176457107937,
      "grad_norm": 6.335738658905029,
      "learning_rate": 4.817622794286151e-05,
      "loss": 2.1247,
      "step": 19100
    },
    {
      "epoch": 1.4666564815522114,
      "grad_norm": 6.929213523864746,
      "learning_rate": 4.8166679398059735e-05,
      "loss": 2.1158,
      "step": 19200
    },
    {
      "epoch": 1.4742953173936293,
      "grad_norm": 10.119876861572266,
      "learning_rate": 4.8157130853257964e-05,
      "loss": 2.0233,
      "step": 19300
    },
    {
      "epoch": 1.481934153235047,
      "grad_norm": 7.158876419067383,
      "learning_rate": 4.81475823084562e-05,
      "loss": 2.1422,
      "step": 19400
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 6.830562114715576,
      "learning_rate": 4.813803376365442e-05,
      "loss": 2.0661,
      "step": 19500
    },
    {
      "epoch": 1.4972118249178825,
      "grad_norm": 10.07793140411377,
      "learning_rate": 4.812848521885265e-05,
      "loss": 2.131,
      "step": 19600
    },
    {
      "epoch": 1.5048506607593004,
      "grad_norm": 8.671719551086426,
      "learning_rate": 4.811893667405088e-05,
      "loss": 1.9824,
      "step": 19700
    },
    {
      "epoch": 1.512489496600718,
      "grad_norm": 7.908980846405029,
      "learning_rate": 4.810938812924911e-05,
      "loss": 2.0756,
      "step": 19800
    },
    {
      "epoch": 1.5201283324421357,
      "grad_norm": 8.15880012512207,
      "learning_rate": 4.809983958444733e-05,
      "loss": 2.1846,
      "step": 19900
    },
    {
      "epoch": 1.5277671682835536,
      "grad_norm": 7.454707622528076,
      "learning_rate": 4.8090291039645565e-05,
      "loss": 2.12,
      "step": 20000
    },
    {
      "epoch": 1.5354060041249713,
      "grad_norm": 7.984645366668701,
      "learning_rate": 4.808074249484379e-05,
      "loss": 2.0787,
      "step": 20100
    },
    {
      "epoch": 1.543044839966389,
      "grad_norm": 6.085328102111816,
      "learning_rate": 4.8071193950042015e-05,
      "loss": 2.0866,
      "step": 20200
    },
    {
      "epoch": 1.5506836758078069,
      "grad_norm": 6.883598804473877,
      "learning_rate": 4.8061645405240244e-05,
      "loss": 2.0859,
      "step": 20300
    },
    {
      "epoch": 1.5583225116492248,
      "grad_norm": 4.631409645080566,
      "learning_rate": 4.805209686043847e-05,
      "loss": 2.0848,
      "step": 20400
    },
    {
      "epoch": 1.5659613474906424,
      "grad_norm": 6.208423614501953,
      "learning_rate": 4.80425483156367e-05,
      "loss": 2.0695,
      "step": 20500
    },
    {
      "epoch": 1.57360018333206,
      "grad_norm": 5.781252384185791,
      "learning_rate": 4.8032999770834923e-05,
      "loss": 2.0689,
      "step": 20600
    },
    {
      "epoch": 1.581239019173478,
      "grad_norm": 7.69540548324585,
      "learning_rate": 4.802345122603316e-05,
      "loss": 2.1134,
      "step": 20700
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 7.608880996704102,
      "learning_rate": 4.801390268123138e-05,
      "loss": 2.1149,
      "step": 20800
    },
    {
      "epoch": 1.5965166908563135,
      "grad_norm": 5.767796993255615,
      "learning_rate": 4.800435413642961e-05,
      "loss": 2.0865,
      "step": 20900
    },
    {
      "epoch": 1.6041555266977312,
      "grad_norm": 7.411304473876953,
      "learning_rate": 4.799480559162784e-05,
      "loss": 2.1134,
      "step": 21000
    },
    {
      "epoch": 1.611794362539149,
      "grad_norm": 9.366665840148926,
      "learning_rate": 4.798525704682607e-05,
      "loss": 2.2083,
      "step": 21100
    },
    {
      "epoch": 1.6194331983805668,
      "grad_norm": 5.280661582946777,
      "learning_rate": 4.797570850202429e-05,
      "loss": 2.1291,
      "step": 21200
    },
    {
      "epoch": 1.6270720342219844,
      "grad_norm": 7.239759922027588,
      "learning_rate": 4.7966159957222524e-05,
      "loss": 2.1111,
      "step": 21300
    },
    {
      "epoch": 1.6347108700634023,
      "grad_norm": 6.495144367218018,
      "learning_rate": 4.7956611412420746e-05,
      "loss": 2.1279,
      "step": 21400
    },
    {
      "epoch": 1.6423497059048202,
      "grad_norm": 11.616400718688965,
      "learning_rate": 4.7947062867618975e-05,
      "loss": 2.1289,
      "step": 21500
    },
    {
      "epoch": 1.6499885417462379,
      "grad_norm": 6.318342685699463,
      "learning_rate": 4.7937514322817204e-05,
      "loss": 2.0135,
      "step": 21600
    },
    {
      "epoch": 1.6576273775876555,
      "grad_norm": 6.691891193389893,
      "learning_rate": 4.792796577801543e-05,
      "loss": 2.1528,
      "step": 21700
    },
    {
      "epoch": 1.6652662134290734,
      "grad_norm": 6.510889530181885,
      "learning_rate": 4.791841723321366e-05,
      "loss": 2.0998,
      "step": 21800
    },
    {
      "epoch": 1.6729050492704913,
      "grad_norm": 6.448395252227783,
      "learning_rate": 4.790886868841189e-05,
      "loss": 2.0227,
      "step": 21900
    },
    {
      "epoch": 1.680543885111909,
      "grad_norm": 7.7097883224487305,
      "learning_rate": 4.789932014361012e-05,
      "loss": 2.1052,
      "step": 22000
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 7.259430408477783,
      "learning_rate": 4.788977159880834e-05,
      "loss": 1.9969,
      "step": 22100
    },
    {
      "epoch": 1.6958215567947446,
      "grad_norm": 5.595186710357666,
      "learning_rate": 4.7880223054006576e-05,
      "loss": 2.051,
      "step": 22200
    },
    {
      "epoch": 1.7034603926361622,
      "grad_norm": 7.063502788543701,
      "learning_rate": 4.78706745092048e-05,
      "loss": 2.0621,
      "step": 22300
    },
    {
      "epoch": 1.71109922847758,
      "grad_norm": 4.706174850463867,
      "learning_rate": 4.7861125964403026e-05,
      "loss": 2.0619,
      "step": 22400
    },
    {
      "epoch": 1.7187380643189978,
      "grad_norm": 6.189508438110352,
      "learning_rate": 4.7851577419601255e-05,
      "loss": 2.073,
      "step": 22500
    },
    {
      "epoch": 1.7263769001604157,
      "grad_norm": 5.720125198364258,
      "learning_rate": 4.7842028874799484e-05,
      "loss": 2.1133,
      "step": 22600
    },
    {
      "epoch": 1.7340157360018333,
      "grad_norm": 6.075953483581543,
      "learning_rate": 4.7832480329997706e-05,
      "loss": 2.1745,
      "step": 22700
    },
    {
      "epoch": 1.741654571843251,
      "grad_norm": 5.3307204246521,
      "learning_rate": 4.782293178519594e-05,
      "loss": 2.1429,
      "step": 22800
    },
    {
      "epoch": 1.749293407684669,
      "grad_norm": 7.866816997528076,
      "learning_rate": 4.781338324039416e-05,
      "loss": 2.0203,
      "step": 22900
    },
    {
      "epoch": 1.7569322435260868,
      "grad_norm": 6.780681610107422,
      "learning_rate": 4.780383469559239e-05,
      "loss": 2.0573,
      "step": 23000
    },
    {
      "epoch": 1.7645710793675042,
      "grad_norm": 19.883563995361328,
      "learning_rate": 4.779428615079062e-05,
      "loss": 2.0596,
      "step": 23100
    },
    {
      "epoch": 1.7722099152089221,
      "grad_norm": 6.18963098526001,
      "learning_rate": 4.778473760598885e-05,
      "loss": 2.0956,
      "step": 23200
    },
    {
      "epoch": 1.77984875105034,
      "grad_norm": 5.8509368896484375,
      "learning_rate": 4.777518906118708e-05,
      "loss": 2.1604,
      "step": 23300
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 7.531468868255615,
      "learning_rate": 4.7765640516385307e-05,
      "loss": 2.0937,
      "step": 23400
    },
    {
      "epoch": 1.7951264227331754,
      "grad_norm": 5.8480448722839355,
      "learning_rate": 4.7756091971583535e-05,
      "loss": 2.0255,
      "step": 23500
    },
    {
      "epoch": 1.8027652585745932,
      "grad_norm": 6.717202663421631,
      "learning_rate": 4.774654342678176e-05,
      "loss": 2.1257,
      "step": 23600
    },
    {
      "epoch": 1.8104040944160111,
      "grad_norm": 8.620172500610352,
      "learning_rate": 4.773699488197999e-05,
      "loss": 2.1146,
      "step": 23700
    },
    {
      "epoch": 1.8180429302574288,
      "grad_norm": 6.350916862487793,
      "learning_rate": 4.7727446337178215e-05,
      "loss": 2.1073,
      "step": 23800
    },
    {
      "epoch": 1.8256817660988465,
      "grad_norm": 7.68890380859375,
      "learning_rate": 4.771789779237644e-05,
      "loss": 2.045,
      "step": 23900
    },
    {
      "epoch": 1.8333206019402644,
      "grad_norm": 6.118442535400391,
      "learning_rate": 4.770834924757467e-05,
      "loss": 2.1102,
      "step": 24000
    },
    {
      "epoch": 1.840959437781682,
      "grad_norm": 7.501459121704102,
      "learning_rate": 4.76988007027729e-05,
      "loss": 2.003,
      "step": 24100
    },
    {
      "epoch": 1.8485982736230997,
      "grad_norm": 7.404692649841309,
      "learning_rate": 4.768925215797113e-05,
      "loss": 2.1705,
      "step": 24200
    },
    {
      "epoch": 1.8562371094645176,
      "grad_norm": 5.780813217163086,
      "learning_rate": 4.767970361316936e-05,
      "loss": 1.9208,
      "step": 24300
    },
    {
      "epoch": 1.8638759453059355,
      "grad_norm": 8.854307174682617,
      "learning_rate": 4.767015506836759e-05,
      "loss": 2.0863,
      "step": 24400
    },
    {
      "epoch": 1.8715147811473531,
      "grad_norm": 6.320497035980225,
      "learning_rate": 4.766060652356581e-05,
      "loss": 2.1524,
      "step": 24500
    },
    {
      "epoch": 1.8791536169887708,
      "grad_norm": 5.635225772857666,
      "learning_rate": 4.7651057978764044e-05,
      "loss": 2.1911,
      "step": 24600
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 9.815608024597168,
      "learning_rate": 4.7641509433962266e-05,
      "loss": 2.1289,
      "step": 24700
    },
    {
      "epoch": 1.8944312886716066,
      "grad_norm": 7.404934406280518,
      "learning_rate": 4.7631960889160495e-05,
      "loss": 2.0928,
      "step": 24800
    },
    {
      "epoch": 1.9020701245130243,
      "grad_norm": 7.061444282531738,
      "learning_rate": 4.762241234435872e-05,
      "loss": 2.0445,
      "step": 24900
    },
    {
      "epoch": 1.909708960354442,
      "grad_norm": 6.627874851226807,
      "learning_rate": 4.761286379955695e-05,
      "loss": 2.0147,
      "step": 25000
    },
    {
      "epoch": 1.9173477961958598,
      "grad_norm": 6.008119583129883,
      "learning_rate": 4.7603315254755174e-05,
      "loss": 2.0846,
      "step": 25100
    },
    {
      "epoch": 1.9249866320372775,
      "grad_norm": 7.064857482910156,
      "learning_rate": 4.759376670995341e-05,
      "loss": 2.1278,
      "step": 25200
    },
    {
      "epoch": 1.9326254678786952,
      "grad_norm": 5.387286186218262,
      "learning_rate": 4.758421816515163e-05,
      "loss": 2.0353,
      "step": 25300
    },
    {
      "epoch": 1.940264303720113,
      "grad_norm": 8.503556251525879,
      "learning_rate": 4.757466962034986e-05,
      "loss": 2.188,
      "step": 25400
    },
    {
      "epoch": 1.947903139561531,
      "grad_norm": 7.054375648498535,
      "learning_rate": 4.756512107554809e-05,
      "loss": 2.1458,
      "step": 25500
    },
    {
      "epoch": 1.9555419754029486,
      "grad_norm": 5.338288307189941,
      "learning_rate": 4.755557253074632e-05,
      "loss": 2.1058,
      "step": 25600
    },
    {
      "epoch": 1.9631808112443663,
      "grad_norm": 7.746931076049805,
      "learning_rate": 4.7546023985944546e-05,
      "loss": 2.0863,
      "step": 25700
    },
    {
      "epoch": 1.9708196470857842,
      "grad_norm": 6.400956630706787,
      "learning_rate": 4.7536475441142775e-05,
      "loss": 2.1427,
      "step": 25800
    },
    {
      "epoch": 1.978458482927202,
      "grad_norm": 10.486132621765137,
      "learning_rate": 4.7526926896341004e-05,
      "loss": 2.0076,
      "step": 25900
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 8.27220344543457,
      "learning_rate": 4.7517378351539225e-05,
      "loss": 2.0328,
      "step": 26000
    },
    {
      "epoch": 1.9937361546100374,
      "grad_norm": 5.7237935066223145,
      "learning_rate": 4.7507829806737454e-05,
      "loss": 2.2377,
      "step": 26100
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9962618350982666,
      "eval_runtime": 1.4704,
      "eval_samples_per_second": 469.274,
      "eval_steps_per_second": 469.274,
      "step": 26182
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8800349235534668,
      "eval_runtime": 29.0522,
      "eval_samples_per_second": 450.603,
      "eval_steps_per_second": 450.603,
      "step": 26182
    },
    {
      "epoch": 2.0013749904514553,
      "grad_norm": 6.817381858825684,
      "learning_rate": 4.749828126193568e-05,
      "loss": 2.0565,
      "step": 26200
    },
    {
      "epoch": 2.009013826292873,
      "grad_norm": 11.244606018066406,
      "learning_rate": 4.748873271713391e-05,
      "loss": 2.0057,
      "step": 26300
    },
    {
      "epoch": 2.0166526621342906,
      "grad_norm": 6.868297100067139,
      "learning_rate": 4.7479184172332133e-05,
      "loss": 2.0217,
      "step": 26400
    },
    {
      "epoch": 2.0242914979757085,
      "grad_norm": 6.660096645355225,
      "learning_rate": 4.746963562753037e-05,
      "loss": 1.9928,
      "step": 26500
    },
    {
      "epoch": 2.0319303338171264,
      "grad_norm": 6.91060209274292,
      "learning_rate": 4.746008708272859e-05,
      "loss": 1.9819,
      "step": 26600
    },
    {
      "epoch": 2.039569169658544,
      "grad_norm": 7.778163433074951,
      "learning_rate": 4.745053853792682e-05,
      "loss": 1.9089,
      "step": 26700
    },
    {
      "epoch": 2.0472080054999617,
      "grad_norm": 7.306168079376221,
      "learning_rate": 4.744098999312505e-05,
      "loss": 1.9492,
      "step": 26800
    },
    {
      "epoch": 2.0548468413413796,
      "grad_norm": 7.843794822692871,
      "learning_rate": 4.743144144832328e-05,
      "loss": 2.0876,
      "step": 26900
    },
    {
      "epoch": 2.0624856771827975,
      "grad_norm": 7.8681960105896,
      "learning_rate": 4.7421892903521506e-05,
      "loss": 2.0907,
      "step": 27000
    },
    {
      "epoch": 2.070124513024215,
      "grad_norm": 8.103500366210938,
      "learning_rate": 4.7412344358719734e-05,
      "loss": 1.9656,
      "step": 27100
    },
    {
      "epoch": 2.077763348865633,
      "grad_norm": 7.608842372894287,
      "learning_rate": 4.740279581391796e-05,
      "loss": 2.1192,
      "step": 27200
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 7.439151763916016,
      "learning_rate": 4.7393247269116185e-05,
      "loss": 2.0896,
      "step": 27300
    },
    {
      "epoch": 2.093041020548468,
      "grad_norm": 7.122698783874512,
      "learning_rate": 4.738369872431442e-05,
      "loss": 2.1007,
      "step": 27400
    },
    {
      "epoch": 2.100679856389886,
      "grad_norm": 8.457472801208496,
      "learning_rate": 4.737415017951264e-05,
      "loss": 2.0949,
      "step": 27500
    },
    {
      "epoch": 2.108318692231304,
      "grad_norm": 6.767539978027344,
      "learning_rate": 4.736460163471087e-05,
      "loss": 2.0088,
      "step": 27600
    },
    {
      "epoch": 2.115957528072722,
      "grad_norm": 10.456007957458496,
      "learning_rate": 4.73550530899091e-05,
      "loss": 2.1249,
      "step": 27700
    },
    {
      "epoch": 2.1235963639141393,
      "grad_norm": 6.959116458892822,
      "learning_rate": 4.734550454510733e-05,
      "loss": 2.0668,
      "step": 27800
    },
    {
      "epoch": 2.131235199755557,
      "grad_norm": 6.9348554611206055,
      "learning_rate": 4.733595600030555e-05,
      "loss": 2.0704,
      "step": 27900
    },
    {
      "epoch": 2.138874035596975,
      "grad_norm": 7.02557897567749,
      "learning_rate": 4.7326407455503786e-05,
      "loss": 1.9644,
      "step": 28000
    },
    {
      "epoch": 2.146512871438393,
      "grad_norm": 5.740081310272217,
      "learning_rate": 4.731685891070201e-05,
      "loss": 1.9997,
      "step": 28100
    },
    {
      "epoch": 2.1541517072798104,
      "grad_norm": 5.333535671234131,
      "learning_rate": 4.7307310365900236e-05,
      "loss": 2.034,
      "step": 28200
    },
    {
      "epoch": 2.1617905431212283,
      "grad_norm": 7.8875627517700195,
      "learning_rate": 4.729776182109847e-05,
      "loss": 2.064,
      "step": 28300
    },
    {
      "epoch": 2.169429378962646,
      "grad_norm": 6.151160717010498,
      "learning_rate": 4.7288213276296694e-05,
      "loss": 2.0566,
      "step": 28400
    },
    {
      "epoch": 2.1770682148040637,
      "grad_norm": 6.696822166442871,
      "learning_rate": 4.727866473149492e-05,
      "loss": 2.1381,
      "step": 28500
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 8.55803394317627,
      "learning_rate": 4.726911618669315e-05,
      "loss": 2.0781,
      "step": 28600
    },
    {
      "epoch": 2.1923458864868994,
      "grad_norm": 8.144116401672363,
      "learning_rate": 4.725956764189138e-05,
      "loss": 2.1067,
      "step": 28700
    },
    {
      "epoch": 2.1999847223283173,
      "grad_norm": 7.96404504776001,
      "learning_rate": 4.72500190970896e-05,
      "loss": 2.0211,
      "step": 28800
    },
    {
      "epoch": 2.2076235581697348,
      "grad_norm": 4.944112777709961,
      "learning_rate": 4.724047055228784e-05,
      "loss": 2.0507,
      "step": 28900
    },
    {
      "epoch": 2.2152623940111527,
      "grad_norm": 7.3640546798706055,
      "learning_rate": 4.723092200748606e-05,
      "loss": 2.0425,
      "step": 29000
    },
    {
      "epoch": 2.2229012298525705,
      "grad_norm": 4.651801586151123,
      "learning_rate": 4.722137346268429e-05,
      "loss": 2.0633,
      "step": 29100
    },
    {
      "epoch": 2.2305400656939884,
      "grad_norm": 6.244165420532227,
      "learning_rate": 4.7211824917882517e-05,
      "loss": 2.0743,
      "step": 29200
    },
    {
      "epoch": 2.238178901535406,
      "grad_norm": 5.743988990783691,
      "learning_rate": 4.7202276373080745e-05,
      "loss": 2.0717,
      "step": 29300
    },
    {
      "epoch": 2.2458177373768238,
      "grad_norm": 7.269573211669922,
      "learning_rate": 4.7192727828278974e-05,
      "loss": 2.023,
      "step": 29400
    },
    {
      "epoch": 2.2534565732182417,
      "grad_norm": 10.042598724365234,
      "learning_rate": 4.71831792834772e-05,
      "loss": 2.0277,
      "step": 29500
    },
    {
      "epoch": 2.261095409059659,
      "grad_norm": 5.945451736450195,
      "learning_rate": 4.717363073867543e-05,
      "loss": 2.0532,
      "step": 29600
    },
    {
      "epoch": 2.268734244901077,
      "grad_norm": 6.320197582244873,
      "learning_rate": 4.716408219387365e-05,
      "loss": 2.0466,
      "step": 29700
    },
    {
      "epoch": 2.276373080742495,
      "grad_norm": 7.087867259979248,
      "learning_rate": 4.715453364907189e-05,
      "loss": 2.1105,
      "step": 29800
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 7.793264865875244,
      "learning_rate": 4.714498510427011e-05,
      "loss": 2.0119,
      "step": 29900
    },
    {
      "epoch": 2.2916507524253302,
      "grad_norm": 6.077270984649658,
      "learning_rate": 4.713543655946834e-05,
      "loss": 1.9424,
      "step": 30000
    },
    {
      "epoch": 2.299289588266748,
      "grad_norm": 7.086963653564453,
      "learning_rate": 4.712588801466657e-05,
      "loss": 2.0196,
      "step": 30100
    },
    {
      "epoch": 2.306928424108166,
      "grad_norm": 10.96168041229248,
      "learning_rate": 4.71163394698648e-05,
      "loss": 2.0765,
      "step": 30200
    },
    {
      "epoch": 2.314567259949584,
      "grad_norm": 5.819302558898926,
      "learning_rate": 4.710679092506302e-05,
      "loss": 2.0435,
      "step": 30300
    },
    {
      "epoch": 2.3222060957910013,
      "grad_norm": 8.288297653198242,
      "learning_rate": 4.7097242380261254e-05,
      "loss": 2.0802,
      "step": 30400
    },
    {
      "epoch": 2.3298449316324192,
      "grad_norm": 7.493959426879883,
      "learning_rate": 4.7087693835459476e-05,
      "loss": 2.0668,
      "step": 30500
    },
    {
      "epoch": 2.337483767473837,
      "grad_norm": 6.819565296173096,
      "learning_rate": 4.7078145290657705e-05,
      "loss": 1.9919,
      "step": 30600
    },
    {
      "epoch": 2.3451226033152546,
      "grad_norm": 6.398709774017334,
      "learning_rate": 4.706859674585593e-05,
      "loss": 2.0856,
      "step": 30700
    },
    {
      "epoch": 2.3527614391566725,
      "grad_norm": 6.601033687591553,
      "learning_rate": 4.705904820105416e-05,
      "loss": 2.1292,
      "step": 30800
    },
    {
      "epoch": 2.3604002749980904,
      "grad_norm": 6.466694355010986,
      "learning_rate": 4.704949965625239e-05,
      "loss": 1.986,
      "step": 30900
    },
    {
      "epoch": 2.3680391108395082,
      "grad_norm": 6.904707431793213,
      "learning_rate": 4.703995111145062e-05,
      "loss": 1.9932,
      "step": 31000
    },
    {
      "epoch": 2.3756779466809257,
      "grad_norm": 7.548303604125977,
      "learning_rate": 4.703040256664885e-05,
      "loss": 2.0323,
      "step": 31100
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 6.41606330871582,
      "learning_rate": 4.702085402184707e-05,
      "loss": 2.0982,
      "step": 31200
    },
    {
      "epoch": 2.3909556183637615,
      "grad_norm": 6.027804374694824,
      "learning_rate": 4.7011305477045306e-05,
      "loss": 2.037,
      "step": 31300
    },
    {
      "epoch": 2.3985944542051794,
      "grad_norm": 6.4340386390686035,
      "learning_rate": 4.700175693224353e-05,
      "loss": 2.0111,
      "step": 31400
    },
    {
      "epoch": 2.406233290046597,
      "grad_norm": 8.73943042755127,
      "learning_rate": 4.6992208387441756e-05,
      "loss": 2.0277,
      "step": 31500
    },
    {
      "epoch": 2.4138721258880147,
      "grad_norm": 7.977520942687988,
      "learning_rate": 4.6982659842639985e-05,
      "loss": 2.0634,
      "step": 31600
    },
    {
      "epoch": 2.4215109617294326,
      "grad_norm": 7.668363571166992,
      "learning_rate": 4.6973111297838214e-05,
      "loss": 2.0443,
      "step": 31700
    },
    {
      "epoch": 2.42914979757085,
      "grad_norm": 6.828366756439209,
      "learning_rate": 4.6963562753036435e-05,
      "loss": 2.0871,
      "step": 31800
    },
    {
      "epoch": 2.436788633412268,
      "grad_norm": 5.7837371826171875,
      "learning_rate": 4.6954014208234664e-05,
      "loss": 2.0751,
      "step": 31900
    },
    {
      "epoch": 2.444427469253686,
      "grad_norm": 7.245515823364258,
      "learning_rate": 4.694446566343289e-05,
      "loss": 2.1526,
      "step": 32000
    },
    {
      "epoch": 2.4520663050951033,
      "grad_norm": 7.257945537567139,
      "learning_rate": 4.693491711863112e-05,
      "loss": 1.9555,
      "step": 32100
    },
    {
      "epoch": 2.459705140936521,
      "grad_norm": 6.512086868286133,
      "learning_rate": 4.692536857382935e-05,
      "loss": 2.1093,
      "step": 32200
    },
    {
      "epoch": 2.467343976777939,
      "grad_norm": 6.712020397186279,
      "learning_rate": 4.691582002902758e-05,
      "loss": 2.0778,
      "step": 32300
    },
    {
      "epoch": 2.474982812619357,
      "grad_norm": 9.774744033813477,
      "learning_rate": 4.690627148422581e-05,
      "loss": 1.9949,
      "step": 32400
    },
    {
      "epoch": 2.482621648460775,
      "grad_norm": 5.743819236755371,
      "learning_rate": 4.689672293942403e-05,
      "loss": 2.0392,
      "step": 32500
    },
    {
      "epoch": 2.4902604843021923,
      "grad_norm": 8.905407905578613,
      "learning_rate": 4.6887174394622265e-05,
      "loss": 2.0175,
      "step": 32600
    },
    {
      "epoch": 2.49789932014361,
      "grad_norm": 6.232547760009766,
      "learning_rate": 4.687762584982049e-05,
      "loss": 2.075,
      "step": 32700
    },
    {
      "epoch": 2.505538155985028,
      "grad_norm": 6.273828983306885,
      "learning_rate": 4.6868077305018716e-05,
      "loss": 2.0673,
      "step": 32800
    },
    {
      "epoch": 2.5131769918264455,
      "grad_norm": 5.690840721130371,
      "learning_rate": 4.6858528760216944e-05,
      "loss": 2.0156,
      "step": 32900
    },
    {
      "epoch": 2.5208158276678634,
      "grad_norm": 7.847567081451416,
      "learning_rate": 4.684898021541517e-05,
      "loss": 2.0722,
      "step": 33000
    },
    {
      "epoch": 2.5284546635092813,
      "grad_norm": 5.735627174377441,
      "learning_rate": 4.68394316706134e-05,
      "loss": 1.9621,
      "step": 33100
    },
    {
      "epoch": 2.5360934993506987,
      "grad_norm": 7.62235689163208,
      "learning_rate": 4.682988312581163e-05,
      "loss": 1.9697,
      "step": 33200
    },
    {
      "epoch": 2.5437323351921166,
      "grad_norm": 7.807041645050049,
      "learning_rate": 4.682033458100986e-05,
      "loss": 2.0385,
      "step": 33300
    },
    {
      "epoch": 2.5513711710335345,
      "grad_norm": 6.06990909576416,
      "learning_rate": 4.681078603620808e-05,
      "loss": 2.0684,
      "step": 33400
    },
    {
      "epoch": 2.5590100068749524,
      "grad_norm": 7.017930507659912,
      "learning_rate": 4.6801237491406316e-05,
      "loss": 2.0018,
      "step": 33500
    },
    {
      "epoch": 2.5666488427163703,
      "grad_norm": 6.334123611450195,
      "learning_rate": 4.679168894660454e-05,
      "loss": 2.1273,
      "step": 33600
    },
    {
      "epoch": 2.5742876785577877,
      "grad_norm": 5.8035101890563965,
      "learning_rate": 4.678214040180277e-05,
      "loss": 2.0262,
      "step": 33700
    },
    {
      "epoch": 2.5819265143992056,
      "grad_norm": 8.67058277130127,
      "learning_rate": 4.6772591857000996e-05,
      "loss": 2.05,
      "step": 33800
    },
    {
      "epoch": 2.5895653502406235,
      "grad_norm": 6.027514457702637,
      "learning_rate": 4.6763043312199224e-05,
      "loss": 2.0558,
      "step": 33900
    },
    {
      "epoch": 2.597204186082041,
      "grad_norm": 8.157918930053711,
      "learning_rate": 4.6753494767397446e-05,
      "loss": 2.0364,
      "step": 34000
    },
    {
      "epoch": 2.604843021923459,
      "grad_norm": 6.71254301071167,
      "learning_rate": 4.674394622259568e-05,
      "loss": 2.071,
      "step": 34100
    },
    {
      "epoch": 2.6124818577648767,
      "grad_norm": 6.104252338409424,
      "learning_rate": 4.6734397677793904e-05,
      "loss": 1.9971,
      "step": 34200
    },
    {
      "epoch": 2.620120693606294,
      "grad_norm": 6.495115756988525,
      "learning_rate": 4.672484913299213e-05,
      "loss": 2.017,
      "step": 34300
    },
    {
      "epoch": 2.627759529447712,
      "grad_norm": 9.145675659179688,
      "learning_rate": 4.671530058819036e-05,
      "loss": 1.9045,
      "step": 34400
    },
    {
      "epoch": 2.63539836528913,
      "grad_norm": 5.56348180770874,
      "learning_rate": 4.670575204338859e-05,
      "loss": 1.975,
      "step": 34500
    },
    {
      "epoch": 2.643037201130548,
      "grad_norm": 8.201665878295898,
      "learning_rate": 4.669620349858682e-05,
      "loss": 2.0265,
      "step": 34600
    },
    {
      "epoch": 2.6506760369719657,
      "grad_norm": 7.426966667175293,
      "learning_rate": 4.668665495378505e-05,
      "loss": 1.9824,
      "step": 34700
    },
    {
      "epoch": 2.658314872813383,
      "grad_norm": 6.087449550628662,
      "learning_rate": 4.6677106408983276e-05,
      "loss": 2.0742,
      "step": 34800
    },
    {
      "epoch": 2.665953708654801,
      "grad_norm": 8.503381729125977,
      "learning_rate": 4.66675578641815e-05,
      "loss": 2.1246,
      "step": 34900
    },
    {
      "epoch": 2.673592544496219,
      "grad_norm": 6.27324104309082,
      "learning_rate": 4.665800931937973e-05,
      "loss": 2.1401,
      "step": 35000
    },
    {
      "epoch": 2.6812313803376364,
      "grad_norm": 6.7396087646484375,
      "learning_rate": 4.6648460774577955e-05,
      "loss": 2.1122,
      "step": 35100
    },
    {
      "epoch": 2.6888702161790543,
      "grad_norm": 5.610233306884766,
      "learning_rate": 4.6638912229776184e-05,
      "loss": 2.043,
      "step": 35200
    },
    {
      "epoch": 2.696509052020472,
      "grad_norm": 7.961142539978027,
      "learning_rate": 4.662936368497441e-05,
      "loss": 2.0619,
      "step": 35300
    },
    {
      "epoch": 2.7041478878618896,
      "grad_norm": 7.008105278015137,
      "learning_rate": 4.661981514017264e-05,
      "loss": 1.9604,
      "step": 35400
    },
    {
      "epoch": 2.7117867237033075,
      "grad_norm": 5.190394401550293,
      "learning_rate": 4.661026659537086e-05,
      "loss": 2.1064,
      "step": 35500
    },
    {
      "epoch": 2.7194255595447254,
      "grad_norm": 6.54360294342041,
      "learning_rate": 4.66007180505691e-05,
      "loss": 2.1196,
      "step": 35600
    },
    {
      "epoch": 2.7270643953861433,
      "grad_norm": 6.34268856048584,
      "learning_rate": 4.659116950576732e-05,
      "loss": 2.027,
      "step": 35700
    },
    {
      "epoch": 2.7347032312275608,
      "grad_norm": 5.506364822387695,
      "learning_rate": 4.658162096096555e-05,
      "loss": 2.0029,
      "step": 35800
    },
    {
      "epoch": 2.7423420670689787,
      "grad_norm": 6.506877899169922,
      "learning_rate": 4.657207241616378e-05,
      "loss": 2.0305,
      "step": 35900
    },
    {
      "epoch": 2.7499809029103965,
      "grad_norm": 3.9788358211517334,
      "learning_rate": 4.656252387136201e-05,
      "loss": 1.9248,
      "step": 36000
    },
    {
      "epoch": 2.7576197387518144,
      "grad_norm": 7.015749931335449,
      "learning_rate": 4.6552975326560235e-05,
      "loss": 1.9649,
      "step": 36100
    },
    {
      "epoch": 2.765258574593232,
      "grad_norm": 6.213406562805176,
      "learning_rate": 4.6543426781758464e-05,
      "loss": 1.9604,
      "step": 36200
    },
    {
      "epoch": 2.7728974104346498,
      "grad_norm": 7.5010199546813965,
      "learning_rate": 4.653387823695669e-05,
      "loss": 2.01,
      "step": 36300
    },
    {
      "epoch": 2.7805362462760677,
      "grad_norm": 10.815936088562012,
      "learning_rate": 4.6524329692154915e-05,
      "loss": 2.0721,
      "step": 36400
    },
    {
      "epoch": 2.788175082117485,
      "grad_norm": 8.307320594787598,
      "learning_rate": 4.651478114735315e-05,
      "loss": 2.0448,
      "step": 36500
    },
    {
      "epoch": 2.795813917958903,
      "grad_norm": 6.190130233764648,
      "learning_rate": 4.650523260255137e-05,
      "loss": 2.0245,
      "step": 36600
    },
    {
      "epoch": 2.803452753800321,
      "grad_norm": 6.052091121673584,
      "learning_rate": 4.64956840577496e-05,
      "loss": 2.0836,
      "step": 36700
    },
    {
      "epoch": 2.8110915896417383,
      "grad_norm": 7.273357391357422,
      "learning_rate": 4.648613551294783e-05,
      "loss": 2.0031,
      "step": 36800
    },
    {
      "epoch": 2.818730425483156,
      "grad_norm": 7.748610019683838,
      "learning_rate": 4.647658696814606e-05,
      "loss": 1.9415,
      "step": 36900
    },
    {
      "epoch": 2.826369261324574,
      "grad_norm": 7.363180637359619,
      "learning_rate": 4.646703842334429e-05,
      "loss": 2.042,
      "step": 37000
    },
    {
      "epoch": 2.834008097165992,
      "grad_norm": 6.2733564376831055,
      "learning_rate": 4.6457489878542516e-05,
      "loss": 1.9737,
      "step": 37100
    },
    {
      "epoch": 2.84164693300741,
      "grad_norm": 8.402475357055664,
      "learning_rate": 4.6447941333740744e-05,
      "loss": 2.0863,
      "step": 37200
    },
    {
      "epoch": 2.8492857688488273,
      "grad_norm": 7.812938690185547,
      "learning_rate": 4.6438392788938966e-05,
      "loss": 2.0119,
      "step": 37300
    },
    {
      "epoch": 2.8569246046902452,
      "grad_norm": 7.788450717926025,
      "learning_rate": 4.64288442441372e-05,
      "loss": 2.1179,
      "step": 37400
    },
    {
      "epoch": 2.864563440531663,
      "grad_norm": 6.900868892669678,
      "learning_rate": 4.6419295699335424e-05,
      "loss": 1.9481,
      "step": 37500
    },
    {
      "epoch": 2.8722022763730806,
      "grad_norm": 5.8211259841918945,
      "learning_rate": 4.640974715453365e-05,
      "loss": 2.1289,
      "step": 37600
    },
    {
      "epoch": 2.8798411122144985,
      "grad_norm": 5.377330780029297,
      "learning_rate": 4.6400198609731874e-05,
      "loss": 2.0466,
      "step": 37700
    },
    {
      "epoch": 2.8874799480559163,
      "grad_norm": 6.862639427185059,
      "learning_rate": 4.639065006493011e-05,
      "loss": 2.0121,
      "step": 37800
    },
    {
      "epoch": 2.895118783897334,
      "grad_norm": 5.8920698165893555,
      "learning_rate": 4.638110152012833e-05,
      "loss": 1.9999,
      "step": 37900
    },
    {
      "epoch": 2.9027576197387517,
      "grad_norm": 7.047788143157959,
      "learning_rate": 4.637155297532656e-05,
      "loss": 1.9514,
      "step": 38000
    },
    {
      "epoch": 2.9103964555801696,
      "grad_norm": 6.60552453994751,
      "learning_rate": 4.636200443052479e-05,
      "loss": 2.0771,
      "step": 38100
    },
    {
      "epoch": 2.9180352914215875,
      "grad_norm": 6.756577491760254,
      "learning_rate": 4.635245588572302e-05,
      "loss": 2.1688,
      "step": 38200
    },
    {
      "epoch": 2.9256741272630054,
      "grad_norm": 6.406771659851074,
      "learning_rate": 4.6342907340921246e-05,
      "loss": 2.0141,
      "step": 38300
    },
    {
      "epoch": 2.933312963104423,
      "grad_norm": 6.147496700286865,
      "learning_rate": 4.6333358796119475e-05,
      "loss": 2.037,
      "step": 38400
    },
    {
      "epoch": 2.9409517989458407,
      "grad_norm": 6.549181938171387,
      "learning_rate": 4.6323810251317704e-05,
      "loss": 2.103,
      "step": 38500
    },
    {
      "epoch": 2.9485906347872586,
      "grad_norm": 8.879324913024902,
      "learning_rate": 4.6314261706515926e-05,
      "loss": 2.0206,
      "step": 38600
    },
    {
      "epoch": 2.956229470628676,
      "grad_norm": 6.552047252655029,
      "learning_rate": 4.630471316171416e-05,
      "loss": 1.9887,
      "step": 38700
    },
    {
      "epoch": 2.963868306470094,
      "grad_norm": 6.169914722442627,
      "learning_rate": 4.629516461691238e-05,
      "loss": 1.9505,
      "step": 38800
    },
    {
      "epoch": 2.971507142311512,
      "grad_norm": 6.05596399307251,
      "learning_rate": 4.628561607211061e-05,
      "loss": 2.1698,
      "step": 38900
    },
    {
      "epoch": 2.9791459781529293,
      "grad_norm": 6.614627361297607,
      "learning_rate": 4.627606752730884e-05,
      "loss": 2.1571,
      "step": 39000
    },
    {
      "epoch": 2.986784813994347,
      "grad_norm": 8.500330924987793,
      "learning_rate": 4.626651898250707e-05,
      "loss": 1.9714,
      "step": 39100
    },
    {
      "epoch": 2.994423649835765,
      "grad_norm": 6.359987735748291,
      "learning_rate": 4.625697043770529e-05,
      "loss": 2.0664,
      "step": 39200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.956020712852478,
      "eval_runtime": 3.0419,
      "eval_samples_per_second": 226.832,
      "eval_steps_per_second": 226.832,
      "step": 39273
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.8215460777282715,
      "eval_runtime": 57.9188,
      "eval_samples_per_second": 226.023,
      "eval_steps_per_second": 226.023,
      "step": 39273
    },
    {
      "epoch": 3.002062485677183,
      "grad_norm": 8.853819847106934,
      "learning_rate": 4.6247421892903526e-05,
      "loss": 2.0702,
      "step": 39300
    },
    {
      "epoch": 3.0097013215186004,
      "grad_norm": 6.595559120178223,
      "learning_rate": 4.623787334810175e-05,
      "loss": 1.905,
      "step": 39400
    },
    {
      "epoch": 3.0173401573600183,
      "grad_norm": 10.330395698547363,
      "learning_rate": 4.622832480329998e-05,
      "loss": 1.9888,
      "step": 39500
    },
    {
      "epoch": 3.024978993201436,
      "grad_norm": 6.6225762367248535,
      "learning_rate": 4.6218776258498206e-05,
      "loss": 1.9949,
      "step": 39600
    },
    {
      "epoch": 3.032617829042854,
      "grad_norm": 14.128105163574219,
      "learning_rate": 4.6209227713696434e-05,
      "loss": 2.0285,
      "step": 39700
    },
    {
      "epoch": 3.0402566648842715,
      "grad_norm": 6.09885311126709,
      "learning_rate": 4.619967916889466e-05,
      "loss": 1.9186,
      "step": 39800
    },
    {
      "epoch": 3.0478955007256894,
      "grad_norm": 5.306074619293213,
      "learning_rate": 4.619013062409289e-05,
      "loss": 1.9025,
      "step": 39900
    },
    {
      "epoch": 3.0555343365671073,
      "grad_norm": 8.875261306762695,
      "learning_rate": 4.618058207929112e-05,
      "loss": 1.9754,
      "step": 40000
    },
    {
      "epoch": 3.0631731724085247,
      "grad_norm": 8.786972045898438,
      "learning_rate": 4.617103353448934e-05,
      "loss": 2.0414,
      "step": 40100
    },
    {
      "epoch": 3.0708120082499426,
      "grad_norm": 5.99552059173584,
      "learning_rate": 4.616148498968758e-05,
      "loss": 2.1144,
      "step": 40200
    },
    {
      "epoch": 3.0784508440913605,
      "grad_norm": 6.164271354675293,
      "learning_rate": 4.61519364448858e-05,
      "loss": 1.9699,
      "step": 40300
    },
    {
      "epoch": 3.0860896799327784,
      "grad_norm": 10.463454246520996,
      "learning_rate": 4.614238790008403e-05,
      "loss": 2.1251,
      "step": 40400
    },
    {
      "epoch": 3.093728515774196,
      "grad_norm": 6.676143646240234,
      "learning_rate": 4.613283935528226e-05,
      "loss": 1.9814,
      "step": 40500
    },
    {
      "epoch": 3.1013673516156137,
      "grad_norm": 6.554495334625244,
      "learning_rate": 4.6123290810480486e-05,
      "loss": 2.048,
      "step": 40600
    },
    {
      "epoch": 3.1090061874570316,
      "grad_norm": 8.24526596069336,
      "learning_rate": 4.611374226567871e-05,
      "loss": 2.0579,
      "step": 40700
    },
    {
      "epoch": 3.1166450232984495,
      "grad_norm": 6.204075813293457,
      "learning_rate": 4.610419372087694e-05,
      "loss": 1.9798,
      "step": 40800
    },
    {
      "epoch": 3.124283859139867,
      "grad_norm": 10.288970947265625,
      "learning_rate": 4.6094645176075165e-05,
      "loss": 2.0361,
      "step": 40900
    },
    {
      "epoch": 3.131922694981285,
      "grad_norm": 4.832223892211914,
      "learning_rate": 4.6085096631273394e-05,
      "loss": 1.9967,
      "step": 41000
    },
    {
      "epoch": 3.1395615308227027,
      "grad_norm": 9.446552276611328,
      "learning_rate": 4.607554808647163e-05,
      "loss": 1.9676,
      "step": 41100
    },
    {
      "epoch": 3.14720036666412,
      "grad_norm": 6.816980361938477,
      "learning_rate": 4.606599954166985e-05,
      "loss": 2.0608,
      "step": 41200
    },
    {
      "epoch": 3.154839202505538,
      "grad_norm": 7.757654190063477,
      "learning_rate": 4.605645099686808e-05,
      "loss": 2.0318,
      "step": 41300
    },
    {
      "epoch": 3.162478038346956,
      "grad_norm": 7.175195693969727,
      "learning_rate": 4.604690245206631e-05,
      "loss": 2.0196,
      "step": 41400
    },
    {
      "epoch": 3.170116874188374,
      "grad_norm": 5.186682224273682,
      "learning_rate": 4.603735390726454e-05,
      "loss": 1.9421,
      "step": 41500
    },
    {
      "epoch": 3.1777557100297913,
      "grad_norm": 7.818681716918945,
      "learning_rate": 4.602780536246276e-05,
      "loss": 1.9723,
      "step": 41600
    },
    {
      "epoch": 3.185394545871209,
      "grad_norm": 6.980311870574951,
      "learning_rate": 4.6018256817660995e-05,
      "loss": 1.9165,
      "step": 41700
    },
    {
      "epoch": 3.193033381712627,
      "grad_norm": 7.583540439605713,
      "learning_rate": 4.600870827285922e-05,
      "loss": 1.9788,
      "step": 41800
    },
    {
      "epoch": 3.200672217554045,
      "grad_norm": 7.46795129776001,
      "learning_rate": 4.5999159728057445e-05,
      "loss": 1.9436,
      "step": 41900
    },
    {
      "epoch": 3.2083110533954624,
      "grad_norm": 7.82221794128418,
      "learning_rate": 4.5989611183255674e-05,
      "loss": 2.0255,
      "step": 42000
    },
    {
      "epoch": 3.2159498892368803,
      "grad_norm": 9.661761283874512,
      "learning_rate": 4.59800626384539e-05,
      "loss": 1.8896,
      "step": 42100
    },
    {
      "epoch": 3.223588725078298,
      "grad_norm": 6.426016330718994,
      "learning_rate": 4.597051409365213e-05,
      "loss": 2.0628,
      "step": 42200
    },
    {
      "epoch": 3.2312275609197156,
      "grad_norm": 7.0409626960754395,
      "learning_rate": 4.596096554885036e-05,
      "loss": 2.0107,
      "step": 42300
    },
    {
      "epoch": 3.2388663967611335,
      "grad_norm": 5.736508369445801,
      "learning_rate": 4.595141700404859e-05,
      "loss": 2.0262,
      "step": 42400
    },
    {
      "epoch": 3.2465052326025514,
      "grad_norm": 6.778249740600586,
      "learning_rate": 4.594186845924681e-05,
      "loss": 2.0228,
      "step": 42500
    },
    {
      "epoch": 3.2541440684439693,
      "grad_norm": 8.188097953796387,
      "learning_rate": 4.5932319914445046e-05,
      "loss": 1.9206,
      "step": 42600
    },
    {
      "epoch": 3.2617829042853868,
      "grad_norm": 6.598724842071533,
      "learning_rate": 4.592277136964327e-05,
      "loss": 2.0492,
      "step": 42700
    },
    {
      "epoch": 3.2694217401268046,
      "grad_norm": 8.242944717407227,
      "learning_rate": 4.59132228248415e-05,
      "loss": 1.982,
      "step": 42800
    },
    {
      "epoch": 3.2770605759682225,
      "grad_norm": 8.155143737792969,
      "learning_rate": 4.5903674280039726e-05,
      "loss": 1.9606,
      "step": 42900
    },
    {
      "epoch": 3.2846994118096404,
      "grad_norm": 8.229649543762207,
      "learning_rate": 4.5894125735237954e-05,
      "loss": 2.0188,
      "step": 43000
    },
    {
      "epoch": 3.292338247651058,
      "grad_norm": 7.207655906677246,
      "learning_rate": 4.5884577190436176e-05,
      "loss": 1.9789,
      "step": 43100
    },
    {
      "epoch": 3.2999770834924758,
      "grad_norm": 8.212202072143555,
      "learning_rate": 4.587502864563441e-05,
      "loss": 2.0081,
      "step": 43200
    },
    {
      "epoch": 3.3076159193338937,
      "grad_norm": 6.047013282775879,
      "learning_rate": 4.5865480100832634e-05,
      "loss": 2.0135,
      "step": 43300
    },
    {
      "epoch": 3.315254755175311,
      "grad_norm": 5.854637622833252,
      "learning_rate": 4.585593155603086e-05,
      "loss": 2.0698,
      "step": 43400
    },
    {
      "epoch": 3.322893591016729,
      "grad_norm": 5.609196186065674,
      "learning_rate": 4.584638301122909e-05,
      "loss": 1.949,
      "step": 43500
    },
    {
      "epoch": 3.330532426858147,
      "grad_norm": 8.607077598571777,
      "learning_rate": 4.583683446642732e-05,
      "loss": 2.0034,
      "step": 43600
    },
    {
      "epoch": 3.3381712626995648,
      "grad_norm": 7.13531494140625,
      "learning_rate": 4.582728592162555e-05,
      "loss": 1.9281,
      "step": 43700
    },
    {
      "epoch": 3.345810098540982,
      "grad_norm": 5.353226661682129,
      "learning_rate": 4.581773737682377e-05,
      "loss": 2.0378,
      "step": 43800
    },
    {
      "epoch": 3.3534489343824,
      "grad_norm": 6.871962070465088,
      "learning_rate": 4.5808188832022006e-05,
      "loss": 2.0381,
      "step": 43900
    },
    {
      "epoch": 3.361087770223818,
      "grad_norm": 7.070187091827393,
      "learning_rate": 4.579864028722023e-05,
      "loss": 2.0414,
      "step": 44000
    },
    {
      "epoch": 3.368726606065236,
      "grad_norm": 5.815462589263916,
      "learning_rate": 4.5789091742418456e-05,
      "loss": 1.846,
      "step": 44100
    },
    {
      "epoch": 3.3763654419066533,
      "grad_norm": 6.808154582977295,
      "learning_rate": 4.5779543197616685e-05,
      "loss": 1.9798,
      "step": 44200
    },
    {
      "epoch": 3.384004277748071,
      "grad_norm": 7.205514430999756,
      "learning_rate": 4.5769994652814914e-05,
      "loss": 2.0217,
      "step": 44300
    },
    {
      "epoch": 3.391643113589489,
      "grad_norm": 6.275455474853516,
      "learning_rate": 4.5760446108013136e-05,
      "loss": 2.0749,
      "step": 44400
    },
    {
      "epoch": 3.3992819494309066,
      "grad_norm": 7.210625648498535,
      "learning_rate": 4.575089756321137e-05,
      "loss": 2.0981,
      "step": 44500
    },
    {
      "epoch": 3.4069207852723244,
      "grad_norm": 7.109739780426025,
      "learning_rate": 4.574134901840959e-05,
      "loss": 2.0452,
      "step": 44600
    },
    {
      "epoch": 3.4145596211137423,
      "grad_norm": 6.4921979904174805,
      "learning_rate": 4.573180047360782e-05,
      "loss": 2.0038,
      "step": 44700
    },
    {
      "epoch": 3.42219845695516,
      "grad_norm": 6.5360822677612305,
      "learning_rate": 4.572225192880605e-05,
      "loss": 2.0164,
      "step": 44800
    },
    {
      "epoch": 3.4298372927965777,
      "grad_norm": 6.190916538238525,
      "learning_rate": 4.571270338400428e-05,
      "loss": 2.0037,
      "step": 44900
    },
    {
      "epoch": 3.4374761286379956,
      "grad_norm": 7.206357002258301,
      "learning_rate": 4.570315483920251e-05,
      "loss": 2.0718,
      "step": 45000
    },
    {
      "epoch": 3.4451149644794135,
      "grad_norm": 8.121214866638184,
      "learning_rate": 4.5693606294400736e-05,
      "loss": 2.0669,
      "step": 45100
    },
    {
      "epoch": 3.4527538003208313,
      "grad_norm": 6.4986724853515625,
      "learning_rate": 4.5684057749598965e-05,
      "loss": 2.0822,
      "step": 45200
    },
    {
      "epoch": 3.460392636162249,
      "grad_norm": 5.510618686676025,
      "learning_rate": 4.567450920479719e-05,
      "loss": 1.9723,
      "step": 45300
    },
    {
      "epoch": 3.4680314720036667,
      "grad_norm": 7.590890407562256,
      "learning_rate": 4.566496065999542e-05,
      "loss": 2.0038,
      "step": 45400
    },
    {
      "epoch": 3.4756703078450846,
      "grad_norm": 6.417936325073242,
      "learning_rate": 4.5655412115193644e-05,
      "loss": 2.0518,
      "step": 45500
    },
    {
      "epoch": 3.483309143686502,
      "grad_norm": 5.924548149108887,
      "learning_rate": 4.564586357039187e-05,
      "loss": 2.0806,
      "step": 45600
    },
    {
      "epoch": 3.49094797952792,
      "grad_norm": 6.933248043060303,
      "learning_rate": 4.56363150255901e-05,
      "loss": 2.0207,
      "step": 45700
    },
    {
      "epoch": 3.498586815369338,
      "grad_norm": 9.148240089416504,
      "learning_rate": 4.562676648078833e-05,
      "loss": 1.9876,
      "step": 45800
    },
    {
      "epoch": 3.5062256512107552,
      "grad_norm": 8.503438949584961,
      "learning_rate": 4.561721793598656e-05,
      "loss": 1.8333,
      "step": 45900
    },
    {
      "epoch": 3.513864487052173,
      "grad_norm": 7.609984874725342,
      "learning_rate": 4.560766939118479e-05,
      "loss": 1.9453,
      "step": 46000
    },
    {
      "epoch": 3.521503322893591,
      "grad_norm": 8.995497703552246,
      "learning_rate": 4.5598120846383017e-05,
      "loss": 1.9898,
      "step": 46100
    },
    {
      "epoch": 3.529142158735009,
      "grad_norm": 8.925196647644043,
      "learning_rate": 4.558857230158124e-05,
      "loss": 2.0588,
      "step": 46200
    },
    {
      "epoch": 3.536780994576427,
      "grad_norm": 7.179499626159668,
      "learning_rate": 4.5579023756779474e-05,
      "loss": 2.0441,
      "step": 46300
    },
    {
      "epoch": 3.5444198304178443,
      "grad_norm": 5.773114204406738,
      "learning_rate": 4.5569475211977696e-05,
      "loss": 2.0275,
      "step": 46400
    },
    {
      "epoch": 3.552058666259262,
      "grad_norm": 5.931734561920166,
      "learning_rate": 4.5559926667175925e-05,
      "loss": 1.8768,
      "step": 46500
    },
    {
      "epoch": 3.55969750210068,
      "grad_norm": 5.921706676483154,
      "learning_rate": 4.555037812237415e-05,
      "loss": 1.9509,
      "step": 46600
    },
    {
      "epoch": 3.5673363379420975,
      "grad_norm": 7.752471923828125,
      "learning_rate": 4.554082957757238e-05,
      "loss": 1.9046,
      "step": 46700
    },
    {
      "epoch": 3.5749751737835154,
      "grad_norm": 9.368300437927246,
      "learning_rate": 4.5531281032770604e-05,
      "loss": 1.9905,
      "step": 46800
    },
    {
      "epoch": 3.5826140096249333,
      "grad_norm": 6.952995300292969,
      "learning_rate": 4.552173248796884e-05,
      "loss": 2.0278,
      "step": 46900
    },
    {
      "epoch": 3.5902528454663507,
      "grad_norm": 6.365713119506836,
      "learning_rate": 4.551218394316706e-05,
      "loss": 2.0738,
      "step": 47000
    },
    {
      "epoch": 3.5978916813077686,
      "grad_norm": 6.764621734619141,
      "learning_rate": 4.550263539836529e-05,
      "loss": 1.9885,
      "step": 47100
    },
    {
      "epoch": 3.6055305171491865,
      "grad_norm": 7.812902450561523,
      "learning_rate": 4.549308685356352e-05,
      "loss": 1.9548,
      "step": 47200
    },
    {
      "epoch": 3.6131693529906044,
      "grad_norm": 7.405207633972168,
      "learning_rate": 4.548353830876175e-05,
      "loss": 2.0384,
      "step": 47300
    },
    {
      "epoch": 3.6208081888320223,
      "grad_norm": 8.965970039367676,
      "learning_rate": 4.5473989763959976e-05,
      "loss": 1.9923,
      "step": 47400
    },
    {
      "epoch": 3.6284470246734397,
      "grad_norm": 6.6861443519592285,
      "learning_rate": 4.5464441219158205e-05,
      "loss": 1.9756,
      "step": 47500
    },
    {
      "epoch": 3.6360858605148576,
      "grad_norm": 5.839308261871338,
      "learning_rate": 4.5454892674356433e-05,
      "loss": 2.0055,
      "step": 47600
    },
    {
      "epoch": 3.6437246963562755,
      "grad_norm": 9.701184272766113,
      "learning_rate": 4.5445344129554655e-05,
      "loss": 1.9728,
      "step": 47700
    },
    {
      "epoch": 3.651363532197693,
      "grad_norm": 6.794645309448242,
      "learning_rate": 4.543579558475289e-05,
      "loss": 2.0321,
      "step": 47800
    },
    {
      "epoch": 3.659002368039111,
      "grad_norm": 8.940417289733887,
      "learning_rate": 4.542624703995111e-05,
      "loss": 2.0372,
      "step": 47900
    },
    {
      "epoch": 3.6666412038805287,
      "grad_norm": 6.787924766540527,
      "learning_rate": 4.541669849514934e-05,
      "loss": 2.0457,
      "step": 48000
    },
    {
      "epoch": 3.674280039721946,
      "grad_norm": 8.246941566467285,
      "learning_rate": 4.540714995034757e-05,
      "loss": 2.0214,
      "step": 48100
    },
    {
      "epoch": 3.681918875563364,
      "grad_norm": 6.680868625640869,
      "learning_rate": 4.53976014055458e-05,
      "loss": 2.1036,
      "step": 48200
    },
    {
      "epoch": 3.689557711404782,
      "grad_norm": 6.381515979766846,
      "learning_rate": 4.538805286074402e-05,
      "loss": 1.9878,
      "step": 48300
    },
    {
      "epoch": 3.6971965472462,
      "grad_norm": 7.496274948120117,
      "learning_rate": 4.5378504315942256e-05,
      "loss": 2.0583,
      "step": 48400
    },
    {
      "epoch": 3.7048353830876173,
      "grad_norm": 6.436630725860596,
      "learning_rate": 4.536895577114048e-05,
      "loss": 2.0585,
      "step": 48500
    },
    {
      "epoch": 3.712474218929035,
      "grad_norm": 6.811107158660889,
      "learning_rate": 4.535940722633871e-05,
      "loss": 1.9973,
      "step": 48600
    },
    {
      "epoch": 3.720113054770453,
      "grad_norm": 5.0561628341674805,
      "learning_rate": 4.5349858681536936e-05,
      "loss": 2.0431,
      "step": 48700
    },
    {
      "epoch": 3.727751890611871,
      "grad_norm": 6.530940055847168,
      "learning_rate": 4.5340310136735164e-05,
      "loss": 1.9765,
      "step": 48800
    },
    {
      "epoch": 3.7353907264532884,
      "grad_norm": 6.262913703918457,
      "learning_rate": 4.533076159193339e-05,
      "loss": 1.9986,
      "step": 48900
    },
    {
      "epoch": 3.7430295622947063,
      "grad_norm": 5.263329029083252,
      "learning_rate": 4.532121304713162e-05,
      "loss": 2.0563,
      "step": 49000
    },
    {
      "epoch": 3.750668398136124,
      "grad_norm": 6.313994884490967,
      "learning_rate": 4.531166450232985e-05,
      "loss": 1.9602,
      "step": 49100
    },
    {
      "epoch": 3.7583072339775416,
      "grad_norm": 6.686409950256348,
      "learning_rate": 4.530211595752807e-05,
      "loss": 1.9809,
      "step": 49200
    },
    {
      "epoch": 3.7659460698189595,
      "grad_norm": 7.2685675621032715,
      "learning_rate": 4.52925674127263e-05,
      "loss": 2.0372,
      "step": 49300
    },
    {
      "epoch": 3.7735849056603774,
      "grad_norm": 6.320578575134277,
      "learning_rate": 4.528301886792453e-05,
      "loss": 1.9472,
      "step": 49400
    },
    {
      "epoch": 3.781223741501795,
      "grad_norm": 8.661364555358887,
      "learning_rate": 4.527347032312276e-05,
      "loss": 1.9197,
      "step": 49500
    },
    {
      "epoch": 3.7888625773432127,
      "grad_norm": 8.701038360595703,
      "learning_rate": 4.526392177832098e-05,
      "loss": 2.0922,
      "step": 49600
    },
    {
      "epoch": 3.7965014131846306,
      "grad_norm": 6.471027374267578,
      "learning_rate": 4.5254373233519216e-05,
      "loss": 2.0278,
      "step": 49700
    },
    {
      "epoch": 3.8041402490260485,
      "grad_norm": 8.41858959197998,
      "learning_rate": 4.524482468871744e-05,
      "loss": 1.9727,
      "step": 49800
    },
    {
      "epoch": 3.8117790848674664,
      "grad_norm": 6.340753078460693,
      "learning_rate": 4.5235276143915666e-05,
      "loss": 1.9682,
      "step": 49900
    },
    {
      "epoch": 3.819417920708884,
      "grad_norm": 6.557938575744629,
      "learning_rate": 4.52257275991139e-05,
      "loss": 1.9616,
      "step": 50000
    },
    {
      "epoch": 3.8270567565503018,
      "grad_norm": 5.470424652099609,
      "learning_rate": 4.5216179054312124e-05,
      "loss": 1.9374,
      "step": 50100
    },
    {
      "epoch": 3.8346955923917196,
      "grad_norm": 8.79299545288086,
      "learning_rate": 4.520663050951035e-05,
      "loss": 1.9392,
      "step": 50200
    },
    {
      "epoch": 3.842334428233137,
      "grad_norm": 5.646761417388916,
      "learning_rate": 4.519708196470858e-05,
      "loss": 1.9746,
      "step": 50300
    },
    {
      "epoch": 3.849973264074555,
      "grad_norm": 3.9076032638549805,
      "learning_rate": 4.518753341990681e-05,
      "loss": 1.9359,
      "step": 50400
    },
    {
      "epoch": 3.857612099915973,
      "grad_norm": 5.63897180557251,
      "learning_rate": 4.517798487510503e-05,
      "loss": 1.986,
      "step": 50500
    },
    {
      "epoch": 3.8652509357573903,
      "grad_norm": 5.407123565673828,
      "learning_rate": 4.516843633030327e-05,
      "loss": 1.9624,
      "step": 50600
    },
    {
      "epoch": 3.872889771598808,
      "grad_norm": 6.732994556427002,
      "learning_rate": 4.515888778550149e-05,
      "loss": 2.0008,
      "step": 50700
    },
    {
      "epoch": 3.880528607440226,
      "grad_norm": 7.200456619262695,
      "learning_rate": 4.514933924069972e-05,
      "loss": 2.0571,
      "step": 50800
    },
    {
      "epoch": 3.888167443281644,
      "grad_norm": 6.585333824157715,
      "learning_rate": 4.5139790695897946e-05,
      "loss": 2.0315,
      "step": 50900
    },
    {
      "epoch": 3.895806279123062,
      "grad_norm": 5.874913215637207,
      "learning_rate": 4.5130242151096175e-05,
      "loss": 2.0467,
      "step": 51000
    },
    {
      "epoch": 3.9034451149644793,
      "grad_norm": 8.3264741897583,
      "learning_rate": 4.5120693606294404e-05,
      "loss": 1.9699,
      "step": 51100
    },
    {
      "epoch": 3.911083950805897,
      "grad_norm": 6.979541778564453,
      "learning_rate": 4.511114506149263e-05,
      "loss": 1.9673,
      "step": 51200
    },
    {
      "epoch": 3.918722786647315,
      "grad_norm": 4.919912338256836,
      "learning_rate": 4.510159651669086e-05,
      "loss": 2.0659,
      "step": 51300
    },
    {
      "epoch": 3.9263616224887326,
      "grad_norm": 6.604058265686035,
      "learning_rate": 4.509204797188908e-05,
      "loss": 1.9504,
      "step": 51400
    },
    {
      "epoch": 3.9340004583301504,
      "grad_norm": 7.700245380401611,
      "learning_rate": 4.508249942708732e-05,
      "loss": 2.1178,
      "step": 51500
    },
    {
      "epoch": 3.9416392941715683,
      "grad_norm": 8.08391284942627,
      "learning_rate": 4.507295088228554e-05,
      "loss": 1.9923,
      "step": 51600
    },
    {
      "epoch": 3.9492781300129858,
      "grad_norm": 6.759174346923828,
      "learning_rate": 4.506340233748377e-05,
      "loss": 1.8691,
      "step": 51700
    },
    {
      "epoch": 3.9569169658544037,
      "grad_norm": 5.603715896606445,
      "learning_rate": 4.5053853792682e-05,
      "loss": 1.9299,
      "step": 51800
    },
    {
      "epoch": 3.9645558016958216,
      "grad_norm": 6.781336784362793,
      "learning_rate": 4.5044305247880227e-05,
      "loss": 2.0232,
      "step": 51900
    },
    {
      "epoch": 3.9721946375372394,
      "grad_norm": 6.775801181793213,
      "learning_rate": 4.503475670307845e-05,
      "loss": 2.0433,
      "step": 52000
    },
    {
      "epoch": 3.9798334733786573,
      "grad_norm": 8.976642608642578,
      "learning_rate": 4.5025208158276684e-05,
      "loss": 2.0242,
      "step": 52100
    },
    {
      "epoch": 3.987472309220075,
      "grad_norm": 5.605884075164795,
      "learning_rate": 4.5015659613474906e-05,
      "loss": 1.9461,
      "step": 52200
    },
    {
      "epoch": 3.9951111450614927,
      "grad_norm": 5.7515997886657715,
      "learning_rate": 4.5006111068673135e-05,
      "loss": 2.0327,
      "step": 52300
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9326626062393188,
      "eval_runtime": 3.0245,
      "eval_samples_per_second": 228.14,
      "eval_steps_per_second": 228.14,
      "step": 52364
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.7839194536209106,
      "eval_runtime": 57.129,
      "eval_samples_per_second": 229.148,
      "eval_steps_per_second": 229.148,
      "step": 52364
    },
    {
      "epoch": 4.002749980902911,
      "grad_norm": 8.508038520812988,
      "learning_rate": 4.499656252387136e-05,
      "loss": 1.9709,
      "step": 52400
    },
    {
      "epoch": 4.010388816744328,
      "grad_norm": 7.96737813949585,
      "learning_rate": 4.498701397906959e-05,
      "loss": 2.0317,
      "step": 52500
    },
    {
      "epoch": 4.018027652585746,
      "grad_norm": 7.937137603759766,
      "learning_rate": 4.497746543426782e-05,
      "loss": 1.9303,
      "step": 52600
    },
    {
      "epoch": 4.025666488427164,
      "grad_norm": 6.5024285316467285,
      "learning_rate": 4.496791688946605e-05,
      "loss": 1.9336,
      "step": 52700
    },
    {
      "epoch": 4.033305324268581,
      "grad_norm": 6.824344158172607,
      "learning_rate": 4.495836834466428e-05,
      "loss": 1.9143,
      "step": 52800
    },
    {
      "epoch": 4.04094416011,
      "grad_norm": 6.322193622589111,
      "learning_rate": 4.49488197998625e-05,
      "loss": 1.9843,
      "step": 52900
    },
    {
      "epoch": 4.048582995951417,
      "grad_norm": 7.656571388244629,
      "learning_rate": 4.4939271255060735e-05,
      "loss": 1.9486,
      "step": 53000
    },
    {
      "epoch": 4.0562218317928345,
      "grad_norm": 5.755255699157715,
      "learning_rate": 4.492972271025896e-05,
      "loss": 1.9418,
      "step": 53100
    },
    {
      "epoch": 4.063860667634253,
      "grad_norm": 8.343432426452637,
      "learning_rate": 4.4920174165457186e-05,
      "loss": 1.9642,
      "step": 53200
    },
    {
      "epoch": 4.07149950347567,
      "grad_norm": 6.103137493133545,
      "learning_rate": 4.4910625620655415e-05,
      "loss": 2.0084,
      "step": 53300
    },
    {
      "epoch": 4.079138339317088,
      "grad_norm": 8.019643783569336,
      "learning_rate": 4.4901077075853643e-05,
      "loss": 2.0208,
      "step": 53400
    },
    {
      "epoch": 4.086777175158506,
      "grad_norm": 8.313331604003906,
      "learning_rate": 4.4891528531051865e-05,
      "loss": 2.0144,
      "step": 53500
    },
    {
      "epoch": 4.0944160109999235,
      "grad_norm": 8.188743591308594,
      "learning_rate": 4.48819799862501e-05,
      "loss": 1.9031,
      "step": 53600
    },
    {
      "epoch": 4.102054846841342,
      "grad_norm": 7.807158946990967,
      "learning_rate": 4.487243144144832e-05,
      "loss": 2.0103,
      "step": 53700
    },
    {
      "epoch": 4.109693682682759,
      "grad_norm": 5.625879764556885,
      "learning_rate": 4.486288289664655e-05,
      "loss": 1.9777,
      "step": 53800
    },
    {
      "epoch": 4.117332518524177,
      "grad_norm": 7.48794412612915,
      "learning_rate": 4.485333435184478e-05,
      "loss": 2.0394,
      "step": 53900
    },
    {
      "epoch": 4.124971354365595,
      "grad_norm": 7.894604682922363,
      "learning_rate": 4.484378580704301e-05,
      "loss": 2.0643,
      "step": 54000
    },
    {
      "epoch": 4.1326101902070125,
      "grad_norm": 6.035438060760498,
      "learning_rate": 4.483423726224124e-05,
      "loss": 1.9581,
      "step": 54100
    },
    {
      "epoch": 4.14024902604843,
      "grad_norm": 9.646893501281738,
      "learning_rate": 4.4824688717439466e-05,
      "loss": 1.9861,
      "step": 54200
    },
    {
      "epoch": 4.147887861889848,
      "grad_norm": 6.093780040740967,
      "learning_rate": 4.4815140172637695e-05,
      "loss": 1.9112,
      "step": 54300
    },
    {
      "epoch": 4.155526697731266,
      "grad_norm": 8.302281379699707,
      "learning_rate": 4.480559162783592e-05,
      "loss": 1.9369,
      "step": 54400
    },
    {
      "epoch": 4.163165533572683,
      "grad_norm": 7.694951057434082,
      "learning_rate": 4.479604308303415e-05,
      "loss": 1.8962,
      "step": 54500
    },
    {
      "epoch": 4.1708043694141015,
      "grad_norm": 5.155496120452881,
      "learning_rate": 4.4786494538232374e-05,
      "loss": 1.9833,
      "step": 54600
    },
    {
      "epoch": 4.178443205255519,
      "grad_norm": 5.507457733154297,
      "learning_rate": 4.47769459934306e-05,
      "loss": 1.8555,
      "step": 54700
    },
    {
      "epoch": 4.186082041096936,
      "grad_norm": 6.579160690307617,
      "learning_rate": 4.476739744862883e-05,
      "loss": 1.953,
      "step": 54800
    },
    {
      "epoch": 4.193720876938355,
      "grad_norm": 6.491966724395752,
      "learning_rate": 4.475784890382706e-05,
      "loss": 1.9537,
      "step": 54900
    },
    {
      "epoch": 4.201359712779772,
      "grad_norm": 6.084700584411621,
      "learning_rate": 4.474830035902529e-05,
      "loss": 1.9398,
      "step": 55000
    },
    {
      "epoch": 4.2089985486211905,
      "grad_norm": 7.608360767364502,
      "learning_rate": 4.473875181422351e-05,
      "loss": 2.0359,
      "step": 55100
    },
    {
      "epoch": 4.216637384462608,
      "grad_norm": 7.041507244110107,
      "learning_rate": 4.4729203269421746e-05,
      "loss": 1.9514,
      "step": 55200
    },
    {
      "epoch": 4.224276220304025,
      "grad_norm": 8.634286880493164,
      "learning_rate": 4.471965472461997e-05,
      "loss": 1.9751,
      "step": 55300
    },
    {
      "epoch": 4.231915056145444,
      "grad_norm": 5.8987717628479,
      "learning_rate": 4.47101061798182e-05,
      "loss": 2.0173,
      "step": 55400
    },
    {
      "epoch": 4.239553891986861,
      "grad_norm": 5.461936950683594,
      "learning_rate": 4.4700557635016426e-05,
      "loss": 2.0241,
      "step": 55500
    },
    {
      "epoch": 4.247192727828279,
      "grad_norm": 7.499683856964111,
      "learning_rate": 4.4691009090214654e-05,
      "loss": 2.0103,
      "step": 55600
    },
    {
      "epoch": 4.254831563669697,
      "grad_norm": 7.233046054840088,
      "learning_rate": 4.4681460545412876e-05,
      "loss": 1.8797,
      "step": 55700
    },
    {
      "epoch": 4.262470399511114,
      "grad_norm": 6.561880588531494,
      "learning_rate": 4.467191200061111e-05,
      "loss": 1.9274,
      "step": 55800
    },
    {
      "epoch": 4.270109235352532,
      "grad_norm": 6.521469593048096,
      "learning_rate": 4.4662363455809334e-05,
      "loss": 2.0291,
      "step": 55900
    },
    {
      "epoch": 4.27774807119395,
      "grad_norm": 7.503207683563232,
      "learning_rate": 4.465281491100756e-05,
      "loss": 1.9189,
      "step": 56000
    },
    {
      "epoch": 4.285386907035368,
      "grad_norm": 9.410048484802246,
      "learning_rate": 4.464326636620579e-05,
      "loss": 2.0294,
      "step": 56100
    },
    {
      "epoch": 4.293025742876786,
      "grad_norm": 7.940431594848633,
      "learning_rate": 4.463371782140402e-05,
      "loss": 2.0973,
      "step": 56200
    },
    {
      "epoch": 4.300664578718203,
      "grad_norm": 4.950938701629639,
      "learning_rate": 4.462416927660225e-05,
      "loss": 2.0314,
      "step": 56300
    },
    {
      "epoch": 4.308303414559621,
      "grad_norm": 8.065248489379883,
      "learning_rate": 4.461462073180048e-05,
      "loss": 1.9599,
      "step": 56400
    },
    {
      "epoch": 4.315942250401039,
      "grad_norm": 7.789547920227051,
      "learning_rate": 4.4605072186998706e-05,
      "loss": 2.0135,
      "step": 56500
    },
    {
      "epoch": 4.323581086242457,
      "grad_norm": 7.07938814163208,
      "learning_rate": 4.459552364219693e-05,
      "loss": 1.8887,
      "step": 56600
    },
    {
      "epoch": 4.331219922083874,
      "grad_norm": 7.223280429840088,
      "learning_rate": 4.458597509739516e-05,
      "loss": 1.9328,
      "step": 56700
    },
    {
      "epoch": 4.338858757925292,
      "grad_norm": 5.282668590545654,
      "learning_rate": 4.4576426552593385e-05,
      "loss": 1.8914,
      "step": 56800
    },
    {
      "epoch": 4.34649759376671,
      "grad_norm": 7.545406818389893,
      "learning_rate": 4.4566878007791614e-05,
      "loss": 2.0297,
      "step": 56900
    },
    {
      "epoch": 4.354136429608127,
      "grad_norm": 7.173372268676758,
      "learning_rate": 4.455732946298984e-05,
      "loss": 2.035,
      "step": 57000
    },
    {
      "epoch": 4.361775265449546,
      "grad_norm": 6.260436058044434,
      "learning_rate": 4.454778091818807e-05,
      "loss": 2.0087,
      "step": 57100
    },
    {
      "epoch": 4.369414101290963,
      "grad_norm": 10.63586139678955,
      "learning_rate": 4.453823237338629e-05,
      "loss": 2.0108,
      "step": 57200
    },
    {
      "epoch": 4.377052937132381,
      "grad_norm": 6.055496692657471,
      "learning_rate": 4.452868382858453e-05,
      "loss": 1.9937,
      "step": 57300
    },
    {
      "epoch": 4.384691772973799,
      "grad_norm": 4.529940605163574,
      "learning_rate": 4.451913528378275e-05,
      "loss": 1.9762,
      "step": 57400
    },
    {
      "epoch": 4.392330608815216,
      "grad_norm": 5.336458683013916,
      "learning_rate": 4.450958673898098e-05,
      "loss": 1.9803,
      "step": 57500
    },
    {
      "epoch": 4.399969444656635,
      "grad_norm": 7.936870098114014,
      "learning_rate": 4.450003819417921e-05,
      "loss": 2.0048,
      "step": 57600
    },
    {
      "epoch": 4.407608280498052,
      "grad_norm": 6.1727800369262695,
      "learning_rate": 4.4490489649377437e-05,
      "loss": 1.9583,
      "step": 57700
    },
    {
      "epoch": 4.4152471163394695,
      "grad_norm": 6.8504791259765625,
      "learning_rate": 4.4480941104575665e-05,
      "loss": 1.9156,
      "step": 57800
    },
    {
      "epoch": 4.422885952180888,
      "grad_norm": 5.553173542022705,
      "learning_rate": 4.4471392559773894e-05,
      "loss": 2.1044,
      "step": 57900
    },
    {
      "epoch": 4.430524788022305,
      "grad_norm": 8.633173942565918,
      "learning_rate": 4.446184401497212e-05,
      "loss": 1.9696,
      "step": 58000
    },
    {
      "epoch": 4.438163623863723,
      "grad_norm": 9.197064399719238,
      "learning_rate": 4.4452295470170345e-05,
      "loss": 2.009,
      "step": 58100
    },
    {
      "epoch": 4.445802459705141,
      "grad_norm": 7.6563262939453125,
      "learning_rate": 4.444274692536858e-05,
      "loss": 1.9462,
      "step": 58200
    },
    {
      "epoch": 4.4534412955465585,
      "grad_norm": 7.038527011871338,
      "learning_rate": 4.44331983805668e-05,
      "loss": 2.0086,
      "step": 58300
    },
    {
      "epoch": 4.461080131387977,
      "grad_norm": 6.402144908905029,
      "learning_rate": 4.442364983576503e-05,
      "loss": 1.926,
      "step": 58400
    },
    {
      "epoch": 4.468718967229394,
      "grad_norm": 9.302424430847168,
      "learning_rate": 4.441410129096326e-05,
      "loss": 1.8922,
      "step": 58500
    },
    {
      "epoch": 4.476357803070812,
      "grad_norm": 7.2428998947143555,
      "learning_rate": 4.440455274616149e-05,
      "loss": 1.9853,
      "step": 58600
    },
    {
      "epoch": 4.48399663891223,
      "grad_norm": 6.064920425415039,
      "learning_rate": 4.439500420135972e-05,
      "loss": 1.793,
      "step": 58700
    },
    {
      "epoch": 4.4916354747536476,
      "grad_norm": 12.604310989379883,
      "learning_rate": 4.4385455656557945e-05,
      "loss": 1.9379,
      "step": 58800
    },
    {
      "epoch": 4.499274310595065,
      "grad_norm": 8.235838890075684,
      "learning_rate": 4.4375907111756174e-05,
      "loss": 1.9827,
      "step": 58900
    },
    {
      "epoch": 4.506913146436483,
      "grad_norm": 7.30864953994751,
      "learning_rate": 4.4366358566954396e-05,
      "loss": 2.014,
      "step": 59000
    },
    {
      "epoch": 4.514551982277901,
      "grad_norm": 5.918211936950684,
      "learning_rate": 4.435681002215263e-05,
      "loss": 1.9833,
      "step": 59100
    },
    {
      "epoch": 4.522190818119318,
      "grad_norm": 8.649335861206055,
      "learning_rate": 4.4347261477350853e-05,
      "loss": 1.9271,
      "step": 59200
    },
    {
      "epoch": 4.529829653960737,
      "grad_norm": 6.060311317443848,
      "learning_rate": 4.433771293254908e-05,
      "loss": 1.9613,
      "step": 59300
    },
    {
      "epoch": 4.537468489802154,
      "grad_norm": 6.388965129852295,
      "learning_rate": 4.432816438774731e-05,
      "loss": 1.9895,
      "step": 59400
    },
    {
      "epoch": 4.545107325643572,
      "grad_norm": 6.666771411895752,
      "learning_rate": 4.431861584294554e-05,
      "loss": 1.9075,
      "step": 59500
    },
    {
      "epoch": 4.55274616148499,
      "grad_norm": 7.97987174987793,
      "learning_rate": 4.430906729814376e-05,
      "loss": 1.9604,
      "step": 59600
    },
    {
      "epoch": 4.560384997326407,
      "grad_norm": 9.024439811706543,
      "learning_rate": 4.4299518753342e-05,
      "loss": 1.9002,
      "step": 59700
    },
    {
      "epoch": 4.568023833167826,
      "grad_norm": 7.852283477783203,
      "learning_rate": 4.428997020854022e-05,
      "loss": 2.0282,
      "step": 59800
    },
    {
      "epoch": 4.575662669009243,
      "grad_norm": 8.69587230682373,
      "learning_rate": 4.428042166373845e-05,
      "loss": 1.9062,
      "step": 59900
    },
    {
      "epoch": 4.5833015048506605,
      "grad_norm": 8.097671508789062,
      "learning_rate": 4.4270873118936676e-05,
      "loss": 2.0076,
      "step": 60000
    },
    {
      "epoch": 4.590940340692079,
      "grad_norm": 7.062027454376221,
      "learning_rate": 4.4261324574134905e-05,
      "loss": 2.0645,
      "step": 60100
    },
    {
      "epoch": 4.598579176533496,
      "grad_norm": 5.7104573249816895,
      "learning_rate": 4.4251776029333134e-05,
      "loss": 1.8537,
      "step": 60200
    },
    {
      "epoch": 4.606218012374914,
      "grad_norm": 7.345786094665527,
      "learning_rate": 4.424222748453136e-05,
      "loss": 1.8794,
      "step": 60300
    },
    {
      "epoch": 4.613856848216332,
      "grad_norm": 7.426344871520996,
      "learning_rate": 4.423267893972959e-05,
      "loss": 1.9451,
      "step": 60400
    },
    {
      "epoch": 4.6214956840577495,
      "grad_norm": 6.954622745513916,
      "learning_rate": 4.422313039492781e-05,
      "loss": 1.9907,
      "step": 60500
    },
    {
      "epoch": 4.629134519899168,
      "grad_norm": 5.775232315063477,
      "learning_rate": 4.421358185012604e-05,
      "loss": 1.9152,
      "step": 60600
    },
    {
      "epoch": 4.636773355740585,
      "grad_norm": 6.703962802886963,
      "learning_rate": 4.420403330532427e-05,
      "loss": 1.8844,
      "step": 60700
    },
    {
      "epoch": 4.644412191582003,
      "grad_norm": 9.11629581451416,
      "learning_rate": 4.41944847605225e-05,
      "loss": 1.8694,
      "step": 60800
    },
    {
      "epoch": 4.652051027423421,
      "grad_norm": 6.8822736740112305,
      "learning_rate": 4.418493621572072e-05,
      "loss": 1.9916,
      "step": 60900
    },
    {
      "epoch": 4.6596898632648385,
      "grad_norm": 5.972846984863281,
      "learning_rate": 4.4175387670918956e-05,
      "loss": 2.0354,
      "step": 61000
    },
    {
      "epoch": 4.667328699106256,
      "grad_norm": 7.193078517913818,
      "learning_rate": 4.416583912611718e-05,
      "loss": 2.04,
      "step": 61100
    },
    {
      "epoch": 4.674967534947674,
      "grad_norm": 5.817370414733887,
      "learning_rate": 4.415629058131541e-05,
      "loss": 1.92,
      "step": 61200
    },
    {
      "epoch": 4.682606370789092,
      "grad_norm": 5.4264702796936035,
      "learning_rate": 4.4146742036513636e-05,
      "loss": 2.0153,
      "step": 61300
    },
    {
      "epoch": 4.690245206630509,
      "grad_norm": 6.6571736335754395,
      "learning_rate": 4.4137193491711864e-05,
      "loss": 1.9897,
      "step": 61400
    },
    {
      "epoch": 4.6978840424719275,
      "grad_norm": 4.65815544128418,
      "learning_rate": 4.412764494691009e-05,
      "loss": 1.8937,
      "step": 61500
    },
    {
      "epoch": 4.705522878313345,
      "grad_norm": 10.356531143188477,
      "learning_rate": 4.411809640210832e-05,
      "loss": 1.953,
      "step": 61600
    },
    {
      "epoch": 4.713161714154763,
      "grad_norm": 7.291105270385742,
      "learning_rate": 4.410854785730655e-05,
      "loss": 1.9523,
      "step": 61700
    },
    {
      "epoch": 4.720800549996181,
      "grad_norm": 6.386364936828613,
      "learning_rate": 4.409899931250477e-05,
      "loss": 1.9086,
      "step": 61800
    },
    {
      "epoch": 4.728439385837598,
      "grad_norm": 7.79782247543335,
      "learning_rate": 4.408945076770301e-05,
      "loss": 1.9587,
      "step": 61900
    },
    {
      "epoch": 4.7360782216790165,
      "grad_norm": 9.829187393188477,
      "learning_rate": 4.407990222290123e-05,
      "loss": 1.9727,
      "step": 62000
    },
    {
      "epoch": 4.743717057520434,
      "grad_norm": 8.782186508178711,
      "learning_rate": 4.407035367809946e-05,
      "loss": 1.9872,
      "step": 62100
    },
    {
      "epoch": 4.751355893361851,
      "grad_norm": 6.709308624267578,
      "learning_rate": 4.406080513329769e-05,
      "loss": 1.9239,
      "step": 62200
    },
    {
      "epoch": 4.75899472920327,
      "grad_norm": 7.0496954917907715,
      "learning_rate": 4.4051256588495916e-05,
      "loss": 2.0348,
      "step": 62300
    },
    {
      "epoch": 4.766633565044687,
      "grad_norm": 6.455063343048096,
      "learning_rate": 4.404170804369414e-05,
      "loss": 1.9052,
      "step": 62400
    },
    {
      "epoch": 4.774272400886105,
      "grad_norm": 5.545165061950684,
      "learning_rate": 4.403215949889237e-05,
      "loss": 1.9463,
      "step": 62500
    },
    {
      "epoch": 4.781911236727523,
      "grad_norm": 6.495011329650879,
      "learning_rate": 4.4022610954090595e-05,
      "loss": 1.9441,
      "step": 62600
    },
    {
      "epoch": 4.78955007256894,
      "grad_norm": 6.53316593170166,
      "learning_rate": 4.4013062409288824e-05,
      "loss": 1.8408,
      "step": 62700
    },
    {
      "epoch": 4.797188908410359,
      "grad_norm": 8.846343040466309,
      "learning_rate": 4.400351386448706e-05,
      "loss": 1.9356,
      "step": 62800
    },
    {
      "epoch": 4.804827744251776,
      "grad_norm": 5.349152088165283,
      "learning_rate": 4.399396531968528e-05,
      "loss": 1.8432,
      "step": 62900
    },
    {
      "epoch": 4.812466580093194,
      "grad_norm": 4.980449199676514,
      "learning_rate": 4.398441677488351e-05,
      "loss": 2.0562,
      "step": 63000
    },
    {
      "epoch": 4.820105415934612,
      "grad_norm": 6.319592475891113,
      "learning_rate": 4.397486823008174e-05,
      "loss": 2.0765,
      "step": 63100
    },
    {
      "epoch": 4.827744251776029,
      "grad_norm": 7.001639366149902,
      "learning_rate": 4.396531968527997e-05,
      "loss": 1.9397,
      "step": 63200
    },
    {
      "epoch": 4.835383087617447,
      "grad_norm": 7.432343482971191,
      "learning_rate": 4.395577114047819e-05,
      "loss": 2.0075,
      "step": 63300
    },
    {
      "epoch": 4.843021923458865,
      "grad_norm": 7.410534381866455,
      "learning_rate": 4.3946222595676425e-05,
      "loss": 2.0926,
      "step": 63400
    },
    {
      "epoch": 4.850660759300283,
      "grad_norm": 10.68178653717041,
      "learning_rate": 4.3936674050874647e-05,
      "loss": 2.0201,
      "step": 63500
    },
    {
      "epoch": 4.8582995951417,
      "grad_norm": 7.585289478302002,
      "learning_rate": 4.3927125506072875e-05,
      "loss": 1.9729,
      "step": 63600
    },
    {
      "epoch": 4.865938430983118,
      "grad_norm": 8.411153793334961,
      "learning_rate": 4.3917576961271104e-05,
      "loss": 2.0187,
      "step": 63700
    },
    {
      "epoch": 4.873577266824536,
      "grad_norm": 6.060450553894043,
      "learning_rate": 4.390802841646933e-05,
      "loss": 2.0294,
      "step": 63800
    },
    {
      "epoch": 4.881216102665954,
      "grad_norm": 6.7687835693359375,
      "learning_rate": 4.389847987166756e-05,
      "loss": 1.9438,
      "step": 63900
    },
    {
      "epoch": 4.888854938507372,
      "grad_norm": 8.03898811340332,
      "learning_rate": 4.388893132686579e-05,
      "loss": 2.0187,
      "step": 64000
    },
    {
      "epoch": 4.896493774348789,
      "grad_norm": 7.919973373413086,
      "learning_rate": 4.387938278206402e-05,
      "loss": 2.0279,
      "step": 64100
    },
    {
      "epoch": 4.9041326101902065,
      "grad_norm": 6.375036239624023,
      "learning_rate": 4.386983423726224e-05,
      "loss": 1.9331,
      "step": 64200
    },
    {
      "epoch": 4.911771446031625,
      "grad_norm": 7.394561290740967,
      "learning_rate": 4.3860285692460476e-05,
      "loss": 2.0774,
      "step": 64300
    },
    {
      "epoch": 4.919410281873042,
      "grad_norm": 7.815742015838623,
      "learning_rate": 4.38507371476587e-05,
      "loss": 1.9936,
      "step": 64400
    },
    {
      "epoch": 4.927049117714461,
      "grad_norm": 5.976259708404541,
      "learning_rate": 4.384118860285693e-05,
      "loss": 2.0311,
      "step": 64500
    },
    {
      "epoch": 4.934687953555878,
      "grad_norm": 8.8474702835083,
      "learning_rate": 4.3831640058055155e-05,
      "loss": 1.9679,
      "step": 64600
    },
    {
      "epoch": 4.9423267893972955,
      "grad_norm": 6.681094646453857,
      "learning_rate": 4.3822091513253384e-05,
      "loss": 1.9613,
      "step": 64700
    },
    {
      "epoch": 4.949965625238714,
      "grad_norm": 7.476039886474609,
      "learning_rate": 4.3812542968451606e-05,
      "loss": 1.9983,
      "step": 64800
    },
    {
      "epoch": 4.957604461080131,
      "grad_norm": 6.650850772857666,
      "learning_rate": 4.380299442364984e-05,
      "loss": 2.07,
      "step": 64900
    },
    {
      "epoch": 4.96524329692155,
      "grad_norm": 6.500988006591797,
      "learning_rate": 4.3793445878848063e-05,
      "loss": 1.9295,
      "step": 65000
    },
    {
      "epoch": 4.972882132762967,
      "grad_norm": 5.788300514221191,
      "learning_rate": 4.378389733404629e-05,
      "loss": 1.9698,
      "step": 65100
    },
    {
      "epoch": 4.9805209686043845,
      "grad_norm": 5.706064701080322,
      "learning_rate": 4.377434878924452e-05,
      "loss": 1.973,
      "step": 65200
    },
    {
      "epoch": 4.988159804445802,
      "grad_norm": 6.058688163757324,
      "learning_rate": 4.376480024444275e-05,
      "loss": 1.9481,
      "step": 65300
    },
    {
      "epoch": 4.99579864028722,
      "grad_norm": 9.368419647216797,
      "learning_rate": 4.375525169964098e-05,
      "loss": 2.0272,
      "step": 65400
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.9154002666473389,
      "eval_runtime": 3.0198,
      "eval_samples_per_second": 228.493,
      "eval_steps_per_second": 228.493,
      "step": 65455
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7580475807189941,
      "eval_runtime": 56.7306,
      "eval_samples_per_second": 230.757,
      "eval_steps_per_second": 230.757,
      "step": 65455
    },
    {
      "epoch": 5.003437476128638,
      "grad_norm": 8.282947540283203,
      "learning_rate": 4.374570315483921e-05,
      "loss": 2.0336,
      "step": 65500
    },
    {
      "epoch": 5.011076311970056,
      "grad_norm": 6.394428253173828,
      "learning_rate": 4.3736154610037436e-05,
      "loss": 1.8649,
      "step": 65600
    },
    {
      "epoch": 5.0187151478114735,
      "grad_norm": 6.796335220336914,
      "learning_rate": 4.372660606523566e-05,
      "loss": 1.9792,
      "step": 65700
    },
    {
      "epoch": 5.026353983652891,
      "grad_norm": 5.67604398727417,
      "learning_rate": 4.371705752043389e-05,
      "loss": 1.9136,
      "step": 65800
    },
    {
      "epoch": 5.033992819494309,
      "grad_norm": 10.116143226623535,
      "learning_rate": 4.3707508975632115e-05,
      "loss": 1.946,
      "step": 65900
    },
    {
      "epoch": 5.041631655335727,
      "grad_norm": 7.464380741119385,
      "learning_rate": 4.3697960430830344e-05,
      "loss": 2.0773,
      "step": 66000
    },
    {
      "epoch": 5.049270491177144,
      "grad_norm": 6.860954284667969,
      "learning_rate": 4.368841188602857e-05,
      "loss": 1.9128,
      "step": 66100
    },
    {
      "epoch": 5.0569093270185625,
      "grad_norm": 6.405147552490234,
      "learning_rate": 4.36788633412268e-05,
      "loss": 1.9297,
      "step": 66200
    },
    {
      "epoch": 5.06454816285998,
      "grad_norm": 7.0616350173950195,
      "learning_rate": 4.366931479642502e-05,
      "loss": 1.9405,
      "step": 66300
    },
    {
      "epoch": 5.072186998701398,
      "grad_norm": 6.259587287902832,
      "learning_rate": 4.365976625162325e-05,
      "loss": 1.8854,
      "step": 66400
    },
    {
      "epoch": 5.079825834542816,
      "grad_norm": 7.995483875274658,
      "learning_rate": 4.365021770682148e-05,
      "loss": 2.0372,
      "step": 66500
    },
    {
      "epoch": 5.087464670384233,
      "grad_norm": 9.740421295166016,
      "learning_rate": 4.364066916201971e-05,
      "loss": 1.9864,
      "step": 66600
    },
    {
      "epoch": 5.095103506225652,
      "grad_norm": 7.8686347007751465,
      "learning_rate": 4.363112061721794e-05,
      "loss": 1.8479,
      "step": 66700
    },
    {
      "epoch": 5.102742342067069,
      "grad_norm": 7.35790491104126,
      "learning_rate": 4.3621572072416166e-05,
      "loss": 2.0305,
      "step": 66800
    },
    {
      "epoch": 5.1103811779084864,
      "grad_norm": 8.271369934082031,
      "learning_rate": 4.3612023527614395e-05,
      "loss": 1.9209,
      "step": 66900
    },
    {
      "epoch": 5.118020013749905,
      "grad_norm": 6.007514476776123,
      "learning_rate": 4.360247498281262e-05,
      "loss": 1.8589,
      "step": 67000
    },
    {
      "epoch": 5.125658849591322,
      "grad_norm": 8.321661949157715,
      "learning_rate": 4.359292643801085e-05,
      "loss": 1.8796,
      "step": 67100
    },
    {
      "epoch": 5.13329768543274,
      "grad_norm": 5.827358722686768,
      "learning_rate": 4.3583377893209074e-05,
      "loss": 1.9613,
      "step": 67200
    },
    {
      "epoch": 5.140936521274158,
      "grad_norm": 7.435380935668945,
      "learning_rate": 4.35738293484073e-05,
      "loss": 1.9846,
      "step": 67300
    },
    {
      "epoch": 5.1485753571155755,
      "grad_norm": 5.969575881958008,
      "learning_rate": 4.356428080360553e-05,
      "loss": 1.908,
      "step": 67400
    },
    {
      "epoch": 5.156214192956993,
      "grad_norm": 7.079057693481445,
      "learning_rate": 4.355473225880376e-05,
      "loss": 2.053,
      "step": 67500
    },
    {
      "epoch": 5.163853028798411,
      "grad_norm": 6.2350077629089355,
      "learning_rate": 4.354518371400199e-05,
      "loss": 1.9051,
      "step": 67600
    },
    {
      "epoch": 5.171491864639829,
      "grad_norm": 5.9908952713012695,
      "learning_rate": 4.353563516920022e-05,
      "loss": 2.0638,
      "step": 67700
    },
    {
      "epoch": 5.179130700481247,
      "grad_norm": 7.075287342071533,
      "learning_rate": 4.3526086624398446e-05,
      "loss": 2.0523,
      "step": 67800
    },
    {
      "epoch": 5.1867695363226645,
      "grad_norm": 6.432960033416748,
      "learning_rate": 4.351653807959667e-05,
      "loss": 1.9438,
      "step": 67900
    },
    {
      "epoch": 5.194408372164082,
      "grad_norm": 5.300244331359863,
      "learning_rate": 4.3506989534794904e-05,
      "loss": 1.868,
      "step": 68000
    },
    {
      "epoch": 5.2020472080055,
      "grad_norm": 7.110914707183838,
      "learning_rate": 4.3497440989993126e-05,
      "loss": 1.9814,
      "step": 68100
    },
    {
      "epoch": 5.209686043846918,
      "grad_norm": 7.360859394073486,
      "learning_rate": 4.3487892445191354e-05,
      "loss": 1.9489,
      "step": 68200
    },
    {
      "epoch": 5.217324879688335,
      "grad_norm": 6.817523002624512,
      "learning_rate": 4.347834390038958e-05,
      "loss": 1.9863,
      "step": 68300
    },
    {
      "epoch": 5.2249637155297535,
      "grad_norm": 7.010962009429932,
      "learning_rate": 4.346879535558781e-05,
      "loss": 1.9771,
      "step": 68400
    },
    {
      "epoch": 5.232602551371171,
      "grad_norm": 7.446442604064941,
      "learning_rate": 4.3459246810786034e-05,
      "loss": 1.9669,
      "step": 68500
    },
    {
      "epoch": 5.240241387212588,
      "grad_norm": 10.620014190673828,
      "learning_rate": 4.344969826598427e-05,
      "loss": 1.9004,
      "step": 68600
    },
    {
      "epoch": 5.247880223054007,
      "grad_norm": 8.22510814666748,
      "learning_rate": 4.344014972118249e-05,
      "loss": 1.8788,
      "step": 68700
    },
    {
      "epoch": 5.255519058895424,
      "grad_norm": 5.826534271240234,
      "learning_rate": 4.343060117638072e-05,
      "loss": 1.9747,
      "step": 68800
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 6.887749671936035,
      "learning_rate": 4.342105263157895e-05,
      "loss": 1.8694,
      "step": 68900
    },
    {
      "epoch": 5.27079673057826,
      "grad_norm": 6.581635475158691,
      "learning_rate": 4.341150408677718e-05,
      "loss": 1.9686,
      "step": 69000
    },
    {
      "epoch": 5.278435566419677,
      "grad_norm": 6.657167434692383,
      "learning_rate": 4.3401955541975406e-05,
      "loss": 1.9053,
      "step": 69100
    },
    {
      "epoch": 5.286074402261096,
      "grad_norm": 7.257842063903809,
      "learning_rate": 4.3392406997173635e-05,
      "loss": 1.9613,
      "step": 69200
    },
    {
      "epoch": 5.293713238102513,
      "grad_norm": 6.011744022369385,
      "learning_rate": 4.338285845237186e-05,
      "loss": 1.8651,
      "step": 69300
    },
    {
      "epoch": 5.301352073943931,
      "grad_norm": 7.867451190948486,
      "learning_rate": 4.3373309907570085e-05,
      "loss": 1.9347,
      "step": 69400
    },
    {
      "epoch": 5.308990909785349,
      "grad_norm": 6.607656002044678,
      "learning_rate": 4.336376136276832e-05,
      "loss": 1.9191,
      "step": 69500
    },
    {
      "epoch": 5.316629745626766,
      "grad_norm": 6.8656535148620605,
      "learning_rate": 4.335421281796654e-05,
      "loss": 1.921,
      "step": 69600
    },
    {
      "epoch": 5.324268581468184,
      "grad_norm": 9.455994606018066,
      "learning_rate": 4.334466427316477e-05,
      "loss": 1.9806,
      "step": 69700
    },
    {
      "epoch": 5.331907417309602,
      "grad_norm": 6.293274402618408,
      "learning_rate": 4.3335115728363e-05,
      "loss": 1.9035,
      "step": 69800
    },
    {
      "epoch": 5.33954625315102,
      "grad_norm": 7.371517181396484,
      "learning_rate": 4.332556718356123e-05,
      "loss": 1.9708,
      "step": 69900
    },
    {
      "epoch": 5.347185088992438,
      "grad_norm": 7.388585090637207,
      "learning_rate": 4.331601863875945e-05,
      "loss": 2.0198,
      "step": 70000
    },
    {
      "epoch": 5.354823924833855,
      "grad_norm": 9.63520622253418,
      "learning_rate": 4.3306470093957686e-05,
      "loss": 2.0525,
      "step": 70100
    },
    {
      "epoch": 5.362462760675273,
      "grad_norm": 5.821413516998291,
      "learning_rate": 4.329692154915591e-05,
      "loss": 1.9456,
      "step": 70200
    },
    {
      "epoch": 5.370101596516691,
      "grad_norm": 5.730468273162842,
      "learning_rate": 4.328737300435414e-05,
      "loss": 1.9742,
      "step": 70300
    },
    {
      "epoch": 5.377740432358109,
      "grad_norm": 6.145426273345947,
      "learning_rate": 4.3277824459552365e-05,
      "loss": 1.9301,
      "step": 70400
    },
    {
      "epoch": 5.385379268199526,
      "grad_norm": 5.797201633453369,
      "learning_rate": 4.3268275914750594e-05,
      "loss": 1.9467,
      "step": 70500
    },
    {
      "epoch": 5.393018104040944,
      "grad_norm": 6.520362377166748,
      "learning_rate": 4.325872736994882e-05,
      "loss": 1.9146,
      "step": 70600
    },
    {
      "epoch": 5.400656939882362,
      "grad_norm": 6.545873641967773,
      "learning_rate": 4.324917882514705e-05,
      "loss": 1.8848,
      "step": 70700
    },
    {
      "epoch": 5.408295775723779,
      "grad_norm": 6.293325901031494,
      "learning_rate": 4.323963028034528e-05,
      "loss": 1.9217,
      "step": 70800
    },
    {
      "epoch": 5.415934611565198,
      "grad_norm": 6.723865032196045,
      "learning_rate": 4.32300817355435e-05,
      "loss": 1.9328,
      "step": 70900
    },
    {
      "epoch": 5.423573447406615,
      "grad_norm": 10.02058219909668,
      "learning_rate": 4.322053319074174e-05,
      "loss": 1.9413,
      "step": 71000
    },
    {
      "epoch": 5.431212283248033,
      "grad_norm": 7.466328144073486,
      "learning_rate": 4.321098464593996e-05,
      "loss": 1.9398,
      "step": 71100
    },
    {
      "epoch": 5.438851119089451,
      "grad_norm": 8.173851013183594,
      "learning_rate": 4.320143610113819e-05,
      "loss": 1.9592,
      "step": 71200
    },
    {
      "epoch": 5.446489954930868,
      "grad_norm": 5.881141662597656,
      "learning_rate": 4.319188755633642e-05,
      "loss": 1.9996,
      "step": 71300
    },
    {
      "epoch": 5.454128790772287,
      "grad_norm": 7.450631618499756,
      "learning_rate": 4.3182339011534646e-05,
      "loss": 2.0166,
      "step": 71400
    },
    {
      "epoch": 5.461767626613704,
      "grad_norm": 9.29874324798584,
      "learning_rate": 4.3172790466732874e-05,
      "loss": 1.8761,
      "step": 71500
    },
    {
      "epoch": 5.4694064624551215,
      "grad_norm": 9.069369316101074,
      "learning_rate": 4.31632419219311e-05,
      "loss": 1.9271,
      "step": 71600
    },
    {
      "epoch": 5.47704529829654,
      "grad_norm": 6.768238067626953,
      "learning_rate": 4.315369337712933e-05,
      "loss": 1.9061,
      "step": 71700
    },
    {
      "epoch": 5.484684134137957,
      "grad_norm": 9.053297996520996,
      "learning_rate": 4.3144144832327554e-05,
      "loss": 1.9841,
      "step": 71800
    },
    {
      "epoch": 5.492322969979375,
      "grad_norm": 6.8078765869140625,
      "learning_rate": 4.313459628752579e-05,
      "loss": 1.9296,
      "step": 71900
    },
    {
      "epoch": 5.499961805820793,
      "grad_norm": 7.635904788970947,
      "learning_rate": 4.312504774272401e-05,
      "loss": 1.9066,
      "step": 72000
    },
    {
      "epoch": 5.5076006416622105,
      "grad_norm": 6.712381362915039,
      "learning_rate": 4.311549919792224e-05,
      "loss": 1.9046,
      "step": 72100
    },
    {
      "epoch": 5.515239477503629,
      "grad_norm": 8.585090637207031,
      "learning_rate": 4.310595065312046e-05,
      "loss": 1.9823,
      "step": 72200
    },
    {
      "epoch": 5.522878313345046,
      "grad_norm": 6.261736869812012,
      "learning_rate": 4.30964021083187e-05,
      "loss": 1.8903,
      "step": 72300
    },
    {
      "epoch": 5.530517149186464,
      "grad_norm": 8.226005554199219,
      "learning_rate": 4.308685356351692e-05,
      "loss": 1.9226,
      "step": 72400
    },
    {
      "epoch": 5.538155985027882,
      "grad_norm": 6.0195465087890625,
      "learning_rate": 4.307730501871515e-05,
      "loss": 1.9595,
      "step": 72500
    },
    {
      "epoch": 5.5457948208692995,
      "grad_norm": 8.492079734802246,
      "learning_rate": 4.3067756473913376e-05,
      "loss": 1.963,
      "step": 72600
    },
    {
      "epoch": 5.553433656710717,
      "grad_norm": 5.914926052093506,
      "learning_rate": 4.3058207929111605e-05,
      "loss": 1.9576,
      "step": 72700
    },
    {
      "epoch": 5.561072492552135,
      "grad_norm": 6.397109508514404,
      "learning_rate": 4.3048659384309834e-05,
      "loss": 1.8233,
      "step": 72800
    },
    {
      "epoch": 5.568711328393553,
      "grad_norm": 7.171339988708496,
      "learning_rate": 4.303911083950806e-05,
      "loss": 1.8645,
      "step": 72900
    },
    {
      "epoch": 5.57635016423497,
      "grad_norm": 5.847250461578369,
      "learning_rate": 4.302956229470629e-05,
      "loss": 2.0166,
      "step": 73000
    },
    {
      "epoch": 5.5839890000763885,
      "grad_norm": 7.95049524307251,
      "learning_rate": 4.302001374990451e-05,
      "loss": 1.9279,
      "step": 73100
    },
    {
      "epoch": 5.591627835917806,
      "grad_norm": 6.161503791809082,
      "learning_rate": 4.301046520510275e-05,
      "loss": 1.9432,
      "step": 73200
    },
    {
      "epoch": 5.599266671759224,
      "grad_norm": 9.074132919311523,
      "learning_rate": 4.300091666030097e-05,
      "loss": 1.9615,
      "step": 73300
    },
    {
      "epoch": 5.606905507600642,
      "grad_norm": 6.698201656341553,
      "learning_rate": 4.29913681154992e-05,
      "loss": 1.9249,
      "step": 73400
    },
    {
      "epoch": 5.614544343442059,
      "grad_norm": 7.1738362312316895,
      "learning_rate": 4.298181957069743e-05,
      "loss": 1.915,
      "step": 73500
    },
    {
      "epoch": 5.6221831792834775,
      "grad_norm": 8.812691688537598,
      "learning_rate": 4.2972271025895656e-05,
      "loss": 1.9047,
      "step": 73600
    },
    {
      "epoch": 5.629822015124895,
      "grad_norm": 5.49664831161499,
      "learning_rate": 4.296272248109388e-05,
      "loss": 2.0223,
      "step": 73700
    },
    {
      "epoch": 5.637460850966312,
      "grad_norm": 5.5522308349609375,
      "learning_rate": 4.2953173936292114e-05,
      "loss": 1.9568,
      "step": 73800
    },
    {
      "epoch": 5.645099686807731,
      "grad_norm": 7.464873313903809,
      "learning_rate": 4.2943625391490336e-05,
      "loss": 1.9161,
      "step": 73900
    },
    {
      "epoch": 5.652738522649148,
      "grad_norm": 9.211423873901367,
      "learning_rate": 4.2934076846688564e-05,
      "loss": 1.9931,
      "step": 74000
    },
    {
      "epoch": 5.660377358490566,
      "grad_norm": 6.904640197753906,
      "learning_rate": 4.292452830188679e-05,
      "loss": 1.91,
      "step": 74100
    },
    {
      "epoch": 5.668016194331984,
      "grad_norm": 5.692299842834473,
      "learning_rate": 4.291497975708502e-05,
      "loss": 1.9866,
      "step": 74200
    },
    {
      "epoch": 5.6756550301734014,
      "grad_norm": 8.469564437866211,
      "learning_rate": 4.290543121228325e-05,
      "loss": 2.0173,
      "step": 74300
    },
    {
      "epoch": 5.68329386601482,
      "grad_norm": 7.360446453094482,
      "learning_rate": 4.289588266748148e-05,
      "loss": 1.9639,
      "step": 74400
    },
    {
      "epoch": 5.690932701856237,
      "grad_norm": 6.378288269042969,
      "learning_rate": 4.288633412267971e-05,
      "loss": 1.9766,
      "step": 74500
    },
    {
      "epoch": 5.698571537697655,
      "grad_norm": 5.957810401916504,
      "learning_rate": 4.287678557787793e-05,
      "loss": 2.0065,
      "step": 74600
    },
    {
      "epoch": 5.706210373539073,
      "grad_norm": 6.1016364097595215,
      "learning_rate": 4.2867237033076165e-05,
      "loss": 1.9703,
      "step": 74700
    },
    {
      "epoch": 5.7138492093804905,
      "grad_norm": 8.833038330078125,
      "learning_rate": 4.285768848827439e-05,
      "loss": 1.8403,
      "step": 74800
    },
    {
      "epoch": 5.721488045221908,
      "grad_norm": 6.779305458068848,
      "learning_rate": 4.2848139943472616e-05,
      "loss": 1.9795,
      "step": 74900
    },
    {
      "epoch": 5.729126881063326,
      "grad_norm": 5.619870185852051,
      "learning_rate": 4.2838591398670845e-05,
      "loss": 1.8976,
      "step": 75000
    },
    {
      "epoch": 5.736765716904744,
      "grad_norm": 9.622161865234375,
      "learning_rate": 4.282904285386907e-05,
      "loss": 1.9222,
      "step": 75100
    },
    {
      "epoch": 5.744404552746161,
      "grad_norm": 13.821327209472656,
      "learning_rate": 4.2819494309067295e-05,
      "loss": 1.977,
      "step": 75200
    },
    {
      "epoch": 5.7520433885875795,
      "grad_norm": 8.500223159790039,
      "learning_rate": 4.280994576426553e-05,
      "loss": 1.9018,
      "step": 75300
    },
    {
      "epoch": 5.759682224428997,
      "grad_norm": 7.1965789794921875,
      "learning_rate": 4.280039721946375e-05,
      "loss": 1.9721,
      "step": 75400
    },
    {
      "epoch": 5.767321060270415,
      "grad_norm": 6.635129928588867,
      "learning_rate": 4.279084867466198e-05,
      "loss": 1.9304,
      "step": 75500
    },
    {
      "epoch": 5.774959896111833,
      "grad_norm": 8.306882858276367,
      "learning_rate": 4.278130012986022e-05,
      "loss": 1.9804,
      "step": 75600
    },
    {
      "epoch": 5.78259873195325,
      "grad_norm": 6.787155628204346,
      "learning_rate": 4.277175158505844e-05,
      "loss": 2.073,
      "step": 75700
    },
    {
      "epoch": 5.7902375677946685,
      "grad_norm": 5.6442670822143555,
      "learning_rate": 4.276220304025667e-05,
      "loss": 1.8752,
      "step": 75800
    },
    {
      "epoch": 5.797876403636086,
      "grad_norm": 5.2231035232543945,
      "learning_rate": 4.2752654495454896e-05,
      "loss": 1.806,
      "step": 75900
    },
    {
      "epoch": 5.805515239477503,
      "grad_norm": 6.176200866699219,
      "learning_rate": 4.2743105950653125e-05,
      "loss": 2.0321,
      "step": 76000
    },
    {
      "epoch": 5.813154075318922,
      "grad_norm": 6.061190605163574,
      "learning_rate": 4.273355740585135e-05,
      "loss": 1.98,
      "step": 76100
    },
    {
      "epoch": 5.820792911160339,
      "grad_norm": 6.527734756469727,
      "learning_rate": 4.272400886104958e-05,
      "loss": 1.9082,
      "step": 76200
    },
    {
      "epoch": 5.828431747001757,
      "grad_norm": 8.996277809143066,
      "learning_rate": 4.2714460316247804e-05,
      "loss": 1.9097,
      "step": 76300
    },
    {
      "epoch": 5.836070582843175,
      "grad_norm": 6.12342643737793,
      "learning_rate": 4.270491177144603e-05,
      "loss": 1.9477,
      "step": 76400
    },
    {
      "epoch": 5.843709418684592,
      "grad_norm": 5.547939777374268,
      "learning_rate": 4.269536322664426e-05,
      "loss": 2.0288,
      "step": 76500
    },
    {
      "epoch": 5.851348254526011,
      "grad_norm": 6.079452037811279,
      "learning_rate": 4.268581468184249e-05,
      "loss": 1.9917,
      "step": 76600
    },
    {
      "epoch": 5.858987090367428,
      "grad_norm": 5.362397193908691,
      "learning_rate": 4.267626613704072e-05,
      "loss": 1.9439,
      "step": 76700
    },
    {
      "epoch": 5.866625926208846,
      "grad_norm": 7.105629920959473,
      "learning_rate": 4.266671759223895e-05,
      "loss": 1.9047,
      "step": 76800
    },
    {
      "epoch": 5.874264762050263,
      "grad_norm": 6.661077976226807,
      "learning_rate": 4.2657169047437176e-05,
      "loss": 2.0367,
      "step": 76900
    },
    {
      "epoch": 5.881903597891681,
      "grad_norm": 6.346085071563721,
      "learning_rate": 4.26476205026354e-05,
      "loss": 1.8912,
      "step": 77000
    },
    {
      "epoch": 5.889542433733099,
      "grad_norm": 7.967213153839111,
      "learning_rate": 4.2638071957833634e-05,
      "loss": 1.9213,
      "step": 77100
    },
    {
      "epoch": 5.897181269574517,
      "grad_norm": 7.528043746948242,
      "learning_rate": 4.2628523413031856e-05,
      "loss": 1.9448,
      "step": 77200
    },
    {
      "epoch": 5.904820105415935,
      "grad_norm": 6.253414154052734,
      "learning_rate": 4.2618974868230084e-05,
      "loss": 1.8728,
      "step": 77300
    },
    {
      "epoch": 5.912458941257352,
      "grad_norm": 5.67581033706665,
      "learning_rate": 4.260942632342831e-05,
      "loss": 1.9074,
      "step": 77400
    },
    {
      "epoch": 5.92009777709877,
      "grad_norm": 8.113992691040039,
      "learning_rate": 4.259987777862654e-05,
      "loss": 1.9973,
      "step": 77500
    },
    {
      "epoch": 5.927736612940188,
      "grad_norm": 6.8124494552612305,
      "learning_rate": 4.2590329233824764e-05,
      "loss": 1.9268,
      "step": 77600
    },
    {
      "epoch": 5.935375448781606,
      "grad_norm": 7.727973937988281,
      "learning_rate": 4.2580780689023e-05,
      "loss": 1.8916,
      "step": 77700
    },
    {
      "epoch": 5.943014284623024,
      "grad_norm": 6.607117176055908,
      "learning_rate": 4.257123214422122e-05,
      "loss": 1.9057,
      "step": 77800
    },
    {
      "epoch": 5.950653120464441,
      "grad_norm": 7.215540409088135,
      "learning_rate": 4.256168359941945e-05,
      "loss": 1.9848,
      "step": 77900
    },
    {
      "epoch": 5.9582919563058585,
      "grad_norm": 13.654770851135254,
      "learning_rate": 4.255213505461768e-05,
      "loss": 1.9968,
      "step": 78000
    },
    {
      "epoch": 5.965930792147277,
      "grad_norm": 9.93260383605957,
      "learning_rate": 4.254258650981591e-05,
      "loss": 2.0099,
      "step": 78100
    },
    {
      "epoch": 5.973569627988694,
      "grad_norm": 9.13723373413086,
      "learning_rate": 4.2533037965014136e-05,
      "loss": 1.9496,
      "step": 78200
    },
    {
      "epoch": 5.981208463830113,
      "grad_norm": 7.36950159072876,
      "learning_rate": 4.252348942021236e-05,
      "loss": 1.9862,
      "step": 78300
    },
    {
      "epoch": 5.98884729967153,
      "grad_norm": 8.551340103149414,
      "learning_rate": 4.251394087541059e-05,
      "loss": 1.8955,
      "step": 78400
    },
    {
      "epoch": 5.9964861355129475,
      "grad_norm": 9.279335975646973,
      "learning_rate": 4.2504392330608815e-05,
      "loss": 1.9052,
      "step": 78500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.8964697122573853,
      "eval_runtime": 2.9963,
      "eval_samples_per_second": 230.284,
      "eval_steps_per_second": 230.284,
      "step": 78546
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.7266267538070679,
      "eval_runtime": 46.7596,
      "eval_samples_per_second": 279.964,
      "eval_steps_per_second": 279.964,
      "step": 78546
    },
    {
      "epoch": 6.004124971354366,
      "grad_norm": 7.2588958740234375,
      "learning_rate": 4.2494843785807044e-05,
      "loss": 1.8891,
      "step": 78600
    },
    {
      "epoch": 6.011763807195783,
      "grad_norm": 9.24697494506836,
      "learning_rate": 4.248529524100527e-05,
      "loss": 1.8871,
      "step": 78700
    },
    {
      "epoch": 6.019402643037201,
      "grad_norm": 6.401134490966797,
      "learning_rate": 4.24757466962035e-05,
      "loss": 1.7921,
      "step": 78800
    },
    {
      "epoch": 6.027041478878619,
      "grad_norm": 6.63100004196167,
      "learning_rate": 4.246619815140172e-05,
      "loss": 1.9173,
      "step": 78900
    },
    {
      "epoch": 6.0346803147200365,
      "grad_norm": 11.566176414489746,
      "learning_rate": 4.245664960659996e-05,
      "loss": 1.9175,
      "step": 79000
    },
    {
      "epoch": 6.042319150561455,
      "grad_norm": 6.049561977386475,
      "learning_rate": 4.244710106179818e-05,
      "loss": 1.9972,
      "step": 79100
    },
    {
      "epoch": 6.049957986402872,
      "grad_norm": 7.422032356262207,
      "learning_rate": 4.243755251699641e-05,
      "loss": 1.9896,
      "step": 79200
    },
    {
      "epoch": 6.05759682224429,
      "grad_norm": 5.330448627471924,
      "learning_rate": 4.242800397219464e-05,
      "loss": 1.9791,
      "step": 79300
    },
    {
      "epoch": 6.065235658085708,
      "grad_norm": 6.8091278076171875,
      "learning_rate": 4.2418455427392866e-05,
      "loss": 1.9218,
      "step": 79400
    },
    {
      "epoch": 6.0728744939271255,
      "grad_norm": 7.123209476470947,
      "learning_rate": 4.2408906882591095e-05,
      "loss": 1.9146,
      "step": 79500
    },
    {
      "epoch": 6.080513329768543,
      "grad_norm": 7.321145534515381,
      "learning_rate": 4.2399358337789324e-05,
      "loss": 1.913,
      "step": 79600
    },
    {
      "epoch": 6.088152165609961,
      "grad_norm": 9.134397506713867,
      "learning_rate": 4.238980979298755e-05,
      "loss": 1.8856,
      "step": 79700
    },
    {
      "epoch": 6.095791001451379,
      "grad_norm": 11.001266479492188,
      "learning_rate": 4.2380261248185774e-05,
      "loss": 1.9432,
      "step": 79800
    },
    {
      "epoch": 6.103429837292796,
      "grad_norm": 6.815329551696777,
      "learning_rate": 4.237071270338401e-05,
      "loss": 1.8768,
      "step": 79900
    },
    {
      "epoch": 6.1110686731342145,
      "grad_norm": 7.128061294555664,
      "learning_rate": 4.236116415858223e-05,
      "loss": 1.9972,
      "step": 80000
    },
    {
      "epoch": 6.118707508975632,
      "grad_norm": 4.323829650878906,
      "learning_rate": 4.235161561378046e-05,
      "loss": 1.9623,
      "step": 80100
    },
    {
      "epoch": 6.126346344817049,
      "grad_norm": 7.189273357391357,
      "learning_rate": 4.234206706897869e-05,
      "loss": 1.9225,
      "step": 80200
    },
    {
      "epoch": 6.133985180658468,
      "grad_norm": 7.266464710235596,
      "learning_rate": 4.233251852417692e-05,
      "loss": 1.8256,
      "step": 80300
    },
    {
      "epoch": 6.141624016499885,
      "grad_norm": 7.267820835113525,
      "learning_rate": 4.232296997937515e-05,
      "loss": 1.968,
      "step": 80400
    },
    {
      "epoch": 6.1492628523413035,
      "grad_norm": 6.351726531982422,
      "learning_rate": 4.2313421434573375e-05,
      "loss": 1.8566,
      "step": 80500
    },
    {
      "epoch": 6.156901688182721,
      "grad_norm": 6.630866050720215,
      "learning_rate": 4.2303872889771604e-05,
      "loss": 1.8851,
      "step": 80600
    },
    {
      "epoch": 6.164540524024138,
      "grad_norm": 6.620143413543701,
      "learning_rate": 4.2294324344969826e-05,
      "loss": 1.8518,
      "step": 80700
    },
    {
      "epoch": 6.172179359865557,
      "grad_norm": 8.944049835205078,
      "learning_rate": 4.228477580016806e-05,
      "loss": 1.8931,
      "step": 80800
    },
    {
      "epoch": 6.179818195706974,
      "grad_norm": 5.639257431030273,
      "learning_rate": 4.227522725536628e-05,
      "loss": 1.9448,
      "step": 80900
    },
    {
      "epoch": 6.187457031548392,
      "grad_norm": 6.991621017456055,
      "learning_rate": 4.226567871056451e-05,
      "loss": 1.9526,
      "step": 81000
    },
    {
      "epoch": 6.19509586738981,
      "grad_norm": 7.248317718505859,
      "learning_rate": 4.225613016576274e-05,
      "loss": 2.0004,
      "step": 81100
    },
    {
      "epoch": 6.202734703231227,
      "grad_norm": 6.782402038574219,
      "learning_rate": 4.224658162096097e-05,
      "loss": 1.9792,
      "step": 81200
    },
    {
      "epoch": 6.210373539072645,
      "grad_norm": 9.6021089553833,
      "learning_rate": 4.223703307615919e-05,
      "loss": 1.9141,
      "step": 81300
    },
    {
      "epoch": 6.218012374914063,
      "grad_norm": 6.939171314239502,
      "learning_rate": 4.222748453135743e-05,
      "loss": 1.928,
      "step": 81400
    },
    {
      "epoch": 6.225651210755481,
      "grad_norm": 6.419640064239502,
      "learning_rate": 4.221793598655565e-05,
      "loss": 1.9332,
      "step": 81500
    },
    {
      "epoch": 6.233290046596899,
      "grad_norm": 5.223515510559082,
      "learning_rate": 4.220838744175388e-05,
      "loss": 2.0264,
      "step": 81600
    },
    {
      "epoch": 6.2409288824383164,
      "grad_norm": 8.486376762390137,
      "learning_rate": 4.2198838896952106e-05,
      "loss": 1.8791,
      "step": 81700
    },
    {
      "epoch": 6.248567718279734,
      "grad_norm": 5.002850532531738,
      "learning_rate": 4.2189290352150335e-05,
      "loss": 1.8622,
      "step": 81800
    },
    {
      "epoch": 6.256206554121152,
      "grad_norm": 5.97911262512207,
      "learning_rate": 4.2179741807348563e-05,
      "loss": 1.8277,
      "step": 81900
    },
    {
      "epoch": 6.26384538996257,
      "grad_norm": 6.976006984710693,
      "learning_rate": 4.217019326254679e-05,
      "loss": 1.9763,
      "step": 82000
    },
    {
      "epoch": 6.271484225803987,
      "grad_norm": 9.626128196716309,
      "learning_rate": 4.216064471774502e-05,
      "loss": 1.9803,
      "step": 82100
    },
    {
      "epoch": 6.2791230616454055,
      "grad_norm": 6.96754264831543,
      "learning_rate": 4.215109617294324e-05,
      "loss": 2.0178,
      "step": 82200
    },
    {
      "epoch": 6.286761897486823,
      "grad_norm": 3.6281023025512695,
      "learning_rate": 4.214154762814148e-05,
      "loss": 1.8227,
      "step": 82300
    },
    {
      "epoch": 6.29440073332824,
      "grad_norm": 7.05527925491333,
      "learning_rate": 4.21319990833397e-05,
      "loss": 1.9248,
      "step": 82400
    },
    {
      "epoch": 6.302039569169659,
      "grad_norm": 7.9635419845581055,
      "learning_rate": 4.212245053853793e-05,
      "loss": 1.8619,
      "step": 82500
    },
    {
      "epoch": 6.309678405011076,
      "grad_norm": 6.859588623046875,
      "learning_rate": 4.211290199373616e-05,
      "loss": 1.9144,
      "step": 82600
    },
    {
      "epoch": 6.3173172408524945,
      "grad_norm": 10.175554275512695,
      "learning_rate": 4.2103353448934386e-05,
      "loss": 1.9486,
      "step": 82700
    },
    {
      "epoch": 6.324956076693912,
      "grad_norm": 9.976264953613281,
      "learning_rate": 4.209380490413261e-05,
      "loss": 1.9721,
      "step": 82800
    },
    {
      "epoch": 6.332594912535329,
      "grad_norm": 6.885308742523193,
      "learning_rate": 4.2084256359330844e-05,
      "loss": 1.8342,
      "step": 82900
    },
    {
      "epoch": 6.340233748376748,
      "grad_norm": 7.772210121154785,
      "learning_rate": 4.2074707814529066e-05,
      "loss": 2.0137,
      "step": 83000
    },
    {
      "epoch": 6.347872584218165,
      "grad_norm": 6.790281772613525,
      "learning_rate": 4.2065159269727294e-05,
      "loss": 1.846,
      "step": 83100
    },
    {
      "epoch": 6.355511420059583,
      "grad_norm": 7.9725422859191895,
      "learning_rate": 4.205561072492552e-05,
      "loss": 1.9147,
      "step": 83200
    },
    {
      "epoch": 6.363150255901001,
      "grad_norm": 7.0568647384643555,
      "learning_rate": 4.204606218012375e-05,
      "loss": 1.9707,
      "step": 83300
    },
    {
      "epoch": 6.370789091742418,
      "grad_norm": 7.256809711456299,
      "learning_rate": 4.203651363532198e-05,
      "loss": 1.9695,
      "step": 83400
    },
    {
      "epoch": 6.378427927583836,
      "grad_norm": 8.897439956665039,
      "learning_rate": 4.202696509052021e-05,
      "loss": 1.9733,
      "step": 83500
    },
    {
      "epoch": 6.386066763425254,
      "grad_norm": 6.171499252319336,
      "learning_rate": 4.201741654571844e-05,
      "loss": 1.94,
      "step": 83600
    },
    {
      "epoch": 6.393705599266672,
      "grad_norm": 6.594298362731934,
      "learning_rate": 4.200786800091666e-05,
      "loss": 1.8652,
      "step": 83700
    },
    {
      "epoch": 6.40134443510809,
      "grad_norm": 6.3220391273498535,
      "learning_rate": 4.199831945611489e-05,
      "loss": 1.9137,
      "step": 83800
    },
    {
      "epoch": 6.408983270949507,
      "grad_norm": 6.991079330444336,
      "learning_rate": 4.198877091131312e-05,
      "loss": 1.9749,
      "step": 83900
    },
    {
      "epoch": 6.416622106790925,
      "grad_norm": 10.734260559082031,
      "learning_rate": 4.1979222366511346e-05,
      "loss": 1.9002,
      "step": 84000
    },
    {
      "epoch": 6.424260942632343,
      "grad_norm": 6.801245212554932,
      "learning_rate": 4.196967382170957e-05,
      "loss": 2.0107,
      "step": 84100
    },
    {
      "epoch": 6.431899778473761,
      "grad_norm": 7.216887474060059,
      "learning_rate": 4.19601252769078e-05,
      "loss": 1.9265,
      "step": 84200
    },
    {
      "epoch": 6.439538614315178,
      "grad_norm": 7.5171332359313965,
      "learning_rate": 4.1950576732106025e-05,
      "loss": 1.9452,
      "step": 84300
    },
    {
      "epoch": 6.447177450156596,
      "grad_norm": 5.0432915687561035,
      "learning_rate": 4.1941028187304254e-05,
      "loss": 1.8935,
      "step": 84400
    },
    {
      "epoch": 6.454816285998014,
      "grad_norm": 4.544709205627441,
      "learning_rate": 4.193147964250249e-05,
      "loss": 1.9256,
      "step": 84500
    },
    {
      "epoch": 6.462455121839431,
      "grad_norm": 5.434264659881592,
      "learning_rate": 4.192193109770071e-05,
      "loss": 1.9317,
      "step": 84600
    },
    {
      "epoch": 6.47009395768085,
      "grad_norm": 7.697701930999756,
      "learning_rate": 4.191238255289894e-05,
      "loss": 1.9326,
      "step": 84700
    },
    {
      "epoch": 6.477732793522267,
      "grad_norm": 9.785834312438965,
      "learning_rate": 4.190283400809717e-05,
      "loss": 1.9059,
      "step": 84800
    },
    {
      "epoch": 6.485371629363685,
      "grad_norm": 7.708517551422119,
      "learning_rate": 4.18932854632954e-05,
      "loss": 1.969,
      "step": 84900
    },
    {
      "epoch": 6.493010465205103,
      "grad_norm": 5.659229755401611,
      "learning_rate": 4.188373691849362e-05,
      "loss": 1.9059,
      "step": 85000
    },
    {
      "epoch": 6.50064930104652,
      "grad_norm": 7.607316017150879,
      "learning_rate": 4.1874188373691855e-05,
      "loss": 1.9495,
      "step": 85100
    },
    {
      "epoch": 6.508288136887939,
      "grad_norm": 8.921313285827637,
      "learning_rate": 4.1864639828890076e-05,
      "loss": 2.0022,
      "step": 85200
    },
    {
      "epoch": 6.515926972729356,
      "grad_norm": 7.5027174949646,
      "learning_rate": 4.1855091284088305e-05,
      "loss": 1.8083,
      "step": 85300
    },
    {
      "epoch": 6.5235658085707735,
      "grad_norm": 6.637967586517334,
      "learning_rate": 4.1845542739286534e-05,
      "loss": 1.9427,
      "step": 85400
    },
    {
      "epoch": 6.531204644412192,
      "grad_norm": 4.853468894958496,
      "learning_rate": 4.183599419448476e-05,
      "loss": 1.9019,
      "step": 85500
    },
    {
      "epoch": 6.538843480253609,
      "grad_norm": 7.57349967956543,
      "learning_rate": 4.182644564968299e-05,
      "loss": 2.0049,
      "step": 85600
    },
    {
      "epoch": 6.546482316095027,
      "grad_norm": 7.171957969665527,
      "learning_rate": 4.181689710488122e-05,
      "loss": 1.9313,
      "step": 85700
    },
    {
      "epoch": 6.554121151936445,
      "grad_norm": 5.476834297180176,
      "learning_rate": 4.180734856007945e-05,
      "loss": 1.9735,
      "step": 85800
    },
    {
      "epoch": 6.5617599877778625,
      "grad_norm": 8.476651191711426,
      "learning_rate": 4.179780001527767e-05,
      "loss": 1.8766,
      "step": 85900
    },
    {
      "epoch": 6.569398823619281,
      "grad_norm": 8.496095657348633,
      "learning_rate": 4.1788251470475906e-05,
      "loss": 1.8979,
      "step": 86000
    },
    {
      "epoch": 6.577037659460698,
      "grad_norm": 7.45686674118042,
      "learning_rate": 4.177870292567413e-05,
      "loss": 1.852,
      "step": 86100
    },
    {
      "epoch": 6.584676495302116,
      "grad_norm": 11.81496810913086,
      "learning_rate": 4.176915438087236e-05,
      "loss": 1.9533,
      "step": 86200
    },
    {
      "epoch": 6.592315331143534,
      "grad_norm": 7.235648155212402,
      "learning_rate": 4.1759605836070585e-05,
      "loss": 1.8169,
      "step": 86300
    },
    {
      "epoch": 6.5999541669849515,
      "grad_norm": 7.336433410644531,
      "learning_rate": 4.1750057291268814e-05,
      "loss": 1.87,
      "step": 86400
    },
    {
      "epoch": 6.607593002826369,
      "grad_norm": 6.872750282287598,
      "learning_rate": 4.1740508746467036e-05,
      "loss": 1.9333,
      "step": 86500
    },
    {
      "epoch": 6.615231838667787,
      "grad_norm": 6.837333679199219,
      "learning_rate": 4.173096020166527e-05,
      "loss": 1.9059,
      "step": 86600
    },
    {
      "epoch": 6.622870674509205,
      "grad_norm": 8.98557186126709,
      "learning_rate": 4.172141165686349e-05,
      "loss": 1.8932,
      "step": 86700
    },
    {
      "epoch": 6.630509510350622,
      "grad_norm": 6.85732889175415,
      "learning_rate": 4.171186311206172e-05,
      "loss": 1.976,
      "step": 86800
    },
    {
      "epoch": 6.6381483461920405,
      "grad_norm": 7.461789131164551,
      "learning_rate": 4.170231456725995e-05,
      "loss": 1.9498,
      "step": 86900
    },
    {
      "epoch": 6.645787182033458,
      "grad_norm": 6.117241859436035,
      "learning_rate": 4.169276602245818e-05,
      "loss": 1.9221,
      "step": 87000
    },
    {
      "epoch": 6.653426017874876,
      "grad_norm": 7.47758150100708,
      "learning_rate": 4.168321747765641e-05,
      "loss": 1.9592,
      "step": 87100
    },
    {
      "epoch": 6.661064853716294,
      "grad_norm": 7.245948791503906,
      "learning_rate": 4.167366893285464e-05,
      "loss": 1.8858,
      "step": 87200
    },
    {
      "epoch": 6.668703689557711,
      "grad_norm": 5.874728679656982,
      "learning_rate": 4.1664120388052865e-05,
      "loss": 2.0189,
      "step": 87300
    },
    {
      "epoch": 6.6763425253991295,
      "grad_norm": 7.7914910316467285,
      "learning_rate": 4.165457184325109e-05,
      "loss": 1.9484,
      "step": 87400
    },
    {
      "epoch": 6.683981361240547,
      "grad_norm": 7.909430027008057,
      "learning_rate": 4.164502329844932e-05,
      "loss": 1.8694,
      "step": 87500
    },
    {
      "epoch": 6.691620197081964,
      "grad_norm": 5.933159351348877,
      "learning_rate": 4.1635474753647545e-05,
      "loss": 1.9413,
      "step": 87600
    },
    {
      "epoch": 6.699259032923383,
      "grad_norm": 7.5720438957214355,
      "learning_rate": 4.1625926208845773e-05,
      "loss": 1.8254,
      "step": 87700
    },
    {
      "epoch": 6.7068978687648,
      "grad_norm": 8.064353942871094,
      "learning_rate": 4.1616377664044e-05,
      "loss": 1.9439,
      "step": 87800
    },
    {
      "epoch": 6.714536704606218,
      "grad_norm": 6.204165458679199,
      "learning_rate": 4.160682911924223e-05,
      "loss": 1.8619,
      "step": 87900
    },
    {
      "epoch": 6.722175540447636,
      "grad_norm": 6.340297698974609,
      "learning_rate": 4.159728057444045e-05,
      "loss": 1.9384,
      "step": 88000
    },
    {
      "epoch": 6.729814376289053,
      "grad_norm": 5.755966663360596,
      "learning_rate": 4.158773202963869e-05,
      "loss": 1.9791,
      "step": 88100
    },
    {
      "epoch": 6.737453212130472,
      "grad_norm": 6.966954231262207,
      "learning_rate": 4.157818348483691e-05,
      "loss": 1.9357,
      "step": 88200
    },
    {
      "epoch": 6.745092047971889,
      "grad_norm": 6.085020542144775,
      "learning_rate": 4.156863494003514e-05,
      "loss": 1.8884,
      "step": 88300
    },
    {
      "epoch": 6.752730883813307,
      "grad_norm": 9.461395263671875,
      "learning_rate": 4.155908639523337e-05,
      "loss": 1.8886,
      "step": 88400
    },
    {
      "epoch": 6.760369719654725,
      "grad_norm": 7.6721062660217285,
      "learning_rate": 4.1549537850431596e-05,
      "loss": 1.8945,
      "step": 88500
    },
    {
      "epoch": 6.768008555496142,
      "grad_norm": 7.371833801269531,
      "learning_rate": 4.1539989305629825e-05,
      "loss": 1.8673,
      "step": 88600
    },
    {
      "epoch": 6.77564739133756,
      "grad_norm": 8.066665649414062,
      "learning_rate": 4.1530440760828054e-05,
      "loss": 1.9913,
      "step": 88700
    },
    {
      "epoch": 6.783286227178978,
      "grad_norm": 7.75169038772583,
      "learning_rate": 4.152089221602628e-05,
      "loss": 1.8635,
      "step": 88800
    },
    {
      "epoch": 6.790925063020396,
      "grad_norm": 6.0054144859313965,
      "learning_rate": 4.1511343671224504e-05,
      "loss": 2.0015,
      "step": 88900
    },
    {
      "epoch": 6.798563898861813,
      "grad_norm": 6.238811016082764,
      "learning_rate": 4.150179512642274e-05,
      "loss": 1.8922,
      "step": 89000
    },
    {
      "epoch": 6.8062027347032314,
      "grad_norm": 5.727021217346191,
      "learning_rate": 4.149224658162096e-05,
      "loss": 1.9518,
      "step": 89100
    },
    {
      "epoch": 6.813841570544649,
      "grad_norm": 6.055706024169922,
      "learning_rate": 4.148269803681919e-05,
      "loss": 1.8945,
      "step": 89200
    },
    {
      "epoch": 6.821480406386067,
      "grad_norm": 9.420087814331055,
      "learning_rate": 4.147314949201742e-05,
      "loss": 1.9133,
      "step": 89300
    },
    {
      "epoch": 6.829119242227485,
      "grad_norm": 6.720590591430664,
      "learning_rate": 4.146360094721565e-05,
      "loss": 1.8952,
      "step": 89400
    },
    {
      "epoch": 6.836758078068902,
      "grad_norm": 7.4601521492004395,
      "learning_rate": 4.1454052402413876e-05,
      "loss": 1.8607,
      "step": 89500
    },
    {
      "epoch": 6.84439691391032,
      "grad_norm": 7.076979160308838,
      "learning_rate": 4.14445038576121e-05,
      "loss": 1.8711,
      "step": 89600
    },
    {
      "epoch": 6.852035749751738,
      "grad_norm": 7.241462707519531,
      "learning_rate": 4.1434955312810334e-05,
      "loss": 1.8753,
      "step": 89700
    },
    {
      "epoch": 6.859674585593155,
      "grad_norm": 9.004343032836914,
      "learning_rate": 4.1425406768008556e-05,
      "loss": 2.0075,
      "step": 89800
    },
    {
      "epoch": 6.867313421434574,
      "grad_norm": 6.704357147216797,
      "learning_rate": 4.1415858223206784e-05,
      "loss": 2.0125,
      "step": 89900
    },
    {
      "epoch": 6.874952257275991,
      "grad_norm": 7.32601261138916,
      "learning_rate": 4.140630967840501e-05,
      "loss": 1.928,
      "step": 90000
    },
    {
      "epoch": 6.882591093117409,
      "grad_norm": 8.687419891357422,
      "learning_rate": 4.139676113360324e-05,
      "loss": 1.9677,
      "step": 90100
    },
    {
      "epoch": 6.890229928958827,
      "grad_norm": 6.943373203277588,
      "learning_rate": 4.1387212588801464e-05,
      "loss": 1.9241,
      "step": 90200
    },
    {
      "epoch": 6.897868764800244,
      "grad_norm": 6.364107608795166,
      "learning_rate": 4.13776640439997e-05,
      "loss": 1.887,
      "step": 90300
    },
    {
      "epoch": 6.905507600641663,
      "grad_norm": 7.611913204193115,
      "learning_rate": 4.136811549919792e-05,
      "loss": 1.879,
      "step": 90400
    },
    {
      "epoch": 6.91314643648308,
      "grad_norm": 7.49164342880249,
      "learning_rate": 4.135856695439615e-05,
      "loss": 1.9371,
      "step": 90500
    },
    {
      "epoch": 6.920785272324498,
      "grad_norm": 6.3699631690979,
      "learning_rate": 4.134901840959438e-05,
      "loss": 2.0319,
      "step": 90600
    },
    {
      "epoch": 6.928424108165915,
      "grad_norm": 8.229853630065918,
      "learning_rate": 4.133946986479261e-05,
      "loss": 1.8281,
      "step": 90700
    },
    {
      "epoch": 6.936062944007333,
      "grad_norm": 9.470013618469238,
      "learning_rate": 4.1329921319990836e-05,
      "loss": 2.0553,
      "step": 90800
    },
    {
      "epoch": 6.943701779848751,
      "grad_norm": 8.67546558380127,
      "learning_rate": 4.1320372775189065e-05,
      "loss": 1.9468,
      "step": 90900
    },
    {
      "epoch": 6.951340615690169,
      "grad_norm": 4.956768989562988,
      "learning_rate": 4.131082423038729e-05,
      "loss": 1.9553,
      "step": 91000
    },
    {
      "epoch": 6.958979451531587,
      "grad_norm": 10.878795623779297,
      "learning_rate": 4.1301275685585515e-05,
      "loss": 1.8833,
      "step": 91100
    },
    {
      "epoch": 6.966618287373004,
      "grad_norm": 6.5218000411987305,
      "learning_rate": 4.129172714078375e-05,
      "loss": 2.0165,
      "step": 91200
    },
    {
      "epoch": 6.974257123214422,
      "grad_norm": 7.992405414581299,
      "learning_rate": 4.128217859598197e-05,
      "loss": 1.8266,
      "step": 91300
    },
    {
      "epoch": 6.98189595905584,
      "grad_norm": 6.887451171875,
      "learning_rate": 4.12726300511802e-05,
      "loss": 1.96,
      "step": 91400
    },
    {
      "epoch": 6.989534794897258,
      "grad_norm": 6.1659650802612305,
      "learning_rate": 4.126308150637843e-05,
      "loss": 1.8838,
      "step": 91500
    },
    {
      "epoch": 6.997173630738676,
      "grad_norm": 5.208205223083496,
      "learning_rate": 4.125353296157666e-05,
      "loss": 1.9101,
      "step": 91600
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8913630247116089,
      "eval_runtime": 2.9989,
      "eval_samples_per_second": 230.087,
      "eval_steps_per_second": 230.087,
      "step": 91637
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.7092511653900146,
      "eval_runtime": 57.8408,
      "eval_samples_per_second": 226.328,
      "eval_steps_per_second": 226.328,
      "step": 91637
    },
    {
      "epoch": 7.004812466580093,
      "grad_norm": 6.932521343231201,
      "learning_rate": 4.124398441677488e-05,
      "loss": 1.8892,
      "step": 91700
    },
    {
      "epoch": 7.012451302421511,
      "grad_norm": 6.427302837371826,
      "learning_rate": 4.1234435871973116e-05,
      "loss": 1.8183,
      "step": 91800
    },
    {
      "epoch": 7.020090138262929,
      "grad_norm": 6.158998012542725,
      "learning_rate": 4.122488732717134e-05,
      "loss": 1.8829,
      "step": 91900
    },
    {
      "epoch": 7.027728974104346,
      "grad_norm": 6.991139888763428,
      "learning_rate": 4.1215338782369567e-05,
      "loss": 1.9157,
      "step": 92000
    },
    {
      "epoch": 7.035367809945765,
      "grad_norm": 7.17530632019043,
      "learning_rate": 4.1205790237567795e-05,
      "loss": 1.8419,
      "step": 92100
    },
    {
      "epoch": 7.043006645787182,
      "grad_norm": 6.022710800170898,
      "learning_rate": 4.1196241692766024e-05,
      "loss": 2.0433,
      "step": 92200
    },
    {
      "epoch": 7.0506454816285995,
      "grad_norm": 6.657947540283203,
      "learning_rate": 4.118669314796425e-05,
      "loss": 1.9207,
      "step": 92300
    },
    {
      "epoch": 7.058284317470018,
      "grad_norm": 5.211402893066406,
      "learning_rate": 4.117714460316248e-05,
      "loss": 2.0017,
      "step": 92400
    },
    {
      "epoch": 7.065923153311435,
      "grad_norm": 7.960660457611084,
      "learning_rate": 4.116759605836071e-05,
      "loss": 1.7492,
      "step": 92500
    },
    {
      "epoch": 7.073561989152853,
      "grad_norm": 6.219730377197266,
      "learning_rate": 4.115804751355893e-05,
      "loss": 2.0069,
      "step": 92600
    },
    {
      "epoch": 7.081200824994271,
      "grad_norm": 8.795258522033691,
      "learning_rate": 4.114849896875717e-05,
      "loss": 1.8998,
      "step": 92700
    },
    {
      "epoch": 7.0888396608356885,
      "grad_norm": 8.61971664428711,
      "learning_rate": 4.113895042395539e-05,
      "loss": 1.9345,
      "step": 92800
    },
    {
      "epoch": 7.096478496677107,
      "grad_norm": 6.234696388244629,
      "learning_rate": 4.112940187915362e-05,
      "loss": 1.8842,
      "step": 92900
    },
    {
      "epoch": 7.104117332518524,
      "grad_norm": 6.76055908203125,
      "learning_rate": 4.111985333435185e-05,
      "loss": 1.9317,
      "step": 93000
    },
    {
      "epoch": 7.111756168359942,
      "grad_norm": 7.445959091186523,
      "learning_rate": 4.1110304789550075e-05,
      "loss": 1.8688,
      "step": 93100
    },
    {
      "epoch": 7.11939500420136,
      "grad_norm": 5.848552703857422,
      "learning_rate": 4.1100756244748304e-05,
      "loss": 1.9318,
      "step": 93200
    },
    {
      "epoch": 7.1270338400427775,
      "grad_norm": 6.057756423950195,
      "learning_rate": 4.109120769994653e-05,
      "loss": 1.8848,
      "step": 93300
    },
    {
      "epoch": 7.134672675884195,
      "grad_norm": 7.712060451507568,
      "learning_rate": 4.108165915514476e-05,
      "loss": 1.8438,
      "step": 93400
    },
    {
      "epoch": 7.142311511725613,
      "grad_norm": 8.070381164550781,
      "learning_rate": 4.1072110610342983e-05,
      "loss": 1.9601,
      "step": 93500
    },
    {
      "epoch": 7.149950347567031,
      "grad_norm": 6.406667232513428,
      "learning_rate": 4.106256206554122e-05,
      "loss": 1.9016,
      "step": 93600
    },
    {
      "epoch": 7.157589183408448,
      "grad_norm": 8.080098152160645,
      "learning_rate": 4.105301352073944e-05,
      "loss": 1.9074,
      "step": 93700
    },
    {
      "epoch": 7.1652280192498665,
      "grad_norm": 6.708651542663574,
      "learning_rate": 4.104346497593767e-05,
      "loss": 1.8514,
      "step": 93800
    },
    {
      "epoch": 7.172866855091284,
      "grad_norm": 6.7183074951171875,
      "learning_rate": 4.10339164311359e-05,
      "loss": 1.8213,
      "step": 93900
    },
    {
      "epoch": 7.180505690932701,
      "grad_norm": 8.498520851135254,
      "learning_rate": 4.102436788633413e-05,
      "loss": 1.8668,
      "step": 94000
    },
    {
      "epoch": 7.18814452677412,
      "grad_norm": 7.96572732925415,
      "learning_rate": 4.101481934153235e-05,
      "loss": 1.9368,
      "step": 94100
    },
    {
      "epoch": 7.195783362615537,
      "grad_norm": 8.766860961914062,
      "learning_rate": 4.1005270796730584e-05,
      "loss": 1.918,
      "step": 94200
    },
    {
      "epoch": 7.2034221984569555,
      "grad_norm": 7.426931858062744,
      "learning_rate": 4.0995722251928806e-05,
      "loss": 1.8539,
      "step": 94300
    },
    {
      "epoch": 7.211061034298373,
      "grad_norm": 6.38274621963501,
      "learning_rate": 4.0986173707127035e-05,
      "loss": 1.8814,
      "step": 94400
    },
    {
      "epoch": 7.21869987013979,
      "grad_norm": 10.551604270935059,
      "learning_rate": 4.0976625162325264e-05,
      "loss": 1.8195,
      "step": 94500
    },
    {
      "epoch": 7.226338705981209,
      "grad_norm": 7.053380012512207,
      "learning_rate": 4.096707661752349e-05,
      "loss": 1.9008,
      "step": 94600
    },
    {
      "epoch": 7.233977541822626,
      "grad_norm": 6.207006454467773,
      "learning_rate": 4.095752807272172e-05,
      "loss": 1.9047,
      "step": 94700
    },
    {
      "epoch": 7.241616377664044,
      "grad_norm": 7.182300090789795,
      "learning_rate": 4.094797952791995e-05,
      "loss": 1.9938,
      "step": 94800
    },
    {
      "epoch": 7.249255213505462,
      "grad_norm": 6.7203874588012695,
      "learning_rate": 4.093843098311818e-05,
      "loss": 2.0022,
      "step": 94900
    },
    {
      "epoch": 7.256894049346879,
      "grad_norm": 10.05907154083252,
      "learning_rate": 4.09288824383164e-05,
      "loss": 1.8518,
      "step": 95000
    },
    {
      "epoch": 7.264532885188297,
      "grad_norm": 7.607400417327881,
      "learning_rate": 4.0919333893514636e-05,
      "loss": 1.9055,
      "step": 95100
    },
    {
      "epoch": 7.272171721029715,
      "grad_norm": 6.218204498291016,
      "learning_rate": 4.090978534871286e-05,
      "loss": 1.9059,
      "step": 95200
    },
    {
      "epoch": 7.279810556871133,
      "grad_norm": 7.508163928985596,
      "learning_rate": 4.0900236803911086e-05,
      "loss": 2.0816,
      "step": 95300
    },
    {
      "epoch": 7.287449392712551,
      "grad_norm": 12.678997993469238,
      "learning_rate": 4.089068825910931e-05,
      "loss": 1.8184,
      "step": 95400
    },
    {
      "epoch": 7.295088228553968,
      "grad_norm": 9.505678176879883,
      "learning_rate": 4.0881139714307544e-05,
      "loss": 1.9486,
      "step": 95500
    },
    {
      "epoch": 7.302727064395386,
      "grad_norm": 6.216363906860352,
      "learning_rate": 4.0871591169505766e-05,
      "loss": 2.0309,
      "step": 95600
    },
    {
      "epoch": 7.310365900236804,
      "grad_norm": 7.41878080368042,
      "learning_rate": 4.0862042624703994e-05,
      "loss": 1.9819,
      "step": 95700
    },
    {
      "epoch": 7.318004736078222,
      "grad_norm": 7.145654201507568,
      "learning_rate": 4.085249407990222e-05,
      "loss": 2.0067,
      "step": 95800
    },
    {
      "epoch": 7.325643571919639,
      "grad_norm": 5.664588928222656,
      "learning_rate": 4.084294553510045e-05,
      "loss": 1.954,
      "step": 95900
    },
    {
      "epoch": 7.333282407761057,
      "grad_norm": 6.4692864418029785,
      "learning_rate": 4.083339699029868e-05,
      "loss": 1.93,
      "step": 96000
    },
    {
      "epoch": 7.340921243602475,
      "grad_norm": 7.881568431854248,
      "learning_rate": 4.082384844549691e-05,
      "loss": 1.8849,
      "step": 96100
    },
    {
      "epoch": 7.348560079443892,
      "grad_norm": 7.278811454772949,
      "learning_rate": 4.081429990069514e-05,
      "loss": 1.913,
      "step": 96200
    },
    {
      "epoch": 7.356198915285311,
      "grad_norm": 9.083024978637695,
      "learning_rate": 4.080475135589336e-05,
      "loss": 1.877,
      "step": 96300
    },
    {
      "epoch": 7.363837751126728,
      "grad_norm": 6.05536413192749,
      "learning_rate": 4.0795202811091595e-05,
      "loss": 1.8406,
      "step": 96400
    },
    {
      "epoch": 7.3714765869681464,
      "grad_norm": 6.170466423034668,
      "learning_rate": 4.078565426628982e-05,
      "loss": 1.8806,
      "step": 96500
    },
    {
      "epoch": 7.379115422809564,
      "grad_norm": 8.576214790344238,
      "learning_rate": 4.0776105721488046e-05,
      "loss": 1.8889,
      "step": 96600
    },
    {
      "epoch": 7.386754258650981,
      "grad_norm": 5.960804462432861,
      "learning_rate": 4.0766557176686275e-05,
      "loss": 1.891,
      "step": 96700
    },
    {
      "epoch": 7.3943930944924,
      "grad_norm": 6.188625812530518,
      "learning_rate": 4.07570086318845e-05,
      "loss": 1.9233,
      "step": 96800
    },
    {
      "epoch": 7.402031930333817,
      "grad_norm": 6.06535530090332,
      "learning_rate": 4.0747460087082725e-05,
      "loss": 1.8542,
      "step": 96900
    },
    {
      "epoch": 7.409670766175235,
      "grad_norm": 6.934112071990967,
      "learning_rate": 4.073791154228096e-05,
      "loss": 1.9735,
      "step": 97000
    },
    {
      "epoch": 7.417309602016653,
      "grad_norm": 9.519174575805664,
      "learning_rate": 4.072836299747918e-05,
      "loss": 1.8936,
      "step": 97100
    },
    {
      "epoch": 7.42494843785807,
      "grad_norm": 7.354151725769043,
      "learning_rate": 4.071881445267741e-05,
      "loss": 1.9963,
      "step": 97200
    },
    {
      "epoch": 7.432587273699488,
      "grad_norm": 8.076648712158203,
      "learning_rate": 4.070926590787565e-05,
      "loss": 1.8102,
      "step": 97300
    },
    {
      "epoch": 7.440226109540906,
      "grad_norm": 6.066806316375732,
      "learning_rate": 4.069971736307387e-05,
      "loss": 1.8871,
      "step": 97400
    },
    {
      "epoch": 7.447864945382324,
      "grad_norm": 8.888785362243652,
      "learning_rate": 4.06901688182721e-05,
      "loss": 1.915,
      "step": 97500
    },
    {
      "epoch": 7.455503781223742,
      "grad_norm": 6.44157075881958,
      "learning_rate": 4.0680620273470326e-05,
      "loss": 1.8079,
      "step": 97600
    },
    {
      "epoch": 7.463142617065159,
      "grad_norm": 9.03123950958252,
      "learning_rate": 4.0671071728668555e-05,
      "loss": 1.9568,
      "step": 97700
    },
    {
      "epoch": 7.470781452906577,
      "grad_norm": 8.766897201538086,
      "learning_rate": 4.0661523183866777e-05,
      "loss": 1.9345,
      "step": 97800
    },
    {
      "epoch": 7.478420288747995,
      "grad_norm": 4.641652584075928,
      "learning_rate": 4.065197463906501e-05,
      "loss": 1.9092,
      "step": 97900
    },
    {
      "epoch": 7.486059124589413,
      "grad_norm": 9.084683418273926,
      "learning_rate": 4.0642426094263234e-05,
      "loss": 1.8957,
      "step": 98000
    },
    {
      "epoch": 7.49369796043083,
      "grad_norm": 7.0007734298706055,
      "learning_rate": 4.063287754946146e-05,
      "loss": 1.8585,
      "step": 98100
    },
    {
      "epoch": 7.501336796272248,
      "grad_norm": 8.322876930236816,
      "learning_rate": 4.062332900465969e-05,
      "loss": 1.8668,
      "step": 98200
    },
    {
      "epoch": 7.508975632113666,
      "grad_norm": 5.848152160644531,
      "learning_rate": 4.061378045985792e-05,
      "loss": 1.8486,
      "step": 98300
    },
    {
      "epoch": 7.516614467955083,
      "grad_norm": 6.196250915527344,
      "learning_rate": 4.060423191505615e-05,
      "loss": 1.9036,
      "step": 98400
    },
    {
      "epoch": 7.524253303796502,
      "grad_norm": 6.28348445892334,
      "learning_rate": 4.059468337025438e-05,
      "loss": 1.9008,
      "step": 98500
    },
    {
      "epoch": 7.531892139637919,
      "grad_norm": 7.236030578613281,
      "learning_rate": 4.0585134825452606e-05,
      "loss": 1.9302,
      "step": 98600
    },
    {
      "epoch": 7.539530975479337,
      "grad_norm": 5.648194313049316,
      "learning_rate": 4.057558628065083e-05,
      "loss": 1.8836,
      "step": 98700
    },
    {
      "epoch": 7.547169811320755,
      "grad_norm": 6.7764997482299805,
      "learning_rate": 4.0566037735849064e-05,
      "loss": 1.9395,
      "step": 98800
    },
    {
      "epoch": 7.554808647162172,
      "grad_norm": 7.252432346343994,
      "learning_rate": 4.0556489191047285e-05,
      "loss": 1.9088,
      "step": 98900
    },
    {
      "epoch": 7.562447483003591,
      "grad_norm": 6.0941925048828125,
      "learning_rate": 4.0546940646245514e-05,
      "loss": 1.8785,
      "step": 99000
    },
    {
      "epoch": 7.570086318845008,
      "grad_norm": 9.956002235412598,
      "learning_rate": 4.053739210144374e-05,
      "loss": 1.8053,
      "step": 99100
    },
    {
      "epoch": 7.5777251546864255,
      "grad_norm": 6.594324588775635,
      "learning_rate": 4.052784355664197e-05,
      "loss": 1.8949,
      "step": 99200
    },
    {
      "epoch": 7.585363990527844,
      "grad_norm": 6.864537239074707,
      "learning_rate": 4.0518295011840193e-05,
      "loss": 1.9165,
      "step": 99300
    },
    {
      "epoch": 7.593002826369261,
      "grad_norm": 9.640536308288574,
      "learning_rate": 4.050874646703843e-05,
      "loss": 1.9912,
      "step": 99400
    },
    {
      "epoch": 7.600641662210679,
      "grad_norm": 7.250792980194092,
      "learning_rate": 4.049919792223665e-05,
      "loss": 1.8496,
      "step": 99500
    },
    {
      "epoch": 7.608280498052097,
      "grad_norm": 8.725637435913086,
      "learning_rate": 4.048964937743488e-05,
      "loss": 1.8908,
      "step": 99600
    },
    {
      "epoch": 7.6159193338935145,
      "grad_norm": 6.160686492919922,
      "learning_rate": 4.048010083263311e-05,
      "loss": 1.864,
      "step": 99700
    },
    {
      "epoch": 7.623558169734933,
      "grad_norm": 8.03596305847168,
      "learning_rate": 4.047055228783134e-05,
      "loss": 1.947,
      "step": 99800
    },
    {
      "epoch": 7.63119700557635,
      "grad_norm": 7.6055426597595215,
      "learning_rate": 4.0461003743029566e-05,
      "loss": 1.9075,
      "step": 99900
    },
    {
      "epoch": 7.638835841417768,
      "grad_norm": 5.871568202972412,
      "learning_rate": 4.0451455198227794e-05,
      "loss": 1.9848,
      "step": 100000
    },
    {
      "epoch": 7.646474677259186,
      "grad_norm": 6.180854797363281,
      "learning_rate": 4.044190665342602e-05,
      "loss": 1.9297,
      "step": 100100
    },
    {
      "epoch": 7.6541135131006035,
      "grad_norm": 6.05702018737793,
      "learning_rate": 4.0432358108624245e-05,
      "loss": 1.8946,
      "step": 100200
    },
    {
      "epoch": 7.661752348942021,
      "grad_norm": 7.784659385681152,
      "learning_rate": 4.042280956382248e-05,
      "loss": 1.9405,
      "step": 100300
    },
    {
      "epoch": 7.669391184783439,
      "grad_norm": 8.33116340637207,
      "learning_rate": 4.04132610190207e-05,
      "loss": 1.8977,
      "step": 100400
    },
    {
      "epoch": 7.677030020624857,
      "grad_norm": 8.386066436767578,
      "learning_rate": 4.040371247421893e-05,
      "loss": 1.9205,
      "step": 100500
    },
    {
      "epoch": 7.684668856466274,
      "grad_norm": 5.732582092285156,
      "learning_rate": 4.039416392941716e-05,
      "loss": 1.832,
      "step": 100600
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 6.9475274085998535,
      "learning_rate": 4.038461538461539e-05,
      "loss": 1.8131,
      "step": 100700
    },
    {
      "epoch": 7.69994652814911,
      "grad_norm": 6.400876045227051,
      "learning_rate": 4.037506683981361e-05,
      "loss": 1.9557,
      "step": 100800
    },
    {
      "epoch": 7.707585363990528,
      "grad_norm": 8.037620544433594,
      "learning_rate": 4.0365518295011846e-05,
      "loss": 1.9327,
      "step": 100900
    },
    {
      "epoch": 7.715224199831946,
      "grad_norm": 8.677791595458984,
      "learning_rate": 4.035596975021007e-05,
      "loss": 1.9038,
      "step": 101000
    },
    {
      "epoch": 7.722863035673363,
      "grad_norm": 7.136337757110596,
      "learning_rate": 4.0346421205408296e-05,
      "loss": 1.9671,
      "step": 101100
    },
    {
      "epoch": 7.7305018715147815,
      "grad_norm": 4.773012161254883,
      "learning_rate": 4.0336872660606525e-05,
      "loss": 1.8622,
      "step": 101200
    },
    {
      "epoch": 7.738140707356199,
      "grad_norm": 8.2261323928833,
      "learning_rate": 4.0327324115804754e-05,
      "loss": 1.864,
      "step": 101300
    },
    {
      "epoch": 7.745779543197616,
      "grad_norm": 7.265775680541992,
      "learning_rate": 4.031777557100298e-05,
      "loss": 1.9654,
      "step": 101400
    },
    {
      "epoch": 7.753418379039035,
      "grad_norm": 8.69002914428711,
      "learning_rate": 4.0308227026201204e-05,
      "loss": 1.8385,
      "step": 101500
    },
    {
      "epoch": 7.761057214880452,
      "grad_norm": 6.8413543701171875,
      "learning_rate": 4.029867848139944e-05,
      "loss": 1.8089,
      "step": 101600
    },
    {
      "epoch": 7.76869605072187,
      "grad_norm": 7.541243553161621,
      "learning_rate": 4.028912993659766e-05,
      "loss": 1.9456,
      "step": 101700
    },
    {
      "epoch": 7.776334886563288,
      "grad_norm": 9.413260459899902,
      "learning_rate": 4.027958139179589e-05,
      "loss": 1.8986,
      "step": 101800
    },
    {
      "epoch": 7.783973722404705,
      "grad_norm": 7.666357517242432,
      "learning_rate": 4.027003284699412e-05,
      "loss": 1.9329,
      "step": 101900
    },
    {
      "epoch": 7.791612558246124,
      "grad_norm": 8.372316360473633,
      "learning_rate": 4.026048430219235e-05,
      "loss": 1.8814,
      "step": 102000
    },
    {
      "epoch": 7.799251394087541,
      "grad_norm": 8.546683311462402,
      "learning_rate": 4.0250935757390577e-05,
      "loss": 1.8974,
      "step": 102100
    },
    {
      "epoch": 7.806890229928959,
      "grad_norm": 5.898068428039551,
      "learning_rate": 4.0241387212588805e-05,
      "loss": 1.9245,
      "step": 102200
    },
    {
      "epoch": 7.814529065770377,
      "grad_norm": 5.68485164642334,
      "learning_rate": 4.0231838667787034e-05,
      "loss": 1.9423,
      "step": 102300
    },
    {
      "epoch": 7.822167901611794,
      "grad_norm": 5.412017345428467,
      "learning_rate": 4.0222290122985256e-05,
      "loss": 1.9185,
      "step": 102400
    },
    {
      "epoch": 7.829806737453212,
      "grad_norm": 7.513479709625244,
      "learning_rate": 4.021274157818349e-05,
      "loss": 1.7846,
      "step": 102500
    },
    {
      "epoch": 7.83744557329463,
      "grad_norm": 6.964447975158691,
      "learning_rate": 4.020319303338171e-05,
      "loss": 1.9571,
      "step": 102600
    },
    {
      "epoch": 7.845084409136048,
      "grad_norm": 6.706334114074707,
      "learning_rate": 4.019364448857994e-05,
      "loss": 1.9725,
      "step": 102700
    },
    {
      "epoch": 7.852723244977465,
      "grad_norm": 7.611640453338623,
      "learning_rate": 4.018409594377817e-05,
      "loss": 1.8651,
      "step": 102800
    },
    {
      "epoch": 7.860362080818883,
      "grad_norm": 7.732966423034668,
      "learning_rate": 4.01745473989764e-05,
      "loss": 1.8359,
      "step": 102900
    },
    {
      "epoch": 7.868000916660301,
      "grad_norm": 6.5699872970581055,
      "learning_rate": 4.016499885417462e-05,
      "loss": 1.8462,
      "step": 103000
    },
    {
      "epoch": 7.875639752501719,
      "grad_norm": 7.856704235076904,
      "learning_rate": 4.015545030937286e-05,
      "loss": 1.8733,
      "step": 103100
    },
    {
      "epoch": 7.883278588343137,
      "grad_norm": 4.298622131347656,
      "learning_rate": 4.014590176457108e-05,
      "loss": 1.9769,
      "step": 103200
    },
    {
      "epoch": 7.890917424184554,
      "grad_norm": 8.297654151916504,
      "learning_rate": 4.013635321976931e-05,
      "loss": 1.8735,
      "step": 103300
    },
    {
      "epoch": 7.8985562600259716,
      "grad_norm": 5.989462375640869,
      "learning_rate": 4.0126804674967536e-05,
      "loss": 1.9757,
      "step": 103400
    },
    {
      "epoch": 7.90619509586739,
      "grad_norm": 7.762766361236572,
      "learning_rate": 4.0117256130165765e-05,
      "loss": 1.9552,
      "step": 103500
    },
    {
      "epoch": 7.913833931708807,
      "grad_norm": 6.12166690826416,
      "learning_rate": 4.010770758536399e-05,
      "loss": 1.9369,
      "step": 103600
    },
    {
      "epoch": 7.921472767550226,
      "grad_norm": 5.359522342681885,
      "learning_rate": 4.009815904056222e-05,
      "loss": 1.9337,
      "step": 103700
    },
    {
      "epoch": 7.929111603391643,
      "grad_norm": 6.5337300300598145,
      "learning_rate": 4.008861049576045e-05,
      "loss": 1.8817,
      "step": 103800
    },
    {
      "epoch": 7.936750439233061,
      "grad_norm": 6.515676498413086,
      "learning_rate": 4.007906195095867e-05,
      "loss": 1.8102,
      "step": 103900
    },
    {
      "epoch": 7.944389275074479,
      "grad_norm": 7.667769432067871,
      "learning_rate": 4.006951340615691e-05,
      "loss": 1.9503,
      "step": 104000
    },
    {
      "epoch": 7.952028110915896,
      "grad_norm": 7.604263782501221,
      "learning_rate": 4.005996486135513e-05,
      "loss": 1.894,
      "step": 104100
    },
    {
      "epoch": 7.959666946757315,
      "grad_norm": 7.073005199432373,
      "learning_rate": 4.005041631655336e-05,
      "loss": 2.0075,
      "step": 104200
    },
    {
      "epoch": 7.967305782598732,
      "grad_norm": 8.985316276550293,
      "learning_rate": 4.004086777175159e-05,
      "loss": 1.9329,
      "step": 104300
    },
    {
      "epoch": 7.97494461844015,
      "grad_norm": 7.686166763305664,
      "learning_rate": 4.0031319226949816e-05,
      "loss": 1.9738,
      "step": 104400
    },
    {
      "epoch": 7.982583454281567,
      "grad_norm": 7.622411251068115,
      "learning_rate": 4.002177068214804e-05,
      "loss": 1.9224,
      "step": 104500
    },
    {
      "epoch": 7.990222290122985,
      "grad_norm": 5.347498416900635,
      "learning_rate": 4.0012222137346274e-05,
      "loss": 1.9507,
      "step": 104600
    },
    {
      "epoch": 7.997861125964403,
      "grad_norm": 4.974489212036133,
      "learning_rate": 4.0002673592544495e-05,
      "loss": 1.9244,
      "step": 104700
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8740402460098267,
      "eval_runtime": 3.0618,
      "eval_samples_per_second": 225.358,
      "eval_steps_per_second": 225.358,
      "step": 104728
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.6848863363265991,
      "eval_runtime": 58.1864,
      "eval_samples_per_second": 224.984,
      "eval_steps_per_second": 224.984,
      "step": 104728
    },
    {
      "epoch": 8.005499961805821,
      "grad_norm": 7.3907670974731445,
      "learning_rate": 3.9993125047742724e-05,
      "loss": 1.9201,
      "step": 104800
    },
    {
      "epoch": 8.013138797647239,
      "grad_norm": 6.791952610015869,
      "learning_rate": 3.998357650294095e-05,
      "loss": 1.8614,
      "step": 104900
    },
    {
      "epoch": 8.020777633488656,
      "grad_norm": 5.657793045043945,
      "learning_rate": 3.997402795813918e-05,
      "loss": 1.854,
      "step": 105000
    },
    {
      "epoch": 8.028416469330073,
      "grad_norm": 6.661211013793945,
      "learning_rate": 3.996447941333741e-05,
      "loss": 1.8711,
      "step": 105100
    },
    {
      "epoch": 8.036055305171493,
      "grad_norm": 6.627518177032471,
      "learning_rate": 3.995493086853564e-05,
      "loss": 1.8808,
      "step": 105200
    },
    {
      "epoch": 8.04369414101291,
      "grad_norm": 7.4173808097839355,
      "learning_rate": 3.994538232373387e-05,
      "loss": 1.9262,
      "step": 105300
    },
    {
      "epoch": 8.051332976854328,
      "grad_norm": 5.119053840637207,
      "learning_rate": 3.993583377893209e-05,
      "loss": 1.915,
      "step": 105400
    },
    {
      "epoch": 8.058971812695745,
      "grad_norm": 5.93503475189209,
      "learning_rate": 3.9926285234130325e-05,
      "loss": 1.9438,
      "step": 105500
    },
    {
      "epoch": 8.066610648537162,
      "grad_norm": 3.209401845932007,
      "learning_rate": 3.991673668932855e-05,
      "loss": 1.8717,
      "step": 105600
    },
    {
      "epoch": 8.07424948437858,
      "grad_norm": 7.119684219360352,
      "learning_rate": 3.9907188144526776e-05,
      "loss": 1.9568,
      "step": 105700
    },
    {
      "epoch": 8.08188832022,
      "grad_norm": 7.428574085235596,
      "learning_rate": 3.9897639599725004e-05,
      "loss": 1.8568,
      "step": 105800
    },
    {
      "epoch": 8.089527156061417,
      "grad_norm": 6.651711940765381,
      "learning_rate": 3.988809105492323e-05,
      "loss": 1.8542,
      "step": 105900
    },
    {
      "epoch": 8.097165991902834,
      "grad_norm": 7.854244232177734,
      "learning_rate": 3.9878542510121455e-05,
      "loss": 1.8829,
      "step": 106000
    },
    {
      "epoch": 8.104804827744251,
      "grad_norm": 7.359543323516846,
      "learning_rate": 3.986899396531969e-05,
      "loss": 1.9042,
      "step": 106100
    },
    {
      "epoch": 8.112443663585669,
      "grad_norm": 6.333889007568359,
      "learning_rate": 3.985944542051792e-05,
      "loss": 1.853,
      "step": 106200
    },
    {
      "epoch": 8.120082499427088,
      "grad_norm": 6.736392021179199,
      "learning_rate": 3.984989687571614e-05,
      "loss": 1.8664,
      "step": 106300
    },
    {
      "epoch": 8.127721335268506,
      "grad_norm": 6.361771583557129,
      "learning_rate": 3.9840348330914376e-05,
      "loss": 1.8389,
      "step": 106400
    },
    {
      "epoch": 8.135360171109923,
      "grad_norm": 9.79892635345459,
      "learning_rate": 3.98307997861126e-05,
      "loss": 1.8046,
      "step": 106500
    },
    {
      "epoch": 8.14299900695134,
      "grad_norm": 8.218104362487793,
      "learning_rate": 3.982125124131083e-05,
      "loss": 1.7747,
      "step": 106600
    },
    {
      "epoch": 8.150637842792758,
      "grad_norm": 7.302126407623291,
      "learning_rate": 3.981170269650905e-05,
      "loss": 1.9796,
      "step": 106700
    },
    {
      "epoch": 8.158276678634175,
      "grad_norm": 5.342555999755859,
      "learning_rate": 3.9802154151707284e-05,
      "loss": 1.9677,
      "step": 106800
    },
    {
      "epoch": 8.165915514475595,
      "grad_norm": 6.39257287979126,
      "learning_rate": 3.9792605606905506e-05,
      "loss": 1.8577,
      "step": 106900
    },
    {
      "epoch": 8.173554350317012,
      "grad_norm": 8.983709335327148,
      "learning_rate": 3.9783057062103735e-05,
      "loss": 1.9912,
      "step": 107000
    },
    {
      "epoch": 8.18119318615843,
      "grad_norm": 7.216982841491699,
      "learning_rate": 3.9773508517301964e-05,
      "loss": 1.8629,
      "step": 107100
    },
    {
      "epoch": 8.188832021999847,
      "grad_norm": 10.208097457885742,
      "learning_rate": 3.976395997250019e-05,
      "loss": 1.8556,
      "step": 107200
    },
    {
      "epoch": 8.196470857841264,
      "grad_norm": 6.35841703414917,
      "learning_rate": 3.975441142769842e-05,
      "loss": 1.8756,
      "step": 107300
    },
    {
      "epoch": 8.204109693682684,
      "grad_norm": 7.2651777267456055,
      "learning_rate": 3.974486288289665e-05,
      "loss": 1.9212,
      "step": 107400
    },
    {
      "epoch": 8.211748529524101,
      "grad_norm": 6.866548538208008,
      "learning_rate": 3.973531433809488e-05,
      "loss": 1.871,
      "step": 107500
    },
    {
      "epoch": 8.219387365365519,
      "grad_norm": 6.816832065582275,
      "learning_rate": 3.97257657932931e-05,
      "loss": 1.9264,
      "step": 107600
    },
    {
      "epoch": 8.227026201206936,
      "grad_norm": 7.62563943862915,
      "learning_rate": 3.9716217248491336e-05,
      "loss": 1.9088,
      "step": 107700
    },
    {
      "epoch": 8.234665037048353,
      "grad_norm": 9.884265899658203,
      "learning_rate": 3.970666870368956e-05,
      "loss": 1.9152,
      "step": 107800
    },
    {
      "epoch": 8.24230387288977,
      "grad_norm": 7.094790935516357,
      "learning_rate": 3.9697120158887787e-05,
      "loss": 1.8622,
      "step": 107900
    },
    {
      "epoch": 8.24994270873119,
      "grad_norm": 6.558166027069092,
      "learning_rate": 3.9687571614086015e-05,
      "loss": 1.9326,
      "step": 108000
    },
    {
      "epoch": 8.257581544572608,
      "grad_norm": 6.865811347961426,
      "learning_rate": 3.9678023069284244e-05,
      "loss": 1.8545,
      "step": 108100
    },
    {
      "epoch": 8.265220380414025,
      "grad_norm": 8.12035083770752,
      "learning_rate": 3.9668474524482466e-05,
      "loss": 1.8344,
      "step": 108200
    },
    {
      "epoch": 8.272859216255442,
      "grad_norm": 7.868272304534912,
      "learning_rate": 3.96589259796807e-05,
      "loss": 1.8378,
      "step": 108300
    },
    {
      "epoch": 8.28049805209686,
      "grad_norm": 7.348315715789795,
      "learning_rate": 3.964937743487892e-05,
      "loss": 1.929,
      "step": 108400
    },
    {
      "epoch": 8.288136887938279,
      "grad_norm": 8.818187713623047,
      "learning_rate": 3.963982889007715e-05,
      "loss": 1.9339,
      "step": 108500
    },
    {
      "epoch": 8.295775723779697,
      "grad_norm": 8.826532363891602,
      "learning_rate": 3.963028034527538e-05,
      "loss": 1.958,
      "step": 108600
    },
    {
      "epoch": 8.303414559621114,
      "grad_norm": 9.114503860473633,
      "learning_rate": 3.962073180047361e-05,
      "loss": 1.8071,
      "step": 108700
    },
    {
      "epoch": 8.311053395462531,
      "grad_norm": 6.113762855529785,
      "learning_rate": 3.961118325567184e-05,
      "loss": 1.9154,
      "step": 108800
    },
    {
      "epoch": 8.318692231303949,
      "grad_norm": 5.867588043212891,
      "learning_rate": 3.960163471087007e-05,
      "loss": 2.0005,
      "step": 108900
    },
    {
      "epoch": 8.326331067145366,
      "grad_norm": 6.136462211608887,
      "learning_rate": 3.9592086166068295e-05,
      "loss": 1.9744,
      "step": 109000
    },
    {
      "epoch": 8.333969902986786,
      "grad_norm": 7.160208225250244,
      "learning_rate": 3.958253762126652e-05,
      "loss": 1.9355,
      "step": 109100
    },
    {
      "epoch": 8.341608738828203,
      "grad_norm": 9.849566459655762,
      "learning_rate": 3.957298907646475e-05,
      "loss": 1.8866,
      "step": 109200
    },
    {
      "epoch": 8.34924757466962,
      "grad_norm": 6.370260715484619,
      "learning_rate": 3.9563440531662975e-05,
      "loss": 1.8334,
      "step": 109300
    },
    {
      "epoch": 8.356886410511038,
      "grad_norm": 6.12431001663208,
      "learning_rate": 3.95538919868612e-05,
      "loss": 1.8979,
      "step": 109400
    },
    {
      "epoch": 8.364525246352455,
      "grad_norm": 8.905058860778809,
      "learning_rate": 3.954434344205943e-05,
      "loss": 1.7918,
      "step": 109500
    },
    {
      "epoch": 8.372164082193873,
      "grad_norm": 6.47322416305542,
      "learning_rate": 3.953479489725766e-05,
      "loss": 1.9078,
      "step": 109600
    },
    {
      "epoch": 8.379802918035292,
      "grad_norm": 7.281745910644531,
      "learning_rate": 3.952524635245588e-05,
      "loss": 1.9862,
      "step": 109700
    },
    {
      "epoch": 8.38744175387671,
      "grad_norm": 6.587599754333496,
      "learning_rate": 3.951569780765412e-05,
      "loss": 1.8953,
      "step": 109800
    },
    {
      "epoch": 8.395080589718127,
      "grad_norm": 7.781975269317627,
      "learning_rate": 3.950614926285234e-05,
      "loss": 1.8105,
      "step": 109900
    },
    {
      "epoch": 8.402719425559544,
      "grad_norm": 7.487137794494629,
      "learning_rate": 3.949660071805057e-05,
      "loss": 1.877,
      "step": 110000
    },
    {
      "epoch": 8.410358261400962,
      "grad_norm": 6.340487957000732,
      "learning_rate": 3.94870521732488e-05,
      "loss": 1.9517,
      "step": 110100
    },
    {
      "epoch": 8.417997097242381,
      "grad_norm": 7.213089942932129,
      "learning_rate": 3.9477503628447026e-05,
      "loss": 1.8611,
      "step": 110200
    },
    {
      "epoch": 8.425635933083798,
      "grad_norm": 5.665067672729492,
      "learning_rate": 3.9467955083645255e-05,
      "loss": 1.9275,
      "step": 110300
    },
    {
      "epoch": 8.433274768925216,
      "grad_norm": 6.735391616821289,
      "learning_rate": 3.9458406538843484e-05,
      "loss": 1.9364,
      "step": 110400
    },
    {
      "epoch": 8.440913604766633,
      "grad_norm": 6.6625165939331055,
      "learning_rate": 3.944885799404171e-05,
      "loss": 1.9108,
      "step": 110500
    },
    {
      "epoch": 8.44855244060805,
      "grad_norm": 7.028385162353516,
      "learning_rate": 3.9439309449239934e-05,
      "loss": 1.9646,
      "step": 110600
    },
    {
      "epoch": 8.45619127644947,
      "grad_norm": 9.916251182556152,
      "learning_rate": 3.942976090443817e-05,
      "loss": 1.8526,
      "step": 110700
    },
    {
      "epoch": 8.463830112290887,
      "grad_norm": 7.446462631225586,
      "learning_rate": 3.942021235963639e-05,
      "loss": 1.8819,
      "step": 110800
    },
    {
      "epoch": 8.471468948132305,
      "grad_norm": 7.576835632324219,
      "learning_rate": 3.941066381483462e-05,
      "loss": 1.9291,
      "step": 110900
    },
    {
      "epoch": 8.479107783973722,
      "grad_norm": 6.80585241317749,
      "learning_rate": 3.940111527003285e-05,
      "loss": 1.9482,
      "step": 111000
    },
    {
      "epoch": 8.48674661981514,
      "grad_norm": 8.397109985351562,
      "learning_rate": 3.939156672523108e-05,
      "loss": 1.9299,
      "step": 111100
    },
    {
      "epoch": 8.494385455656557,
      "grad_norm": 6.896072864532471,
      "learning_rate": 3.9382018180429306e-05,
      "loss": 1.8515,
      "step": 111200
    },
    {
      "epoch": 8.502024291497976,
      "grad_norm": 6.7842206954956055,
      "learning_rate": 3.9372469635627535e-05,
      "loss": 1.8498,
      "step": 111300
    },
    {
      "epoch": 8.509663127339394,
      "grad_norm": 6.4165449142456055,
      "learning_rate": 3.9362921090825764e-05,
      "loss": 1.9243,
      "step": 111400
    },
    {
      "epoch": 8.517301963180811,
      "grad_norm": 6.893211841583252,
      "learning_rate": 3.9353372546023986e-05,
      "loss": 1.8055,
      "step": 111500
    },
    {
      "epoch": 8.524940799022229,
      "grad_norm": 8.69597053527832,
      "learning_rate": 3.934382400122222e-05,
      "loss": 1.9339,
      "step": 111600
    },
    {
      "epoch": 8.532579634863646,
      "grad_norm": 6.884964942932129,
      "learning_rate": 3.933427545642044e-05,
      "loss": 1.8557,
      "step": 111700
    },
    {
      "epoch": 8.540218470705064,
      "grad_norm": 7.719566345214844,
      "learning_rate": 3.932472691161867e-05,
      "loss": 1.8688,
      "step": 111800
    },
    {
      "epoch": 8.547857306546483,
      "grad_norm": 5.11598014831543,
      "learning_rate": 3.93151783668169e-05,
      "loss": 1.9059,
      "step": 111900
    },
    {
      "epoch": 8.5554961423879,
      "grad_norm": 6.1615400314331055,
      "learning_rate": 3.930562982201513e-05,
      "loss": 1.8533,
      "step": 112000
    },
    {
      "epoch": 8.563134978229318,
      "grad_norm": 7.095123767852783,
      "learning_rate": 3.929608127721335e-05,
      "loss": 1.9158,
      "step": 112100
    },
    {
      "epoch": 8.570773814070735,
      "grad_norm": 11.11430549621582,
      "learning_rate": 3.9286532732411586e-05,
      "loss": 1.9522,
      "step": 112200
    },
    {
      "epoch": 8.578412649912153,
      "grad_norm": 5.468932628631592,
      "learning_rate": 3.927698418760981e-05,
      "loss": 1.9004,
      "step": 112300
    },
    {
      "epoch": 8.586051485753572,
      "grad_norm": 6.322302341461182,
      "learning_rate": 3.926743564280804e-05,
      "loss": 1.9001,
      "step": 112400
    },
    {
      "epoch": 8.59369032159499,
      "grad_norm": 7.594686508178711,
      "learning_rate": 3.9257887098006266e-05,
      "loss": 1.8404,
      "step": 112500
    },
    {
      "epoch": 8.601329157436407,
      "grad_norm": 6.646020412445068,
      "learning_rate": 3.9248338553204494e-05,
      "loss": 1.9015,
      "step": 112600
    },
    {
      "epoch": 8.608967993277824,
      "grad_norm": 9.000675201416016,
      "learning_rate": 3.923879000840272e-05,
      "loss": 1.822,
      "step": 112700
    },
    {
      "epoch": 8.616606829119242,
      "grad_norm": 8.228757858276367,
      "learning_rate": 3.9229241463600945e-05,
      "loss": 1.8488,
      "step": 112800
    },
    {
      "epoch": 8.624245664960661,
      "grad_norm": 8.474183082580566,
      "learning_rate": 3.921969291879918e-05,
      "loss": 2.0328,
      "step": 112900
    },
    {
      "epoch": 8.631884500802078,
      "grad_norm": 5.269909858703613,
      "learning_rate": 3.92101443739974e-05,
      "loss": 1.8034,
      "step": 113000
    },
    {
      "epoch": 8.639523336643496,
      "grad_norm": 8.857209205627441,
      "learning_rate": 3.920059582919563e-05,
      "loss": 1.9153,
      "step": 113100
    },
    {
      "epoch": 8.647162172484913,
      "grad_norm": 5.47458553314209,
      "learning_rate": 3.919104728439386e-05,
      "loss": 1.8601,
      "step": 113200
    },
    {
      "epoch": 8.65480100832633,
      "grad_norm": 10.326153755187988,
      "learning_rate": 3.918149873959209e-05,
      "loss": 1.9446,
      "step": 113300
    },
    {
      "epoch": 8.662439844167748,
      "grad_norm": 5.72794771194458,
      "learning_rate": 3.917195019479031e-05,
      "loss": 1.9336,
      "step": 113400
    },
    {
      "epoch": 8.670078680009167,
      "grad_norm": 8.302186965942383,
      "learning_rate": 3.9162401649988546e-05,
      "loss": 1.9507,
      "step": 113500
    },
    {
      "epoch": 8.677717515850585,
      "grad_norm": 7.666833877563477,
      "learning_rate": 3.915285310518677e-05,
      "loss": 1.9545,
      "step": 113600
    },
    {
      "epoch": 8.685356351692002,
      "grad_norm": 7.432289123535156,
      "learning_rate": 3.9143304560384997e-05,
      "loss": 1.9562,
      "step": 113700
    },
    {
      "epoch": 8.69299518753342,
      "grad_norm": 8.397261619567871,
      "learning_rate": 3.9133756015583225e-05,
      "loss": 1.8765,
      "step": 113800
    },
    {
      "epoch": 8.700634023374837,
      "grad_norm": 9.030607223510742,
      "learning_rate": 3.9124207470781454e-05,
      "loss": 1.8745,
      "step": 113900
    },
    {
      "epoch": 8.708272859216255,
      "grad_norm": 5.546651363372803,
      "learning_rate": 3.911465892597968e-05,
      "loss": 2.004,
      "step": 114000
    },
    {
      "epoch": 8.715911695057674,
      "grad_norm": 7.382510185241699,
      "learning_rate": 3.910511038117791e-05,
      "loss": 1.9403,
      "step": 114100
    },
    {
      "epoch": 8.723550530899091,
      "grad_norm": 6.479975700378418,
      "learning_rate": 3.909556183637614e-05,
      "loss": 1.8881,
      "step": 114200
    },
    {
      "epoch": 8.731189366740509,
      "grad_norm": 8.03995132446289,
      "learning_rate": 3.908601329157436e-05,
      "loss": 1.9251,
      "step": 114300
    },
    {
      "epoch": 8.738828202581926,
      "grad_norm": 7.698390007019043,
      "learning_rate": 3.90764647467726e-05,
      "loss": 1.8886,
      "step": 114400
    },
    {
      "epoch": 8.746467038423344,
      "grad_norm": 7.368738174438477,
      "learning_rate": 3.906691620197082e-05,
      "loss": 1.846,
      "step": 114500
    },
    {
      "epoch": 8.754105874264763,
      "grad_norm": 6.531544208526611,
      "learning_rate": 3.905736765716905e-05,
      "loss": 1.8791,
      "step": 114600
    },
    {
      "epoch": 8.76174471010618,
      "grad_norm": 6.104403495788574,
      "learning_rate": 3.904781911236728e-05,
      "loss": 1.8486,
      "step": 114700
    },
    {
      "epoch": 8.769383545947598,
      "grad_norm": 7.05229377746582,
      "learning_rate": 3.9038270567565505e-05,
      "loss": 1.8618,
      "step": 114800
    },
    {
      "epoch": 8.777022381789015,
      "grad_norm": 6.806093215942383,
      "learning_rate": 3.9028722022763734e-05,
      "loss": 1.8588,
      "step": 114900
    },
    {
      "epoch": 8.784661217630433,
      "grad_norm": 7.4027791023254395,
      "learning_rate": 3.901917347796196e-05,
      "loss": 1.7898,
      "step": 115000
    },
    {
      "epoch": 8.79230005347185,
      "grad_norm": 8.029460906982422,
      "learning_rate": 3.900962493316019e-05,
      "loss": 1.8693,
      "step": 115100
    },
    {
      "epoch": 8.79993888931327,
      "grad_norm": 7.686296463012695,
      "learning_rate": 3.900007638835841e-05,
      "loss": 1.874,
      "step": 115200
    },
    {
      "epoch": 8.807577725154687,
      "grad_norm": 6.934915542602539,
      "learning_rate": 3.899052784355665e-05,
      "loss": 1.8864,
      "step": 115300
    },
    {
      "epoch": 8.815216560996104,
      "grad_norm": 6.947837829589844,
      "learning_rate": 3.898097929875487e-05,
      "loss": 1.9323,
      "step": 115400
    },
    {
      "epoch": 8.822855396837522,
      "grad_norm": 7.605008602142334,
      "learning_rate": 3.89714307539531e-05,
      "loss": 1.8685,
      "step": 115500
    },
    {
      "epoch": 8.830494232678939,
      "grad_norm": 5.987123012542725,
      "learning_rate": 3.896188220915133e-05,
      "loss": 1.8467,
      "step": 115600
    },
    {
      "epoch": 8.838133068520358,
      "grad_norm": 4.49821662902832,
      "learning_rate": 3.895233366434956e-05,
      "loss": 1.8969,
      "step": 115700
    },
    {
      "epoch": 8.845771904361776,
      "grad_norm": 6.457118034362793,
      "learning_rate": 3.894278511954778e-05,
      "loss": 1.8798,
      "step": 115800
    },
    {
      "epoch": 8.853410740203193,
      "grad_norm": 8.133734703063965,
      "learning_rate": 3.8933236574746014e-05,
      "loss": 1.9292,
      "step": 115900
    },
    {
      "epoch": 8.86104957604461,
      "grad_norm": 6.973058223724365,
      "learning_rate": 3.8923688029944236e-05,
      "loss": 1.9438,
      "step": 116000
    },
    {
      "epoch": 8.868688411886028,
      "grad_norm": 5.9709649085998535,
      "learning_rate": 3.8914139485142465e-05,
      "loss": 1.8853,
      "step": 116100
    },
    {
      "epoch": 8.876327247727446,
      "grad_norm": 9.76035213470459,
      "learning_rate": 3.8904590940340694e-05,
      "loss": 1.85,
      "step": 116200
    },
    {
      "epoch": 8.883966083568865,
      "grad_norm": 6.897641658782959,
      "learning_rate": 3.889504239553892e-05,
      "loss": 1.8787,
      "step": 116300
    },
    {
      "epoch": 8.891604919410282,
      "grad_norm": 6.446702003479004,
      "learning_rate": 3.888549385073715e-05,
      "loss": 1.8675,
      "step": 116400
    },
    {
      "epoch": 8.8992437552517,
      "grad_norm": 5.5054731369018555,
      "learning_rate": 3.887594530593538e-05,
      "loss": 1.8786,
      "step": 116500
    },
    {
      "epoch": 8.906882591093117,
      "grad_norm": 7.537604808807373,
      "learning_rate": 3.886639676113361e-05,
      "loss": 1.9299,
      "step": 116600
    },
    {
      "epoch": 8.914521426934535,
      "grad_norm": 7.923770904541016,
      "learning_rate": 3.885684821633183e-05,
      "loss": 1.9001,
      "step": 116700
    },
    {
      "epoch": 8.922160262775954,
      "grad_norm": 6.256862640380859,
      "learning_rate": 3.8847299671530066e-05,
      "loss": 1.8088,
      "step": 116800
    },
    {
      "epoch": 8.929799098617371,
      "grad_norm": 7.23984956741333,
      "learning_rate": 3.883775112672829e-05,
      "loss": 1.8998,
      "step": 116900
    },
    {
      "epoch": 8.937437934458789,
      "grad_norm": 8.56287956237793,
      "learning_rate": 3.8828202581926516e-05,
      "loss": 1.8393,
      "step": 117000
    },
    {
      "epoch": 8.945076770300206,
      "grad_norm": 6.151912212371826,
      "learning_rate": 3.8818654037124745e-05,
      "loss": 1.9055,
      "step": 117100
    },
    {
      "epoch": 8.952715606141624,
      "grad_norm": 6.146624565124512,
      "learning_rate": 3.8809105492322974e-05,
      "loss": 1.7918,
      "step": 117200
    },
    {
      "epoch": 8.960354441983041,
      "grad_norm": 6.025155544281006,
      "learning_rate": 3.8799556947521196e-05,
      "loss": 1.8701,
      "step": 117300
    },
    {
      "epoch": 8.96799327782446,
      "grad_norm": 4.958206653594971,
      "learning_rate": 3.879000840271943e-05,
      "loss": 1.7928,
      "step": 117400
    },
    {
      "epoch": 8.975632113665878,
      "grad_norm": 5.605995178222656,
      "learning_rate": 3.878045985791765e-05,
      "loss": 1.9038,
      "step": 117500
    },
    {
      "epoch": 8.983270949507295,
      "grad_norm": 9.76594066619873,
      "learning_rate": 3.877091131311588e-05,
      "loss": 1.8753,
      "step": 117600
    },
    {
      "epoch": 8.990909785348713,
      "grad_norm": 7.737526893615723,
      "learning_rate": 3.876136276831411e-05,
      "loss": 1.892,
      "step": 117700
    },
    {
      "epoch": 8.99854862119013,
      "grad_norm": 6.636022090911865,
      "learning_rate": 3.875181422351234e-05,
      "loss": 1.882,
      "step": 117800
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8690009117126465,
      "eval_runtime": 3.0008,
      "eval_samples_per_second": 229.937,
      "eval_steps_per_second": 229.937,
      "step": 117819
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.6703754663467407,
      "eval_runtime": 56.5542,
      "eval_samples_per_second": 231.477,
      "eval_steps_per_second": 231.477,
      "step": 117819
    },
    {
      "epoch": 9.00618745703155,
      "grad_norm": 6.747211933135986,
      "learning_rate": 3.874226567871057e-05,
      "loss": 1.7932,
      "step": 117900
    },
    {
      "epoch": 9.013826292872967,
      "grad_norm": 5.9481682777404785,
      "learning_rate": 3.8732717133908796e-05,
      "loss": 1.9173,
      "step": 118000
    },
    {
      "epoch": 9.021465128714384,
      "grad_norm": 7.05121374130249,
      "learning_rate": 3.8723168589107025e-05,
      "loss": 1.8036,
      "step": 118100
    },
    {
      "epoch": 9.029103964555802,
      "grad_norm": 8.426045417785645,
      "learning_rate": 3.871362004430525e-05,
      "loss": 1.8932,
      "step": 118200
    },
    {
      "epoch": 9.036742800397219,
      "grad_norm": 6.716715335845947,
      "learning_rate": 3.8704071499503476e-05,
      "loss": 1.8316,
      "step": 118300
    },
    {
      "epoch": 9.044381636238636,
      "grad_norm": 6.4266886711120605,
      "learning_rate": 3.8694522954701704e-05,
      "loss": 1.7899,
      "step": 118400
    },
    {
      "epoch": 9.052020472080056,
      "grad_norm": 6.269611358642578,
      "learning_rate": 3.868497440989993e-05,
      "loss": 1.8958,
      "step": 118500
    },
    {
      "epoch": 9.059659307921473,
      "grad_norm": 7.0102410316467285,
      "learning_rate": 3.8675425865098155e-05,
      "loss": 1.8691,
      "step": 118600
    },
    {
      "epoch": 9.06729814376289,
      "grad_norm": 8.516416549682617,
      "learning_rate": 3.866587732029639e-05,
      "loss": 1.9311,
      "step": 118700
    },
    {
      "epoch": 9.074936979604308,
      "grad_norm": 6.952121734619141,
      "learning_rate": 3.865632877549461e-05,
      "loss": 1.7745,
      "step": 118800
    },
    {
      "epoch": 9.082575815445725,
      "grad_norm": 6.414597034454346,
      "learning_rate": 3.864678023069284e-05,
      "loss": 2.0035,
      "step": 118900
    },
    {
      "epoch": 9.090214651287145,
      "grad_norm": 5.515068054199219,
      "learning_rate": 3.8637231685891077e-05,
      "loss": 1.7935,
      "step": 119000
    },
    {
      "epoch": 9.097853487128562,
      "grad_norm": 7.817251205444336,
      "learning_rate": 3.86276831410893e-05,
      "loss": 1.9066,
      "step": 119100
    },
    {
      "epoch": 9.10549232296998,
      "grad_norm": 5.996766090393066,
      "learning_rate": 3.861813459628753e-05,
      "loss": 1.8912,
      "step": 119200
    },
    {
      "epoch": 9.113131158811397,
      "grad_norm": 6.388099670410156,
      "learning_rate": 3.8608586051485756e-05,
      "loss": 1.8733,
      "step": 119300
    },
    {
      "epoch": 9.120769994652814,
      "grad_norm": 6.855689525604248,
      "learning_rate": 3.8599037506683985e-05,
      "loss": 1.8757,
      "step": 119400
    },
    {
      "epoch": 9.128408830494232,
      "grad_norm": 7.06190299987793,
      "learning_rate": 3.8589488961882207e-05,
      "loss": 1.8188,
      "step": 119500
    },
    {
      "epoch": 9.136047666335651,
      "grad_norm": 8.225825309753418,
      "learning_rate": 3.857994041708044e-05,
      "loss": 1.821,
      "step": 119600
    },
    {
      "epoch": 9.143686502177069,
      "grad_norm": 9.17504596710205,
      "learning_rate": 3.8570391872278664e-05,
      "loss": 1.8251,
      "step": 119700
    },
    {
      "epoch": 9.151325338018486,
      "grad_norm": 5.024287223815918,
      "learning_rate": 3.856084332747689e-05,
      "loss": 1.7796,
      "step": 119800
    },
    {
      "epoch": 9.158964173859903,
      "grad_norm": 5.596312999725342,
      "learning_rate": 3.855129478267512e-05,
      "loss": 1.9084,
      "step": 119900
    },
    {
      "epoch": 9.166603009701321,
      "grad_norm": 7.062551975250244,
      "learning_rate": 3.854174623787335e-05,
      "loss": 1.9745,
      "step": 120000
    },
    {
      "epoch": 9.17424184554274,
      "grad_norm": 7.4750075340271,
      "learning_rate": 3.853219769307158e-05,
      "loss": 1.8248,
      "step": 120100
    },
    {
      "epoch": 9.181880681384158,
      "grad_norm": 8.929186820983887,
      "learning_rate": 3.852264914826981e-05,
      "loss": 1.7361,
      "step": 120200
    },
    {
      "epoch": 9.189519517225575,
      "grad_norm": 6.495011329650879,
      "learning_rate": 3.8513100603468036e-05,
      "loss": 1.862,
      "step": 120300
    },
    {
      "epoch": 9.197158353066992,
      "grad_norm": 8.399240493774414,
      "learning_rate": 3.850355205866626e-05,
      "loss": 1.9057,
      "step": 120400
    },
    {
      "epoch": 9.20479718890841,
      "grad_norm": 6.776754379272461,
      "learning_rate": 3.8494003513864493e-05,
      "loss": 1.8252,
      "step": 120500
    },
    {
      "epoch": 9.212436024749827,
      "grad_norm": 6.055707931518555,
      "learning_rate": 3.8484454969062715e-05,
      "loss": 1.9124,
      "step": 120600
    },
    {
      "epoch": 9.220074860591247,
      "grad_norm": 6.4599127769470215,
      "learning_rate": 3.8474906424260944e-05,
      "loss": 1.8385,
      "step": 120700
    },
    {
      "epoch": 9.227713696432664,
      "grad_norm": 9.4035005569458,
      "learning_rate": 3.846535787945917e-05,
      "loss": 1.8397,
      "step": 120800
    },
    {
      "epoch": 9.235352532274081,
      "grad_norm": 5.84060001373291,
      "learning_rate": 3.84558093346574e-05,
      "loss": 1.8037,
      "step": 120900
    },
    {
      "epoch": 9.242991368115499,
      "grad_norm": 5.818294048309326,
      "learning_rate": 3.844626078985562e-05,
      "loss": 1.8945,
      "step": 121000
    },
    {
      "epoch": 9.250630203956916,
      "grad_norm": 7.6006011962890625,
      "learning_rate": 3.843671224505386e-05,
      "loss": 1.8439,
      "step": 121100
    },
    {
      "epoch": 9.258269039798336,
      "grad_norm": 6.071735382080078,
      "learning_rate": 3.842716370025208e-05,
      "loss": 1.8432,
      "step": 121200
    },
    {
      "epoch": 9.265907875639753,
      "grad_norm": 6.131033420562744,
      "learning_rate": 3.841761515545031e-05,
      "loss": 1.9166,
      "step": 121300
    },
    {
      "epoch": 9.27354671148117,
      "grad_norm": 9.130270004272461,
      "learning_rate": 3.840806661064854e-05,
      "loss": 1.8481,
      "step": 121400
    },
    {
      "epoch": 9.281185547322588,
      "grad_norm": 6.630194187164307,
      "learning_rate": 3.839851806584677e-05,
      "loss": 1.8529,
      "step": 121500
    },
    {
      "epoch": 9.288824383164005,
      "grad_norm": 6.643918514251709,
      "learning_rate": 3.8388969521044996e-05,
      "loss": 1.9393,
      "step": 121600
    },
    {
      "epoch": 9.296463219005423,
      "grad_norm": 6.630173683166504,
      "learning_rate": 3.8379420976243224e-05,
      "loss": 1.795,
      "step": 121700
    },
    {
      "epoch": 9.304102054846842,
      "grad_norm": 4.87973690032959,
      "learning_rate": 3.836987243144145e-05,
      "loss": 1.9228,
      "step": 121800
    },
    {
      "epoch": 9.31174089068826,
      "grad_norm": 9.39721393585205,
      "learning_rate": 3.8360323886639675e-05,
      "loss": 1.8776,
      "step": 121900
    },
    {
      "epoch": 9.319379726529677,
      "grad_norm": 5.941691875457764,
      "learning_rate": 3.835077534183791e-05,
      "loss": 1.7931,
      "step": 122000
    },
    {
      "epoch": 9.327018562371094,
      "grad_norm": 7.64035701751709,
      "learning_rate": 3.834122679703613e-05,
      "loss": 1.8968,
      "step": 122100
    },
    {
      "epoch": 9.334657398212512,
      "grad_norm": 6.646050930023193,
      "learning_rate": 3.833167825223436e-05,
      "loss": 1.7941,
      "step": 122200
    },
    {
      "epoch": 9.34229623405393,
      "grad_norm": 7.750432014465332,
      "learning_rate": 3.832212970743259e-05,
      "loss": 1.9,
      "step": 122300
    },
    {
      "epoch": 9.349935069895349,
      "grad_norm": 6.744895935058594,
      "learning_rate": 3.831258116263082e-05,
      "loss": 1.9621,
      "step": 122400
    },
    {
      "epoch": 9.357573905736766,
      "grad_norm": 7.4470319747924805,
      "learning_rate": 3.830303261782904e-05,
      "loss": 1.9145,
      "step": 122500
    },
    {
      "epoch": 9.365212741578183,
      "grad_norm": 8.337971687316895,
      "learning_rate": 3.8293484073027276e-05,
      "loss": 1.9717,
      "step": 122600
    },
    {
      "epoch": 9.3728515774196,
      "grad_norm": 6.6585774421691895,
      "learning_rate": 3.82839355282255e-05,
      "loss": 1.9382,
      "step": 122700
    },
    {
      "epoch": 9.380490413261018,
      "grad_norm": 7.737183570861816,
      "learning_rate": 3.8274386983423726e-05,
      "loss": 1.9135,
      "step": 122800
    },
    {
      "epoch": 9.388129249102438,
      "grad_norm": 8.179949760437012,
      "learning_rate": 3.8264838438621955e-05,
      "loss": 1.975,
      "step": 122900
    },
    {
      "epoch": 9.395768084943855,
      "grad_norm": 7.117197513580322,
      "learning_rate": 3.8255289893820184e-05,
      "loss": 1.8145,
      "step": 123000
    },
    {
      "epoch": 9.403406920785272,
      "grad_norm": 7.18269681930542,
      "learning_rate": 3.824574134901841e-05,
      "loss": 1.8385,
      "step": 123100
    },
    {
      "epoch": 9.41104575662669,
      "grad_norm": 4.6340227127075195,
      "learning_rate": 3.823619280421664e-05,
      "loss": 1.8802,
      "step": 123200
    },
    {
      "epoch": 9.418684592468107,
      "grad_norm": 7.850452423095703,
      "learning_rate": 3.822664425941487e-05,
      "loss": 1.8721,
      "step": 123300
    },
    {
      "epoch": 9.426323428309527,
      "grad_norm": 7.826013088226318,
      "learning_rate": 3.821709571461309e-05,
      "loss": 1.8677,
      "step": 123400
    },
    {
      "epoch": 9.433962264150944,
      "grad_norm": 8.321167945861816,
      "learning_rate": 3.820754716981133e-05,
      "loss": 1.9477,
      "step": 123500
    },
    {
      "epoch": 9.441601099992361,
      "grad_norm": 6.246913909912109,
      "learning_rate": 3.819799862500955e-05,
      "loss": 1.8461,
      "step": 123600
    },
    {
      "epoch": 9.449239935833779,
      "grad_norm": 7.957286357879639,
      "learning_rate": 3.818845008020778e-05,
      "loss": 1.8046,
      "step": 123700
    },
    {
      "epoch": 9.456878771675196,
      "grad_norm": 7.493075847625732,
      "learning_rate": 3.8178901535406006e-05,
      "loss": 1.8236,
      "step": 123800
    },
    {
      "epoch": 9.464517607516614,
      "grad_norm": 7.453444004058838,
      "learning_rate": 3.8169352990604235e-05,
      "loss": 1.9321,
      "step": 123900
    },
    {
      "epoch": 9.472156443358033,
      "grad_norm": 9.636492729187012,
      "learning_rate": 3.8159804445802464e-05,
      "loss": 1.9305,
      "step": 124000
    },
    {
      "epoch": 9.47979527919945,
      "grad_norm": 8.210527420043945,
      "learning_rate": 3.8150255901000686e-05,
      "loss": 1.8425,
      "step": 124100
    },
    {
      "epoch": 9.487434115040868,
      "grad_norm": 6.102813243865967,
      "learning_rate": 3.814070735619892e-05,
      "loss": 1.9282,
      "step": 124200
    },
    {
      "epoch": 9.495072950882285,
      "grad_norm": 10.403205871582031,
      "learning_rate": 3.813115881139714e-05,
      "loss": 1.8876,
      "step": 124300
    },
    {
      "epoch": 9.502711786723703,
      "grad_norm": 7.939474105834961,
      "learning_rate": 3.812161026659537e-05,
      "loss": 1.8614,
      "step": 124400
    },
    {
      "epoch": 9.51035062256512,
      "grad_norm": 6.381683349609375,
      "learning_rate": 3.81120617217936e-05,
      "loss": 1.8416,
      "step": 124500
    },
    {
      "epoch": 9.51798945840654,
      "grad_norm": 8.34192943572998,
      "learning_rate": 3.810251317699183e-05,
      "loss": 1.8332,
      "step": 124600
    },
    {
      "epoch": 9.525628294247957,
      "grad_norm": 7.305561065673828,
      "learning_rate": 3.809296463219005e-05,
      "loss": 1.8984,
      "step": 124700
    },
    {
      "epoch": 9.533267130089374,
      "grad_norm": 5.843663215637207,
      "learning_rate": 3.8083416087388287e-05,
      "loss": 1.9041,
      "step": 124800
    },
    {
      "epoch": 9.540905965930792,
      "grad_norm": 10.594039916992188,
      "learning_rate": 3.807386754258651e-05,
      "loss": 1.7788,
      "step": 124900
    },
    {
      "epoch": 9.54854480177221,
      "grad_norm": 6.0338215827941895,
      "learning_rate": 3.806431899778474e-05,
      "loss": 1.882,
      "step": 125000
    },
    {
      "epoch": 9.556183637613628,
      "grad_norm": 6.9411773681640625,
      "learning_rate": 3.8054770452982966e-05,
      "loss": 1.9084,
      "step": 125100
    },
    {
      "epoch": 9.563822473455046,
      "grad_norm": 8.136488914489746,
      "learning_rate": 3.8045221908181195e-05,
      "loss": 1.8905,
      "step": 125200
    },
    {
      "epoch": 9.571461309296463,
      "grad_norm": 6.268734455108643,
      "learning_rate": 3.803567336337942e-05,
      "loss": 1.8631,
      "step": 125300
    },
    {
      "epoch": 9.57910014513788,
      "grad_norm": 6.793465614318848,
      "learning_rate": 3.802612481857765e-05,
      "loss": 1.8372,
      "step": 125400
    },
    {
      "epoch": 9.586738980979298,
      "grad_norm": 7.537399768829346,
      "learning_rate": 3.801657627377588e-05,
      "loss": 1.9455,
      "step": 125500
    },
    {
      "epoch": 9.594377816820717,
      "grad_norm": 7.555716037750244,
      "learning_rate": 3.80070277289741e-05,
      "loss": 1.8467,
      "step": 125600
    },
    {
      "epoch": 9.602016652662135,
      "grad_norm": 6.037914752960205,
      "learning_rate": 3.799747918417234e-05,
      "loss": 1.9531,
      "step": 125700
    },
    {
      "epoch": 9.609655488503552,
      "grad_norm": 7.178473472595215,
      "learning_rate": 3.798793063937056e-05,
      "loss": 1.9051,
      "step": 125800
    },
    {
      "epoch": 9.61729432434497,
      "grad_norm": 5.613424777984619,
      "learning_rate": 3.797838209456879e-05,
      "loss": 1.8909,
      "step": 125900
    },
    {
      "epoch": 9.624933160186387,
      "grad_norm": 7.685488700866699,
      "learning_rate": 3.796883354976702e-05,
      "loss": 1.7352,
      "step": 126000
    },
    {
      "epoch": 9.632571996027805,
      "grad_norm": 6.3588361740112305,
      "learning_rate": 3.7959285004965246e-05,
      "loss": 1.8078,
      "step": 126100
    },
    {
      "epoch": 9.640210831869224,
      "grad_norm": 7.462490081787109,
      "learning_rate": 3.794973646016347e-05,
      "loss": 1.9565,
      "step": 126200
    },
    {
      "epoch": 9.647849667710641,
      "grad_norm": 4.789541244506836,
      "learning_rate": 3.7940187915361703e-05,
      "loss": 1.8563,
      "step": 126300
    },
    {
      "epoch": 9.655488503552059,
      "grad_norm": 7.781432151794434,
      "learning_rate": 3.7930639370559925e-05,
      "loss": 1.8951,
      "step": 126400
    },
    {
      "epoch": 9.663127339393476,
      "grad_norm": 6.1348795890808105,
      "learning_rate": 3.7921090825758154e-05,
      "loss": 1.8556,
      "step": 126500
    },
    {
      "epoch": 9.670766175234894,
      "grad_norm": 8.821931838989258,
      "learning_rate": 3.791154228095638e-05,
      "loss": 1.951,
      "step": 126600
    },
    {
      "epoch": 9.678405011076311,
      "grad_norm": 6.498342037200928,
      "learning_rate": 3.790199373615461e-05,
      "loss": 1.8779,
      "step": 126700
    },
    {
      "epoch": 9.68604384691773,
      "grad_norm": 5.851454257965088,
      "learning_rate": 3.789244519135284e-05,
      "loss": 1.9812,
      "step": 126800
    },
    {
      "epoch": 9.693682682759148,
      "grad_norm": 8.265083312988281,
      "learning_rate": 3.788289664655107e-05,
      "loss": 1.8704,
      "step": 126900
    },
    {
      "epoch": 9.701321518600565,
      "grad_norm": 6.537714004516602,
      "learning_rate": 3.78733481017493e-05,
      "loss": 1.9282,
      "step": 127000
    },
    {
      "epoch": 9.708960354441983,
      "grad_norm": 7.147091865539551,
      "learning_rate": 3.786379955694752e-05,
      "loss": 1.896,
      "step": 127100
    },
    {
      "epoch": 9.7165991902834,
      "grad_norm": 7.240853786468506,
      "learning_rate": 3.7854251012145755e-05,
      "loss": 1.8871,
      "step": 127200
    },
    {
      "epoch": 9.72423802612482,
      "grad_norm": 10.622154235839844,
      "learning_rate": 3.784470246734398e-05,
      "loss": 1.929,
      "step": 127300
    },
    {
      "epoch": 9.731876861966237,
      "grad_norm": 9.556715965270996,
      "learning_rate": 3.7835153922542205e-05,
      "loss": 1.9415,
      "step": 127400
    },
    {
      "epoch": 9.739515697807654,
      "grad_norm": 6.836235046386719,
      "learning_rate": 3.7825605377740434e-05,
      "loss": 1.8551,
      "step": 127500
    },
    {
      "epoch": 9.747154533649072,
      "grad_norm": 7.701223850250244,
      "learning_rate": 3.781605683293866e-05,
      "loss": 1.8662,
      "step": 127600
    },
    {
      "epoch": 9.75479336949049,
      "grad_norm": 6.79110860824585,
      "learning_rate": 3.7806508288136885e-05,
      "loss": 1.8657,
      "step": 127700
    },
    {
      "epoch": 9.762432205331907,
      "grad_norm": 6.678338050842285,
      "learning_rate": 3.779695974333512e-05,
      "loss": 1.7695,
      "step": 127800
    },
    {
      "epoch": 9.770071041173326,
      "grad_norm": 7.051865577697754,
      "learning_rate": 3.778741119853335e-05,
      "loss": 1.8924,
      "step": 127900
    },
    {
      "epoch": 9.777709877014743,
      "grad_norm": 13.131865501403809,
      "learning_rate": 3.777786265373157e-05,
      "loss": 1.9197,
      "step": 128000
    },
    {
      "epoch": 9.78534871285616,
      "grad_norm": 7.984005928039551,
      "learning_rate": 3.7768314108929806e-05,
      "loss": 1.9349,
      "step": 128100
    },
    {
      "epoch": 9.792987548697578,
      "grad_norm": 7.101174831390381,
      "learning_rate": 3.775876556412803e-05,
      "loss": 1.8216,
      "step": 128200
    },
    {
      "epoch": 9.800626384538996,
      "grad_norm": 6.41784143447876,
      "learning_rate": 3.774921701932626e-05,
      "loss": 1.8756,
      "step": 128300
    },
    {
      "epoch": 9.808265220380415,
      "grad_norm": 7.439627647399902,
      "learning_rate": 3.7739668474524486e-05,
      "loss": 1.8968,
      "step": 128400
    },
    {
      "epoch": 9.815904056221832,
      "grad_norm": 6.93464994430542,
      "learning_rate": 3.7730119929722714e-05,
      "loss": 1.8401,
      "step": 128500
    },
    {
      "epoch": 9.82354289206325,
      "grad_norm": 7.821578025817871,
      "learning_rate": 3.7720571384920936e-05,
      "loss": 1.8786,
      "step": 128600
    },
    {
      "epoch": 9.831181727904667,
      "grad_norm": 7.291140079498291,
      "learning_rate": 3.771102284011917e-05,
      "loss": 1.8857,
      "step": 128700
    },
    {
      "epoch": 9.838820563746085,
      "grad_norm": 6.4490790367126465,
      "learning_rate": 3.7701474295317394e-05,
      "loss": 1.832,
      "step": 128800
    },
    {
      "epoch": 9.846459399587502,
      "grad_norm": 5.56199836730957,
      "learning_rate": 3.769192575051562e-05,
      "loss": 1.8685,
      "step": 128900
    },
    {
      "epoch": 9.854098235428921,
      "grad_norm": 8.559523582458496,
      "learning_rate": 3.768237720571385e-05,
      "loss": 1.8686,
      "step": 129000
    },
    {
      "epoch": 9.861737071270339,
      "grad_norm": 5.1879048347473145,
      "learning_rate": 3.767282866091208e-05,
      "loss": 1.8624,
      "step": 129100
    },
    {
      "epoch": 9.869375907111756,
      "grad_norm": 7.24326753616333,
      "learning_rate": 3.766328011611031e-05,
      "loss": 1.8444,
      "step": 129200
    },
    {
      "epoch": 9.877014742953174,
      "grad_norm": 5.27509069442749,
      "learning_rate": 3.765373157130854e-05,
      "loss": 1.9756,
      "step": 129300
    },
    {
      "epoch": 9.884653578794591,
      "grad_norm": 7.0064873695373535,
      "learning_rate": 3.7644183026506766e-05,
      "loss": 1.9527,
      "step": 129400
    },
    {
      "epoch": 9.89229241463601,
      "grad_norm": 6.558749675750732,
      "learning_rate": 3.763463448170499e-05,
      "loss": 1.7571,
      "step": 129500
    },
    {
      "epoch": 9.899931250477428,
      "grad_norm": 6.539523601531982,
      "learning_rate": 3.762508593690322e-05,
      "loss": 1.8764,
      "step": 129600
    },
    {
      "epoch": 9.907570086318845,
      "grad_norm": 6.285376071929932,
      "learning_rate": 3.7615537392101445e-05,
      "loss": 1.9177,
      "step": 129700
    },
    {
      "epoch": 9.915208922160263,
      "grad_norm": 6.258729934692383,
      "learning_rate": 3.7605988847299674e-05,
      "loss": 1.9501,
      "step": 129800
    },
    {
      "epoch": 9.92284775800168,
      "grad_norm": 8.337038040161133,
      "learning_rate": 3.7596440302497896e-05,
      "loss": 1.8376,
      "step": 129900
    },
    {
      "epoch": 9.930486593843098,
      "grad_norm": 5.765321254730225,
      "learning_rate": 3.758689175769613e-05,
      "loss": 1.999,
      "step": 130000
    },
    {
      "epoch": 9.938125429684517,
      "grad_norm": 7.138646125793457,
      "learning_rate": 3.757734321289435e-05,
      "loss": 1.9408,
      "step": 130100
    },
    {
      "epoch": 9.945764265525934,
      "grad_norm": 5.975494384765625,
      "learning_rate": 3.756779466809258e-05,
      "loss": 1.7806,
      "step": 130200
    },
    {
      "epoch": 9.953403101367352,
      "grad_norm": 6.708974361419678,
      "learning_rate": 3.755824612329081e-05,
      "loss": 1.778,
      "step": 130300
    },
    {
      "epoch": 9.961041937208769,
      "grad_norm": 8.916874885559082,
      "learning_rate": 3.754869757848904e-05,
      "loss": 1.9818,
      "step": 130400
    },
    {
      "epoch": 9.968680773050187,
      "grad_norm": 7.778391361236572,
      "learning_rate": 3.753914903368727e-05,
      "loss": 1.8987,
      "step": 130500
    },
    {
      "epoch": 9.976319608891606,
      "grad_norm": 9.2202787399292,
      "learning_rate": 3.7529600488885497e-05,
      "loss": 1.8861,
      "step": 130600
    },
    {
      "epoch": 9.983958444733023,
      "grad_norm": 5.720977306365967,
      "learning_rate": 3.7520051944083725e-05,
      "loss": 1.8533,
      "step": 130700
    },
    {
      "epoch": 9.99159728057444,
      "grad_norm": 5.4271111488342285,
      "learning_rate": 3.751050339928195e-05,
      "loss": 1.9275,
      "step": 130800
    },
    {
      "epoch": 9.999236116415858,
      "grad_norm": 6.454805850982666,
      "learning_rate": 3.750095485448018e-05,
      "loss": 1.8254,
      "step": 130900
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8665070533752441,
      "eval_runtime": 3.0089,
      "eval_samples_per_second": 229.323,
      "eval_steps_per_second": 229.323,
      "step": 130910
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.658726453781128,
      "eval_runtime": 56.0814,
      "eval_samples_per_second": 233.429,
      "eval_steps_per_second": 233.429,
      "step": 130910
    },
    {
      "epoch": 10.006874952257276,
      "grad_norm": 8.97809886932373,
      "learning_rate": 3.7491406309678405e-05,
      "loss": 1.7793,
      "step": 131000
    },
    {
      "epoch": 10.014513788098693,
      "grad_norm": 8.146158218383789,
      "learning_rate": 3.748185776487663e-05,
      "loss": 1.7744,
      "step": 131100
    },
    {
      "epoch": 10.022152623940112,
      "grad_norm": 6.894863128662109,
      "learning_rate": 3.747230922007486e-05,
      "loss": 1.8662,
      "step": 131200
    },
    {
      "epoch": 10.02979145978153,
      "grad_norm": 6.968770980834961,
      "learning_rate": 3.746276067527309e-05,
      "loss": 1.7604,
      "step": 131300
    },
    {
      "epoch": 10.037430295622947,
      "grad_norm": 6.265608787536621,
      "learning_rate": 3.745321213047131e-05,
      "loss": 1.8916,
      "step": 131400
    },
    {
      "epoch": 10.045069131464365,
      "grad_norm": 9.435650825500488,
      "learning_rate": 3.744366358566955e-05,
      "loss": 1.7992,
      "step": 131500
    },
    {
      "epoch": 10.052707967305782,
      "grad_norm": 6.327167510986328,
      "learning_rate": 3.743411504086777e-05,
      "loss": 1.8898,
      "step": 131600
    },
    {
      "epoch": 10.060346803147201,
      "grad_norm": 6.777761936187744,
      "learning_rate": 3.7424566496066e-05,
      "loss": 1.8171,
      "step": 131700
    },
    {
      "epoch": 10.067985638988619,
      "grad_norm": 7.095935344696045,
      "learning_rate": 3.741501795126423e-05,
      "loss": 1.9462,
      "step": 131800
    },
    {
      "epoch": 10.075624474830036,
      "grad_norm": 6.379728317260742,
      "learning_rate": 3.7405469406462456e-05,
      "loss": 1.7687,
      "step": 131900
    },
    {
      "epoch": 10.083263310671454,
      "grad_norm": 6.675591468811035,
      "learning_rate": 3.7395920861660685e-05,
      "loss": 1.8542,
      "step": 132000
    },
    {
      "epoch": 10.090902146512871,
      "grad_norm": 8.287853240966797,
      "learning_rate": 3.7386372316858913e-05,
      "loss": 1.8629,
      "step": 132100
    },
    {
      "epoch": 10.098540982354288,
      "grad_norm": 5.832002639770508,
      "learning_rate": 3.737682377205714e-05,
      "loss": 1.9032,
      "step": 132200
    },
    {
      "epoch": 10.106179818195708,
      "grad_norm": 7.416268348693848,
      "learning_rate": 3.7367275227255364e-05,
      "loss": 1.8246,
      "step": 132300
    },
    {
      "epoch": 10.113818654037125,
      "grad_norm": 7.4715447425842285,
      "learning_rate": 3.73577266824536e-05,
      "loss": 1.8602,
      "step": 132400
    },
    {
      "epoch": 10.121457489878543,
      "grad_norm": 8.857538223266602,
      "learning_rate": 3.734817813765182e-05,
      "loss": 1.8281,
      "step": 132500
    },
    {
      "epoch": 10.12909632571996,
      "grad_norm": 7.513948440551758,
      "learning_rate": 3.733862959285005e-05,
      "loss": 1.8948,
      "step": 132600
    },
    {
      "epoch": 10.136735161561377,
      "grad_norm": 6.470956325531006,
      "learning_rate": 3.732908104804828e-05,
      "loss": 1.7862,
      "step": 132700
    },
    {
      "epoch": 10.144373997402797,
      "grad_norm": 6.321525573730469,
      "learning_rate": 3.731953250324651e-05,
      "loss": 1.8111,
      "step": 132800
    },
    {
      "epoch": 10.152012833244214,
      "grad_norm": 6.644221305847168,
      "learning_rate": 3.7309983958444736e-05,
      "loss": 1.8136,
      "step": 132900
    },
    {
      "epoch": 10.159651669085632,
      "grad_norm": 8.37417221069336,
      "learning_rate": 3.7300435413642965e-05,
      "loss": 1.9235,
      "step": 133000
    },
    {
      "epoch": 10.167290504927049,
      "grad_norm": 7.108537673950195,
      "learning_rate": 3.7290886868841194e-05,
      "loss": 1.8448,
      "step": 133100
    },
    {
      "epoch": 10.174929340768466,
      "grad_norm": 8.591059684753418,
      "learning_rate": 3.7281338324039415e-05,
      "loss": 1.8456,
      "step": 133200
    },
    {
      "epoch": 10.182568176609884,
      "grad_norm": 7.346791744232178,
      "learning_rate": 3.727178977923765e-05,
      "loss": 1.827,
      "step": 133300
    },
    {
      "epoch": 10.190207012451303,
      "grad_norm": 6.292884349822998,
      "learning_rate": 3.726224123443587e-05,
      "loss": 1.8775,
      "step": 133400
    },
    {
      "epoch": 10.19784584829272,
      "grad_norm": 6.816336631774902,
      "learning_rate": 3.72526926896341e-05,
      "loss": 1.8272,
      "step": 133500
    },
    {
      "epoch": 10.205484684134138,
      "grad_norm": 6.990103721618652,
      "learning_rate": 3.724314414483233e-05,
      "loss": 1.9783,
      "step": 133600
    },
    {
      "epoch": 10.213123519975555,
      "grad_norm": 9.326465606689453,
      "learning_rate": 3.723359560003056e-05,
      "loss": 1.8942,
      "step": 133700
    },
    {
      "epoch": 10.220762355816973,
      "grad_norm": 5.617641925811768,
      "learning_rate": 3.722404705522878e-05,
      "loss": 1.7748,
      "step": 133800
    },
    {
      "epoch": 10.228401191658392,
      "grad_norm": 6.977113723754883,
      "learning_rate": 3.7214498510427016e-05,
      "loss": 1.8392,
      "step": 133900
    },
    {
      "epoch": 10.23604002749981,
      "grad_norm": 11.905871391296387,
      "learning_rate": 3.720494996562524e-05,
      "loss": 1.8698,
      "step": 134000
    },
    {
      "epoch": 10.243678863341227,
      "grad_norm": 6.378593921661377,
      "learning_rate": 3.719540142082347e-05,
      "loss": 1.8611,
      "step": 134100
    },
    {
      "epoch": 10.251317699182644,
      "grad_norm": 6.6363139152526855,
      "learning_rate": 3.7185852876021696e-05,
      "loss": 1.8625,
      "step": 134200
    },
    {
      "epoch": 10.258956535024062,
      "grad_norm": 6.328771114349365,
      "learning_rate": 3.7176304331219924e-05,
      "loss": 1.9014,
      "step": 134300
    },
    {
      "epoch": 10.26659537086548,
      "grad_norm": 7.246350288391113,
      "learning_rate": 3.716675578641815e-05,
      "loss": 1.8941,
      "step": 134400
    },
    {
      "epoch": 10.274234206706899,
      "grad_norm": 5.978972911834717,
      "learning_rate": 3.715720724161638e-05,
      "loss": 1.9921,
      "step": 134500
    },
    {
      "epoch": 10.281873042548316,
      "grad_norm": 6.402256011962891,
      "learning_rate": 3.714765869681461e-05,
      "loss": 1.8658,
      "step": 134600
    },
    {
      "epoch": 10.289511878389733,
      "grad_norm": 8.023486137390137,
      "learning_rate": 3.713811015201283e-05,
      "loss": 1.862,
      "step": 134700
    },
    {
      "epoch": 10.297150714231151,
      "grad_norm": 5.642609119415283,
      "learning_rate": 3.712856160721107e-05,
      "loss": 1.8822,
      "step": 134800
    },
    {
      "epoch": 10.304789550072568,
      "grad_norm": 6.194344997406006,
      "learning_rate": 3.711901306240929e-05,
      "loss": 1.825,
      "step": 134900
    },
    {
      "epoch": 10.312428385913986,
      "grad_norm": 6.953502655029297,
      "learning_rate": 3.710946451760752e-05,
      "loss": 1.8901,
      "step": 135000
    },
    {
      "epoch": 10.320067221755405,
      "grad_norm": 11.231208801269531,
      "learning_rate": 3.709991597280575e-05,
      "loss": 1.8687,
      "step": 135100
    },
    {
      "epoch": 10.327706057596822,
      "grad_norm": 6.500087261199951,
      "learning_rate": 3.7090367428003976e-05,
      "loss": 1.8824,
      "step": 135200
    },
    {
      "epoch": 10.33534489343824,
      "grad_norm": 6.524011611938477,
      "learning_rate": 3.70808188832022e-05,
      "loss": 1.7981,
      "step": 135300
    },
    {
      "epoch": 10.342983729279657,
      "grad_norm": 7.094750881195068,
      "learning_rate": 3.707127033840043e-05,
      "loss": 1.8804,
      "step": 135400
    },
    {
      "epoch": 10.350622565121075,
      "grad_norm": 8.128438949584961,
      "learning_rate": 3.7061721793598655e-05,
      "loss": 1.7419,
      "step": 135500
    },
    {
      "epoch": 10.358261400962494,
      "grad_norm": 7.573164463043213,
      "learning_rate": 3.7052173248796884e-05,
      "loss": 1.7788,
      "step": 135600
    },
    {
      "epoch": 10.365900236803911,
      "grad_norm": 7.89602518081665,
      "learning_rate": 3.704262470399511e-05,
      "loss": 1.8565,
      "step": 135700
    },
    {
      "epoch": 10.373539072645329,
      "grad_norm": 6.548999786376953,
      "learning_rate": 3.703307615919334e-05,
      "loss": 1.9307,
      "step": 135800
    },
    {
      "epoch": 10.381177908486746,
      "grad_norm": 6.204903602600098,
      "learning_rate": 3.702352761439157e-05,
      "loss": 1.8091,
      "step": 135900
    },
    {
      "epoch": 10.388816744328164,
      "grad_norm": 7.264629364013672,
      "learning_rate": 3.701397906958979e-05,
      "loss": 1.9,
      "step": 136000
    },
    {
      "epoch": 10.396455580169583,
      "grad_norm": 7.202680587768555,
      "learning_rate": 3.700443052478803e-05,
      "loss": 1.8943,
      "step": 136100
    },
    {
      "epoch": 10.404094416011,
      "grad_norm": 6.542779445648193,
      "learning_rate": 3.699488197998625e-05,
      "loss": 1.8851,
      "step": 136200
    },
    {
      "epoch": 10.411733251852418,
      "grad_norm": 5.599010944366455,
      "learning_rate": 3.698533343518448e-05,
      "loss": 1.8621,
      "step": 136300
    },
    {
      "epoch": 10.419372087693835,
      "grad_norm": 7.212004661560059,
      "learning_rate": 3.6975784890382707e-05,
      "loss": 1.929,
      "step": 136400
    },
    {
      "epoch": 10.427010923535253,
      "grad_norm": 7.293776035308838,
      "learning_rate": 3.6966236345580935e-05,
      "loss": 1.8949,
      "step": 136500
    },
    {
      "epoch": 10.43464975937667,
      "grad_norm": 7.666909694671631,
      "learning_rate": 3.6956687800779164e-05,
      "loss": 1.9024,
      "step": 136600
    },
    {
      "epoch": 10.44228859521809,
      "grad_norm": 5.985862731933594,
      "learning_rate": 3.694713925597739e-05,
      "loss": 1.8562,
      "step": 136700
    },
    {
      "epoch": 10.449927431059507,
      "grad_norm": 8.009770393371582,
      "learning_rate": 3.693759071117562e-05,
      "loss": 1.882,
      "step": 136800
    },
    {
      "epoch": 10.457566266900924,
      "grad_norm": 6.399127960205078,
      "learning_rate": 3.692804216637384e-05,
      "loss": 1.7881,
      "step": 136900
    },
    {
      "epoch": 10.465205102742342,
      "grad_norm": 6.456094264984131,
      "learning_rate": 3.691849362157208e-05,
      "loss": 1.8329,
      "step": 137000
    },
    {
      "epoch": 10.47284393858376,
      "grad_norm": 7.07948637008667,
      "learning_rate": 3.69089450767703e-05,
      "loss": 1.8671,
      "step": 137100
    },
    {
      "epoch": 10.480482774425177,
      "grad_norm": 6.425454139709473,
      "learning_rate": 3.689939653196853e-05,
      "loss": 1.8723,
      "step": 137200
    },
    {
      "epoch": 10.488121610266596,
      "grad_norm": 7.40205192565918,
      "learning_rate": 3.688984798716676e-05,
      "loss": 1.8321,
      "step": 137300
    },
    {
      "epoch": 10.495760446108013,
      "grad_norm": 8.454545974731445,
      "learning_rate": 3.688029944236499e-05,
      "loss": 1.7596,
      "step": 137400
    },
    {
      "epoch": 10.50339928194943,
      "grad_norm": 8.819326400756836,
      "learning_rate": 3.687075089756321e-05,
      "loss": 1.8421,
      "step": 137500
    },
    {
      "epoch": 10.511038117790848,
      "grad_norm": 9.270895957946777,
      "learning_rate": 3.6861202352761444e-05,
      "loss": 1.9026,
      "step": 137600
    },
    {
      "epoch": 10.518676953632266,
      "grad_norm": 8.469711303710938,
      "learning_rate": 3.6851653807959666e-05,
      "loss": 1.8491,
      "step": 137700
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 9.24942684173584,
      "learning_rate": 3.6842105263157895e-05,
      "loss": 1.845,
      "step": 137800
    },
    {
      "epoch": 10.533954625315102,
      "grad_norm": 6.715758800506592,
      "learning_rate": 3.6832556718356123e-05,
      "loss": 1.846,
      "step": 137900
    },
    {
      "epoch": 10.54159346115652,
      "grad_norm": 6.417778968811035,
      "learning_rate": 3.682300817355435e-05,
      "loss": 1.7701,
      "step": 138000
    },
    {
      "epoch": 10.549232296997937,
      "grad_norm": 6.340048789978027,
      "learning_rate": 3.681345962875258e-05,
      "loss": 1.8579,
      "step": 138100
    },
    {
      "epoch": 10.556871132839355,
      "grad_norm": 7.7856903076171875,
      "learning_rate": 3.680391108395081e-05,
      "loss": 1.9286,
      "step": 138200
    },
    {
      "epoch": 10.564509968680774,
      "grad_norm": 6.5771803855896,
      "learning_rate": 3.679436253914904e-05,
      "loss": 1.8865,
      "step": 138300
    },
    {
      "epoch": 10.572148804522191,
      "grad_norm": 9.472565650939941,
      "learning_rate": 3.678481399434726e-05,
      "loss": 1.9988,
      "step": 138400
    },
    {
      "epoch": 10.579787640363609,
      "grad_norm": 5.457387924194336,
      "learning_rate": 3.6775265449545496e-05,
      "loss": 1.8103,
      "step": 138500
    },
    {
      "epoch": 10.587426476205026,
      "grad_norm": 8.490449905395508,
      "learning_rate": 3.676571690474372e-05,
      "loss": 1.7169,
      "step": 138600
    },
    {
      "epoch": 10.595065312046444,
      "grad_norm": 6.105509281158447,
      "learning_rate": 3.6756168359941946e-05,
      "loss": 1.9338,
      "step": 138700
    },
    {
      "epoch": 10.602704147887861,
      "grad_norm": 8.93940544128418,
      "learning_rate": 3.6746619815140175e-05,
      "loss": 1.9197,
      "step": 138800
    },
    {
      "epoch": 10.61034298372928,
      "grad_norm": 6.934995651245117,
      "learning_rate": 3.6737071270338404e-05,
      "loss": 1.9008,
      "step": 138900
    },
    {
      "epoch": 10.617981819570698,
      "grad_norm": 9.363043785095215,
      "learning_rate": 3.6727522725536625e-05,
      "loss": 1.8701,
      "step": 139000
    },
    {
      "epoch": 10.625620655412115,
      "grad_norm": 7.4650421142578125,
      "learning_rate": 3.671797418073486e-05,
      "loss": 1.8972,
      "step": 139100
    },
    {
      "epoch": 10.633259491253533,
      "grad_norm": 6.840356826782227,
      "learning_rate": 3.670842563593308e-05,
      "loss": 1.8989,
      "step": 139200
    },
    {
      "epoch": 10.64089832709495,
      "grad_norm": 6.3988356590271,
      "learning_rate": 3.669887709113131e-05,
      "loss": 1.9253,
      "step": 139300
    },
    {
      "epoch": 10.648537162936368,
      "grad_norm": 8.292935371398926,
      "learning_rate": 3.668932854632954e-05,
      "loss": 1.8801,
      "step": 139400
    },
    {
      "epoch": 10.656175998777787,
      "grad_norm": 6.468079566955566,
      "learning_rate": 3.667978000152777e-05,
      "loss": 1.8294,
      "step": 139500
    },
    {
      "epoch": 10.663814834619204,
      "grad_norm": 9.804224014282227,
      "learning_rate": 3.6670231456726e-05,
      "loss": 1.9223,
      "step": 139600
    },
    {
      "epoch": 10.671453670460622,
      "grad_norm": 9.591634750366211,
      "learning_rate": 3.6660682911924226e-05,
      "loss": 1.8849,
      "step": 139700
    },
    {
      "epoch": 10.67909250630204,
      "grad_norm": 8.11794662475586,
      "learning_rate": 3.6651134367122455e-05,
      "loss": 1.871,
      "step": 139800
    },
    {
      "epoch": 10.686731342143457,
      "grad_norm": 7.6948981285095215,
      "learning_rate": 3.664158582232068e-05,
      "loss": 1.8707,
      "step": 139900
    },
    {
      "epoch": 10.694370177984876,
      "grad_norm": 11.874822616577148,
      "learning_rate": 3.663203727751891e-05,
      "loss": 1.8829,
      "step": 140000
    },
    {
      "epoch": 10.702009013826293,
      "grad_norm": 8.921900749206543,
      "learning_rate": 3.6622488732717134e-05,
      "loss": 1.8645,
      "step": 140100
    },
    {
      "epoch": 10.70964784966771,
      "grad_norm": 6.978734493255615,
      "learning_rate": 3.661294018791536e-05,
      "loss": 1.8161,
      "step": 140200
    },
    {
      "epoch": 10.717286685509128,
      "grad_norm": 8.720818519592285,
      "learning_rate": 3.660339164311359e-05,
      "loss": 1.9557,
      "step": 140300
    },
    {
      "epoch": 10.724925521350546,
      "grad_norm": 8.148018836975098,
      "learning_rate": 3.659384309831182e-05,
      "loss": 1.8481,
      "step": 140400
    },
    {
      "epoch": 10.732564357191965,
      "grad_norm": 6.152205944061279,
      "learning_rate": 3.658429455351004e-05,
      "loss": 1.8368,
      "step": 140500
    },
    {
      "epoch": 10.740203193033382,
      "grad_norm": 8.658672332763672,
      "learning_rate": 3.657474600870828e-05,
      "loss": 1.8905,
      "step": 140600
    },
    {
      "epoch": 10.7478420288748,
      "grad_norm": 5.272498607635498,
      "learning_rate": 3.6565197463906506e-05,
      "loss": 1.9187,
      "step": 140700
    },
    {
      "epoch": 10.755480864716217,
      "grad_norm": 8.696893692016602,
      "learning_rate": 3.655564891910473e-05,
      "loss": 1.8305,
      "step": 140800
    },
    {
      "epoch": 10.763119700557635,
      "grad_norm": 7.222473621368408,
      "learning_rate": 3.6546100374302964e-05,
      "loss": 1.8876,
      "step": 140900
    },
    {
      "epoch": 10.770758536399052,
      "grad_norm": 7.834645748138428,
      "learning_rate": 3.6536551829501186e-05,
      "loss": 1.8752,
      "step": 141000
    },
    {
      "epoch": 10.778397372240471,
      "grad_norm": 5.2517313957214355,
      "learning_rate": 3.6527003284699414e-05,
      "loss": 1.8467,
      "step": 141100
    },
    {
      "epoch": 10.786036208081889,
      "grad_norm": 6.861004829406738,
      "learning_rate": 3.651745473989764e-05,
      "loss": 1.849,
      "step": 141200
    },
    {
      "epoch": 10.793675043923306,
      "grad_norm": 7.485384941101074,
      "learning_rate": 3.650790619509587e-05,
      "loss": 1.8501,
      "step": 141300
    },
    {
      "epoch": 10.801313879764724,
      "grad_norm": 8.36518383026123,
      "learning_rate": 3.6498357650294094e-05,
      "loss": 1.8684,
      "step": 141400
    },
    {
      "epoch": 10.808952715606141,
      "grad_norm": 6.575120449066162,
      "learning_rate": 3.648880910549232e-05,
      "loss": 1.757,
      "step": 141500
    },
    {
      "epoch": 10.816591551447559,
      "grad_norm": 7.215404510498047,
      "learning_rate": 3.647926056069055e-05,
      "loss": 1.8765,
      "step": 141600
    },
    {
      "epoch": 10.824230387288978,
      "grad_norm": 6.644181251525879,
      "learning_rate": 3.646971201588878e-05,
      "loss": 1.9069,
      "step": 141700
    },
    {
      "epoch": 10.831869223130395,
      "grad_norm": 6.671082496643066,
      "learning_rate": 3.646016347108701e-05,
      "loss": 1.8628,
      "step": 141800
    },
    {
      "epoch": 10.839508058971813,
      "grad_norm": 5.803050994873047,
      "learning_rate": 3.645061492628524e-05,
      "loss": 1.8656,
      "step": 141900
    },
    {
      "epoch": 10.84714689481323,
      "grad_norm": 8.223944664001465,
      "learning_rate": 3.6441066381483466e-05,
      "loss": 1.8365,
      "step": 142000
    },
    {
      "epoch": 10.854785730654648,
      "grad_norm": 6.9382195472717285,
      "learning_rate": 3.643151783668169e-05,
      "loss": 1.9738,
      "step": 142100
    },
    {
      "epoch": 10.862424566496067,
      "grad_norm": 7.36473274230957,
      "learning_rate": 3.642196929187992e-05,
      "loss": 1.9267,
      "step": 142200
    },
    {
      "epoch": 10.870063402337484,
      "grad_norm": 6.946353912353516,
      "learning_rate": 3.6412420747078145e-05,
      "loss": 1.8775,
      "step": 142300
    },
    {
      "epoch": 10.877702238178902,
      "grad_norm": 6.707222938537598,
      "learning_rate": 3.6402872202276374e-05,
      "loss": 1.8657,
      "step": 142400
    },
    {
      "epoch": 10.88534107402032,
      "grad_norm": 6.50792121887207,
      "learning_rate": 3.63933236574746e-05,
      "loss": 1.8695,
      "step": 142500
    },
    {
      "epoch": 10.892979909861737,
      "grad_norm": 7.4328508377075195,
      "learning_rate": 3.638377511267283e-05,
      "loss": 1.8554,
      "step": 142600
    },
    {
      "epoch": 10.900618745703154,
      "grad_norm": 6.811720371246338,
      "learning_rate": 3.637422656787105e-05,
      "loss": 1.8189,
      "step": 142700
    },
    {
      "epoch": 10.908257581544573,
      "grad_norm": 7.792500019073486,
      "learning_rate": 3.636467802306929e-05,
      "loss": 1.8544,
      "step": 142800
    },
    {
      "epoch": 10.91589641738599,
      "grad_norm": 7.683583736419678,
      "learning_rate": 3.635512947826751e-05,
      "loss": 1.9091,
      "step": 142900
    },
    {
      "epoch": 10.923535253227408,
      "grad_norm": 7.246335983276367,
      "learning_rate": 3.634558093346574e-05,
      "loss": 1.8171,
      "step": 143000
    },
    {
      "epoch": 10.931174089068826,
      "grad_norm": 5.552861213684082,
      "learning_rate": 3.633603238866397e-05,
      "loss": 1.7921,
      "step": 143100
    },
    {
      "epoch": 10.938812924910243,
      "grad_norm": 6.433248519897461,
      "learning_rate": 3.63264838438622e-05,
      "loss": 1.9417,
      "step": 143200
    },
    {
      "epoch": 10.946451760751662,
      "grad_norm": 6.4715399742126465,
      "learning_rate": 3.6316935299060425e-05,
      "loss": 1.8413,
      "step": 143300
    },
    {
      "epoch": 10.95409059659308,
      "grad_norm": 9.325068473815918,
      "learning_rate": 3.6307386754258654e-05,
      "loss": 1.8675,
      "step": 143400
    },
    {
      "epoch": 10.961729432434497,
      "grad_norm": 6.978683948516846,
      "learning_rate": 3.629783820945688e-05,
      "loss": 1.8631,
      "step": 143500
    },
    {
      "epoch": 10.969368268275915,
      "grad_norm": 7.305328845977783,
      "learning_rate": 3.6288289664655105e-05,
      "loss": 1.8896,
      "step": 143600
    },
    {
      "epoch": 10.977007104117332,
      "grad_norm": 4.480597496032715,
      "learning_rate": 3.627874111985334e-05,
      "loss": 1.9346,
      "step": 143700
    },
    {
      "epoch": 10.98464593995875,
      "grad_norm": 8.173741340637207,
      "learning_rate": 3.626919257505156e-05,
      "loss": 1.8856,
      "step": 143800
    },
    {
      "epoch": 10.992284775800169,
      "grad_norm": 7.325103759765625,
      "learning_rate": 3.625964403024979e-05,
      "loss": 1.8636,
      "step": 143900
    },
    {
      "epoch": 10.999923611641586,
      "grad_norm": 8.412529945373535,
      "learning_rate": 3.625009548544802e-05,
      "loss": 1.782,
      "step": 144000
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.8594026565551758,
      "eval_runtime": 1.4549,
      "eval_samples_per_second": 474.275,
      "eval_steps_per_second": 474.275,
      "step": 144001
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.6421529054641724,
      "eval_runtime": 28.1097,
      "eval_samples_per_second": 465.712,
      "eval_steps_per_second": 465.712,
      "step": 144001
    },
    {
      "epoch": 11.007562447483004,
      "grad_norm": 8.37990665435791,
      "learning_rate": 3.624054694064625e-05,
      "loss": 1.8375,
      "step": 144100
    },
    {
      "epoch": 11.015201283324421,
      "grad_norm": 7.041351795196533,
      "learning_rate": 3.623099839584447e-05,
      "loss": 1.8863,
      "step": 144200
    },
    {
      "epoch": 11.022840119165839,
      "grad_norm": 5.770143032073975,
      "learning_rate": 3.6221449851042706e-05,
      "loss": 1.8831,
      "step": 144300
    },
    {
      "epoch": 11.030478955007258,
      "grad_norm": 6.4870076179504395,
      "learning_rate": 3.621190130624093e-05,
      "loss": 1.7708,
      "step": 144400
    },
    {
      "epoch": 11.038117790848675,
      "grad_norm": 7.514697551727295,
      "learning_rate": 3.6202352761439156e-05,
      "loss": 1.9151,
      "step": 144500
    },
    {
      "epoch": 11.045756626690093,
      "grad_norm": 8.932602882385254,
      "learning_rate": 3.6192804216637385e-05,
      "loss": 1.7773,
      "step": 144600
    },
    {
      "epoch": 11.05339546253151,
      "grad_norm": 6.518604755401611,
      "learning_rate": 3.6183255671835614e-05,
      "loss": 1.9174,
      "step": 144700
    },
    {
      "epoch": 11.061034298372928,
      "grad_norm": 8.25277328491211,
      "learning_rate": 3.617370712703384e-05,
      "loss": 1.8485,
      "step": 144800
    },
    {
      "epoch": 11.068673134214345,
      "grad_norm": 7.218301773071289,
      "learning_rate": 3.616415858223207e-05,
      "loss": 1.8232,
      "step": 144900
    },
    {
      "epoch": 11.076311970055764,
      "grad_norm": 7.233745574951172,
      "learning_rate": 3.61546100374303e-05,
      "loss": 1.8594,
      "step": 145000
    },
    {
      "epoch": 11.083950805897182,
      "grad_norm": 11.290826797485352,
      "learning_rate": 3.614506149262852e-05,
      "loss": 1.8597,
      "step": 145100
    },
    {
      "epoch": 11.091589641738599,
      "grad_norm": 6.861207962036133,
      "learning_rate": 3.613551294782676e-05,
      "loss": 1.9048,
      "step": 145200
    },
    {
      "epoch": 11.099228477580017,
      "grad_norm": 6.278375625610352,
      "learning_rate": 3.612596440302498e-05,
      "loss": 1.8565,
      "step": 145300
    },
    {
      "epoch": 11.106867313421434,
      "grad_norm": 7.961702346801758,
      "learning_rate": 3.611641585822321e-05,
      "loss": 1.9159,
      "step": 145400
    },
    {
      "epoch": 11.114506149262853,
      "grad_norm": 7.325916767120361,
      "learning_rate": 3.6106867313421436e-05,
      "loss": 1.7986,
      "step": 145500
    },
    {
      "epoch": 11.12214498510427,
      "grad_norm": 8.266899108886719,
      "learning_rate": 3.6097318768619665e-05,
      "loss": 1.9033,
      "step": 145600
    },
    {
      "epoch": 11.129783820945688,
      "grad_norm": 8.718323707580566,
      "learning_rate": 3.6087770223817894e-05,
      "loss": 1.8524,
      "step": 145700
    },
    {
      "epoch": 11.137422656787106,
      "grad_norm": 9.433831214904785,
      "learning_rate": 3.607822167901612e-05,
      "loss": 1.8036,
      "step": 145800
    },
    {
      "epoch": 11.145061492628523,
      "grad_norm": 7.345550060272217,
      "learning_rate": 3.606867313421435e-05,
      "loss": 1.8811,
      "step": 145900
    },
    {
      "epoch": 11.15270032846994,
      "grad_norm": 6.870015621185303,
      "learning_rate": 3.605912458941257e-05,
      "loss": 1.8341,
      "step": 146000
    },
    {
      "epoch": 11.16033916431136,
      "grad_norm": 6.909578800201416,
      "learning_rate": 3.604957604461081e-05,
      "loss": 1.8337,
      "step": 146100
    },
    {
      "epoch": 11.167978000152777,
      "grad_norm": 6.994271278381348,
      "learning_rate": 3.604002749980903e-05,
      "loss": 1.8592,
      "step": 146200
    },
    {
      "epoch": 11.175616835994195,
      "grad_norm": 6.633540630340576,
      "learning_rate": 3.603047895500726e-05,
      "loss": 1.7839,
      "step": 146300
    },
    {
      "epoch": 11.183255671835612,
      "grad_norm": 9.192654609680176,
      "learning_rate": 3.602093041020549e-05,
      "loss": 1.7828,
      "step": 146400
    },
    {
      "epoch": 11.19089450767703,
      "grad_norm": 8.221048355102539,
      "learning_rate": 3.6011381865403716e-05,
      "loss": 1.913,
      "step": 146500
    },
    {
      "epoch": 11.198533343518449,
      "grad_norm": 8.025799751281738,
      "learning_rate": 3.600183332060194e-05,
      "loss": 1.8702,
      "step": 146600
    },
    {
      "epoch": 11.206172179359866,
      "grad_norm": 8.265376091003418,
      "learning_rate": 3.5992284775800174e-05,
      "loss": 1.9597,
      "step": 146700
    },
    {
      "epoch": 11.213811015201284,
      "grad_norm": 8.165350914001465,
      "learning_rate": 3.5982736230998396e-05,
      "loss": 1.8476,
      "step": 146800
    },
    {
      "epoch": 11.221449851042701,
      "grad_norm": 9.47076416015625,
      "learning_rate": 3.5973187686196624e-05,
      "loss": 1.768,
      "step": 146900
    },
    {
      "epoch": 11.229088686884118,
      "grad_norm": 7.80311393737793,
      "learning_rate": 3.596363914139485e-05,
      "loss": 1.7634,
      "step": 147000
    },
    {
      "epoch": 11.236727522725536,
      "grad_norm": 6.157132148742676,
      "learning_rate": 3.595409059659308e-05,
      "loss": 1.8567,
      "step": 147100
    },
    {
      "epoch": 11.244366358566955,
      "grad_norm": 6.720829486846924,
      "learning_rate": 3.594454205179131e-05,
      "loss": 1.773,
      "step": 147200
    },
    {
      "epoch": 11.252005194408373,
      "grad_norm": 7.531181335449219,
      "learning_rate": 3.593499350698953e-05,
      "loss": 1.8336,
      "step": 147300
    },
    {
      "epoch": 11.25964403024979,
      "grad_norm": 5.208984851837158,
      "learning_rate": 3.592544496218777e-05,
      "loss": 1.8929,
      "step": 147400
    },
    {
      "epoch": 11.267282866091207,
      "grad_norm": 6.041004180908203,
      "learning_rate": 3.591589641738599e-05,
      "loss": 1.7836,
      "step": 147500
    },
    {
      "epoch": 11.274921701932625,
      "grad_norm": 6.840009689331055,
      "learning_rate": 3.590634787258422e-05,
      "loss": 1.8204,
      "step": 147600
    },
    {
      "epoch": 11.282560537774042,
      "grad_norm": 7.20481014251709,
      "learning_rate": 3.589679932778245e-05,
      "loss": 1.8051,
      "step": 147700
    },
    {
      "epoch": 11.290199373615462,
      "grad_norm": 6.437813758850098,
      "learning_rate": 3.5887250782980676e-05,
      "loss": 1.8351,
      "step": 147800
    },
    {
      "epoch": 11.297838209456879,
      "grad_norm": 6.30376672744751,
      "learning_rate": 3.58777022381789e-05,
      "loss": 1.9851,
      "step": 147900
    },
    {
      "epoch": 11.305477045298296,
      "grad_norm": 7.674008369445801,
      "learning_rate": 3.586815369337713e-05,
      "loss": 1.9195,
      "step": 148000
    },
    {
      "epoch": 11.313115881139714,
      "grad_norm": 10.949092864990234,
      "learning_rate": 3.5858605148575355e-05,
      "loss": 1.842,
      "step": 148100
    },
    {
      "epoch": 11.320754716981131,
      "grad_norm": 6.500110626220703,
      "learning_rate": 3.5849056603773584e-05,
      "loss": 1.7676,
      "step": 148200
    },
    {
      "epoch": 11.32839355282255,
      "grad_norm": 7.713833332061768,
      "learning_rate": 3.583950805897181e-05,
      "loss": 1.8766,
      "step": 148300
    },
    {
      "epoch": 11.336032388663968,
      "grad_norm": 6.10723876953125,
      "learning_rate": 3.582995951417004e-05,
      "loss": 1.8858,
      "step": 148400
    },
    {
      "epoch": 11.343671224505385,
      "grad_norm": 6.475439071655273,
      "learning_rate": 3.582041096936827e-05,
      "loss": 1.8585,
      "step": 148500
    },
    {
      "epoch": 11.351310060346803,
      "grad_norm": 6.124383926391602,
      "learning_rate": 3.58108624245665e-05,
      "loss": 1.8606,
      "step": 148600
    },
    {
      "epoch": 11.35894889618822,
      "grad_norm": 8.395011901855469,
      "learning_rate": 3.580131387976473e-05,
      "loss": 1.7963,
      "step": 148700
    },
    {
      "epoch": 11.36658773202964,
      "grad_norm": 7.545248508453369,
      "learning_rate": 3.579176533496295e-05,
      "loss": 1.8436,
      "step": 148800
    },
    {
      "epoch": 11.374226567871057,
      "grad_norm": 13.468994140625,
      "learning_rate": 3.5782216790161185e-05,
      "loss": 1.8923,
      "step": 148900
    },
    {
      "epoch": 11.381865403712474,
      "grad_norm": 6.749504566192627,
      "learning_rate": 3.577266824535941e-05,
      "loss": 1.8307,
      "step": 149000
    },
    {
      "epoch": 11.389504239553892,
      "grad_norm": 8.702638626098633,
      "learning_rate": 3.5763119700557635e-05,
      "loss": 1.7663,
      "step": 149100
    },
    {
      "epoch": 11.39714307539531,
      "grad_norm": 6.836621284484863,
      "learning_rate": 3.5753571155755864e-05,
      "loss": 1.9048,
      "step": 149200
    },
    {
      "epoch": 11.404781911236727,
      "grad_norm": 7.375463008880615,
      "learning_rate": 3.574402261095409e-05,
      "loss": 1.8564,
      "step": 149300
    },
    {
      "epoch": 11.412420747078146,
      "grad_norm": 5.589132785797119,
      "learning_rate": 3.5734474066152315e-05,
      "loss": 1.8685,
      "step": 149400
    },
    {
      "epoch": 11.420059582919563,
      "grad_norm": 4.348840236663818,
      "learning_rate": 3.572492552135055e-05,
      "loss": 1.7824,
      "step": 149500
    },
    {
      "epoch": 11.427698418760981,
      "grad_norm": 8.780802726745605,
      "learning_rate": 3.571537697654878e-05,
      "loss": 1.8287,
      "step": 149600
    },
    {
      "epoch": 11.435337254602398,
      "grad_norm": 5.641475200653076,
      "learning_rate": 3.5705828431747e-05,
      "loss": 1.9745,
      "step": 149700
    },
    {
      "epoch": 11.442976090443816,
      "grad_norm": 6.601625442504883,
      "learning_rate": 3.5696279886945236e-05,
      "loss": 1.8774,
      "step": 149800
    },
    {
      "epoch": 11.450614926285233,
      "grad_norm": 5.991428852081299,
      "learning_rate": 3.568673134214346e-05,
      "loss": 1.8441,
      "step": 149900
    },
    {
      "epoch": 11.458253762126652,
      "grad_norm": 8.419744491577148,
      "learning_rate": 3.567718279734169e-05,
      "loss": 1.8153,
      "step": 150000
    },
    {
      "epoch": 11.46589259796807,
      "grad_norm": 6.292227268218994,
      "learning_rate": 3.5667634252539916e-05,
      "loss": 1.8362,
      "step": 150100
    },
    {
      "epoch": 11.473531433809487,
      "grad_norm": 7.803722858428955,
      "learning_rate": 3.5658085707738144e-05,
      "loss": 1.802,
      "step": 150200
    },
    {
      "epoch": 11.481170269650905,
      "grad_norm": 5.470301151275635,
      "learning_rate": 3.5648537162936366e-05,
      "loss": 1.8241,
      "step": 150300
    },
    {
      "epoch": 11.488809105492322,
      "grad_norm": 6.972206115722656,
      "learning_rate": 3.56389886181346e-05,
      "loss": 1.8338,
      "step": 150400
    },
    {
      "epoch": 11.496447941333741,
      "grad_norm": 6.41881799697876,
      "learning_rate": 3.5629440073332824e-05,
      "loss": 1.8151,
      "step": 150500
    },
    {
      "epoch": 11.504086777175159,
      "grad_norm": 6.789417743682861,
      "learning_rate": 3.561989152853105e-05,
      "loss": 1.9207,
      "step": 150600
    },
    {
      "epoch": 11.511725613016576,
      "grad_norm": 4.710587024688721,
      "learning_rate": 3.561034298372928e-05,
      "loss": 1.8398,
      "step": 150700
    },
    {
      "epoch": 11.519364448857994,
      "grad_norm": 7.974522590637207,
      "learning_rate": 3.560079443892751e-05,
      "loss": 1.8677,
      "step": 150800
    },
    {
      "epoch": 11.527003284699411,
      "grad_norm": 6.851857662200928,
      "learning_rate": 3.559124589412574e-05,
      "loss": 1.8061,
      "step": 150900
    },
    {
      "epoch": 11.53464212054083,
      "grad_norm": 8.184491157531738,
      "learning_rate": 3.558169734932397e-05,
      "loss": 1.8281,
      "step": 151000
    },
    {
      "epoch": 11.542280956382248,
      "grad_norm": 8.868368148803711,
      "learning_rate": 3.5572148804522196e-05,
      "loss": 1.8263,
      "step": 151100
    },
    {
      "epoch": 11.549919792223665,
      "grad_norm": 6.731265544891357,
      "learning_rate": 3.556260025972042e-05,
      "loss": 1.7777,
      "step": 151200
    },
    {
      "epoch": 11.557558628065083,
      "grad_norm": 7.117761135101318,
      "learning_rate": 3.555305171491865e-05,
      "loss": 1.9057,
      "step": 151300
    },
    {
      "epoch": 11.5651974639065,
      "grad_norm": 8.22644329071045,
      "learning_rate": 3.5543503170116875e-05,
      "loss": 1.7811,
      "step": 151400
    },
    {
      "epoch": 11.572836299747918,
      "grad_norm": 8.507442474365234,
      "learning_rate": 3.5533954625315104e-05,
      "loss": 1.9128,
      "step": 151500
    },
    {
      "epoch": 11.580475135589337,
      "grad_norm": 7.193526744842529,
      "learning_rate": 3.552440608051333e-05,
      "loss": 1.7828,
      "step": 151600
    },
    {
      "epoch": 11.588113971430754,
      "grad_norm": 6.765320777893066,
      "learning_rate": 3.551485753571156e-05,
      "loss": 1.8005,
      "step": 151700
    },
    {
      "epoch": 11.595752807272172,
      "grad_norm": 4.012053966522217,
      "learning_rate": 3.550530899090978e-05,
      "loss": 1.927,
      "step": 151800
    },
    {
      "epoch": 11.60339164311359,
      "grad_norm": 7.996554374694824,
      "learning_rate": 3.549576044610802e-05,
      "loss": 1.8222,
      "step": 151900
    },
    {
      "epoch": 11.611030478955007,
      "grad_norm": 6.835017204284668,
      "learning_rate": 3.548621190130624e-05,
      "loss": 1.8014,
      "step": 152000
    },
    {
      "epoch": 11.618669314796424,
      "grad_norm": 6.106975078582764,
      "learning_rate": 3.547666335650447e-05,
      "loss": 1.8942,
      "step": 152100
    },
    {
      "epoch": 11.626308150637843,
      "grad_norm": 11.158735275268555,
      "learning_rate": 3.54671148117027e-05,
      "loss": 1.9433,
      "step": 152200
    },
    {
      "epoch": 11.63394698647926,
      "grad_norm": 7.83474063873291,
      "learning_rate": 3.5457566266900926e-05,
      "loss": 1.8747,
      "step": 152300
    },
    {
      "epoch": 11.641585822320678,
      "grad_norm": 6.685579776763916,
      "learning_rate": 3.5448017722099155e-05,
      "loss": 1.8019,
      "step": 152400
    },
    {
      "epoch": 11.649224658162096,
      "grad_norm": 6.450292110443115,
      "learning_rate": 3.5438469177297384e-05,
      "loss": 1.7991,
      "step": 152500
    },
    {
      "epoch": 11.656863494003513,
      "grad_norm": 5.390358924865723,
      "learning_rate": 3.542892063249561e-05,
      "loss": 1.8524,
      "step": 152600
    },
    {
      "epoch": 11.664502329844932,
      "grad_norm": 7.667755603790283,
      "learning_rate": 3.5419372087693834e-05,
      "loss": 1.7969,
      "step": 152700
    },
    {
      "epoch": 11.67214116568635,
      "grad_norm": 12.549548149108887,
      "learning_rate": 3.540982354289207e-05,
      "loss": 1.8954,
      "step": 152800
    },
    {
      "epoch": 11.679780001527767,
      "grad_norm": 6.1386213302612305,
      "learning_rate": 3.540027499809029e-05,
      "loss": 1.9234,
      "step": 152900
    },
    {
      "epoch": 11.687418837369185,
      "grad_norm": 8.018194198608398,
      "learning_rate": 3.539072645328852e-05,
      "loss": 1.9679,
      "step": 153000
    },
    {
      "epoch": 11.695057673210602,
      "grad_norm": 5.355414390563965,
      "learning_rate": 3.538117790848674e-05,
      "loss": 1.8542,
      "step": 153100
    },
    {
      "epoch": 11.702696509052021,
      "grad_norm": 6.6174821853637695,
      "learning_rate": 3.537162936368498e-05,
      "loss": 1.8656,
      "step": 153200
    },
    {
      "epoch": 11.710335344893439,
      "grad_norm": 10.165875434875488,
      "learning_rate": 3.53620808188832e-05,
      "loss": 1.823,
      "step": 153300
    },
    {
      "epoch": 11.717974180734856,
      "grad_norm": 6.236189365386963,
      "learning_rate": 3.535253227408143e-05,
      "loss": 1.9186,
      "step": 153400
    },
    {
      "epoch": 11.725613016576274,
      "grad_norm": 7.411640644073486,
      "learning_rate": 3.534298372927966e-05,
      "loss": 1.895,
      "step": 153500
    },
    {
      "epoch": 11.733251852417691,
      "grad_norm": 9.6221342086792,
      "learning_rate": 3.5333435184477886e-05,
      "loss": 1.8362,
      "step": 153600
    },
    {
      "epoch": 11.740890688259109,
      "grad_norm": 8.038143157958984,
      "learning_rate": 3.5323886639676115e-05,
      "loss": 1.88,
      "step": 153700
    },
    {
      "epoch": 11.748529524100528,
      "grad_norm": 6.258163928985596,
      "learning_rate": 3.531433809487434e-05,
      "loss": 1.8093,
      "step": 153800
    },
    {
      "epoch": 11.756168359941945,
      "grad_norm": 7.028309345245361,
      "learning_rate": 3.530478955007257e-05,
      "loss": 1.858,
      "step": 153900
    },
    {
      "epoch": 11.763807195783363,
      "grad_norm": 7.980084419250488,
      "learning_rate": 3.5295241005270794e-05,
      "loss": 1.7948,
      "step": 154000
    },
    {
      "epoch": 11.77144603162478,
      "grad_norm": 7.427552700042725,
      "learning_rate": 3.528569246046903e-05,
      "loss": 1.8315,
      "step": 154100
    },
    {
      "epoch": 11.779084867466198,
      "grad_norm": 9.270488739013672,
      "learning_rate": 3.527614391566725e-05,
      "loss": 1.8886,
      "step": 154200
    },
    {
      "epoch": 11.786723703307615,
      "grad_norm": 8.649572372436523,
      "learning_rate": 3.526659537086548e-05,
      "loss": 1.8463,
      "step": 154300
    },
    {
      "epoch": 11.794362539149034,
      "grad_norm": 7.799062252044678,
      "learning_rate": 3.525704682606371e-05,
      "loss": 1.8974,
      "step": 154400
    },
    {
      "epoch": 11.802001374990452,
      "grad_norm": 5.517913341522217,
      "learning_rate": 3.524749828126194e-05,
      "loss": 1.8641,
      "step": 154500
    },
    {
      "epoch": 11.80964021083187,
      "grad_norm": 8.161224365234375,
      "learning_rate": 3.5237949736460166e-05,
      "loss": 1.8551,
      "step": 154600
    },
    {
      "epoch": 11.817279046673287,
      "grad_norm": 7.604467868804932,
      "learning_rate": 3.5228401191658395e-05,
      "loss": 1.9161,
      "step": 154700
    },
    {
      "epoch": 11.824917882514704,
      "grad_norm": 9.51728630065918,
      "learning_rate": 3.5218852646856623e-05,
      "loss": 1.9178,
      "step": 154800
    },
    {
      "epoch": 11.832556718356123,
      "grad_norm": 7.406682968139648,
      "learning_rate": 3.5209304102054845e-05,
      "loss": 1.8768,
      "step": 154900
    },
    {
      "epoch": 11.84019555419754,
      "grad_norm": 7.645480632781982,
      "learning_rate": 3.519975555725308e-05,
      "loss": 1.9373,
      "step": 155000
    },
    {
      "epoch": 11.847834390038958,
      "grad_norm": 6.708364009857178,
      "learning_rate": 3.51902070124513e-05,
      "loss": 1.9421,
      "step": 155100
    },
    {
      "epoch": 11.855473225880376,
      "grad_norm": 6.715641975402832,
      "learning_rate": 3.518065846764953e-05,
      "loss": 1.7555,
      "step": 155200
    },
    {
      "epoch": 11.863112061721793,
      "grad_norm": 7.837518215179443,
      "learning_rate": 3.517110992284776e-05,
      "loss": 1.8996,
      "step": 155300
    },
    {
      "epoch": 11.87075089756321,
      "grad_norm": 8.445080757141113,
      "learning_rate": 3.516156137804599e-05,
      "loss": 1.8073,
      "step": 155400
    },
    {
      "epoch": 11.87838973340463,
      "grad_norm": 7.235276222229004,
      "learning_rate": 3.515201283324421e-05,
      "loss": 1.8881,
      "step": 155500
    },
    {
      "epoch": 11.886028569246047,
      "grad_norm": 6.667518138885498,
      "learning_rate": 3.5142464288442446e-05,
      "loss": 1.8213,
      "step": 155600
    },
    {
      "epoch": 11.893667405087465,
      "grad_norm": 5.877161979675293,
      "learning_rate": 3.513291574364067e-05,
      "loss": 1.8593,
      "step": 155700
    },
    {
      "epoch": 11.901306240928882,
      "grad_norm": 6.899318218231201,
      "learning_rate": 3.51233671988389e-05,
      "loss": 1.842,
      "step": 155800
    },
    {
      "epoch": 11.9089450767703,
      "grad_norm": 6.84172248840332,
      "learning_rate": 3.5113818654037126e-05,
      "loss": 1.8564,
      "step": 155900
    },
    {
      "epoch": 11.916583912611719,
      "grad_norm": 5.39500617980957,
      "learning_rate": 3.5104270109235354e-05,
      "loss": 1.8756,
      "step": 156000
    },
    {
      "epoch": 11.924222748453136,
      "grad_norm": 6.769361972808838,
      "learning_rate": 3.509472156443358e-05,
      "loss": 1.9435,
      "step": 156100
    },
    {
      "epoch": 11.931861584294554,
      "grad_norm": 6.553615093231201,
      "learning_rate": 3.508517301963181e-05,
      "loss": 1.9358,
      "step": 156200
    },
    {
      "epoch": 11.939500420135971,
      "grad_norm": 8.131731033325195,
      "learning_rate": 3.507562447483004e-05,
      "loss": 1.8387,
      "step": 156300
    },
    {
      "epoch": 11.947139255977389,
      "grad_norm": 7.775845050811768,
      "learning_rate": 3.506607593002826e-05,
      "loss": 1.8912,
      "step": 156400
    },
    {
      "epoch": 11.954778091818806,
      "grad_norm": 5.324609756469727,
      "learning_rate": 3.50565273852265e-05,
      "loss": 1.7989,
      "step": 156500
    },
    {
      "epoch": 11.962416927660225,
      "grad_norm": 6.650191783905029,
      "learning_rate": 3.504697884042472e-05,
      "loss": 1.8791,
      "step": 156600
    },
    {
      "epoch": 11.970055763501643,
      "grad_norm": 8.2806396484375,
      "learning_rate": 3.503743029562295e-05,
      "loss": 1.7553,
      "step": 156700
    },
    {
      "epoch": 11.97769459934306,
      "grad_norm": 6.93882417678833,
      "learning_rate": 3.502788175082118e-05,
      "loss": 1.9273,
      "step": 156800
    },
    {
      "epoch": 11.985333435184478,
      "grad_norm": 7.399771213531494,
      "learning_rate": 3.5018333206019406e-05,
      "loss": 1.8226,
      "step": 156900
    },
    {
      "epoch": 11.992972271025895,
      "grad_norm": 7.274272918701172,
      "learning_rate": 3.500878466121763e-05,
      "loss": 1.7577,
      "step": 157000
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.8464173078536987,
      "eval_runtime": 3.0087,
      "eval_samples_per_second": 229.335,
      "eval_steps_per_second": 229.335,
      "step": 157092
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.6226534843444824,
      "eval_runtime": 55.8504,
      "eval_samples_per_second": 234.394,
      "eval_steps_per_second": 234.394,
      "step": 157092
    },
    {
      "epoch": 12.000611106867314,
      "grad_norm": 8.96702766418457,
      "learning_rate": 3.499923611641586e-05,
      "loss": 1.7703,
      "step": 157100
    },
    {
      "epoch": 12.008249942708732,
      "grad_norm": 9.307470321655273,
      "learning_rate": 3.4989687571614085e-05,
      "loss": 1.7426,
      "step": 157200
    },
    {
      "epoch": 12.01588877855015,
      "grad_norm": 8.802338600158691,
      "learning_rate": 3.4980139026812314e-05,
      "loss": 1.8527,
      "step": 157300
    },
    {
      "epoch": 12.023527614391567,
      "grad_norm": 6.798205375671387,
      "learning_rate": 3.497059048201054e-05,
      "loss": 1.8703,
      "step": 157400
    },
    {
      "epoch": 12.031166450232984,
      "grad_norm": 6.10877799987793,
      "learning_rate": 3.496104193720877e-05,
      "loss": 1.8764,
      "step": 157500
    },
    {
      "epoch": 12.038805286074401,
      "grad_norm": 7.972250938415527,
      "learning_rate": 3.4951493392407e-05,
      "loss": 1.919,
      "step": 157600
    },
    {
      "epoch": 12.04644412191582,
      "grad_norm": 7.327376365661621,
      "learning_rate": 3.494194484760523e-05,
      "loss": 1.8652,
      "step": 157700
    },
    {
      "epoch": 12.054082957757238,
      "grad_norm": 7.871817111968994,
      "learning_rate": 3.493239630280346e-05,
      "loss": 1.9116,
      "step": 157800
    },
    {
      "epoch": 12.061721793598656,
      "grad_norm": 10.640113830566406,
      "learning_rate": 3.492284775800168e-05,
      "loss": 1.8184,
      "step": 157900
    },
    {
      "epoch": 12.069360629440073,
      "grad_norm": 7.051883220672607,
      "learning_rate": 3.4913299213199915e-05,
      "loss": 1.8202,
      "step": 158000
    },
    {
      "epoch": 12.07699946528149,
      "grad_norm": 7.287295341491699,
      "learning_rate": 3.4903750668398136e-05,
      "loss": 1.8208,
      "step": 158100
    },
    {
      "epoch": 12.08463830112291,
      "grad_norm": 7.00303840637207,
      "learning_rate": 3.4894202123596365e-05,
      "loss": 1.8561,
      "step": 158200
    },
    {
      "epoch": 12.092277136964327,
      "grad_norm": 7.264316082000732,
      "learning_rate": 3.4884653578794594e-05,
      "loss": 1.7246,
      "step": 158300
    },
    {
      "epoch": 12.099915972805745,
      "grad_norm": 8.093740463256836,
      "learning_rate": 3.487510503399282e-05,
      "loss": 1.9204,
      "step": 158400
    },
    {
      "epoch": 12.107554808647162,
      "grad_norm": 9.169230461120605,
      "learning_rate": 3.486555648919105e-05,
      "loss": 1.7641,
      "step": 158500
    },
    {
      "epoch": 12.11519364448858,
      "grad_norm": 7.278743267059326,
      "learning_rate": 3.485600794438927e-05,
      "loss": 1.8275,
      "step": 158600
    },
    {
      "epoch": 12.122832480329997,
      "grad_norm": 6.939357280731201,
      "learning_rate": 3.484645939958751e-05,
      "loss": 1.7628,
      "step": 158700
    },
    {
      "epoch": 12.130471316171416,
      "grad_norm": 6.861677169799805,
      "learning_rate": 3.483691085478573e-05,
      "loss": 1.8959,
      "step": 158800
    },
    {
      "epoch": 12.138110152012834,
      "grad_norm": 8.102751731872559,
      "learning_rate": 3.482736230998396e-05,
      "loss": 1.8566,
      "step": 158900
    },
    {
      "epoch": 12.145748987854251,
      "grad_norm": 11.02625846862793,
      "learning_rate": 3.481781376518219e-05,
      "loss": 1.8107,
      "step": 159000
    },
    {
      "epoch": 12.153387823695669,
      "grad_norm": 8.012741088867188,
      "learning_rate": 3.480826522038042e-05,
      "loss": 1.9177,
      "step": 159100
    },
    {
      "epoch": 12.161026659537086,
      "grad_norm": 6.9142165184021,
      "learning_rate": 3.479871667557864e-05,
      "loss": 1.9162,
      "step": 159200
    },
    {
      "epoch": 12.168665495378505,
      "grad_norm": 3.5240702629089355,
      "learning_rate": 3.4789168130776874e-05,
      "loss": 1.8534,
      "step": 159300
    },
    {
      "epoch": 12.176304331219923,
      "grad_norm": 6.56785774230957,
      "learning_rate": 3.4779619585975096e-05,
      "loss": 1.8228,
      "step": 159400
    },
    {
      "epoch": 12.18394316706134,
      "grad_norm": 7.999584197998047,
      "learning_rate": 3.4770071041173325e-05,
      "loss": 1.7237,
      "step": 159500
    },
    {
      "epoch": 12.191582002902758,
      "grad_norm": 4.836167812347412,
      "learning_rate": 3.476052249637155e-05,
      "loss": 1.8516,
      "step": 159600
    },
    {
      "epoch": 12.199220838744175,
      "grad_norm": 8.198348045349121,
      "learning_rate": 3.475097395156978e-05,
      "loss": 1.8159,
      "step": 159700
    },
    {
      "epoch": 12.206859674585592,
      "grad_norm": 5.950707912445068,
      "learning_rate": 3.474142540676801e-05,
      "loss": 1.784,
      "step": 159800
    },
    {
      "epoch": 12.214498510427012,
      "grad_norm": 8.12209701538086,
      "learning_rate": 3.473187686196624e-05,
      "loss": 1.8033,
      "step": 159900
    },
    {
      "epoch": 12.222137346268429,
      "grad_norm": 7.432234287261963,
      "learning_rate": 3.472232831716447e-05,
      "loss": 1.7958,
      "step": 160000
    },
    {
      "epoch": 12.229776182109847,
      "grad_norm": 6.944311141967773,
      "learning_rate": 3.471277977236269e-05,
      "loss": 1.8584,
      "step": 160100
    },
    {
      "epoch": 12.237415017951264,
      "grad_norm": 5.97017240524292,
      "learning_rate": 3.4703231227560925e-05,
      "loss": 1.9025,
      "step": 160200
    },
    {
      "epoch": 12.245053853792681,
      "grad_norm": 9.490880966186523,
      "learning_rate": 3.469368268275915e-05,
      "loss": 1.8546,
      "step": 160300
    },
    {
      "epoch": 12.252692689634099,
      "grad_norm": 7.6901631355285645,
      "learning_rate": 3.4684134137957376e-05,
      "loss": 1.7975,
      "step": 160400
    },
    {
      "epoch": 12.260331525475518,
      "grad_norm": 5.996103763580322,
      "learning_rate": 3.4674585593155605e-05,
      "loss": 1.8901,
      "step": 160500
    },
    {
      "epoch": 12.267970361316936,
      "grad_norm": 8.381311416625977,
      "learning_rate": 3.4665037048353833e-05,
      "loss": 1.8246,
      "step": 160600
    },
    {
      "epoch": 12.275609197158353,
      "grad_norm": 7.8689703941345215,
      "learning_rate": 3.4655488503552055e-05,
      "loss": 1.9212,
      "step": 160700
    },
    {
      "epoch": 12.28324803299977,
      "grad_norm": 10.675178527832031,
      "learning_rate": 3.464593995875029e-05,
      "loss": 1.8631,
      "step": 160800
    },
    {
      "epoch": 12.290886868841188,
      "grad_norm": 10.260869026184082,
      "learning_rate": 3.463639141394851e-05,
      "loss": 1.7587,
      "step": 160900
    },
    {
      "epoch": 12.298525704682607,
      "grad_norm": 6.538741111755371,
      "learning_rate": 3.462684286914674e-05,
      "loss": 1.8315,
      "step": 161000
    },
    {
      "epoch": 12.306164540524025,
      "grad_norm": 8.087081909179688,
      "learning_rate": 3.461729432434497e-05,
      "loss": 1.9084,
      "step": 161100
    },
    {
      "epoch": 12.313803376365442,
      "grad_norm": 6.954928398132324,
      "learning_rate": 3.46077457795432e-05,
      "loss": 1.7518,
      "step": 161200
    },
    {
      "epoch": 12.32144221220686,
      "grad_norm": 5.966416358947754,
      "learning_rate": 3.459819723474143e-05,
      "loss": 1.8864,
      "step": 161300
    },
    {
      "epoch": 12.329081048048277,
      "grad_norm": 7.680028915405273,
      "learning_rate": 3.4588648689939656e-05,
      "loss": 1.7934,
      "step": 161400
    },
    {
      "epoch": 12.336719883889696,
      "grad_norm": 7.007966995239258,
      "learning_rate": 3.4579100145137885e-05,
      "loss": 1.7702,
      "step": 161500
    },
    {
      "epoch": 12.344358719731114,
      "grad_norm": 9.548535346984863,
      "learning_rate": 3.456955160033611e-05,
      "loss": 1.8077,
      "step": 161600
    },
    {
      "epoch": 12.351997555572531,
      "grad_norm": 5.194395542144775,
      "learning_rate": 3.456000305553434e-05,
      "loss": 1.861,
      "step": 161700
    },
    {
      "epoch": 12.359636391413948,
      "grad_norm": 9.179131507873535,
      "learning_rate": 3.4550454510732564e-05,
      "loss": 1.8026,
      "step": 161800
    },
    {
      "epoch": 12.367275227255366,
      "grad_norm": 6.801276683807373,
      "learning_rate": 3.454090596593079e-05,
      "loss": 1.891,
      "step": 161900
    },
    {
      "epoch": 12.374914063096783,
      "grad_norm": 8.896690368652344,
      "learning_rate": 3.453135742112902e-05,
      "loss": 1.8366,
      "step": 162000
    },
    {
      "epoch": 12.382552898938203,
      "grad_norm": 6.637382984161377,
      "learning_rate": 3.452180887632725e-05,
      "loss": 1.8063,
      "step": 162100
    },
    {
      "epoch": 12.39019173477962,
      "grad_norm": 7.431937217712402,
      "learning_rate": 3.451226033152547e-05,
      "loss": 1.8592,
      "step": 162200
    },
    {
      "epoch": 12.397830570621037,
      "grad_norm": 9.117759704589844,
      "learning_rate": 3.450271178672371e-05,
      "loss": 1.8975,
      "step": 162300
    },
    {
      "epoch": 12.405469406462455,
      "grad_norm": 6.897256374359131,
      "learning_rate": 3.4493163241921936e-05,
      "loss": 1.8138,
      "step": 162400
    },
    {
      "epoch": 12.413108242303872,
      "grad_norm": 7.295255184173584,
      "learning_rate": 3.448361469712016e-05,
      "loss": 1.8025,
      "step": 162500
    },
    {
      "epoch": 12.42074707814529,
      "grad_norm": 6.40500545501709,
      "learning_rate": 3.4474066152318394e-05,
      "loss": 1.8963,
      "step": 162600
    },
    {
      "epoch": 12.428385913986709,
      "grad_norm": 6.160856246948242,
      "learning_rate": 3.4464517607516616e-05,
      "loss": 1.8331,
      "step": 162700
    },
    {
      "epoch": 12.436024749828126,
      "grad_norm": 7.307678699493408,
      "learning_rate": 3.4454969062714844e-05,
      "loss": 1.852,
      "step": 162800
    },
    {
      "epoch": 12.443663585669544,
      "grad_norm": 6.75320291519165,
      "learning_rate": 3.444542051791307e-05,
      "loss": 1.8309,
      "step": 162900
    },
    {
      "epoch": 12.451302421510961,
      "grad_norm": 7.022307872772217,
      "learning_rate": 3.44358719731113e-05,
      "loss": 1.8481,
      "step": 163000
    },
    {
      "epoch": 12.458941257352379,
      "grad_norm": 8.330829620361328,
      "learning_rate": 3.4426323428309524e-05,
      "loss": 1.8767,
      "step": 163100
    },
    {
      "epoch": 12.466580093193798,
      "grad_norm": 9.48421859741211,
      "learning_rate": 3.441677488350776e-05,
      "loss": 1.8533,
      "step": 163200
    },
    {
      "epoch": 12.474218929035215,
      "grad_norm": 6.860836505889893,
      "learning_rate": 3.440722633870598e-05,
      "loss": 1.9337,
      "step": 163300
    },
    {
      "epoch": 12.481857764876633,
      "grad_norm": 6.761324882507324,
      "learning_rate": 3.439767779390421e-05,
      "loss": 1.9406,
      "step": 163400
    },
    {
      "epoch": 12.48949660071805,
      "grad_norm": 7.769667148590088,
      "learning_rate": 3.438812924910244e-05,
      "loss": 1.8722,
      "step": 163500
    },
    {
      "epoch": 12.497135436559468,
      "grad_norm": 7.515331745147705,
      "learning_rate": 3.437858070430067e-05,
      "loss": 1.8257,
      "step": 163600
    },
    {
      "epoch": 12.504774272400887,
      "grad_norm": 6.526425361633301,
      "learning_rate": 3.4369032159498896e-05,
      "loss": 1.9114,
      "step": 163700
    },
    {
      "epoch": 12.512413108242304,
      "grad_norm": 5.8581461906433105,
      "learning_rate": 3.4359483614697125e-05,
      "loss": 1.8128,
      "step": 163800
    },
    {
      "epoch": 12.520051944083722,
      "grad_norm": 7.905884742736816,
      "learning_rate": 3.434993506989535e-05,
      "loss": 1.8077,
      "step": 163900
    },
    {
      "epoch": 12.52769077992514,
      "grad_norm": 8.58720588684082,
      "learning_rate": 3.4340386525093575e-05,
      "loss": 1.7796,
      "step": 164000
    },
    {
      "epoch": 12.535329615766557,
      "grad_norm": 6.833446979522705,
      "learning_rate": 3.433083798029181e-05,
      "loss": 1.8401,
      "step": 164100
    },
    {
      "epoch": 12.542968451607974,
      "grad_norm": 5.970796585083008,
      "learning_rate": 3.432128943549003e-05,
      "loss": 1.9696,
      "step": 164200
    },
    {
      "epoch": 12.550607287449393,
      "grad_norm": 7.462148189544678,
      "learning_rate": 3.431174089068826e-05,
      "loss": 1.8527,
      "step": 164300
    },
    {
      "epoch": 12.558246123290811,
      "grad_norm": 7.137752532958984,
      "learning_rate": 3.430219234588648e-05,
      "loss": 1.7472,
      "step": 164400
    },
    {
      "epoch": 12.565884959132228,
      "grad_norm": 6.9783735275268555,
      "learning_rate": 3.429264380108472e-05,
      "loss": 1.7804,
      "step": 164500
    },
    {
      "epoch": 12.573523794973646,
      "grad_norm": 7.194533824920654,
      "learning_rate": 3.428309525628294e-05,
      "loss": 1.7885,
      "step": 164600
    },
    {
      "epoch": 12.581162630815063,
      "grad_norm": 7.200197696685791,
      "learning_rate": 3.427354671148117e-05,
      "loss": 1.8003,
      "step": 164700
    },
    {
      "epoch": 12.58880146665648,
      "grad_norm": 8.952008247375488,
      "learning_rate": 3.42639981666794e-05,
      "loss": 1.8888,
      "step": 164800
    },
    {
      "epoch": 12.5964403024979,
      "grad_norm": 7.414583206176758,
      "learning_rate": 3.425444962187763e-05,
      "loss": 1.8808,
      "step": 164900
    },
    {
      "epoch": 12.604079138339317,
      "grad_norm": 10.212509155273438,
      "learning_rate": 3.4244901077075855e-05,
      "loss": 1.8095,
      "step": 165000
    },
    {
      "epoch": 12.611717974180735,
      "grad_norm": 6.772115230560303,
      "learning_rate": 3.4235352532274084e-05,
      "loss": 1.9252,
      "step": 165100
    },
    {
      "epoch": 12.619356810022152,
      "grad_norm": 10.163904190063477,
      "learning_rate": 3.422580398747231e-05,
      "loss": 1.8336,
      "step": 165200
    },
    {
      "epoch": 12.62699564586357,
      "grad_norm": 6.240813732147217,
      "learning_rate": 3.4216255442670535e-05,
      "loss": 1.7629,
      "step": 165300
    },
    {
      "epoch": 12.634634481704989,
      "grad_norm": 6.90096378326416,
      "learning_rate": 3.420670689786877e-05,
      "loss": 1.7384,
      "step": 165400
    },
    {
      "epoch": 12.642273317546406,
      "grad_norm": 10.762445449829102,
      "learning_rate": 3.419715835306699e-05,
      "loss": 1.8484,
      "step": 165500
    },
    {
      "epoch": 12.649912153387824,
      "grad_norm": 6.482374668121338,
      "learning_rate": 3.418760980826522e-05,
      "loss": 1.7703,
      "step": 165600
    },
    {
      "epoch": 12.657550989229241,
      "grad_norm": 8.014102935791016,
      "learning_rate": 3.417806126346345e-05,
      "loss": 1.8805,
      "step": 165700
    },
    {
      "epoch": 12.665189825070659,
      "grad_norm": 7.429783821105957,
      "learning_rate": 3.416851271866168e-05,
      "loss": 1.8387,
      "step": 165800
    },
    {
      "epoch": 12.672828660912078,
      "grad_norm": 7.461959362030029,
      "learning_rate": 3.41589641738599e-05,
      "loss": 1.8052,
      "step": 165900
    },
    {
      "epoch": 12.680467496753495,
      "grad_norm": 7.830836296081543,
      "learning_rate": 3.4149415629058135e-05,
      "loss": 1.8086,
      "step": 166000
    },
    {
      "epoch": 12.688106332594913,
      "grad_norm": 6.943875312805176,
      "learning_rate": 3.413986708425636e-05,
      "loss": 1.8832,
      "step": 166100
    },
    {
      "epoch": 12.69574516843633,
      "grad_norm": 9.736698150634766,
      "learning_rate": 3.4130318539454586e-05,
      "loss": 1.8081,
      "step": 166200
    },
    {
      "epoch": 12.703384004277748,
      "grad_norm": 6.7307515144348145,
      "learning_rate": 3.4120769994652815e-05,
      "loss": 1.9201,
      "step": 166300
    },
    {
      "epoch": 12.711022840119165,
      "grad_norm": 9.698387145996094,
      "learning_rate": 3.4111221449851043e-05,
      "loss": 1.7412,
      "step": 166400
    },
    {
      "epoch": 12.718661675960584,
      "grad_norm": 9.663301467895508,
      "learning_rate": 3.410167290504927e-05,
      "loss": 1.8045,
      "step": 166500
    },
    {
      "epoch": 12.726300511802002,
      "grad_norm": 8.087865829467773,
      "learning_rate": 3.40921243602475e-05,
      "loss": 1.783,
      "step": 166600
    },
    {
      "epoch": 12.73393934764342,
      "grad_norm": 6.256688117980957,
      "learning_rate": 3.408257581544573e-05,
      "loss": 1.8944,
      "step": 166700
    },
    {
      "epoch": 12.741578183484837,
      "grad_norm": 12.486039161682129,
      "learning_rate": 3.407302727064395e-05,
      "loss": 1.9433,
      "step": 166800
    },
    {
      "epoch": 12.749217019326254,
      "grad_norm": 6.931737422943115,
      "learning_rate": 3.406347872584219e-05,
      "loss": 1.9301,
      "step": 166900
    },
    {
      "epoch": 12.756855855167672,
      "grad_norm": 7.06089973449707,
      "learning_rate": 3.405393018104041e-05,
      "loss": 1.7939,
      "step": 167000
    },
    {
      "epoch": 12.76449469100909,
      "grad_norm": 7.967402458190918,
      "learning_rate": 3.404438163623864e-05,
      "loss": 1.8651,
      "step": 167100
    },
    {
      "epoch": 12.772133526850508,
      "grad_norm": 6.125797271728516,
      "learning_rate": 3.4034833091436866e-05,
      "loss": 1.8286,
      "step": 167200
    },
    {
      "epoch": 12.779772362691926,
      "grad_norm": 8.154086112976074,
      "learning_rate": 3.4025284546635095e-05,
      "loss": 1.8891,
      "step": 167300
    },
    {
      "epoch": 12.787411198533343,
      "grad_norm": 8.318978309631348,
      "learning_rate": 3.4015736001833324e-05,
      "loss": 1.8332,
      "step": 167400
    },
    {
      "epoch": 12.79505003437476,
      "grad_norm": 7.93678092956543,
      "learning_rate": 3.400618745703155e-05,
      "loss": 1.816,
      "step": 167500
    },
    {
      "epoch": 12.80268887021618,
      "grad_norm": 7.270356178283691,
      "learning_rate": 3.399663891222978e-05,
      "loss": 1.7613,
      "step": 167600
    },
    {
      "epoch": 12.810327706057597,
      "grad_norm": 8.544879913330078,
      "learning_rate": 3.3987090367428e-05,
      "loss": 1.7467,
      "step": 167700
    },
    {
      "epoch": 12.817966541899015,
      "grad_norm": 5.589412212371826,
      "learning_rate": 3.397754182262624e-05,
      "loss": 1.867,
      "step": 167800
    },
    {
      "epoch": 12.825605377740432,
      "grad_norm": 9.533681869506836,
      "learning_rate": 3.396799327782446e-05,
      "loss": 1.7738,
      "step": 167900
    },
    {
      "epoch": 12.83324421358185,
      "grad_norm": 7.057750225067139,
      "learning_rate": 3.395844473302269e-05,
      "loss": 1.8168,
      "step": 168000
    },
    {
      "epoch": 12.840883049423267,
      "grad_norm": 9.86406135559082,
      "learning_rate": 3.394889618822092e-05,
      "loss": 1.8325,
      "step": 168100
    },
    {
      "epoch": 12.848521885264686,
      "grad_norm": 8.745585441589355,
      "learning_rate": 3.3939347643419146e-05,
      "loss": 1.7969,
      "step": 168200
    },
    {
      "epoch": 12.856160721106104,
      "grad_norm": 6.316221237182617,
      "learning_rate": 3.392979909861737e-05,
      "loss": 1.9211,
      "step": 168300
    },
    {
      "epoch": 12.863799556947521,
      "grad_norm": 8.935234069824219,
      "learning_rate": 3.3920250553815604e-05,
      "loss": 1.7905,
      "step": 168400
    },
    {
      "epoch": 12.871438392788939,
      "grad_norm": 9.016255378723145,
      "learning_rate": 3.3910702009013826e-05,
      "loss": 1.7813,
      "step": 168500
    },
    {
      "epoch": 12.879077228630356,
      "grad_norm": 7.643639087677002,
      "learning_rate": 3.3901153464212054e-05,
      "loss": 1.9157,
      "step": 168600
    },
    {
      "epoch": 12.886716064471775,
      "grad_norm": 7.24433708190918,
      "learning_rate": 3.389160491941028e-05,
      "loss": 1.8675,
      "step": 168700
    },
    {
      "epoch": 12.894354900313193,
      "grad_norm": 7.19793176651001,
      "learning_rate": 3.388205637460851e-05,
      "loss": 1.8371,
      "step": 168800
    },
    {
      "epoch": 12.90199373615461,
      "grad_norm": 7.207381725311279,
      "learning_rate": 3.387250782980674e-05,
      "loss": 1.8255,
      "step": 168900
    },
    {
      "epoch": 12.909632571996028,
      "grad_norm": 6.689040184020996,
      "learning_rate": 3.386295928500497e-05,
      "loss": 1.9325,
      "step": 169000
    },
    {
      "epoch": 12.917271407837445,
      "grad_norm": 7.834989070892334,
      "learning_rate": 3.38534107402032e-05,
      "loss": 1.8637,
      "step": 169100
    },
    {
      "epoch": 12.924910243678863,
      "grad_norm": 7.609443664550781,
      "learning_rate": 3.384386219540142e-05,
      "loss": 1.7951,
      "step": 169200
    },
    {
      "epoch": 12.932549079520282,
      "grad_norm": 7.591589450836182,
      "learning_rate": 3.3834313650599655e-05,
      "loss": 1.8756,
      "step": 169300
    },
    {
      "epoch": 12.9401879153617,
      "grad_norm": 5.2344160079956055,
      "learning_rate": 3.382476510579788e-05,
      "loss": 1.8056,
      "step": 169400
    },
    {
      "epoch": 12.947826751203117,
      "grad_norm": 8.155325889587402,
      "learning_rate": 3.3815216560996106e-05,
      "loss": 1.8262,
      "step": 169500
    },
    {
      "epoch": 12.955465587044534,
      "grad_norm": 7.871623992919922,
      "learning_rate": 3.3805668016194335e-05,
      "loss": 1.8856,
      "step": 169600
    },
    {
      "epoch": 12.963104422885952,
      "grad_norm": 7.267577171325684,
      "learning_rate": 3.379611947139256e-05,
      "loss": 1.9629,
      "step": 169700
    },
    {
      "epoch": 12.97074325872737,
      "grad_norm": 6.6280059814453125,
      "learning_rate": 3.3786570926590785e-05,
      "loss": 1.7806,
      "step": 169800
    },
    {
      "epoch": 12.978382094568788,
      "grad_norm": 7.459558486938477,
      "learning_rate": 3.377702238178902e-05,
      "loss": 1.9033,
      "step": 169900
    },
    {
      "epoch": 12.986020930410206,
      "grad_norm": 4.39229154586792,
      "learning_rate": 3.376747383698724e-05,
      "loss": 1.8849,
      "step": 170000
    },
    {
      "epoch": 12.993659766251623,
      "grad_norm": 7.97425651550293,
      "learning_rate": 3.375792529218547e-05,
      "loss": 1.8422,
      "step": 170100
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.8343966007232666,
      "eval_runtime": 2.9631,
      "eval_samples_per_second": 232.861,
      "eval_steps_per_second": 232.861,
      "step": 170183
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.6061362028121948,
      "eval_runtime": 56.4495,
      "eval_samples_per_second": 231.906,
      "eval_steps_per_second": 231.906,
      "step": 170183
    },
    {
      "epoch": 13.00129860209304,
      "grad_norm": 5.792617321014404,
      "learning_rate": 3.37483767473837e-05,
      "loss": 1.8909,
      "step": 170200
    },
    {
      "epoch": 13.008937437934458,
      "grad_norm": 7.518474102020264,
      "learning_rate": 3.373882820258193e-05,
      "loss": 1.864,
      "step": 170300
    },
    {
      "epoch": 13.016576273775877,
      "grad_norm": 9.367551803588867,
      "learning_rate": 3.372927965778016e-05,
      "loss": 1.7481,
      "step": 170400
    },
    {
      "epoch": 13.024215109617295,
      "grad_norm": 5.499079704284668,
      "learning_rate": 3.371973111297838e-05,
      "loss": 1.7117,
      "step": 170500
    },
    {
      "epoch": 13.031853945458712,
      "grad_norm": 7.209479331970215,
      "learning_rate": 3.3710182568176615e-05,
      "loss": 1.9112,
      "step": 170600
    },
    {
      "epoch": 13.03949278130013,
      "grad_norm": 7.913454055786133,
      "learning_rate": 3.3700634023374837e-05,
      "loss": 1.8351,
      "step": 170700
    },
    {
      "epoch": 13.047131617141547,
      "grad_norm": 12.257139205932617,
      "learning_rate": 3.3691085478573065e-05,
      "loss": 1.8133,
      "step": 170800
    },
    {
      "epoch": 13.054770452982966,
      "grad_norm": 6.429915428161621,
      "learning_rate": 3.3681536933771294e-05,
      "loss": 1.8619,
      "step": 170900
    },
    {
      "epoch": 13.062409288824384,
      "grad_norm": 6.7106451988220215,
      "learning_rate": 3.367198838896952e-05,
      "loss": 1.7741,
      "step": 171000
    },
    {
      "epoch": 13.070048124665801,
      "grad_norm": 7.624319076538086,
      "learning_rate": 3.3662439844167745e-05,
      "loss": 1.8587,
      "step": 171100
    },
    {
      "epoch": 13.077686960507219,
      "grad_norm": 8.495088577270508,
      "learning_rate": 3.365289129936598e-05,
      "loss": 1.8804,
      "step": 171200
    },
    {
      "epoch": 13.085325796348636,
      "grad_norm": 6.5813374519348145,
      "learning_rate": 3.364334275456421e-05,
      "loss": 1.7349,
      "step": 171300
    },
    {
      "epoch": 13.092964632190053,
      "grad_norm": 8.687350273132324,
      "learning_rate": 3.363379420976243e-05,
      "loss": 1.8474,
      "step": 171400
    },
    {
      "epoch": 13.100603468031473,
      "grad_norm": 7.836844444274902,
      "learning_rate": 3.3624245664960666e-05,
      "loss": 1.8049,
      "step": 171500
    },
    {
      "epoch": 13.10824230387289,
      "grad_norm": 6.815825939178467,
      "learning_rate": 3.361469712015889e-05,
      "loss": 1.8733,
      "step": 171600
    },
    {
      "epoch": 13.115881139714308,
      "grad_norm": 7.362214088439941,
      "learning_rate": 3.360514857535712e-05,
      "loss": 1.8322,
      "step": 171700
    },
    {
      "epoch": 13.123519975555725,
      "grad_norm": 6.286625385284424,
      "learning_rate": 3.3595600030555345e-05,
      "loss": 1.9064,
      "step": 171800
    },
    {
      "epoch": 13.131158811397142,
      "grad_norm": 5.851914405822754,
      "learning_rate": 3.3586051485753574e-05,
      "loss": 1.7759,
      "step": 171900
    },
    {
      "epoch": 13.138797647238562,
      "grad_norm": 10.614632606506348,
      "learning_rate": 3.3576502940951796e-05,
      "loss": 1.8645,
      "step": 172000
    },
    {
      "epoch": 13.14643648307998,
      "grad_norm": 8.579380989074707,
      "learning_rate": 3.356695439615003e-05,
      "loss": 1.7526,
      "step": 172100
    },
    {
      "epoch": 13.154075318921397,
      "grad_norm": 5.878569602966309,
      "learning_rate": 3.3557405851348253e-05,
      "loss": 1.8833,
      "step": 172200
    },
    {
      "epoch": 13.161714154762814,
      "grad_norm": 6.960598945617676,
      "learning_rate": 3.354785730654648e-05,
      "loss": 1.8059,
      "step": 172300
    },
    {
      "epoch": 13.169352990604231,
      "grad_norm": 9.323053359985352,
      "learning_rate": 3.353830876174471e-05,
      "loss": 1.9411,
      "step": 172400
    },
    {
      "epoch": 13.176991826445649,
      "grad_norm": 6.870269298553467,
      "learning_rate": 3.352876021694294e-05,
      "loss": 1.8027,
      "step": 172500
    },
    {
      "epoch": 13.184630662287068,
      "grad_norm": 6.559760570526123,
      "learning_rate": 3.351921167214117e-05,
      "loss": 1.7843,
      "step": 172600
    },
    {
      "epoch": 13.192269498128486,
      "grad_norm": 6.932727336883545,
      "learning_rate": 3.35096631273394e-05,
      "loss": 1.7915,
      "step": 172700
    },
    {
      "epoch": 13.199908333969903,
      "grad_norm": 7.400261878967285,
      "learning_rate": 3.3500114582537626e-05,
      "loss": 1.8397,
      "step": 172800
    },
    {
      "epoch": 13.20754716981132,
      "grad_norm": 9.217691421508789,
      "learning_rate": 3.349056603773585e-05,
      "loss": 1.8096,
      "step": 172900
    },
    {
      "epoch": 13.215186005652738,
      "grad_norm": 5.877747058868408,
      "learning_rate": 3.348101749293408e-05,
      "loss": 1.8085,
      "step": 173000
    },
    {
      "epoch": 13.222824841494157,
      "grad_norm": 9.75544548034668,
      "learning_rate": 3.3471468948132305e-05,
      "loss": 1.8385,
      "step": 173100
    },
    {
      "epoch": 13.230463677335575,
      "grad_norm": 5.433610439300537,
      "learning_rate": 3.3461920403330534e-05,
      "loss": 1.8582,
      "step": 173200
    },
    {
      "epoch": 13.238102513176992,
      "grad_norm": 6.505154609680176,
      "learning_rate": 3.345237185852876e-05,
      "loss": 1.8562,
      "step": 173300
    },
    {
      "epoch": 13.24574134901841,
      "grad_norm": 7.514537334442139,
      "learning_rate": 3.344282331372699e-05,
      "loss": 1.893,
      "step": 173400
    },
    {
      "epoch": 13.253380184859827,
      "grad_norm": 8.758321762084961,
      "learning_rate": 3.343327476892521e-05,
      "loss": 1.8135,
      "step": 173500
    },
    {
      "epoch": 13.261019020701244,
      "grad_norm": 6.764735698699951,
      "learning_rate": 3.342372622412345e-05,
      "loss": 1.7838,
      "step": 173600
    },
    {
      "epoch": 13.268657856542664,
      "grad_norm": 9.31310749053955,
      "learning_rate": 3.341417767932167e-05,
      "loss": 1.8895,
      "step": 173700
    },
    {
      "epoch": 13.276296692384081,
      "grad_norm": 7.004260063171387,
      "learning_rate": 3.34046291345199e-05,
      "loss": 1.8913,
      "step": 173800
    },
    {
      "epoch": 13.283935528225499,
      "grad_norm": 8.369721412658691,
      "learning_rate": 3.339508058971813e-05,
      "loss": 1.8605,
      "step": 173900
    },
    {
      "epoch": 13.291574364066916,
      "grad_norm": 5.972111225128174,
      "learning_rate": 3.3385532044916356e-05,
      "loss": 1.8187,
      "step": 174000
    },
    {
      "epoch": 13.299213199908333,
      "grad_norm": 7.14736795425415,
      "learning_rate": 3.3375983500114585e-05,
      "loss": 1.7478,
      "step": 174100
    },
    {
      "epoch": 13.306852035749753,
      "grad_norm": 10.46445083618164,
      "learning_rate": 3.3366434955312814e-05,
      "loss": 1.9426,
      "step": 174200
    },
    {
      "epoch": 13.31449087159117,
      "grad_norm": 5.213400840759277,
      "learning_rate": 3.335688641051104e-05,
      "loss": 1.8453,
      "step": 174300
    },
    {
      "epoch": 13.322129707432588,
      "grad_norm": 5.2749223709106445,
      "learning_rate": 3.3347337865709264e-05,
      "loss": 1.7251,
      "step": 174400
    },
    {
      "epoch": 13.329768543274005,
      "grad_norm": 11.426427841186523,
      "learning_rate": 3.33377893209075e-05,
      "loss": 1.8979,
      "step": 174500
    },
    {
      "epoch": 13.337407379115422,
      "grad_norm": 7.219330310821533,
      "learning_rate": 3.332824077610572e-05,
      "loss": 1.8521,
      "step": 174600
    },
    {
      "epoch": 13.34504621495684,
      "grad_norm": 8.758378982543945,
      "learning_rate": 3.331869223130395e-05,
      "loss": 1.7167,
      "step": 174700
    },
    {
      "epoch": 13.352685050798259,
      "grad_norm": 7.395586967468262,
      "learning_rate": 3.330914368650218e-05,
      "loss": 1.8045,
      "step": 174800
    },
    {
      "epoch": 13.360323886639677,
      "grad_norm": 8.1490478515625,
      "learning_rate": 3.329959514170041e-05,
      "loss": 1.7531,
      "step": 174900
    },
    {
      "epoch": 13.367962722481094,
      "grad_norm": 8.670279502868652,
      "learning_rate": 3.329004659689863e-05,
      "loss": 1.831,
      "step": 175000
    },
    {
      "epoch": 13.375601558322511,
      "grad_norm": 6.784622669219971,
      "learning_rate": 3.3280498052096865e-05,
      "loss": 1.8385,
      "step": 175100
    },
    {
      "epoch": 13.383240394163929,
      "grad_norm": 6.415031433105469,
      "learning_rate": 3.327094950729509e-05,
      "loss": 1.8312,
      "step": 175200
    },
    {
      "epoch": 13.390879230005346,
      "grad_norm": 6.213111877441406,
      "learning_rate": 3.3261400962493316e-05,
      "loss": 1.7748,
      "step": 175300
    },
    {
      "epoch": 13.398518065846766,
      "grad_norm": 6.4257283210754395,
      "learning_rate": 3.325185241769155e-05,
      "loss": 1.8768,
      "step": 175400
    },
    {
      "epoch": 13.406156901688183,
      "grad_norm": 6.1745758056640625,
      "learning_rate": 3.324230387288977e-05,
      "loss": 1.8646,
      "step": 175500
    },
    {
      "epoch": 13.4137957375296,
      "grad_norm": 8.59897232055664,
      "learning_rate": 3.3232755328088e-05,
      "loss": 1.8127,
      "step": 175600
    },
    {
      "epoch": 13.421434573371018,
      "grad_norm": 5.7393646240234375,
      "learning_rate": 3.322320678328623e-05,
      "loss": 1.86,
      "step": 175700
    },
    {
      "epoch": 13.429073409212435,
      "grad_norm": 6.173426151275635,
      "learning_rate": 3.321365823848446e-05,
      "loss": 1.7841,
      "step": 175800
    },
    {
      "epoch": 13.436712245053855,
      "grad_norm": 6.8583807945251465,
      "learning_rate": 3.320410969368268e-05,
      "loss": 1.8785,
      "step": 175900
    },
    {
      "epoch": 13.444351080895272,
      "grad_norm": 7.0481672286987305,
      "learning_rate": 3.319456114888091e-05,
      "loss": 1.7994,
      "step": 176000
    },
    {
      "epoch": 13.45198991673669,
      "grad_norm": 6.8078694343566895,
      "learning_rate": 3.318501260407914e-05,
      "loss": 1.8434,
      "step": 176100
    },
    {
      "epoch": 13.459628752578107,
      "grad_norm": 6.480025291442871,
      "learning_rate": 3.317546405927737e-05,
      "loss": 1.8238,
      "step": 176200
    },
    {
      "epoch": 13.467267588419524,
      "grad_norm": 7.112099647521973,
      "learning_rate": 3.3165915514475596e-05,
      "loss": 1.7206,
      "step": 176300
    },
    {
      "epoch": 13.474906424260944,
      "grad_norm": 8.181755065917969,
      "learning_rate": 3.3156366969673825e-05,
      "loss": 1.7537,
      "step": 176400
    },
    {
      "epoch": 13.482545260102361,
      "grad_norm": 7.550228118896484,
      "learning_rate": 3.314681842487205e-05,
      "loss": 1.7928,
      "step": 176500
    },
    {
      "epoch": 13.490184095943778,
      "grad_norm": 5.794078826904297,
      "learning_rate": 3.3137269880070275e-05,
      "loss": 1.8424,
      "step": 176600
    },
    {
      "epoch": 13.497822931785196,
      "grad_norm": 9.006640434265137,
      "learning_rate": 3.312772133526851e-05,
      "loss": 1.8461,
      "step": 176700
    },
    {
      "epoch": 13.505461767626613,
      "grad_norm": 5.994270324707031,
      "learning_rate": 3.311817279046673e-05,
      "loss": 1.8062,
      "step": 176800
    },
    {
      "epoch": 13.51310060346803,
      "grad_norm": 6.7888383865356445,
      "learning_rate": 3.310862424566496e-05,
      "loss": 1.8884,
      "step": 176900
    },
    {
      "epoch": 13.52073943930945,
      "grad_norm": 8.49032974243164,
      "learning_rate": 3.309907570086319e-05,
      "loss": 1.8335,
      "step": 177000
    },
    {
      "epoch": 13.528378275150867,
      "grad_norm": 4.639689922332764,
      "learning_rate": 3.308952715606142e-05,
      "loss": 1.7307,
      "step": 177100
    },
    {
      "epoch": 13.536017110992285,
      "grad_norm": 6.247945785522461,
      "learning_rate": 3.307997861125964e-05,
      "loss": 1.8217,
      "step": 177200
    },
    {
      "epoch": 13.543655946833702,
      "grad_norm": 6.940518856048584,
      "learning_rate": 3.3070430066457876e-05,
      "loss": 1.8619,
      "step": 177300
    },
    {
      "epoch": 13.55129478267512,
      "grad_norm": 7.183942794799805,
      "learning_rate": 3.30608815216561e-05,
      "loss": 1.87,
      "step": 177400
    },
    {
      "epoch": 13.558933618516537,
      "grad_norm": 7.81813383102417,
      "learning_rate": 3.305133297685433e-05,
      "loss": 2.0036,
      "step": 177500
    },
    {
      "epoch": 13.566572454357956,
      "grad_norm": 7.938180446624756,
      "learning_rate": 3.3041784432052555e-05,
      "loss": 1.8345,
      "step": 177600
    },
    {
      "epoch": 13.574211290199374,
      "grad_norm": 8.59598159790039,
      "learning_rate": 3.3032235887250784e-05,
      "loss": 1.7568,
      "step": 177700
    },
    {
      "epoch": 13.581850126040791,
      "grad_norm": 8.858444213867188,
      "learning_rate": 3.302268734244901e-05,
      "loss": 1.8369,
      "step": 177800
    },
    {
      "epoch": 13.589488961882209,
      "grad_norm": 10.176196098327637,
      "learning_rate": 3.301313879764724e-05,
      "loss": 1.8397,
      "step": 177900
    },
    {
      "epoch": 13.597127797723626,
      "grad_norm": 5.968623161315918,
      "learning_rate": 3.300359025284547e-05,
      "loss": 1.91,
      "step": 178000
    },
    {
      "epoch": 13.604766633565045,
      "grad_norm": 7.482292175292969,
      "learning_rate": 3.299404170804369e-05,
      "loss": 1.8411,
      "step": 178100
    },
    {
      "epoch": 13.612405469406463,
      "grad_norm": 8.869373321533203,
      "learning_rate": 3.298449316324193e-05,
      "loss": 1.8099,
      "step": 178200
    },
    {
      "epoch": 13.62004430524788,
      "grad_norm": 13.683374404907227,
      "learning_rate": 3.297494461844015e-05,
      "loss": 1.8354,
      "step": 178300
    },
    {
      "epoch": 13.627683141089298,
      "grad_norm": 8.073217391967773,
      "learning_rate": 3.296539607363838e-05,
      "loss": 1.8493,
      "step": 178400
    },
    {
      "epoch": 13.635321976930715,
      "grad_norm": 5.483936786651611,
      "learning_rate": 3.295584752883661e-05,
      "loss": 1.8438,
      "step": 178500
    },
    {
      "epoch": 13.642960812772134,
      "grad_norm": 7.252169609069824,
      "learning_rate": 3.2946298984034836e-05,
      "loss": 1.8089,
      "step": 178600
    },
    {
      "epoch": 13.650599648613552,
      "grad_norm": 6.59181547164917,
      "learning_rate": 3.293675043923306e-05,
      "loss": 1.8127,
      "step": 178700
    },
    {
      "epoch": 13.65823848445497,
      "grad_norm": 6.052206993103027,
      "learning_rate": 3.292720189443129e-05,
      "loss": 1.8334,
      "step": 178800
    },
    {
      "epoch": 13.665877320296387,
      "grad_norm": 7.95693302154541,
      "learning_rate": 3.2917653349629515e-05,
      "loss": 1.8655,
      "step": 178900
    },
    {
      "epoch": 13.673516156137804,
      "grad_norm": 6.51785135269165,
      "learning_rate": 3.2908104804827744e-05,
      "loss": 1.8205,
      "step": 179000
    },
    {
      "epoch": 13.681154991979222,
      "grad_norm": 6.543642044067383,
      "learning_rate": 3.289855626002597e-05,
      "loss": 1.8026,
      "step": 179100
    },
    {
      "epoch": 13.688793827820641,
      "grad_norm": 6.657672882080078,
      "learning_rate": 3.28890077152242e-05,
      "loss": 1.8189,
      "step": 179200
    },
    {
      "epoch": 13.696432663662058,
      "grad_norm": 9.096649169921875,
      "learning_rate": 3.287945917042243e-05,
      "loss": 1.7824,
      "step": 179300
    },
    {
      "epoch": 13.704071499503476,
      "grad_norm": 6.8625168800354,
      "learning_rate": 3.286991062562066e-05,
      "loss": 1.79,
      "step": 179400
    },
    {
      "epoch": 13.711710335344893,
      "grad_norm": 7.529833793640137,
      "learning_rate": 3.286036208081889e-05,
      "loss": 1.898,
      "step": 179500
    },
    {
      "epoch": 13.71934917118631,
      "grad_norm": 8.16894245147705,
      "learning_rate": 3.285081353601711e-05,
      "loss": 1.8256,
      "step": 179600
    },
    {
      "epoch": 13.726988007027728,
      "grad_norm": 6.92106819152832,
      "learning_rate": 3.2841264991215344e-05,
      "loss": 1.7775,
      "step": 179700
    },
    {
      "epoch": 13.734626842869147,
      "grad_norm": 7.541997909545898,
      "learning_rate": 3.2831716446413566e-05,
      "loss": 1.8349,
      "step": 179800
    },
    {
      "epoch": 13.742265678710565,
      "grad_norm": 7.676377773284912,
      "learning_rate": 3.2822167901611795e-05,
      "loss": 1.7447,
      "step": 179900
    },
    {
      "epoch": 13.749904514551982,
      "grad_norm": 8.765867233276367,
      "learning_rate": 3.2812619356810024e-05,
      "loss": 1.9315,
      "step": 180000
    },
    {
      "epoch": 13.7575433503934,
      "grad_norm": 6.695838928222656,
      "learning_rate": 3.280307081200825e-05,
      "loss": 1.8071,
      "step": 180100
    },
    {
      "epoch": 13.765182186234817,
      "grad_norm": 7.875421524047852,
      "learning_rate": 3.279352226720648e-05,
      "loss": 1.845,
      "step": 180200
    },
    {
      "epoch": 13.772821022076236,
      "grad_norm": 9.300948143005371,
      "learning_rate": 3.278397372240471e-05,
      "loss": 1.8311,
      "step": 180300
    },
    {
      "epoch": 13.780459857917654,
      "grad_norm": 9.38219928741455,
      "learning_rate": 3.277442517760294e-05,
      "loss": 1.8365,
      "step": 180400
    },
    {
      "epoch": 13.788098693759071,
      "grad_norm": 8.115035057067871,
      "learning_rate": 3.276487663280116e-05,
      "loss": 1.7856,
      "step": 180500
    },
    {
      "epoch": 13.795737529600489,
      "grad_norm": 6.065045356750488,
      "learning_rate": 3.2755328087999396e-05,
      "loss": 1.7984,
      "step": 180600
    },
    {
      "epoch": 13.803376365441906,
      "grad_norm": 7.597946643829346,
      "learning_rate": 3.274577954319762e-05,
      "loss": 1.8484,
      "step": 180700
    },
    {
      "epoch": 13.811015201283324,
      "grad_norm": 7.350810527801514,
      "learning_rate": 3.2736230998395847e-05,
      "loss": 1.7281,
      "step": 180800
    },
    {
      "epoch": 13.818654037124743,
      "grad_norm": 8.263887405395508,
      "learning_rate": 3.2726682453594075e-05,
      "loss": 1.8327,
      "step": 180900
    },
    {
      "epoch": 13.82629287296616,
      "grad_norm": 7.577747821807861,
      "learning_rate": 3.2717133908792304e-05,
      "loss": 1.9007,
      "step": 181000
    },
    {
      "epoch": 13.833931708807578,
      "grad_norm": 9.293489456176758,
      "learning_rate": 3.2707585363990526e-05,
      "loss": 1.8281,
      "step": 181100
    },
    {
      "epoch": 13.841570544648995,
      "grad_norm": 6.397637367248535,
      "learning_rate": 3.269803681918876e-05,
      "loss": 1.8569,
      "step": 181200
    },
    {
      "epoch": 13.849209380490413,
      "grad_norm": 6.815495491027832,
      "learning_rate": 3.268848827438698e-05,
      "loss": 1.8894,
      "step": 181300
    },
    {
      "epoch": 13.856848216331832,
      "grad_norm": 4.370175838470459,
      "learning_rate": 3.267893972958521e-05,
      "loss": 1.8471,
      "step": 181400
    },
    {
      "epoch": 13.86448705217325,
      "grad_norm": 8.56910228729248,
      "learning_rate": 3.266939118478344e-05,
      "loss": 1.8948,
      "step": 181500
    },
    {
      "epoch": 13.872125888014667,
      "grad_norm": 8.040534019470215,
      "learning_rate": 3.265984263998167e-05,
      "loss": 1.8719,
      "step": 181600
    },
    {
      "epoch": 13.879764723856084,
      "grad_norm": 4.6158342361450195,
      "learning_rate": 3.26502940951799e-05,
      "loss": 1.8804,
      "step": 181700
    },
    {
      "epoch": 13.887403559697502,
      "grad_norm": 8.5643949508667,
      "learning_rate": 3.264074555037812e-05,
      "loss": 1.8406,
      "step": 181800
    },
    {
      "epoch": 13.895042395538919,
      "grad_norm": 7.04425573348999,
      "learning_rate": 3.2631197005576355e-05,
      "loss": 1.7647,
      "step": 181900
    },
    {
      "epoch": 13.902681231380338,
      "grad_norm": 7.090687274932861,
      "learning_rate": 3.262164846077458e-05,
      "loss": 1.8342,
      "step": 182000
    },
    {
      "epoch": 13.910320067221756,
      "grad_norm": 10.688027381896973,
      "learning_rate": 3.2612099915972806e-05,
      "loss": 1.7436,
      "step": 182100
    },
    {
      "epoch": 13.917958903063173,
      "grad_norm": 8.310447692871094,
      "learning_rate": 3.2602551371171035e-05,
      "loss": 1.8735,
      "step": 182200
    },
    {
      "epoch": 13.92559773890459,
      "grad_norm": 6.230475425720215,
      "learning_rate": 3.259300282636926e-05,
      "loss": 1.8052,
      "step": 182300
    },
    {
      "epoch": 13.933236574746008,
      "grad_norm": 6.634023666381836,
      "learning_rate": 3.2583454281567485e-05,
      "loss": 1.7827,
      "step": 182400
    },
    {
      "epoch": 13.940875410587427,
      "grad_norm": 7.092044830322266,
      "learning_rate": 3.257390573676572e-05,
      "loss": 1.9083,
      "step": 182500
    },
    {
      "epoch": 13.948514246428845,
      "grad_norm": 6.55086088180542,
      "learning_rate": 3.256435719196394e-05,
      "loss": 1.7631,
      "step": 182600
    },
    {
      "epoch": 13.956153082270262,
      "grad_norm": 7.70496940612793,
      "learning_rate": 3.255480864716217e-05,
      "loss": 1.8793,
      "step": 182700
    },
    {
      "epoch": 13.96379191811168,
      "grad_norm": 7.44459342956543,
      "learning_rate": 3.25452601023604e-05,
      "loss": 1.8236,
      "step": 182800
    },
    {
      "epoch": 13.971430753953097,
      "grad_norm": 7.969545364379883,
      "learning_rate": 3.253571155755863e-05,
      "loss": 1.8476,
      "step": 182900
    },
    {
      "epoch": 13.979069589794515,
      "grad_norm": 7.173862457275391,
      "learning_rate": 3.252616301275686e-05,
      "loss": 1.7881,
      "step": 183000
    },
    {
      "epoch": 13.986708425635934,
      "grad_norm": 6.115562915802002,
      "learning_rate": 3.2516614467955086e-05,
      "loss": 1.9157,
      "step": 183100
    },
    {
      "epoch": 13.994347261477351,
      "grad_norm": 5.198310375213623,
      "learning_rate": 3.2507065923153315e-05,
      "loss": 1.7898,
      "step": 183200
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8370361328125,
      "eval_runtime": 2.9896,
      "eval_samples_per_second": 230.801,
      "eval_steps_per_second": 230.801,
      "step": 183274
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.6003259420394897,
      "eval_runtime": 55.9379,
      "eval_samples_per_second": 234.028,
      "eval_steps_per_second": 234.028,
      "step": 183274
    },
    {
      "epoch": 14.001986097318769,
      "grad_norm": 7.932343482971191,
      "learning_rate": 3.249751737835154e-05,
      "loss": 1.8005,
      "step": 183300
    },
    {
      "epoch": 14.009624933160186,
      "grad_norm": 10.413928985595703,
      "learning_rate": 3.248796883354977e-05,
      "loss": 1.7773,
      "step": 183400
    },
    {
      "epoch": 14.017263769001604,
      "grad_norm": 6.412398338317871,
      "learning_rate": 3.2478420288747994e-05,
      "loss": 1.8192,
      "step": 183500
    },
    {
      "epoch": 14.024902604843023,
      "grad_norm": 7.245870113372803,
      "learning_rate": 3.246887174394622e-05,
      "loss": 1.7956,
      "step": 183600
    },
    {
      "epoch": 14.03254144068444,
      "grad_norm": 7.287215709686279,
      "learning_rate": 3.245932319914445e-05,
      "loss": 1.8488,
      "step": 183700
    },
    {
      "epoch": 14.040180276525858,
      "grad_norm": 8.072850227355957,
      "learning_rate": 3.244977465434268e-05,
      "loss": 1.8425,
      "step": 183800
    },
    {
      "epoch": 14.047819112367275,
      "grad_norm": 7.051199913024902,
      "learning_rate": 3.24402261095409e-05,
      "loss": 1.8268,
      "step": 183900
    },
    {
      "epoch": 14.055457948208693,
      "grad_norm": 6.645903587341309,
      "learning_rate": 3.243067756473914e-05,
      "loss": 1.828,
      "step": 184000
    },
    {
      "epoch": 14.06309678405011,
      "grad_norm": 6.587838172912598,
      "learning_rate": 3.2421129019937366e-05,
      "loss": 1.8347,
      "step": 184100
    },
    {
      "epoch": 14.07073561989153,
      "grad_norm": 6.584880352020264,
      "learning_rate": 3.241158047513559e-05,
      "loss": 1.76,
      "step": 184200
    },
    {
      "epoch": 14.078374455732947,
      "grad_norm": 8.094122886657715,
      "learning_rate": 3.2402031930333824e-05,
      "loss": 1.8565,
      "step": 184300
    },
    {
      "epoch": 14.086013291574364,
      "grad_norm": 7.463325023651123,
      "learning_rate": 3.2392483385532046e-05,
      "loss": 1.8607,
      "step": 184400
    },
    {
      "epoch": 14.093652127415782,
      "grad_norm": 7.780216217041016,
      "learning_rate": 3.2382934840730274e-05,
      "loss": 1.7351,
      "step": 184500
    },
    {
      "epoch": 14.101290963257199,
      "grad_norm": 8.401945114135742,
      "learning_rate": 3.23733862959285e-05,
      "loss": 1.7764,
      "step": 184600
    },
    {
      "epoch": 14.108929799098618,
      "grad_norm": 6.08218240737915,
      "learning_rate": 3.236383775112673e-05,
      "loss": 1.8461,
      "step": 184700
    },
    {
      "epoch": 14.116568634940036,
      "grad_norm": 7.2689900398254395,
      "learning_rate": 3.2354289206324954e-05,
      "loss": 1.8226,
      "step": 184800
    },
    {
      "epoch": 14.124207470781453,
      "grad_norm": 8.623388290405273,
      "learning_rate": 3.234474066152319e-05,
      "loss": 1.8261,
      "step": 184900
    },
    {
      "epoch": 14.13184630662287,
      "grad_norm": 8.178000450134277,
      "learning_rate": 3.233519211672141e-05,
      "loss": 1.8659,
      "step": 185000
    },
    {
      "epoch": 14.139485142464288,
      "grad_norm": 5.407064914703369,
      "learning_rate": 3.232564357191964e-05,
      "loss": 1.7727,
      "step": 185100
    },
    {
      "epoch": 14.147123978305705,
      "grad_norm": 6.403450012207031,
      "learning_rate": 3.231609502711787e-05,
      "loss": 1.9266,
      "step": 185200
    },
    {
      "epoch": 14.154762814147125,
      "grad_norm": 6.656612873077393,
      "learning_rate": 3.23065464823161e-05,
      "loss": 1.8642,
      "step": 185300
    },
    {
      "epoch": 14.162401649988542,
      "grad_norm": 7.251781940460205,
      "learning_rate": 3.2296997937514326e-05,
      "loss": 1.8025,
      "step": 185400
    },
    {
      "epoch": 14.17004048582996,
      "grad_norm": 8.555450439453125,
      "learning_rate": 3.2287449392712554e-05,
      "loss": 1.8526,
      "step": 185500
    },
    {
      "epoch": 14.177679321671377,
      "grad_norm": 7.087225437164307,
      "learning_rate": 3.227790084791078e-05,
      "loss": 1.782,
      "step": 185600
    },
    {
      "epoch": 14.185318157512794,
      "grad_norm": 9.286269187927246,
      "learning_rate": 3.2268352303109005e-05,
      "loss": 1.8282,
      "step": 185700
    },
    {
      "epoch": 14.192956993354214,
      "grad_norm": 6.895440578460693,
      "learning_rate": 3.225880375830724e-05,
      "loss": 1.7995,
      "step": 185800
    },
    {
      "epoch": 14.200595829195631,
      "grad_norm": 7.168159484863281,
      "learning_rate": 3.224925521350546e-05,
      "loss": 1.7745,
      "step": 185900
    },
    {
      "epoch": 14.208234665037049,
      "grad_norm": 8.04859447479248,
      "learning_rate": 3.223970666870369e-05,
      "loss": 1.8683,
      "step": 186000
    },
    {
      "epoch": 14.215873500878466,
      "grad_norm": 6.621502876281738,
      "learning_rate": 3.223015812390192e-05,
      "loss": 1.8188,
      "step": 186100
    },
    {
      "epoch": 14.223512336719883,
      "grad_norm": 7.900248050689697,
      "learning_rate": 3.222060957910015e-05,
      "loss": 1.7958,
      "step": 186200
    },
    {
      "epoch": 14.231151172561301,
      "grad_norm": 7.013895034790039,
      "learning_rate": 3.221106103429837e-05,
      "loss": 1.7122,
      "step": 186300
    },
    {
      "epoch": 14.23879000840272,
      "grad_norm": 6.879655361175537,
      "learning_rate": 3.2201512489496606e-05,
      "loss": 1.8635,
      "step": 186400
    },
    {
      "epoch": 14.246428844244138,
      "grad_norm": 7.316961765289307,
      "learning_rate": 3.219196394469483e-05,
      "loss": 1.7721,
      "step": 186500
    },
    {
      "epoch": 14.254067680085555,
      "grad_norm": 7.780004024505615,
      "learning_rate": 3.2182415399893057e-05,
      "loss": 1.8384,
      "step": 186600
    },
    {
      "epoch": 14.261706515926972,
      "grad_norm": 5.302781581878662,
      "learning_rate": 3.2172866855091285e-05,
      "loss": 1.8658,
      "step": 186700
    },
    {
      "epoch": 14.26934535176839,
      "grad_norm": 5.558959484100342,
      "learning_rate": 3.2163318310289514e-05,
      "loss": 1.9228,
      "step": 186800
    },
    {
      "epoch": 14.27698418760981,
      "grad_norm": 6.9062819480896,
      "learning_rate": 3.215376976548774e-05,
      "loss": 1.8445,
      "step": 186900
    },
    {
      "epoch": 14.284623023451227,
      "grad_norm": 7.028890132904053,
      "learning_rate": 3.214422122068597e-05,
      "loss": 1.8259,
      "step": 187000
    },
    {
      "epoch": 14.292261859292644,
      "grad_norm": 8.394436836242676,
      "learning_rate": 3.21346726758842e-05,
      "loss": 1.8029,
      "step": 187100
    },
    {
      "epoch": 14.299900695134061,
      "grad_norm": 5.882689476013184,
      "learning_rate": 3.212512413108242e-05,
      "loss": 1.8121,
      "step": 187200
    },
    {
      "epoch": 14.307539530975479,
      "grad_norm": 9.978178024291992,
      "learning_rate": 3.211557558628066e-05,
      "loss": 1.8174,
      "step": 187300
    },
    {
      "epoch": 14.315178366816896,
      "grad_norm": 7.338516712188721,
      "learning_rate": 3.210602704147888e-05,
      "loss": 1.8114,
      "step": 187400
    },
    {
      "epoch": 14.322817202658316,
      "grad_norm": 8.76569652557373,
      "learning_rate": 3.209647849667711e-05,
      "loss": 1.847,
      "step": 187500
    },
    {
      "epoch": 14.330456038499733,
      "grad_norm": 8.95849323272705,
      "learning_rate": 3.208692995187533e-05,
      "loss": 1.8637,
      "step": 187600
    },
    {
      "epoch": 14.33809487434115,
      "grad_norm": 6.337198734283447,
      "learning_rate": 3.2077381407073565e-05,
      "loss": 1.7475,
      "step": 187700
    },
    {
      "epoch": 14.345733710182568,
      "grad_norm": 7.240879058837891,
      "learning_rate": 3.206783286227179e-05,
      "loss": 1.8088,
      "step": 187800
    },
    {
      "epoch": 14.353372546023985,
      "grad_norm": 6.972143173217773,
      "learning_rate": 3.2058284317470016e-05,
      "loss": 1.8269,
      "step": 187900
    },
    {
      "epoch": 14.361011381865403,
      "grad_norm": 9.936235427856445,
      "learning_rate": 3.2048735772668245e-05,
      "loss": 1.79,
      "step": 188000
    },
    {
      "epoch": 14.368650217706822,
      "grad_norm": 7.045119285583496,
      "learning_rate": 3.203918722786647e-05,
      "loss": 1.8098,
      "step": 188100
    },
    {
      "epoch": 14.37628905354824,
      "grad_norm": 5.711673736572266,
      "learning_rate": 3.20296386830647e-05,
      "loss": 1.8402,
      "step": 188200
    },
    {
      "epoch": 14.383927889389657,
      "grad_norm": 7.849396228790283,
      "learning_rate": 3.202009013826293e-05,
      "loss": 1.8077,
      "step": 188300
    },
    {
      "epoch": 14.391566725231074,
      "grad_norm": 7.561467170715332,
      "learning_rate": 3.201054159346116e-05,
      "loss": 1.8744,
      "step": 188400
    },
    {
      "epoch": 14.399205561072492,
      "grad_norm": 6.727321147918701,
      "learning_rate": 3.200099304865938e-05,
      "loss": 1.8883,
      "step": 188500
    },
    {
      "epoch": 14.406844396913911,
      "grad_norm": 10.993700981140137,
      "learning_rate": 3.199144450385762e-05,
      "loss": 1.862,
      "step": 188600
    },
    {
      "epoch": 14.414483232755329,
      "grad_norm": 9.272433280944824,
      "learning_rate": 3.198189595905584e-05,
      "loss": 1.8552,
      "step": 188700
    },
    {
      "epoch": 14.422122068596746,
      "grad_norm": 7.836032867431641,
      "learning_rate": 3.197234741425407e-05,
      "loss": 1.7091,
      "step": 188800
    },
    {
      "epoch": 14.429760904438163,
      "grad_norm": 7.755955219268799,
      "learning_rate": 3.1962798869452296e-05,
      "loss": 1.7492,
      "step": 188900
    },
    {
      "epoch": 14.43739974027958,
      "grad_norm": 6.639308452606201,
      "learning_rate": 3.1953250324650525e-05,
      "loss": 1.8064,
      "step": 189000
    },
    {
      "epoch": 14.445038576121,
      "grad_norm": 7.362409591674805,
      "learning_rate": 3.1943701779848754e-05,
      "loss": 1.8149,
      "step": 189100
    },
    {
      "epoch": 14.452677411962418,
      "grad_norm": 9.021829605102539,
      "learning_rate": 3.193415323504698e-05,
      "loss": 1.9285,
      "step": 189200
    },
    {
      "epoch": 14.460316247803835,
      "grad_norm": 6.218352317810059,
      "learning_rate": 3.192460469024521e-05,
      "loss": 1.7877,
      "step": 189300
    },
    {
      "epoch": 14.467955083645252,
      "grad_norm": 5.259843826293945,
      "learning_rate": 3.191505614544343e-05,
      "loss": 1.8876,
      "step": 189400
    },
    {
      "epoch": 14.47559391948667,
      "grad_norm": 6.024620532989502,
      "learning_rate": 3.190550760064167e-05,
      "loss": 1.8703,
      "step": 189500
    },
    {
      "epoch": 14.483232755328087,
      "grad_norm": 7.727804660797119,
      "learning_rate": 3.189595905583989e-05,
      "loss": 1.6901,
      "step": 189600
    },
    {
      "epoch": 14.490871591169507,
      "grad_norm": 6.905629634857178,
      "learning_rate": 3.188641051103812e-05,
      "loss": 1.9326,
      "step": 189700
    },
    {
      "epoch": 14.498510427010924,
      "grad_norm": 7.490853309631348,
      "learning_rate": 3.187686196623635e-05,
      "loss": 1.8289,
      "step": 189800
    },
    {
      "epoch": 14.506149262852341,
      "grad_norm": 6.8139567375183105,
      "learning_rate": 3.1867313421434576e-05,
      "loss": 1.7501,
      "step": 189900
    },
    {
      "epoch": 14.513788098693759,
      "grad_norm": 7.129657745361328,
      "learning_rate": 3.18577648766328e-05,
      "loss": 1.7889,
      "step": 190000
    },
    {
      "epoch": 14.521426934535176,
      "grad_norm": 7.131406307220459,
      "learning_rate": 3.1848216331831034e-05,
      "loss": 1.8215,
      "step": 190100
    },
    {
      "epoch": 14.529065770376594,
      "grad_norm": 8.883182525634766,
      "learning_rate": 3.1838667787029256e-05,
      "loss": 1.8264,
      "step": 190200
    },
    {
      "epoch": 14.536704606218013,
      "grad_norm": 7.750130653381348,
      "learning_rate": 3.1829119242227484e-05,
      "loss": 1.7368,
      "step": 190300
    },
    {
      "epoch": 14.54434344205943,
      "grad_norm": 7.821580410003662,
      "learning_rate": 3.181957069742571e-05,
      "loss": 1.843,
      "step": 190400
    },
    {
      "epoch": 14.551982277900848,
      "grad_norm": 7.395490646362305,
      "learning_rate": 3.181002215262394e-05,
      "loss": 1.79,
      "step": 190500
    },
    {
      "epoch": 14.559621113742265,
      "grad_norm": 6.074987888336182,
      "learning_rate": 3.180047360782217e-05,
      "loss": 1.8254,
      "step": 190600
    },
    {
      "epoch": 14.567259949583683,
      "grad_norm": 6.571435928344727,
      "learning_rate": 3.17909250630204e-05,
      "loss": 1.805,
      "step": 190700
    },
    {
      "epoch": 14.574898785425102,
      "grad_norm": 8.107994079589844,
      "learning_rate": 3.178137651821863e-05,
      "loss": 1.7373,
      "step": 190800
    },
    {
      "epoch": 14.58253762126652,
      "grad_norm": 5.6645894050598145,
      "learning_rate": 3.177182797341685e-05,
      "loss": 1.8385,
      "step": 190900
    },
    {
      "epoch": 14.590176457107937,
      "grad_norm": 7.28494119644165,
      "learning_rate": 3.1762279428615085e-05,
      "loss": 1.8031,
      "step": 191000
    },
    {
      "epoch": 14.597815292949354,
      "grad_norm": 6.928328514099121,
      "learning_rate": 3.175273088381331e-05,
      "loss": 1.7864,
      "step": 191100
    },
    {
      "epoch": 14.605454128790772,
      "grad_norm": 5.650442123413086,
      "learning_rate": 3.1743182339011536e-05,
      "loss": 1.788,
      "step": 191200
    },
    {
      "epoch": 14.613092964632191,
      "grad_norm": 7.073073387145996,
      "learning_rate": 3.1733633794209764e-05,
      "loss": 1.782,
      "step": 191300
    },
    {
      "epoch": 14.620731800473608,
      "grad_norm": 8.389670372009277,
      "learning_rate": 3.172408524940799e-05,
      "loss": 1.7452,
      "step": 191400
    },
    {
      "epoch": 14.628370636315026,
      "grad_norm": 9.627970695495605,
      "learning_rate": 3.1714536704606215e-05,
      "loss": 1.9175,
      "step": 191500
    },
    {
      "epoch": 14.636009472156443,
      "grad_norm": 6.709389686584473,
      "learning_rate": 3.170498815980445e-05,
      "loss": 1.741,
      "step": 191600
    },
    {
      "epoch": 14.64364830799786,
      "grad_norm": 7.99521017074585,
      "learning_rate": 3.169543961500267e-05,
      "loss": 1.8026,
      "step": 191700
    },
    {
      "epoch": 14.651287143839278,
      "grad_norm": 6.970609664916992,
      "learning_rate": 3.16858910702009e-05,
      "loss": 1.8949,
      "step": 191800
    },
    {
      "epoch": 14.658925979680697,
      "grad_norm": 6.796565055847168,
      "learning_rate": 3.167634252539913e-05,
      "loss": 1.8759,
      "step": 191900
    },
    {
      "epoch": 14.666564815522115,
      "grad_norm": 5.866334438323975,
      "learning_rate": 3.166679398059736e-05,
      "loss": 1.7567,
      "step": 192000
    },
    {
      "epoch": 14.674203651363532,
      "grad_norm": 5.376385688781738,
      "learning_rate": 3.165724543579559e-05,
      "loss": 1.8776,
      "step": 192100
    },
    {
      "epoch": 14.68184248720495,
      "grad_norm": 6.6811957359313965,
      "learning_rate": 3.1647696890993816e-05,
      "loss": 1.8191,
      "step": 192200
    },
    {
      "epoch": 14.689481323046367,
      "grad_norm": 7.357375621795654,
      "learning_rate": 3.1638148346192045e-05,
      "loss": 1.7721,
      "step": 192300
    },
    {
      "epoch": 14.697120158887785,
      "grad_norm": 7.03179407119751,
      "learning_rate": 3.1628599801390267e-05,
      "loss": 1.9137,
      "step": 192400
    },
    {
      "epoch": 14.704758994729204,
      "grad_norm": 8.652607917785645,
      "learning_rate": 3.16190512565885e-05,
      "loss": 1.8612,
      "step": 192500
    },
    {
      "epoch": 14.712397830570621,
      "grad_norm": 7.480954647064209,
      "learning_rate": 3.1609502711786724e-05,
      "loss": 1.8297,
      "step": 192600
    },
    {
      "epoch": 14.720036666412039,
      "grad_norm": 7.091863632202148,
      "learning_rate": 3.159995416698495e-05,
      "loss": 1.8662,
      "step": 192700
    },
    {
      "epoch": 14.727675502253456,
      "grad_norm": 7.998465061187744,
      "learning_rate": 3.159040562218318e-05,
      "loss": 1.7546,
      "step": 192800
    },
    {
      "epoch": 14.735314338094874,
      "grad_norm": 7.070348262786865,
      "learning_rate": 3.158085707738141e-05,
      "loss": 1.8234,
      "step": 192900
    },
    {
      "epoch": 14.742953173936293,
      "grad_norm": 6.166301250457764,
      "learning_rate": 3.157130853257964e-05,
      "loss": 1.6916,
      "step": 193000
    },
    {
      "epoch": 14.75059200977771,
      "grad_norm": 13.802870750427246,
      "learning_rate": 3.156175998777787e-05,
      "loss": 1.851,
      "step": 193100
    },
    {
      "epoch": 14.758230845619128,
      "grad_norm": 6.387571811676025,
      "learning_rate": 3.1552211442976096e-05,
      "loss": 1.8138,
      "step": 193200
    },
    {
      "epoch": 14.765869681460545,
      "grad_norm": 4.780669212341309,
      "learning_rate": 3.154266289817432e-05,
      "loss": 1.7413,
      "step": 193300
    },
    {
      "epoch": 14.773508517301963,
      "grad_norm": 6.492524147033691,
      "learning_rate": 3.153311435337255e-05,
      "loss": 1.9199,
      "step": 193400
    },
    {
      "epoch": 14.78114735314338,
      "grad_norm": 5.647018909454346,
      "learning_rate": 3.1523565808570775e-05,
      "loss": 1.8685,
      "step": 193500
    },
    {
      "epoch": 14.7887861889848,
      "grad_norm": 4.908605098724365,
      "learning_rate": 3.1514017263769004e-05,
      "loss": 1.8471,
      "step": 193600
    },
    {
      "epoch": 14.796425024826217,
      "grad_norm": 9.000594139099121,
      "learning_rate": 3.1504468718967226e-05,
      "loss": 1.8739,
      "step": 193700
    },
    {
      "epoch": 14.804063860667634,
      "grad_norm": 6.001301288604736,
      "learning_rate": 3.149492017416546e-05,
      "loss": 1.8197,
      "step": 193800
    },
    {
      "epoch": 14.811702696509052,
      "grad_norm": 10.211634635925293,
      "learning_rate": 3.148537162936368e-05,
      "loss": 1.786,
      "step": 193900
    },
    {
      "epoch": 14.81934153235047,
      "grad_norm": 9.208786010742188,
      "learning_rate": 3.147582308456191e-05,
      "loss": 1.8833,
      "step": 194000
    },
    {
      "epoch": 14.826980368191888,
      "grad_norm": 6.594954013824463,
      "learning_rate": 3.146627453976014e-05,
      "loss": 1.8578,
      "step": 194100
    },
    {
      "epoch": 14.834619204033306,
      "grad_norm": 7.385560035705566,
      "learning_rate": 3.145672599495837e-05,
      "loss": 1.8464,
      "step": 194200
    },
    {
      "epoch": 14.842258039874723,
      "grad_norm": 6.59326171875,
      "learning_rate": 3.14471774501566e-05,
      "loss": 1.839,
      "step": 194300
    },
    {
      "epoch": 14.84989687571614,
      "grad_norm": 7.780028343200684,
      "learning_rate": 3.143762890535483e-05,
      "loss": 1.8129,
      "step": 194400
    },
    {
      "epoch": 14.857535711557558,
      "grad_norm": 7.400375843048096,
      "learning_rate": 3.1428080360553056e-05,
      "loss": 1.6991,
      "step": 194500
    },
    {
      "epoch": 14.865174547398976,
      "grad_norm": 7.105075359344482,
      "learning_rate": 3.141853181575128e-05,
      "loss": 1.8423,
      "step": 194600
    },
    {
      "epoch": 14.872813383240395,
      "grad_norm": 5.159773349761963,
      "learning_rate": 3.140898327094951e-05,
      "loss": 1.806,
      "step": 194700
    },
    {
      "epoch": 14.880452219081812,
      "grad_norm": 6.195019721984863,
      "learning_rate": 3.1399434726147735e-05,
      "loss": 1.7796,
      "step": 194800
    },
    {
      "epoch": 14.88809105492323,
      "grad_norm": 6.974509239196777,
      "learning_rate": 3.1389886181345964e-05,
      "loss": 1.8644,
      "step": 194900
    },
    {
      "epoch": 14.895729890764647,
      "grad_norm": 8.452290534973145,
      "learning_rate": 3.138033763654419e-05,
      "loss": 1.8878,
      "step": 195000
    },
    {
      "epoch": 14.903368726606065,
      "grad_norm": 7.3362579345703125,
      "learning_rate": 3.137078909174242e-05,
      "loss": 1.8539,
      "step": 195100
    },
    {
      "epoch": 14.911007562447484,
      "grad_norm": 4.313356399536133,
      "learning_rate": 3.136124054694064e-05,
      "loss": 1.8059,
      "step": 195200
    },
    {
      "epoch": 14.918646398288901,
      "grad_norm": 6.009322643280029,
      "learning_rate": 3.135169200213888e-05,
      "loss": 1.8716,
      "step": 195300
    },
    {
      "epoch": 14.926285234130319,
      "grad_norm": 5.8546857833862305,
      "learning_rate": 3.13421434573371e-05,
      "loss": 1.8473,
      "step": 195400
    },
    {
      "epoch": 14.933924069971736,
      "grad_norm": 8.52117919921875,
      "learning_rate": 3.133259491253533e-05,
      "loss": 1.8053,
      "step": 195500
    },
    {
      "epoch": 14.941562905813154,
      "grad_norm": 12.799127578735352,
      "learning_rate": 3.132304636773356e-05,
      "loss": 1.8247,
      "step": 195600
    },
    {
      "epoch": 14.949201741654571,
      "grad_norm": 7.381688594818115,
      "learning_rate": 3.1313497822931786e-05,
      "loss": 1.8288,
      "step": 195700
    },
    {
      "epoch": 14.95684057749599,
      "grad_norm": 6.616836071014404,
      "learning_rate": 3.1303949278130015e-05,
      "loss": 1.8761,
      "step": 195800
    },
    {
      "epoch": 14.964479413337408,
      "grad_norm": 4.79996395111084,
      "learning_rate": 3.1294400733328244e-05,
      "loss": 1.8262,
      "step": 195900
    },
    {
      "epoch": 14.972118249178825,
      "grad_norm": 8.503844261169434,
      "learning_rate": 3.128485218852647e-05,
      "loss": 1.7517,
      "step": 196000
    },
    {
      "epoch": 14.979757085020243,
      "grad_norm": 6.090100288391113,
      "learning_rate": 3.1275303643724694e-05,
      "loss": 1.7978,
      "step": 196100
    },
    {
      "epoch": 14.98739592086166,
      "grad_norm": 6.628315448760986,
      "learning_rate": 3.126575509892293e-05,
      "loss": 1.7713,
      "step": 196200
    },
    {
      "epoch": 14.99503475670308,
      "grad_norm": 7.914328575134277,
      "learning_rate": 3.125620655412115e-05,
      "loss": 1.8409,
      "step": 196300
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.835679531097412,
      "eval_runtime": 3.0057,
      "eval_samples_per_second": 229.564,
      "eval_steps_per_second": 229.564,
      "step": 196365
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.5906472206115723,
      "eval_runtime": 55.991,
      "eval_samples_per_second": 233.805,
      "eval_steps_per_second": 233.805,
      "step": 196365
    },
    {
      "epoch": 15.002673592544497,
      "grad_norm": 6.498357772827148,
      "learning_rate": 3.124665800931938e-05,
      "loss": 1.7341,
      "step": 196400
    },
    {
      "epoch": 15.010312428385914,
      "grad_norm": 6.136819362640381,
      "learning_rate": 3.123710946451761e-05,
      "loss": 1.7833,
      "step": 196500
    },
    {
      "epoch": 15.017951264227332,
      "grad_norm": 6.928144454956055,
      "learning_rate": 3.122756091971584e-05,
      "loss": 1.8408,
      "step": 196600
    },
    {
      "epoch": 15.025590100068749,
      "grad_norm": 7.5449910163879395,
      "learning_rate": 3.121801237491406e-05,
      "loss": 1.7502,
      "step": 196700
    },
    {
      "epoch": 15.033228935910167,
      "grad_norm": 6.680169582366943,
      "learning_rate": 3.1208463830112295e-05,
      "loss": 1.7923,
      "step": 196800
    },
    {
      "epoch": 15.040867771751586,
      "grad_norm": 9.799920082092285,
      "learning_rate": 3.1198915285310524e-05,
      "loss": 1.8021,
      "step": 196900
    },
    {
      "epoch": 15.048506607593003,
      "grad_norm": 7.04863977432251,
      "learning_rate": 3.1189366740508746e-05,
      "loss": 1.8555,
      "step": 197000
    },
    {
      "epoch": 15.05614544343442,
      "grad_norm": 4.924487590789795,
      "learning_rate": 3.117981819570698e-05,
      "loss": 1.8432,
      "step": 197100
    },
    {
      "epoch": 15.063784279275838,
      "grad_norm": 6.625602722167969,
      "learning_rate": 3.11702696509052e-05,
      "loss": 1.7854,
      "step": 197200
    },
    {
      "epoch": 15.071423115117256,
      "grad_norm": 6.7746124267578125,
      "learning_rate": 3.116072110610343e-05,
      "loss": 1.7764,
      "step": 197300
    },
    {
      "epoch": 15.079061950958675,
      "grad_norm": 6.141939163208008,
      "learning_rate": 3.115117256130166e-05,
      "loss": 1.8644,
      "step": 197400
    },
    {
      "epoch": 15.086700786800092,
      "grad_norm": 8.001838684082031,
      "learning_rate": 3.114162401649989e-05,
      "loss": 1.7508,
      "step": 197500
    },
    {
      "epoch": 15.09433962264151,
      "grad_norm": 7.399478912353516,
      "learning_rate": 3.113207547169811e-05,
      "loss": 1.7956,
      "step": 197600
    },
    {
      "epoch": 15.101978458482927,
      "grad_norm": 9.14604377746582,
      "learning_rate": 3.1122526926896347e-05,
      "loss": 1.7443,
      "step": 197700
    },
    {
      "epoch": 15.109617294324345,
      "grad_norm": 6.041464805603027,
      "learning_rate": 3.111297838209457e-05,
      "loss": 1.7952,
      "step": 197800
    },
    {
      "epoch": 15.117256130165762,
      "grad_norm": 6.777815818786621,
      "learning_rate": 3.11034298372928e-05,
      "loss": 1.7886,
      "step": 197900
    },
    {
      "epoch": 15.124894966007181,
      "grad_norm": 5.2957763671875,
      "learning_rate": 3.1093881292491026e-05,
      "loss": 1.7916,
      "step": 198000
    },
    {
      "epoch": 15.132533801848599,
      "grad_norm": 8.241706848144531,
      "learning_rate": 3.1084332747689255e-05,
      "loss": 1.748,
      "step": 198100
    },
    {
      "epoch": 15.140172637690016,
      "grad_norm": 6.964229106903076,
      "learning_rate": 3.107478420288748e-05,
      "loss": 1.8754,
      "step": 198200
    },
    {
      "epoch": 15.147811473531434,
      "grad_norm": 6.627554893493652,
      "learning_rate": 3.106523565808571e-05,
      "loss": 1.8268,
      "step": 198300
    },
    {
      "epoch": 15.155450309372851,
      "grad_norm": 7.207785129547119,
      "learning_rate": 3.105568711328394e-05,
      "loss": 1.7898,
      "step": 198400
    },
    {
      "epoch": 15.16308914521427,
      "grad_norm": 7.825875759124756,
      "learning_rate": 3.104613856848216e-05,
      "loss": 1.791,
      "step": 198500
    },
    {
      "epoch": 15.170727981055688,
      "grad_norm": 5.7302937507629395,
      "learning_rate": 3.10365900236804e-05,
      "loss": 1.7933,
      "step": 198600
    },
    {
      "epoch": 15.178366816897105,
      "grad_norm": 4.972012042999268,
      "learning_rate": 3.102704147887862e-05,
      "loss": 1.8123,
      "step": 198700
    },
    {
      "epoch": 15.186005652738523,
      "grad_norm": 7.459253311157227,
      "learning_rate": 3.101749293407685e-05,
      "loss": 1.8555,
      "step": 198800
    },
    {
      "epoch": 15.19364448857994,
      "grad_norm": 6.320633411407471,
      "learning_rate": 3.100794438927508e-05,
      "loss": 1.8643,
      "step": 198900
    },
    {
      "epoch": 15.201283324421357,
      "grad_norm": 8.319025039672852,
      "learning_rate": 3.0998395844473306e-05,
      "loss": 1.8885,
      "step": 199000
    },
    {
      "epoch": 15.208922160262777,
      "grad_norm": 6.64126443862915,
      "learning_rate": 3.098884729967153e-05,
      "loss": 1.9025,
      "step": 199100
    },
    {
      "epoch": 15.216560996104194,
      "grad_norm": 8.199554443359375,
      "learning_rate": 3.097929875486976e-05,
      "loss": 1.8198,
      "step": 199200
    },
    {
      "epoch": 15.224199831945612,
      "grad_norm": 7.018219947814941,
      "learning_rate": 3.0969750210067985e-05,
      "loss": 1.7911,
      "step": 199300
    },
    {
      "epoch": 15.231838667787029,
      "grad_norm": 5.6327714920043945,
      "learning_rate": 3.0960201665266214e-05,
      "loss": 1.7518,
      "step": 199400
    },
    {
      "epoch": 15.239477503628446,
      "grad_norm": 6.4301934242248535,
      "learning_rate": 3.095065312046444e-05,
      "loss": 1.7858,
      "step": 199500
    },
    {
      "epoch": 15.247116339469866,
      "grad_norm": 8.386165618896484,
      "learning_rate": 3.094110457566267e-05,
      "loss": 1.7618,
      "step": 199600
    },
    {
      "epoch": 15.254755175311283,
      "grad_norm": 6.841829299926758,
      "learning_rate": 3.09315560308609e-05,
      "loss": 1.7734,
      "step": 199700
    },
    {
      "epoch": 15.2623940111527,
      "grad_norm": 5.576964855194092,
      "learning_rate": 3.092200748605912e-05,
      "loss": 1.8468,
      "step": 199800
    },
    {
      "epoch": 15.270032846994118,
      "grad_norm": 7.645705223083496,
      "learning_rate": 3.091245894125736e-05,
      "loss": 1.734,
      "step": 199900
    },
    {
      "epoch": 15.277671682835535,
      "grad_norm": 5.577629566192627,
      "learning_rate": 3.090291039645558e-05,
      "loss": 1.7651,
      "step": 200000
    },
    {
      "epoch": 15.285310518676953,
      "grad_norm": 8.011137008666992,
      "learning_rate": 3.089336185165381e-05,
      "loss": 1.8104,
      "step": 200100
    },
    {
      "epoch": 15.292949354518372,
      "grad_norm": 7.207944393157959,
      "learning_rate": 3.088381330685204e-05,
      "loss": 1.6835,
      "step": 200200
    },
    {
      "epoch": 15.30058819035979,
      "grad_norm": 6.613495826721191,
      "learning_rate": 3.0874264762050266e-05,
      "loss": 1.7219,
      "step": 200300
    },
    {
      "epoch": 15.308227026201207,
      "grad_norm": 7.896143913269043,
      "learning_rate": 3.086471621724849e-05,
      "loss": 1.7567,
      "step": 200400
    },
    {
      "epoch": 15.315865862042624,
      "grad_norm": 7.515092849731445,
      "learning_rate": 3.085516767244672e-05,
      "loss": 1.8156,
      "step": 200500
    },
    {
      "epoch": 15.323504697884042,
      "grad_norm": 5.414842128753662,
      "learning_rate": 3.0845619127644945e-05,
      "loss": 1.6875,
      "step": 200600
    },
    {
      "epoch": 15.33114353372546,
      "grad_norm": 7.619110107421875,
      "learning_rate": 3.0836070582843174e-05,
      "loss": 1.8896,
      "step": 200700
    },
    {
      "epoch": 15.338782369566879,
      "grad_norm": 9.66968059539795,
      "learning_rate": 3.08265220380414e-05,
      "loss": 1.8533,
      "step": 200800
    },
    {
      "epoch": 15.346421205408296,
      "grad_norm": 7.644092559814453,
      "learning_rate": 3.081697349323963e-05,
      "loss": 1.7955,
      "step": 200900
    },
    {
      "epoch": 15.354060041249713,
      "grad_norm": 6.767613410949707,
      "learning_rate": 3.080742494843786e-05,
      "loss": 1.8096,
      "step": 201000
    },
    {
      "epoch": 15.361698877091131,
      "grad_norm": 10.54128646850586,
      "learning_rate": 3.079787640363609e-05,
      "loss": 1.7669,
      "step": 201100
    },
    {
      "epoch": 15.369337712932548,
      "grad_norm": 6.831560134887695,
      "learning_rate": 3.078832785883432e-05,
      "loss": 1.8558,
      "step": 201200
    },
    {
      "epoch": 15.376976548773968,
      "grad_norm": 8.270625114440918,
      "learning_rate": 3.077877931403254e-05,
      "loss": 1.9704,
      "step": 201300
    },
    {
      "epoch": 15.384615384615385,
      "grad_norm": 6.419985294342041,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 1.8,
      "step": 201400
    },
    {
      "epoch": 15.392254220456802,
      "grad_norm": 7.976985931396484,
      "learning_rate": 3.0759682224428996e-05,
      "loss": 1.8816,
      "step": 201500
    },
    {
      "epoch": 15.39989305629822,
      "grad_norm": 8.596476554870605,
      "learning_rate": 3.0750133679627225e-05,
      "loss": 1.7656,
      "step": 201600
    },
    {
      "epoch": 15.407531892139637,
      "grad_norm": 7.8143630027771,
      "learning_rate": 3.0740585134825454e-05,
      "loss": 1.7893,
      "step": 201700
    },
    {
      "epoch": 15.415170727981057,
      "grad_norm": 14.528560638427734,
      "learning_rate": 3.073103659002368e-05,
      "loss": 1.8931,
      "step": 201800
    },
    {
      "epoch": 15.422809563822474,
      "grad_norm": 9.373527526855469,
      "learning_rate": 3.072148804522191e-05,
      "loss": 1.8915,
      "step": 201900
    },
    {
      "epoch": 15.430448399663891,
      "grad_norm": 9.514281272888184,
      "learning_rate": 3.071193950042014e-05,
      "loss": 1.8626,
      "step": 202000
    },
    {
      "epoch": 15.438087235505309,
      "grad_norm": 5.298540115356445,
      "learning_rate": 3.070239095561837e-05,
      "loss": 1.8298,
      "step": 202100
    },
    {
      "epoch": 15.445726071346726,
      "grad_norm": 9.936150550842285,
      "learning_rate": 3.069284241081659e-05,
      "loss": 1.8395,
      "step": 202200
    },
    {
      "epoch": 15.453364907188144,
      "grad_norm": 6.811467170715332,
      "learning_rate": 3.0683293866014826e-05,
      "loss": 1.8446,
      "step": 202300
    },
    {
      "epoch": 15.461003743029563,
      "grad_norm": 7.426900386810303,
      "learning_rate": 3.067374532121305e-05,
      "loss": 1.7365,
      "step": 202400
    },
    {
      "epoch": 15.46864257887098,
      "grad_norm": 6.80399227142334,
      "learning_rate": 3.0664196776411276e-05,
      "loss": 1.7743,
      "step": 202500
    },
    {
      "epoch": 15.476281414712398,
      "grad_norm": 8.798967361450195,
      "learning_rate": 3.0654648231609505e-05,
      "loss": 1.8716,
      "step": 202600
    },
    {
      "epoch": 15.483920250553815,
      "grad_norm": 7.80206823348999,
      "learning_rate": 3.0645099686807734e-05,
      "loss": 1.8339,
      "step": 202700
    },
    {
      "epoch": 15.491559086395233,
      "grad_norm": 6.289823532104492,
      "learning_rate": 3.0635551142005956e-05,
      "loss": 1.786,
      "step": 202800
    },
    {
      "epoch": 15.49919792223665,
      "grad_norm": 6.3960442543029785,
      "learning_rate": 3.062600259720419e-05,
      "loss": 1.8155,
      "step": 202900
    },
    {
      "epoch": 15.50683675807807,
      "grad_norm": 6.297511577606201,
      "learning_rate": 3.061645405240241e-05,
      "loss": 1.7985,
      "step": 203000
    },
    {
      "epoch": 15.514475593919487,
      "grad_norm": 6.949118614196777,
      "learning_rate": 3.060690550760064e-05,
      "loss": 1.7953,
      "step": 203100
    },
    {
      "epoch": 15.522114429760904,
      "grad_norm": 6.263204574584961,
      "learning_rate": 3.059735696279887e-05,
      "loss": 1.815,
      "step": 203200
    },
    {
      "epoch": 15.529753265602322,
      "grad_norm": 5.768085479736328,
      "learning_rate": 3.05878084179971e-05,
      "loss": 1.8519,
      "step": 203300
    },
    {
      "epoch": 15.53739210144374,
      "grad_norm": 5.485513210296631,
      "learning_rate": 3.057825987319533e-05,
      "loss": 1.7499,
      "step": 203400
    },
    {
      "epoch": 15.545030937285159,
      "grad_norm": 6.4178266525268555,
      "learning_rate": 3.0568711328393557e-05,
      "loss": 1.8152,
      "step": 203500
    },
    {
      "epoch": 15.552669773126576,
      "grad_norm": 6.454259872436523,
      "learning_rate": 3.0559162783591785e-05,
      "loss": 1.8291,
      "step": 203600
    },
    {
      "epoch": 15.560308608967993,
      "grad_norm": 5.263783931732178,
      "learning_rate": 3.054961423879001e-05,
      "loss": 1.8156,
      "step": 203700
    },
    {
      "epoch": 15.56794744480941,
      "grad_norm": 6.408703804016113,
      "learning_rate": 3.054006569398824e-05,
      "loss": 1.8452,
      "step": 203800
    },
    {
      "epoch": 15.575586280650828,
      "grad_norm": 4.614593982696533,
      "learning_rate": 3.0530517149186465e-05,
      "loss": 1.9296,
      "step": 203900
    },
    {
      "epoch": 15.583225116492248,
      "grad_norm": 6.614397048950195,
      "learning_rate": 3.052096860438469e-05,
      "loss": 1.7705,
      "step": 204000
    },
    {
      "epoch": 15.590863952333665,
      "grad_norm": 7.7851362228393555,
      "learning_rate": 3.0511420059582922e-05,
      "loss": 1.7389,
      "step": 204100
    },
    {
      "epoch": 15.598502788175082,
      "grad_norm": 5.380385875701904,
      "learning_rate": 3.050187151478115e-05,
      "loss": 1.7573,
      "step": 204200
    },
    {
      "epoch": 15.6061416240165,
      "grad_norm": 6.449427127838135,
      "learning_rate": 3.0492322969979376e-05,
      "loss": 1.8787,
      "step": 204300
    },
    {
      "epoch": 15.613780459857917,
      "grad_norm": 6.669731616973877,
      "learning_rate": 3.0482774425177608e-05,
      "loss": 1.8672,
      "step": 204400
    },
    {
      "epoch": 15.621419295699335,
      "grad_norm": 12.868766784667969,
      "learning_rate": 3.0473225880375833e-05,
      "loss": 1.8338,
      "step": 204500
    },
    {
      "epoch": 15.629058131540754,
      "grad_norm": 7.999019145965576,
      "learning_rate": 3.046367733557406e-05,
      "loss": 1.7588,
      "step": 204600
    },
    {
      "epoch": 15.636696967382171,
      "grad_norm": 7.218428611755371,
      "learning_rate": 3.045412879077229e-05,
      "loss": 1.7967,
      "step": 204700
    },
    {
      "epoch": 15.644335803223589,
      "grad_norm": 7.797420978546143,
      "learning_rate": 3.0444580245970516e-05,
      "loss": 1.8286,
      "step": 204800
    },
    {
      "epoch": 15.651974639065006,
      "grad_norm": 7.9979681968688965,
      "learning_rate": 3.043503170116874e-05,
      "loss": 1.818,
      "step": 204900
    },
    {
      "epoch": 15.659613474906424,
      "grad_norm": 8.147671699523926,
      "learning_rate": 3.042548315636697e-05,
      "loss": 1.742,
      "step": 205000
    },
    {
      "epoch": 15.667252310747841,
      "grad_norm": 7.010636329650879,
      "learning_rate": 3.04159346115652e-05,
      "loss": 1.7924,
      "step": 205100
    },
    {
      "epoch": 15.67489114658926,
      "grad_norm": 6.804633617401123,
      "learning_rate": 3.0406386066763427e-05,
      "loss": 1.8343,
      "step": 205200
    },
    {
      "epoch": 15.682529982430678,
      "grad_norm": 6.350664138793945,
      "learning_rate": 3.0396837521961653e-05,
      "loss": 1.9425,
      "step": 205300
    },
    {
      "epoch": 15.690168818272095,
      "grad_norm": 10.090653419494629,
      "learning_rate": 3.0387288977159885e-05,
      "loss": 1.7741,
      "step": 205400
    },
    {
      "epoch": 15.697807654113513,
      "grad_norm": 5.053668022155762,
      "learning_rate": 3.037774043235811e-05,
      "loss": 1.7971,
      "step": 205500
    },
    {
      "epoch": 15.70544648995493,
      "grad_norm": 7.958095550537109,
      "learning_rate": 3.0368191887556335e-05,
      "loss": 1.8441,
      "step": 205600
    },
    {
      "epoch": 15.71308532579635,
      "grad_norm": 6.694836616516113,
      "learning_rate": 3.0358643342754568e-05,
      "loss": 1.8079,
      "step": 205700
    },
    {
      "epoch": 15.720724161637767,
      "grad_norm": 5.548126697540283,
      "learning_rate": 3.0349094797952793e-05,
      "loss": 1.8566,
      "step": 205800
    },
    {
      "epoch": 15.728362997479184,
      "grad_norm": 8.651900291442871,
      "learning_rate": 3.0339546253151018e-05,
      "loss": 1.8479,
      "step": 205900
    },
    {
      "epoch": 15.736001833320602,
      "grad_norm": 7.340324878692627,
      "learning_rate": 3.032999770834925e-05,
      "loss": 1.8051,
      "step": 206000
    },
    {
      "epoch": 15.74364066916202,
      "grad_norm": 5.455927848815918,
      "learning_rate": 3.0320449163547475e-05,
      "loss": 1.7315,
      "step": 206100
    },
    {
      "epoch": 15.751279505003437,
      "grad_norm": 6.798962593078613,
      "learning_rate": 3.03109006187457e-05,
      "loss": 1.7761,
      "step": 206200
    },
    {
      "epoch": 15.758918340844856,
      "grad_norm": 7.796934604644775,
      "learning_rate": 3.0301352073943933e-05,
      "loss": 1.7753,
      "step": 206300
    },
    {
      "epoch": 15.766557176686273,
      "grad_norm": 7.187536239624023,
      "learning_rate": 3.0291803529142158e-05,
      "loss": 1.914,
      "step": 206400
    },
    {
      "epoch": 15.77419601252769,
      "grad_norm": 7.053549289703369,
      "learning_rate": 3.0282254984340387e-05,
      "loss": 1.7722,
      "step": 206500
    },
    {
      "epoch": 15.781834848369108,
      "grad_norm": 7.688111305236816,
      "learning_rate": 3.027270643953862e-05,
      "loss": 1.72,
      "step": 206600
    },
    {
      "epoch": 15.789473684210526,
      "grad_norm": 7.894654750823975,
      "learning_rate": 3.0263157894736844e-05,
      "loss": 1.867,
      "step": 206700
    },
    {
      "epoch": 15.797112520051945,
      "grad_norm": 9.048358917236328,
      "learning_rate": 3.025360934993507e-05,
      "loss": 1.7636,
      "step": 206800
    },
    {
      "epoch": 15.804751355893362,
      "grad_norm": 7.481373310089111,
      "learning_rate": 3.02440608051333e-05,
      "loss": 1.8207,
      "step": 206900
    },
    {
      "epoch": 15.81239019173478,
      "grad_norm": 6.681945323944092,
      "learning_rate": 3.0234512260331527e-05,
      "loss": 1.8607,
      "step": 207000
    },
    {
      "epoch": 15.820029027576197,
      "grad_norm": 6.979000091552734,
      "learning_rate": 3.0224963715529752e-05,
      "loss": 1.8135,
      "step": 207100
    },
    {
      "epoch": 15.827667863417615,
      "grad_norm": 7.754973888397217,
      "learning_rate": 3.0215415170727984e-05,
      "loss": 1.7639,
      "step": 207200
    },
    {
      "epoch": 15.835306699259032,
      "grad_norm": 6.266541004180908,
      "learning_rate": 3.020586662592621e-05,
      "loss": 1.8658,
      "step": 207300
    },
    {
      "epoch": 15.842945535100451,
      "grad_norm": 6.80443000793457,
      "learning_rate": 3.0196318081124435e-05,
      "loss": 1.7311,
      "step": 207400
    },
    {
      "epoch": 15.850584370941869,
      "grad_norm": 7.271223545074463,
      "learning_rate": 3.0186769536322667e-05,
      "loss": 1.9218,
      "step": 207500
    },
    {
      "epoch": 15.858223206783286,
      "grad_norm": 6.778144836425781,
      "learning_rate": 3.0177220991520892e-05,
      "loss": 1.8102,
      "step": 207600
    },
    {
      "epoch": 15.865862042624704,
      "grad_norm": 6.430551052093506,
      "learning_rate": 3.016767244671912e-05,
      "loss": 1.8567,
      "step": 207700
    },
    {
      "epoch": 15.873500878466121,
      "grad_norm": 4.627230167388916,
      "learning_rate": 3.015812390191735e-05,
      "loss": 1.7509,
      "step": 207800
    },
    {
      "epoch": 15.88113971430754,
      "grad_norm": 7.114925384521484,
      "learning_rate": 3.014857535711558e-05,
      "loss": 1.88,
      "step": 207900
    },
    {
      "epoch": 15.888778550148958,
      "grad_norm": 9.313143730163574,
      "learning_rate": 3.0139026812313804e-05,
      "loss": 1.8236,
      "step": 208000
    },
    {
      "epoch": 15.896417385990375,
      "grad_norm": 7.914331912994385,
      "learning_rate": 3.0129478267512036e-05,
      "loss": 1.875,
      "step": 208100
    },
    {
      "epoch": 15.904056221831793,
      "grad_norm": 7.145300388336182,
      "learning_rate": 3.011992972271026e-05,
      "loss": 1.8037,
      "step": 208200
    },
    {
      "epoch": 15.91169505767321,
      "grad_norm": 7.504497528076172,
      "learning_rate": 3.0110381177908486e-05,
      "loss": 1.8484,
      "step": 208300
    },
    {
      "epoch": 15.919333893514628,
      "grad_norm": 5.797214984893799,
      "learning_rate": 3.010083263310672e-05,
      "loss": 1.7689,
      "step": 208400
    },
    {
      "epoch": 15.926972729356047,
      "grad_norm": 6.731279373168945,
      "learning_rate": 3.0091284088304944e-05,
      "loss": 1.7886,
      "step": 208500
    },
    {
      "epoch": 15.934611565197464,
      "grad_norm": 6.431270599365234,
      "learning_rate": 3.008173554350317e-05,
      "loss": 1.7941,
      "step": 208600
    },
    {
      "epoch": 15.942250401038882,
      "grad_norm": 5.921044826507568,
      "learning_rate": 3.00721869987014e-05,
      "loss": 1.923,
      "step": 208700
    },
    {
      "epoch": 15.9498892368803,
      "grad_norm": 8.269485473632812,
      "learning_rate": 3.0062638453899626e-05,
      "loss": 1.8417,
      "step": 208800
    },
    {
      "epoch": 15.957528072721717,
      "grad_norm": 5.174891471862793,
      "learning_rate": 3.0053089909097852e-05,
      "loss": 1.7577,
      "step": 208900
    },
    {
      "epoch": 15.965166908563136,
      "grad_norm": 6.074162006378174,
      "learning_rate": 3.0043541364296084e-05,
      "loss": 1.9089,
      "step": 209000
    },
    {
      "epoch": 15.972805744404553,
      "grad_norm": 6.405548095703125,
      "learning_rate": 3.0033992819494313e-05,
      "loss": 1.8496,
      "step": 209100
    },
    {
      "epoch": 15.98044458024597,
      "grad_norm": 8.301740646362305,
      "learning_rate": 3.0024444274692538e-05,
      "loss": 1.775,
      "step": 209200
    },
    {
      "epoch": 15.988083416087388,
      "grad_norm": 5.713977336883545,
      "learning_rate": 3.001489572989077e-05,
      "loss": 1.7793,
      "step": 209300
    },
    {
      "epoch": 15.995722251928806,
      "grad_norm": 7.570613384246826,
      "learning_rate": 3.0005347185088995e-05,
      "loss": 1.8931,
      "step": 209400
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.8299696445465088,
      "eval_runtime": 2.9725,
      "eval_samples_per_second": 232.125,
      "eval_steps_per_second": 232.125,
      "step": 209456
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.580440878868103,
      "eval_runtime": 56.3839,
      "eval_samples_per_second": 232.176,
      "eval_steps_per_second": 232.176,
      "step": 209456
    },
    {
      "epoch": 16.003361087770223,
      "grad_norm": 10.032341003417969,
      "learning_rate": 2.999579864028722e-05,
      "loss": 1.8453,
      "step": 209500
    },
    {
      "epoch": 16.010999923611642,
      "grad_norm": 8.3373384475708,
      "learning_rate": 2.9986250095485453e-05,
      "loss": 1.821,
      "step": 209600
    },
    {
      "epoch": 16.018638759453058,
      "grad_norm": 6.134609222412109,
      "learning_rate": 2.9976701550683678e-05,
      "loss": 1.795,
      "step": 209700
    },
    {
      "epoch": 16.026277595294477,
      "grad_norm": 8.643655776977539,
      "learning_rate": 2.9967153005881903e-05,
      "loss": 1.7902,
      "step": 209800
    },
    {
      "epoch": 16.033916431135896,
      "grad_norm": 8.618485450744629,
      "learning_rate": 2.9957604461080135e-05,
      "loss": 1.758,
      "step": 209900
    },
    {
      "epoch": 16.041555266977312,
      "grad_norm": 6.715732097625732,
      "learning_rate": 2.994805591627836e-05,
      "loss": 1.7695,
      "step": 210000
    },
    {
      "epoch": 16.04919410281873,
      "grad_norm": 7.824522495269775,
      "learning_rate": 2.9938507371476586e-05,
      "loss": 1.7496,
      "step": 210100
    },
    {
      "epoch": 16.056832938660147,
      "grad_norm": 5.353793621063232,
      "learning_rate": 2.9928958826674818e-05,
      "loss": 1.8705,
      "step": 210200
    },
    {
      "epoch": 16.064471774501566,
      "grad_norm": 7.9726409912109375,
      "learning_rate": 2.9919410281873043e-05,
      "loss": 1.7011,
      "step": 210300
    },
    {
      "epoch": 16.072110610342985,
      "grad_norm": 7.607356548309326,
      "learning_rate": 2.9909861737071272e-05,
      "loss": 1.7332,
      "step": 210400
    },
    {
      "epoch": 16.0797494461844,
      "grad_norm": 7.4206414222717285,
      "learning_rate": 2.9900313192269497e-05,
      "loss": 1.8048,
      "step": 210500
    },
    {
      "epoch": 16.08738828202582,
      "grad_norm": 7.135745525360107,
      "learning_rate": 2.989076464746773e-05,
      "loss": 1.8657,
      "step": 210600
    },
    {
      "epoch": 16.095027117867236,
      "grad_norm": 6.779121398925781,
      "learning_rate": 2.9881216102665955e-05,
      "loss": 1.8203,
      "step": 210700
    },
    {
      "epoch": 16.102665953708655,
      "grad_norm": 9.73283863067627,
      "learning_rate": 2.987166755786418e-05,
      "loss": 1.6911,
      "step": 210800
    },
    {
      "epoch": 16.11030478955007,
      "grad_norm": 6.51403284072876,
      "learning_rate": 2.9862119013062412e-05,
      "loss": 1.8082,
      "step": 210900
    },
    {
      "epoch": 16.11794362539149,
      "grad_norm": 7.237917900085449,
      "learning_rate": 2.9852570468260637e-05,
      "loss": 1.7673,
      "step": 211000
    },
    {
      "epoch": 16.12558246123291,
      "grad_norm": 5.9743146896362305,
      "learning_rate": 2.9843021923458863e-05,
      "loss": 1.8197,
      "step": 211100
    },
    {
      "epoch": 16.133221297074325,
      "grad_norm": 7.6026506423950195,
      "learning_rate": 2.9833473378657095e-05,
      "loss": 1.7869,
      "step": 211200
    },
    {
      "epoch": 16.140860132915744,
      "grad_norm": 7.515950679779053,
      "learning_rate": 2.982392483385532e-05,
      "loss": 1.8133,
      "step": 211300
    },
    {
      "epoch": 16.14849896875716,
      "grad_norm": 8.851868629455566,
      "learning_rate": 2.981437628905355e-05,
      "loss": 1.8606,
      "step": 211400
    },
    {
      "epoch": 16.15613780459858,
      "grad_norm": 8.225741386413574,
      "learning_rate": 2.9804827744251777e-05,
      "loss": 1.7923,
      "step": 211500
    },
    {
      "epoch": 16.16377664044,
      "grad_norm": 7.549466133117676,
      "learning_rate": 2.9795279199450006e-05,
      "loss": 1.8082,
      "step": 211600
    },
    {
      "epoch": 16.171415476281414,
      "grad_norm": 6.450052261352539,
      "learning_rate": 2.978573065464823e-05,
      "loss": 1.8001,
      "step": 211700
    },
    {
      "epoch": 16.179054312122833,
      "grad_norm": 9.136712074279785,
      "learning_rate": 2.9776182109846464e-05,
      "loss": 1.6978,
      "step": 211800
    },
    {
      "epoch": 16.18669314796425,
      "grad_norm": 7.423101902008057,
      "learning_rate": 2.976663356504469e-05,
      "loss": 1.8301,
      "step": 211900
    },
    {
      "epoch": 16.194331983805668,
      "grad_norm": 6.925805568695068,
      "learning_rate": 2.9757085020242914e-05,
      "loss": 1.7813,
      "step": 212000
    },
    {
      "epoch": 16.201970819647087,
      "grad_norm": 8.165740966796875,
      "learning_rate": 2.9747536475441146e-05,
      "loss": 1.871,
      "step": 212100
    },
    {
      "epoch": 16.209609655488503,
      "grad_norm": 7.838139057159424,
      "learning_rate": 2.973798793063937e-05,
      "loss": 1.805,
      "step": 212200
    },
    {
      "epoch": 16.217248491329922,
      "grad_norm": 7.285720348358154,
      "learning_rate": 2.9728439385837597e-05,
      "loss": 1.7794,
      "step": 212300
    },
    {
      "epoch": 16.224887327171338,
      "grad_norm": 6.180286884307861,
      "learning_rate": 2.971889084103583e-05,
      "loss": 1.8104,
      "step": 212400
    },
    {
      "epoch": 16.232526163012757,
      "grad_norm": 6.986376762390137,
      "learning_rate": 2.9709342296234054e-05,
      "loss": 1.7765,
      "step": 212500
    },
    {
      "epoch": 16.240164998854176,
      "grad_norm": 6.538605213165283,
      "learning_rate": 2.969979375143228e-05,
      "loss": 1.7184,
      "step": 212600
    },
    {
      "epoch": 16.247803834695592,
      "grad_norm": 6.627618312835693,
      "learning_rate": 2.969024520663051e-05,
      "loss": 1.7735,
      "step": 212700
    },
    {
      "epoch": 16.25544267053701,
      "grad_norm": 8.563419342041016,
      "learning_rate": 2.9680696661828737e-05,
      "loss": 1.8541,
      "step": 212800
    },
    {
      "epoch": 16.263081506378427,
      "grad_norm": 5.790489673614502,
      "learning_rate": 2.9671148117026966e-05,
      "loss": 1.7159,
      "step": 212900
    },
    {
      "epoch": 16.270720342219846,
      "grad_norm": 6.745151996612549,
      "learning_rate": 2.9661599572225194e-05,
      "loss": 1.8631,
      "step": 213000
    },
    {
      "epoch": 16.27835917806126,
      "grad_norm": 8.034250259399414,
      "learning_rate": 2.9652051027423423e-05,
      "loss": 1.8038,
      "step": 213100
    },
    {
      "epoch": 16.28599801390268,
      "grad_norm": 7.282996654510498,
      "learning_rate": 2.964250248262165e-05,
      "loss": 1.7523,
      "step": 213200
    },
    {
      "epoch": 16.2936368497441,
      "grad_norm": 7.648863315582275,
      "learning_rate": 2.963295393781988e-05,
      "loss": 1.8297,
      "step": 213300
    },
    {
      "epoch": 16.301275685585516,
      "grad_norm": 5.024760723114014,
      "learning_rate": 2.9623405393018106e-05,
      "loss": 1.831,
      "step": 213400
    },
    {
      "epoch": 16.308914521426935,
      "grad_norm": 7.854061603546143,
      "learning_rate": 2.961385684821633e-05,
      "loss": 1.7061,
      "step": 213500
    },
    {
      "epoch": 16.31655335726835,
      "grad_norm": 9.43213176727295,
      "learning_rate": 2.9604308303414563e-05,
      "loss": 1.763,
      "step": 213600
    },
    {
      "epoch": 16.32419219310977,
      "grad_norm": 6.773952484130859,
      "learning_rate": 2.959475975861279e-05,
      "loss": 1.781,
      "step": 213700
    },
    {
      "epoch": 16.33183102895119,
      "grad_norm": 7.441649436950684,
      "learning_rate": 2.9585211213811014e-05,
      "loss": 1.7448,
      "step": 213800
    },
    {
      "epoch": 16.339469864792605,
      "grad_norm": 7.455110549926758,
      "learning_rate": 2.9575662669009246e-05,
      "loss": 1.9163,
      "step": 213900
    },
    {
      "epoch": 16.347108700634024,
      "grad_norm": 5.844193935394287,
      "learning_rate": 2.956611412420747e-05,
      "loss": 1.8991,
      "step": 214000
    },
    {
      "epoch": 16.35474753647544,
      "grad_norm": 9.738507270812988,
      "learning_rate": 2.95565655794057e-05,
      "loss": 1.848,
      "step": 214100
    },
    {
      "epoch": 16.36238637231686,
      "grad_norm": 8.441614151000977,
      "learning_rate": 2.954701703460393e-05,
      "loss": 1.7598,
      "step": 214200
    },
    {
      "epoch": 16.370025208158278,
      "grad_norm": 4.8719611167907715,
      "learning_rate": 2.9537468489802157e-05,
      "loss": 1.8285,
      "step": 214300
    },
    {
      "epoch": 16.377664043999694,
      "grad_norm": 8.059210777282715,
      "learning_rate": 2.9527919945000382e-05,
      "loss": 1.8608,
      "step": 214400
    },
    {
      "epoch": 16.385302879841113,
      "grad_norm": 6.814229965209961,
      "learning_rate": 2.9518371400198615e-05,
      "loss": 1.7452,
      "step": 214500
    },
    {
      "epoch": 16.39294171568253,
      "grad_norm": 7.69185733795166,
      "learning_rate": 2.950882285539684e-05,
      "loss": 1.8689,
      "step": 214600
    },
    {
      "epoch": 16.400580551523948,
      "grad_norm": 9.905681610107422,
      "learning_rate": 2.9499274310595065e-05,
      "loss": 1.796,
      "step": 214700
    },
    {
      "epoch": 16.408219387365367,
      "grad_norm": 7.221034049987793,
      "learning_rate": 2.9489725765793297e-05,
      "loss": 1.7937,
      "step": 214800
    },
    {
      "epoch": 16.415858223206783,
      "grad_norm": 7.263393402099609,
      "learning_rate": 2.9480177220991523e-05,
      "loss": 1.8565,
      "step": 214900
    },
    {
      "epoch": 16.423497059048202,
      "grad_norm": 7.251102447509766,
      "learning_rate": 2.9470628676189748e-05,
      "loss": 1.8085,
      "step": 215000
    },
    {
      "epoch": 16.431135894889618,
      "grad_norm": 5.672348499298096,
      "learning_rate": 2.946108013138798e-05,
      "loss": 1.7637,
      "step": 215100
    },
    {
      "epoch": 16.438774730731037,
      "grad_norm": 9.071537017822266,
      "learning_rate": 2.9451531586586205e-05,
      "loss": 1.8609,
      "step": 215200
    },
    {
      "epoch": 16.446413566572453,
      "grad_norm": 8.287641525268555,
      "learning_rate": 2.944198304178443e-05,
      "loss": 1.8695,
      "step": 215300
    },
    {
      "epoch": 16.454052402413872,
      "grad_norm": 6.493729114532471,
      "learning_rate": 2.9432434496982663e-05,
      "loss": 1.8918,
      "step": 215400
    },
    {
      "epoch": 16.46169123825529,
      "grad_norm": 7.204474449157715,
      "learning_rate": 2.942288595218089e-05,
      "loss": 1.8107,
      "step": 215500
    },
    {
      "epoch": 16.469330074096707,
      "grad_norm": 6.691741943359375,
      "learning_rate": 2.9413337407379117e-05,
      "loss": 1.8175,
      "step": 215600
    },
    {
      "epoch": 16.476968909938126,
      "grad_norm": 9.413825035095215,
      "learning_rate": 2.940378886257735e-05,
      "loss": 1.7308,
      "step": 215700
    },
    {
      "epoch": 16.48460774577954,
      "grad_norm": 8.077703475952148,
      "learning_rate": 2.9394240317775574e-05,
      "loss": 1.7956,
      "step": 215800
    },
    {
      "epoch": 16.49224658162096,
      "grad_norm": 6.293660640716553,
      "learning_rate": 2.93846917729738e-05,
      "loss": 1.8915,
      "step": 215900
    },
    {
      "epoch": 16.49988541746238,
      "grad_norm": 9.00628662109375,
      "learning_rate": 2.937514322817203e-05,
      "loss": 1.8017,
      "step": 216000
    },
    {
      "epoch": 16.507524253303796,
      "grad_norm": 5.863797187805176,
      "learning_rate": 2.9365594683370257e-05,
      "loss": 1.7743,
      "step": 216100
    },
    {
      "epoch": 16.515163089145215,
      "grad_norm": 5.717832088470459,
      "learning_rate": 2.9356046138568482e-05,
      "loss": 1.7519,
      "step": 216200
    },
    {
      "epoch": 16.52280192498663,
      "grad_norm": 4.842100143432617,
      "learning_rate": 2.9346497593766707e-05,
      "loss": 1.9131,
      "step": 216300
    },
    {
      "epoch": 16.53044076082805,
      "grad_norm": 6.229935169219971,
      "learning_rate": 2.933694904896494e-05,
      "loss": 1.8345,
      "step": 216400
    },
    {
      "epoch": 16.53807959666947,
      "grad_norm": 9.107804298400879,
      "learning_rate": 2.9327400504163165e-05,
      "loss": 1.8403,
      "step": 216500
    },
    {
      "epoch": 16.545718432510885,
      "grad_norm": 8.501057624816895,
      "learning_rate": 2.9317851959361393e-05,
      "loss": 1.8027,
      "step": 216600
    },
    {
      "epoch": 16.553357268352304,
      "grad_norm": 6.385912895202637,
      "learning_rate": 2.9308303414559622e-05,
      "loss": 1.9647,
      "step": 216700
    },
    {
      "epoch": 16.56099610419372,
      "grad_norm": 7.765920162200928,
      "learning_rate": 2.929875486975785e-05,
      "loss": 1.7618,
      "step": 216800
    },
    {
      "epoch": 16.56863494003514,
      "grad_norm": 6.742931842803955,
      "learning_rate": 2.9289206324956076e-05,
      "loss": 1.7091,
      "step": 216900
    },
    {
      "epoch": 16.576273775876558,
      "grad_norm": 6.51510763168335,
      "learning_rate": 2.9279657780154308e-05,
      "loss": 1.7199,
      "step": 217000
    },
    {
      "epoch": 16.583912611717974,
      "grad_norm": 9.974574089050293,
      "learning_rate": 2.9270109235352533e-05,
      "loss": 1.7253,
      "step": 217100
    },
    {
      "epoch": 16.591551447559393,
      "grad_norm": 8.77800178527832,
      "learning_rate": 2.926056069055076e-05,
      "loss": 1.8359,
      "step": 217200
    },
    {
      "epoch": 16.59919028340081,
      "grad_norm": 5.491400718688965,
      "learning_rate": 2.925101214574899e-05,
      "loss": 1.8006,
      "step": 217300
    },
    {
      "epoch": 16.606829119242228,
      "grad_norm": 7.60468053817749,
      "learning_rate": 2.9241463600947216e-05,
      "loss": 1.7803,
      "step": 217400
    },
    {
      "epoch": 16.614467955083644,
      "grad_norm": 6.676432132720947,
      "learning_rate": 2.923191505614544e-05,
      "loss": 1.8142,
      "step": 217500
    },
    {
      "epoch": 16.622106790925063,
      "grad_norm": 6.97766637802124,
      "learning_rate": 2.9222366511343674e-05,
      "loss": 1.8163,
      "step": 217600
    },
    {
      "epoch": 16.629745626766482,
      "grad_norm": 8.390186309814453,
      "learning_rate": 2.92128179665419e-05,
      "loss": 1.9356,
      "step": 217700
    },
    {
      "epoch": 16.637384462607898,
      "grad_norm": 7.548335075378418,
      "learning_rate": 2.9203269421740128e-05,
      "loss": 1.9405,
      "step": 217800
    },
    {
      "epoch": 16.645023298449317,
      "grad_norm": 7.389998912811279,
      "learning_rate": 2.9193720876938356e-05,
      "loss": 1.7922,
      "step": 217900
    },
    {
      "epoch": 16.652662134290733,
      "grad_norm": 7.052065372467041,
      "learning_rate": 2.9184172332136585e-05,
      "loss": 1.8262,
      "step": 218000
    },
    {
      "epoch": 16.660300970132152,
      "grad_norm": 6.977088928222656,
      "learning_rate": 2.917462378733481e-05,
      "loss": 1.739,
      "step": 218100
    },
    {
      "epoch": 16.66793980597357,
      "grad_norm": 8.670587539672852,
      "learning_rate": 2.9165075242533042e-05,
      "loss": 1.8078,
      "step": 218200
    },
    {
      "epoch": 16.675578641814987,
      "grad_norm": 9.794130325317383,
      "learning_rate": 2.9155526697731268e-05,
      "loss": 1.6557,
      "step": 218300
    },
    {
      "epoch": 16.683217477656406,
      "grad_norm": 6.194499492645264,
      "learning_rate": 2.9145978152929493e-05,
      "loss": 1.752,
      "step": 218400
    },
    {
      "epoch": 16.69085631349782,
      "grad_norm": 7.705116271972656,
      "learning_rate": 2.9136429608127725e-05,
      "loss": 1.8084,
      "step": 218500
    },
    {
      "epoch": 16.69849514933924,
      "grad_norm": 5.760083198547363,
      "learning_rate": 2.912688106332595e-05,
      "loss": 1.737,
      "step": 218600
    },
    {
      "epoch": 16.70613398518066,
      "grad_norm": 9.053531646728516,
      "learning_rate": 2.9117332518524176e-05,
      "loss": 1.7873,
      "step": 218700
    },
    {
      "epoch": 16.713772821022076,
      "grad_norm": 8.113349914550781,
      "learning_rate": 2.9107783973722408e-05,
      "loss": 1.8996,
      "step": 218800
    },
    {
      "epoch": 16.721411656863495,
      "grad_norm": 7.875026226043701,
      "learning_rate": 2.9098235428920633e-05,
      "loss": 1.8491,
      "step": 218900
    },
    {
      "epoch": 16.72905049270491,
      "grad_norm": 6.380151271820068,
      "learning_rate": 2.908868688411886e-05,
      "loss": 1.7621,
      "step": 219000
    },
    {
      "epoch": 16.73668932854633,
      "grad_norm": 5.69621467590332,
      "learning_rate": 2.907913833931709e-05,
      "loss": 1.7669,
      "step": 219100
    },
    {
      "epoch": 16.744328164387746,
      "grad_norm": 6.653720378875732,
      "learning_rate": 2.9069589794515316e-05,
      "loss": 1.8053,
      "step": 219200
    },
    {
      "epoch": 16.751967000229165,
      "grad_norm": 7.250881195068359,
      "learning_rate": 2.9060041249713544e-05,
      "loss": 1.7648,
      "step": 219300
    },
    {
      "epoch": 16.759605836070584,
      "grad_norm": 5.620666027069092,
      "learning_rate": 2.9050492704911773e-05,
      "loss": 1.8475,
      "step": 219400
    },
    {
      "epoch": 16.767244671912,
      "grad_norm": 9.306373596191406,
      "learning_rate": 2.9040944160110002e-05,
      "loss": 1.8744,
      "step": 219500
    },
    {
      "epoch": 16.77488350775342,
      "grad_norm": 7.0125627517700195,
      "learning_rate": 2.9031395615308227e-05,
      "loss": 1.7719,
      "step": 219600
    },
    {
      "epoch": 16.782522343594835,
      "grad_norm": 6.103316307067871,
      "learning_rate": 2.902184707050646e-05,
      "loss": 1.8069,
      "step": 219700
    },
    {
      "epoch": 16.790161179436254,
      "grad_norm": 7.273890495300293,
      "learning_rate": 2.9012298525704684e-05,
      "loss": 1.8191,
      "step": 219800
    },
    {
      "epoch": 16.797800015277673,
      "grad_norm": 9.259002685546875,
      "learning_rate": 2.900274998090291e-05,
      "loss": 1.8326,
      "step": 219900
    },
    {
      "epoch": 16.80543885111909,
      "grad_norm": 6.51919412612915,
      "learning_rate": 2.8993201436101142e-05,
      "loss": 1.8622,
      "step": 220000
    },
    {
      "epoch": 16.813077686960508,
      "grad_norm": 5.75629186630249,
      "learning_rate": 2.8983652891299367e-05,
      "loss": 1.8711,
      "step": 220100
    },
    {
      "epoch": 16.820716522801924,
      "grad_norm": 6.576385498046875,
      "learning_rate": 2.8974104346497592e-05,
      "loss": 1.8013,
      "step": 220200
    },
    {
      "epoch": 16.828355358643343,
      "grad_norm": 7.537395000457764,
      "learning_rate": 2.8964555801695825e-05,
      "loss": 1.8593,
      "step": 220300
    },
    {
      "epoch": 16.835994194484762,
      "grad_norm": 4.785842418670654,
      "learning_rate": 2.895500725689405e-05,
      "loss": 1.8469,
      "step": 220400
    },
    {
      "epoch": 16.843633030326178,
      "grad_norm": 5.560904502868652,
      "learning_rate": 2.894545871209228e-05,
      "loss": 1.8589,
      "step": 220500
    },
    {
      "epoch": 16.851271866167597,
      "grad_norm": 6.458804130554199,
      "learning_rate": 2.8935910167290507e-05,
      "loss": 1.7754,
      "step": 220600
    },
    {
      "epoch": 16.858910702009013,
      "grad_norm": 5.668095111846924,
      "learning_rate": 2.8926361622488736e-05,
      "loss": 1.7917,
      "step": 220700
    },
    {
      "epoch": 16.86654953785043,
      "grad_norm": 6.217703342437744,
      "learning_rate": 2.891681307768696e-05,
      "loss": 1.7242,
      "step": 220800
    },
    {
      "epoch": 16.87418837369185,
      "grad_norm": 6.939016819000244,
      "learning_rate": 2.8907264532885193e-05,
      "loss": 1.7846,
      "step": 220900
    },
    {
      "epoch": 16.881827209533267,
      "grad_norm": 4.927008628845215,
      "learning_rate": 2.889771598808342e-05,
      "loss": 1.8365,
      "step": 221000
    },
    {
      "epoch": 16.889466045374686,
      "grad_norm": 7.97245454788208,
      "learning_rate": 2.8888167443281644e-05,
      "loss": 1.8042,
      "step": 221100
    },
    {
      "epoch": 16.8971048812161,
      "grad_norm": 6.526853561401367,
      "learning_rate": 2.8878618898479876e-05,
      "loss": 1.8756,
      "step": 221200
    },
    {
      "epoch": 16.90474371705752,
      "grad_norm": 8.567038536071777,
      "learning_rate": 2.88690703536781e-05,
      "loss": 1.8244,
      "step": 221300
    },
    {
      "epoch": 16.91238255289894,
      "grad_norm": 8.436361312866211,
      "learning_rate": 2.8859521808876327e-05,
      "loss": 1.8227,
      "step": 221400
    },
    {
      "epoch": 16.920021388740356,
      "grad_norm": 7.050465106964111,
      "learning_rate": 2.884997326407456e-05,
      "loss": 1.8055,
      "step": 221500
    },
    {
      "epoch": 16.927660224581775,
      "grad_norm": 7.172518730163574,
      "learning_rate": 2.8840424719272784e-05,
      "loss": 1.9123,
      "step": 221600
    },
    {
      "epoch": 16.93529906042319,
      "grad_norm": 10.423399925231934,
      "learning_rate": 2.883087617447101e-05,
      "loss": 1.777,
      "step": 221700
    },
    {
      "epoch": 16.94293789626461,
      "grad_norm": 8.92538070678711,
      "learning_rate": 2.882132762966924e-05,
      "loss": 1.831,
      "step": 221800
    },
    {
      "epoch": 16.950576732106025,
      "grad_norm": 6.40793514251709,
      "learning_rate": 2.881177908486747e-05,
      "loss": 1.799,
      "step": 221900
    },
    {
      "epoch": 16.958215567947445,
      "grad_norm": 5.976106643676758,
      "learning_rate": 2.8802230540065695e-05,
      "loss": 1.7275,
      "step": 222000
    },
    {
      "epoch": 16.965854403788864,
      "grad_norm": 7.8568525314331055,
      "learning_rate": 2.879268199526392e-05,
      "loss": 1.7083,
      "step": 222100
    },
    {
      "epoch": 16.97349323963028,
      "grad_norm": 8.980036735534668,
      "learning_rate": 2.8783133450462153e-05,
      "loss": 1.8101,
      "step": 222200
    },
    {
      "epoch": 16.9811320754717,
      "grad_norm": 6.749788761138916,
      "learning_rate": 2.8773584905660378e-05,
      "loss": 1.7406,
      "step": 222300
    },
    {
      "epoch": 16.988770911313114,
      "grad_norm": 6.102129936218262,
      "learning_rate": 2.8764036360858603e-05,
      "loss": 1.7717,
      "step": 222400
    },
    {
      "epoch": 16.996409747154534,
      "grad_norm": 5.944299697875977,
      "learning_rate": 2.8754487816056835e-05,
      "loss": 1.7398,
      "step": 222500
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.8334380388259888,
      "eval_runtime": 2.9945,
      "eval_samples_per_second": 230.419,
      "eval_steps_per_second": 230.419,
      "step": 222547
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.5780305862426758,
      "eval_runtime": 56.5065,
      "eval_samples_per_second": 231.673,
      "eval_steps_per_second": 231.673,
      "step": 222547
    },
    {
      "epoch": 17.004048582995953,
      "grad_norm": 8.501283645629883,
      "learning_rate": 2.874493927125506e-05,
      "loss": 1.8084,
      "step": 222600
    },
    {
      "epoch": 17.01168741883737,
      "grad_norm": 7.758347988128662,
      "learning_rate": 2.8735390726453286e-05,
      "loss": 1.8548,
      "step": 222700
    },
    {
      "epoch": 17.019326254678788,
      "grad_norm": 7.021058559417725,
      "learning_rate": 2.8725842181651518e-05,
      "loss": 1.7133,
      "step": 222800
    },
    {
      "epoch": 17.026965090520203,
      "grad_norm": 6.949697494506836,
      "learning_rate": 2.8716293636849743e-05,
      "loss": 1.77,
      "step": 222900
    },
    {
      "epoch": 17.034603926361623,
      "grad_norm": 9.012808799743652,
      "learning_rate": 2.8706745092047972e-05,
      "loss": 1.7536,
      "step": 223000
    },
    {
      "epoch": 17.042242762203042,
      "grad_norm": 5.79227876663208,
      "learning_rate": 2.86971965472462e-05,
      "loss": 1.798,
      "step": 223100
    },
    {
      "epoch": 17.049881598044458,
      "grad_norm": 6.7834649085998535,
      "learning_rate": 2.868764800244443e-05,
      "loss": 1.6845,
      "step": 223200
    },
    {
      "epoch": 17.057520433885877,
      "grad_norm": 8.218812942504883,
      "learning_rate": 2.8678099457642655e-05,
      "loss": 1.751,
      "step": 223300
    },
    {
      "epoch": 17.065159269727292,
      "grad_norm": 7.273437976837158,
      "learning_rate": 2.8668550912840887e-05,
      "loss": 1.7143,
      "step": 223400
    },
    {
      "epoch": 17.07279810556871,
      "grad_norm": 9.864053726196289,
      "learning_rate": 2.8659002368039112e-05,
      "loss": 1.764,
      "step": 223500
    },
    {
      "epoch": 17.080436941410127,
      "grad_norm": 6.821249961853027,
      "learning_rate": 2.8649453823237338e-05,
      "loss": 1.78,
      "step": 223600
    },
    {
      "epoch": 17.088075777251547,
      "grad_norm": 8.986919403076172,
      "learning_rate": 2.863990527843557e-05,
      "loss": 1.8299,
      "step": 223700
    },
    {
      "epoch": 17.095714613092966,
      "grad_norm": 6.564943790435791,
      "learning_rate": 2.8630356733633795e-05,
      "loss": 1.7268,
      "step": 223800
    },
    {
      "epoch": 17.10335344893438,
      "grad_norm": 7.446660995483398,
      "learning_rate": 2.862080818883202e-05,
      "loss": 1.7262,
      "step": 223900
    },
    {
      "epoch": 17.1109922847758,
      "grad_norm": 7.3319854736328125,
      "learning_rate": 2.8611259644030252e-05,
      "loss": 1.8278,
      "step": 224000
    },
    {
      "epoch": 17.118631120617216,
      "grad_norm": 7.550279140472412,
      "learning_rate": 2.8601711099228478e-05,
      "loss": 1.7924,
      "step": 224100
    },
    {
      "epoch": 17.126269956458636,
      "grad_norm": 7.549015998840332,
      "learning_rate": 2.8592162554426706e-05,
      "loss": 1.7564,
      "step": 224200
    },
    {
      "epoch": 17.133908792300055,
      "grad_norm": 4.636620998382568,
      "learning_rate": 2.8582614009624935e-05,
      "loss": 1.7939,
      "step": 224300
    },
    {
      "epoch": 17.14154762814147,
      "grad_norm": 6.060583591461182,
      "learning_rate": 2.8573065464823164e-05,
      "loss": 1.7695,
      "step": 224400
    },
    {
      "epoch": 17.14918646398289,
      "grad_norm": 8.071826934814453,
      "learning_rate": 2.856351692002139e-05,
      "loss": 1.7904,
      "step": 224500
    },
    {
      "epoch": 17.156825299824305,
      "grad_norm": 7.7515106201171875,
      "learning_rate": 2.855396837521962e-05,
      "loss": 1.8001,
      "step": 224600
    },
    {
      "epoch": 17.164464135665725,
      "grad_norm": 6.645386695861816,
      "learning_rate": 2.8544419830417846e-05,
      "loss": 1.7561,
      "step": 224700
    },
    {
      "epoch": 17.172102971507144,
      "grad_norm": 6.277299880981445,
      "learning_rate": 2.8534871285616072e-05,
      "loss": 1.7789,
      "step": 224800
    },
    {
      "epoch": 17.17974180734856,
      "grad_norm": 7.751746654510498,
      "learning_rate": 2.8525322740814304e-05,
      "loss": 1.794,
      "step": 224900
    },
    {
      "epoch": 17.18738064318998,
      "grad_norm": 8.387718200683594,
      "learning_rate": 2.851577419601253e-05,
      "loss": 1.8251,
      "step": 225000
    },
    {
      "epoch": 17.195019479031394,
      "grad_norm": 8.835945129394531,
      "learning_rate": 2.8506225651210754e-05,
      "loss": 1.726,
      "step": 225100
    },
    {
      "epoch": 17.202658314872814,
      "grad_norm": 7.71370792388916,
      "learning_rate": 2.8496677106408986e-05,
      "loss": 1.8139,
      "step": 225200
    },
    {
      "epoch": 17.210297150714233,
      "grad_norm": 6.263960838317871,
      "learning_rate": 2.8487128561607212e-05,
      "loss": 1.8348,
      "step": 225300
    },
    {
      "epoch": 17.21793598655565,
      "grad_norm": 8.83164119720459,
      "learning_rate": 2.8477580016805437e-05,
      "loss": 1.7319,
      "step": 225400
    },
    {
      "epoch": 17.225574822397068,
      "grad_norm": 5.745524883270264,
      "learning_rate": 2.846803147200367e-05,
      "loss": 1.7547,
      "step": 225500
    },
    {
      "epoch": 17.233213658238483,
      "grad_norm": 7.342573165893555,
      "learning_rate": 2.8458482927201894e-05,
      "loss": 1.7715,
      "step": 225600
    },
    {
      "epoch": 17.240852494079903,
      "grad_norm": 6.808232307434082,
      "learning_rate": 2.8448934382400123e-05,
      "loss": 1.7616,
      "step": 225700
    },
    {
      "epoch": 17.24849132992132,
      "grad_norm": 8.041102409362793,
      "learning_rate": 2.8439385837598352e-05,
      "loss": 1.7318,
      "step": 225800
    },
    {
      "epoch": 17.256130165762738,
      "grad_norm": 7.440920829772949,
      "learning_rate": 2.842983729279658e-05,
      "loss": 1.8226,
      "step": 225900
    },
    {
      "epoch": 17.263769001604157,
      "grad_norm": 7.081944942474365,
      "learning_rate": 2.8420288747994806e-05,
      "loss": 1.7378,
      "step": 226000
    },
    {
      "epoch": 17.271407837445572,
      "grad_norm": 8.28518009185791,
      "learning_rate": 2.8410740203193038e-05,
      "loss": 1.8076,
      "step": 226100
    },
    {
      "epoch": 17.27904667328699,
      "grad_norm": 6.681065559387207,
      "learning_rate": 2.8401191658391263e-05,
      "loss": 1.806,
      "step": 226200
    },
    {
      "epoch": 17.286685509128407,
      "grad_norm": 7.123602867126465,
      "learning_rate": 2.839164311358949e-05,
      "loss": 1.7343,
      "step": 226300
    },
    {
      "epoch": 17.294324344969827,
      "grad_norm": 8.285400390625,
      "learning_rate": 2.838209456878772e-05,
      "loss": 1.7681,
      "step": 226400
    },
    {
      "epoch": 17.301963180811246,
      "grad_norm": 6.734460353851318,
      "learning_rate": 2.8372546023985946e-05,
      "loss": 1.7944,
      "step": 226500
    },
    {
      "epoch": 17.30960201665266,
      "grad_norm": 7.020438194274902,
      "learning_rate": 2.836299747918417e-05,
      "loss": 1.8145,
      "step": 226600
    },
    {
      "epoch": 17.31724085249408,
      "grad_norm": 9.930909156799316,
      "learning_rate": 2.8353448934382403e-05,
      "loss": 1.8255,
      "step": 226700
    },
    {
      "epoch": 17.324879688335496,
      "grad_norm": 7.948287010192871,
      "learning_rate": 2.834390038958063e-05,
      "loss": 1.799,
      "step": 226800
    },
    {
      "epoch": 17.332518524176916,
      "grad_norm": 5.893709182739258,
      "learning_rate": 2.8334351844778857e-05,
      "loss": 1.791,
      "step": 226900
    },
    {
      "epoch": 17.340157360018335,
      "grad_norm": 7.774892330169678,
      "learning_rate": 2.8324803299977086e-05,
      "loss": 1.8178,
      "step": 227000
    },
    {
      "epoch": 17.34779619585975,
      "grad_norm": 6.677944660186768,
      "learning_rate": 2.8315254755175315e-05,
      "loss": 1.7693,
      "step": 227100
    },
    {
      "epoch": 17.35543503170117,
      "grad_norm": 7.158008575439453,
      "learning_rate": 2.830570621037354e-05,
      "loss": 1.7969,
      "step": 227200
    },
    {
      "epoch": 17.363073867542585,
      "grad_norm": 7.139148235321045,
      "learning_rate": 2.8296157665571772e-05,
      "loss": 1.8165,
      "step": 227300
    },
    {
      "epoch": 17.370712703384005,
      "grad_norm": 7.827920913696289,
      "learning_rate": 2.8286609120769997e-05,
      "loss": 1.8133,
      "step": 227400
    },
    {
      "epoch": 17.378351539225424,
      "grad_norm": 7.8352813720703125,
      "learning_rate": 2.8277060575968223e-05,
      "loss": 1.7456,
      "step": 227500
    },
    {
      "epoch": 17.38599037506684,
      "grad_norm": 6.750216484069824,
      "learning_rate": 2.8267512031166455e-05,
      "loss": 1.8195,
      "step": 227600
    },
    {
      "epoch": 17.39362921090826,
      "grad_norm": 7.190784931182861,
      "learning_rate": 2.825796348636468e-05,
      "loss": 1.8437,
      "step": 227700
    },
    {
      "epoch": 17.401268046749674,
      "grad_norm": 9.199481964111328,
      "learning_rate": 2.8248414941562905e-05,
      "loss": 1.7293,
      "step": 227800
    },
    {
      "epoch": 17.408906882591094,
      "grad_norm": 7.744147777557373,
      "learning_rate": 2.823886639676113e-05,
      "loss": 1.7828,
      "step": 227900
    },
    {
      "epoch": 17.41654571843251,
      "grad_norm": 8.522337913513184,
      "learning_rate": 2.8229317851959363e-05,
      "loss": 1.8933,
      "step": 228000
    },
    {
      "epoch": 17.42418455427393,
      "grad_norm": 10.463046073913574,
      "learning_rate": 2.8219769307157588e-05,
      "loss": 1.82,
      "step": 228100
    },
    {
      "epoch": 17.431823390115348,
      "grad_norm": 6.525068283081055,
      "learning_rate": 2.8210220762355817e-05,
      "loss": 1.8242,
      "step": 228200
    },
    {
      "epoch": 17.439462225956763,
      "grad_norm": 6.6699538230896,
      "learning_rate": 2.820067221755405e-05,
      "loss": 1.7152,
      "step": 228300
    },
    {
      "epoch": 17.447101061798183,
      "grad_norm": 5.81090784072876,
      "learning_rate": 2.8191123672752274e-05,
      "loss": 1.7871,
      "step": 228400
    },
    {
      "epoch": 17.454739897639598,
      "grad_norm": 5.4309210777282715,
      "learning_rate": 2.81815751279505e-05,
      "loss": 1.7312,
      "step": 228500
    },
    {
      "epoch": 17.462378733481017,
      "grad_norm": 7.293336868286133,
      "learning_rate": 2.817202658314873e-05,
      "loss": 1.7602,
      "step": 228600
    },
    {
      "epoch": 17.470017569322437,
      "grad_norm": 6.428971767425537,
      "learning_rate": 2.8162478038346957e-05,
      "loss": 1.7951,
      "step": 228700
    },
    {
      "epoch": 17.477656405163852,
      "grad_norm": 5.8258819580078125,
      "learning_rate": 2.8152929493545182e-05,
      "loss": 1.8236,
      "step": 228800
    },
    {
      "epoch": 17.48529524100527,
      "grad_norm": 7.2552714347839355,
      "learning_rate": 2.8143380948743414e-05,
      "loss": 1.7847,
      "step": 228900
    },
    {
      "epoch": 17.492934076846687,
      "grad_norm": 11.304295539855957,
      "learning_rate": 2.813383240394164e-05,
      "loss": 1.8008,
      "step": 229000
    },
    {
      "epoch": 17.500572912688106,
      "grad_norm": 5.817629814147949,
      "learning_rate": 2.8124283859139865e-05,
      "loss": 1.8196,
      "step": 229100
    },
    {
      "epoch": 17.508211748529526,
      "grad_norm": 7.180692672729492,
      "learning_rate": 2.8114735314338097e-05,
      "loss": 1.7274,
      "step": 229200
    },
    {
      "epoch": 17.51585058437094,
      "grad_norm": 4.3508687019348145,
      "learning_rate": 2.8105186769536322e-05,
      "loss": 1.7473,
      "step": 229300
    },
    {
      "epoch": 17.52348942021236,
      "grad_norm": 7.7660298347473145,
      "learning_rate": 2.809563822473455e-05,
      "loss": 1.8587,
      "step": 229400
    },
    {
      "epoch": 17.531128256053776,
      "grad_norm": 6.93145227432251,
      "learning_rate": 2.808608967993278e-05,
      "loss": 1.7933,
      "step": 229500
    },
    {
      "epoch": 17.538767091895195,
      "grad_norm": 8.597105979919434,
      "learning_rate": 2.807654113513101e-05,
      "loss": 1.8798,
      "step": 229600
    },
    {
      "epoch": 17.546405927736615,
      "grad_norm": 5.122994422912598,
      "learning_rate": 2.8066992590329234e-05,
      "loss": 1.7247,
      "step": 229700
    },
    {
      "epoch": 17.55404476357803,
      "grad_norm": 7.258156776428223,
      "learning_rate": 2.8057444045527466e-05,
      "loss": 1.7932,
      "step": 229800
    },
    {
      "epoch": 17.56168359941945,
      "grad_norm": 6.820619583129883,
      "learning_rate": 2.804789550072569e-05,
      "loss": 1.7817,
      "step": 229900
    },
    {
      "epoch": 17.569322435260865,
      "grad_norm": 8.252108573913574,
      "learning_rate": 2.8038346955923916e-05,
      "loss": 1.7409,
      "step": 230000
    },
    {
      "epoch": 17.576961271102284,
      "grad_norm": 10.541058540344238,
      "learning_rate": 2.802879841112215e-05,
      "loss": 1.7767,
      "step": 230100
    },
    {
      "epoch": 17.5846001069437,
      "grad_norm": 8.531997680664062,
      "learning_rate": 2.8019249866320374e-05,
      "loss": 1.8183,
      "step": 230200
    },
    {
      "epoch": 17.59223894278512,
      "grad_norm": 6.013443946838379,
      "learning_rate": 2.80097013215186e-05,
      "loss": 1.75,
      "step": 230300
    },
    {
      "epoch": 17.59987777862654,
      "grad_norm": 6.7462544441223145,
      "learning_rate": 2.800015277671683e-05,
      "loss": 1.8003,
      "step": 230400
    },
    {
      "epoch": 17.607516614467954,
      "grad_norm": 5.326259136199951,
      "learning_rate": 2.7990604231915056e-05,
      "loss": 1.9083,
      "step": 230500
    },
    {
      "epoch": 17.615155450309373,
      "grad_norm": 9.363667488098145,
      "learning_rate": 2.7981055687113285e-05,
      "loss": 1.7934,
      "step": 230600
    },
    {
      "epoch": 17.62279428615079,
      "grad_norm": 5.959409236907959,
      "learning_rate": 2.7971507142311514e-05,
      "loss": 1.8475,
      "step": 230700
    },
    {
      "epoch": 17.63043312199221,
      "grad_norm": 8.672266006469727,
      "learning_rate": 2.7961958597509742e-05,
      "loss": 1.7879,
      "step": 230800
    },
    {
      "epoch": 17.638071957833628,
      "grad_norm": 8.418112754821777,
      "learning_rate": 2.7952410052707968e-05,
      "loss": 1.7863,
      "step": 230900
    },
    {
      "epoch": 17.645710793675043,
      "grad_norm": 6.080643177032471,
      "learning_rate": 2.79428615079062e-05,
      "loss": 1.7819,
      "step": 231000
    },
    {
      "epoch": 17.653349629516462,
      "grad_norm": 9.013139724731445,
      "learning_rate": 2.7933312963104425e-05,
      "loss": 1.8395,
      "step": 231100
    },
    {
      "epoch": 17.660988465357878,
      "grad_norm": 10.476484298706055,
      "learning_rate": 2.792376441830265e-05,
      "loss": 1.8654,
      "step": 231200
    },
    {
      "epoch": 17.668627301199297,
      "grad_norm": 8.22014045715332,
      "learning_rate": 2.7914215873500883e-05,
      "loss": 1.8239,
      "step": 231300
    },
    {
      "epoch": 17.676266137040717,
      "grad_norm": 7.68658971786499,
      "learning_rate": 2.7904667328699108e-05,
      "loss": 1.7998,
      "step": 231400
    },
    {
      "epoch": 17.683904972882132,
      "grad_norm": 7.934796333312988,
      "learning_rate": 2.7895118783897333e-05,
      "loss": 1.7605,
      "step": 231500
    },
    {
      "epoch": 17.69154380872355,
      "grad_norm": 6.6546759605407715,
      "learning_rate": 2.7885570239095565e-05,
      "loss": 1.8166,
      "step": 231600
    },
    {
      "epoch": 17.699182644564967,
      "grad_norm": 9.786514282226562,
      "learning_rate": 2.787602169429379e-05,
      "loss": 1.8423,
      "step": 231700
    },
    {
      "epoch": 17.706821480406386,
      "grad_norm": 7.161447048187256,
      "learning_rate": 2.7866473149492016e-05,
      "loss": 1.923,
      "step": 231800
    },
    {
      "epoch": 17.714460316247802,
      "grad_norm": 8.000249862670898,
      "learning_rate": 2.7856924604690248e-05,
      "loss": 1.7475,
      "step": 231900
    },
    {
      "epoch": 17.72209915208922,
      "grad_norm": 7.113234043121338,
      "learning_rate": 2.7847376059888473e-05,
      "loss": 1.8107,
      "step": 232000
    },
    {
      "epoch": 17.72973798793064,
      "grad_norm": 6.60975980758667,
      "learning_rate": 2.7837827515086702e-05,
      "loss": 1.8159,
      "step": 232100
    },
    {
      "epoch": 17.737376823772056,
      "grad_norm": 6.989168643951416,
      "learning_rate": 2.782827897028493e-05,
      "loss": 1.8305,
      "step": 232200
    },
    {
      "epoch": 17.745015659613475,
      "grad_norm": 6.962953567504883,
      "learning_rate": 2.781873042548316e-05,
      "loss": 1.8575,
      "step": 232300
    },
    {
      "epoch": 17.75265449545489,
      "grad_norm": 8.608323097229004,
      "learning_rate": 2.7809181880681385e-05,
      "loss": 1.8815,
      "step": 232400
    },
    {
      "epoch": 17.76029333129631,
      "grad_norm": 5.526845932006836,
      "learning_rate": 2.7799633335879617e-05,
      "loss": 1.8631,
      "step": 232500
    },
    {
      "epoch": 17.76793216713773,
      "grad_norm": 7.412872314453125,
      "learning_rate": 2.7790084791077842e-05,
      "loss": 1.8144,
      "step": 232600
    },
    {
      "epoch": 17.775571002979145,
      "grad_norm": 8.070611953735352,
      "learning_rate": 2.7780536246276067e-05,
      "loss": 1.7813,
      "step": 232700
    },
    {
      "epoch": 17.783209838820564,
      "grad_norm": 7.591763496398926,
      "learning_rate": 2.77709877014743e-05,
      "loss": 1.7795,
      "step": 232800
    },
    {
      "epoch": 17.79084867466198,
      "grad_norm": 7.206550121307373,
      "learning_rate": 2.7761439156672525e-05,
      "loss": 1.83,
      "step": 232900
    },
    {
      "epoch": 17.7984875105034,
      "grad_norm": 7.926163196563721,
      "learning_rate": 2.775189061187075e-05,
      "loss": 1.9249,
      "step": 233000
    },
    {
      "epoch": 17.80612634634482,
      "grad_norm": 7.121010780334473,
      "learning_rate": 2.7742342067068982e-05,
      "loss": 1.7394,
      "step": 233100
    },
    {
      "epoch": 17.813765182186234,
      "grad_norm": 10.94741153717041,
      "learning_rate": 2.7732793522267207e-05,
      "loss": 1.7554,
      "step": 233200
    },
    {
      "epoch": 17.821404018027653,
      "grad_norm": 7.847622871398926,
      "learning_rate": 2.7723244977465436e-05,
      "loss": 1.8028,
      "step": 233300
    },
    {
      "epoch": 17.82904285386907,
      "grad_norm": 7.062536239624023,
      "learning_rate": 2.7713696432663665e-05,
      "loss": 1.8584,
      "step": 233400
    },
    {
      "epoch": 17.83668168971049,
      "grad_norm": 9.434908866882324,
      "learning_rate": 2.7704147887861893e-05,
      "loss": 1.8266,
      "step": 233500
    },
    {
      "epoch": 17.844320525551908,
      "grad_norm": 9.724963188171387,
      "learning_rate": 2.769459934306012e-05,
      "loss": 1.7733,
      "step": 233600
    },
    {
      "epoch": 17.851959361393323,
      "grad_norm": 8.069439888000488,
      "learning_rate": 2.7685050798258344e-05,
      "loss": 1.8175,
      "step": 233700
    },
    {
      "epoch": 17.859598197234742,
      "grad_norm": 5.290775775909424,
      "learning_rate": 2.7675502253456576e-05,
      "loss": 1.773,
      "step": 233800
    },
    {
      "epoch": 17.867237033076158,
      "grad_norm": 6.895813465118408,
      "learning_rate": 2.76659537086548e-05,
      "loss": 1.7541,
      "step": 233900
    },
    {
      "epoch": 17.874875868917577,
      "grad_norm": 8.96194839477539,
      "learning_rate": 2.7656405163853027e-05,
      "loss": 1.7991,
      "step": 234000
    },
    {
      "epoch": 17.882514704758997,
      "grad_norm": 9.724388122558594,
      "learning_rate": 2.764685661905126e-05,
      "loss": 1.7575,
      "step": 234100
    },
    {
      "epoch": 17.890153540600412,
      "grad_norm": 8.7219820022583,
      "learning_rate": 2.7637308074249484e-05,
      "loss": 1.9251,
      "step": 234200
    },
    {
      "epoch": 17.89779237644183,
      "grad_norm": 7.737878799438477,
      "learning_rate": 2.762775952944771e-05,
      "loss": 1.7882,
      "step": 234300
    },
    {
      "epoch": 17.905431212283247,
      "grad_norm": 6.954885482788086,
      "learning_rate": 2.761821098464594e-05,
      "loss": 1.8653,
      "step": 234400
    },
    {
      "epoch": 17.913070048124666,
      "grad_norm": 6.766438007354736,
      "learning_rate": 2.7608662439844167e-05,
      "loss": 1.6998,
      "step": 234500
    },
    {
      "epoch": 17.920708883966082,
      "grad_norm": 5.908729553222656,
      "learning_rate": 2.7599113895042396e-05,
      "loss": 1.7654,
      "step": 234600
    },
    {
      "epoch": 17.9283477198075,
      "grad_norm": 8.200332641601562,
      "learning_rate": 2.7589565350240628e-05,
      "loss": 1.7796,
      "step": 234700
    },
    {
      "epoch": 17.93598655564892,
      "grad_norm": 7.45107889175415,
      "learning_rate": 2.7580016805438853e-05,
      "loss": 1.8021,
      "step": 234800
    },
    {
      "epoch": 17.943625391490336,
      "grad_norm": 6.744319438934326,
      "learning_rate": 2.7570468260637078e-05,
      "loss": 1.8321,
      "step": 234900
    },
    {
      "epoch": 17.951264227331755,
      "grad_norm": 5.864042282104492,
      "learning_rate": 2.756091971583531e-05,
      "loss": 1.7844,
      "step": 235000
    },
    {
      "epoch": 17.95890306317317,
      "grad_norm": 7.932676792144775,
      "learning_rate": 2.7551371171033536e-05,
      "loss": 1.8319,
      "step": 235100
    },
    {
      "epoch": 17.96654189901459,
      "grad_norm": 7.180896282196045,
      "learning_rate": 2.754182262623176e-05,
      "loss": 1.9068,
      "step": 235200
    },
    {
      "epoch": 17.97418073485601,
      "grad_norm": 7.1504011154174805,
      "learning_rate": 2.7532274081429993e-05,
      "loss": 1.8603,
      "step": 235300
    },
    {
      "epoch": 17.981819570697425,
      "grad_norm": 5.937533855438232,
      "learning_rate": 2.752272553662822e-05,
      "loss": 1.7586,
      "step": 235400
    },
    {
      "epoch": 17.989458406538844,
      "grad_norm": 8.244226455688477,
      "learning_rate": 2.7513176991826444e-05,
      "loss": 1.8312,
      "step": 235500
    },
    {
      "epoch": 17.99709724238026,
      "grad_norm": 8.789660453796387,
      "learning_rate": 2.7503628447024676e-05,
      "loss": 1.8468,
      "step": 235600
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.822707176208496,
      "eval_runtime": 3.0052,
      "eval_samples_per_second": 229.605,
      "eval_steps_per_second": 229.605,
      "step": 235638
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.5620061159133911,
      "eval_runtime": 56.3915,
      "eval_samples_per_second": 232.145,
      "eval_steps_per_second": 232.145,
      "step": 235638
    },
    {
      "epoch": 18.00473607822168,
      "grad_norm": 10.019100189208984,
      "learning_rate": 2.74940799022229e-05,
      "loss": 1.7085,
      "step": 235700
    },
    {
      "epoch": 18.0123749140631,
      "grad_norm": 6.553521156311035,
      "learning_rate": 2.748453135742113e-05,
      "loss": 1.8736,
      "step": 235800
    },
    {
      "epoch": 18.020013749904514,
      "grad_norm": 8.867786407470703,
      "learning_rate": 2.747498281261936e-05,
      "loss": 1.7939,
      "step": 235900
    },
    {
      "epoch": 18.027652585745933,
      "grad_norm": 8.81936264038086,
      "learning_rate": 2.7465434267817587e-05,
      "loss": 1.8205,
      "step": 236000
    },
    {
      "epoch": 18.03529142158735,
      "grad_norm": 7.590775966644287,
      "learning_rate": 2.7455885723015812e-05,
      "loss": 1.8653,
      "step": 236100
    },
    {
      "epoch": 18.042930257428768,
      "grad_norm": 9.917695999145508,
      "learning_rate": 2.7446337178214044e-05,
      "loss": 1.7088,
      "step": 236200
    },
    {
      "epoch": 18.050569093270184,
      "grad_norm": 6.857242107391357,
      "learning_rate": 2.743678863341227e-05,
      "loss": 1.8019,
      "step": 236300
    },
    {
      "epoch": 18.058207929111603,
      "grad_norm": 6.694100379943848,
      "learning_rate": 2.7427240088610495e-05,
      "loss": 1.7537,
      "step": 236400
    },
    {
      "epoch": 18.065846764953022,
      "grad_norm": 8.243744850158691,
      "learning_rate": 2.7417691543808727e-05,
      "loss": 1.7634,
      "step": 236500
    },
    {
      "epoch": 18.073485600794438,
      "grad_norm": 7.18552827835083,
      "learning_rate": 2.7408142999006952e-05,
      "loss": 1.6659,
      "step": 236600
    },
    {
      "epoch": 18.081124436635857,
      "grad_norm": 6.905070781707764,
      "learning_rate": 2.7398594454205178e-05,
      "loss": 1.7618,
      "step": 236700
    },
    {
      "epoch": 18.088763272477273,
      "grad_norm": 6.394491672515869,
      "learning_rate": 2.738904590940341e-05,
      "loss": 1.8354,
      "step": 236800
    },
    {
      "epoch": 18.096402108318692,
      "grad_norm": 7.283252716064453,
      "learning_rate": 2.7379497364601635e-05,
      "loss": 1.7265,
      "step": 236900
    },
    {
      "epoch": 18.10404094416011,
      "grad_norm": 7.033634662628174,
      "learning_rate": 2.736994881979986e-05,
      "loss": 1.8168,
      "step": 237000
    },
    {
      "epoch": 18.111679780001527,
      "grad_norm": 7.204286575317383,
      "learning_rate": 2.7360400274998093e-05,
      "loss": 1.795,
      "step": 237100
    },
    {
      "epoch": 18.119318615842946,
      "grad_norm": 8.180116653442383,
      "learning_rate": 2.735085173019632e-05,
      "loss": 1.7649,
      "step": 237200
    },
    {
      "epoch": 18.126957451684362,
      "grad_norm": 6.927674770355225,
      "learning_rate": 2.7341303185394547e-05,
      "loss": 1.7167,
      "step": 237300
    },
    {
      "epoch": 18.13459628752578,
      "grad_norm": 11.725210189819336,
      "learning_rate": 2.733175464059278e-05,
      "loss": 1.6599,
      "step": 237400
    },
    {
      "epoch": 18.1422351233672,
      "grad_norm": 7.158365249633789,
      "learning_rate": 2.7322206095791004e-05,
      "loss": 1.7746,
      "step": 237500
    },
    {
      "epoch": 18.149873959208616,
      "grad_norm": 6.113236904144287,
      "learning_rate": 2.731265755098923e-05,
      "loss": 1.7533,
      "step": 237600
    },
    {
      "epoch": 18.157512795050035,
      "grad_norm": 7.372956275939941,
      "learning_rate": 2.730310900618746e-05,
      "loss": 1.7597,
      "step": 237700
    },
    {
      "epoch": 18.16515163089145,
      "grad_norm": 7.415124416351318,
      "learning_rate": 2.7293560461385687e-05,
      "loss": 1.8296,
      "step": 237800
    },
    {
      "epoch": 18.17279046673287,
      "grad_norm": 9.746007919311523,
      "learning_rate": 2.7284011916583912e-05,
      "loss": 1.8582,
      "step": 237900
    },
    {
      "epoch": 18.18042930257429,
      "grad_norm": 7.951816082000732,
      "learning_rate": 2.7274463371782144e-05,
      "loss": 1.7696,
      "step": 238000
    },
    {
      "epoch": 18.188068138415705,
      "grad_norm": 6.0495805740356445,
      "learning_rate": 2.726491482698037e-05,
      "loss": 1.7364,
      "step": 238100
    },
    {
      "epoch": 18.195706974257124,
      "grad_norm": 7.73384428024292,
      "learning_rate": 2.7255366282178595e-05,
      "loss": 1.7498,
      "step": 238200
    },
    {
      "epoch": 18.20334581009854,
      "grad_norm": 7.829511642456055,
      "learning_rate": 2.7245817737376827e-05,
      "loss": 1.8793,
      "step": 238300
    },
    {
      "epoch": 18.21098464593996,
      "grad_norm": 7.239713668823242,
      "learning_rate": 2.7236269192575052e-05,
      "loss": 1.7599,
      "step": 238400
    },
    {
      "epoch": 18.218623481781375,
      "grad_norm": 6.079605579376221,
      "learning_rate": 2.722672064777328e-05,
      "loss": 1.7454,
      "step": 238500
    },
    {
      "epoch": 18.226262317622794,
      "grad_norm": 8.369773864746094,
      "learning_rate": 2.721717210297151e-05,
      "loss": 1.8149,
      "step": 238600
    },
    {
      "epoch": 18.233901153464213,
      "grad_norm": 9.02391529083252,
      "learning_rate": 2.7207623558169738e-05,
      "loss": 1.8371,
      "step": 238700
    },
    {
      "epoch": 18.24153998930563,
      "grad_norm": 7.343380451202393,
      "learning_rate": 2.7198075013367963e-05,
      "loss": 1.7144,
      "step": 238800
    },
    {
      "epoch": 18.249178825147048,
      "grad_norm": 5.769071102142334,
      "learning_rate": 2.7188526468566195e-05,
      "loss": 1.8114,
      "step": 238900
    },
    {
      "epoch": 18.256817660988464,
      "grad_norm": 6.325088977813721,
      "learning_rate": 2.717897792376442e-05,
      "loss": 1.7179,
      "step": 239000
    },
    {
      "epoch": 18.264456496829883,
      "grad_norm": 6.965639591217041,
      "learning_rate": 2.7169429378962646e-05,
      "loss": 1.8431,
      "step": 239100
    },
    {
      "epoch": 18.272095332671302,
      "grad_norm": 6.978320598602295,
      "learning_rate": 2.7159880834160878e-05,
      "loss": 1.8072,
      "step": 239200
    },
    {
      "epoch": 18.279734168512718,
      "grad_norm": 8.473089218139648,
      "learning_rate": 2.7150332289359103e-05,
      "loss": 1.769,
      "step": 239300
    },
    {
      "epoch": 18.287373004354137,
      "grad_norm": 7.398226261138916,
      "learning_rate": 2.714078374455733e-05,
      "loss": 1.8374,
      "step": 239400
    },
    {
      "epoch": 18.295011840195553,
      "grad_norm": 7.082970142364502,
      "learning_rate": 2.7131235199755557e-05,
      "loss": 1.908,
      "step": 239500
    },
    {
      "epoch": 18.302650676036972,
      "grad_norm": 7.455836772918701,
      "learning_rate": 2.7121686654953786e-05,
      "loss": 1.7189,
      "step": 239600
    },
    {
      "epoch": 18.31028951187839,
      "grad_norm": 8.256296157836914,
      "learning_rate": 2.7112138110152015e-05,
      "loss": 1.8104,
      "step": 239700
    },
    {
      "epoch": 18.317928347719807,
      "grad_norm": 5.748119354248047,
      "learning_rate": 2.710258956535024e-05,
      "loss": 1.8345,
      "step": 239800
    },
    {
      "epoch": 18.325567183561226,
      "grad_norm": 6.726574897766113,
      "learning_rate": 2.7093041020548472e-05,
      "loss": 1.7691,
      "step": 239900
    },
    {
      "epoch": 18.333206019402642,
      "grad_norm": 7.804013252258301,
      "learning_rate": 2.7083492475746698e-05,
      "loss": 1.7963,
      "step": 240000
    },
    {
      "epoch": 18.34084485524406,
      "grad_norm": 6.156284809112549,
      "learning_rate": 2.7073943930944923e-05,
      "loss": 1.8041,
      "step": 240100
    },
    {
      "epoch": 18.34848369108548,
      "grad_norm": 9.282012939453125,
      "learning_rate": 2.7064395386143155e-05,
      "loss": 1.7548,
      "step": 240200
    },
    {
      "epoch": 18.356122526926896,
      "grad_norm": 8.127195358276367,
      "learning_rate": 2.705484684134138e-05,
      "loss": 1.8284,
      "step": 240300
    },
    {
      "epoch": 18.363761362768315,
      "grad_norm": 7.5755720138549805,
      "learning_rate": 2.7045298296539606e-05,
      "loss": 1.8531,
      "step": 240400
    },
    {
      "epoch": 18.37140019860973,
      "grad_norm": 9.666086196899414,
      "learning_rate": 2.7035749751737838e-05,
      "loss": 1.7425,
      "step": 240500
    },
    {
      "epoch": 18.37903903445115,
      "grad_norm": 5.985454559326172,
      "learning_rate": 2.7026201206936063e-05,
      "loss": 1.7615,
      "step": 240600
    },
    {
      "epoch": 18.386677870292566,
      "grad_norm": 6.967623233795166,
      "learning_rate": 2.7016652662134288e-05,
      "loss": 1.7724,
      "step": 240700
    },
    {
      "epoch": 18.394316706133985,
      "grad_norm": 8.562158584594727,
      "learning_rate": 2.700710411733252e-05,
      "loss": 1.7698,
      "step": 240800
    },
    {
      "epoch": 18.401955541975404,
      "grad_norm": 6.072444438934326,
      "learning_rate": 2.6997555572530746e-05,
      "loss": 1.8102,
      "step": 240900
    },
    {
      "epoch": 18.40959437781682,
      "grad_norm": 8.752595901489258,
      "learning_rate": 2.6988007027728974e-05,
      "loss": 1.8563,
      "step": 241000
    },
    {
      "epoch": 18.41723321365824,
      "grad_norm": 8.295882225036621,
      "learning_rate": 2.6978458482927203e-05,
      "loss": 1.7854,
      "step": 241100
    },
    {
      "epoch": 18.424872049499655,
      "grad_norm": 7.89208459854126,
      "learning_rate": 2.6968909938125432e-05,
      "loss": 1.6894,
      "step": 241200
    },
    {
      "epoch": 18.432510885341074,
      "grad_norm": 7.312723159790039,
      "learning_rate": 2.6959361393323657e-05,
      "loss": 1.7745,
      "step": 241300
    },
    {
      "epoch": 18.440149721182493,
      "grad_norm": 7.9343085289001465,
      "learning_rate": 2.694981284852189e-05,
      "loss": 1.7771,
      "step": 241400
    },
    {
      "epoch": 18.44778855702391,
      "grad_norm": 7.326744556427002,
      "learning_rate": 2.6940264303720114e-05,
      "loss": 1.7501,
      "step": 241500
    },
    {
      "epoch": 18.455427392865328,
      "grad_norm": 6.52875280380249,
      "learning_rate": 2.693071575891834e-05,
      "loss": 1.7252,
      "step": 241600
    },
    {
      "epoch": 18.463066228706744,
      "grad_norm": 6.62938928604126,
      "learning_rate": 2.6921167214116572e-05,
      "loss": 1.8069,
      "step": 241700
    },
    {
      "epoch": 18.470705064548163,
      "grad_norm": 7.529290676116943,
      "learning_rate": 2.6911618669314797e-05,
      "loss": 1.6902,
      "step": 241800
    },
    {
      "epoch": 18.478343900389582,
      "grad_norm": 7.835832595825195,
      "learning_rate": 2.6902070124513022e-05,
      "loss": 1.7253,
      "step": 241900
    },
    {
      "epoch": 18.485982736230998,
      "grad_norm": 10.426091194152832,
      "learning_rate": 2.6892521579711254e-05,
      "loss": 1.7995,
      "step": 242000
    },
    {
      "epoch": 18.493621572072417,
      "grad_norm": 8.197208404541016,
      "learning_rate": 2.688297303490948e-05,
      "loss": 1.8681,
      "step": 242100
    },
    {
      "epoch": 18.501260407913833,
      "grad_norm": 5.224775791168213,
      "learning_rate": 2.687342449010771e-05,
      "loss": 1.82,
      "step": 242200
    },
    {
      "epoch": 18.508899243755252,
      "grad_norm": 5.48012638092041,
      "learning_rate": 2.6863875945305937e-05,
      "loss": 1.8796,
      "step": 242300
    },
    {
      "epoch": 18.51653807959667,
      "grad_norm": 6.201797962188721,
      "learning_rate": 2.6854327400504166e-05,
      "loss": 1.8276,
      "step": 242400
    },
    {
      "epoch": 18.524176915438087,
      "grad_norm": 9.732596397399902,
      "learning_rate": 2.684477885570239e-05,
      "loss": 1.7247,
      "step": 242500
    },
    {
      "epoch": 18.531815751279506,
      "grad_norm": 6.396280288696289,
      "learning_rate": 2.6835230310900623e-05,
      "loss": 1.8627,
      "step": 242600
    },
    {
      "epoch": 18.53945458712092,
      "grad_norm": 6.7175188064575195,
      "learning_rate": 2.682568176609885e-05,
      "loss": 1.7975,
      "step": 242700
    },
    {
      "epoch": 18.54709342296234,
      "grad_norm": 6.884514808654785,
      "learning_rate": 2.6816133221297074e-05,
      "loss": 1.7884,
      "step": 242800
    },
    {
      "epoch": 18.554732258803757,
      "grad_norm": 8.019926071166992,
      "learning_rate": 2.6806584676495306e-05,
      "loss": 1.7883,
      "step": 242900
    },
    {
      "epoch": 18.562371094645176,
      "grad_norm": 7.130098819732666,
      "learning_rate": 2.679703613169353e-05,
      "loss": 1.8319,
      "step": 243000
    },
    {
      "epoch": 18.570009930486595,
      "grad_norm": 5.161909580230713,
      "learning_rate": 2.6787487586891757e-05,
      "loss": 1.7604,
      "step": 243100
    },
    {
      "epoch": 18.57764876632801,
      "grad_norm": 5.927675724029541,
      "learning_rate": 2.677793904208999e-05,
      "loss": 1.8659,
      "step": 243200
    },
    {
      "epoch": 18.58528760216943,
      "grad_norm": 7.516214847564697,
      "learning_rate": 2.6768390497288214e-05,
      "loss": 1.9271,
      "step": 243300
    },
    {
      "epoch": 18.592926438010846,
      "grad_norm": 11.20616340637207,
      "learning_rate": 2.675884195248644e-05,
      "loss": 1.7742,
      "step": 243400
    },
    {
      "epoch": 18.600565273852265,
      "grad_norm": 7.950630187988281,
      "learning_rate": 2.674929340768467e-05,
      "loss": 1.813,
      "step": 243500
    },
    {
      "epoch": 18.608204109693684,
      "grad_norm": 6.3089213371276855,
      "learning_rate": 2.67397448628829e-05,
      "loss": 1.8871,
      "step": 243600
    },
    {
      "epoch": 18.6158429455351,
      "grad_norm": 8.966050148010254,
      "learning_rate": 2.6730196318081125e-05,
      "loss": 1.7065,
      "step": 243700
    },
    {
      "epoch": 18.62348178137652,
      "grad_norm": 10.684454917907715,
      "learning_rate": 2.6720647773279357e-05,
      "loss": 1.8008,
      "step": 243800
    },
    {
      "epoch": 18.631120617217935,
      "grad_norm": 6.6230645179748535,
      "learning_rate": 2.6711099228477583e-05,
      "loss": 1.7882,
      "step": 243900
    },
    {
      "epoch": 18.638759453059354,
      "grad_norm": 5.6710357666015625,
      "learning_rate": 2.6701550683675808e-05,
      "loss": 1.7652,
      "step": 244000
    },
    {
      "epoch": 18.646398288900773,
      "grad_norm": 4.547703742980957,
      "learning_rate": 2.669200213887404e-05,
      "loss": 1.8124,
      "step": 244100
    },
    {
      "epoch": 18.65403712474219,
      "grad_norm": 7.8390631675720215,
      "learning_rate": 2.6682453594072265e-05,
      "loss": 1.7419,
      "step": 244200
    },
    {
      "epoch": 18.661675960583608,
      "grad_norm": 5.7713727951049805,
      "learning_rate": 2.667290504927049e-05,
      "loss": 1.9065,
      "step": 244300
    },
    {
      "epoch": 18.669314796425024,
      "grad_norm": 7.275326728820801,
      "learning_rate": 2.6663356504468723e-05,
      "loss": 1.7998,
      "step": 244400
    },
    {
      "epoch": 18.676953632266443,
      "grad_norm": 8.412632942199707,
      "learning_rate": 2.6653807959666948e-05,
      "loss": 1.7994,
      "step": 244500
    },
    {
      "epoch": 18.68459246810786,
      "grad_norm": 6.443037986755371,
      "learning_rate": 2.6644259414865173e-05,
      "loss": 1.6983,
      "step": 244600
    },
    {
      "epoch": 18.692231303949278,
      "grad_norm": 8.238800048828125,
      "learning_rate": 2.6634710870063405e-05,
      "loss": 1.811,
      "step": 244700
    },
    {
      "epoch": 18.699870139790697,
      "grad_norm": 7.734893321990967,
      "learning_rate": 2.662516232526163e-05,
      "loss": 1.7165,
      "step": 244800
    },
    {
      "epoch": 18.707508975632113,
      "grad_norm": 7.149792194366455,
      "learning_rate": 2.661561378045986e-05,
      "loss": 1.8429,
      "step": 244900
    },
    {
      "epoch": 18.715147811473532,
      "grad_norm": 6.739688396453857,
      "learning_rate": 2.6606065235658088e-05,
      "loss": 1.8317,
      "step": 245000
    },
    {
      "epoch": 18.722786647314948,
      "grad_norm": 6.3798089027404785,
      "learning_rate": 2.6596516690856317e-05,
      "loss": 1.7364,
      "step": 245100
    },
    {
      "epoch": 18.730425483156367,
      "grad_norm": 7.480180740356445,
      "learning_rate": 2.6586968146054542e-05,
      "loss": 1.7574,
      "step": 245200
    },
    {
      "epoch": 18.738064318997786,
      "grad_norm": 11.447808265686035,
      "learning_rate": 2.6577419601252767e-05,
      "loss": 1.8295,
      "step": 245300
    },
    {
      "epoch": 18.7457031548392,
      "grad_norm": 7.1083574295043945,
      "learning_rate": 2.6567871056451e-05,
      "loss": 1.8363,
      "step": 245400
    },
    {
      "epoch": 18.75334199068062,
      "grad_norm": 8.506104469299316,
      "learning_rate": 2.6558322511649225e-05,
      "loss": 1.7714,
      "step": 245500
    },
    {
      "epoch": 18.760980826522037,
      "grad_norm": 8.97549057006836,
      "learning_rate": 2.654877396684745e-05,
      "loss": 1.8214,
      "step": 245600
    },
    {
      "epoch": 18.768619662363456,
      "grad_norm": 7.689046859741211,
      "learning_rate": 2.6539225422045682e-05,
      "loss": 1.79,
      "step": 245700
    },
    {
      "epoch": 18.776258498204875,
      "grad_norm": 6.207085132598877,
      "learning_rate": 2.6529676877243908e-05,
      "loss": 1.7583,
      "step": 245800
    },
    {
      "epoch": 18.78389733404629,
      "grad_norm": 7.568465709686279,
      "learning_rate": 2.6520128332442136e-05,
      "loss": 1.7537,
      "step": 245900
    },
    {
      "epoch": 18.79153616988771,
      "grad_norm": 7.353707790374756,
      "learning_rate": 2.6510579787640365e-05,
      "loss": 1.7977,
      "step": 246000
    },
    {
      "epoch": 18.799175005729126,
      "grad_norm": 6.497218608856201,
      "learning_rate": 2.6501031242838594e-05,
      "loss": 1.6929,
      "step": 246100
    },
    {
      "epoch": 18.806813841570545,
      "grad_norm": 7.10603141784668,
      "learning_rate": 2.649148269803682e-05,
      "loss": 1.6305,
      "step": 246200
    },
    {
      "epoch": 18.814452677411964,
      "grad_norm": 6.595560550689697,
      "learning_rate": 2.648193415323505e-05,
      "loss": 1.8071,
      "step": 246300
    },
    {
      "epoch": 18.82209151325338,
      "grad_norm": 9.030119895935059,
      "learning_rate": 2.6472385608433276e-05,
      "loss": 1.7968,
      "step": 246400
    },
    {
      "epoch": 18.8297303490948,
      "grad_norm": 6.882573127746582,
      "learning_rate": 2.64628370636315e-05,
      "loss": 1.7574,
      "step": 246500
    },
    {
      "epoch": 18.837369184936215,
      "grad_norm": 8.110075950622559,
      "learning_rate": 2.6453288518829734e-05,
      "loss": 1.7728,
      "step": 246600
    },
    {
      "epoch": 18.845008020777634,
      "grad_norm": 9.313435554504395,
      "learning_rate": 2.644373997402796e-05,
      "loss": 1.7303,
      "step": 246700
    },
    {
      "epoch": 18.852646856619053,
      "grad_norm": 6.915787220001221,
      "learning_rate": 2.6434191429226184e-05,
      "loss": 1.8116,
      "step": 246800
    },
    {
      "epoch": 18.86028569246047,
      "grad_norm": 6.856382846832275,
      "learning_rate": 2.6424642884424416e-05,
      "loss": 1.7961,
      "step": 246900
    },
    {
      "epoch": 18.867924528301888,
      "grad_norm": 7.463267803192139,
      "learning_rate": 2.641509433962264e-05,
      "loss": 1.8335,
      "step": 247000
    },
    {
      "epoch": 18.875563364143304,
      "grad_norm": 7.644176006317139,
      "learning_rate": 2.6405545794820867e-05,
      "loss": 1.7632,
      "step": 247100
    },
    {
      "epoch": 18.883202199984723,
      "grad_norm": 4.099947452545166,
      "learning_rate": 2.63959972500191e-05,
      "loss": 1.7686,
      "step": 247200
    },
    {
      "epoch": 18.89084103582614,
      "grad_norm": 7.9961066246032715,
      "learning_rate": 2.6386448705217324e-05,
      "loss": 1.8318,
      "step": 247300
    },
    {
      "epoch": 18.898479871667558,
      "grad_norm": 8.034085273742676,
      "learning_rate": 2.6376900160415553e-05,
      "loss": 1.7829,
      "step": 247400
    },
    {
      "epoch": 18.906118707508977,
      "grad_norm": 5.933337211608887,
      "learning_rate": 2.6367351615613782e-05,
      "loss": 1.8004,
      "step": 247500
    },
    {
      "epoch": 18.913757543350393,
      "grad_norm": 6.869357109069824,
      "learning_rate": 2.635780307081201e-05,
      "loss": 1.8254,
      "step": 247600
    },
    {
      "epoch": 18.921396379191812,
      "grad_norm": 5.228794097900391,
      "learning_rate": 2.6348254526010236e-05,
      "loss": 1.8211,
      "step": 247700
    },
    {
      "epoch": 18.929035215033228,
      "grad_norm": 6.2762298583984375,
      "learning_rate": 2.6338705981208468e-05,
      "loss": 1.7868,
      "step": 247800
    },
    {
      "epoch": 18.936674050874647,
      "grad_norm": 5.646769046783447,
      "learning_rate": 2.6329157436406693e-05,
      "loss": 1.7718,
      "step": 247900
    },
    {
      "epoch": 18.944312886716066,
      "grad_norm": 5.113230228424072,
      "learning_rate": 2.631960889160492e-05,
      "loss": 1.7076,
      "step": 248000
    },
    {
      "epoch": 18.95195172255748,
      "grad_norm": 6.359259128570557,
      "learning_rate": 2.631006034680315e-05,
      "loss": 1.8719,
      "step": 248100
    },
    {
      "epoch": 18.9595905583989,
      "grad_norm": 7.589322566986084,
      "learning_rate": 2.6300511802001376e-05,
      "loss": 1.8903,
      "step": 248200
    },
    {
      "epoch": 18.967229394240317,
      "grad_norm": 9.076083183288574,
      "learning_rate": 2.62909632571996e-05,
      "loss": 1.8422,
      "step": 248300
    },
    {
      "epoch": 18.974868230081736,
      "grad_norm": 8.15712833404541,
      "learning_rate": 2.6281414712397833e-05,
      "loss": 1.8809,
      "step": 248400
    },
    {
      "epoch": 18.982507065923155,
      "grad_norm": 6.359996318817139,
      "learning_rate": 2.627186616759606e-05,
      "loss": 1.7919,
      "step": 248500
    },
    {
      "epoch": 18.99014590176457,
      "grad_norm": 6.716052532196045,
      "learning_rate": 2.6262317622794287e-05,
      "loss": 1.7756,
      "step": 248600
    },
    {
      "epoch": 18.99778473760599,
      "grad_norm": 8.302631378173828,
      "learning_rate": 2.6252769077992516e-05,
      "loss": 1.7364,
      "step": 248700
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.8172224760055542,
      "eval_runtime": 3.1125,
      "eval_samples_per_second": 221.689,
      "eval_steps_per_second": 221.689,
      "step": 248729
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.5539491176605225,
      "eval_runtime": 57.6129,
      "eval_samples_per_second": 227.223,
      "eval_steps_per_second": 227.223,
      "step": 248729
    },
    {
      "epoch": 19.005423573447406,
      "grad_norm": 6.247055530548096,
      "learning_rate": 2.6243220533190745e-05,
      "loss": 1.7436,
      "step": 248800
    },
    {
      "epoch": 19.013062409288825,
      "grad_norm": 7.37984561920166,
      "learning_rate": 2.623367198838897e-05,
      "loss": 1.8005,
      "step": 248900
    },
    {
      "epoch": 19.02070124513024,
      "grad_norm": 8.52739143371582,
      "learning_rate": 2.6224123443587202e-05,
      "loss": 1.6747,
      "step": 249000
    },
    {
      "epoch": 19.02834008097166,
      "grad_norm": 6.2476115226745605,
      "learning_rate": 2.6214574898785427e-05,
      "loss": 1.7488,
      "step": 249100
    },
    {
      "epoch": 19.03597891681308,
      "grad_norm": 7.418903827667236,
      "learning_rate": 2.6205026353983653e-05,
      "loss": 1.8056,
      "step": 249200
    },
    {
      "epoch": 19.043617752654495,
      "grad_norm": 7.81702995300293,
      "learning_rate": 2.6195477809181885e-05,
      "loss": 1.8358,
      "step": 249300
    },
    {
      "epoch": 19.051256588495914,
      "grad_norm": 7.163797855377197,
      "learning_rate": 2.618592926438011e-05,
      "loss": 1.798,
      "step": 249400
    },
    {
      "epoch": 19.05889542433733,
      "grad_norm": 6.571475028991699,
      "learning_rate": 2.6176380719578335e-05,
      "loss": 1.756,
      "step": 249500
    },
    {
      "epoch": 19.06653426017875,
      "grad_norm": 6.060917377471924,
      "learning_rate": 2.6166832174776567e-05,
      "loss": 1.7884,
      "step": 249600
    },
    {
      "epoch": 19.074173096020168,
      "grad_norm": 7.122913360595703,
      "learning_rate": 2.6157283629974793e-05,
      "loss": 1.7617,
      "step": 249700
    },
    {
      "epoch": 19.081811931861584,
      "grad_norm": 8.35456371307373,
      "learning_rate": 2.6147735085173018e-05,
      "loss": 1.7951,
      "step": 249800
    },
    {
      "epoch": 19.089450767703003,
      "grad_norm": 7.931674957275391,
      "learning_rate": 2.613818654037125e-05,
      "loss": 1.8294,
      "step": 249900
    },
    {
      "epoch": 19.09708960354442,
      "grad_norm": 9.372440338134766,
      "learning_rate": 2.612863799556948e-05,
      "loss": 1.7679,
      "step": 250000
    },
    {
      "epoch": 19.104728439385838,
      "grad_norm": 7.565546035766602,
      "learning_rate": 2.6119089450767704e-05,
      "loss": 1.6868,
      "step": 250100
    },
    {
      "epoch": 19.112367275227257,
      "grad_norm": 5.394643783569336,
      "learning_rate": 2.6109540905965936e-05,
      "loss": 1.7942,
      "step": 250200
    },
    {
      "epoch": 19.120006111068673,
      "grad_norm": 6.774890899658203,
      "learning_rate": 2.609999236116416e-05,
      "loss": 1.7656,
      "step": 250300
    },
    {
      "epoch": 19.12764494691009,
      "grad_norm": 7.61396598815918,
      "learning_rate": 2.6090443816362387e-05,
      "loss": 1.7676,
      "step": 250400
    },
    {
      "epoch": 19.135283782751507,
      "grad_norm": 8.707268714904785,
      "learning_rate": 2.608089527156062e-05,
      "loss": 1.8085,
      "step": 250500
    },
    {
      "epoch": 19.142922618592927,
      "grad_norm": 10.01880168914795,
      "learning_rate": 2.6071346726758844e-05,
      "loss": 1.7405,
      "step": 250600
    },
    {
      "epoch": 19.150561454434346,
      "grad_norm": 8.489879608154297,
      "learning_rate": 2.606179818195707e-05,
      "loss": 1.7214,
      "step": 250700
    },
    {
      "epoch": 19.15820029027576,
      "grad_norm": 6.935173988342285,
      "learning_rate": 2.60522496371553e-05,
      "loss": 1.7497,
      "step": 250800
    },
    {
      "epoch": 19.16583912611718,
      "grad_norm": 8.039968490600586,
      "learning_rate": 2.6042701092353527e-05,
      "loss": 1.7594,
      "step": 250900
    },
    {
      "epoch": 19.173477961958596,
      "grad_norm": 7.6216840744018555,
      "learning_rate": 2.6033152547551752e-05,
      "loss": 1.7202,
      "step": 251000
    },
    {
      "epoch": 19.181116797800016,
      "grad_norm": 6.538144588470459,
      "learning_rate": 2.602360400274998e-05,
      "loss": 1.8592,
      "step": 251100
    },
    {
      "epoch": 19.18875563364143,
      "grad_norm": 8.352033615112305,
      "learning_rate": 2.601405545794821e-05,
      "loss": 1.801,
      "step": 251200
    },
    {
      "epoch": 19.19639446948285,
      "grad_norm": 7.430103302001953,
      "learning_rate": 2.6004506913146438e-05,
      "loss": 1.7913,
      "step": 251300
    },
    {
      "epoch": 19.20403330532427,
      "grad_norm": 9.111644744873047,
      "learning_rate": 2.5994958368344664e-05,
      "loss": 1.7973,
      "step": 251400
    },
    {
      "epoch": 19.211672141165685,
      "grad_norm": 5.7232441902160645,
      "learning_rate": 2.5985409823542896e-05,
      "loss": 1.8224,
      "step": 251500
    },
    {
      "epoch": 19.219310977007105,
      "grad_norm": 5.559375762939453,
      "learning_rate": 2.597586127874112e-05,
      "loss": 1.7219,
      "step": 251600
    },
    {
      "epoch": 19.22694981284852,
      "grad_norm": 7.670806407928467,
      "learning_rate": 2.5966312733939346e-05,
      "loss": 1.7312,
      "step": 251700
    },
    {
      "epoch": 19.23458864868994,
      "grad_norm": 8.30897331237793,
      "learning_rate": 2.5956764189137578e-05,
      "loss": 1.7904,
      "step": 251800
    },
    {
      "epoch": 19.24222748453136,
      "grad_norm": 7.530828475952148,
      "learning_rate": 2.5947215644335804e-05,
      "loss": 1.8033,
      "step": 251900
    },
    {
      "epoch": 19.249866320372774,
      "grad_norm": 6.981899738311768,
      "learning_rate": 2.593766709953403e-05,
      "loss": 1.8089,
      "step": 252000
    },
    {
      "epoch": 19.257505156214194,
      "grad_norm": 7.782763957977295,
      "learning_rate": 2.592811855473226e-05,
      "loss": 1.8846,
      "step": 252100
    },
    {
      "epoch": 19.26514399205561,
      "grad_norm": 8.969671249389648,
      "learning_rate": 2.5918570009930486e-05,
      "loss": 1.7469,
      "step": 252200
    },
    {
      "epoch": 19.27278282789703,
      "grad_norm": 10.23464298248291,
      "learning_rate": 2.5909021465128715e-05,
      "loss": 1.7583,
      "step": 252300
    },
    {
      "epoch": 19.280421663738448,
      "grad_norm": 8.516051292419434,
      "learning_rate": 2.5899472920326944e-05,
      "loss": 1.8644,
      "step": 252400
    },
    {
      "epoch": 19.288060499579863,
      "grad_norm": 6.010484218597412,
      "learning_rate": 2.5889924375525172e-05,
      "loss": 1.8306,
      "step": 252500
    },
    {
      "epoch": 19.295699335421283,
      "grad_norm": 7.464481830596924,
      "learning_rate": 2.5880375830723398e-05,
      "loss": 1.7582,
      "step": 252600
    },
    {
      "epoch": 19.3033381712627,
      "grad_norm": 9.615974426269531,
      "learning_rate": 2.587082728592163e-05,
      "loss": 1.7799,
      "step": 252700
    },
    {
      "epoch": 19.310977007104118,
      "grad_norm": 9.310099601745605,
      "learning_rate": 2.5861278741119855e-05,
      "loss": 1.7862,
      "step": 252800
    },
    {
      "epoch": 19.318615842945537,
      "grad_norm": 7.732214450836182,
      "learning_rate": 2.585173019631808e-05,
      "loss": 1.7769,
      "step": 252900
    },
    {
      "epoch": 19.326254678786952,
      "grad_norm": 8.876104354858398,
      "learning_rate": 2.5842181651516312e-05,
      "loss": 1.719,
      "step": 253000
    },
    {
      "epoch": 19.33389351462837,
      "grad_norm": 5.9440484046936035,
      "learning_rate": 2.5832633106714538e-05,
      "loss": 1.6759,
      "step": 253100
    },
    {
      "epoch": 19.341532350469787,
      "grad_norm": 7.690465450286865,
      "learning_rate": 2.5823084561912763e-05,
      "loss": 1.6687,
      "step": 253200
    },
    {
      "epoch": 19.349171186311207,
      "grad_norm": 7.549745559692383,
      "learning_rate": 2.5813536017110995e-05,
      "loss": 1.7989,
      "step": 253300
    },
    {
      "epoch": 19.356810022152622,
      "grad_norm": 7.261623382568359,
      "learning_rate": 2.580398747230922e-05,
      "loss": 1.7589,
      "step": 253400
    },
    {
      "epoch": 19.36444885799404,
      "grad_norm": 6.738675594329834,
      "learning_rate": 2.5794438927507446e-05,
      "loss": 1.6743,
      "step": 253500
    },
    {
      "epoch": 19.37208769383546,
      "grad_norm": 8.468670845031738,
      "learning_rate": 2.5784890382705678e-05,
      "loss": 1.7906,
      "step": 253600
    },
    {
      "epoch": 19.379726529676876,
      "grad_norm": 8.015835762023926,
      "learning_rate": 2.5775341837903903e-05,
      "loss": 1.8773,
      "step": 253700
    },
    {
      "epoch": 19.387365365518296,
      "grad_norm": 6.221723556518555,
      "learning_rate": 2.5765793293102132e-05,
      "loss": 1.7937,
      "step": 253800
    },
    {
      "epoch": 19.39500420135971,
      "grad_norm": 7.831691265106201,
      "learning_rate": 2.575624474830036e-05,
      "loss": 1.8506,
      "step": 253900
    },
    {
      "epoch": 19.40264303720113,
      "grad_norm": 8.503291130065918,
      "learning_rate": 2.574669620349859e-05,
      "loss": 1.8907,
      "step": 254000
    },
    {
      "epoch": 19.41028187304255,
      "grad_norm": 9.022748947143555,
      "learning_rate": 2.5737147658696815e-05,
      "loss": 1.7805,
      "step": 254100
    },
    {
      "epoch": 19.417920708883965,
      "grad_norm": 9.767847061157227,
      "learning_rate": 2.5727599113895047e-05,
      "loss": 1.9194,
      "step": 254200
    },
    {
      "epoch": 19.425559544725385,
      "grad_norm": 10.531556129455566,
      "learning_rate": 2.5718050569093272e-05,
      "loss": 1.7565,
      "step": 254300
    },
    {
      "epoch": 19.4331983805668,
      "grad_norm": 8.053324699401855,
      "learning_rate": 2.5708502024291497e-05,
      "loss": 1.6937,
      "step": 254400
    },
    {
      "epoch": 19.44083721640822,
      "grad_norm": 9.400710105895996,
      "learning_rate": 2.569895347948973e-05,
      "loss": 1.7938,
      "step": 254500
    },
    {
      "epoch": 19.44847605224964,
      "grad_norm": 6.3648786544799805,
      "learning_rate": 2.5689404934687955e-05,
      "loss": 1.7759,
      "step": 254600
    },
    {
      "epoch": 19.456114888091054,
      "grad_norm": 7.675151824951172,
      "learning_rate": 2.567985638988618e-05,
      "loss": 1.7789,
      "step": 254700
    },
    {
      "epoch": 19.463753723932474,
      "grad_norm": 6.093334197998047,
      "learning_rate": 2.5670307845084412e-05,
      "loss": 1.843,
      "step": 254800
    },
    {
      "epoch": 19.47139255977389,
      "grad_norm": 6.525491237640381,
      "learning_rate": 2.5660759300282637e-05,
      "loss": 1.8274,
      "step": 254900
    },
    {
      "epoch": 19.47903139561531,
      "grad_norm": 5.53358268737793,
      "learning_rate": 2.5651210755480866e-05,
      "loss": 1.8053,
      "step": 255000
    },
    {
      "epoch": 19.486670231456728,
      "grad_norm": 3.9261558055877686,
      "learning_rate": 2.5641662210679095e-05,
      "loss": 1.7123,
      "step": 255100
    },
    {
      "epoch": 19.494309067298143,
      "grad_norm": 7.617464542388916,
      "learning_rate": 2.5632113665877323e-05,
      "loss": 1.8752,
      "step": 255200
    },
    {
      "epoch": 19.501947903139563,
      "grad_norm": 5.115971565246582,
      "learning_rate": 2.562256512107555e-05,
      "loss": 1.8179,
      "step": 255300
    },
    {
      "epoch": 19.50958673898098,
      "grad_norm": 6.844132423400879,
      "learning_rate": 2.561301657627378e-05,
      "loss": 1.7441,
      "step": 255400
    },
    {
      "epoch": 19.517225574822398,
      "grad_norm": 11.256169319152832,
      "learning_rate": 2.5603468031472006e-05,
      "loss": 1.6796,
      "step": 255500
    },
    {
      "epoch": 19.524864410663813,
      "grad_norm": 9.109013557434082,
      "learning_rate": 2.559391948667023e-05,
      "loss": 1.8151,
      "step": 255600
    },
    {
      "epoch": 19.532503246505232,
      "grad_norm": 7.8251495361328125,
      "learning_rate": 2.5584370941868463e-05,
      "loss": 1.8123,
      "step": 255700
    },
    {
      "epoch": 19.54014208234665,
      "grad_norm": 5.873737812042236,
      "learning_rate": 2.557482239706669e-05,
      "loss": 1.7768,
      "step": 255800
    },
    {
      "epoch": 19.547780918188067,
      "grad_norm": 8.475873947143555,
      "learning_rate": 2.5565273852264914e-05,
      "loss": 1.8123,
      "step": 255900
    },
    {
      "epoch": 19.555419754029487,
      "grad_norm": 3.2983429431915283,
      "learning_rate": 2.5555725307463146e-05,
      "loss": 1.7622,
      "step": 256000
    },
    {
      "epoch": 19.563058589870902,
      "grad_norm": 7.786007404327393,
      "learning_rate": 2.554617676266137e-05,
      "loss": 1.8374,
      "step": 256100
    },
    {
      "epoch": 19.57069742571232,
      "grad_norm": 9.183237075805664,
      "learning_rate": 2.5536628217859597e-05,
      "loss": 1.829,
      "step": 256200
    },
    {
      "epoch": 19.57833626155374,
      "grad_norm": 7.126972675323486,
      "learning_rate": 2.552707967305783e-05,
      "loss": 1.7183,
      "step": 256300
    },
    {
      "epoch": 19.585975097395156,
      "grad_norm": 4.92606782913208,
      "learning_rate": 2.5517531128256058e-05,
      "loss": 1.7368,
      "step": 256400
    },
    {
      "epoch": 19.593613933236576,
      "grad_norm": 7.411762714385986,
      "learning_rate": 2.5507982583454283e-05,
      "loss": 1.7955,
      "step": 256500
    },
    {
      "epoch": 19.60125276907799,
      "grad_norm": 9.169584274291992,
      "learning_rate": 2.5498434038652515e-05,
      "loss": 1.7952,
      "step": 256600
    },
    {
      "epoch": 19.60889160491941,
      "grad_norm": 8.142279624938965,
      "learning_rate": 2.548888549385074e-05,
      "loss": 1.701,
      "step": 256700
    },
    {
      "epoch": 19.61653044076083,
      "grad_norm": 6.227843284606934,
      "learning_rate": 2.5479336949048966e-05,
      "loss": 1.7521,
      "step": 256800
    },
    {
      "epoch": 19.624169276602245,
      "grad_norm": 8.325855255126953,
      "learning_rate": 2.546978840424719e-05,
      "loss": 1.8362,
      "step": 256900
    },
    {
      "epoch": 19.631808112443665,
      "grad_norm": 7.851476192474365,
      "learning_rate": 2.5460239859445423e-05,
      "loss": 1.8341,
      "step": 257000
    },
    {
      "epoch": 19.63944694828508,
      "grad_norm": 7.345451831817627,
      "learning_rate": 2.5450691314643648e-05,
      "loss": 1.8492,
      "step": 257100
    },
    {
      "epoch": 19.6470857841265,
      "grad_norm": 7.132684230804443,
      "learning_rate": 2.5441142769841874e-05,
      "loss": 1.7855,
      "step": 257200
    },
    {
      "epoch": 19.654724619967915,
      "grad_norm": 8.484800338745117,
      "learning_rate": 2.5431594225040106e-05,
      "loss": 1.6964,
      "step": 257300
    },
    {
      "epoch": 19.662363455809334,
      "grad_norm": 6.787213325500488,
      "learning_rate": 2.542204568023833e-05,
      "loss": 1.7848,
      "step": 257400
    },
    {
      "epoch": 19.670002291650754,
      "grad_norm": 6.507683753967285,
      "learning_rate": 2.541249713543656e-05,
      "loss": 1.8462,
      "step": 257500
    },
    {
      "epoch": 19.67764112749217,
      "grad_norm": 6.9251861572265625,
      "learning_rate": 2.5402948590634788e-05,
      "loss": 1.7642,
      "step": 257600
    },
    {
      "epoch": 19.68527996333359,
      "grad_norm": 7.703364849090576,
      "learning_rate": 2.5393400045833017e-05,
      "loss": 1.7597,
      "step": 257700
    },
    {
      "epoch": 19.692918799175004,
      "grad_norm": 7.812880039215088,
      "learning_rate": 2.5383851501031242e-05,
      "loss": 1.7911,
      "step": 257800
    },
    {
      "epoch": 19.700557635016423,
      "grad_norm": 11.238703727722168,
      "learning_rate": 2.5374302956229474e-05,
      "loss": 1.7985,
      "step": 257900
    },
    {
      "epoch": 19.708196470857843,
      "grad_norm": 8.501546859741211,
      "learning_rate": 2.53647544114277e-05,
      "loss": 1.842,
      "step": 258000
    },
    {
      "epoch": 19.715835306699258,
      "grad_norm": 7.704554080963135,
      "learning_rate": 2.5355205866625925e-05,
      "loss": 1.7885,
      "step": 258100
    },
    {
      "epoch": 19.723474142540677,
      "grad_norm": 8.252143859863281,
      "learning_rate": 2.5345657321824157e-05,
      "loss": 1.6234,
      "step": 258200
    },
    {
      "epoch": 19.731112978382093,
      "grad_norm": 7.986732006072998,
      "learning_rate": 2.5336108777022382e-05,
      "loss": 1.8148,
      "step": 258300
    },
    {
      "epoch": 19.738751814223512,
      "grad_norm": 8.043939590454102,
      "learning_rate": 2.5326560232220608e-05,
      "loss": 1.6717,
      "step": 258400
    },
    {
      "epoch": 19.74639065006493,
      "grad_norm": 8.911226272583008,
      "learning_rate": 2.531701168741884e-05,
      "loss": 1.8202,
      "step": 258500
    },
    {
      "epoch": 19.754029485906347,
      "grad_norm": 7.462522029876709,
      "learning_rate": 2.5307463142617065e-05,
      "loss": 1.8205,
      "step": 258600
    },
    {
      "epoch": 19.761668321747766,
      "grad_norm": 6.3718109130859375,
      "learning_rate": 2.529791459781529e-05,
      "loss": 1.6975,
      "step": 258700
    },
    {
      "epoch": 19.769307157589182,
      "grad_norm": 7.8210978507995605,
      "learning_rate": 2.5288366053013522e-05,
      "loss": 1.8944,
      "step": 258800
    },
    {
      "epoch": 19.7769459934306,
      "grad_norm": 6.197768688201904,
      "learning_rate": 2.527881750821175e-05,
      "loss": 1.7728,
      "step": 258900
    },
    {
      "epoch": 19.78458482927202,
      "grad_norm": 8.384961128234863,
      "learning_rate": 2.5269268963409976e-05,
      "loss": 1.8065,
      "step": 259000
    },
    {
      "epoch": 19.792223665113436,
      "grad_norm": 9.955377578735352,
      "learning_rate": 2.525972041860821e-05,
      "loss": 1.7413,
      "step": 259100
    },
    {
      "epoch": 19.799862500954855,
      "grad_norm": 6.592031955718994,
      "learning_rate": 2.5250171873806434e-05,
      "loss": 1.7264,
      "step": 259200
    },
    {
      "epoch": 19.80750133679627,
      "grad_norm": 6.486998081207275,
      "learning_rate": 2.524062332900466e-05,
      "loss": 1.8263,
      "step": 259300
    },
    {
      "epoch": 19.81514017263769,
      "grad_norm": 7.439182281494141,
      "learning_rate": 2.523107478420289e-05,
      "loss": 1.8279,
      "step": 259400
    },
    {
      "epoch": 19.82277900847911,
      "grad_norm": 9.476179122924805,
      "learning_rate": 2.5221526239401117e-05,
      "loss": 1.769,
      "step": 259500
    },
    {
      "epoch": 19.830417844320525,
      "grad_norm": 7.324405670166016,
      "learning_rate": 2.5211977694599342e-05,
      "loss": 1.8294,
      "step": 259600
    },
    {
      "epoch": 19.838056680161944,
      "grad_norm": 6.543698787689209,
      "learning_rate": 2.5202429149797574e-05,
      "loss": 1.6594,
      "step": 259700
    },
    {
      "epoch": 19.84569551600336,
      "grad_norm": 8.440583229064941,
      "learning_rate": 2.51928806049958e-05,
      "loss": 1.7656,
      "step": 259800
    },
    {
      "epoch": 19.85333435184478,
      "grad_norm": 6.735774040222168,
      "learning_rate": 2.5183332060194025e-05,
      "loss": 1.8659,
      "step": 259900
    },
    {
      "epoch": 19.860973187686195,
      "grad_norm": 6.387149333953857,
      "learning_rate": 2.5173783515392257e-05,
      "loss": 1.7583,
      "step": 260000
    },
    {
      "epoch": 19.868612023527614,
      "grad_norm": 8.211061477661133,
      "learning_rate": 2.5164234970590482e-05,
      "loss": 1.7732,
      "step": 260100
    },
    {
      "epoch": 19.876250859369033,
      "grad_norm": 7.062028408050537,
      "learning_rate": 2.515468642578871e-05,
      "loss": 1.7522,
      "step": 260200
    },
    {
      "epoch": 19.88388969521045,
      "grad_norm": 6.225802898406982,
      "learning_rate": 2.514513788098694e-05,
      "loss": 1.7555,
      "step": 260300
    },
    {
      "epoch": 19.89152853105187,
      "grad_norm": 7.618464469909668,
      "learning_rate": 2.5135589336185168e-05,
      "loss": 1.7392,
      "step": 260400
    },
    {
      "epoch": 19.899167366893284,
      "grad_norm": 6.215937614440918,
      "learning_rate": 2.5126040791383393e-05,
      "loss": 1.7525,
      "step": 260500
    },
    {
      "epoch": 19.906806202734703,
      "grad_norm": 7.226278305053711,
      "learning_rate": 2.5116492246581625e-05,
      "loss": 1.8533,
      "step": 260600
    },
    {
      "epoch": 19.914445038576122,
      "grad_norm": 9.10888385772705,
      "learning_rate": 2.510694370177985e-05,
      "loss": 1.806,
      "step": 260700
    },
    {
      "epoch": 19.922083874417538,
      "grad_norm": 8.451748847961426,
      "learning_rate": 2.5097395156978076e-05,
      "loss": 1.8237,
      "step": 260800
    },
    {
      "epoch": 19.929722710258957,
      "grad_norm": 4.700009822845459,
      "learning_rate": 2.5087846612176308e-05,
      "loss": 1.7814,
      "step": 260900
    },
    {
      "epoch": 19.937361546100373,
      "grad_norm": 7.303822040557861,
      "learning_rate": 2.5078298067374533e-05,
      "loss": 1.774,
      "step": 261000
    },
    {
      "epoch": 19.945000381941792,
      "grad_norm": 9.861743927001953,
      "learning_rate": 2.506874952257276e-05,
      "loss": 1.8545,
      "step": 261100
    },
    {
      "epoch": 19.95263921778321,
      "grad_norm": 6.904852390289307,
      "learning_rate": 2.505920097777099e-05,
      "loss": 1.8078,
      "step": 261200
    },
    {
      "epoch": 19.960278053624627,
      "grad_norm": 7.157458305358887,
      "learning_rate": 2.5049652432969216e-05,
      "loss": 1.7915,
      "step": 261300
    },
    {
      "epoch": 19.967916889466046,
      "grad_norm": 6.9495649337768555,
      "learning_rate": 2.5040103888167445e-05,
      "loss": 1.8194,
      "step": 261400
    },
    {
      "epoch": 19.975555725307462,
      "grad_norm": 7.370140552520752,
      "learning_rate": 2.5030555343365673e-05,
      "loss": 1.7636,
      "step": 261500
    },
    {
      "epoch": 19.98319456114888,
      "grad_norm": 7.740662574768066,
      "learning_rate": 2.5021006798563902e-05,
      "loss": 1.7073,
      "step": 261600
    },
    {
      "epoch": 19.990833396990297,
      "grad_norm": 6.73961877822876,
      "learning_rate": 2.5011458253762127e-05,
      "loss": 1.7939,
      "step": 261700
    },
    {
      "epoch": 19.998472232831716,
      "grad_norm": 6.576571941375732,
      "learning_rate": 2.500190970896036e-05,
      "loss": 1.8327,
      "step": 261800
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.8127367496490479,
      "eval_runtime": 1.4789,
      "eval_samples_per_second": 466.563,
      "eval_steps_per_second": 466.563,
      "step": 261820
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.5462490320205688,
      "eval_runtime": 38.0565,
      "eval_samples_per_second": 343.989,
      "eval_steps_per_second": 343.989,
      "step": 261820
    },
    {
      "epoch": 20.006111068673135,
      "grad_norm": 6.331457138061523,
      "learning_rate": 2.4992361164158585e-05,
      "loss": 1.8202,
      "step": 261900
    },
    {
      "epoch": 20.01374990451455,
      "grad_norm": 6.339601993560791,
      "learning_rate": 2.498281261935681e-05,
      "loss": 1.7838,
      "step": 262000
    },
    {
      "epoch": 20.02138874035597,
      "grad_norm": 6.924256324768066,
      "learning_rate": 2.497326407455504e-05,
      "loss": 1.76,
      "step": 262100
    },
    {
      "epoch": 20.029027576197386,
      "grad_norm": 7.935174465179443,
      "learning_rate": 2.4963715529753268e-05,
      "loss": 1.7598,
      "step": 262200
    },
    {
      "epoch": 20.036666412038805,
      "grad_norm": 6.757978916168213,
      "learning_rate": 2.4954166984951493e-05,
      "loss": 1.7877,
      "step": 262300
    },
    {
      "epoch": 20.044305247880224,
      "grad_norm": 8.12645435333252,
      "learning_rate": 2.494461844014972e-05,
      "loss": 1.7808,
      "step": 262400
    },
    {
      "epoch": 20.05194408372164,
      "grad_norm": 7.154923439025879,
      "learning_rate": 2.493506989534795e-05,
      "loss": 1.7345,
      "step": 262500
    },
    {
      "epoch": 20.05958291956306,
      "grad_norm": 9.76746940612793,
      "learning_rate": 2.4925521350546176e-05,
      "loss": 1.8137,
      "step": 262600
    },
    {
      "epoch": 20.067221755404475,
      "grad_norm": 7.740725040435791,
      "learning_rate": 2.4915972805744404e-05,
      "loss": 1.8279,
      "step": 262700
    },
    {
      "epoch": 20.074860591245894,
      "grad_norm": 5.052449703216553,
      "learning_rate": 2.4906424260942633e-05,
      "loss": 1.8177,
      "step": 262800
    },
    {
      "epoch": 20.082499427087313,
      "grad_norm": 8.677539825439453,
      "learning_rate": 2.489687571614086e-05,
      "loss": 1.7324,
      "step": 262900
    },
    {
      "epoch": 20.09013826292873,
      "grad_norm": 8.796772956848145,
      "learning_rate": 2.488732717133909e-05,
      "loss": 1.8159,
      "step": 263000
    },
    {
      "epoch": 20.09777709877015,
      "grad_norm": 8.56759262084961,
      "learning_rate": 2.487777862653732e-05,
      "loss": 1.7224,
      "step": 263100
    },
    {
      "epoch": 20.105415934611564,
      "grad_norm": 6.049667835235596,
      "learning_rate": 2.4868230081735544e-05,
      "loss": 1.8554,
      "step": 263200
    },
    {
      "epoch": 20.113054770452983,
      "grad_norm": 7.7177019119262695,
      "learning_rate": 2.4858681536933773e-05,
      "loss": 1.718,
      "step": 263300
    },
    {
      "epoch": 20.120693606294402,
      "grad_norm": 9.567590713500977,
      "learning_rate": 2.4849132992132e-05,
      "loss": 1.774,
      "step": 263400
    },
    {
      "epoch": 20.128332442135818,
      "grad_norm": 8.521903038024902,
      "learning_rate": 2.4839584447330227e-05,
      "loss": 1.8185,
      "step": 263500
    },
    {
      "epoch": 20.135971277977237,
      "grad_norm": 4.195415496826172,
      "learning_rate": 2.4830035902528456e-05,
      "loss": 1.7638,
      "step": 263600
    },
    {
      "epoch": 20.143610113818653,
      "grad_norm": 6.5639967918396,
      "learning_rate": 2.4820487357726684e-05,
      "loss": 1.8109,
      "step": 263700
    },
    {
      "epoch": 20.151248949660072,
      "grad_norm": 7.240155220031738,
      "learning_rate": 2.481093881292491e-05,
      "loss": 1.7945,
      "step": 263800
    },
    {
      "epoch": 20.158887785501488,
      "grad_norm": 4.524588108062744,
      "learning_rate": 2.480139026812314e-05,
      "loss": 1.7847,
      "step": 263900
    },
    {
      "epoch": 20.166526621342907,
      "grad_norm": 8.519088745117188,
      "learning_rate": 2.4791841723321367e-05,
      "loss": 1.7507,
      "step": 264000
    },
    {
      "epoch": 20.174165457184326,
      "grad_norm": 7.034207344055176,
      "learning_rate": 2.4782293178519596e-05,
      "loss": 1.7621,
      "step": 264100
    },
    {
      "epoch": 20.181804293025742,
      "grad_norm": 4.937370777130127,
      "learning_rate": 2.4772744633717824e-05,
      "loss": 1.8009,
      "step": 264200
    },
    {
      "epoch": 20.18944312886716,
      "grad_norm": 8.989465713500977,
      "learning_rate": 2.476319608891605e-05,
      "loss": 1.7518,
      "step": 264300
    },
    {
      "epoch": 20.197081964708577,
      "grad_norm": 8.881061553955078,
      "learning_rate": 2.475364754411428e-05,
      "loss": 1.7152,
      "step": 264400
    },
    {
      "epoch": 20.204720800549996,
      "grad_norm": 6.757695198059082,
      "learning_rate": 2.4744098999312507e-05,
      "loss": 1.7951,
      "step": 264500
    },
    {
      "epoch": 20.212359636391415,
      "grad_norm": 6.197822570800781,
      "learning_rate": 2.4734550454510732e-05,
      "loss": 1.6606,
      "step": 264600
    },
    {
      "epoch": 20.21999847223283,
      "grad_norm": 9.165003776550293,
      "learning_rate": 2.472500190970896e-05,
      "loss": 1.801,
      "step": 264700
    },
    {
      "epoch": 20.22763730807425,
      "grad_norm": 7.135333061218262,
      "learning_rate": 2.471545336490719e-05,
      "loss": 1.7603,
      "step": 264800
    },
    {
      "epoch": 20.235276143915666,
      "grad_norm": 7.22377872467041,
      "learning_rate": 2.4705904820105415e-05,
      "loss": 1.8204,
      "step": 264900
    },
    {
      "epoch": 20.242914979757085,
      "grad_norm": 7.203999042510986,
      "learning_rate": 2.4696356275303644e-05,
      "loss": 1.7295,
      "step": 265000
    },
    {
      "epoch": 20.250553815598504,
      "grad_norm": 5.9303879737854,
      "learning_rate": 2.4686807730501873e-05,
      "loss": 1.6631,
      "step": 265100
    },
    {
      "epoch": 20.25819265143992,
      "grad_norm": 7.891974449157715,
      "learning_rate": 2.46772591857001e-05,
      "loss": 1.8228,
      "step": 265200
    },
    {
      "epoch": 20.26583148728134,
      "grad_norm": 9.90658187866211,
      "learning_rate": 2.466771064089833e-05,
      "loss": 1.8234,
      "step": 265300
    },
    {
      "epoch": 20.273470323122755,
      "grad_norm": 7.838770389556885,
      "learning_rate": 2.465816209609656e-05,
      "loss": 1.8283,
      "step": 265400
    },
    {
      "epoch": 20.281109158964174,
      "grad_norm": 6.8418288230896,
      "learning_rate": 2.4648613551294784e-05,
      "loss": 1.7648,
      "step": 265500
    },
    {
      "epoch": 20.288747994805593,
      "grad_norm": 6.949066162109375,
      "learning_rate": 2.4639065006493013e-05,
      "loss": 1.766,
      "step": 265600
    },
    {
      "epoch": 20.29638683064701,
      "grad_norm": 6.822576522827148,
      "learning_rate": 2.462951646169124e-05,
      "loss": 1.7801,
      "step": 265700
    },
    {
      "epoch": 20.304025666488428,
      "grad_norm": 7.382580280303955,
      "learning_rate": 2.4619967916889467e-05,
      "loss": 1.7644,
      "step": 265800
    },
    {
      "epoch": 20.311664502329844,
      "grad_norm": 6.787778854370117,
      "learning_rate": 2.4610419372087695e-05,
      "loss": 1.8102,
      "step": 265900
    },
    {
      "epoch": 20.319303338171263,
      "grad_norm": 5.7273383140563965,
      "learning_rate": 2.4600870827285924e-05,
      "loss": 1.7916,
      "step": 266000
    },
    {
      "epoch": 20.32694217401268,
      "grad_norm": 8.705297470092773,
      "learning_rate": 2.459132228248415e-05,
      "loss": 1.759,
      "step": 266100
    },
    {
      "epoch": 20.334581009854098,
      "grad_norm": 7.099379062652588,
      "learning_rate": 2.4581773737682378e-05,
      "loss": 1.8484,
      "step": 266200
    },
    {
      "epoch": 20.342219845695517,
      "grad_norm": 6.740135192871094,
      "learning_rate": 2.4572225192880607e-05,
      "loss": 1.6741,
      "step": 266300
    },
    {
      "epoch": 20.349858681536933,
      "grad_norm": 9.722932815551758,
      "learning_rate": 2.4562676648078832e-05,
      "loss": 1.8086,
      "step": 266400
    },
    {
      "epoch": 20.357497517378352,
      "grad_norm": 7.13271951675415,
      "learning_rate": 2.455312810327706e-05,
      "loss": 1.7309,
      "step": 266500
    },
    {
      "epoch": 20.365136353219768,
      "grad_norm": 8.900910377502441,
      "learning_rate": 2.454357955847529e-05,
      "loss": 1.7555,
      "step": 266600
    },
    {
      "epoch": 20.372775189061187,
      "grad_norm": 7.920672416687012,
      "learning_rate": 2.4534031013673518e-05,
      "loss": 1.765,
      "step": 266700
    },
    {
      "epoch": 20.380414024902606,
      "grad_norm": 7.787484645843506,
      "learning_rate": 2.4524482468871747e-05,
      "loss": 1.8387,
      "step": 266800
    },
    {
      "epoch": 20.388052860744022,
      "grad_norm": 8.635891914367676,
      "learning_rate": 2.4514933924069972e-05,
      "loss": 1.839,
      "step": 266900
    },
    {
      "epoch": 20.39569169658544,
      "grad_norm": 8.021379470825195,
      "learning_rate": 2.45053853792682e-05,
      "loss": 1.8172,
      "step": 267000
    },
    {
      "epoch": 20.403330532426857,
      "grad_norm": 4.964816093444824,
      "learning_rate": 2.449583683446643e-05,
      "loss": 1.9197,
      "step": 267100
    },
    {
      "epoch": 20.410969368268276,
      "grad_norm": 8.991214752197266,
      "learning_rate": 2.4486288289664655e-05,
      "loss": 1.7427,
      "step": 267200
    },
    {
      "epoch": 20.418608204109695,
      "grad_norm": 7.944890022277832,
      "learning_rate": 2.4476739744862883e-05,
      "loss": 1.867,
      "step": 267300
    },
    {
      "epoch": 20.42624703995111,
      "grad_norm": 6.1211700439453125,
      "learning_rate": 2.4467191200061112e-05,
      "loss": 1.7066,
      "step": 267400
    },
    {
      "epoch": 20.43388587579253,
      "grad_norm": 9.297627449035645,
      "learning_rate": 2.4457642655259337e-05,
      "loss": 1.6467,
      "step": 267500
    },
    {
      "epoch": 20.441524711633946,
      "grad_norm": 7.939676761627197,
      "learning_rate": 2.4448094110457566e-05,
      "loss": 1.7292,
      "step": 267600
    },
    {
      "epoch": 20.449163547475365,
      "grad_norm": 7.802010536193848,
      "learning_rate": 2.4438545565655795e-05,
      "loss": 1.821,
      "step": 267700
    },
    {
      "epoch": 20.456802383316784,
      "grad_norm": 10.25533676147461,
      "learning_rate": 2.4428997020854024e-05,
      "loss": 1.7274,
      "step": 267800
    },
    {
      "epoch": 20.4644412191582,
      "grad_norm": 9.602995872497559,
      "learning_rate": 2.4419448476052252e-05,
      "loss": 1.7954,
      "step": 267900
    },
    {
      "epoch": 20.47208005499962,
      "grad_norm": 7.936100482940674,
      "learning_rate": 2.440989993125048e-05,
      "loss": 1.7724,
      "step": 268000
    },
    {
      "epoch": 20.479718890841035,
      "grad_norm": 8.265363693237305,
      "learning_rate": 2.4400351386448706e-05,
      "loss": 1.8137,
      "step": 268100
    },
    {
      "epoch": 20.487357726682454,
      "grad_norm": 7.417712688446045,
      "learning_rate": 2.4390802841646935e-05,
      "loss": 1.6716,
      "step": 268200
    },
    {
      "epoch": 20.49499656252387,
      "grad_norm": 7.119606971740723,
      "learning_rate": 2.4381254296845164e-05,
      "loss": 1.8237,
      "step": 268300
    },
    {
      "epoch": 20.50263539836529,
      "grad_norm": 11.92236614227295,
      "learning_rate": 2.437170575204339e-05,
      "loss": 1.7216,
      "step": 268400
    },
    {
      "epoch": 20.510274234206708,
      "grad_norm": 6.767589092254639,
      "learning_rate": 2.4362157207241618e-05,
      "loss": 1.8848,
      "step": 268500
    },
    {
      "epoch": 20.517913070048124,
      "grad_norm": 6.916305065155029,
      "learning_rate": 2.4352608662439846e-05,
      "loss": 1.809,
      "step": 268600
    },
    {
      "epoch": 20.525551905889543,
      "grad_norm": 6.640739440917969,
      "learning_rate": 2.434306011763807e-05,
      "loss": 1.7847,
      "step": 268700
    },
    {
      "epoch": 20.53319074173096,
      "grad_norm": 9.88626766204834,
      "learning_rate": 2.43335115728363e-05,
      "loss": 1.6793,
      "step": 268800
    },
    {
      "epoch": 20.540829577572378,
      "grad_norm": 8.7982759475708,
      "learning_rate": 2.432396302803453e-05,
      "loss": 1.779,
      "step": 268900
    },
    {
      "epoch": 20.548468413413797,
      "grad_norm": 7.96862268447876,
      "learning_rate": 2.4314414483232754e-05,
      "loss": 1.7527,
      "step": 269000
    },
    {
      "epoch": 20.556107249255213,
      "grad_norm": 8.723724365234375,
      "learning_rate": 2.4304865938430983e-05,
      "loss": 1.8228,
      "step": 269100
    },
    {
      "epoch": 20.563746085096632,
      "grad_norm": 4.1878557205200195,
      "learning_rate": 2.429531739362921e-05,
      "loss": 1.722,
      "step": 269200
    },
    {
      "epoch": 20.571384920938048,
      "grad_norm": 6.829499244689941,
      "learning_rate": 2.428576884882744e-05,
      "loss": 1.8934,
      "step": 269300
    },
    {
      "epoch": 20.579023756779467,
      "grad_norm": 8.538832664489746,
      "learning_rate": 2.427622030402567e-05,
      "loss": 1.7102,
      "step": 269400
    },
    {
      "epoch": 20.586662592620886,
      "grad_norm": 6.817370414733887,
      "learning_rate": 2.4266671759223898e-05,
      "loss": 1.832,
      "step": 269500
    },
    {
      "epoch": 20.594301428462302,
      "grad_norm": 6.688027381896973,
      "learning_rate": 2.4257123214422123e-05,
      "loss": 1.7045,
      "step": 269600
    },
    {
      "epoch": 20.60194026430372,
      "grad_norm": 7.177577495574951,
      "learning_rate": 2.4247574669620352e-05,
      "loss": 1.7002,
      "step": 269700
    },
    {
      "epoch": 20.609579100145137,
      "grad_norm": 5.998094081878662,
      "learning_rate": 2.4238026124818577e-05,
      "loss": 1.7613,
      "step": 269800
    },
    {
      "epoch": 20.617217935986556,
      "grad_norm": 7.309361457824707,
      "learning_rate": 2.4228477580016806e-05,
      "loss": 1.6787,
      "step": 269900
    },
    {
      "epoch": 20.62485677182797,
      "grad_norm": 7.599937438964844,
      "learning_rate": 2.4218929035215034e-05,
      "loss": 1.7612,
      "step": 270000
    },
    {
      "epoch": 20.63249560766939,
      "grad_norm": 6.260167121887207,
      "learning_rate": 2.420938049041326e-05,
      "loss": 1.7813,
      "step": 270100
    },
    {
      "epoch": 20.64013444351081,
      "grad_norm": 8.166933059692383,
      "learning_rate": 2.419983194561149e-05,
      "loss": 1.7698,
      "step": 270200
    },
    {
      "epoch": 20.647773279352226,
      "grad_norm": 7.590663909912109,
      "learning_rate": 2.4190283400809717e-05,
      "loss": 1.8018,
      "step": 270300
    },
    {
      "epoch": 20.655412115193645,
      "grad_norm": 10.161627769470215,
      "learning_rate": 2.4180734856007946e-05,
      "loss": 1.8374,
      "step": 270400
    },
    {
      "epoch": 20.66305095103506,
      "grad_norm": 7.373344421386719,
      "learning_rate": 2.4171186311206175e-05,
      "loss": 1.7928,
      "step": 270500
    },
    {
      "epoch": 20.67068978687648,
      "grad_norm": 9.796122550964355,
      "learning_rate": 2.4161637766404403e-05,
      "loss": 1.7551,
      "step": 270600
    },
    {
      "epoch": 20.6783286227179,
      "grad_norm": 7.702237129211426,
      "learning_rate": 2.415208922160263e-05,
      "loss": 1.7552,
      "step": 270700
    },
    {
      "epoch": 20.685967458559315,
      "grad_norm": 7.020776748657227,
      "learning_rate": 2.4142540676800857e-05,
      "loss": 1.7448,
      "step": 270800
    },
    {
      "epoch": 20.693606294400734,
      "grad_norm": 6.391157627105713,
      "learning_rate": 2.4132992131999086e-05,
      "loss": 1.7046,
      "step": 270900
    },
    {
      "epoch": 20.70124513024215,
      "grad_norm": 6.358717441558838,
      "learning_rate": 2.412344358719731e-05,
      "loss": 1.7485,
      "step": 271000
    },
    {
      "epoch": 20.70888396608357,
      "grad_norm": 7.485467433929443,
      "learning_rate": 2.411389504239554e-05,
      "loss": 1.7952,
      "step": 271100
    },
    {
      "epoch": 20.716522801924988,
      "grad_norm": 9.066753387451172,
      "learning_rate": 2.410434649759377e-05,
      "loss": 1.7522,
      "step": 271200
    },
    {
      "epoch": 20.724161637766404,
      "grad_norm": 5.433282852172852,
      "learning_rate": 2.4094797952791994e-05,
      "loss": 1.734,
      "step": 271300
    },
    {
      "epoch": 20.731800473607823,
      "grad_norm": 6.750128746032715,
      "learning_rate": 2.4085249407990223e-05,
      "loss": 1.8091,
      "step": 271400
    },
    {
      "epoch": 20.73943930944924,
      "grad_norm": 6.682025909423828,
      "learning_rate": 2.407570086318845e-05,
      "loss": 1.6966,
      "step": 271500
    },
    {
      "epoch": 20.747078145290658,
      "grad_norm": 5.940717697143555,
      "learning_rate": 2.4066152318386677e-05,
      "loss": 1.7761,
      "step": 271600
    },
    {
      "epoch": 20.754716981132077,
      "grad_norm": 9.82996940612793,
      "learning_rate": 2.405660377358491e-05,
      "loss": 1.766,
      "step": 271700
    },
    {
      "epoch": 20.762355816973493,
      "grad_norm": 7.332681655883789,
      "learning_rate": 2.4047055228783137e-05,
      "loss": 1.7106,
      "step": 271800
    },
    {
      "epoch": 20.769994652814912,
      "grad_norm": 5.894590377807617,
      "learning_rate": 2.4037506683981363e-05,
      "loss": 1.7322,
      "step": 271900
    },
    {
      "epoch": 20.777633488656328,
      "grad_norm": 7.29469108581543,
      "learning_rate": 2.402795813917959e-05,
      "loss": 1.7478,
      "step": 272000
    },
    {
      "epoch": 20.785272324497747,
      "grad_norm": 8.271656036376953,
      "learning_rate": 2.401840959437782e-05,
      "loss": 1.8095,
      "step": 272100
    },
    {
      "epoch": 20.792911160339166,
      "grad_norm": 7.260009765625,
      "learning_rate": 2.4008861049576045e-05,
      "loss": 1.7688,
      "step": 272200
    },
    {
      "epoch": 20.80054999618058,
      "grad_norm": 6.767120838165283,
      "learning_rate": 2.3999312504774274e-05,
      "loss": 1.7341,
      "step": 272300
    },
    {
      "epoch": 20.808188832022,
      "grad_norm": 7.642522811889648,
      "learning_rate": 2.3989763959972503e-05,
      "loss": 1.7954,
      "step": 272400
    },
    {
      "epoch": 20.815827667863417,
      "grad_norm": 6.284935474395752,
      "learning_rate": 2.3980215415170728e-05,
      "loss": 1.7901,
      "step": 272500
    },
    {
      "epoch": 20.823466503704836,
      "grad_norm": 7.945003509521484,
      "learning_rate": 2.3970666870368957e-05,
      "loss": 1.86,
      "step": 272600
    },
    {
      "epoch": 20.83110533954625,
      "grad_norm": 10.740880966186523,
      "learning_rate": 2.3961118325567182e-05,
      "loss": 1.7884,
      "step": 272700
    },
    {
      "epoch": 20.83874417538767,
      "grad_norm": 8.57234001159668,
      "learning_rate": 2.395156978076541e-05,
      "loss": 1.7271,
      "step": 272800
    },
    {
      "epoch": 20.84638301122909,
      "grad_norm": 6.441096782684326,
      "learning_rate": 2.394202123596364e-05,
      "loss": 1.7343,
      "step": 272900
    },
    {
      "epoch": 20.854021847070506,
      "grad_norm": 6.752243995666504,
      "learning_rate": 2.3932472691161868e-05,
      "loss": 1.7596,
      "step": 273000
    },
    {
      "epoch": 20.861660682911925,
      "grad_norm": 8.168573379516602,
      "learning_rate": 2.3922924146360097e-05,
      "loss": 1.7463,
      "step": 273100
    },
    {
      "epoch": 20.86929951875334,
      "grad_norm": 7.163170337677002,
      "learning_rate": 2.3913375601558326e-05,
      "loss": 1.8404,
      "step": 273200
    },
    {
      "epoch": 20.87693835459476,
      "grad_norm": 7.1168060302734375,
      "learning_rate": 2.390382705675655e-05,
      "loss": 1.848,
      "step": 273300
    },
    {
      "epoch": 20.88457719043618,
      "grad_norm": 7.8873138427734375,
      "learning_rate": 2.389427851195478e-05,
      "loss": 1.8713,
      "step": 273400
    },
    {
      "epoch": 20.892216026277595,
      "grad_norm": 6.16652250289917,
      "learning_rate": 2.3884729967153008e-05,
      "loss": 1.7649,
      "step": 273500
    },
    {
      "epoch": 20.899854862119014,
      "grad_norm": 7.577007293701172,
      "learning_rate": 2.3875181422351234e-05,
      "loss": 1.7507,
      "step": 273600
    },
    {
      "epoch": 20.90749369796043,
      "grad_norm": 11.688929557800293,
      "learning_rate": 2.3865632877549462e-05,
      "loss": 1.7365,
      "step": 273700
    },
    {
      "epoch": 20.91513253380185,
      "grad_norm": 6.339113712310791,
      "learning_rate": 2.385608433274769e-05,
      "loss": 1.8102,
      "step": 273800
    },
    {
      "epoch": 20.922771369643268,
      "grad_norm": 5.962918281555176,
      "learning_rate": 2.3846535787945916e-05,
      "loss": 1.8245,
      "step": 273900
    },
    {
      "epoch": 20.930410205484684,
      "grad_norm": 6.725999355316162,
      "learning_rate": 2.3836987243144145e-05,
      "loss": 1.7836,
      "step": 274000
    },
    {
      "epoch": 20.938049041326103,
      "grad_norm": 5.394026279449463,
      "learning_rate": 2.3827438698342374e-05,
      "loss": 1.6983,
      "step": 274100
    },
    {
      "epoch": 20.94568787716752,
      "grad_norm": 6.838855266571045,
      "learning_rate": 2.3817890153540602e-05,
      "loss": 1.8927,
      "step": 274200
    },
    {
      "epoch": 20.953326713008938,
      "grad_norm": 6.383695602416992,
      "learning_rate": 2.380834160873883e-05,
      "loss": 1.8029,
      "step": 274300
    },
    {
      "epoch": 20.960965548850353,
      "grad_norm": 7.512608051300049,
      "learning_rate": 2.379879306393706e-05,
      "loss": 1.8634,
      "step": 274400
    },
    {
      "epoch": 20.968604384691773,
      "grad_norm": 9.085541725158691,
      "learning_rate": 2.3789244519135285e-05,
      "loss": 1.8111,
      "step": 274500
    },
    {
      "epoch": 20.976243220533192,
      "grad_norm": 10.91028118133545,
      "learning_rate": 2.3779695974333514e-05,
      "loss": 1.6896,
      "step": 274600
    },
    {
      "epoch": 20.983882056374608,
      "grad_norm": 8.48848819732666,
      "learning_rate": 2.3770147429531742e-05,
      "loss": 1.8318,
      "step": 274700
    },
    {
      "epoch": 20.991520892216027,
      "grad_norm": 7.871469020843506,
      "learning_rate": 2.3760598884729968e-05,
      "loss": 1.7345,
      "step": 274800
    },
    {
      "epoch": 20.999159728057442,
      "grad_norm": 8.003584861755371,
      "learning_rate": 2.3751050339928196e-05,
      "loss": 1.8216,
      "step": 274900
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.8113192319869995,
      "eval_runtime": 3.0117,
      "eval_samples_per_second": 229.105,
      "eval_steps_per_second": 229.105,
      "step": 274911
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.5382132530212402,
      "eval_runtime": 56.1795,
      "eval_samples_per_second": 233.021,
      "eval_steps_per_second": 233.021,
      "step": 274911
    },
    {
      "epoch": 21.00679856389886,
      "grad_norm": 6.949795246124268,
      "learning_rate": 2.3741501795126425e-05,
      "loss": 1.8062,
      "step": 275000
    },
    {
      "epoch": 21.01443739974028,
      "grad_norm": 6.892788887023926,
      "learning_rate": 2.373195325032465e-05,
      "loss": 1.7535,
      "step": 275100
    },
    {
      "epoch": 21.022076235581697,
      "grad_norm": 7.164505481719971,
      "learning_rate": 2.372240470552288e-05,
      "loss": 1.7221,
      "step": 275200
    },
    {
      "epoch": 21.029715071423116,
      "grad_norm": 7.212516784667969,
      "learning_rate": 2.3712856160721108e-05,
      "loss": 1.7491,
      "step": 275300
    },
    {
      "epoch": 21.03735390726453,
      "grad_norm": 8.3806734085083,
      "learning_rate": 2.3703307615919333e-05,
      "loss": 1.7754,
      "step": 275400
    },
    {
      "epoch": 21.04499274310595,
      "grad_norm": 6.006799697875977,
      "learning_rate": 2.3693759071117562e-05,
      "loss": 1.8098,
      "step": 275500
    },
    {
      "epoch": 21.05263157894737,
      "grad_norm": 8.3858642578125,
      "learning_rate": 2.368421052631579e-05,
      "loss": 1.7495,
      "step": 275600
    },
    {
      "epoch": 21.060270414788786,
      "grad_norm": 7.00961971282959,
      "learning_rate": 2.367466198151402e-05,
      "loss": 1.6693,
      "step": 275700
    },
    {
      "epoch": 21.067909250630205,
      "grad_norm": 7.425256729125977,
      "learning_rate": 2.3665113436712248e-05,
      "loss": 1.805,
      "step": 275800
    },
    {
      "epoch": 21.07554808647162,
      "grad_norm": 10.019676208496094,
      "learning_rate": 2.3655564891910473e-05,
      "loss": 1.6848,
      "step": 275900
    },
    {
      "epoch": 21.08318692231304,
      "grad_norm": 6.762725353240967,
      "learning_rate": 2.3646016347108702e-05,
      "loss": 1.8698,
      "step": 276000
    },
    {
      "epoch": 21.09082575815446,
      "grad_norm": 8.101149559020996,
      "learning_rate": 2.363646780230693e-05,
      "loss": 1.7225,
      "step": 276100
    },
    {
      "epoch": 21.098464593995875,
      "grad_norm": 10.953503608703613,
      "learning_rate": 2.3626919257505156e-05,
      "loss": 1.6843,
      "step": 276200
    },
    {
      "epoch": 21.106103429837294,
      "grad_norm": 7.3821024894714355,
      "learning_rate": 2.3617370712703385e-05,
      "loss": 1.6775,
      "step": 276300
    },
    {
      "epoch": 21.11374226567871,
      "grad_norm": 7.961270332336426,
      "learning_rate": 2.3607822167901613e-05,
      "loss": 1.8086,
      "step": 276400
    },
    {
      "epoch": 21.12138110152013,
      "grad_norm": 6.884787559509277,
      "learning_rate": 2.359827362309984e-05,
      "loss": 1.8616,
      "step": 276500
    },
    {
      "epoch": 21.129019937361544,
      "grad_norm": 6.968957424163818,
      "learning_rate": 2.3588725078298067e-05,
      "loss": 1.7611,
      "step": 276600
    },
    {
      "epoch": 21.136658773202964,
      "grad_norm": 8.770781517028809,
      "learning_rate": 2.3579176533496296e-05,
      "loss": 1.728,
      "step": 276700
    },
    {
      "epoch": 21.144297609044383,
      "grad_norm": 10.90809440612793,
      "learning_rate": 2.3569627988694525e-05,
      "loss": 1.7292,
      "step": 276800
    },
    {
      "epoch": 21.1519364448858,
      "grad_norm": 5.520751953125,
      "learning_rate": 2.3560079443892753e-05,
      "loss": 1.5273,
      "step": 276900
    },
    {
      "epoch": 21.159575280727218,
      "grad_norm": 7.0399088859558105,
      "learning_rate": 2.3550530899090982e-05,
      "loss": 1.7095,
      "step": 277000
    },
    {
      "epoch": 21.167214116568633,
      "grad_norm": 6.307295799255371,
      "learning_rate": 2.3540982354289207e-05,
      "loss": 1.7672,
      "step": 277100
    },
    {
      "epoch": 21.174852952410053,
      "grad_norm": 7.06673002243042,
      "learning_rate": 2.3531433809487436e-05,
      "loss": 1.7107,
      "step": 277200
    },
    {
      "epoch": 21.182491788251472,
      "grad_norm": 10.20231819152832,
      "learning_rate": 2.3521885264685665e-05,
      "loss": 1.8355,
      "step": 277300
    },
    {
      "epoch": 21.190130624092888,
      "grad_norm": 6.786630630493164,
      "learning_rate": 2.351233671988389e-05,
      "loss": 1.8715,
      "step": 277400
    },
    {
      "epoch": 21.197769459934307,
      "grad_norm": 6.017123699188232,
      "learning_rate": 2.350278817508212e-05,
      "loss": 1.7,
      "step": 277500
    },
    {
      "epoch": 21.205408295775722,
      "grad_norm": 7.607572555541992,
      "learning_rate": 2.3493239630280347e-05,
      "loss": 1.7746,
      "step": 277600
    },
    {
      "epoch": 21.21304713161714,
      "grad_norm": 5.942332744598389,
      "learning_rate": 2.3483691085478573e-05,
      "loss": 1.7919,
      "step": 277700
    },
    {
      "epoch": 21.22068596745856,
      "grad_norm": 7.415020942687988,
      "learning_rate": 2.34741425406768e-05,
      "loss": 1.8157,
      "step": 277800
    },
    {
      "epoch": 21.228324803299977,
      "grad_norm": 6.06465482711792,
      "learning_rate": 2.346459399587503e-05,
      "loss": 1.759,
      "step": 277900
    },
    {
      "epoch": 21.235963639141396,
      "grad_norm": 10.361551284790039,
      "learning_rate": 2.3455045451073255e-05,
      "loss": 1.8039,
      "step": 278000
    },
    {
      "epoch": 21.24360247498281,
      "grad_norm": 8.834137916564941,
      "learning_rate": 2.3445496906271487e-05,
      "loss": 1.6455,
      "step": 278100
    },
    {
      "epoch": 21.25124131082423,
      "grad_norm": 5.950827121734619,
      "learning_rate": 2.3435948361469716e-05,
      "loss": 1.8084,
      "step": 278200
    },
    {
      "epoch": 21.25888014666565,
      "grad_norm": 7.646331787109375,
      "learning_rate": 2.342639981666794e-05,
      "loss": 1.7264,
      "step": 278300
    },
    {
      "epoch": 21.266518982507066,
      "grad_norm": 8.66696834564209,
      "learning_rate": 2.341685127186617e-05,
      "loss": 1.8314,
      "step": 278400
    },
    {
      "epoch": 21.274157818348485,
      "grad_norm": 7.141238212585449,
      "learning_rate": 2.3407302727064395e-05,
      "loss": 1.7555,
      "step": 278500
    },
    {
      "epoch": 21.2817966541899,
      "grad_norm": 8.34810733795166,
      "learning_rate": 2.3397754182262624e-05,
      "loss": 1.8481,
      "step": 278600
    },
    {
      "epoch": 21.28943549003132,
      "grad_norm": 7.653770923614502,
      "learning_rate": 2.3388205637460853e-05,
      "loss": 1.705,
      "step": 278700
    },
    {
      "epoch": 21.297074325872735,
      "grad_norm": 6.767919540405273,
      "learning_rate": 2.3378657092659078e-05,
      "loss": 1.6418,
      "step": 278800
    },
    {
      "epoch": 21.304713161714155,
      "grad_norm": 11.868330955505371,
      "learning_rate": 2.3369108547857307e-05,
      "loss": 1.7377,
      "step": 278900
    },
    {
      "epoch": 21.312351997555574,
      "grad_norm": 8.14263916015625,
      "learning_rate": 2.3359560003055536e-05,
      "loss": 1.7073,
      "step": 279000
    },
    {
      "epoch": 21.31999083339699,
      "grad_norm": 6.759406566619873,
      "learning_rate": 2.335001145825376e-05,
      "loss": 1.7636,
      "step": 279100
    },
    {
      "epoch": 21.32762966923841,
      "grad_norm": 6.755765438079834,
      "learning_rate": 2.334046291345199e-05,
      "loss": 1.8277,
      "step": 279200
    },
    {
      "epoch": 21.335268505079824,
      "grad_norm": 6.809390068054199,
      "learning_rate": 2.3330914368650218e-05,
      "loss": 1.7613,
      "step": 279300
    },
    {
      "epoch": 21.342907340921244,
      "grad_norm": 7.2188591957092285,
      "learning_rate": 2.3321365823848447e-05,
      "loss": 1.7957,
      "step": 279400
    },
    {
      "epoch": 21.350546176762663,
      "grad_norm": 8.022159576416016,
      "learning_rate": 2.3311817279046676e-05,
      "loss": 1.8432,
      "step": 279500
    },
    {
      "epoch": 21.35818501260408,
      "grad_norm": 7.112551212310791,
      "learning_rate": 2.3302268734244904e-05,
      "loss": 1.7127,
      "step": 279600
    },
    {
      "epoch": 21.365823848445498,
      "grad_norm": 5.258622169494629,
      "learning_rate": 2.329272018944313e-05,
      "loss": 1.8434,
      "step": 279700
    },
    {
      "epoch": 21.373462684286913,
      "grad_norm": 6.74606990814209,
      "learning_rate": 2.3283171644641358e-05,
      "loss": 1.7587,
      "step": 279800
    },
    {
      "epoch": 21.381101520128333,
      "grad_norm": 11.817872047424316,
      "learning_rate": 2.3273623099839587e-05,
      "loss": 1.8155,
      "step": 279900
    },
    {
      "epoch": 21.38874035596975,
      "grad_norm": 5.860716819763184,
      "learning_rate": 2.3264074555037812e-05,
      "loss": 1.8111,
      "step": 280000
    },
    {
      "epoch": 21.396379191811167,
      "grad_norm": 6.667972564697266,
      "learning_rate": 2.325452601023604e-05,
      "loss": 1.8188,
      "step": 280100
    },
    {
      "epoch": 21.404018027652587,
      "grad_norm": 10.377957344055176,
      "learning_rate": 2.324497746543427e-05,
      "loss": 1.7767,
      "step": 280200
    },
    {
      "epoch": 21.411656863494002,
      "grad_norm": 7.11997652053833,
      "learning_rate": 2.3235428920632495e-05,
      "loss": 1.7693,
      "step": 280300
    },
    {
      "epoch": 21.41929569933542,
      "grad_norm": 7.217661380767822,
      "learning_rate": 2.3225880375830724e-05,
      "loss": 1.8867,
      "step": 280400
    },
    {
      "epoch": 21.42693453517684,
      "grad_norm": 7.80193567276001,
      "learning_rate": 2.3216331831028952e-05,
      "loss": 1.7644,
      "step": 280500
    },
    {
      "epoch": 21.434573371018256,
      "grad_norm": 8.160707473754883,
      "learning_rate": 2.320678328622718e-05,
      "loss": 1.8074,
      "step": 280600
    },
    {
      "epoch": 21.442212206859676,
      "grad_norm": 5.316622734069824,
      "learning_rate": 2.319723474142541e-05,
      "loss": 1.7705,
      "step": 280700
    },
    {
      "epoch": 21.44985104270109,
      "grad_norm": 12.748658180236816,
      "learning_rate": 2.318768619662364e-05,
      "loss": 1.871,
      "step": 280800
    },
    {
      "epoch": 21.45748987854251,
      "grad_norm": 8.155524253845215,
      "learning_rate": 2.3178137651821864e-05,
      "loss": 1.7827,
      "step": 280900
    },
    {
      "epoch": 21.465128714383926,
      "grad_norm": 6.463585376739502,
      "learning_rate": 2.3168589107020092e-05,
      "loss": 1.7974,
      "step": 281000
    },
    {
      "epoch": 21.472767550225345,
      "grad_norm": 8.288084030151367,
      "learning_rate": 2.315904056221832e-05,
      "loss": 1.7289,
      "step": 281100
    },
    {
      "epoch": 21.480406386066765,
      "grad_norm": 7.424131393432617,
      "learning_rate": 2.3149492017416546e-05,
      "loss": 1.7668,
      "step": 281200
    },
    {
      "epoch": 21.48804522190818,
      "grad_norm": 6.593140125274658,
      "learning_rate": 2.3139943472614775e-05,
      "loss": 1.7212,
      "step": 281300
    },
    {
      "epoch": 21.4956840577496,
      "grad_norm": 8.594148635864258,
      "learning_rate": 2.3130394927813e-05,
      "loss": 1.7717,
      "step": 281400
    },
    {
      "epoch": 21.503322893591015,
      "grad_norm": 6.910604953765869,
      "learning_rate": 2.312084638301123e-05,
      "loss": 1.772,
      "step": 281500
    },
    {
      "epoch": 21.510961729432434,
      "grad_norm": 7.771298885345459,
      "learning_rate": 2.3111297838209458e-05,
      "loss": 1.7896,
      "step": 281600
    },
    {
      "epoch": 21.518600565273854,
      "grad_norm": 5.440075397491455,
      "learning_rate": 2.3101749293407683e-05,
      "loss": 1.7785,
      "step": 281700
    },
    {
      "epoch": 21.52623940111527,
      "grad_norm": 7.572168827056885,
      "learning_rate": 2.3092200748605912e-05,
      "loss": 1.7591,
      "step": 281800
    },
    {
      "epoch": 21.53387823695669,
      "grad_norm": 8.74734115600586,
      "learning_rate": 2.308265220380414e-05,
      "loss": 1.7419,
      "step": 281900
    },
    {
      "epoch": 21.541517072798104,
      "grad_norm": 6.475425720214844,
      "learning_rate": 2.307310365900237e-05,
      "loss": 1.7691,
      "step": 282000
    },
    {
      "epoch": 21.549155908639523,
      "grad_norm": 7.155986309051514,
      "learning_rate": 2.3063555114200598e-05,
      "loss": 1.7296,
      "step": 282100
    },
    {
      "epoch": 21.556794744480943,
      "grad_norm": 7.351195335388184,
      "learning_rate": 2.3054006569398827e-05,
      "loss": 1.7784,
      "step": 282200
    },
    {
      "epoch": 21.56443358032236,
      "grad_norm": 9.865375518798828,
      "learning_rate": 2.3044458024597052e-05,
      "loss": 1.8042,
      "step": 282300
    },
    {
      "epoch": 21.572072416163778,
      "grad_norm": 8.382478713989258,
      "learning_rate": 2.303490947979528e-05,
      "loss": 1.7675,
      "step": 282400
    },
    {
      "epoch": 21.579711252005193,
      "grad_norm": 7.915172576904297,
      "learning_rate": 2.302536093499351e-05,
      "loss": 1.7439,
      "step": 282500
    },
    {
      "epoch": 21.587350087846612,
      "grad_norm": 4.076250076293945,
      "learning_rate": 2.3015812390191735e-05,
      "loss": 1.7387,
      "step": 282600
    },
    {
      "epoch": 21.594988923688028,
      "grad_norm": 5.623746871948242,
      "learning_rate": 2.3006263845389963e-05,
      "loss": 1.7538,
      "step": 282700
    },
    {
      "epoch": 21.602627759529447,
      "grad_norm": 7.347135543823242,
      "learning_rate": 2.2996715300588192e-05,
      "loss": 1.8331,
      "step": 282800
    },
    {
      "epoch": 21.610266595370867,
      "grad_norm": 8.049092292785645,
      "learning_rate": 2.2987166755786417e-05,
      "loss": 1.8238,
      "step": 282900
    },
    {
      "epoch": 21.617905431212282,
      "grad_norm": 8.22758960723877,
      "learning_rate": 2.2977618210984646e-05,
      "loss": 1.6403,
      "step": 283000
    },
    {
      "epoch": 21.6255442670537,
      "grad_norm": 6.42843770980835,
      "learning_rate": 2.2968069666182875e-05,
      "loss": 1.7769,
      "step": 283100
    },
    {
      "epoch": 21.633183102895117,
      "grad_norm": 8.258647918701172,
      "learning_rate": 2.2958521121381103e-05,
      "loss": 1.8286,
      "step": 283200
    },
    {
      "epoch": 21.640821938736536,
      "grad_norm": 7.613786220550537,
      "learning_rate": 2.2948972576579332e-05,
      "loss": 1.8092,
      "step": 283300
    },
    {
      "epoch": 21.648460774577956,
      "grad_norm": 7.708261489868164,
      "learning_rate": 2.293942403177756e-05,
      "loss": 1.8174,
      "step": 283400
    },
    {
      "epoch": 21.65609961041937,
      "grad_norm": 7.5704665184021,
      "learning_rate": 2.2929875486975786e-05,
      "loss": 1.6544,
      "step": 283500
    },
    {
      "epoch": 21.66373844626079,
      "grad_norm": 7.030709743499756,
      "learning_rate": 2.2920326942174015e-05,
      "loss": 1.7914,
      "step": 283600
    },
    {
      "epoch": 21.671377282102206,
      "grad_norm": 8.02849292755127,
      "learning_rate": 2.2910778397372243e-05,
      "loss": 1.7602,
      "step": 283700
    },
    {
      "epoch": 21.679016117943625,
      "grad_norm": 8.124923706054688,
      "learning_rate": 2.290122985257047e-05,
      "loss": 1.766,
      "step": 283800
    },
    {
      "epoch": 21.686654953785045,
      "grad_norm": 6.92450475692749,
      "learning_rate": 2.2891681307768697e-05,
      "loss": 1.8558,
      "step": 283900
    },
    {
      "epoch": 21.69429378962646,
      "grad_norm": 7.9752702713012695,
      "learning_rate": 2.2882132762966926e-05,
      "loss": 1.7662,
      "step": 284000
    },
    {
      "epoch": 21.70193262546788,
      "grad_norm": 8.349867820739746,
      "learning_rate": 2.287258421816515e-05,
      "loss": 1.7092,
      "step": 284100
    },
    {
      "epoch": 21.709571461309295,
      "grad_norm": 9.079878807067871,
      "learning_rate": 2.286303567336338e-05,
      "loss": 1.7829,
      "step": 284200
    },
    {
      "epoch": 21.717210297150714,
      "grad_norm": 8.645689010620117,
      "learning_rate": 2.2853487128561605e-05,
      "loss": 1.7914,
      "step": 284300
    },
    {
      "epoch": 21.724849132992134,
      "grad_norm": 6.60202169418335,
      "learning_rate": 2.2843938583759834e-05,
      "loss": 1.839,
      "step": 284400
    },
    {
      "epoch": 21.73248796883355,
      "grad_norm": 7.311476707458496,
      "learning_rate": 2.2834390038958066e-05,
      "loss": 1.7579,
      "step": 284500
    },
    {
      "epoch": 21.74012680467497,
      "grad_norm": 6.514003753662109,
      "learning_rate": 2.282484149415629e-05,
      "loss": 1.7524,
      "step": 284600
    },
    {
      "epoch": 21.747765640516384,
      "grad_norm": 7.321183681488037,
      "learning_rate": 2.281529294935452e-05,
      "loss": 1.8669,
      "step": 284700
    },
    {
      "epoch": 21.755404476357803,
      "grad_norm": 8.552590370178223,
      "learning_rate": 2.280574440455275e-05,
      "loss": 1.7916,
      "step": 284800
    },
    {
      "epoch": 21.763043312199223,
      "grad_norm": 7.300909996032715,
      "learning_rate": 2.2796195859750974e-05,
      "loss": 1.8056,
      "step": 284900
    },
    {
      "epoch": 21.77068214804064,
      "grad_norm": 7.522503852844238,
      "learning_rate": 2.2786647314949203e-05,
      "loss": 1.8128,
      "step": 285000
    },
    {
      "epoch": 21.778320983882058,
      "grad_norm": 8.751532554626465,
      "learning_rate": 2.277709877014743e-05,
      "loss": 1.7538,
      "step": 285100
    },
    {
      "epoch": 21.785959819723473,
      "grad_norm": 9.016571044921875,
      "learning_rate": 2.2767550225345657e-05,
      "loss": 1.7643,
      "step": 285200
    },
    {
      "epoch": 21.793598655564892,
      "grad_norm": 5.8128533363342285,
      "learning_rate": 2.2758001680543886e-05,
      "loss": 1.8169,
      "step": 285300
    },
    {
      "epoch": 21.801237491406308,
      "grad_norm": 5.721638202667236,
      "learning_rate": 2.2748453135742114e-05,
      "loss": 1.7835,
      "step": 285400
    },
    {
      "epoch": 21.808876327247727,
      "grad_norm": 6.886509895324707,
      "learning_rate": 2.273890459094034e-05,
      "loss": 1.7871,
      "step": 285500
    },
    {
      "epoch": 21.816515163089147,
      "grad_norm": 6.962474822998047,
      "learning_rate": 2.2729356046138568e-05,
      "loss": 1.7713,
      "step": 285600
    },
    {
      "epoch": 21.824153998930562,
      "grad_norm": 9.12755012512207,
      "learning_rate": 2.2719807501336797e-05,
      "loss": 1.8841,
      "step": 285700
    },
    {
      "epoch": 21.83179283477198,
      "grad_norm": 7.505934238433838,
      "learning_rate": 2.2710258956535026e-05,
      "loss": 1.7265,
      "step": 285800
    },
    {
      "epoch": 21.839431670613397,
      "grad_norm": 7.499716281890869,
      "learning_rate": 2.2700710411733254e-05,
      "loss": 1.7866,
      "step": 285900
    },
    {
      "epoch": 21.847070506454816,
      "grad_norm": 6.943342208862305,
      "learning_rate": 2.2691161866931483e-05,
      "loss": 1.7269,
      "step": 286000
    },
    {
      "epoch": 21.854709342296236,
      "grad_norm": 6.303421497344971,
      "learning_rate": 2.268161332212971e-05,
      "loss": 1.801,
      "step": 286100
    },
    {
      "epoch": 21.86234817813765,
      "grad_norm": 7.164680480957031,
      "learning_rate": 2.2672064777327937e-05,
      "loss": 1.6816,
      "step": 286200
    },
    {
      "epoch": 21.86998701397907,
      "grad_norm": 6.381612777709961,
      "learning_rate": 2.2662516232526166e-05,
      "loss": 1.7632,
      "step": 286300
    },
    {
      "epoch": 21.877625849820486,
      "grad_norm": 8.962456703186035,
      "learning_rate": 2.265296768772439e-05,
      "loss": 1.6625,
      "step": 286400
    },
    {
      "epoch": 21.885264685661905,
      "grad_norm": 7.797672748565674,
      "learning_rate": 2.264341914292262e-05,
      "loss": 1.7363,
      "step": 286500
    },
    {
      "epoch": 21.892903521503325,
      "grad_norm": 6.293539524078369,
      "learning_rate": 2.263387059812085e-05,
      "loss": 1.7823,
      "step": 286600
    },
    {
      "epoch": 21.90054235734474,
      "grad_norm": 10.2194185256958,
      "learning_rate": 2.2624322053319074e-05,
      "loss": 1.855,
      "step": 286700
    },
    {
      "epoch": 21.90818119318616,
      "grad_norm": 6.472330093383789,
      "learning_rate": 2.2614773508517302e-05,
      "loss": 1.81,
      "step": 286800
    },
    {
      "epoch": 21.915820029027575,
      "grad_norm": 8.48365306854248,
      "learning_rate": 2.260522496371553e-05,
      "loss": 1.7377,
      "step": 286900
    },
    {
      "epoch": 21.923458864868994,
      "grad_norm": 9.538483619689941,
      "learning_rate": 2.259567641891376e-05,
      "loss": 1.8019,
      "step": 287000
    },
    {
      "epoch": 21.93109770071041,
      "grad_norm": 6.542591094970703,
      "learning_rate": 2.258612787411199e-05,
      "loss": 1.8353,
      "step": 287100
    },
    {
      "epoch": 21.93873653655183,
      "grad_norm": 6.432797431945801,
      "learning_rate": 2.2576579329310214e-05,
      "loss": 1.7168,
      "step": 287200
    },
    {
      "epoch": 21.94637537239325,
      "grad_norm": 8.269052505493164,
      "learning_rate": 2.2567030784508442e-05,
      "loss": 1.7807,
      "step": 287300
    },
    {
      "epoch": 21.954014208234664,
      "grad_norm": 9.281574249267578,
      "learning_rate": 2.255748223970667e-05,
      "loss": 1.7202,
      "step": 287400
    },
    {
      "epoch": 21.961653044076083,
      "grad_norm": 7.103751182556152,
      "learning_rate": 2.2547933694904896e-05,
      "loss": 1.783,
      "step": 287500
    },
    {
      "epoch": 21.9692918799175,
      "grad_norm": 5.465821743011475,
      "learning_rate": 2.2538385150103125e-05,
      "loss": 1.7248,
      "step": 287600
    },
    {
      "epoch": 21.976930715758918,
      "grad_norm": 6.847120761871338,
      "learning_rate": 2.2528836605301354e-05,
      "loss": 1.7916,
      "step": 287700
    },
    {
      "epoch": 21.984569551600337,
      "grad_norm": 6.646319389343262,
      "learning_rate": 2.251928806049958e-05,
      "loss": 1.7681,
      "step": 287800
    },
    {
      "epoch": 21.992208387441753,
      "grad_norm": 9.741917610168457,
      "learning_rate": 2.2509739515697808e-05,
      "loss": 1.8067,
      "step": 287900
    },
    {
      "epoch": 21.999847223283172,
      "grad_norm": 10.708057403564453,
      "learning_rate": 2.2500190970896037e-05,
      "loss": 1.7469,
      "step": 288000
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.81264328956604,
      "eval_runtime": 2.9668,
      "eval_samples_per_second": 232.577,
      "eval_steps_per_second": 232.577,
      "step": 288002
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.5341901779174805,
      "eval_runtime": 55.8694,
      "eval_samples_per_second": 234.314,
      "eval_steps_per_second": 234.314,
      "step": 288002
    },
    {
      "epoch": 22.007486059124588,
      "grad_norm": 9.131793022155762,
      "learning_rate": 2.2490642426094262e-05,
      "loss": 1.7073,
      "step": 288100
    },
    {
      "epoch": 22.015124894966007,
      "grad_norm": 10.550905227661133,
      "learning_rate": 2.248109388129249e-05,
      "loss": 1.6003,
      "step": 288200
    },
    {
      "epoch": 22.022763730807426,
      "grad_norm": 7.800683498382568,
      "learning_rate": 2.247154533649072e-05,
      "loss": 1.799,
      "step": 288300
    },
    {
      "epoch": 22.030402566648842,
      "grad_norm": 4.879334449768066,
      "learning_rate": 2.2461996791688948e-05,
      "loss": 1.8221,
      "step": 288400
    },
    {
      "epoch": 22.03804140249026,
      "grad_norm": 9.057178497314453,
      "learning_rate": 2.2452448246887177e-05,
      "loss": 1.7372,
      "step": 288500
    },
    {
      "epoch": 22.045680238331677,
      "grad_norm": 8.683208465576172,
      "learning_rate": 2.2442899702085405e-05,
      "loss": 1.7715,
      "step": 288600
    },
    {
      "epoch": 22.053319074173096,
      "grad_norm": 7.828526973724365,
      "learning_rate": 2.243335115728363e-05,
      "loss": 1.7166,
      "step": 288700
    },
    {
      "epoch": 22.060957910014515,
      "grad_norm": 7.6651997566223145,
      "learning_rate": 2.242380261248186e-05,
      "loss": 1.8375,
      "step": 288800
    },
    {
      "epoch": 22.06859674585593,
      "grad_norm": 7.080087661743164,
      "learning_rate": 2.2414254067680088e-05,
      "loss": 1.7181,
      "step": 288900
    },
    {
      "epoch": 22.07623558169735,
      "grad_norm": 6.844228744506836,
      "learning_rate": 2.2404705522878313e-05,
      "loss": 1.7945,
      "step": 289000
    },
    {
      "epoch": 22.083874417538766,
      "grad_norm": 6.067282676696777,
      "learning_rate": 2.2395156978076542e-05,
      "loss": 1.6588,
      "step": 289100
    },
    {
      "epoch": 22.091513253380185,
      "grad_norm": 6.600586891174316,
      "learning_rate": 2.238560843327477e-05,
      "loss": 1.7334,
      "step": 289200
    },
    {
      "epoch": 22.0991520892216,
      "grad_norm": 8.525466918945312,
      "learning_rate": 2.2376059888472996e-05,
      "loss": 1.7011,
      "step": 289300
    },
    {
      "epoch": 22.10679092506302,
      "grad_norm": 6.387156963348389,
      "learning_rate": 2.2366511343671225e-05,
      "loss": 1.5936,
      "step": 289400
    },
    {
      "epoch": 22.11442976090444,
      "grad_norm": 6.564311981201172,
      "learning_rate": 2.2356962798869453e-05,
      "loss": 1.7162,
      "step": 289500
    },
    {
      "epoch": 22.122068596745855,
      "grad_norm": 8.997937202453613,
      "learning_rate": 2.2347414254067682e-05,
      "loss": 1.7595,
      "step": 289600
    },
    {
      "epoch": 22.129707432587274,
      "grad_norm": 6.12668514251709,
      "learning_rate": 2.233786570926591e-05,
      "loss": 1.7566,
      "step": 289700
    },
    {
      "epoch": 22.13734626842869,
      "grad_norm": 8.973907470703125,
      "learning_rate": 2.2328317164464136e-05,
      "loss": 1.7582,
      "step": 289800
    },
    {
      "epoch": 22.14498510427011,
      "grad_norm": 7.281890869140625,
      "learning_rate": 2.2318768619662365e-05,
      "loss": 1.6648,
      "step": 289900
    },
    {
      "epoch": 22.15262394011153,
      "grad_norm": 7.4963555335998535,
      "learning_rate": 2.2309220074860593e-05,
      "loss": 1.7187,
      "step": 290000
    },
    {
      "epoch": 22.160262775952944,
      "grad_norm": 7.345983505249023,
      "learning_rate": 2.229967153005882e-05,
      "loss": 1.7002,
      "step": 290100
    },
    {
      "epoch": 22.167901611794363,
      "grad_norm": 7.077147483825684,
      "learning_rate": 2.2290122985257047e-05,
      "loss": 1.8483,
      "step": 290200
    },
    {
      "epoch": 22.17554044763578,
      "grad_norm": 7.886364459991455,
      "learning_rate": 2.2280574440455276e-05,
      "loss": 1.7529,
      "step": 290300
    },
    {
      "epoch": 22.183179283477198,
      "grad_norm": 6.934462070465088,
      "learning_rate": 2.22710258956535e-05,
      "loss": 1.7811,
      "step": 290400
    },
    {
      "epoch": 22.190818119318617,
      "grad_norm": 7.934096336364746,
      "learning_rate": 2.226147735085173e-05,
      "loss": 1.8541,
      "step": 290500
    },
    {
      "epoch": 22.198456955160033,
      "grad_norm": 8.527104377746582,
      "learning_rate": 2.225192880604996e-05,
      "loss": 1.6654,
      "step": 290600
    },
    {
      "epoch": 22.206095791001452,
      "grad_norm": 8.101698875427246,
      "learning_rate": 2.2242380261248184e-05,
      "loss": 1.824,
      "step": 290700
    },
    {
      "epoch": 22.213734626842868,
      "grad_norm": 7.8479743003845215,
      "learning_rate": 2.2232831716446413e-05,
      "loss": 1.7649,
      "step": 290800
    },
    {
      "epoch": 22.221373462684287,
      "grad_norm": 8.191949844360352,
      "learning_rate": 2.222328317164464e-05,
      "loss": 1.7723,
      "step": 290900
    },
    {
      "epoch": 22.229012298525706,
      "grad_norm": 6.164712905883789,
      "learning_rate": 2.221373462684287e-05,
      "loss": 1.7136,
      "step": 291000
    },
    {
      "epoch": 22.236651134367122,
      "grad_norm": 7.610290050506592,
      "learning_rate": 2.22041860820411e-05,
      "loss": 1.7292,
      "step": 291100
    },
    {
      "epoch": 22.24428997020854,
      "grad_norm": 8.54640007019043,
      "learning_rate": 2.2194637537239328e-05,
      "loss": 1.7714,
      "step": 291200
    },
    {
      "epoch": 22.251928806049957,
      "grad_norm": 6.442291736602783,
      "learning_rate": 2.2185088992437553e-05,
      "loss": 1.773,
      "step": 291300
    },
    {
      "epoch": 22.259567641891376,
      "grad_norm": 9.496057510375977,
      "learning_rate": 2.217554044763578e-05,
      "loss": 1.7255,
      "step": 291400
    },
    {
      "epoch": 22.267206477732792,
      "grad_norm": 10.349329948425293,
      "learning_rate": 2.216599190283401e-05,
      "loss": 1.7019,
      "step": 291500
    },
    {
      "epoch": 22.27484531357421,
      "grad_norm": 7.981611251831055,
      "learning_rate": 2.2156443358032236e-05,
      "loss": 1.813,
      "step": 291600
    },
    {
      "epoch": 22.28248414941563,
      "grad_norm": 6.384208679199219,
      "learning_rate": 2.2146894813230464e-05,
      "loss": 1.6836,
      "step": 291700
    },
    {
      "epoch": 22.290122985257046,
      "grad_norm": 7.025900363922119,
      "learning_rate": 2.2137346268428693e-05,
      "loss": 1.7203,
      "step": 291800
    },
    {
      "epoch": 22.297761821098465,
      "grad_norm": 6.278139114379883,
      "learning_rate": 2.212779772362692e-05,
      "loss": 1.8035,
      "step": 291900
    },
    {
      "epoch": 22.30540065693988,
      "grad_norm": 7.923183441162109,
      "learning_rate": 2.2118249178825147e-05,
      "loss": 1.7345,
      "step": 292000
    },
    {
      "epoch": 22.3130394927813,
      "grad_norm": 7.016764163970947,
      "learning_rate": 2.2108700634023376e-05,
      "loss": 1.7161,
      "step": 292100
    },
    {
      "epoch": 22.32067832862272,
      "grad_norm": 7.853909492492676,
      "learning_rate": 2.2099152089221604e-05,
      "loss": 1.7971,
      "step": 292200
    },
    {
      "epoch": 22.328317164464135,
      "grad_norm": 6.98914098739624,
      "learning_rate": 2.2089603544419833e-05,
      "loss": 1.8343,
      "step": 292300
    },
    {
      "epoch": 22.335956000305554,
      "grad_norm": 8.116922378540039,
      "learning_rate": 2.2080054999618062e-05,
      "loss": 1.8111,
      "step": 292400
    },
    {
      "epoch": 22.34359483614697,
      "grad_norm": 8.95726203918457,
      "learning_rate": 2.2070506454816287e-05,
      "loss": 1.7821,
      "step": 292500
    },
    {
      "epoch": 22.35123367198839,
      "grad_norm": 7.884583950042725,
      "learning_rate": 2.2060957910014516e-05,
      "loss": 1.7504,
      "step": 292600
    },
    {
      "epoch": 22.35887250782981,
      "grad_norm": 6.951511383056641,
      "learning_rate": 2.205140936521274e-05,
      "loss": 1.8486,
      "step": 292700
    },
    {
      "epoch": 22.366511343671224,
      "grad_norm": 5.150637149810791,
      "learning_rate": 2.204186082041097e-05,
      "loss": 1.7252,
      "step": 292800
    },
    {
      "epoch": 22.374150179512643,
      "grad_norm": 8.327916145324707,
      "learning_rate": 2.20323122756092e-05,
      "loss": 1.7777,
      "step": 292900
    },
    {
      "epoch": 22.38178901535406,
      "grad_norm": 7.467551231384277,
      "learning_rate": 2.2022763730807424e-05,
      "loss": 1.7731,
      "step": 293000
    },
    {
      "epoch": 22.389427851195478,
      "grad_norm": 6.163398742675781,
      "learning_rate": 2.2013215186005652e-05,
      "loss": 1.7081,
      "step": 293100
    },
    {
      "epoch": 22.397066687036897,
      "grad_norm": 8.299909591674805,
      "learning_rate": 2.200366664120388e-05,
      "loss": 1.7403,
      "step": 293200
    },
    {
      "epoch": 22.404705522878313,
      "grad_norm": 5.4268059730529785,
      "learning_rate": 2.199411809640211e-05,
      "loss": 1.7858,
      "step": 293300
    },
    {
      "epoch": 22.412344358719732,
      "grad_norm": 6.508782386779785,
      "learning_rate": 2.198456955160034e-05,
      "loss": 1.7836,
      "step": 293400
    },
    {
      "epoch": 22.419983194561148,
      "grad_norm": 7.203361511230469,
      "learning_rate": 2.1975021006798567e-05,
      "loss": 1.7536,
      "step": 293500
    },
    {
      "epoch": 22.427622030402567,
      "grad_norm": 8.175515174865723,
      "learning_rate": 2.1965472461996793e-05,
      "loss": 1.7521,
      "step": 293600
    },
    {
      "epoch": 22.435260866243983,
      "grad_norm": 7.038247585296631,
      "learning_rate": 2.195592391719502e-05,
      "loss": 1.8078,
      "step": 293700
    },
    {
      "epoch": 22.442899702085402,
      "grad_norm": 7.569565296173096,
      "learning_rate": 2.194637537239325e-05,
      "loss": 1.6973,
      "step": 293800
    },
    {
      "epoch": 22.45053853792682,
      "grad_norm": 7.005307674407959,
      "learning_rate": 2.1936826827591475e-05,
      "loss": 1.7955,
      "step": 293900
    },
    {
      "epoch": 22.458177373768237,
      "grad_norm": 6.61737585067749,
      "learning_rate": 2.1927278282789704e-05,
      "loss": 1.8274,
      "step": 294000
    },
    {
      "epoch": 22.465816209609656,
      "grad_norm": 8.16992473602295,
      "learning_rate": 2.1917729737987933e-05,
      "loss": 1.8714,
      "step": 294100
    },
    {
      "epoch": 22.47345504545107,
      "grad_norm": 8.045730590820312,
      "learning_rate": 2.1908181193186158e-05,
      "loss": 1.8093,
      "step": 294200
    },
    {
      "epoch": 22.48109388129249,
      "grad_norm": 7.166478633880615,
      "learning_rate": 2.1898632648384387e-05,
      "loss": 1.8011,
      "step": 294300
    },
    {
      "epoch": 22.48873271713391,
      "grad_norm": 6.363164901733398,
      "learning_rate": 2.1889084103582615e-05,
      "loss": 1.8516,
      "step": 294400
    },
    {
      "epoch": 22.496371552975326,
      "grad_norm": 7.116775989532471,
      "learning_rate": 2.187953555878084e-05,
      "loss": 1.6406,
      "step": 294500
    },
    {
      "epoch": 22.504010388816745,
      "grad_norm": 6.007747650146484,
      "learning_rate": 2.186998701397907e-05,
      "loss": 1.7495,
      "step": 294600
    },
    {
      "epoch": 22.51164922465816,
      "grad_norm": 6.738962650299072,
      "learning_rate": 2.1860438469177298e-05,
      "loss": 1.7571,
      "step": 294700
    },
    {
      "epoch": 22.51928806049958,
      "grad_norm": 6.709102630615234,
      "learning_rate": 2.1850889924375527e-05,
      "loss": 1.7345,
      "step": 294800
    },
    {
      "epoch": 22.526926896341,
      "grad_norm": 6.9742326736450195,
      "learning_rate": 2.1841341379573755e-05,
      "loss": 1.7334,
      "step": 294900
    },
    {
      "epoch": 22.534565732182415,
      "grad_norm": 8.127815246582031,
      "learning_rate": 2.1831792834771984e-05,
      "loss": 1.8026,
      "step": 295000
    },
    {
      "epoch": 22.542204568023834,
      "grad_norm": 10.035994529724121,
      "learning_rate": 2.182224428997021e-05,
      "loss": 1.7874,
      "step": 295100
    },
    {
      "epoch": 22.54984340386525,
      "grad_norm": 7.822000503540039,
      "learning_rate": 2.1812695745168438e-05,
      "loss": 1.7655,
      "step": 295200
    },
    {
      "epoch": 22.55748223970667,
      "grad_norm": 7.731709003448486,
      "learning_rate": 2.1803147200366667e-05,
      "loss": 1.7515,
      "step": 295300
    },
    {
      "epoch": 22.565121075548085,
      "grad_norm": 10.028853416442871,
      "learning_rate": 2.1793598655564892e-05,
      "loss": 1.6853,
      "step": 295400
    },
    {
      "epoch": 22.572759911389504,
      "grad_norm": 6.077056407928467,
      "learning_rate": 2.178405011076312e-05,
      "loss": 1.7553,
      "step": 295500
    },
    {
      "epoch": 22.580398747230923,
      "grad_norm": 7.834423065185547,
      "learning_rate": 2.1774501565961346e-05,
      "loss": 1.7562,
      "step": 295600
    },
    {
      "epoch": 22.58803758307234,
      "grad_norm": 5.967681407928467,
      "learning_rate": 2.1764953021159575e-05,
      "loss": 1.7633,
      "step": 295700
    },
    {
      "epoch": 22.595676418913758,
      "grad_norm": 9.65639591217041,
      "learning_rate": 2.1755404476357803e-05,
      "loss": 1.7645,
      "step": 295800
    },
    {
      "epoch": 22.603315254755174,
      "grad_norm": 6.360071182250977,
      "learning_rate": 2.1745855931556032e-05,
      "loss": 1.7454,
      "step": 295900
    },
    {
      "epoch": 22.610954090596593,
      "grad_norm": 8.614202499389648,
      "learning_rate": 2.173630738675426e-05,
      "loss": 1.86,
      "step": 296000
    },
    {
      "epoch": 22.618592926438012,
      "grad_norm": 10.035598754882812,
      "learning_rate": 2.172675884195249e-05,
      "loss": 1.82,
      "step": 296100
    },
    {
      "epoch": 22.626231762279428,
      "grad_norm": 6.854973316192627,
      "learning_rate": 2.1717210297150715e-05,
      "loss": 1.7891,
      "step": 296200
    },
    {
      "epoch": 22.633870598120847,
      "grad_norm": 6.822646141052246,
      "learning_rate": 2.1707661752348944e-05,
      "loss": 1.7158,
      "step": 296300
    },
    {
      "epoch": 22.641509433962263,
      "grad_norm": 6.936903953552246,
      "learning_rate": 2.1698113207547172e-05,
      "loss": 1.7656,
      "step": 296400
    },
    {
      "epoch": 22.649148269803682,
      "grad_norm": 7.692327499389648,
      "learning_rate": 2.1688564662745398e-05,
      "loss": 1.7904,
      "step": 296500
    },
    {
      "epoch": 22.6567871056451,
      "grad_norm": 7.447389125823975,
      "learning_rate": 2.1679016117943626e-05,
      "loss": 1.8596,
      "step": 296600
    },
    {
      "epoch": 22.664425941486517,
      "grad_norm": 5.489579677581787,
      "learning_rate": 2.1669467573141855e-05,
      "loss": 1.689,
      "step": 296700
    },
    {
      "epoch": 22.672064777327936,
      "grad_norm": 10.967538833618164,
      "learning_rate": 2.165991902834008e-05,
      "loss": 1.8073,
      "step": 296800
    },
    {
      "epoch": 22.67970361316935,
      "grad_norm": 5.314170837402344,
      "learning_rate": 2.165037048353831e-05,
      "loss": 1.8344,
      "step": 296900
    },
    {
      "epoch": 22.68734244901077,
      "grad_norm": 10.319432258605957,
      "learning_rate": 2.1640821938736538e-05,
      "loss": 1.7644,
      "step": 297000
    },
    {
      "epoch": 22.69498128485219,
      "grad_norm": 6.671469688415527,
      "learning_rate": 2.1631273393934763e-05,
      "loss": 1.6894,
      "step": 297100
    },
    {
      "epoch": 22.702620120693606,
      "grad_norm": 6.235836982727051,
      "learning_rate": 2.162172484913299e-05,
      "loss": 1.8394,
      "step": 297200
    },
    {
      "epoch": 22.710258956535025,
      "grad_norm": 7.801389694213867,
      "learning_rate": 2.161217630433122e-05,
      "loss": 1.7676,
      "step": 297300
    },
    {
      "epoch": 22.71789779237644,
      "grad_norm": 9.729578018188477,
      "learning_rate": 2.160262775952945e-05,
      "loss": 1.8072,
      "step": 297400
    },
    {
      "epoch": 22.72553662821786,
      "grad_norm": 7.060674667358398,
      "learning_rate": 2.1593079214727678e-05,
      "loss": 1.7182,
      "step": 297500
    },
    {
      "epoch": 22.73317546405928,
      "grad_norm": 9.96513557434082,
      "learning_rate": 2.1583530669925906e-05,
      "loss": 1.8002,
      "step": 297600
    },
    {
      "epoch": 22.740814299900695,
      "grad_norm": 5.940311431884766,
      "learning_rate": 2.1573982125124132e-05,
      "loss": 1.7626,
      "step": 297700
    },
    {
      "epoch": 22.748453135742114,
      "grad_norm": 10.517505645751953,
      "learning_rate": 2.156443358032236e-05,
      "loss": 1.7452,
      "step": 297800
    },
    {
      "epoch": 22.75609197158353,
      "grad_norm": 9.282072067260742,
      "learning_rate": 2.155488503552059e-05,
      "loss": 1.7471,
      "step": 297900
    },
    {
      "epoch": 22.76373080742495,
      "grad_norm": 9.086833000183105,
      "learning_rate": 2.1545336490718814e-05,
      "loss": 1.8066,
      "step": 298000
    },
    {
      "epoch": 22.771369643266365,
      "grad_norm": 8.59422779083252,
      "learning_rate": 2.1535787945917043e-05,
      "loss": 1.7145,
      "step": 298100
    },
    {
      "epoch": 22.779008479107784,
      "grad_norm": 7.519413948059082,
      "learning_rate": 2.1526239401115272e-05,
      "loss": 1.7917,
      "step": 298200
    },
    {
      "epoch": 22.786647314949203,
      "grad_norm": 7.5261125564575195,
      "learning_rate": 2.1516690856313497e-05,
      "loss": 1.7593,
      "step": 298300
    },
    {
      "epoch": 22.79428615079062,
      "grad_norm": 5.66596794128418,
      "learning_rate": 2.1507142311511726e-05,
      "loss": 1.7097,
      "step": 298400
    },
    {
      "epoch": 22.801924986632038,
      "grad_norm": 6.788210391998291,
      "learning_rate": 2.1497593766709954e-05,
      "loss": 1.7685,
      "step": 298500
    },
    {
      "epoch": 22.809563822473454,
      "grad_norm": 6.625046730041504,
      "learning_rate": 2.1488045221908183e-05,
      "loss": 1.7198,
      "step": 298600
    },
    {
      "epoch": 22.817202658314873,
      "grad_norm": 9.13627815246582,
      "learning_rate": 2.1478496677106412e-05,
      "loss": 1.6985,
      "step": 298700
    },
    {
      "epoch": 22.824841494156292,
      "grad_norm": 5.484628200531006,
      "learning_rate": 2.1468948132304637e-05,
      "loss": 1.8245,
      "step": 298800
    },
    {
      "epoch": 22.832480329997708,
      "grad_norm": 6.035617828369141,
      "learning_rate": 2.1459399587502866e-05,
      "loss": 1.7907,
      "step": 298900
    },
    {
      "epoch": 22.840119165839127,
      "grad_norm": 5.5082292556762695,
      "learning_rate": 2.1449851042701095e-05,
      "loss": 1.755,
      "step": 299000
    },
    {
      "epoch": 22.847758001680543,
      "grad_norm": 5.836551189422607,
      "learning_rate": 2.144030249789932e-05,
      "loss": 1.7758,
      "step": 299100
    },
    {
      "epoch": 22.855396837521962,
      "grad_norm": 7.461276054382324,
      "learning_rate": 2.143075395309755e-05,
      "loss": 1.7196,
      "step": 299200
    },
    {
      "epoch": 22.86303567336338,
      "grad_norm": 8.61085033416748,
      "learning_rate": 2.1421205408295777e-05,
      "loss": 1.7432,
      "step": 299300
    },
    {
      "epoch": 22.870674509204797,
      "grad_norm": 9.933581352233887,
      "learning_rate": 2.1411656863494003e-05,
      "loss": 1.7627,
      "step": 299400
    },
    {
      "epoch": 22.878313345046216,
      "grad_norm": 8.5602388381958,
      "learning_rate": 2.140210831869223e-05,
      "loss": 1.7751,
      "step": 299500
    },
    {
      "epoch": 22.88595218088763,
      "grad_norm": 6.662770748138428,
      "learning_rate": 2.139255977389046e-05,
      "loss": 1.7548,
      "step": 299600
    },
    {
      "epoch": 22.89359101672905,
      "grad_norm": 6.316424369812012,
      "learning_rate": 2.1383011229088685e-05,
      "loss": 1.7442,
      "step": 299700
    },
    {
      "epoch": 22.901229852570467,
      "grad_norm": 7.856546401977539,
      "learning_rate": 2.1373462684286917e-05,
      "loss": 1.7919,
      "step": 299800
    },
    {
      "epoch": 22.908868688411886,
      "grad_norm": 8.483757019042969,
      "learning_rate": 2.1363914139485146e-05,
      "loss": 1.6597,
      "step": 299900
    },
    {
      "epoch": 22.916507524253305,
      "grad_norm": 9.342902183532715,
      "learning_rate": 2.135436559468337e-05,
      "loss": 1.8319,
      "step": 300000
    },
    {
      "epoch": 22.92414636009472,
      "grad_norm": 7.801899433135986,
      "learning_rate": 2.13448170498816e-05,
      "loss": 1.815,
      "step": 300100
    },
    {
      "epoch": 22.93178519593614,
      "grad_norm": 7.835451602935791,
      "learning_rate": 2.133526850507983e-05,
      "loss": 1.7808,
      "step": 300200
    },
    {
      "epoch": 22.939424031777556,
      "grad_norm": 6.718344211578369,
      "learning_rate": 2.1325719960278054e-05,
      "loss": 1.8464,
      "step": 300300
    },
    {
      "epoch": 22.947062867618975,
      "grad_norm": 7.130067825317383,
      "learning_rate": 2.1316171415476283e-05,
      "loss": 1.6976,
      "step": 300400
    },
    {
      "epoch": 22.954701703460394,
      "grad_norm": 9.59188461303711,
      "learning_rate": 2.130662287067451e-05,
      "loss": 1.7711,
      "step": 300500
    },
    {
      "epoch": 22.96234053930181,
      "grad_norm": 7.505475997924805,
      "learning_rate": 2.1297074325872737e-05,
      "loss": 1.8652,
      "step": 300600
    },
    {
      "epoch": 22.96997937514323,
      "grad_norm": 10.537636756896973,
      "learning_rate": 2.1287525781070965e-05,
      "loss": 1.8479,
      "step": 300700
    },
    {
      "epoch": 22.977618210984645,
      "grad_norm": 8.116134643554688,
      "learning_rate": 2.1277977236269194e-05,
      "loss": 1.783,
      "step": 300800
    },
    {
      "epoch": 22.985257046826064,
      "grad_norm": 6.95487117767334,
      "learning_rate": 2.126842869146742e-05,
      "loss": 1.8887,
      "step": 300900
    },
    {
      "epoch": 22.992895882667483,
      "grad_norm": 8.173431396484375,
      "learning_rate": 2.1258880146665648e-05,
      "loss": 1.7758,
      "step": 301000
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.807942509651184,
      "eval_runtime": 2.969,
      "eval_samples_per_second": 232.402,
      "eval_steps_per_second": 232.402,
      "step": 301093
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.5275275707244873,
      "eval_runtime": 56.2275,
      "eval_samples_per_second": 232.822,
      "eval_steps_per_second": 232.822,
      "step": 301093
    },
    {
      "epoch": 23.0005347185089,
      "grad_norm": 8.422428131103516,
      "learning_rate": 2.1249331601863877e-05,
      "loss": 1.7588,
      "step": 301100
    },
    {
      "epoch": 23.008173554350318,
      "grad_norm": 7.366790771484375,
      "learning_rate": 2.1239783057062105e-05,
      "loss": 1.7135,
      "step": 301200
    },
    {
      "epoch": 23.015812390191734,
      "grad_norm": 7.494377613067627,
      "learning_rate": 2.1230234512260334e-05,
      "loss": 1.8158,
      "step": 301300
    },
    {
      "epoch": 23.023451226033153,
      "grad_norm": 7.744711875915527,
      "learning_rate": 2.122068596745856e-05,
      "loss": 1.7188,
      "step": 301400
    },
    {
      "epoch": 23.031090061874572,
      "grad_norm": 8.195359230041504,
      "learning_rate": 2.1211137422656788e-05,
      "loss": 1.7108,
      "step": 301500
    },
    {
      "epoch": 23.038728897715988,
      "grad_norm": 8.940998077392578,
      "learning_rate": 2.1201588877855017e-05,
      "loss": 1.7805,
      "step": 301600
    },
    {
      "epoch": 23.046367733557407,
      "grad_norm": 6.821164608001709,
      "learning_rate": 2.1192040333053242e-05,
      "loss": 1.8428,
      "step": 301700
    },
    {
      "epoch": 23.054006569398823,
      "grad_norm": 8.644043922424316,
      "learning_rate": 2.118249178825147e-05,
      "loss": 1.705,
      "step": 301800
    },
    {
      "epoch": 23.06164540524024,
      "grad_norm": 6.253509044647217,
      "learning_rate": 2.11729432434497e-05,
      "loss": 1.7112,
      "step": 301900
    },
    {
      "epoch": 23.069284241081657,
      "grad_norm": 5.483264446258545,
      "learning_rate": 2.1163394698647925e-05,
      "loss": 1.7684,
      "step": 302000
    },
    {
      "epoch": 23.076923076923077,
      "grad_norm": 7.509031772613525,
      "learning_rate": 2.1153846153846154e-05,
      "loss": 1.7833,
      "step": 302100
    },
    {
      "epoch": 23.084561912764496,
      "grad_norm": 5.3209943771362305,
      "learning_rate": 2.1144297609044382e-05,
      "loss": 1.6427,
      "step": 302200
    },
    {
      "epoch": 23.09220074860591,
      "grad_norm": 6.563165664672852,
      "learning_rate": 2.113474906424261e-05,
      "loss": 1.8388,
      "step": 302300
    },
    {
      "epoch": 23.09983958444733,
      "grad_norm": 13.887690544128418,
      "learning_rate": 2.112520051944084e-05,
      "loss": 1.7993,
      "step": 302400
    },
    {
      "epoch": 23.107478420288746,
      "grad_norm": 5.287382125854492,
      "learning_rate": 2.111565197463907e-05,
      "loss": 1.7337,
      "step": 302500
    },
    {
      "epoch": 23.115117256130166,
      "grad_norm": 7.765808582305908,
      "learning_rate": 2.1106103429837294e-05,
      "loss": 1.8567,
      "step": 302600
    },
    {
      "epoch": 23.122756091971585,
      "grad_norm": 6.316300392150879,
      "learning_rate": 2.1096554885035522e-05,
      "loss": 1.7762,
      "step": 302700
    },
    {
      "epoch": 23.130394927813,
      "grad_norm": 7.046214580535889,
      "learning_rate": 2.108700634023375e-05,
      "loss": 1.7883,
      "step": 302800
    },
    {
      "epoch": 23.13803376365442,
      "grad_norm": 8.615602493286133,
      "learning_rate": 2.1077457795431976e-05,
      "loss": 1.7906,
      "step": 302900
    },
    {
      "epoch": 23.145672599495835,
      "grad_norm": 6.828227996826172,
      "learning_rate": 2.1067909250630205e-05,
      "loss": 1.6354,
      "step": 303000
    },
    {
      "epoch": 23.153311435337255,
      "grad_norm": 8.608834266662598,
      "learning_rate": 2.1058360705828434e-05,
      "loss": 1.7422,
      "step": 303100
    },
    {
      "epoch": 23.160950271178674,
      "grad_norm": 7.617129325866699,
      "learning_rate": 2.104881216102666e-05,
      "loss": 1.7494,
      "step": 303200
    },
    {
      "epoch": 23.16858910702009,
      "grad_norm": 9.285796165466309,
      "learning_rate": 2.1039263616224888e-05,
      "loss": 1.7149,
      "step": 303300
    },
    {
      "epoch": 23.17622794286151,
      "grad_norm": 7.1173553466796875,
      "learning_rate": 2.1029715071423116e-05,
      "loss": 1.7761,
      "step": 303400
    },
    {
      "epoch": 23.183866778702924,
      "grad_norm": 12.04659652709961,
      "learning_rate": 2.1020166526621342e-05,
      "loss": 1.8139,
      "step": 303500
    },
    {
      "epoch": 23.191505614544344,
      "grad_norm": 6.812756061553955,
      "learning_rate": 2.101061798181957e-05,
      "loss": 1.6813,
      "step": 303600
    },
    {
      "epoch": 23.199144450385763,
      "grad_norm": 6.490686416625977,
      "learning_rate": 2.10010694370178e-05,
      "loss": 1.7238,
      "step": 303700
    },
    {
      "epoch": 23.20678328622718,
      "grad_norm": 7.107298374176025,
      "learning_rate": 2.0991520892216028e-05,
      "loss": 1.7428,
      "step": 303800
    },
    {
      "epoch": 23.214422122068598,
      "grad_norm": 7.410256862640381,
      "learning_rate": 2.0981972347414256e-05,
      "loss": 1.7284,
      "step": 303900
    },
    {
      "epoch": 23.222060957910013,
      "grad_norm": 7.429452419281006,
      "learning_rate": 2.0972423802612485e-05,
      "loss": 1.7769,
      "step": 304000
    },
    {
      "epoch": 23.229699793751433,
      "grad_norm": 6.589245319366455,
      "learning_rate": 2.096287525781071e-05,
      "loss": 1.7241,
      "step": 304100
    },
    {
      "epoch": 23.23733862959285,
      "grad_norm": 7.588831901550293,
      "learning_rate": 2.095332671300894e-05,
      "loss": 1.6393,
      "step": 304200
    },
    {
      "epoch": 23.244977465434268,
      "grad_norm": 5.7440338134765625,
      "learning_rate": 2.0943778168207164e-05,
      "loss": 1.83,
      "step": 304300
    },
    {
      "epoch": 23.252616301275687,
      "grad_norm": 7.099207401275635,
      "learning_rate": 2.0934229623405393e-05,
      "loss": 1.747,
      "step": 304400
    },
    {
      "epoch": 23.260255137117102,
      "grad_norm": 9.937212944030762,
      "learning_rate": 2.0924681078603622e-05,
      "loss": 1.769,
      "step": 304500
    },
    {
      "epoch": 23.26789397295852,
      "grad_norm": 9.317076683044434,
      "learning_rate": 2.0915132533801847e-05,
      "loss": 1.8145,
      "step": 304600
    },
    {
      "epoch": 23.275532808799937,
      "grad_norm": 6.044997692108154,
      "learning_rate": 2.0905583989000076e-05,
      "loss": 1.7353,
      "step": 304700
    },
    {
      "epoch": 23.283171644641357,
      "grad_norm": 4.500545501708984,
      "learning_rate": 2.0896035444198305e-05,
      "loss": 1.7152,
      "step": 304800
    },
    {
      "epoch": 23.290810480482776,
      "grad_norm": 9.26644515991211,
      "learning_rate": 2.0886486899396533e-05,
      "loss": 1.6316,
      "step": 304900
    },
    {
      "epoch": 23.29844931632419,
      "grad_norm": 6.664829730987549,
      "learning_rate": 2.0876938354594762e-05,
      "loss": 1.7793,
      "step": 305000
    },
    {
      "epoch": 23.30608815216561,
      "grad_norm": 8.684588432312012,
      "learning_rate": 2.086738980979299e-05,
      "loss": 1.7217,
      "step": 305100
    },
    {
      "epoch": 23.313726988007026,
      "grad_norm": 8.474748611450195,
      "learning_rate": 2.0857841264991216e-05,
      "loss": 1.7463,
      "step": 305200
    },
    {
      "epoch": 23.321365823848446,
      "grad_norm": 6.700318336486816,
      "learning_rate": 2.0848292720189445e-05,
      "loss": 1.7284,
      "step": 305300
    },
    {
      "epoch": 23.329004659689865,
      "grad_norm": 6.992745399475098,
      "learning_rate": 2.0838744175387673e-05,
      "loss": 1.813,
      "step": 305400
    },
    {
      "epoch": 23.33664349553128,
      "grad_norm": 7.685094833374023,
      "learning_rate": 2.08291956305859e-05,
      "loss": 1.7653,
      "step": 305500
    },
    {
      "epoch": 23.3442823313727,
      "grad_norm": 7.709230899810791,
      "learning_rate": 2.0819647085784127e-05,
      "loss": 1.7176,
      "step": 305600
    },
    {
      "epoch": 23.351921167214115,
      "grad_norm": 9.359807014465332,
      "learning_rate": 2.0810098540982356e-05,
      "loss": 1.7898,
      "step": 305700
    },
    {
      "epoch": 23.359560003055535,
      "grad_norm": 7.866086483001709,
      "learning_rate": 2.080054999618058e-05,
      "loss": 1.7016,
      "step": 305800
    },
    {
      "epoch": 23.367198838896954,
      "grad_norm": 8.377233505249023,
      "learning_rate": 2.079100145137881e-05,
      "loss": 1.7853,
      "step": 305900
    },
    {
      "epoch": 23.37483767473837,
      "grad_norm": 6.930601119995117,
      "learning_rate": 2.078145290657704e-05,
      "loss": 1.7017,
      "step": 306000
    },
    {
      "epoch": 23.38247651057979,
      "grad_norm": 8.033870697021484,
      "learning_rate": 2.0771904361775264e-05,
      "loss": 1.8362,
      "step": 306100
    },
    {
      "epoch": 23.390115346421204,
      "grad_norm": 6.183837413787842,
      "learning_rate": 2.0762355816973496e-05,
      "loss": 1.712,
      "step": 306200
    },
    {
      "epoch": 23.397754182262624,
      "grad_norm": 12.671236991882324,
      "learning_rate": 2.0752807272171725e-05,
      "loss": 1.8056,
      "step": 306300
    },
    {
      "epoch": 23.40539301810404,
      "grad_norm": 8.047821044921875,
      "learning_rate": 2.074325872736995e-05,
      "loss": 1.7815,
      "step": 306400
    },
    {
      "epoch": 23.41303185394546,
      "grad_norm": 8.102989196777344,
      "learning_rate": 2.073371018256818e-05,
      "loss": 1.7938,
      "step": 306500
    },
    {
      "epoch": 23.420670689786878,
      "grad_norm": 8.952101707458496,
      "learning_rate": 2.0724161637766407e-05,
      "loss": 1.8458,
      "step": 306600
    },
    {
      "epoch": 23.428309525628293,
      "grad_norm": 9.791794776916504,
      "learning_rate": 2.0714613092964633e-05,
      "loss": 1.8203,
      "step": 306700
    },
    {
      "epoch": 23.435948361469713,
      "grad_norm": 7.708705902099609,
      "learning_rate": 2.070506454816286e-05,
      "loss": 1.7733,
      "step": 306800
    },
    {
      "epoch": 23.44358719731113,
      "grad_norm": 10.593588829040527,
      "learning_rate": 2.069551600336109e-05,
      "loss": 1.6579,
      "step": 306900
    },
    {
      "epoch": 23.451226033152548,
      "grad_norm": 7.601651191711426,
      "learning_rate": 2.0685967458559315e-05,
      "loss": 1.7166,
      "step": 307000
    },
    {
      "epoch": 23.458864868993967,
      "grad_norm": 6.473424911499023,
      "learning_rate": 2.0676418913757544e-05,
      "loss": 1.6616,
      "step": 307100
    },
    {
      "epoch": 23.466503704835382,
      "grad_norm": 8.240485191345215,
      "learning_rate": 2.066687036895577e-05,
      "loss": 1.7107,
      "step": 307200
    },
    {
      "epoch": 23.4741425406768,
      "grad_norm": 8.863001823425293,
      "learning_rate": 2.0657321824153998e-05,
      "loss": 1.7264,
      "step": 307300
    },
    {
      "epoch": 23.481781376518217,
      "grad_norm": 8.249733924865723,
      "learning_rate": 2.0647773279352227e-05,
      "loss": 1.8986,
      "step": 307400
    },
    {
      "epoch": 23.489420212359637,
      "grad_norm": 8.867937088012695,
      "learning_rate": 2.0638224734550456e-05,
      "loss": 1.8173,
      "step": 307500
    },
    {
      "epoch": 23.497059048201056,
      "grad_norm": 8.941814422607422,
      "learning_rate": 2.0628676189748684e-05,
      "loss": 1.8288,
      "step": 307600
    },
    {
      "epoch": 23.50469788404247,
      "grad_norm": 8.407513618469238,
      "learning_rate": 2.0619127644946913e-05,
      "loss": 1.7781,
      "step": 307700
    },
    {
      "epoch": 23.51233671988389,
      "grad_norm": 7.864110946655273,
      "learning_rate": 2.0609579100145138e-05,
      "loss": 1.6725,
      "step": 307800
    },
    {
      "epoch": 23.519975555725306,
      "grad_norm": 7.5505571365356445,
      "learning_rate": 2.0600030555343367e-05,
      "loss": 1.714,
      "step": 307900
    },
    {
      "epoch": 23.527614391566726,
      "grad_norm": 6.847011566162109,
      "learning_rate": 2.0590482010541596e-05,
      "loss": 1.7679,
      "step": 308000
    },
    {
      "epoch": 23.53525322740814,
      "grad_norm": 6.886944770812988,
      "learning_rate": 2.058093346573982e-05,
      "loss": 1.8635,
      "step": 308100
    },
    {
      "epoch": 23.54289206324956,
      "grad_norm": 7.144097328186035,
      "learning_rate": 2.057138492093805e-05,
      "loss": 1.6769,
      "step": 308200
    },
    {
      "epoch": 23.55053089909098,
      "grad_norm": 5.4459099769592285,
      "learning_rate": 2.056183637613628e-05,
      "loss": 1.7508,
      "step": 308300
    },
    {
      "epoch": 23.558169734932395,
      "grad_norm": 8.144064903259277,
      "learning_rate": 2.0552287831334504e-05,
      "loss": 1.7987,
      "step": 308400
    },
    {
      "epoch": 23.565808570773815,
      "grad_norm": 6.4175705909729,
      "learning_rate": 2.0542739286532732e-05,
      "loss": 1.771,
      "step": 308500
    },
    {
      "epoch": 23.57344740661523,
      "grad_norm": 8.219498634338379,
      "learning_rate": 2.053319074173096e-05,
      "loss": 1.762,
      "step": 308600
    },
    {
      "epoch": 23.58108624245665,
      "grad_norm": 5.916723728179932,
      "learning_rate": 2.052364219692919e-05,
      "loss": 1.8121,
      "step": 308700
    },
    {
      "epoch": 23.58872507829807,
      "grad_norm": 6.677918434143066,
      "learning_rate": 2.051409365212742e-05,
      "loss": 1.8,
      "step": 308800
    },
    {
      "epoch": 23.596363914139484,
      "grad_norm": 7.759140968322754,
      "learning_rate": 2.0504545107325647e-05,
      "loss": 1.7327,
      "step": 308900
    },
    {
      "epoch": 23.604002749980904,
      "grad_norm": 7.0521721839904785,
      "learning_rate": 2.0494996562523872e-05,
      "loss": 1.8274,
      "step": 309000
    },
    {
      "epoch": 23.61164158582232,
      "grad_norm": 6.953361988067627,
      "learning_rate": 2.04854480177221e-05,
      "loss": 1.7868,
      "step": 309100
    },
    {
      "epoch": 23.61928042166374,
      "grad_norm": 7.768464088439941,
      "learning_rate": 2.047589947292033e-05,
      "loss": 1.7678,
      "step": 309200
    },
    {
      "epoch": 23.626919257505158,
      "grad_norm": 8.30528736114502,
      "learning_rate": 2.0466350928118555e-05,
      "loss": 1.8198,
      "step": 309300
    },
    {
      "epoch": 23.634558093346573,
      "grad_norm": 6.893669128417969,
      "learning_rate": 2.0456802383316784e-05,
      "loss": 1.7788,
      "step": 309400
    },
    {
      "epoch": 23.642196929187993,
      "grad_norm": 7.991361141204834,
      "learning_rate": 2.0447253838515012e-05,
      "loss": 1.7576,
      "step": 309500
    },
    {
      "epoch": 23.649835765029408,
      "grad_norm": 7.34401273727417,
      "learning_rate": 2.0437705293713238e-05,
      "loss": 1.7163,
      "step": 309600
    },
    {
      "epoch": 23.657474600870827,
      "grad_norm": 10.075803756713867,
      "learning_rate": 2.0428156748911466e-05,
      "loss": 1.7532,
      "step": 309700
    },
    {
      "epoch": 23.665113436712247,
      "grad_norm": 7.740286350250244,
      "learning_rate": 2.0418608204109695e-05,
      "loss": 1.7889,
      "step": 309800
    },
    {
      "epoch": 23.672752272553662,
      "grad_norm": 7.0923590660095215,
      "learning_rate": 2.040905965930792e-05,
      "loss": 1.8241,
      "step": 309900
    },
    {
      "epoch": 23.68039110839508,
      "grad_norm": 7.048255443572998,
      "learning_rate": 2.039951111450615e-05,
      "loss": 1.7005,
      "step": 310000
    },
    {
      "epoch": 23.688029944236497,
      "grad_norm": 7.161330223083496,
      "learning_rate": 2.0389962569704378e-05,
      "loss": 1.7486,
      "step": 310100
    },
    {
      "epoch": 23.695668780077916,
      "grad_norm": 7.973998069763184,
      "learning_rate": 2.0380414024902607e-05,
      "loss": 1.7207,
      "step": 310200
    },
    {
      "epoch": 23.703307615919336,
      "grad_norm": 8.710269927978516,
      "learning_rate": 2.0370865480100835e-05,
      "loss": 1.7179,
      "step": 310300
    },
    {
      "epoch": 23.71094645176075,
      "grad_norm": 9.374805450439453,
      "learning_rate": 2.036131693529906e-05,
      "loss": 1.8494,
      "step": 310400
    },
    {
      "epoch": 23.71858528760217,
      "grad_norm": 8.853946685791016,
      "learning_rate": 2.035176839049729e-05,
      "loss": 1.7717,
      "step": 310500
    },
    {
      "epoch": 23.726224123443586,
      "grad_norm": 8.260078430175781,
      "learning_rate": 2.0342219845695518e-05,
      "loss": 1.7419,
      "step": 310600
    },
    {
      "epoch": 23.733862959285005,
      "grad_norm": 7.8638739585876465,
      "learning_rate": 2.0332671300893743e-05,
      "loss": 1.7926,
      "step": 310700
    },
    {
      "epoch": 23.74150179512642,
      "grad_norm": 8.7380952835083,
      "learning_rate": 2.0323122756091972e-05,
      "loss": 1.8668,
      "step": 310800
    },
    {
      "epoch": 23.74914063096784,
      "grad_norm": 9.91012954711914,
      "learning_rate": 2.03135742112902e-05,
      "loss": 1.8066,
      "step": 310900
    },
    {
      "epoch": 23.75677946680926,
      "grad_norm": 6.745604515075684,
      "learning_rate": 2.0304025666488426e-05,
      "loss": 1.7176,
      "step": 311000
    },
    {
      "epoch": 23.764418302650675,
      "grad_norm": 11.048321723937988,
      "learning_rate": 2.0294477121686655e-05,
      "loss": 1.7739,
      "step": 311100
    },
    {
      "epoch": 23.772057138492094,
      "grad_norm": 7.322597026824951,
      "learning_rate": 2.0284928576884883e-05,
      "loss": 1.7355,
      "step": 311200
    },
    {
      "epoch": 23.77969597433351,
      "grad_norm": 6.96715784072876,
      "learning_rate": 2.0275380032083112e-05,
      "loss": 1.7716,
      "step": 311300
    },
    {
      "epoch": 23.78733481017493,
      "grad_norm": 7.659399032592773,
      "learning_rate": 2.026583148728134e-05,
      "loss": 1.7029,
      "step": 311400
    },
    {
      "epoch": 23.79497364601635,
      "grad_norm": 8.31025505065918,
      "learning_rate": 2.025628294247957e-05,
      "loss": 1.6693,
      "step": 311500
    },
    {
      "epoch": 23.802612481857764,
      "grad_norm": 8.454305648803711,
      "learning_rate": 2.0246734397677795e-05,
      "loss": 1.805,
      "step": 311600
    },
    {
      "epoch": 23.810251317699183,
      "grad_norm": 7.165660381317139,
      "learning_rate": 2.0237185852876023e-05,
      "loss": 1.6983,
      "step": 311700
    },
    {
      "epoch": 23.8178901535406,
      "grad_norm": 8.920737266540527,
      "learning_rate": 2.0227637308074252e-05,
      "loss": 1.7263,
      "step": 311800
    },
    {
      "epoch": 23.82552898938202,
      "grad_norm": 7.840099334716797,
      "learning_rate": 2.0218088763272477e-05,
      "loss": 1.7476,
      "step": 311900
    },
    {
      "epoch": 23.833167825223438,
      "grad_norm": 13.451593399047852,
      "learning_rate": 2.0208540218470706e-05,
      "loss": 1.8659,
      "step": 312000
    },
    {
      "epoch": 23.840806661064853,
      "grad_norm": 7.902390480041504,
      "learning_rate": 2.0198991673668935e-05,
      "loss": 1.7762,
      "step": 312100
    },
    {
      "epoch": 23.848445496906272,
      "grad_norm": 6.684814929962158,
      "learning_rate": 2.018944312886716e-05,
      "loss": 1.7583,
      "step": 312200
    },
    {
      "epoch": 23.856084332747688,
      "grad_norm": 7.447088718414307,
      "learning_rate": 2.017989458406539e-05,
      "loss": 1.6944,
      "step": 312300
    },
    {
      "epoch": 23.863723168589107,
      "grad_norm": 7.509500980377197,
      "learning_rate": 2.0170346039263617e-05,
      "loss": 1.7109,
      "step": 312400
    },
    {
      "epoch": 23.871362004430523,
      "grad_norm": 8.311033248901367,
      "learning_rate": 2.0160797494461843e-05,
      "loss": 1.6729,
      "step": 312500
    },
    {
      "epoch": 23.879000840271942,
      "grad_norm": 5.03309965133667,
      "learning_rate": 2.015124894966007e-05,
      "loss": 1.799,
      "step": 312600
    },
    {
      "epoch": 23.88663967611336,
      "grad_norm": 9.863799095153809,
      "learning_rate": 2.0141700404858304e-05,
      "loss": 1.7544,
      "step": 312700
    },
    {
      "epoch": 23.894278511954777,
      "grad_norm": 8.452240943908691,
      "learning_rate": 2.013215186005653e-05,
      "loss": 1.7103,
      "step": 312800
    },
    {
      "epoch": 23.901917347796196,
      "grad_norm": 6.3246612548828125,
      "learning_rate": 2.0122603315254758e-05,
      "loss": 1.772,
      "step": 312900
    },
    {
      "epoch": 23.909556183637612,
      "grad_norm": 7.70436954498291,
      "learning_rate": 2.0113054770452983e-05,
      "loss": 1.7292,
      "step": 313000
    },
    {
      "epoch": 23.91719501947903,
      "grad_norm": 7.167388439178467,
      "learning_rate": 2.010350622565121e-05,
      "loss": 1.6807,
      "step": 313100
    },
    {
      "epoch": 23.92483385532045,
      "grad_norm": 5.6873698234558105,
      "learning_rate": 2.009395768084944e-05,
      "loss": 1.8444,
      "step": 313200
    },
    {
      "epoch": 23.932472691161866,
      "grad_norm": 9.435582160949707,
      "learning_rate": 2.0084409136047666e-05,
      "loss": 1.7633,
      "step": 313300
    },
    {
      "epoch": 23.940111527003285,
      "grad_norm": 7.54159688949585,
      "learning_rate": 2.0074860591245894e-05,
      "loss": 1.7265,
      "step": 313400
    },
    {
      "epoch": 23.9477503628447,
      "grad_norm": 8.650357246398926,
      "learning_rate": 2.0065312046444123e-05,
      "loss": 1.7541,
      "step": 313500
    },
    {
      "epoch": 23.95538919868612,
      "grad_norm": 8.467952728271484,
      "learning_rate": 2.0055763501642348e-05,
      "loss": 1.7694,
      "step": 313600
    },
    {
      "epoch": 23.96302803452754,
      "grad_norm": 6.217966556549072,
      "learning_rate": 2.0046214956840577e-05,
      "loss": 1.7737,
      "step": 313700
    },
    {
      "epoch": 23.970666870368955,
      "grad_norm": 6.776928901672363,
      "learning_rate": 2.0036666412038806e-05,
      "loss": 1.8104,
      "step": 313800
    },
    {
      "epoch": 23.978305706210374,
      "grad_norm": 9.132868766784668,
      "learning_rate": 2.0027117867237034e-05,
      "loss": 1.7814,
      "step": 313900
    },
    {
      "epoch": 23.98594454205179,
      "grad_norm": 6.409921646118164,
      "learning_rate": 2.0017569322435263e-05,
      "loss": 1.7393,
      "step": 314000
    },
    {
      "epoch": 23.99358337789321,
      "grad_norm": 7.402824878692627,
      "learning_rate": 2.0008020777633492e-05,
      "loss": 1.7706,
      "step": 314100
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.807784914970398,
      "eval_runtime": 2.951,
      "eval_samples_per_second": 233.82,
      "eval_steps_per_second": 233.82,
      "step": 314184
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.5234888792037964,
      "eval_runtime": 56.2519,
      "eval_samples_per_second": 232.721,
      "eval_steps_per_second": 232.721,
      "step": 314184
    },
    {
      "epoch": 24.00122221373463,
      "grad_norm": 9.841156005859375,
      "learning_rate": 1.9998472232831717e-05,
      "loss": 1.7818,
      "step": 314200
    },
    {
      "epoch": 24.008861049576044,
      "grad_norm": 9.176459312438965,
      "learning_rate": 1.9988923688029946e-05,
      "loss": 1.7055,
      "step": 314300
    },
    {
      "epoch": 24.016499885417463,
      "grad_norm": 7.379898548126221,
      "learning_rate": 1.9979375143228174e-05,
      "loss": 1.7531,
      "step": 314400
    },
    {
      "epoch": 24.02413872125888,
      "grad_norm": 7.40364408493042,
      "learning_rate": 1.99698265984264e-05,
      "loss": 1.7662,
      "step": 314500
    },
    {
      "epoch": 24.0317775571003,
      "grad_norm": 9.635486602783203,
      "learning_rate": 1.996027805362463e-05,
      "loss": 1.7537,
      "step": 314600
    },
    {
      "epoch": 24.039416392941714,
      "grad_norm": 9.6243896484375,
      "learning_rate": 1.9950729508822857e-05,
      "loss": 1.7288,
      "step": 314700
    },
    {
      "epoch": 24.047055228783133,
      "grad_norm": 7.633730888366699,
      "learning_rate": 1.9941180964021082e-05,
      "loss": 1.7374,
      "step": 314800
    },
    {
      "epoch": 24.054694064624552,
      "grad_norm": 7.321319103240967,
      "learning_rate": 1.993163241921931e-05,
      "loss": 1.7076,
      "step": 314900
    },
    {
      "epoch": 24.062332900465968,
      "grad_norm": 5.508523464202881,
      "learning_rate": 1.992208387441754e-05,
      "loss": 1.6458,
      "step": 315000
    },
    {
      "epoch": 24.069971736307387,
      "grad_norm": 8.439191818237305,
      "learning_rate": 1.991253532961577e-05,
      "loss": 1.8488,
      "step": 315100
    },
    {
      "epoch": 24.077610572148803,
      "grad_norm": 5.49492883682251,
      "learning_rate": 1.9902986784813997e-05,
      "loss": 1.7337,
      "step": 315200
    },
    {
      "epoch": 24.085249407990222,
      "grad_norm": 7.576766014099121,
      "learning_rate": 1.9893438240012226e-05,
      "loss": 1.6312,
      "step": 315300
    },
    {
      "epoch": 24.09288824383164,
      "grad_norm": 7.851743698120117,
      "learning_rate": 1.988388969521045e-05,
      "loss": 1.7412,
      "step": 315400
    },
    {
      "epoch": 24.100527079673057,
      "grad_norm": 6.825649738311768,
      "learning_rate": 1.987434115040868e-05,
      "loss": 1.8123,
      "step": 315500
    },
    {
      "epoch": 24.108165915514476,
      "grad_norm": 7.449955463409424,
      "learning_rate": 1.986479260560691e-05,
      "loss": 1.6696,
      "step": 315600
    },
    {
      "epoch": 24.115804751355892,
      "grad_norm": 9.68055248260498,
      "learning_rate": 1.9855244060805134e-05,
      "loss": 1.7152,
      "step": 315700
    },
    {
      "epoch": 24.12344358719731,
      "grad_norm": 8.416329383850098,
      "learning_rate": 1.9845695516003363e-05,
      "loss": 1.7047,
      "step": 315800
    },
    {
      "epoch": 24.13108242303873,
      "grad_norm": 9.318474769592285,
      "learning_rate": 1.9836146971201588e-05,
      "loss": 1.8813,
      "step": 315900
    },
    {
      "epoch": 24.138721258880146,
      "grad_norm": 8.040684700012207,
      "learning_rate": 1.9826598426399817e-05,
      "loss": 1.7329,
      "step": 316000
    },
    {
      "epoch": 24.146360094721565,
      "grad_norm": 8.588288307189941,
      "learning_rate": 1.9817049881598045e-05,
      "loss": 1.8343,
      "step": 316100
    },
    {
      "epoch": 24.15399893056298,
      "grad_norm": 6.0561370849609375,
      "learning_rate": 1.980750133679627e-05,
      "loss": 1.7536,
      "step": 316200
    },
    {
      "epoch": 24.1616377664044,
      "grad_norm": 7.08419942855835,
      "learning_rate": 1.97979527919945e-05,
      "loss": 1.7764,
      "step": 316300
    },
    {
      "epoch": 24.16927660224582,
      "grad_norm": 6.578727722167969,
      "learning_rate": 1.9788404247192728e-05,
      "loss": 1.8307,
      "step": 316400
    },
    {
      "epoch": 24.176915438087235,
      "grad_norm": 7.772817134857178,
      "learning_rate": 1.9778855702390957e-05,
      "loss": 1.7265,
      "step": 316500
    },
    {
      "epoch": 24.184554273928654,
      "grad_norm": 7.822020053863525,
      "learning_rate": 1.9769307157589185e-05,
      "loss": 1.7782,
      "step": 316600
    },
    {
      "epoch": 24.19219310977007,
      "grad_norm": 6.456700325012207,
      "learning_rate": 1.9759758612787414e-05,
      "loss": 1.7151,
      "step": 316700
    },
    {
      "epoch": 24.19983194561149,
      "grad_norm": 8.83229923248291,
      "learning_rate": 1.975021006798564e-05,
      "loss": 1.7978,
      "step": 316800
    },
    {
      "epoch": 24.207470781452905,
      "grad_norm": 7.13965368270874,
      "learning_rate": 1.9740661523183868e-05,
      "loss": 1.7108,
      "step": 316900
    },
    {
      "epoch": 24.215109617294324,
      "grad_norm": 8.964947700500488,
      "learning_rate": 1.9731112978382097e-05,
      "loss": 1.7799,
      "step": 317000
    },
    {
      "epoch": 24.222748453135743,
      "grad_norm": 7.780929088592529,
      "learning_rate": 1.9721564433580322e-05,
      "loss": 1.6874,
      "step": 317100
    },
    {
      "epoch": 24.23038728897716,
      "grad_norm": 8.600340843200684,
      "learning_rate": 1.971201588877855e-05,
      "loss": 1.7576,
      "step": 317200
    },
    {
      "epoch": 24.238026124818578,
      "grad_norm": 7.494586944580078,
      "learning_rate": 1.970246734397678e-05,
      "loss": 1.69,
      "step": 317300
    },
    {
      "epoch": 24.245664960659994,
      "grad_norm": 7.558692455291748,
      "learning_rate": 1.9692918799175005e-05,
      "loss": 1.7042,
      "step": 317400
    },
    {
      "epoch": 24.253303796501413,
      "grad_norm": 6.660530090332031,
      "learning_rate": 1.9683370254373233e-05,
      "loss": 1.7646,
      "step": 317500
    },
    {
      "epoch": 24.260942632342832,
      "grad_norm": 7.460618019104004,
      "learning_rate": 1.9673821709571462e-05,
      "loss": 1.8093,
      "step": 317600
    },
    {
      "epoch": 24.268581468184248,
      "grad_norm": 7.511472225189209,
      "learning_rate": 1.966427316476969e-05,
      "loss": 1.6544,
      "step": 317700
    },
    {
      "epoch": 24.276220304025667,
      "grad_norm": 10.735740661621094,
      "learning_rate": 1.965472461996792e-05,
      "loss": 1.7498,
      "step": 317800
    },
    {
      "epoch": 24.283859139867083,
      "grad_norm": 6.903317451477051,
      "learning_rate": 1.9645176075166148e-05,
      "loss": 1.7697,
      "step": 317900
    },
    {
      "epoch": 24.291497975708502,
      "grad_norm": 8.566771507263184,
      "learning_rate": 1.9635627530364373e-05,
      "loss": 1.7441,
      "step": 318000
    },
    {
      "epoch": 24.29913681154992,
      "grad_norm": 7.621842384338379,
      "learning_rate": 1.9626078985562602e-05,
      "loss": 1.7796,
      "step": 318100
    },
    {
      "epoch": 24.306775647391337,
      "grad_norm": 7.188255786895752,
      "learning_rate": 1.961653044076083e-05,
      "loss": 1.7548,
      "step": 318200
    },
    {
      "epoch": 24.314414483232756,
      "grad_norm": 9.045233726501465,
      "learning_rate": 1.9606981895959056e-05,
      "loss": 1.725,
      "step": 318300
    },
    {
      "epoch": 24.322053319074172,
      "grad_norm": 6.995550632476807,
      "learning_rate": 1.9597433351157285e-05,
      "loss": 1.8327,
      "step": 318400
    },
    {
      "epoch": 24.32969215491559,
      "grad_norm": 6.911868572235107,
      "learning_rate": 1.9587884806355514e-05,
      "loss": 1.7395,
      "step": 318500
    },
    {
      "epoch": 24.33733099075701,
      "grad_norm": 8.130823135375977,
      "learning_rate": 1.957833626155374e-05,
      "loss": 1.7071,
      "step": 318600
    },
    {
      "epoch": 24.344969826598426,
      "grad_norm": 7.992706775665283,
      "learning_rate": 1.9568787716751968e-05,
      "loss": 1.7863,
      "step": 318700
    },
    {
      "epoch": 24.352608662439845,
      "grad_norm": 8.040630340576172,
      "learning_rate": 1.9559239171950193e-05,
      "loss": 1.7333,
      "step": 318800
    },
    {
      "epoch": 24.36024749828126,
      "grad_norm": 8.53856086730957,
      "learning_rate": 1.954969062714842e-05,
      "loss": 1.7065,
      "step": 318900
    },
    {
      "epoch": 24.36788633412268,
      "grad_norm": 10.112092018127441,
      "learning_rate": 1.954014208234665e-05,
      "loss": 1.7088,
      "step": 319000
    },
    {
      "epoch": 24.375525169964096,
      "grad_norm": 5.550354957580566,
      "learning_rate": 1.953059353754488e-05,
      "loss": 1.7159,
      "step": 319100
    },
    {
      "epoch": 24.383164005805515,
      "grad_norm": 6.846292018890381,
      "learning_rate": 1.9521044992743108e-05,
      "loss": 1.7462,
      "step": 319200
    },
    {
      "epoch": 24.390802841646934,
      "grad_norm": 8.076081275939941,
      "learning_rate": 1.9511496447941336e-05,
      "loss": 1.7533,
      "step": 319300
    },
    {
      "epoch": 24.39844167748835,
      "grad_norm": 9.148497581481934,
      "learning_rate": 1.950194790313956e-05,
      "loss": 1.772,
      "step": 319400
    },
    {
      "epoch": 24.40608051332977,
      "grad_norm": 7.241796970367432,
      "learning_rate": 1.949239935833779e-05,
      "loss": 1.8329,
      "step": 319500
    },
    {
      "epoch": 24.413719349171185,
      "grad_norm": 8.984614372253418,
      "learning_rate": 1.948285081353602e-05,
      "loss": 1.6417,
      "step": 319600
    },
    {
      "epoch": 24.421358185012604,
      "grad_norm": 6.410027503967285,
      "learning_rate": 1.9473302268734244e-05,
      "loss": 1.7987,
      "step": 319700
    },
    {
      "epoch": 24.428997020854023,
      "grad_norm": 7.744045734405518,
      "learning_rate": 1.9463753723932473e-05,
      "loss": 1.7712,
      "step": 319800
    },
    {
      "epoch": 24.43663585669544,
      "grad_norm": 9.160550117492676,
      "learning_rate": 1.9454205179130702e-05,
      "loss": 1.7973,
      "step": 319900
    },
    {
      "epoch": 24.444274692536858,
      "grad_norm": 6.692356109619141,
      "learning_rate": 1.9444656634328927e-05,
      "loss": 1.6513,
      "step": 320000
    },
    {
      "epoch": 24.451913528378274,
      "grad_norm": 7.266176223754883,
      "learning_rate": 1.9435108089527156e-05,
      "loss": 1.7152,
      "step": 320100
    },
    {
      "epoch": 24.459552364219693,
      "grad_norm": 8.197775840759277,
      "learning_rate": 1.9425559544725384e-05,
      "loss": 1.7942,
      "step": 320200
    },
    {
      "epoch": 24.467191200061112,
      "grad_norm": 8.858805656433105,
      "learning_rate": 1.9416010999923613e-05,
      "loss": 1.7565,
      "step": 320300
    },
    {
      "epoch": 24.474830035902528,
      "grad_norm": 6.673120021820068,
      "learning_rate": 1.9406462455121842e-05,
      "loss": 1.7983,
      "step": 320400
    },
    {
      "epoch": 24.482468871743947,
      "grad_norm": 9.331645011901855,
      "learning_rate": 1.939691391032007e-05,
      "loss": 1.76,
      "step": 320500
    },
    {
      "epoch": 24.490107707585363,
      "grad_norm": 9.166784286499023,
      "learning_rate": 1.9387365365518296e-05,
      "loss": 1.7429,
      "step": 320600
    },
    {
      "epoch": 24.497746543426782,
      "grad_norm": 7.647189617156982,
      "learning_rate": 1.9377816820716524e-05,
      "loss": 1.821,
      "step": 320700
    },
    {
      "epoch": 24.505385379268198,
      "grad_norm": 9.550345420837402,
      "learning_rate": 1.9368268275914753e-05,
      "loss": 1.7704,
      "step": 320800
    },
    {
      "epoch": 24.513024215109617,
      "grad_norm": 14.338406562805176,
      "learning_rate": 1.935871973111298e-05,
      "loss": 1.7367,
      "step": 320900
    },
    {
      "epoch": 24.520663050951036,
      "grad_norm": 7.18557071685791,
      "learning_rate": 1.9349171186311207e-05,
      "loss": 1.6944,
      "step": 321000
    },
    {
      "epoch": 24.528301886792452,
      "grad_norm": 8.022649765014648,
      "learning_rate": 1.9339622641509436e-05,
      "loss": 1.7482,
      "step": 321100
    },
    {
      "epoch": 24.53594072263387,
      "grad_norm": 8.299484252929688,
      "learning_rate": 1.933007409670766e-05,
      "loss": 1.7564,
      "step": 321200
    },
    {
      "epoch": 24.543579558475287,
      "grad_norm": 7.315999507904053,
      "learning_rate": 1.932052555190589e-05,
      "loss": 1.6897,
      "step": 321300
    },
    {
      "epoch": 24.551218394316706,
      "grad_norm": 9.306777000427246,
      "learning_rate": 1.931097700710412e-05,
      "loss": 1.8092,
      "step": 321400
    },
    {
      "epoch": 24.558857230158125,
      "grad_norm": 6.1514081954956055,
      "learning_rate": 1.9301428462302347e-05,
      "loss": 1.7914,
      "step": 321500
    },
    {
      "epoch": 24.56649606599954,
      "grad_norm": 7.040826320648193,
      "learning_rate": 1.9291879917500576e-05,
      "loss": 1.7978,
      "step": 321600
    },
    {
      "epoch": 24.57413490184096,
      "grad_norm": 9.41280460357666,
      "learning_rate": 1.92823313726988e-05,
      "loss": 1.7226,
      "step": 321700
    },
    {
      "epoch": 24.581773737682376,
      "grad_norm": 7.729842185974121,
      "learning_rate": 1.927278282789703e-05,
      "loss": 1.7771,
      "step": 321800
    },
    {
      "epoch": 24.589412573523795,
      "grad_norm": 8.416801452636719,
      "learning_rate": 1.926323428309526e-05,
      "loss": 1.83,
      "step": 321900
    },
    {
      "epoch": 24.597051409365214,
      "grad_norm": 5.00540018081665,
      "learning_rate": 1.9253685738293484e-05,
      "loss": 1.7805,
      "step": 322000
    },
    {
      "epoch": 24.60469024520663,
      "grad_norm": 8.338164329528809,
      "learning_rate": 1.9244137193491713e-05,
      "loss": 1.7815,
      "step": 322100
    },
    {
      "epoch": 24.61232908104805,
      "grad_norm": 10.006860733032227,
      "learning_rate": 1.923458864868994e-05,
      "loss": 1.8215,
      "step": 322200
    },
    {
      "epoch": 24.619967916889465,
      "grad_norm": 6.675002574920654,
      "learning_rate": 1.9225040103888167e-05,
      "loss": 1.6601,
      "step": 322300
    },
    {
      "epoch": 24.627606752730884,
      "grad_norm": 12.664966583251953,
      "learning_rate": 1.9215491559086395e-05,
      "loss": 1.8355,
      "step": 322400
    },
    {
      "epoch": 24.635245588572303,
      "grad_norm": 7.060582160949707,
      "learning_rate": 1.9205943014284624e-05,
      "loss": 1.6733,
      "step": 322500
    },
    {
      "epoch": 24.64288442441372,
      "grad_norm": 10.921721458435059,
      "learning_rate": 1.919639446948285e-05,
      "loss": 1.8969,
      "step": 322600
    },
    {
      "epoch": 24.650523260255138,
      "grad_norm": 7.678159713745117,
      "learning_rate": 1.9186845924681078e-05,
      "loss": 1.7468,
      "step": 322700
    },
    {
      "epoch": 24.658162096096554,
      "grad_norm": 8.468422889709473,
      "learning_rate": 1.9177297379879307e-05,
      "loss": 1.7573,
      "step": 322800
    },
    {
      "epoch": 24.665800931937973,
      "grad_norm": 11.9805326461792,
      "learning_rate": 1.9167748835077535e-05,
      "loss": 1.8481,
      "step": 322900
    },
    {
      "epoch": 24.673439767779392,
      "grad_norm": 7.647448539733887,
      "learning_rate": 1.9158200290275764e-05,
      "loss": 1.8258,
      "step": 323000
    },
    {
      "epoch": 24.681078603620808,
      "grad_norm": 10.740863800048828,
      "learning_rate": 1.9148651745473993e-05,
      "loss": 1.6456,
      "step": 323100
    },
    {
      "epoch": 24.688717439462227,
      "grad_norm": 8.068282127380371,
      "learning_rate": 1.9139103200672218e-05,
      "loss": 1.7052,
      "step": 323200
    },
    {
      "epoch": 24.696356275303643,
      "grad_norm": 7.350575923919678,
      "learning_rate": 1.9129554655870447e-05,
      "loss": 1.8094,
      "step": 323300
    },
    {
      "epoch": 24.703995111145062,
      "grad_norm": 9.76183795928955,
      "learning_rate": 1.9120006111068675e-05,
      "loss": 1.8469,
      "step": 323400
    },
    {
      "epoch": 24.711633946986478,
      "grad_norm": 9.250322341918945,
      "learning_rate": 1.91104575662669e-05,
      "loss": 1.6134,
      "step": 323500
    },
    {
      "epoch": 24.719272782827897,
      "grad_norm": 6.60658597946167,
      "learning_rate": 1.910090902146513e-05,
      "loss": 1.7662,
      "step": 323600
    },
    {
      "epoch": 24.726911618669316,
      "grad_norm": 6.092432975769043,
      "learning_rate": 1.9091360476663358e-05,
      "loss": 1.7571,
      "step": 323700
    },
    {
      "epoch": 24.73455045451073,
      "grad_norm": 7.769630432128906,
      "learning_rate": 1.9081811931861583e-05,
      "loss": 1.795,
      "step": 323800
    },
    {
      "epoch": 24.74218929035215,
      "grad_norm": 6.667459487915039,
      "learning_rate": 1.9072263387059812e-05,
      "loss": 1.7595,
      "step": 323900
    },
    {
      "epoch": 24.749828126193567,
      "grad_norm": 9.426572799682617,
      "learning_rate": 1.906271484225804e-05,
      "loss": 1.7916,
      "step": 324000
    },
    {
      "epoch": 24.757466962034986,
      "grad_norm": 16.198537826538086,
      "learning_rate": 1.905316629745627e-05,
      "loss": 1.7679,
      "step": 324100
    },
    {
      "epoch": 24.765105797876405,
      "grad_norm": 11.536022186279297,
      "learning_rate": 1.9043617752654498e-05,
      "loss": 1.7565,
      "step": 324200
    },
    {
      "epoch": 24.77274463371782,
      "grad_norm": 6.775091171264648,
      "learning_rate": 1.9034069207852727e-05,
      "loss": 1.7359,
      "step": 324300
    },
    {
      "epoch": 24.78038346955924,
      "grad_norm": 7.247828960418701,
      "learning_rate": 1.9024520663050952e-05,
      "loss": 1.8025,
      "step": 324400
    },
    {
      "epoch": 24.788022305400656,
      "grad_norm": 8.801544189453125,
      "learning_rate": 1.901497211824918e-05,
      "loss": 1.7596,
      "step": 324500
    },
    {
      "epoch": 24.795661141242075,
      "grad_norm": 8.812378883361816,
      "learning_rate": 1.9005423573447406e-05,
      "loss": 1.8244,
      "step": 324600
    },
    {
      "epoch": 24.803299977083494,
      "grad_norm": 7.5457539558410645,
      "learning_rate": 1.8995875028645635e-05,
      "loss": 1.7432,
      "step": 324700
    },
    {
      "epoch": 24.81093881292491,
      "grad_norm": 5.813148498535156,
      "learning_rate": 1.8986326483843864e-05,
      "loss": 1.8118,
      "step": 324800
    },
    {
      "epoch": 24.81857764876633,
      "grad_norm": 9.232657432556152,
      "learning_rate": 1.897677793904209e-05,
      "loss": 1.7961,
      "step": 324900
    },
    {
      "epoch": 24.826216484607745,
      "grad_norm": 6.509008407592773,
      "learning_rate": 1.8967229394240318e-05,
      "loss": 1.7268,
      "step": 325000
    },
    {
      "epoch": 24.833855320449164,
      "grad_norm": 6.803765296936035,
      "learning_rate": 1.8957680849438546e-05,
      "loss": 1.7787,
      "step": 325100
    },
    {
      "epoch": 24.84149415629058,
      "grad_norm": 9.213643074035645,
      "learning_rate": 1.894813230463677e-05,
      "loss": 1.8238,
      "step": 325200
    },
    {
      "epoch": 24.849132992132,
      "grad_norm": 7.310687065124512,
      "learning_rate": 1.8938583759835e-05,
      "loss": 1.7046,
      "step": 325300
    },
    {
      "epoch": 24.856771827973418,
      "grad_norm": 10.423882484436035,
      "learning_rate": 1.892903521503323e-05,
      "loss": 1.7447,
      "step": 325400
    },
    {
      "epoch": 24.864410663814834,
      "grad_norm": 7.83802604675293,
      "learning_rate": 1.8919486670231458e-05,
      "loss": 1.7584,
      "step": 325500
    },
    {
      "epoch": 24.872049499656253,
      "grad_norm": 7.29543924331665,
      "learning_rate": 1.8909938125429686e-05,
      "loss": 1.8337,
      "step": 325600
    },
    {
      "epoch": 24.87968833549767,
      "grad_norm": 7.8650102615356445,
      "learning_rate": 1.8900389580627915e-05,
      "loss": 1.7281,
      "step": 325700
    },
    {
      "epoch": 24.887327171339088,
      "grad_norm": 10.162335395812988,
      "learning_rate": 1.889084103582614e-05,
      "loss": 1.7953,
      "step": 325800
    },
    {
      "epoch": 24.894966007180507,
      "grad_norm": 10.080343246459961,
      "learning_rate": 1.888129249102437e-05,
      "loss": 1.7516,
      "step": 325900
    },
    {
      "epoch": 24.902604843021923,
      "grad_norm": 8.638224601745605,
      "learning_rate": 1.8871743946222598e-05,
      "loss": 1.7528,
      "step": 326000
    },
    {
      "epoch": 24.910243678863342,
      "grad_norm": 5.910965919494629,
      "learning_rate": 1.8862195401420823e-05,
      "loss": 1.6955,
      "step": 326100
    },
    {
      "epoch": 24.917882514704758,
      "grad_norm": 7.72973108291626,
      "learning_rate": 1.8852646856619052e-05,
      "loss": 1.6499,
      "step": 326200
    },
    {
      "epoch": 24.925521350546177,
      "grad_norm": 6.958319664001465,
      "learning_rate": 1.884309831181728e-05,
      "loss": 1.8054,
      "step": 326300
    },
    {
      "epoch": 24.933160186387596,
      "grad_norm": 9.641101837158203,
      "learning_rate": 1.8833549767015506e-05,
      "loss": 1.7135,
      "step": 326400
    },
    {
      "epoch": 24.94079902222901,
      "grad_norm": 10.003581047058105,
      "learning_rate": 1.8824001222213734e-05,
      "loss": 1.7833,
      "step": 326500
    },
    {
      "epoch": 24.94843785807043,
      "grad_norm": 9.841123580932617,
      "learning_rate": 1.8814452677411963e-05,
      "loss": 1.686,
      "step": 326600
    },
    {
      "epoch": 24.956076693911847,
      "grad_norm": 7.892796993255615,
      "learning_rate": 1.8804904132610192e-05,
      "loss": 1.7364,
      "step": 326700
    },
    {
      "epoch": 24.963715529753266,
      "grad_norm": 10.742575645446777,
      "learning_rate": 1.879535558780842e-05,
      "loss": 1.7465,
      "step": 326800
    },
    {
      "epoch": 24.971354365594685,
      "grad_norm": 7.081240653991699,
      "learning_rate": 1.878580704300665e-05,
      "loss": 1.6497,
      "step": 326900
    },
    {
      "epoch": 24.9789932014361,
      "grad_norm": 5.863734245300293,
      "learning_rate": 1.8776258498204875e-05,
      "loss": 1.8335,
      "step": 327000
    },
    {
      "epoch": 24.98663203727752,
      "grad_norm": 6.30209493637085,
      "learning_rate": 1.8766709953403103e-05,
      "loss": 1.7179,
      "step": 327100
    },
    {
      "epoch": 24.994270873118936,
      "grad_norm": 11.529491424560547,
      "learning_rate": 1.8757161408601332e-05,
      "loss": 1.7892,
      "step": 327200
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.8060497045516968,
      "eval_runtime": 2.968,
      "eval_samples_per_second": 232.48,
      "eval_steps_per_second": 232.48,
      "step": 327275
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.51704740524292,
      "eval_runtime": 56.1043,
      "eval_samples_per_second": 233.333,
      "eval_steps_per_second": 233.333,
      "step": 327275
    },
    {
      "epoch": 25.001909708960355,
      "grad_norm": 6.072840213775635,
      "learning_rate": 1.8747612863799557e-05,
      "loss": 1.6914,
      "step": 327300
    },
    {
      "epoch": 25.00954854480177,
      "grad_norm": 9.087309837341309,
      "learning_rate": 1.8738064318997786e-05,
      "loss": 1.7009,
      "step": 327400
    },
    {
      "epoch": 25.01718738064319,
      "grad_norm": 8.080610275268555,
      "learning_rate": 1.872851577419601e-05,
      "loss": 1.7424,
      "step": 327500
    },
    {
      "epoch": 25.02482621648461,
      "grad_norm": 8.356223106384277,
      "learning_rate": 1.871896722939424e-05,
      "loss": 1.6886,
      "step": 327600
    },
    {
      "epoch": 25.032465052326025,
      "grad_norm": 7.8139214515686035,
      "learning_rate": 1.870941868459247e-05,
      "loss": 1.7446,
      "step": 327700
    },
    {
      "epoch": 25.040103888167444,
      "grad_norm": 5.866079807281494,
      "learning_rate": 1.8699870139790694e-05,
      "loss": 1.7792,
      "step": 327800
    },
    {
      "epoch": 25.04774272400886,
      "grad_norm": 6.751689434051514,
      "learning_rate": 1.8690321594988926e-05,
      "loss": 1.6878,
      "step": 327900
    },
    {
      "epoch": 25.05538155985028,
      "grad_norm": 8.287800788879395,
      "learning_rate": 1.8680773050187155e-05,
      "loss": 1.8209,
      "step": 328000
    },
    {
      "epoch": 25.063020395691698,
      "grad_norm": 8.183354377746582,
      "learning_rate": 1.867122450538538e-05,
      "loss": 1.8037,
      "step": 328100
    },
    {
      "epoch": 25.070659231533114,
      "grad_norm": 11.730512619018555,
      "learning_rate": 1.866167596058361e-05,
      "loss": 1.7086,
      "step": 328200
    },
    {
      "epoch": 25.078298067374533,
      "grad_norm": 7.510214805603027,
      "learning_rate": 1.8652127415781837e-05,
      "loss": 1.7109,
      "step": 328300
    },
    {
      "epoch": 25.08593690321595,
      "grad_norm": 8.924334526062012,
      "learning_rate": 1.8642578870980063e-05,
      "loss": 1.7119,
      "step": 328400
    },
    {
      "epoch": 25.093575739057368,
      "grad_norm": 7.618710517883301,
      "learning_rate": 1.863303032617829e-05,
      "loss": 1.6962,
      "step": 328500
    },
    {
      "epoch": 25.101214574898787,
      "grad_norm": 5.774076461791992,
      "learning_rate": 1.862348178137652e-05,
      "loss": 1.6458,
      "step": 328600
    },
    {
      "epoch": 25.108853410740203,
      "grad_norm": 9.838034629821777,
      "learning_rate": 1.8613933236574745e-05,
      "loss": 1.7561,
      "step": 328700
    },
    {
      "epoch": 25.116492246581622,
      "grad_norm": 8.697670936584473,
      "learning_rate": 1.8604384691772974e-05,
      "loss": 1.719,
      "step": 328800
    },
    {
      "epoch": 25.124131082423037,
      "grad_norm": 8.707180976867676,
      "learning_rate": 1.8594836146971203e-05,
      "loss": 1.7363,
      "step": 328900
    },
    {
      "epoch": 25.131769918264457,
      "grad_norm": 9.095959663391113,
      "learning_rate": 1.8585287602169428e-05,
      "loss": 1.8567,
      "step": 329000
    },
    {
      "epoch": 25.139408754105876,
      "grad_norm": 8.149759292602539,
      "learning_rate": 1.8575739057367657e-05,
      "loss": 1.7057,
      "step": 329100
    },
    {
      "epoch": 25.14704758994729,
      "grad_norm": 7.313483238220215,
      "learning_rate": 1.8566190512565885e-05,
      "loss": 1.7176,
      "step": 329200
    },
    {
      "epoch": 25.15468642578871,
      "grad_norm": 6.472639560699463,
      "learning_rate": 1.8556641967764114e-05,
      "loss": 1.6333,
      "step": 329300
    },
    {
      "epoch": 25.162325261630127,
      "grad_norm": 8.034153938293457,
      "learning_rate": 1.8547093422962343e-05,
      "loss": 1.7008,
      "step": 329400
    },
    {
      "epoch": 25.169964097471546,
      "grad_norm": 7.225561618804932,
      "learning_rate": 1.853754487816057e-05,
      "loss": 1.6685,
      "step": 329500
    },
    {
      "epoch": 25.17760293331296,
      "grad_norm": 6.919077396392822,
      "learning_rate": 1.8527996333358797e-05,
      "loss": 1.8394,
      "step": 329600
    },
    {
      "epoch": 25.18524176915438,
      "grad_norm": 10.198993682861328,
      "learning_rate": 1.8518447788557026e-05,
      "loss": 1.7653,
      "step": 329700
    },
    {
      "epoch": 25.1928806049958,
      "grad_norm": 6.8069868087768555,
      "learning_rate": 1.8508899243755254e-05,
      "loss": 1.7304,
      "step": 329800
    },
    {
      "epoch": 25.200519440837216,
      "grad_norm": 5.956119060516357,
      "learning_rate": 1.849935069895348e-05,
      "loss": 1.8505,
      "step": 329900
    },
    {
      "epoch": 25.208158276678635,
      "grad_norm": 7.005870819091797,
      "learning_rate": 1.8489802154151708e-05,
      "loss": 1.7155,
      "step": 330000
    },
    {
      "epoch": 25.21579711252005,
      "grad_norm": 7.0752644538879395,
      "learning_rate": 1.8480253609349937e-05,
      "loss": 1.7992,
      "step": 330100
    },
    {
      "epoch": 25.22343594836147,
      "grad_norm": 8.582741737365723,
      "learning_rate": 1.8470705064548162e-05,
      "loss": 1.8495,
      "step": 330200
    },
    {
      "epoch": 25.23107478420289,
      "grad_norm": 8.219377517700195,
      "learning_rate": 1.846115651974639e-05,
      "loss": 1.7603,
      "step": 330300
    },
    {
      "epoch": 25.238713620044305,
      "grad_norm": 7.649241924285889,
      "learning_rate": 1.845160797494462e-05,
      "loss": 1.791,
      "step": 330400
    },
    {
      "epoch": 25.246352455885724,
      "grad_norm": 9.348695755004883,
      "learning_rate": 1.8442059430142848e-05,
      "loss": 1.6608,
      "step": 330500
    },
    {
      "epoch": 25.25399129172714,
      "grad_norm": 6.39242696762085,
      "learning_rate": 1.8432510885341077e-05,
      "loss": 1.8149,
      "step": 330600
    },
    {
      "epoch": 25.26163012756856,
      "grad_norm": 11.049959182739258,
      "learning_rate": 1.8422962340539302e-05,
      "loss": 1.7698,
      "step": 330700
    },
    {
      "epoch": 25.269268963409978,
      "grad_norm": 8.658055305480957,
      "learning_rate": 1.841341379573753e-05,
      "loss": 1.6981,
      "step": 330800
    },
    {
      "epoch": 25.276907799251394,
      "grad_norm": 7.712008953094482,
      "learning_rate": 1.840386525093576e-05,
      "loss": 1.7037,
      "step": 330900
    },
    {
      "epoch": 25.284546635092813,
      "grad_norm": 7.237318992614746,
      "learning_rate": 1.8394316706133985e-05,
      "loss": 1.8342,
      "step": 331000
    },
    {
      "epoch": 25.29218547093423,
      "grad_norm": 9.396753311157227,
      "learning_rate": 1.8384768161332214e-05,
      "loss": 1.776,
      "step": 331100
    },
    {
      "epoch": 25.299824306775648,
      "grad_norm": 8.930739402770996,
      "learning_rate": 1.8375219616530442e-05,
      "loss": 1.8377,
      "step": 331200
    },
    {
      "epoch": 25.307463142617067,
      "grad_norm": 7.653509616851807,
      "learning_rate": 1.8365671071728668e-05,
      "loss": 1.7172,
      "step": 331300
    },
    {
      "epoch": 25.315101978458483,
      "grad_norm": 8.268383979797363,
      "learning_rate": 1.8356122526926896e-05,
      "loss": 1.7917,
      "step": 331400
    },
    {
      "epoch": 25.3227408142999,
      "grad_norm": 8.274065971374512,
      "learning_rate": 1.8346573982125125e-05,
      "loss": 1.8497,
      "step": 331500
    },
    {
      "epoch": 25.330379650141317,
      "grad_norm": 4.440268516540527,
      "learning_rate": 1.833702543732335e-05,
      "loss": 1.749,
      "step": 331600
    },
    {
      "epoch": 25.338018485982737,
      "grad_norm": 7.175546169281006,
      "learning_rate": 1.832747689252158e-05,
      "loss": 1.7084,
      "step": 331700
    },
    {
      "epoch": 25.345657321824152,
      "grad_norm": 11.588356971740723,
      "learning_rate": 1.8317928347719808e-05,
      "loss": 1.7809,
      "step": 331800
    },
    {
      "epoch": 25.35329615766557,
      "grad_norm": 7.5105438232421875,
      "learning_rate": 1.8308379802918036e-05,
      "loss": 1.7085,
      "step": 331900
    },
    {
      "epoch": 25.36093499350699,
      "grad_norm": 6.262847900390625,
      "learning_rate": 1.8298831258116265e-05,
      "loss": 1.8207,
      "step": 332000
    },
    {
      "epoch": 25.368573829348406,
      "grad_norm": 9.461645126342773,
      "learning_rate": 1.8289282713314494e-05,
      "loss": 1.7447,
      "step": 332100
    },
    {
      "epoch": 25.376212665189826,
      "grad_norm": 7.269127368927002,
      "learning_rate": 1.827973416851272e-05,
      "loss": 1.7417,
      "step": 332200
    },
    {
      "epoch": 25.38385150103124,
      "grad_norm": 7.816921234130859,
      "learning_rate": 1.8270185623710948e-05,
      "loss": 1.7411,
      "step": 332300
    },
    {
      "epoch": 25.39149033687266,
      "grad_norm": 6.852450847625732,
      "learning_rate": 1.8260637078909177e-05,
      "loss": 1.7076,
      "step": 332400
    },
    {
      "epoch": 25.39912917271408,
      "grad_norm": 7.859094619750977,
      "learning_rate": 1.8251088534107402e-05,
      "loss": 1.7313,
      "step": 332500
    },
    {
      "epoch": 25.406768008555495,
      "grad_norm": 8.361146926879883,
      "learning_rate": 1.824153998930563e-05,
      "loss": 1.8271,
      "step": 332600
    },
    {
      "epoch": 25.414406844396915,
      "grad_norm": 6.928584575653076,
      "learning_rate": 1.823199144450386e-05,
      "loss": 1.7728,
      "step": 332700
    },
    {
      "epoch": 25.42204568023833,
      "grad_norm": 7.512825965881348,
      "learning_rate": 1.8222442899702085e-05,
      "loss": 1.7239,
      "step": 332800
    },
    {
      "epoch": 25.42968451607975,
      "grad_norm": 7.462856292724609,
      "learning_rate": 1.8212894354900313e-05,
      "loss": 1.6823,
      "step": 332900
    },
    {
      "epoch": 25.43732335192117,
      "grad_norm": 6.203347206115723,
      "learning_rate": 1.8203345810098542e-05,
      "loss": 1.7586,
      "step": 333000
    },
    {
      "epoch": 25.444962187762584,
      "grad_norm": 7.962479591369629,
      "learning_rate": 1.819379726529677e-05,
      "loss": 1.7909,
      "step": 333100
    },
    {
      "epoch": 25.452601023604004,
      "grad_norm": 5.740406513214111,
      "learning_rate": 1.8184248720495e-05,
      "loss": 1.7365,
      "step": 333200
    },
    {
      "epoch": 25.46023985944542,
      "grad_norm": 8.86953353881836,
      "learning_rate": 1.8174700175693225e-05,
      "loss": 1.7379,
      "step": 333300
    },
    {
      "epoch": 25.46787869528684,
      "grad_norm": 5.732144832611084,
      "learning_rate": 1.8165151630891453e-05,
      "loss": 1.6776,
      "step": 333400
    },
    {
      "epoch": 25.475517531128258,
      "grad_norm": 8.256998062133789,
      "learning_rate": 1.8155603086089682e-05,
      "loss": 1.6734,
      "step": 333500
    },
    {
      "epoch": 25.483156366969673,
      "grad_norm": 7.787600994110107,
      "learning_rate": 1.8146054541287907e-05,
      "loss": 1.7066,
      "step": 333600
    },
    {
      "epoch": 25.490795202811093,
      "grad_norm": 8.877877235412598,
      "learning_rate": 1.8136505996486136e-05,
      "loss": 1.6337,
      "step": 333700
    },
    {
      "epoch": 25.49843403865251,
      "grad_norm": 8.005453109741211,
      "learning_rate": 1.8126957451684365e-05,
      "loss": 1.716,
      "step": 333800
    },
    {
      "epoch": 25.506072874493928,
      "grad_norm": 6.973790168762207,
      "learning_rate": 1.811740890688259e-05,
      "loss": 1.7147,
      "step": 333900
    },
    {
      "epoch": 25.513711710335343,
      "grad_norm": 8.131416320800781,
      "learning_rate": 1.810786036208082e-05,
      "loss": 1.7418,
      "step": 334000
    },
    {
      "epoch": 25.521350546176762,
      "grad_norm": 8.87594223022461,
      "learning_rate": 1.8098311817279047e-05,
      "loss": 1.6866,
      "step": 334100
    },
    {
      "epoch": 25.52898938201818,
      "grad_norm": 27.978713989257812,
      "learning_rate": 1.8088763272477273e-05,
      "loss": 1.7985,
      "step": 334200
    },
    {
      "epoch": 25.536628217859597,
      "grad_norm": 7.624000072479248,
      "learning_rate": 1.80792147276755e-05,
      "loss": 1.8218,
      "step": 334300
    },
    {
      "epoch": 25.544267053701017,
      "grad_norm": 7.962432861328125,
      "learning_rate": 1.8069666182873733e-05,
      "loss": 1.7487,
      "step": 334400
    },
    {
      "epoch": 25.551905889542432,
      "grad_norm": 8.65139389038086,
      "learning_rate": 1.806011763807196e-05,
      "loss": 1.8107,
      "step": 334500
    },
    {
      "epoch": 25.55954472538385,
      "grad_norm": 9.867142677307129,
      "learning_rate": 1.8050569093270187e-05,
      "loss": 1.7055,
      "step": 334600
    },
    {
      "epoch": 25.56718356122527,
      "grad_norm": 5.1350321769714355,
      "learning_rate": 1.8041020548468416e-05,
      "loss": 1.6572,
      "step": 334700
    },
    {
      "epoch": 25.574822397066686,
      "grad_norm": 8.135071754455566,
      "learning_rate": 1.803147200366664e-05,
      "loss": 1.6355,
      "step": 334800
    },
    {
      "epoch": 25.582461232908106,
      "grad_norm": 8.493582725524902,
      "learning_rate": 1.802192345886487e-05,
      "loss": 1.7662,
      "step": 334900
    },
    {
      "epoch": 25.59010006874952,
      "grad_norm": 11.1504545211792,
      "learning_rate": 1.80123749140631e-05,
      "loss": 1.736,
      "step": 335000
    },
    {
      "epoch": 25.59773890459094,
      "grad_norm": 7.987360000610352,
      "learning_rate": 1.8002826369261324e-05,
      "loss": 1.7436,
      "step": 335100
    },
    {
      "epoch": 25.60537774043236,
      "grad_norm": 9.584588050842285,
      "learning_rate": 1.7993277824459553e-05,
      "loss": 1.791,
      "step": 335200
    },
    {
      "epoch": 25.613016576273775,
      "grad_norm": 7.873789310455322,
      "learning_rate": 1.798372927965778e-05,
      "loss": 1.7147,
      "step": 335300
    },
    {
      "epoch": 25.620655412115195,
      "grad_norm": 8.380846977233887,
      "learning_rate": 1.7974180734856007e-05,
      "loss": 1.7504,
      "step": 335400
    },
    {
      "epoch": 25.62829424795661,
      "grad_norm": 6.1737284660339355,
      "learning_rate": 1.7964632190054236e-05,
      "loss": 1.7588,
      "step": 335500
    },
    {
      "epoch": 25.63593308379803,
      "grad_norm": 6.998619079589844,
      "learning_rate": 1.7955083645252464e-05,
      "loss": 1.7089,
      "step": 335600
    },
    {
      "epoch": 25.64357191963945,
      "grad_norm": 8.898451805114746,
      "learning_rate": 1.7945535100450693e-05,
      "loss": 1.757,
      "step": 335700
    },
    {
      "epoch": 25.651210755480864,
      "grad_norm": 9.814516067504883,
      "learning_rate": 1.793598655564892e-05,
      "loss": 1.6392,
      "step": 335800
    },
    {
      "epoch": 25.658849591322284,
      "grad_norm": 9.224801063537598,
      "learning_rate": 1.792643801084715e-05,
      "loss": 1.7934,
      "step": 335900
    },
    {
      "epoch": 25.6664884271637,
      "grad_norm": 8.319979667663574,
      "learning_rate": 1.7916889466045376e-05,
      "loss": 1.8195,
      "step": 336000
    },
    {
      "epoch": 25.67412726300512,
      "grad_norm": 6.797351837158203,
      "learning_rate": 1.7907340921243604e-05,
      "loss": 1.7196,
      "step": 336100
    },
    {
      "epoch": 25.681766098846534,
      "grad_norm": 7.906470775604248,
      "learning_rate": 1.789779237644183e-05,
      "loss": 1.7594,
      "step": 336200
    },
    {
      "epoch": 25.689404934687953,
      "grad_norm": 9.043683052062988,
      "learning_rate": 1.7888243831640058e-05,
      "loss": 1.817,
      "step": 336300
    },
    {
      "epoch": 25.697043770529373,
      "grad_norm": 8.429035186767578,
      "learning_rate": 1.7878695286838287e-05,
      "loss": 1.8344,
      "step": 336400
    },
    {
      "epoch": 25.70468260637079,
      "grad_norm": 9.611922264099121,
      "learning_rate": 1.7869146742036512e-05,
      "loss": 1.7035,
      "step": 336500
    },
    {
      "epoch": 25.712321442212208,
      "grad_norm": 6.2691650390625,
      "learning_rate": 1.785959819723474e-05,
      "loss": 1.722,
      "step": 336600
    },
    {
      "epoch": 25.719960278053623,
      "grad_norm": 7.762559413909912,
      "learning_rate": 1.785004965243297e-05,
      "loss": 1.7809,
      "step": 336700
    },
    {
      "epoch": 25.727599113895042,
      "grad_norm": 6.617394924163818,
      "learning_rate": 1.78405011076312e-05,
      "loss": 1.7084,
      "step": 336800
    },
    {
      "epoch": 25.73523794973646,
      "grad_norm": 7.52209997177124,
      "learning_rate": 1.7830952562829427e-05,
      "loss": 1.7657,
      "step": 336900
    },
    {
      "epoch": 25.742876785577877,
      "grad_norm": 8.36017894744873,
      "learning_rate": 1.7821404018027656e-05,
      "loss": 1.8004,
      "step": 337000
    },
    {
      "epoch": 25.750515621419297,
      "grad_norm": 9.743833541870117,
      "learning_rate": 1.781185547322588e-05,
      "loss": 1.6993,
      "step": 337100
    },
    {
      "epoch": 25.758154457260712,
      "grad_norm": 7.583211421966553,
      "learning_rate": 1.780230692842411e-05,
      "loss": 1.8286,
      "step": 337200
    },
    {
      "epoch": 25.76579329310213,
      "grad_norm": 8.442118644714355,
      "learning_rate": 1.779275838362234e-05,
      "loss": 1.7924,
      "step": 337300
    },
    {
      "epoch": 25.77343212894355,
      "grad_norm": 7.918787479400635,
      "learning_rate": 1.7783209838820564e-05,
      "loss": 1.7298,
      "step": 337400
    },
    {
      "epoch": 25.781070964784966,
      "grad_norm": 7.453122615814209,
      "learning_rate": 1.7773661294018792e-05,
      "loss": 1.7641,
      "step": 337500
    },
    {
      "epoch": 25.788709800626386,
      "grad_norm": 7.315866947174072,
      "learning_rate": 1.776411274921702e-05,
      "loss": 1.6971,
      "step": 337600
    },
    {
      "epoch": 25.7963486364678,
      "grad_norm": 8.558135986328125,
      "learning_rate": 1.7754564204415246e-05,
      "loss": 1.792,
      "step": 337700
    },
    {
      "epoch": 25.80398747230922,
      "grad_norm": 9.886393547058105,
      "learning_rate": 1.7745015659613475e-05,
      "loss": 1.8033,
      "step": 337800
    },
    {
      "epoch": 25.811626308150636,
      "grad_norm": 7.154398441314697,
      "learning_rate": 1.7735467114811704e-05,
      "loss": 1.768,
      "step": 337900
    },
    {
      "epoch": 25.819265143992055,
      "grad_norm": 6.687176704406738,
      "learning_rate": 1.772591857000993e-05,
      "loss": 1.8152,
      "step": 338000
    },
    {
      "epoch": 25.826903979833475,
      "grad_norm": 6.62334680557251,
      "learning_rate": 1.7716370025208158e-05,
      "loss": 1.7235,
      "step": 338100
    },
    {
      "epoch": 25.83454281567489,
      "grad_norm": 9.314254760742188,
      "learning_rate": 1.7706821480406387e-05,
      "loss": 1.7148,
      "step": 338200
    },
    {
      "epoch": 25.84218165151631,
      "grad_norm": 8.031670570373535,
      "learning_rate": 1.7697272935604615e-05,
      "loss": 1.7874,
      "step": 338300
    },
    {
      "epoch": 25.849820487357725,
      "grad_norm": 7.489258289337158,
      "learning_rate": 1.7687724390802844e-05,
      "loss": 1.7796,
      "step": 338400
    },
    {
      "epoch": 25.857459323199144,
      "grad_norm": 7.339222431182861,
      "learning_rate": 1.7678175846001073e-05,
      "loss": 1.8123,
      "step": 338500
    },
    {
      "epoch": 25.865098159040564,
      "grad_norm": 6.837703227996826,
      "learning_rate": 1.7668627301199298e-05,
      "loss": 1.7063,
      "step": 338600
    },
    {
      "epoch": 25.87273699488198,
      "grad_norm": 7.225393772125244,
      "learning_rate": 1.7659078756397527e-05,
      "loss": 1.7573,
      "step": 338700
    },
    {
      "epoch": 25.8803758307234,
      "grad_norm": 8.987585067749023,
      "learning_rate": 1.7649530211595755e-05,
      "loss": 1.7276,
      "step": 338800
    },
    {
      "epoch": 25.888014666564814,
      "grad_norm": 6.237219333648682,
      "learning_rate": 1.763998166679398e-05,
      "loss": 1.741,
      "step": 338900
    },
    {
      "epoch": 25.895653502406233,
      "grad_norm": 5.790903091430664,
      "learning_rate": 1.763043312199221e-05,
      "loss": 1.7072,
      "step": 339000
    },
    {
      "epoch": 25.903292338247653,
      "grad_norm": 8.431375503540039,
      "learning_rate": 1.7620884577190435e-05,
      "loss": 1.7019,
      "step": 339100
    },
    {
      "epoch": 25.910931174089068,
      "grad_norm": 8.03297233581543,
      "learning_rate": 1.7611336032388663e-05,
      "loss": 1.7261,
      "step": 339200
    },
    {
      "epoch": 25.918570009930487,
      "grad_norm": 9.09687614440918,
      "learning_rate": 1.7601787487586892e-05,
      "loss": 1.7186,
      "step": 339300
    },
    {
      "epoch": 25.926208845771903,
      "grad_norm": 9.002360343933105,
      "learning_rate": 1.759223894278512e-05,
      "loss": 1.8059,
      "step": 339400
    },
    {
      "epoch": 25.933847681613322,
      "grad_norm": 8.366929054260254,
      "learning_rate": 1.758269039798335e-05,
      "loss": 1.7628,
      "step": 339500
    },
    {
      "epoch": 25.94148651745474,
      "grad_norm": 5.51389217376709,
      "learning_rate": 1.7573141853181578e-05,
      "loss": 1.734,
      "step": 339600
    },
    {
      "epoch": 25.949125353296157,
      "grad_norm": 8.309370994567871,
      "learning_rate": 1.7563593308379803e-05,
      "loss": 1.7584,
      "step": 339700
    },
    {
      "epoch": 25.956764189137576,
      "grad_norm": 12.316145896911621,
      "learning_rate": 1.7554044763578032e-05,
      "loss": 1.7953,
      "step": 339800
    },
    {
      "epoch": 25.964403024978992,
      "grad_norm": 7.288844585418701,
      "learning_rate": 1.754449621877626e-05,
      "loss": 1.7586,
      "step": 339900
    },
    {
      "epoch": 25.97204186082041,
      "grad_norm": 7.705577850341797,
      "learning_rate": 1.7534947673974486e-05,
      "loss": 1.7848,
      "step": 340000
    },
    {
      "epoch": 25.97968069666183,
      "grad_norm": 8.147586822509766,
      "learning_rate": 1.7525399129172715e-05,
      "loss": 1.7242,
      "step": 340100
    },
    {
      "epoch": 25.987319532503246,
      "grad_norm": 4.731416702270508,
      "learning_rate": 1.7515850584370943e-05,
      "loss": 1.7106,
      "step": 340200
    },
    {
      "epoch": 25.994958368344665,
      "grad_norm": 7.91616678237915,
      "learning_rate": 1.750630203956917e-05,
      "loss": 1.7851,
      "step": 340300
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.799271821975708,
      "eval_runtime": 2.9584,
      "eval_samples_per_second": 233.231,
      "eval_steps_per_second": 233.231,
      "step": 340366
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.509091854095459,
      "eval_runtime": 56.1825,
      "eval_samples_per_second": 233.008,
      "eval_steps_per_second": 233.008,
      "step": 340366
    },
    {
      "epoch": 26.00259720418608,
      "grad_norm": 7.611261367797852,
      "learning_rate": 1.7496753494767397e-05,
      "loss": 1.7657,
      "step": 340400
    },
    {
      "epoch": 26.0102360400275,
      "grad_norm": 6.163564205169678,
      "learning_rate": 1.7487204949965626e-05,
      "loss": 1.7047,
      "step": 340500
    },
    {
      "epoch": 26.017874875868916,
      "grad_norm": 18.014001846313477,
      "learning_rate": 1.747765640516385e-05,
      "loss": 1.7889,
      "step": 340600
    },
    {
      "epoch": 26.025513711710335,
      "grad_norm": 10.048970222473145,
      "learning_rate": 1.746810786036208e-05,
      "loss": 1.7438,
      "step": 340700
    },
    {
      "epoch": 26.033152547551754,
      "grad_norm": 7.320094108581543,
      "learning_rate": 1.7458559315560312e-05,
      "loss": 1.7637,
      "step": 340800
    },
    {
      "epoch": 26.04079138339317,
      "grad_norm": 7.25214958190918,
      "learning_rate": 1.7449010770758538e-05,
      "loss": 1.8331,
      "step": 340900
    },
    {
      "epoch": 26.04843021923459,
      "grad_norm": 7.639355182647705,
      "learning_rate": 1.7439462225956766e-05,
      "loss": 1.6763,
      "step": 341000
    },
    {
      "epoch": 26.056069055076005,
      "grad_norm": 7.929017543792725,
      "learning_rate": 1.7429913681154995e-05,
      "loss": 1.7806,
      "step": 341100
    },
    {
      "epoch": 26.063707890917424,
      "grad_norm": 7.661686897277832,
      "learning_rate": 1.742036513635322e-05,
      "loss": 1.7987,
      "step": 341200
    },
    {
      "epoch": 26.071346726758843,
      "grad_norm": 8.411754608154297,
      "learning_rate": 1.741081659155145e-05,
      "loss": 1.6308,
      "step": 341300
    },
    {
      "epoch": 26.07898556260026,
      "grad_norm": 6.657934188842773,
      "learning_rate": 1.7401268046749678e-05,
      "loss": 1.7341,
      "step": 341400
    },
    {
      "epoch": 26.08662439844168,
      "grad_norm": 6.283450126647949,
      "learning_rate": 1.7391719501947903e-05,
      "loss": 1.6898,
      "step": 341500
    },
    {
      "epoch": 26.094263234283094,
      "grad_norm": 9.576536178588867,
      "learning_rate": 1.738217095714613e-05,
      "loss": 1.7848,
      "step": 341600
    },
    {
      "epoch": 26.101902070124513,
      "grad_norm": 10.046102523803711,
      "learning_rate": 1.7372622412344357e-05,
      "loss": 1.7547,
      "step": 341700
    },
    {
      "epoch": 26.109540905965932,
      "grad_norm": 6.617671012878418,
      "learning_rate": 1.7363073867542586e-05,
      "loss": 1.7068,
      "step": 341800
    },
    {
      "epoch": 26.117179741807348,
      "grad_norm": 9.184194564819336,
      "learning_rate": 1.7353525322740814e-05,
      "loss": 1.7777,
      "step": 341900
    },
    {
      "epoch": 26.124818577648767,
      "grad_norm": 6.351149082183838,
      "learning_rate": 1.7343976777939043e-05,
      "loss": 1.7825,
      "step": 342000
    },
    {
      "epoch": 26.132457413490183,
      "grad_norm": 9.604132652282715,
      "learning_rate": 1.733442823313727e-05,
      "loss": 1.7653,
      "step": 342100
    },
    {
      "epoch": 26.140096249331602,
      "grad_norm": 5.092426300048828,
      "learning_rate": 1.73248796883355e-05,
      "loss": 1.6904,
      "step": 342200
    },
    {
      "epoch": 26.147735085173018,
      "grad_norm": 5.971704959869385,
      "learning_rate": 1.7315331143533726e-05,
      "loss": 1.7555,
      "step": 342300
    },
    {
      "epoch": 26.155373921014437,
      "grad_norm": 7.032042503356934,
      "learning_rate": 1.7305782598731954e-05,
      "loss": 1.6914,
      "step": 342400
    },
    {
      "epoch": 26.163012756855856,
      "grad_norm": 7.925370216369629,
      "learning_rate": 1.7296234053930183e-05,
      "loss": 1.7305,
      "step": 342500
    },
    {
      "epoch": 26.170651592697272,
      "grad_norm": 5.3682708740234375,
      "learning_rate": 1.728668550912841e-05,
      "loss": 1.6885,
      "step": 342600
    },
    {
      "epoch": 26.17829042853869,
      "grad_norm": 10.312788963317871,
      "learning_rate": 1.7277136964326637e-05,
      "loss": 1.7458,
      "step": 342700
    },
    {
      "epoch": 26.185929264380107,
      "grad_norm": 5.896518230438232,
      "learning_rate": 1.7267588419524866e-05,
      "loss": 1.6845,
      "step": 342800
    },
    {
      "epoch": 26.193568100221526,
      "grad_norm": 7.054032325744629,
      "learning_rate": 1.725803987472309e-05,
      "loss": 1.8825,
      "step": 342900
    },
    {
      "epoch": 26.201206936062945,
      "grad_norm": 9.329004287719727,
      "learning_rate": 1.724849132992132e-05,
      "loss": 1.7667,
      "step": 343000
    },
    {
      "epoch": 26.20884577190436,
      "grad_norm": 10.625710487365723,
      "learning_rate": 1.723894278511955e-05,
      "loss": 1.6947,
      "step": 343100
    },
    {
      "epoch": 26.21648460774578,
      "grad_norm": 5.524168491363525,
      "learning_rate": 1.7229394240317777e-05,
      "loss": 1.8462,
      "step": 343200
    },
    {
      "epoch": 26.224123443587196,
      "grad_norm": 7.524863243103027,
      "learning_rate": 1.7219845695516006e-05,
      "loss": 1.7391,
      "step": 343300
    },
    {
      "epoch": 26.231762279428615,
      "grad_norm": 7.210514545440674,
      "learning_rate": 1.7210297150714235e-05,
      "loss": 1.6952,
      "step": 343400
    },
    {
      "epoch": 26.239401115270034,
      "grad_norm": 8.93256664276123,
      "learning_rate": 1.720074860591246e-05,
      "loss": 1.762,
      "step": 343500
    },
    {
      "epoch": 26.24703995111145,
      "grad_norm": 8.243019104003906,
      "learning_rate": 1.719120006111069e-05,
      "loss": 1.7776,
      "step": 343600
    },
    {
      "epoch": 26.25467878695287,
      "grad_norm": 7.239837646484375,
      "learning_rate": 1.7181651516308917e-05,
      "loss": 1.7269,
      "step": 343700
    },
    {
      "epoch": 26.262317622794285,
      "grad_norm": 7.591602325439453,
      "learning_rate": 1.7172102971507143e-05,
      "loss": 1.7333,
      "step": 343800
    },
    {
      "epoch": 26.269956458635704,
      "grad_norm": 6.353014945983887,
      "learning_rate": 1.716255442670537e-05,
      "loss": 1.7389,
      "step": 343900
    },
    {
      "epoch": 26.277595294477123,
      "grad_norm": 7.309032917022705,
      "learning_rate": 1.71530058819036e-05,
      "loss": 1.8222,
      "step": 344000
    },
    {
      "epoch": 26.28523413031854,
      "grad_norm": 8.679221153259277,
      "learning_rate": 1.7143457337101825e-05,
      "loss": 1.6976,
      "step": 344100
    },
    {
      "epoch": 26.29287296615996,
      "grad_norm": 6.18479585647583,
      "learning_rate": 1.7133908792300054e-05,
      "loss": 1.6217,
      "step": 344200
    },
    {
      "epoch": 26.300511802001374,
      "grad_norm": 8.085516929626465,
      "learning_rate": 1.7124360247498283e-05,
      "loss": 1.7422,
      "step": 344300
    },
    {
      "epoch": 26.308150637842793,
      "grad_norm": 8.696911811828613,
      "learning_rate": 1.7114811702696508e-05,
      "loss": 1.7323,
      "step": 344400
    },
    {
      "epoch": 26.31578947368421,
      "grad_norm": 7.249979019165039,
      "learning_rate": 1.7105263157894737e-05,
      "loss": 1.685,
      "step": 344500
    },
    {
      "epoch": 26.323428309525628,
      "grad_norm": 5.959765434265137,
      "learning_rate": 1.7095714613092965e-05,
      "loss": 1.7328,
      "step": 344600
    },
    {
      "epoch": 26.331067145367047,
      "grad_norm": 7.448666572570801,
      "learning_rate": 1.7086166068291194e-05,
      "loss": 1.8808,
      "step": 344700
    },
    {
      "epoch": 26.338705981208463,
      "grad_norm": 7.645656108856201,
      "learning_rate": 1.7076617523489423e-05,
      "loss": 1.6425,
      "step": 344800
    },
    {
      "epoch": 26.346344817049882,
      "grad_norm": 6.276449680328369,
      "learning_rate": 1.7067068978687648e-05,
      "loss": 1.7808,
      "step": 344900
    },
    {
      "epoch": 26.353983652891298,
      "grad_norm": 7.654251575469971,
      "learning_rate": 1.7057520433885877e-05,
      "loss": 1.6604,
      "step": 345000
    },
    {
      "epoch": 26.361622488732717,
      "grad_norm": 6.5480756759643555,
      "learning_rate": 1.7047971889084105e-05,
      "loss": 1.6809,
      "step": 345100
    },
    {
      "epoch": 26.369261324574136,
      "grad_norm": 10.034764289855957,
      "learning_rate": 1.703842334428233e-05,
      "loss": 1.7641,
      "step": 345200
    },
    {
      "epoch": 26.376900160415552,
      "grad_norm": 7.069597244262695,
      "learning_rate": 1.702887479948056e-05,
      "loss": 1.7317,
      "step": 345300
    },
    {
      "epoch": 26.38453899625697,
      "grad_norm": 6.92454719543457,
      "learning_rate": 1.7019326254678788e-05,
      "loss": 1.7008,
      "step": 345400
    },
    {
      "epoch": 26.392177832098387,
      "grad_norm": 5.648057460784912,
      "learning_rate": 1.7009777709877013e-05,
      "loss": 1.6912,
      "step": 345500
    },
    {
      "epoch": 26.399816667939806,
      "grad_norm": 7.528579235076904,
      "learning_rate": 1.7000229165075242e-05,
      "loss": 1.7632,
      "step": 345600
    },
    {
      "epoch": 26.407455503781225,
      "grad_norm": 6.833191394805908,
      "learning_rate": 1.699068062027347e-05,
      "loss": 1.7196,
      "step": 345700
    },
    {
      "epoch": 26.41509433962264,
      "grad_norm": 6.540133476257324,
      "learning_rate": 1.69811320754717e-05,
      "loss": 1.8349,
      "step": 345800
    },
    {
      "epoch": 26.42273317546406,
      "grad_norm": 6.400369644165039,
      "learning_rate": 1.6971583530669928e-05,
      "loss": 1.6808,
      "step": 345900
    },
    {
      "epoch": 26.430372011305476,
      "grad_norm": 13.471284866333008,
      "learning_rate": 1.6962034985868157e-05,
      "loss": 1.803,
      "step": 346000
    },
    {
      "epoch": 26.438010847146895,
      "grad_norm": 5.70880651473999,
      "learning_rate": 1.6952486441066382e-05,
      "loss": 1.7284,
      "step": 346100
    },
    {
      "epoch": 26.445649682988314,
      "grad_norm": 6.312962055206299,
      "learning_rate": 1.694293789626461e-05,
      "loss": 1.7816,
      "step": 346200
    },
    {
      "epoch": 26.45328851882973,
      "grad_norm": 7.557792663574219,
      "learning_rate": 1.693338935146284e-05,
      "loss": 1.7131,
      "step": 346300
    },
    {
      "epoch": 26.46092735467115,
      "grad_norm": 8.511882781982422,
      "learning_rate": 1.6923840806661065e-05,
      "loss": 1.7183,
      "step": 346400
    },
    {
      "epoch": 26.468566190512565,
      "grad_norm": 8.48825454711914,
      "learning_rate": 1.6914292261859294e-05,
      "loss": 1.765,
      "step": 346500
    },
    {
      "epoch": 26.476205026353984,
      "grad_norm": 5.840282440185547,
      "learning_rate": 1.6904743717057522e-05,
      "loss": 1.762,
      "step": 346600
    },
    {
      "epoch": 26.4838438621954,
      "grad_norm": 9.548721313476562,
      "learning_rate": 1.6895195172255748e-05,
      "loss": 1.6824,
      "step": 346700
    },
    {
      "epoch": 26.49148269803682,
      "grad_norm": 7.858485698699951,
      "learning_rate": 1.6885646627453976e-05,
      "loss": 1.6404,
      "step": 346800
    },
    {
      "epoch": 26.499121533878238,
      "grad_norm": 7.22233247756958,
      "learning_rate": 1.6876098082652205e-05,
      "loss": 1.7588,
      "step": 346900
    },
    {
      "epoch": 26.506760369719654,
      "grad_norm": 9.22665786743164,
      "learning_rate": 1.686654953785043e-05,
      "loss": 1.6847,
      "step": 347000
    },
    {
      "epoch": 26.514399205561073,
      "grad_norm": 6.675160884857178,
      "learning_rate": 1.685700099304866e-05,
      "loss": 1.821,
      "step": 347100
    },
    {
      "epoch": 26.52203804140249,
      "grad_norm": 7.463222980499268,
      "learning_rate": 1.684745244824689e-05,
      "loss": 1.6708,
      "step": 347200
    },
    {
      "epoch": 26.529676877243908,
      "grad_norm": 8.537875175476074,
      "learning_rate": 1.6837903903445116e-05,
      "loss": 1.8236,
      "step": 347300
    },
    {
      "epoch": 26.537315713085327,
      "grad_norm": 7.092630386352539,
      "learning_rate": 1.6828355358643345e-05,
      "loss": 1.6328,
      "step": 347400
    },
    {
      "epoch": 26.544954548926743,
      "grad_norm": 9.210018157958984,
      "learning_rate": 1.681880681384157e-05,
      "loss": 1.7225,
      "step": 347500
    },
    {
      "epoch": 26.552593384768162,
      "grad_norm": 7.382829189300537,
      "learning_rate": 1.68092582690398e-05,
      "loss": 1.7721,
      "step": 347600
    },
    {
      "epoch": 26.560232220609578,
      "grad_norm": 8.716255187988281,
      "learning_rate": 1.6799709724238028e-05,
      "loss": 1.7514,
      "step": 347700
    },
    {
      "epoch": 26.567871056450997,
      "grad_norm": 6.318422317504883,
      "learning_rate": 1.6790161179436253e-05,
      "loss": 1.7387,
      "step": 347800
    },
    {
      "epoch": 26.575509892292416,
      "grad_norm": 6.797330379486084,
      "learning_rate": 1.678061263463448e-05,
      "loss": 1.6857,
      "step": 347900
    },
    {
      "epoch": 26.583148728133832,
      "grad_norm": 6.842964172363281,
      "learning_rate": 1.677106408983271e-05,
      "loss": 1.7574,
      "step": 348000
    },
    {
      "epoch": 26.59078756397525,
      "grad_norm": 7.698759078979492,
      "learning_rate": 1.6761515545030936e-05,
      "loss": 1.7277,
      "step": 348100
    },
    {
      "epoch": 26.598426399816667,
      "grad_norm": 7.3814167976379395,
      "learning_rate": 1.6751967000229164e-05,
      "loss": 1.7663,
      "step": 348200
    },
    {
      "epoch": 26.606065235658086,
      "grad_norm": 8.360140800476074,
      "learning_rate": 1.6742418455427393e-05,
      "loss": 1.657,
      "step": 348300
    },
    {
      "epoch": 26.613704071499505,
      "grad_norm": 6.127201080322266,
      "learning_rate": 1.6732869910625622e-05,
      "loss": 1.7075,
      "step": 348400
    },
    {
      "epoch": 26.62134290734092,
      "grad_norm": 9.215860366821289,
      "learning_rate": 1.672332136582385e-05,
      "loss": 1.7323,
      "step": 348500
    },
    {
      "epoch": 26.62898174318234,
      "grad_norm": 6.2058234214782715,
      "learning_rate": 1.671377282102208e-05,
      "loss": 1.7839,
      "step": 348600
    },
    {
      "epoch": 26.636620579023756,
      "grad_norm": 7.427937984466553,
      "learning_rate": 1.6704224276220304e-05,
      "loss": 1.6526,
      "step": 348700
    },
    {
      "epoch": 26.644259414865175,
      "grad_norm": 7.435171604156494,
      "learning_rate": 1.6694675731418533e-05,
      "loss": 1.8299,
      "step": 348800
    },
    {
      "epoch": 26.65189825070659,
      "grad_norm": 6.768707752227783,
      "learning_rate": 1.6685127186616762e-05,
      "loss": 1.6821,
      "step": 348900
    },
    {
      "epoch": 26.65953708654801,
      "grad_norm": 7.597314357757568,
      "learning_rate": 1.6675578641814987e-05,
      "loss": 1.7844,
      "step": 349000
    },
    {
      "epoch": 26.66717592238943,
      "grad_norm": 6.479342460632324,
      "learning_rate": 1.6666030097013216e-05,
      "loss": 1.7732,
      "step": 349100
    },
    {
      "epoch": 26.674814758230845,
      "grad_norm": 7.49653959274292,
      "learning_rate": 1.6656481552211445e-05,
      "loss": 1.7414,
      "step": 349200
    },
    {
      "epoch": 26.682453594072264,
      "grad_norm": 6.801429748535156,
      "learning_rate": 1.664693300740967e-05,
      "loss": 1.7992,
      "step": 349300
    },
    {
      "epoch": 26.69009242991368,
      "grad_norm": 8.061182975769043,
      "learning_rate": 1.66373844626079e-05,
      "loss": 1.6333,
      "step": 349400
    },
    {
      "epoch": 26.6977312657551,
      "grad_norm": 6.817072868347168,
      "learning_rate": 1.6627835917806127e-05,
      "loss": 1.8651,
      "step": 349500
    },
    {
      "epoch": 26.705370101596518,
      "grad_norm": 7.695119857788086,
      "learning_rate": 1.6618287373004356e-05,
      "loss": 1.6533,
      "step": 349600
    },
    {
      "epoch": 26.713008937437934,
      "grad_norm": 7.696572303771973,
      "learning_rate": 1.6608738828202585e-05,
      "loss": 1.7924,
      "step": 349700
    },
    {
      "epoch": 26.720647773279353,
      "grad_norm": 5.779927730560303,
      "learning_rate": 1.6599190283400813e-05,
      "loss": 1.7535,
      "step": 349800
    },
    {
      "epoch": 26.72828660912077,
      "grad_norm": 6.683744430541992,
      "learning_rate": 1.658964173859904e-05,
      "loss": 1.762,
      "step": 349900
    },
    {
      "epoch": 26.735925444962188,
      "grad_norm": 6.697628974914551,
      "learning_rate": 1.6580093193797267e-05,
      "loss": 1.7566,
      "step": 350000
    },
    {
      "epoch": 26.743564280803607,
      "grad_norm": 8.558892250061035,
      "learning_rate": 1.6570544648995496e-05,
      "loss": 1.7493,
      "step": 350100
    },
    {
      "epoch": 26.751203116645023,
      "grad_norm": 6.517202854156494,
      "learning_rate": 1.656099610419372e-05,
      "loss": 1.7033,
      "step": 350200
    },
    {
      "epoch": 26.758841952486442,
      "grad_norm": 6.351068496704102,
      "learning_rate": 1.655144755939195e-05,
      "loss": 1.7622,
      "step": 350300
    },
    {
      "epoch": 26.766480788327858,
      "grad_norm": 6.842100620269775,
      "learning_rate": 1.6541899014590175e-05,
      "loss": 1.9591,
      "step": 350400
    },
    {
      "epoch": 26.774119624169277,
      "grad_norm": 6.817836761474609,
      "learning_rate": 1.6532350469788404e-05,
      "loss": 1.7728,
      "step": 350500
    },
    {
      "epoch": 26.781758460010693,
      "grad_norm": 7.0844244956970215,
      "learning_rate": 1.6522801924986633e-05,
      "loss": 1.6722,
      "step": 350600
    },
    {
      "epoch": 26.789397295852112,
      "grad_norm": 8.081238746643066,
      "learning_rate": 1.6513253380184858e-05,
      "loss": 1.7704,
      "step": 350700
    },
    {
      "epoch": 26.79703613169353,
      "grad_norm": 9.311211585998535,
      "learning_rate": 1.6503704835383087e-05,
      "loss": 1.7725,
      "step": 350800
    },
    {
      "epoch": 26.804674967534947,
      "grad_norm": 8.016634941101074,
      "learning_rate": 1.6494156290581315e-05,
      "loss": 1.7158,
      "step": 350900
    },
    {
      "epoch": 26.812313803376366,
      "grad_norm": 9.096275329589844,
      "learning_rate": 1.6484607745779544e-05,
      "loss": 1.7296,
      "step": 351000
    },
    {
      "epoch": 26.81995263921778,
      "grad_norm": 11.111652374267578,
      "learning_rate": 1.6475059200977773e-05,
      "loss": 1.8171,
      "step": 351100
    },
    {
      "epoch": 26.8275914750592,
      "grad_norm": 8.071196556091309,
      "learning_rate": 1.6465510656176e-05,
      "loss": 1.7955,
      "step": 351200
    },
    {
      "epoch": 26.83523031090062,
      "grad_norm": 7.654961109161377,
      "learning_rate": 1.6455962111374227e-05,
      "loss": 1.7387,
      "step": 351300
    },
    {
      "epoch": 26.842869146742036,
      "grad_norm": 8.474392890930176,
      "learning_rate": 1.6446413566572455e-05,
      "loss": 1.7507,
      "step": 351400
    },
    {
      "epoch": 26.850507982583455,
      "grad_norm": 8.825450897216797,
      "learning_rate": 1.6436865021770684e-05,
      "loss": 1.6642,
      "step": 351500
    },
    {
      "epoch": 26.85814681842487,
      "grad_norm": 7.9554524421691895,
      "learning_rate": 1.642731647696891e-05,
      "loss": 1.7964,
      "step": 351600
    },
    {
      "epoch": 26.86578565426629,
      "grad_norm": 9.413352012634277,
      "learning_rate": 1.6417767932167138e-05,
      "loss": 1.7091,
      "step": 351700
    },
    {
      "epoch": 26.87342449010771,
      "grad_norm": 7.6903533935546875,
      "learning_rate": 1.6408219387365367e-05,
      "loss": 1.6889,
      "step": 351800
    },
    {
      "epoch": 26.881063325949125,
      "grad_norm": 7.740516662597656,
      "learning_rate": 1.6398670842563592e-05,
      "loss": 1.7475,
      "step": 351900
    },
    {
      "epoch": 26.888702161790544,
      "grad_norm": 8.344500541687012,
      "learning_rate": 1.638912229776182e-05,
      "loss": 1.7797,
      "step": 352000
    },
    {
      "epoch": 26.89634099763196,
      "grad_norm": 10.256415367126465,
      "learning_rate": 1.637957375296005e-05,
      "loss": 1.7599,
      "step": 352100
    },
    {
      "epoch": 26.90397983347338,
      "grad_norm": 8.733173370361328,
      "learning_rate": 1.6370025208158278e-05,
      "loss": 1.7805,
      "step": 352200
    },
    {
      "epoch": 26.911618669314798,
      "grad_norm": 7.754538059234619,
      "learning_rate": 1.6360476663356507e-05,
      "loss": 1.7325,
      "step": 352300
    },
    {
      "epoch": 26.919257505156214,
      "grad_norm": 8.237996101379395,
      "learning_rate": 1.6350928118554736e-05,
      "loss": 1.7443,
      "step": 352400
    },
    {
      "epoch": 26.926896340997633,
      "grad_norm": 6.712337017059326,
      "learning_rate": 1.634137957375296e-05,
      "loss": 1.7067,
      "step": 352500
    },
    {
      "epoch": 26.93453517683905,
      "grad_norm": 8.795464515686035,
      "learning_rate": 1.633183102895119e-05,
      "loss": 1.6967,
      "step": 352600
    },
    {
      "epoch": 26.942174012680468,
      "grad_norm": 7.392759799957275,
      "learning_rate": 1.6322282484149418e-05,
      "loss": 1.7709,
      "step": 352700
    },
    {
      "epoch": 26.949812848521887,
      "grad_norm": 10.614913940429688,
      "learning_rate": 1.6312733939347644e-05,
      "loss": 1.8023,
      "step": 352800
    },
    {
      "epoch": 26.957451684363303,
      "grad_norm": 7.689861297607422,
      "learning_rate": 1.6303185394545872e-05,
      "loss": 1.7394,
      "step": 352900
    },
    {
      "epoch": 26.965090520204722,
      "grad_norm": 9.254685401916504,
      "learning_rate": 1.62936368497441e-05,
      "loss": 1.7384,
      "step": 353000
    },
    {
      "epoch": 26.972729356046138,
      "grad_norm": 6.811893463134766,
      "learning_rate": 1.6284088304942326e-05,
      "loss": 1.7145,
      "step": 353100
    },
    {
      "epoch": 26.980368191887557,
      "grad_norm": 5.493240833282471,
      "learning_rate": 1.6274539760140555e-05,
      "loss": 1.7931,
      "step": 353200
    },
    {
      "epoch": 26.988007027728973,
      "grad_norm": 5.861467361450195,
      "learning_rate": 1.626499121533878e-05,
      "loss": 1.7664,
      "step": 353300
    },
    {
      "epoch": 26.99564586357039,
      "grad_norm": 8.374265670776367,
      "learning_rate": 1.625544267053701e-05,
      "loss": 1.7439,
      "step": 353400
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.7994333505630493,
      "eval_runtime": 2.9721,
      "eval_samples_per_second": 232.159,
      "eval_steps_per_second": 232.159,
      "step": 353457
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.5069167613983154,
      "eval_runtime": 56.3509,
      "eval_samples_per_second": 232.312,
      "eval_steps_per_second": 232.312,
      "step": 353457
    },
    {
      "epoch": 27.00328469941181,
      "grad_norm": 6.801314830780029,
      "learning_rate": 1.6245894125735238e-05,
      "loss": 1.8175,
      "step": 353500
    },
    {
      "epoch": 27.010923535253227,
      "grad_norm": 8.490996360778809,
      "learning_rate": 1.6236345580933466e-05,
      "loss": 1.6278,
      "step": 353600
    },
    {
      "epoch": 27.018562371094646,
      "grad_norm": 6.673489570617676,
      "learning_rate": 1.6226797036131695e-05,
      "loss": 1.6925,
      "step": 353700
    },
    {
      "epoch": 27.02620120693606,
      "grad_norm": 8.012910842895508,
      "learning_rate": 1.6217248491329924e-05,
      "loss": 1.7215,
      "step": 353800
    },
    {
      "epoch": 27.03384004277748,
      "grad_norm": 8.383406639099121,
      "learning_rate": 1.620769994652815e-05,
      "loss": 1.8003,
      "step": 353900
    },
    {
      "epoch": 27.0414788786189,
      "grad_norm": 8.424145698547363,
      "learning_rate": 1.6198151401726378e-05,
      "loss": 1.7903,
      "step": 354000
    },
    {
      "epoch": 27.049117714460316,
      "grad_norm": 6.347095012664795,
      "learning_rate": 1.6188602856924606e-05,
      "loss": 1.7881,
      "step": 354100
    },
    {
      "epoch": 27.056756550301735,
      "grad_norm": 6.168262958526611,
      "learning_rate": 1.6179054312122832e-05,
      "loss": 1.7181,
      "step": 354200
    },
    {
      "epoch": 27.06439538614315,
      "grad_norm": 7.246547698974609,
      "learning_rate": 1.616950576732106e-05,
      "loss": 1.7151,
      "step": 354300
    },
    {
      "epoch": 27.07203422198457,
      "grad_norm": 9.332073211669922,
      "learning_rate": 1.615995722251929e-05,
      "loss": 1.8238,
      "step": 354400
    },
    {
      "epoch": 27.07967305782599,
      "grad_norm": 5.399816036224365,
      "learning_rate": 1.6150408677717514e-05,
      "loss": 1.6563,
      "step": 354500
    },
    {
      "epoch": 27.087311893667405,
      "grad_norm": 8.333868026733398,
      "learning_rate": 1.6140860132915743e-05,
      "loss": 1.7417,
      "step": 354600
    },
    {
      "epoch": 27.094950729508824,
      "grad_norm": 8.956005096435547,
      "learning_rate": 1.6131311588113972e-05,
      "loss": 1.7115,
      "step": 354700
    },
    {
      "epoch": 27.10258956535024,
      "grad_norm": 7.843566417694092,
      "learning_rate": 1.61217630433122e-05,
      "loss": 1.8321,
      "step": 354800
    },
    {
      "epoch": 27.11022840119166,
      "grad_norm": 7.25788688659668,
      "learning_rate": 1.611221449851043e-05,
      "loss": 1.7834,
      "step": 354900
    },
    {
      "epoch": 27.117867237033074,
      "grad_norm": 6.663064479827881,
      "learning_rate": 1.6102665953708658e-05,
      "loss": 1.7098,
      "step": 355000
    },
    {
      "epoch": 27.125506072874494,
      "grad_norm": 7.202954292297363,
      "learning_rate": 1.6093117408906883e-05,
      "loss": 1.6551,
      "step": 355100
    },
    {
      "epoch": 27.133144908715913,
      "grad_norm": 10.743228912353516,
      "learning_rate": 1.6083568864105112e-05,
      "loss": 1.7583,
      "step": 355200
    },
    {
      "epoch": 27.14078374455733,
      "grad_norm": 7.083704471588135,
      "learning_rate": 1.607402031930334e-05,
      "loss": 1.7172,
      "step": 355300
    },
    {
      "epoch": 27.148422580398748,
      "grad_norm": 7.811010837554932,
      "learning_rate": 1.6064471774501566e-05,
      "loss": 1.8005,
      "step": 355400
    },
    {
      "epoch": 27.156061416240163,
      "grad_norm": 7.208825588226318,
      "learning_rate": 1.6054923229699795e-05,
      "loss": 1.6884,
      "step": 355500
    },
    {
      "epoch": 27.163700252081583,
      "grad_norm": 8.700272560119629,
      "learning_rate": 1.6045374684898023e-05,
      "loss": 1.78,
      "step": 355600
    },
    {
      "epoch": 27.171339087923002,
      "grad_norm": 4.057047367095947,
      "learning_rate": 1.603582614009625e-05,
      "loss": 1.6747,
      "step": 355700
    },
    {
      "epoch": 27.178977923764418,
      "grad_norm": 9.654265403747559,
      "learning_rate": 1.6026277595294477e-05,
      "loss": 1.7433,
      "step": 355800
    },
    {
      "epoch": 27.186616759605837,
      "grad_norm": 7.93416166305542,
      "learning_rate": 1.6016729050492706e-05,
      "loss": 1.7357,
      "step": 355900
    },
    {
      "epoch": 27.194255595447252,
      "grad_norm": 5.1107707023620605,
      "learning_rate": 1.6007180505690935e-05,
      "loss": 1.7208,
      "step": 356000
    },
    {
      "epoch": 27.20189443128867,
      "grad_norm": 14.496049880981445,
      "learning_rate": 1.5997631960889163e-05,
      "loss": 1.8189,
      "step": 356100
    },
    {
      "epoch": 27.20953326713009,
      "grad_norm": 6.0135040283203125,
      "learning_rate": 1.598808341608739e-05,
      "loss": 1.7689,
      "step": 356200
    },
    {
      "epoch": 27.217172102971507,
      "grad_norm": 5.007671356201172,
      "learning_rate": 1.5978534871285617e-05,
      "loss": 1.7403,
      "step": 356300
    },
    {
      "epoch": 27.224810938812926,
      "grad_norm": 6.91925573348999,
      "learning_rate": 1.5968986326483846e-05,
      "loss": 1.8001,
      "step": 356400
    },
    {
      "epoch": 27.23244977465434,
      "grad_norm": 6.581614971160889,
      "learning_rate": 1.595943778168207e-05,
      "loss": 1.6689,
      "step": 356500
    },
    {
      "epoch": 27.24008861049576,
      "grad_norm": 6.339712142944336,
      "learning_rate": 1.59498892368803e-05,
      "loss": 1.7788,
      "step": 356600
    },
    {
      "epoch": 27.24772744633718,
      "grad_norm": 5.8517680168151855,
      "learning_rate": 1.594034069207853e-05,
      "loss": 1.7478,
      "step": 356700
    },
    {
      "epoch": 27.255366282178596,
      "grad_norm": 8.294599533081055,
      "learning_rate": 1.5930792147276754e-05,
      "loss": 1.7367,
      "step": 356800
    },
    {
      "epoch": 27.263005118020015,
      "grad_norm": 7.568295001983643,
      "learning_rate": 1.5921243602474983e-05,
      "loss": 1.6763,
      "step": 356900
    },
    {
      "epoch": 27.27064395386143,
      "grad_norm": 7.877847194671631,
      "learning_rate": 1.591169505767321e-05,
      "loss": 1.6776,
      "step": 357000
    },
    {
      "epoch": 27.27828278970285,
      "grad_norm": 5.306056499481201,
      "learning_rate": 1.5902146512871437e-05,
      "loss": 1.7017,
      "step": 357100
    },
    {
      "epoch": 27.285921625544265,
      "grad_norm": 9.232086181640625,
      "learning_rate": 1.5892597968069665e-05,
      "loss": 1.6612,
      "step": 357200
    },
    {
      "epoch": 27.293560461385685,
      "grad_norm": 5.705577373504639,
      "learning_rate": 1.5883049423267894e-05,
      "loss": 1.7034,
      "step": 357300
    },
    {
      "epoch": 27.301199297227104,
      "grad_norm": 7.216434001922607,
      "learning_rate": 1.5873500878466123e-05,
      "loss": 1.7942,
      "step": 357400
    },
    {
      "epoch": 27.30883813306852,
      "grad_norm": 7.539292335510254,
      "learning_rate": 1.586395233366435e-05,
      "loss": 1.7436,
      "step": 357500
    },
    {
      "epoch": 27.31647696890994,
      "grad_norm": 9.189743041992188,
      "learning_rate": 1.585440378886258e-05,
      "loss": 1.7123,
      "step": 357600
    },
    {
      "epoch": 27.324115804751354,
      "grad_norm": 9.225152969360352,
      "learning_rate": 1.5844855244060806e-05,
      "loss": 1.7229,
      "step": 357700
    },
    {
      "epoch": 27.331754640592774,
      "grad_norm": 7.105591297149658,
      "learning_rate": 1.5835306699259034e-05,
      "loss": 1.729,
      "step": 357800
    },
    {
      "epoch": 27.339393476434193,
      "grad_norm": 12.425490379333496,
      "learning_rate": 1.5825758154457263e-05,
      "loss": 1.6148,
      "step": 357900
    },
    {
      "epoch": 27.34703231227561,
      "grad_norm": 7.108888149261475,
      "learning_rate": 1.5816209609655488e-05,
      "loss": 1.7867,
      "step": 358000
    },
    {
      "epoch": 27.354671148117028,
      "grad_norm": 7.216751575469971,
      "learning_rate": 1.5806661064853717e-05,
      "loss": 1.739,
      "step": 358100
    },
    {
      "epoch": 27.362309983958443,
      "grad_norm": 5.558475017547607,
      "learning_rate": 1.5797112520051946e-05,
      "loss": 1.7368,
      "step": 358200
    },
    {
      "epoch": 27.369948819799863,
      "grad_norm": 5.723504066467285,
      "learning_rate": 1.578756397525017e-05,
      "loss": 1.6737,
      "step": 358300
    },
    {
      "epoch": 27.377587655641282,
      "grad_norm": 7.0610246658325195,
      "learning_rate": 1.57780154304484e-05,
      "loss": 1.7651,
      "step": 358400
    },
    {
      "epoch": 27.385226491482697,
      "grad_norm": 8.549152374267578,
      "learning_rate": 1.5768466885646628e-05,
      "loss": 1.7864,
      "step": 358500
    },
    {
      "epoch": 27.392865327324117,
      "grad_norm": 6.351821422576904,
      "learning_rate": 1.5758918340844857e-05,
      "loss": 1.6868,
      "step": 358600
    },
    {
      "epoch": 27.400504163165532,
      "grad_norm": 8.199076652526855,
      "learning_rate": 1.5749369796043086e-05,
      "loss": 1.6639,
      "step": 358700
    },
    {
      "epoch": 27.40814299900695,
      "grad_norm": 7.799257755279541,
      "learning_rate": 1.5739821251241314e-05,
      "loss": 1.7618,
      "step": 358800
    },
    {
      "epoch": 27.41578183484837,
      "grad_norm": 8.099334716796875,
      "learning_rate": 1.573027270643954e-05,
      "loss": 1.8112,
      "step": 358900
    },
    {
      "epoch": 27.423420670689787,
      "grad_norm": 7.23398494720459,
      "learning_rate": 1.572072416163777e-05,
      "loss": 1.6764,
      "step": 359000
    },
    {
      "epoch": 27.431059506531206,
      "grad_norm": 7.369584083557129,
      "learning_rate": 1.5711175616835994e-05,
      "loss": 1.8492,
      "step": 359100
    },
    {
      "epoch": 27.43869834237262,
      "grad_norm": 9.55711555480957,
      "learning_rate": 1.5701627072034222e-05,
      "loss": 1.8237,
      "step": 359200
    },
    {
      "epoch": 27.44633717821404,
      "grad_norm": 7.655033588409424,
      "learning_rate": 1.569207852723245e-05,
      "loss": 1.8122,
      "step": 359300
    },
    {
      "epoch": 27.453976014055456,
      "grad_norm": 9.960684776306152,
      "learning_rate": 1.5682529982430676e-05,
      "loss": 1.7238,
      "step": 359400
    },
    {
      "epoch": 27.461614849896876,
      "grad_norm": 12.413873672485352,
      "learning_rate": 1.5672981437628905e-05,
      "loss": 1.7904,
      "step": 359500
    },
    {
      "epoch": 27.469253685738295,
      "grad_norm": 6.753123760223389,
      "learning_rate": 1.5663432892827134e-05,
      "loss": 1.705,
      "step": 359600
    },
    {
      "epoch": 27.47689252157971,
      "grad_norm": 7.745940685272217,
      "learning_rate": 1.565388434802536e-05,
      "loss": 1.8216,
      "step": 359700
    },
    {
      "epoch": 27.48453135742113,
      "grad_norm": 8.150315284729004,
      "learning_rate": 1.5644335803223588e-05,
      "loss": 1.629,
      "step": 359800
    },
    {
      "epoch": 27.492170193262545,
      "grad_norm": 7.585042953491211,
      "learning_rate": 1.5634787258421816e-05,
      "loss": 1.6869,
      "step": 359900
    },
    {
      "epoch": 27.499809029103965,
      "grad_norm": 6.883453845977783,
      "learning_rate": 1.5625238713620045e-05,
      "loss": 1.6652,
      "step": 360000
    },
    {
      "epoch": 27.507447864945384,
      "grad_norm": 8.125273704528809,
      "learning_rate": 1.5615690168818274e-05,
      "loss": 1.7465,
      "step": 360100
    },
    {
      "epoch": 27.5150867007868,
      "grad_norm": 6.258383274078369,
      "learning_rate": 1.5606141624016503e-05,
      "loss": 1.7516,
      "step": 360200
    },
    {
      "epoch": 27.52272553662822,
      "grad_norm": 6.443289279937744,
      "learning_rate": 1.5596593079214728e-05,
      "loss": 1.8054,
      "step": 360300
    },
    {
      "epoch": 27.530364372469634,
      "grad_norm": 6.00265645980835,
      "learning_rate": 1.5587044534412957e-05,
      "loss": 1.8112,
      "step": 360400
    },
    {
      "epoch": 27.538003208311054,
      "grad_norm": 10.10006046295166,
      "learning_rate": 1.5577495989611185e-05,
      "loss": 1.6892,
      "step": 360500
    },
    {
      "epoch": 27.545642044152473,
      "grad_norm": 6.934227466583252,
      "learning_rate": 1.556794744480941e-05,
      "loss": 1.7665,
      "step": 360600
    },
    {
      "epoch": 27.55328087999389,
      "grad_norm": 9.562941551208496,
      "learning_rate": 1.555839890000764e-05,
      "loss": 1.7083,
      "step": 360700
    },
    {
      "epoch": 27.560919715835308,
      "grad_norm": 5.8292412757873535,
      "learning_rate": 1.5548850355205868e-05,
      "loss": 1.7036,
      "step": 360800
    },
    {
      "epoch": 27.568558551676723,
      "grad_norm": 9.311787605285645,
      "learning_rate": 1.5539301810404093e-05,
      "loss": 1.6978,
      "step": 360900
    },
    {
      "epoch": 27.576197387518143,
      "grad_norm": 7.505264759063721,
      "learning_rate": 1.5529753265602322e-05,
      "loss": 1.8118,
      "step": 361000
    },
    {
      "epoch": 27.58383622335956,
      "grad_norm": 8.331161499023438,
      "learning_rate": 1.552020472080055e-05,
      "loss": 1.7306,
      "step": 361100
    },
    {
      "epoch": 27.591475059200977,
      "grad_norm": 6.541897773742676,
      "learning_rate": 1.551065617599878e-05,
      "loss": 1.8542,
      "step": 361200
    },
    {
      "epoch": 27.599113895042397,
      "grad_norm": 7.788546562194824,
      "learning_rate": 1.5501107631197008e-05,
      "loss": 1.6194,
      "step": 361300
    },
    {
      "epoch": 27.606752730883812,
      "grad_norm": 8.877016067504883,
      "learning_rate": 1.5491559086395237e-05,
      "loss": 1.7833,
      "step": 361400
    },
    {
      "epoch": 27.61439156672523,
      "grad_norm": 7.4667744636535645,
      "learning_rate": 1.5482010541593462e-05,
      "loss": 1.7332,
      "step": 361500
    },
    {
      "epoch": 27.622030402566647,
      "grad_norm": 8.037275314331055,
      "learning_rate": 1.547246199679169e-05,
      "loss": 1.6484,
      "step": 361600
    },
    {
      "epoch": 27.629669238408066,
      "grad_norm": 8.421417236328125,
      "learning_rate": 1.546291345198992e-05,
      "loss": 1.7688,
      "step": 361700
    },
    {
      "epoch": 27.637308074249486,
      "grad_norm": 8.0711669921875,
      "learning_rate": 1.5453364907188145e-05,
      "loss": 1.7644,
      "step": 361800
    },
    {
      "epoch": 27.6449469100909,
      "grad_norm": 6.548343658447266,
      "learning_rate": 1.5443816362386373e-05,
      "loss": 1.6038,
      "step": 361900
    },
    {
      "epoch": 27.65258574593232,
      "grad_norm": 8.78615951538086,
      "learning_rate": 1.54342678175846e-05,
      "loss": 1.6623,
      "step": 362000
    },
    {
      "epoch": 27.660224581773736,
      "grad_norm": 7.765559196472168,
      "learning_rate": 1.5424719272782827e-05,
      "loss": 1.6253,
      "step": 362100
    },
    {
      "epoch": 27.667863417615155,
      "grad_norm": 7.073924541473389,
      "learning_rate": 1.5415170727981056e-05,
      "loss": 1.8666,
      "step": 362200
    },
    {
      "epoch": 27.675502253456575,
      "grad_norm": 6.633662223815918,
      "learning_rate": 1.540562218317928e-05,
      "loss": 1.7668,
      "step": 362300
    },
    {
      "epoch": 27.68314108929799,
      "grad_norm": 7.038923263549805,
      "learning_rate": 1.539607363837751e-05,
      "loss": 1.7216,
      "step": 362400
    },
    {
      "epoch": 27.69077992513941,
      "grad_norm": 7.512940883636475,
      "learning_rate": 1.5386525093575742e-05,
      "loss": 1.657,
      "step": 362500
    },
    {
      "epoch": 27.698418760980825,
      "grad_norm": 5.7381720542907715,
      "learning_rate": 1.5376976548773967e-05,
      "loss": 1.6694,
      "step": 362600
    },
    {
      "epoch": 27.706057596822244,
      "grad_norm": 6.776025295257568,
      "learning_rate": 1.5367428003972196e-05,
      "loss": 1.7417,
      "step": 362700
    },
    {
      "epoch": 27.713696432663664,
      "grad_norm": 8.817004203796387,
      "learning_rate": 1.5357879459170425e-05,
      "loss": 1.6691,
      "step": 362800
    },
    {
      "epoch": 27.72133526850508,
      "grad_norm": 8.292085647583008,
      "learning_rate": 1.534833091436865e-05,
      "loss": 1.7852,
      "step": 362900
    },
    {
      "epoch": 27.7289741043465,
      "grad_norm": 5.939583778381348,
      "learning_rate": 1.533878236956688e-05,
      "loss": 1.755,
      "step": 363000
    },
    {
      "epoch": 27.736612940187914,
      "grad_norm": 8.656103134155273,
      "learning_rate": 1.5329233824765107e-05,
      "loss": 1.7103,
      "step": 363100
    },
    {
      "epoch": 27.744251776029333,
      "grad_norm": 6.875808238983154,
      "learning_rate": 1.5319685279963333e-05,
      "loss": 1.7537,
      "step": 363200
    },
    {
      "epoch": 27.75189061187075,
      "grad_norm": 6.733249664306641,
      "learning_rate": 1.531013673516156e-05,
      "loss": 1.7828,
      "step": 363300
    },
    {
      "epoch": 27.75952944771217,
      "grad_norm": 7.951796054840088,
      "learning_rate": 1.530058819035979e-05,
      "loss": 1.7601,
      "step": 363400
    },
    {
      "epoch": 27.767168283553588,
      "grad_norm": 8.301880836486816,
      "learning_rate": 1.5291039645558015e-05,
      "loss": 1.7972,
      "step": 363500
    },
    {
      "epoch": 27.774807119395003,
      "grad_norm": 3.2351107597351074,
      "learning_rate": 1.5281491100756244e-05,
      "loss": 1.7049,
      "step": 363600
    },
    {
      "epoch": 27.782445955236422,
      "grad_norm": 10.51971435546875,
      "learning_rate": 1.5271942555954473e-05,
      "loss": 1.7282,
      "step": 363700
    },
    {
      "epoch": 27.790084791077838,
      "grad_norm": 11.260645866394043,
      "learning_rate": 1.52623940111527e-05,
      "loss": 1.8418,
      "step": 363800
    },
    {
      "epoch": 27.797723626919257,
      "grad_norm": 7.368049621582031,
      "learning_rate": 1.5252845466350929e-05,
      "loss": 1.7756,
      "step": 363900
    },
    {
      "epoch": 27.805362462760677,
      "grad_norm": 7.19425630569458,
      "learning_rate": 1.5243296921549157e-05,
      "loss": 1.7479,
      "step": 364000
    },
    {
      "epoch": 27.813001298602092,
      "grad_norm": 8.072525978088379,
      "learning_rate": 1.5233748376747384e-05,
      "loss": 1.7896,
      "step": 364100
    },
    {
      "epoch": 27.82064013444351,
      "grad_norm": 8.581159591674805,
      "learning_rate": 1.5224199831945613e-05,
      "loss": 1.7274,
      "step": 364200
    },
    {
      "epoch": 27.828278970284927,
      "grad_norm": 7.945428848266602,
      "learning_rate": 1.5214651287143842e-05,
      "loss": 1.7353,
      "step": 364300
    },
    {
      "epoch": 27.835917806126346,
      "grad_norm": 8.290781021118164,
      "learning_rate": 1.5205102742342067e-05,
      "loss": 1.7691,
      "step": 364400
    },
    {
      "epoch": 27.843556641967766,
      "grad_norm": 9.5606107711792,
      "learning_rate": 1.5195554197540296e-05,
      "loss": 1.6759,
      "step": 364500
    },
    {
      "epoch": 27.85119547780918,
      "grad_norm": 9.858778953552246,
      "learning_rate": 1.5186005652738524e-05,
      "loss": 1.7301,
      "step": 364600
    },
    {
      "epoch": 27.8588343136506,
      "grad_norm": 7.077622413635254,
      "learning_rate": 1.5176457107936751e-05,
      "loss": 1.8223,
      "step": 364700
    },
    {
      "epoch": 27.866473149492016,
      "grad_norm": 4.002991199493408,
      "learning_rate": 1.516690856313498e-05,
      "loss": 1.7834,
      "step": 364800
    },
    {
      "epoch": 27.874111985333435,
      "grad_norm": 5.809868812561035,
      "learning_rate": 1.5157360018333205e-05,
      "loss": 1.6677,
      "step": 364900
    },
    {
      "epoch": 27.881750821174855,
      "grad_norm": 7.890819549560547,
      "learning_rate": 1.5147811473531434e-05,
      "loss": 1.7907,
      "step": 365000
    },
    {
      "epoch": 27.88938965701627,
      "grad_norm": 7.777173042297363,
      "learning_rate": 1.5138262928729663e-05,
      "loss": 1.6714,
      "step": 365100
    },
    {
      "epoch": 27.89702849285769,
      "grad_norm": 9.105986595153809,
      "learning_rate": 1.5128714383927888e-05,
      "loss": 1.76,
      "step": 365200
    },
    {
      "epoch": 27.904667328699105,
      "grad_norm": 6.96569299697876,
      "learning_rate": 1.5119165839126118e-05,
      "loss": 1.6684,
      "step": 365300
    },
    {
      "epoch": 27.912306164540524,
      "grad_norm": 7.608011722564697,
      "learning_rate": 1.5109617294324347e-05,
      "loss": 1.8493,
      "step": 365400
    },
    {
      "epoch": 27.919945000381944,
      "grad_norm": 9.012374877929688,
      "learning_rate": 1.5100068749522572e-05,
      "loss": 1.8168,
      "step": 365500
    },
    {
      "epoch": 27.92758383622336,
      "grad_norm": 9.740509986877441,
      "learning_rate": 1.5090520204720801e-05,
      "loss": 1.6861,
      "step": 365600
    },
    {
      "epoch": 27.93522267206478,
      "grad_norm": 6.2742600440979,
      "learning_rate": 1.508097165991903e-05,
      "loss": 1.8,
      "step": 365700
    },
    {
      "epoch": 27.942861507906194,
      "grad_norm": 8.559272766113281,
      "learning_rate": 1.5071423115117255e-05,
      "loss": 1.7059,
      "step": 365800
    },
    {
      "epoch": 27.950500343747613,
      "grad_norm": 9.939861297607422,
      "learning_rate": 1.5061874570315484e-05,
      "loss": 1.6763,
      "step": 365900
    },
    {
      "epoch": 27.95813917958903,
      "grad_norm": 5.91610860824585,
      "learning_rate": 1.5052326025513712e-05,
      "loss": 1.7307,
      "step": 366000
    },
    {
      "epoch": 27.96577801543045,
      "grad_norm": 7.132079124450684,
      "learning_rate": 1.504277748071194e-05,
      "loss": 1.834,
      "step": 366100
    },
    {
      "epoch": 27.973416851271868,
      "grad_norm": 7.273199558258057,
      "learning_rate": 1.5033228935910168e-05,
      "loss": 1.7308,
      "step": 366200
    },
    {
      "epoch": 27.981055687113283,
      "grad_norm": 9.624832153320312,
      "learning_rate": 1.5023680391108397e-05,
      "loss": 1.8249,
      "step": 366300
    },
    {
      "epoch": 27.988694522954702,
      "grad_norm": 7.729089260101318,
      "learning_rate": 1.5014131846306622e-05,
      "loss": 1.8093,
      "step": 366400
    },
    {
      "epoch": 27.996333358796118,
      "grad_norm": 7.3742780685424805,
      "learning_rate": 1.5004583301504851e-05,
      "loss": 1.7995,
      "step": 366500
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.7960841655731201,
      "eval_runtime": 1.7542,
      "eval_samples_per_second": 393.334,
      "eval_steps_per_second": 393.334,
      "step": 366548
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.5027244091033936,
      "eval_runtime": 32.2302,
      "eval_samples_per_second": 406.172,
      "eval_steps_per_second": 406.172,
      "step": 366548
    },
    {
      "epoch": 28.003972194637537,
      "grad_norm": 6.633873462677002,
      "learning_rate": 1.499503475670308e-05,
      "loss": 1.7661,
      "step": 366600
    },
    {
      "epoch": 28.011611030478957,
      "grad_norm": 9.231110572814941,
      "learning_rate": 1.4985486211901307e-05,
      "loss": 1.7725,
      "step": 366700
    },
    {
      "epoch": 28.019249866320372,
      "grad_norm": 7.437419891357422,
      "learning_rate": 1.4975937667099535e-05,
      "loss": 1.7158,
      "step": 366800
    },
    {
      "epoch": 28.02688870216179,
      "grad_norm": 8.658095359802246,
      "learning_rate": 1.4966389122297764e-05,
      "loss": 1.8025,
      "step": 366900
    },
    {
      "epoch": 28.034527538003207,
      "grad_norm": 6.629137992858887,
      "learning_rate": 1.495684057749599e-05,
      "loss": 1.6952,
      "step": 367000
    },
    {
      "epoch": 28.042166373844626,
      "grad_norm": 9.652965545654297,
      "learning_rate": 1.4947292032694218e-05,
      "loss": 1.7016,
      "step": 367100
    },
    {
      "epoch": 28.049805209686046,
      "grad_norm": 7.504531383514404,
      "learning_rate": 1.4937743487892447e-05,
      "loss": 1.7553,
      "step": 367200
    },
    {
      "epoch": 28.05744404552746,
      "grad_norm": 7.68302059173584,
      "learning_rate": 1.4928194943090674e-05,
      "loss": 1.6497,
      "step": 367300
    },
    {
      "epoch": 28.06508288136888,
      "grad_norm": 6.589847087860107,
      "learning_rate": 1.4918646398288902e-05,
      "loss": 1.6207,
      "step": 367400
    },
    {
      "epoch": 28.072721717210296,
      "grad_norm": 7.351074695587158,
      "learning_rate": 1.4909097853487131e-05,
      "loss": 1.7591,
      "step": 367500
    },
    {
      "epoch": 28.080360553051715,
      "grad_norm": 12.136659622192383,
      "learning_rate": 1.4899549308685356e-05,
      "loss": 1.6736,
      "step": 367600
    },
    {
      "epoch": 28.08799938889313,
      "grad_norm": 8.917373657226562,
      "learning_rate": 1.4890000763883585e-05,
      "loss": 1.7542,
      "step": 367700
    },
    {
      "epoch": 28.09563822473455,
      "grad_norm": 8.65466594696045,
      "learning_rate": 1.4880452219081812e-05,
      "loss": 1.7375,
      "step": 367800
    },
    {
      "epoch": 28.10327706057597,
      "grad_norm": 9.793342590332031,
      "learning_rate": 1.487090367428004e-05,
      "loss": 1.6837,
      "step": 367900
    },
    {
      "epoch": 28.110915896417385,
      "grad_norm": 7.971443176269531,
      "learning_rate": 1.486135512947827e-05,
      "loss": 1.7822,
      "step": 368000
    },
    {
      "epoch": 28.118554732258804,
      "grad_norm": 8.061392784118652,
      "learning_rate": 1.4851806584676495e-05,
      "loss": 1.8541,
      "step": 368100
    },
    {
      "epoch": 28.12619356810022,
      "grad_norm": 6.63853120803833,
      "learning_rate": 1.4842258039874723e-05,
      "loss": 1.7239,
      "step": 368200
    },
    {
      "epoch": 28.13383240394164,
      "grad_norm": 7.177718639373779,
      "learning_rate": 1.4832709495072952e-05,
      "loss": 1.8341,
      "step": 368300
    },
    {
      "epoch": 28.14147123978306,
      "grad_norm": 6.039846897125244,
      "learning_rate": 1.4823160950271177e-05,
      "loss": 1.8053,
      "step": 368400
    },
    {
      "epoch": 28.149110075624474,
      "grad_norm": 7.737789630889893,
      "learning_rate": 1.4813612405469408e-05,
      "loss": 1.7754,
      "step": 368500
    },
    {
      "epoch": 28.156748911465893,
      "grad_norm": 7.0125732421875,
      "learning_rate": 1.4804063860667636e-05,
      "loss": 1.7149,
      "step": 368600
    },
    {
      "epoch": 28.16438774730731,
      "grad_norm": 8.225655555725098,
      "learning_rate": 1.4794515315865862e-05,
      "loss": 1.7658,
      "step": 368700
    },
    {
      "epoch": 28.172026583148728,
      "grad_norm": 7.980583190917969,
      "learning_rate": 1.478496677106409e-05,
      "loss": 1.762,
      "step": 368800
    },
    {
      "epoch": 28.179665418990147,
      "grad_norm": 5.049177169799805,
      "learning_rate": 1.477541822626232e-05,
      "loss": 1.7347,
      "step": 368900
    },
    {
      "epoch": 28.187304254831563,
      "grad_norm": 6.677426338195801,
      "learning_rate": 1.4765869681460544e-05,
      "loss": 1.7536,
      "step": 369000
    },
    {
      "epoch": 28.194943090672982,
      "grad_norm": 5.1405816078186035,
      "learning_rate": 1.4756321136658773e-05,
      "loss": 1.6548,
      "step": 369100
    },
    {
      "epoch": 28.202581926514398,
      "grad_norm": 7.229092597961426,
      "learning_rate": 1.4746772591857002e-05,
      "loss": 1.7285,
      "step": 369200
    },
    {
      "epoch": 28.210220762355817,
      "grad_norm": 8.443099975585938,
      "learning_rate": 1.4737224047055229e-05,
      "loss": 1.7965,
      "step": 369300
    },
    {
      "epoch": 28.217859598197236,
      "grad_norm": 9.635926246643066,
      "learning_rate": 1.4727675502253458e-05,
      "loss": 1.8778,
      "step": 369400
    },
    {
      "epoch": 28.225498434038652,
      "grad_norm": 7.710989475250244,
      "learning_rate": 1.4718126957451686e-05,
      "loss": 1.6763,
      "step": 369500
    },
    {
      "epoch": 28.23313726988007,
      "grad_norm": 7.778542995452881,
      "learning_rate": 1.4708578412649912e-05,
      "loss": 1.7048,
      "step": 369600
    },
    {
      "epoch": 28.240776105721487,
      "grad_norm": 8.378094673156738,
      "learning_rate": 1.469902986784814e-05,
      "loss": 1.7481,
      "step": 369700
    },
    {
      "epoch": 28.248414941562906,
      "grad_norm": 6.937422275543213,
      "learning_rate": 1.4689481323046369e-05,
      "loss": 1.7775,
      "step": 369800
    },
    {
      "epoch": 28.256053777404322,
      "grad_norm": 8.401443481445312,
      "learning_rate": 1.4679932778244596e-05,
      "loss": 1.6521,
      "step": 369900
    },
    {
      "epoch": 28.26369261324574,
      "grad_norm": 8.736326217651367,
      "learning_rate": 1.4670384233442825e-05,
      "loss": 1.6874,
      "step": 370000
    },
    {
      "epoch": 28.27133144908716,
      "grad_norm": 8.61174201965332,
      "learning_rate": 1.4660835688641053e-05,
      "loss": 1.6244,
      "step": 370100
    },
    {
      "epoch": 28.278970284928576,
      "grad_norm": 6.366905689239502,
      "learning_rate": 1.4651287143839279e-05,
      "loss": 1.6919,
      "step": 370200
    },
    {
      "epoch": 28.286609120769995,
      "grad_norm": 8.676143646240234,
      "learning_rate": 1.4641738599037507e-05,
      "loss": 1.6522,
      "step": 370300
    },
    {
      "epoch": 28.29424795661141,
      "grad_norm": 6.171839237213135,
      "learning_rate": 1.4632190054235736e-05,
      "loss": 1.8206,
      "step": 370400
    },
    {
      "epoch": 28.30188679245283,
      "grad_norm": 7.802419185638428,
      "learning_rate": 1.4622641509433963e-05,
      "loss": 1.7528,
      "step": 370500
    },
    {
      "epoch": 28.30952562829425,
      "grad_norm": 9.639469146728516,
      "learning_rate": 1.4613092964632192e-05,
      "loss": 1.7637,
      "step": 370600
    },
    {
      "epoch": 28.317164464135665,
      "grad_norm": 8.188919067382812,
      "learning_rate": 1.4603544419830417e-05,
      "loss": 1.7882,
      "step": 370700
    },
    {
      "epoch": 28.324803299977084,
      "grad_norm": 10.132237434387207,
      "learning_rate": 1.4593995875028646e-05,
      "loss": 1.8428,
      "step": 370800
    },
    {
      "epoch": 28.3324421358185,
      "grad_norm": 5.646218776702881,
      "learning_rate": 1.4584447330226874e-05,
      "loss": 1.7311,
      "step": 370900
    },
    {
      "epoch": 28.34008097165992,
      "grad_norm": 6.019060134887695,
      "learning_rate": 1.4574898785425101e-05,
      "loss": 1.7372,
      "step": 371000
    },
    {
      "epoch": 28.34771980750134,
      "grad_norm": 7.533006191253662,
      "learning_rate": 1.456535024062333e-05,
      "loss": 1.664,
      "step": 371100
    },
    {
      "epoch": 28.355358643342754,
      "grad_norm": 7.615792274475098,
      "learning_rate": 1.4555801695821559e-05,
      "loss": 1.7614,
      "step": 371200
    },
    {
      "epoch": 28.362997479184173,
      "grad_norm": 8.728718757629395,
      "learning_rate": 1.4546253151019784e-05,
      "loss": 1.7086,
      "step": 371300
    },
    {
      "epoch": 28.37063631502559,
      "grad_norm": 7.533926963806152,
      "learning_rate": 1.4536704606218013e-05,
      "loss": 1.7017,
      "step": 371400
    },
    {
      "epoch": 28.378275150867008,
      "grad_norm": 10.143059730529785,
      "learning_rate": 1.4527156061416241e-05,
      "loss": 1.7543,
      "step": 371500
    },
    {
      "epoch": 28.385913986708427,
      "grad_norm": 7.556307792663574,
      "learning_rate": 1.4517607516614467e-05,
      "loss": 1.7713,
      "step": 371600
    },
    {
      "epoch": 28.393552822549843,
      "grad_norm": 6.8458356857299805,
      "learning_rate": 1.4508058971812697e-05,
      "loss": 1.7581,
      "step": 371700
    },
    {
      "epoch": 28.401191658391262,
      "grad_norm": 8.3348388671875,
      "learning_rate": 1.4498510427010926e-05,
      "loss": 1.6684,
      "step": 371800
    },
    {
      "epoch": 28.408830494232678,
      "grad_norm": 8.420702934265137,
      "learning_rate": 1.4488961882209151e-05,
      "loss": 1.7257,
      "step": 371900
    },
    {
      "epoch": 28.416469330074097,
      "grad_norm": 7.846085071563721,
      "learning_rate": 1.447941333740738e-05,
      "loss": 1.8836,
      "step": 372000
    },
    {
      "epoch": 28.424108165915513,
      "grad_norm": 6.785959720611572,
      "learning_rate": 1.4469864792605609e-05,
      "loss": 1.7875,
      "step": 372100
    },
    {
      "epoch": 28.431747001756932,
      "grad_norm": 12.111531257629395,
      "learning_rate": 1.4460316247803834e-05,
      "loss": 1.7477,
      "step": 372200
    },
    {
      "epoch": 28.43938583759835,
      "grad_norm": 9.93564224243164,
      "learning_rate": 1.4450767703002063e-05,
      "loss": 1.6138,
      "step": 372300
    },
    {
      "epoch": 28.447024673439767,
      "grad_norm": 6.7741780281066895,
      "learning_rate": 1.4441219158200291e-05,
      "loss": 1.6798,
      "step": 372400
    },
    {
      "epoch": 28.454663509281186,
      "grad_norm": 9.253827095031738,
      "learning_rate": 1.4431670613398518e-05,
      "loss": 1.7704,
      "step": 372500
    },
    {
      "epoch": 28.462302345122602,
      "grad_norm": 7.280179023742676,
      "learning_rate": 1.4422122068596747e-05,
      "loss": 1.7759,
      "step": 372600
    },
    {
      "epoch": 28.46994118096402,
      "grad_norm": 6.560749053955078,
      "learning_rate": 1.4412573523794976e-05,
      "loss": 1.7126,
      "step": 372700
    },
    {
      "epoch": 28.47758001680544,
      "grad_norm": 8.8894624710083,
      "learning_rate": 1.4403024978993201e-05,
      "loss": 1.6726,
      "step": 372800
    },
    {
      "epoch": 28.485218852646856,
      "grad_norm": 9.238412857055664,
      "learning_rate": 1.439347643419143e-05,
      "loss": 1.6277,
      "step": 372900
    },
    {
      "epoch": 28.492857688488275,
      "grad_norm": 8.971776008605957,
      "learning_rate": 1.4383927889389658e-05,
      "loss": 1.7318,
      "step": 373000
    },
    {
      "epoch": 28.50049652432969,
      "grad_norm": 7.478376388549805,
      "learning_rate": 1.4374379344587885e-05,
      "loss": 1.7361,
      "step": 373100
    },
    {
      "epoch": 28.50813536017111,
      "grad_norm": 8.638016700744629,
      "learning_rate": 1.4364830799786114e-05,
      "loss": 1.7163,
      "step": 373200
    },
    {
      "epoch": 28.51577419601253,
      "grad_norm": 7.900969982147217,
      "learning_rate": 1.4355282254984343e-05,
      "loss": 1.6878,
      "step": 373300
    },
    {
      "epoch": 28.523413031853945,
      "grad_norm": 8.101505279541016,
      "learning_rate": 1.4345733710182568e-05,
      "loss": 1.6613,
      "step": 373400
    },
    {
      "epoch": 28.531051867695364,
      "grad_norm": 9.419455528259277,
      "learning_rate": 1.4336185165380797e-05,
      "loss": 1.7685,
      "step": 373500
    },
    {
      "epoch": 28.53869070353678,
      "grad_norm": 8.721231460571289,
      "learning_rate": 1.4326636620579024e-05,
      "loss": 1.6755,
      "step": 373600
    },
    {
      "epoch": 28.5463295393782,
      "grad_norm": 7.654781341552734,
      "learning_rate": 1.4317088075777252e-05,
      "loss": 1.6407,
      "step": 373700
    },
    {
      "epoch": 28.55396837521962,
      "grad_norm": 8.570399284362793,
      "learning_rate": 1.4307539530975481e-05,
      "loss": 1.7419,
      "step": 373800
    },
    {
      "epoch": 28.561607211061034,
      "grad_norm": 7.30954122543335,
      "learning_rate": 1.4297990986173706e-05,
      "loss": 1.7108,
      "step": 373900
    },
    {
      "epoch": 28.569246046902453,
      "grad_norm": 10.534001350402832,
      "learning_rate": 1.4288442441371935e-05,
      "loss": 1.782,
      "step": 374000
    },
    {
      "epoch": 28.57688488274387,
      "grad_norm": 9.32059097290039,
      "learning_rate": 1.4278893896570164e-05,
      "loss": 1.6776,
      "step": 374100
    },
    {
      "epoch": 28.584523718585288,
      "grad_norm": 6.844306468963623,
      "learning_rate": 1.426934535176839e-05,
      "loss": 1.7856,
      "step": 374200
    },
    {
      "epoch": 28.592162554426704,
      "grad_norm": 7.507824897766113,
      "learning_rate": 1.425979680696662e-05,
      "loss": 1.7218,
      "step": 374300
    },
    {
      "epoch": 28.599801390268123,
      "grad_norm": 5.754891872406006,
      "learning_rate": 1.4250248262164848e-05,
      "loss": 1.7079,
      "step": 374400
    },
    {
      "epoch": 28.607440226109542,
      "grad_norm": 9.226972579956055,
      "learning_rate": 1.4240699717363073e-05,
      "loss": 1.8838,
      "step": 374500
    },
    {
      "epoch": 28.615079061950958,
      "grad_norm": 7.933073043823242,
      "learning_rate": 1.4231151172561302e-05,
      "loss": 1.7116,
      "step": 374600
    },
    {
      "epoch": 28.622717897792377,
      "grad_norm": 6.319435119628906,
      "learning_rate": 1.4221602627759531e-05,
      "loss": 1.8067,
      "step": 374700
    },
    {
      "epoch": 28.630356733633793,
      "grad_norm": 12.409139633178711,
      "learning_rate": 1.4212054082957756e-05,
      "loss": 1.7583,
      "step": 374800
    },
    {
      "epoch": 28.637995569475212,
      "grad_norm": 7.161133289337158,
      "learning_rate": 1.4202505538155987e-05,
      "loss": 1.6535,
      "step": 374900
    },
    {
      "epoch": 28.64563440531663,
      "grad_norm": 6.937716007232666,
      "learning_rate": 1.4192956993354215e-05,
      "loss": 1.7895,
      "step": 375000
    },
    {
      "epoch": 28.653273241158047,
      "grad_norm": 7.801546096801758,
      "learning_rate": 1.418340844855244e-05,
      "loss": 1.7956,
      "step": 375100
    },
    {
      "epoch": 28.660912076999466,
      "grad_norm": 7.131285667419434,
      "learning_rate": 1.417385990375067e-05,
      "loss": 1.7349,
      "step": 375200
    },
    {
      "epoch": 28.66855091284088,
      "grad_norm": 8.372562408447266,
      "learning_rate": 1.4164311358948898e-05,
      "loss": 1.7346,
      "step": 375300
    },
    {
      "epoch": 28.6761897486823,
      "grad_norm": 11.489283561706543,
      "learning_rate": 1.4154762814147123e-05,
      "loss": 1.794,
      "step": 375400
    },
    {
      "epoch": 28.68382858452372,
      "grad_norm": 7.7754693031311035,
      "learning_rate": 1.4145214269345352e-05,
      "loss": 1.7146,
      "step": 375500
    },
    {
      "epoch": 28.691467420365136,
      "grad_norm": 4.9722185134887695,
      "learning_rate": 1.413566572454358e-05,
      "loss": 1.756,
      "step": 375600
    },
    {
      "epoch": 28.699106256206555,
      "grad_norm": 7.521700382232666,
      "learning_rate": 1.4126117179741808e-05,
      "loss": 1.7293,
      "step": 375700
    },
    {
      "epoch": 28.70674509204797,
      "grad_norm": 7.770811080932617,
      "learning_rate": 1.4116568634940036e-05,
      "loss": 1.7396,
      "step": 375800
    },
    {
      "epoch": 28.71438392788939,
      "grad_norm": 6.877709865570068,
      "learning_rate": 1.4107020090138265e-05,
      "loss": 1.6275,
      "step": 375900
    },
    {
      "epoch": 28.722022763730806,
      "grad_norm": 6.163436412811279,
      "learning_rate": 1.409747154533649e-05,
      "loss": 1.8088,
      "step": 376000
    },
    {
      "epoch": 28.729661599572225,
      "grad_norm": 6.974923133850098,
      "learning_rate": 1.4087923000534719e-05,
      "loss": 1.8052,
      "step": 376100
    },
    {
      "epoch": 28.737300435413644,
      "grad_norm": 6.0990986824035645,
      "learning_rate": 1.4078374455732948e-05,
      "loss": 1.703,
      "step": 376200
    },
    {
      "epoch": 28.74493927125506,
      "grad_norm": 8.390549659729004,
      "learning_rate": 1.4068825910931175e-05,
      "loss": 1.756,
      "step": 376300
    },
    {
      "epoch": 28.75257810709648,
      "grad_norm": 6.42083740234375,
      "learning_rate": 1.4059277366129403e-05,
      "loss": 1.7099,
      "step": 376400
    },
    {
      "epoch": 28.760216942937895,
      "grad_norm": 7.008425712585449,
      "learning_rate": 1.4049728821327629e-05,
      "loss": 1.7833,
      "step": 376500
    },
    {
      "epoch": 28.767855778779314,
      "grad_norm": 7.325803756713867,
      "learning_rate": 1.4040180276525857e-05,
      "loss": 1.76,
      "step": 376600
    },
    {
      "epoch": 28.775494614620733,
      "grad_norm": 8.25166130065918,
      "learning_rate": 1.4030631731724086e-05,
      "loss": 1.7316,
      "step": 376700
    },
    {
      "epoch": 28.78313345046215,
      "grad_norm": 8.440878868103027,
      "learning_rate": 1.4021083186922313e-05,
      "loss": 1.7271,
      "step": 376800
    },
    {
      "epoch": 28.790772286303568,
      "grad_norm": 7.633055210113525,
      "learning_rate": 1.4011534642120542e-05,
      "loss": 1.7495,
      "step": 376900
    },
    {
      "epoch": 28.798411122144984,
      "grad_norm": 8.268789291381836,
      "learning_rate": 1.400198609731877e-05,
      "loss": 1.7712,
      "step": 377000
    },
    {
      "epoch": 28.806049957986403,
      "grad_norm": 7.226950645446777,
      "learning_rate": 1.3992437552516996e-05,
      "loss": 1.8332,
      "step": 377100
    },
    {
      "epoch": 28.813688793827822,
      "grad_norm": 6.609003067016602,
      "learning_rate": 1.3982889007715224e-05,
      "loss": 1.7135,
      "step": 377200
    },
    {
      "epoch": 28.821327629669238,
      "grad_norm": 7.350014686584473,
      "learning_rate": 1.3973340462913453e-05,
      "loss": 1.7546,
      "step": 377300
    },
    {
      "epoch": 28.828966465510657,
      "grad_norm": 7.229901313781738,
      "learning_rate": 1.396379191811168e-05,
      "loss": 1.8595,
      "step": 377400
    },
    {
      "epoch": 28.836605301352073,
      "grad_norm": 8.447403907775879,
      "learning_rate": 1.3954243373309909e-05,
      "loss": 1.7526,
      "step": 377500
    },
    {
      "epoch": 28.844244137193492,
      "grad_norm": 11.906830787658691,
      "learning_rate": 1.3944694828508138e-05,
      "loss": 1.7143,
      "step": 377600
    },
    {
      "epoch": 28.85188297303491,
      "grad_norm": 10.126628875732422,
      "learning_rate": 1.3935146283706363e-05,
      "loss": 1.7799,
      "step": 377700
    },
    {
      "epoch": 28.859521808876327,
      "grad_norm": 7.215076923370361,
      "learning_rate": 1.3925597738904592e-05,
      "loss": 1.6838,
      "step": 377800
    },
    {
      "epoch": 28.867160644717746,
      "grad_norm": 4.3983073234558105,
      "learning_rate": 1.391604919410282e-05,
      "loss": 1.6492,
      "step": 377900
    },
    {
      "epoch": 28.87479948055916,
      "grad_norm": 9.025221824645996,
      "learning_rate": 1.3906500649301046e-05,
      "loss": 1.7651,
      "step": 378000
    },
    {
      "epoch": 28.88243831640058,
      "grad_norm": 7.390972137451172,
      "learning_rate": 1.3896952104499274e-05,
      "loss": 1.7288,
      "step": 378100
    },
    {
      "epoch": 28.890077152242,
      "grad_norm": 5.9495930671691895,
      "learning_rate": 1.3887403559697505e-05,
      "loss": 1.7971,
      "step": 378200
    },
    {
      "epoch": 28.897715988083416,
      "grad_norm": 6.974593162536621,
      "learning_rate": 1.387785501489573e-05,
      "loss": 1.7264,
      "step": 378300
    },
    {
      "epoch": 28.905354823924835,
      "grad_norm": 6.480473518371582,
      "learning_rate": 1.3868306470093959e-05,
      "loss": 1.7618,
      "step": 378400
    },
    {
      "epoch": 28.91299365976625,
      "grad_norm": 7.257026195526123,
      "learning_rate": 1.3858757925292187e-05,
      "loss": 1.6599,
      "step": 378500
    },
    {
      "epoch": 28.92063249560767,
      "grad_norm": 6.172298431396484,
      "learning_rate": 1.3849209380490413e-05,
      "loss": 1.7709,
      "step": 378600
    },
    {
      "epoch": 28.928271331449086,
      "grad_norm": 6.746857166290283,
      "learning_rate": 1.3839660835688641e-05,
      "loss": 1.6467,
      "step": 378700
    },
    {
      "epoch": 28.935910167290505,
      "grad_norm": 6.241247177124023,
      "learning_rate": 1.383011229088687e-05,
      "loss": 1.7032,
      "step": 378800
    },
    {
      "epoch": 28.943549003131924,
      "grad_norm": 7.164666652679443,
      "learning_rate": 1.3820563746085097e-05,
      "loss": 1.7067,
      "step": 378900
    },
    {
      "epoch": 28.95118783897334,
      "grad_norm": 8.30008316040039,
      "learning_rate": 1.3811015201283326e-05,
      "loss": 1.7079,
      "step": 379000
    },
    {
      "epoch": 28.95882667481476,
      "grad_norm": 8.948047637939453,
      "learning_rate": 1.3801466656481554e-05,
      "loss": 1.6862,
      "step": 379100
    },
    {
      "epoch": 28.966465510656175,
      "grad_norm": 6.653477668762207,
      "learning_rate": 1.379191811167978e-05,
      "loss": 1.6806,
      "step": 379200
    },
    {
      "epoch": 28.974104346497594,
      "grad_norm": 7.746960163116455,
      "learning_rate": 1.3782369566878008e-05,
      "loss": 1.6608,
      "step": 379300
    },
    {
      "epoch": 28.981743182339013,
      "grad_norm": 9.693133354187012,
      "learning_rate": 1.3772821022076235e-05,
      "loss": 1.7407,
      "step": 379400
    },
    {
      "epoch": 28.98938201818043,
      "grad_norm": 7.009393215179443,
      "learning_rate": 1.3763272477274464e-05,
      "loss": 1.7419,
      "step": 379500
    },
    {
      "epoch": 28.997020854021848,
      "grad_norm": 6.845920562744141,
      "learning_rate": 1.3753723932472693e-05,
      "loss": 1.7253,
      "step": 379600
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.7940239906311035,
      "eval_runtime": 1.7468,
      "eval_samples_per_second": 394.997,
      "eval_steps_per_second": 394.997,
      "step": 379639
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.4969485998153687,
      "eval_runtime": 32.1849,
      "eval_samples_per_second": 406.744,
      "eval_steps_per_second": 406.744,
      "step": 379639
    },
    {
      "epoch": 29.004659689863264,
      "grad_norm": 8.109636306762695,
      "learning_rate": 1.3744175387670918e-05,
      "loss": 1.793,
      "step": 379700
    },
    {
      "epoch": 29.012298525704683,
      "grad_norm": 9.10397720336914,
      "learning_rate": 1.3734626842869147e-05,
      "loss": 1.7398,
      "step": 379800
    },
    {
      "epoch": 29.019937361546102,
      "grad_norm": 9.96618938446045,
      "learning_rate": 1.3725078298067375e-05,
      "loss": 1.6096,
      "step": 379900
    },
    {
      "epoch": 29.027576197387518,
      "grad_norm": 5.197146892547607,
      "learning_rate": 1.3715529753265602e-05,
      "loss": 1.714,
      "step": 380000
    },
    {
      "epoch": 29.035215033228937,
      "grad_norm": 8.278071403503418,
      "learning_rate": 1.3705981208463831e-05,
      "loss": 1.7154,
      "step": 380100
    },
    {
      "epoch": 29.042853869070353,
      "grad_norm": 9.184534072875977,
      "learning_rate": 1.369643266366206e-05,
      "loss": 1.7423,
      "step": 380200
    },
    {
      "epoch": 29.050492704911772,
      "grad_norm": 7.465129375457764,
      "learning_rate": 1.3686884118860285e-05,
      "loss": 1.6494,
      "step": 380300
    },
    {
      "epoch": 29.058131540753187,
      "grad_norm": 6.2003889083862305,
      "learning_rate": 1.3677335574058514e-05,
      "loss": 1.6968,
      "step": 380400
    },
    {
      "epoch": 29.065770376594607,
      "grad_norm": 8.289762496948242,
      "learning_rate": 1.3667787029256743e-05,
      "loss": 1.7159,
      "step": 380500
    },
    {
      "epoch": 29.073409212436026,
      "grad_norm": 6.7083330154418945,
      "learning_rate": 1.365823848445497e-05,
      "loss": 1.7128,
      "step": 380600
    },
    {
      "epoch": 29.08104804827744,
      "grad_norm": 9.081555366516113,
      "learning_rate": 1.3648689939653198e-05,
      "loss": 1.8217,
      "step": 380700
    },
    {
      "epoch": 29.08868688411886,
      "grad_norm": 6.708615779876709,
      "learning_rate": 1.3639141394851427e-05,
      "loss": 1.8058,
      "step": 380800
    },
    {
      "epoch": 29.096325719960277,
      "grad_norm": 10.673542976379395,
      "learning_rate": 1.3629592850049652e-05,
      "loss": 1.7367,
      "step": 380900
    },
    {
      "epoch": 29.103964555801696,
      "grad_norm": 8.16540241241455,
      "learning_rate": 1.3620044305247881e-05,
      "loss": 1.7799,
      "step": 381000
    },
    {
      "epoch": 29.111603391643115,
      "grad_norm": 8.83974838256836,
      "learning_rate": 1.361049576044611e-05,
      "loss": 1.7921,
      "step": 381100
    },
    {
      "epoch": 29.11924222748453,
      "grad_norm": 6.832357406616211,
      "learning_rate": 1.3600947215644335e-05,
      "loss": 1.8221,
      "step": 381200
    },
    {
      "epoch": 29.12688106332595,
      "grad_norm": 7.158745765686035,
      "learning_rate": 1.3591398670842564e-05,
      "loss": 1.6108,
      "step": 381300
    },
    {
      "epoch": 29.134519899167366,
      "grad_norm": 8.658652305603027,
      "learning_rate": 1.3581850126040794e-05,
      "loss": 1.8362,
      "step": 381400
    },
    {
      "epoch": 29.142158735008785,
      "grad_norm": 6.894432067871094,
      "learning_rate": 1.357230158123902e-05,
      "loss": 1.7109,
      "step": 381500
    },
    {
      "epoch": 29.149797570850204,
      "grad_norm": 5.412685394287109,
      "learning_rate": 1.3562753036437248e-05,
      "loss": 1.7299,
      "step": 381600
    },
    {
      "epoch": 29.15743640669162,
      "grad_norm": 8.661799430847168,
      "learning_rate": 1.3553204491635477e-05,
      "loss": 1.6931,
      "step": 381700
    },
    {
      "epoch": 29.16507524253304,
      "grad_norm": 8.947245597839355,
      "learning_rate": 1.3543655946833702e-05,
      "loss": 1.7822,
      "step": 381800
    },
    {
      "epoch": 29.172714078374455,
      "grad_norm": 9.271346092224121,
      "learning_rate": 1.353410740203193e-05,
      "loss": 1.7781,
      "step": 381900
    },
    {
      "epoch": 29.180352914215874,
      "grad_norm": 8.93265151977539,
      "learning_rate": 1.352455885723016e-05,
      "loss": 1.7266,
      "step": 382000
    },
    {
      "epoch": 29.187991750057293,
      "grad_norm": 5.324115753173828,
      "learning_rate": 1.3515010312428386e-05,
      "loss": 1.6745,
      "step": 382100
    },
    {
      "epoch": 29.19563058589871,
      "grad_norm": 8.024389266967773,
      "learning_rate": 1.3505461767626615e-05,
      "loss": 1.6806,
      "step": 382200
    },
    {
      "epoch": 29.203269421740128,
      "grad_norm": 6.313789367675781,
      "learning_rate": 1.349591322282484e-05,
      "loss": 1.7501,
      "step": 382300
    },
    {
      "epoch": 29.210908257581544,
      "grad_norm": 8.814421653747559,
      "learning_rate": 1.3486364678023069e-05,
      "loss": 1.7148,
      "step": 382400
    },
    {
      "epoch": 29.218547093422963,
      "grad_norm": 8.717055320739746,
      "learning_rate": 1.3476816133221298e-05,
      "loss": 1.7637,
      "step": 382500
    },
    {
      "epoch": 29.22618592926438,
      "grad_norm": 6.486382961273193,
      "learning_rate": 1.3467267588419525e-05,
      "loss": 1.7164,
      "step": 382600
    },
    {
      "epoch": 29.233824765105798,
      "grad_norm": 5.711924076080322,
      "learning_rate": 1.3457719043617753e-05,
      "loss": 1.7678,
      "step": 382700
    },
    {
      "epoch": 29.241463600947217,
      "grad_norm": 7.645467758178711,
      "learning_rate": 1.3448170498815982e-05,
      "loss": 1.779,
      "step": 382800
    },
    {
      "epoch": 29.249102436788633,
      "grad_norm": 7.485116481781006,
      "learning_rate": 1.3438621954014207e-05,
      "loss": 1.6474,
      "step": 382900
    },
    {
      "epoch": 29.25674127263005,
      "grad_norm": 8.307126998901367,
      "learning_rate": 1.3429073409212436e-05,
      "loss": 1.7334,
      "step": 383000
    },
    {
      "epoch": 29.264380108471467,
      "grad_norm": 6.829418659210205,
      "learning_rate": 1.3419524864410665e-05,
      "loss": 1.6791,
      "step": 383100
    },
    {
      "epoch": 29.272018944312887,
      "grad_norm": 7.118804454803467,
      "learning_rate": 1.3409976319608892e-05,
      "loss": 1.7084,
      "step": 383200
    },
    {
      "epoch": 29.279657780154306,
      "grad_norm": 9.395989418029785,
      "learning_rate": 1.340042777480712e-05,
      "loss": 1.7421,
      "step": 383300
    },
    {
      "epoch": 29.28729661599572,
      "grad_norm": 8.650091171264648,
      "learning_rate": 1.339087923000535e-05,
      "loss": 1.7082,
      "step": 383400
    },
    {
      "epoch": 29.29493545183714,
      "grad_norm": 9.032007217407227,
      "learning_rate": 1.3381330685203575e-05,
      "loss": 1.7617,
      "step": 383500
    },
    {
      "epoch": 29.302574287678556,
      "grad_norm": 8.255420684814453,
      "learning_rate": 1.3371782140401803e-05,
      "loss": 1.6535,
      "step": 383600
    },
    {
      "epoch": 29.310213123519976,
      "grad_norm": 5.363658428192139,
      "learning_rate": 1.3362233595600032e-05,
      "loss": 1.7084,
      "step": 383700
    },
    {
      "epoch": 29.317851959361395,
      "grad_norm": 7.805542945861816,
      "learning_rate": 1.3352685050798259e-05,
      "loss": 1.7521,
      "step": 383800
    },
    {
      "epoch": 29.32549079520281,
      "grad_norm": 7.895519733428955,
      "learning_rate": 1.3343136505996488e-05,
      "loss": 1.7788,
      "step": 383900
    },
    {
      "epoch": 29.33312963104423,
      "grad_norm": 10.139320373535156,
      "learning_rate": 1.3333587961194716e-05,
      "loss": 1.6876,
      "step": 384000
    },
    {
      "epoch": 29.340768466885645,
      "grad_norm": 6.475164413452148,
      "learning_rate": 1.3324039416392942e-05,
      "loss": 1.702,
      "step": 384100
    },
    {
      "epoch": 29.348407302727065,
      "grad_norm": 7.516044616699219,
      "learning_rate": 1.331449087159117e-05,
      "loss": 1.7239,
      "step": 384200
    },
    {
      "epoch": 29.356046138568484,
      "grad_norm": 6.437967300415039,
      "learning_rate": 1.3304942326789399e-05,
      "loss": 1.7949,
      "step": 384300
    },
    {
      "epoch": 29.3636849744099,
      "grad_norm": 7.820051670074463,
      "learning_rate": 1.3295393781987624e-05,
      "loss": 1.6557,
      "step": 384400
    },
    {
      "epoch": 29.37132381025132,
      "grad_norm": 8.331255912780762,
      "learning_rate": 1.3285845237185853e-05,
      "loss": 1.7235,
      "step": 384500
    },
    {
      "epoch": 29.378962646092734,
      "grad_norm": 7.123678207397461,
      "learning_rate": 1.3276296692384083e-05,
      "loss": 1.7654,
      "step": 384600
    },
    {
      "epoch": 29.386601481934154,
      "grad_norm": 6.653287410736084,
      "learning_rate": 1.3266748147582309e-05,
      "loss": 1.7727,
      "step": 384700
    },
    {
      "epoch": 29.39424031777557,
      "grad_norm": 7.070986270904541,
      "learning_rate": 1.3257199602780537e-05,
      "loss": 1.76,
      "step": 384800
    },
    {
      "epoch": 29.40187915361699,
      "grad_norm": 7.862368583679199,
      "learning_rate": 1.3247651057978766e-05,
      "loss": 1.6692,
      "step": 384900
    },
    {
      "epoch": 29.409517989458408,
      "grad_norm": 6.262198448181152,
      "learning_rate": 1.3238102513176991e-05,
      "loss": 1.7892,
      "step": 385000
    },
    {
      "epoch": 29.417156825299823,
      "grad_norm": 8.894343376159668,
      "learning_rate": 1.322855396837522e-05,
      "loss": 1.6043,
      "step": 385100
    },
    {
      "epoch": 29.424795661141243,
      "grad_norm": 7.766401290893555,
      "learning_rate": 1.3219005423573447e-05,
      "loss": 1.7365,
      "step": 385200
    },
    {
      "epoch": 29.43243449698266,
      "grad_norm": 9.644558906555176,
      "learning_rate": 1.3209456878771676e-05,
      "loss": 1.7343,
      "step": 385300
    },
    {
      "epoch": 29.440073332824078,
      "grad_norm": 9.202898979187012,
      "learning_rate": 1.3199908333969904e-05,
      "loss": 1.7319,
      "step": 385400
    },
    {
      "epoch": 29.447712168665497,
      "grad_norm": 9.160794258117676,
      "learning_rate": 1.319035978916813e-05,
      "loss": 1.7416,
      "step": 385500
    },
    {
      "epoch": 29.455351004506912,
      "grad_norm": 6.482892990112305,
      "learning_rate": 1.3180811244366358e-05,
      "loss": 1.7987,
      "step": 385600
    },
    {
      "epoch": 29.46298984034833,
      "grad_norm": 7.933274745941162,
      "learning_rate": 1.3171262699564587e-05,
      "loss": 1.7244,
      "step": 385700
    },
    {
      "epoch": 29.470628676189747,
      "grad_norm": 8.093485832214355,
      "learning_rate": 1.3161714154762814e-05,
      "loss": 1.7626,
      "step": 385800
    },
    {
      "epoch": 29.478267512031167,
      "grad_norm": 7.083919525146484,
      "learning_rate": 1.3152165609961043e-05,
      "loss": 1.7053,
      "step": 385900
    },
    {
      "epoch": 29.485906347872586,
      "grad_norm": 8.635954856872559,
      "learning_rate": 1.3142617065159272e-05,
      "loss": 1.8316,
      "step": 386000
    },
    {
      "epoch": 29.493545183714,
      "grad_norm": 8.202434539794922,
      "learning_rate": 1.3133068520357497e-05,
      "loss": 1.7909,
      "step": 386100
    },
    {
      "epoch": 29.50118401955542,
      "grad_norm": 6.96928596496582,
      "learning_rate": 1.3123519975555726e-05,
      "loss": 1.7118,
      "step": 386200
    },
    {
      "epoch": 29.508822855396836,
      "grad_norm": 6.35385274887085,
      "learning_rate": 1.3113971430753954e-05,
      "loss": 1.6956,
      "step": 386300
    },
    {
      "epoch": 29.516461691238256,
      "grad_norm": 8.509242057800293,
      "learning_rate": 1.3104422885952181e-05,
      "loss": 1.718,
      "step": 386400
    },
    {
      "epoch": 29.524100527079675,
      "grad_norm": 9.880939483642578,
      "learning_rate": 1.309487434115041e-05,
      "loss": 1.6557,
      "step": 386500
    },
    {
      "epoch": 29.53173936292109,
      "grad_norm": 4.632048606872559,
      "learning_rate": 1.3085325796348639e-05,
      "loss": 1.7387,
      "step": 386600
    },
    {
      "epoch": 29.53937819876251,
      "grad_norm": 7.521873950958252,
      "learning_rate": 1.3075777251546864e-05,
      "loss": 1.7148,
      "step": 386700
    },
    {
      "epoch": 29.547017034603925,
      "grad_norm": 7.569997310638428,
      "learning_rate": 1.3066228706745093e-05,
      "loss": 1.7941,
      "step": 386800
    },
    {
      "epoch": 29.554655870445345,
      "grad_norm": 8.79593563079834,
      "learning_rate": 1.3056680161943321e-05,
      "loss": 1.6765,
      "step": 386900
    },
    {
      "epoch": 29.56229470628676,
      "grad_norm": 8.001350402832031,
      "learning_rate": 1.3047131617141548e-05,
      "loss": 1.7379,
      "step": 387000
    },
    {
      "epoch": 29.56993354212818,
      "grad_norm": 6.878335475921631,
      "learning_rate": 1.3037583072339777e-05,
      "loss": 1.8014,
      "step": 387100
    },
    {
      "epoch": 29.5775723779696,
      "grad_norm": 9.004250526428223,
      "learning_rate": 1.3028034527538006e-05,
      "loss": 1.7003,
      "step": 387200
    },
    {
      "epoch": 29.585211213811014,
      "grad_norm": 6.332674026489258,
      "learning_rate": 1.3018485982736231e-05,
      "loss": 1.6866,
      "step": 387300
    },
    {
      "epoch": 29.592850049652434,
      "grad_norm": 8.341044425964355,
      "learning_rate": 1.300893743793446e-05,
      "loss": 1.6843,
      "step": 387400
    },
    {
      "epoch": 29.60048888549385,
      "grad_norm": 10.067159652709961,
      "learning_rate": 1.2999388893132688e-05,
      "loss": 1.7161,
      "step": 387500
    },
    {
      "epoch": 29.60812772133527,
      "grad_norm": 4.515952110290527,
      "learning_rate": 1.2989840348330914e-05,
      "loss": 1.6983,
      "step": 387600
    },
    {
      "epoch": 29.615766557176688,
      "grad_norm": 6.949063301086426,
      "learning_rate": 1.2980291803529142e-05,
      "loss": 1.7314,
      "step": 387700
    },
    {
      "epoch": 29.623405393018103,
      "grad_norm": 7.740012168884277,
      "learning_rate": 1.2970743258727373e-05,
      "loss": 1.713,
      "step": 387800
    },
    {
      "epoch": 29.631044228859523,
      "grad_norm": 10.353334426879883,
      "learning_rate": 1.2961194713925598e-05,
      "loss": 1.8512,
      "step": 387900
    },
    {
      "epoch": 29.63868306470094,
      "grad_norm": 7.262265682220459,
      "learning_rate": 1.2951646169123827e-05,
      "loss": 1.7762,
      "step": 388000
    },
    {
      "epoch": 29.646321900542357,
      "grad_norm": 9.145370483398438,
      "learning_rate": 1.2942097624322052e-05,
      "loss": 1.7475,
      "step": 388100
    },
    {
      "epoch": 29.653960736383777,
      "grad_norm": 8.711116790771484,
      "learning_rate": 1.293254907952028e-05,
      "loss": 1.7838,
      "step": 388200
    },
    {
      "epoch": 29.661599572225192,
      "grad_norm": 6.245921611785889,
      "learning_rate": 1.292300053471851e-05,
      "loss": 1.7604,
      "step": 388300
    },
    {
      "epoch": 29.66923840806661,
      "grad_norm": 13.269573211669922,
      "learning_rate": 1.2913451989916736e-05,
      "loss": 1.8048,
      "step": 388400
    },
    {
      "epoch": 29.676877243908027,
      "grad_norm": 8.1171236038208,
      "learning_rate": 1.2903903445114965e-05,
      "loss": 1.675,
      "step": 388500
    },
    {
      "epoch": 29.684516079749447,
      "grad_norm": 9.866114616394043,
      "learning_rate": 1.2894354900313194e-05,
      "loss": 1.7665,
      "step": 388600
    },
    {
      "epoch": 29.692154915590862,
      "grad_norm": 7.089951992034912,
      "learning_rate": 1.288480635551142e-05,
      "loss": 1.7148,
      "step": 388700
    },
    {
      "epoch": 29.69979375143228,
      "grad_norm": 6.892064094543457,
      "learning_rate": 1.2875257810709648e-05,
      "loss": 1.8282,
      "step": 388800
    },
    {
      "epoch": 29.7074325872737,
      "grad_norm": 7.285722732543945,
      "learning_rate": 1.2865709265907877e-05,
      "loss": 1.7394,
      "step": 388900
    },
    {
      "epoch": 29.715071423115116,
      "grad_norm": 6.340723514556885,
      "learning_rate": 1.2856160721106104e-05,
      "loss": 1.7896,
      "step": 389000
    },
    {
      "epoch": 29.722710258956536,
      "grad_norm": 5.72337532043457,
      "learning_rate": 1.2846612176304332e-05,
      "loss": 1.7113,
      "step": 389100
    },
    {
      "epoch": 29.73034909479795,
      "grad_norm": 9.953580856323242,
      "learning_rate": 1.2837063631502561e-05,
      "loss": 1.8183,
      "step": 389200
    },
    {
      "epoch": 29.73798793063937,
      "grad_norm": 8.702313423156738,
      "learning_rate": 1.2827515086700786e-05,
      "loss": 1.7311,
      "step": 389300
    },
    {
      "epoch": 29.74562676648079,
      "grad_norm": 6.818990707397461,
      "learning_rate": 1.2817966541899015e-05,
      "loss": 1.6875,
      "step": 389400
    },
    {
      "epoch": 29.753265602322205,
      "grad_norm": 8.000431060791016,
      "learning_rate": 1.2808417997097244e-05,
      "loss": 1.7227,
      "step": 389500
    },
    {
      "epoch": 29.760904438163625,
      "grad_norm": 7.687990665435791,
      "learning_rate": 1.279886945229547e-05,
      "loss": 1.7089,
      "step": 389600
    },
    {
      "epoch": 29.76854327400504,
      "grad_norm": 5.827245712280273,
      "learning_rate": 1.27893209074937e-05,
      "loss": 1.7954,
      "step": 389700
    },
    {
      "epoch": 29.77618210984646,
      "grad_norm": 12.049588203430176,
      "learning_rate": 1.2779772362691928e-05,
      "loss": 1.7633,
      "step": 389800
    },
    {
      "epoch": 29.78382094568788,
      "grad_norm": 8.329534530639648,
      "learning_rate": 1.2770223817890153e-05,
      "loss": 1.7457,
      "step": 389900
    },
    {
      "epoch": 29.791459781529294,
      "grad_norm": 7.982699394226074,
      "learning_rate": 1.2760675273088382e-05,
      "loss": 1.7097,
      "step": 390000
    },
    {
      "epoch": 29.799098617370714,
      "grad_norm": 8.105587005615234,
      "learning_rate": 1.275112672828661e-05,
      "loss": 1.727,
      "step": 390100
    },
    {
      "epoch": 29.80673745321213,
      "grad_norm": 6.904303073883057,
      "learning_rate": 1.2741578183484838e-05,
      "loss": 1.8004,
      "step": 390200
    },
    {
      "epoch": 29.81437628905355,
      "grad_norm": 7.429755687713623,
      "learning_rate": 1.2732029638683066e-05,
      "loss": 1.6839,
      "step": 390300
    },
    {
      "epoch": 29.822015124894968,
      "grad_norm": 6.352120876312256,
      "learning_rate": 1.2722481093881295e-05,
      "loss": 1.7663,
      "step": 390400
    },
    {
      "epoch": 29.829653960736383,
      "grad_norm": 6.37359094619751,
      "learning_rate": 1.271293254907952e-05,
      "loss": 1.7194,
      "step": 390500
    },
    {
      "epoch": 29.837292796577803,
      "grad_norm": 6.7253851890563965,
      "learning_rate": 1.2703384004277749e-05,
      "loss": 1.5834,
      "step": 390600
    },
    {
      "epoch": 29.844931632419218,
      "grad_norm": 8.003911018371582,
      "learning_rate": 1.2693835459475978e-05,
      "loss": 1.7341,
      "step": 390700
    },
    {
      "epoch": 29.852570468260637,
      "grad_norm": 6.928172588348389,
      "learning_rate": 1.2684286914674203e-05,
      "loss": 1.7035,
      "step": 390800
    },
    {
      "epoch": 29.860209304102057,
      "grad_norm": 8.362269401550293,
      "learning_rate": 1.2674738369872432e-05,
      "loss": 1.7945,
      "step": 390900
    },
    {
      "epoch": 29.867848139943472,
      "grad_norm": 8.91445541381836,
      "learning_rate": 1.2665189825070659e-05,
      "loss": 1.7373,
      "step": 391000
    },
    {
      "epoch": 29.87548697578489,
      "grad_norm": 7.980774879455566,
      "learning_rate": 1.2655641280268887e-05,
      "loss": 1.6706,
      "step": 391100
    },
    {
      "epoch": 29.883125811626307,
      "grad_norm": 7.729929447174072,
      "learning_rate": 1.2646092735467116e-05,
      "loss": 1.7407,
      "step": 391200
    },
    {
      "epoch": 29.890764647467726,
      "grad_norm": 9.263104438781738,
      "learning_rate": 1.2636544190665341e-05,
      "loss": 1.7552,
      "step": 391300
    },
    {
      "epoch": 29.898403483309142,
      "grad_norm": 5.768435001373291,
      "learning_rate": 1.262699564586357e-05,
      "loss": 1.6195,
      "step": 391400
    },
    {
      "epoch": 29.90604231915056,
      "grad_norm": 7.2917633056640625,
      "learning_rate": 1.2617447101061799e-05,
      "loss": 1.6662,
      "step": 391500
    },
    {
      "epoch": 29.91368115499198,
      "grad_norm": 6.740406513214111,
      "learning_rate": 1.2607898556260026e-05,
      "loss": 1.7179,
      "step": 391600
    },
    {
      "epoch": 29.921319990833396,
      "grad_norm": 7.858548641204834,
      "learning_rate": 1.2598350011458255e-05,
      "loss": 1.6789,
      "step": 391700
    },
    {
      "epoch": 29.928958826674815,
      "grad_norm": 6.753012657165527,
      "learning_rate": 1.2588801466656483e-05,
      "loss": 1.7561,
      "step": 391800
    },
    {
      "epoch": 29.93659766251623,
      "grad_norm": 9.08177661895752,
      "learning_rate": 1.2579252921854709e-05,
      "loss": 1.726,
      "step": 391900
    },
    {
      "epoch": 29.94423649835765,
      "grad_norm": 6.383667469024658,
      "learning_rate": 1.2569704377052937e-05,
      "loss": 1.7287,
      "step": 392000
    },
    {
      "epoch": 29.95187533419907,
      "grad_norm": 5.286324977874756,
      "learning_rate": 1.2560155832251166e-05,
      "loss": 1.67,
      "step": 392100
    },
    {
      "epoch": 29.959514170040485,
      "grad_norm": 7.5480451583862305,
      "learning_rate": 1.2550607287449393e-05,
      "loss": 1.6911,
      "step": 392200
    },
    {
      "epoch": 29.967153005881904,
      "grad_norm": 9.451245307922363,
      "learning_rate": 1.2541058742647622e-05,
      "loss": 1.7045,
      "step": 392300
    },
    {
      "epoch": 29.97479184172332,
      "grad_norm": 9.124898910522461,
      "learning_rate": 1.253151019784585e-05,
      "loss": 1.7358,
      "step": 392400
    },
    {
      "epoch": 29.98243067756474,
      "grad_norm": 8.146832466125488,
      "learning_rate": 1.2521961653044076e-05,
      "loss": 1.668,
      "step": 392500
    },
    {
      "epoch": 29.99006951340616,
      "grad_norm": 8.498897552490234,
      "learning_rate": 1.2512413108242304e-05,
      "loss": 1.6742,
      "step": 392600
    },
    {
      "epoch": 29.997708349247574,
      "grad_norm": 5.637760162353516,
      "learning_rate": 1.2502864563440533e-05,
      "loss": 1.7416,
      "step": 392700
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.7975640296936035,
      "eval_runtime": 1.7587,
      "eval_samples_per_second": 392.346,
      "eval_steps_per_second": 392.346,
      "step": 392730
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.494747519493103,
      "eval_runtime": 32.1956,
      "eval_samples_per_second": 406.608,
      "eval_steps_per_second": 406.608,
      "step": 392730
    },
    {
      "epoch": 30.005347185088993,
      "grad_norm": 6.88716983795166,
      "learning_rate": 1.249331601863876e-05,
      "loss": 1.6995,
      "step": 392800
    },
    {
      "epoch": 30.01298602093041,
      "grad_norm": 9.773292541503906,
      "learning_rate": 1.2483767473836989e-05,
      "loss": 1.7387,
      "step": 392900
    },
    {
      "epoch": 30.02062485677183,
      "grad_norm": 8.894396781921387,
      "learning_rate": 1.2474218929035216e-05,
      "loss": 1.6768,
      "step": 393000
    },
    {
      "epoch": 30.028263692613244,
      "grad_norm": 7.1242218017578125,
      "learning_rate": 1.2464670384233443e-05,
      "loss": 1.7584,
      "step": 393100
    },
    {
      "epoch": 30.035902528454663,
      "grad_norm": 7.766704082489014,
      "learning_rate": 1.2455121839431671e-05,
      "loss": 1.6541,
      "step": 393200
    },
    {
      "epoch": 30.043541364296082,
      "grad_norm": 6.6885552406311035,
      "learning_rate": 1.2445573294629898e-05,
      "loss": 1.6967,
      "step": 393300
    },
    {
      "epoch": 30.051180200137498,
      "grad_norm": 8.5612211227417,
      "learning_rate": 1.2436024749828127e-05,
      "loss": 1.6473,
      "step": 393400
    },
    {
      "epoch": 30.058819035978917,
      "grad_norm": 8.38014030456543,
      "learning_rate": 1.2426476205026356e-05,
      "loss": 1.685,
      "step": 393500
    },
    {
      "epoch": 30.066457871820333,
      "grad_norm": 7.513660907745361,
      "learning_rate": 1.2416927660224583e-05,
      "loss": 1.8012,
      "step": 393600
    },
    {
      "epoch": 30.074096707661752,
      "grad_norm": 6.746115684509277,
      "learning_rate": 1.240737911542281e-05,
      "loss": 1.7573,
      "step": 393700
    },
    {
      "epoch": 30.08173554350317,
      "grad_norm": 6.0289435386657715,
      "learning_rate": 1.2397830570621038e-05,
      "loss": 1.7455,
      "step": 393800
    },
    {
      "epoch": 30.089374379344587,
      "grad_norm": 8.224270820617676,
      "learning_rate": 1.2388282025819265e-05,
      "loss": 1.7201,
      "step": 393900
    },
    {
      "epoch": 30.097013215186006,
      "grad_norm": 6.803345680236816,
      "learning_rate": 1.2378733481017492e-05,
      "loss": 1.7362,
      "step": 394000
    },
    {
      "epoch": 30.104652051027422,
      "grad_norm": 7.567510604858398,
      "learning_rate": 1.2369184936215721e-05,
      "loss": 1.7539,
      "step": 394100
    },
    {
      "epoch": 30.11229088686884,
      "grad_norm": 7.047310829162598,
      "learning_rate": 1.235963639141395e-05,
      "loss": 1.8013,
      "step": 394200
    },
    {
      "epoch": 30.11992972271026,
      "grad_norm": 7.302912712097168,
      "learning_rate": 1.2350087846612177e-05,
      "loss": 1.737,
      "step": 394300
    },
    {
      "epoch": 30.127568558551676,
      "grad_norm": 10.329777717590332,
      "learning_rate": 1.2340539301810404e-05,
      "loss": 1.725,
      "step": 394400
    },
    {
      "epoch": 30.135207394393095,
      "grad_norm": 8.235647201538086,
      "learning_rate": 1.2330990757008633e-05,
      "loss": 1.7722,
      "step": 394500
    },
    {
      "epoch": 30.14284623023451,
      "grad_norm": 8.273592948913574,
      "learning_rate": 1.232144221220686e-05,
      "loss": 1.7113,
      "step": 394600
    },
    {
      "epoch": 30.15048506607593,
      "grad_norm": 7.349365711212158,
      "learning_rate": 1.2311893667405088e-05,
      "loss": 1.6733,
      "step": 394700
    },
    {
      "epoch": 30.15812390191735,
      "grad_norm": 9.093652725219727,
      "learning_rate": 1.2302345122603317e-05,
      "loss": 1.6671,
      "step": 394800
    },
    {
      "epoch": 30.165762737758765,
      "grad_norm": 6.518975257873535,
      "learning_rate": 1.2292796577801544e-05,
      "loss": 1.7272,
      "step": 394900
    },
    {
      "epoch": 30.173401573600184,
      "grad_norm": 5.513068675994873,
      "learning_rate": 1.2283248032999771e-05,
      "loss": 1.6804,
      "step": 395000
    },
    {
      "epoch": 30.1810404094416,
      "grad_norm": 8.673619270324707,
      "learning_rate": 1.2273699488198e-05,
      "loss": 1.6791,
      "step": 395100
    },
    {
      "epoch": 30.18867924528302,
      "grad_norm": 7.613314151763916,
      "learning_rate": 1.2264150943396227e-05,
      "loss": 1.785,
      "step": 395200
    },
    {
      "epoch": 30.196318081124435,
      "grad_norm": 9.592386245727539,
      "learning_rate": 1.2254602398594454e-05,
      "loss": 1.6817,
      "step": 395300
    },
    {
      "epoch": 30.203956916965854,
      "grad_norm": 7.026210784912109,
      "learning_rate": 1.2245053853792684e-05,
      "loss": 1.7032,
      "step": 395400
    },
    {
      "epoch": 30.211595752807273,
      "grad_norm": 8.585895538330078,
      "learning_rate": 1.2235505308990911e-05,
      "loss": 1.691,
      "step": 395500
    },
    {
      "epoch": 30.21923458864869,
      "grad_norm": 5.860722541809082,
      "learning_rate": 1.2225956764189138e-05,
      "loss": 1.7646,
      "step": 395600
    },
    {
      "epoch": 30.22687342449011,
      "grad_norm": 7.492475509643555,
      "learning_rate": 1.2216408219387367e-05,
      "loss": 1.7721,
      "step": 395700
    },
    {
      "epoch": 30.234512260331524,
      "grad_norm": 7.617966175079346,
      "learning_rate": 1.2206859674585594e-05,
      "loss": 1.7364,
      "step": 395800
    },
    {
      "epoch": 30.242151096172943,
      "grad_norm": 8.903218269348145,
      "learning_rate": 1.219731112978382e-05,
      "loss": 1.6008,
      "step": 395900
    },
    {
      "epoch": 30.249789932014362,
      "grad_norm": 6.405325889587402,
      "learning_rate": 1.218776258498205e-05,
      "loss": 1.7626,
      "step": 396000
    },
    {
      "epoch": 30.257428767855778,
      "grad_norm": 7.556180477142334,
      "learning_rate": 1.2178214040180278e-05,
      "loss": 1.7814,
      "step": 396100
    },
    {
      "epoch": 30.265067603697197,
      "grad_norm": 6.881820201873779,
      "learning_rate": 1.2168665495378505e-05,
      "loss": 1.6826,
      "step": 396200
    },
    {
      "epoch": 30.272706439538613,
      "grad_norm": 5.7100324630737305,
      "learning_rate": 1.2159116950576732e-05,
      "loss": 1.7429,
      "step": 396300
    },
    {
      "epoch": 30.280345275380032,
      "grad_norm": 9.6790189743042,
      "learning_rate": 1.214956840577496e-05,
      "loss": 1.6902,
      "step": 396400
    },
    {
      "epoch": 30.28798411122145,
      "grad_norm": 7.979316234588623,
      "learning_rate": 1.2140019860973188e-05,
      "loss": 1.6824,
      "step": 396500
    },
    {
      "epoch": 30.295622947062867,
      "grad_norm": 7.794018268585205,
      "learning_rate": 1.2130471316171416e-05,
      "loss": 1.7317,
      "step": 396600
    },
    {
      "epoch": 30.303261782904286,
      "grad_norm": 7.046475887298584,
      "learning_rate": 1.2120922771369645e-05,
      "loss": 1.6975,
      "step": 396700
    },
    {
      "epoch": 30.310900618745702,
      "grad_norm": 8.472150802612305,
      "learning_rate": 1.2111374226567872e-05,
      "loss": 1.7644,
      "step": 396800
    },
    {
      "epoch": 30.31853945458712,
      "grad_norm": 8.615058898925781,
      "learning_rate": 1.21018256817661e-05,
      "loss": 1.7821,
      "step": 396900
    },
    {
      "epoch": 30.32617829042854,
      "grad_norm": 12.777506828308105,
      "learning_rate": 1.2092277136964328e-05,
      "loss": 1.7453,
      "step": 397000
    },
    {
      "epoch": 30.333817126269956,
      "grad_norm": 7.082250118255615,
      "learning_rate": 1.2082728592162555e-05,
      "loss": 1.6984,
      "step": 397100
    },
    {
      "epoch": 30.341455962111375,
      "grad_norm": 7.921063423156738,
      "learning_rate": 1.2073180047360782e-05,
      "loss": 1.7753,
      "step": 397200
    },
    {
      "epoch": 30.34909479795279,
      "grad_norm": 7.4785261154174805,
      "learning_rate": 1.206363150255901e-05,
      "loss": 1.7642,
      "step": 397300
    },
    {
      "epoch": 30.35673363379421,
      "grad_norm": 8.07075023651123,
      "learning_rate": 1.205408295775724e-05,
      "loss": 1.8121,
      "step": 397400
    },
    {
      "epoch": 30.364372469635626,
      "grad_norm": 7.814085483551025,
      "learning_rate": 1.2044534412955466e-05,
      "loss": 1.6499,
      "step": 397500
    },
    {
      "epoch": 30.372011305477045,
      "grad_norm": 7.454904079437256,
      "learning_rate": 1.2034985868153693e-05,
      "loss": 1.7361,
      "step": 397600
    },
    {
      "epoch": 30.379650141318464,
      "grad_norm": 10.533411979675293,
      "learning_rate": 1.2025437323351922e-05,
      "loss": 1.7402,
      "step": 397700
    },
    {
      "epoch": 30.38728897715988,
      "grad_norm": 8.047045707702637,
      "learning_rate": 1.2015888778550149e-05,
      "loss": 1.7547,
      "step": 397800
    },
    {
      "epoch": 30.3949278130013,
      "grad_norm": 7.892969131469727,
      "learning_rate": 1.2006340233748378e-05,
      "loss": 1.779,
      "step": 397900
    },
    {
      "epoch": 30.402566648842715,
      "grad_norm": 10.915763854980469,
      "learning_rate": 1.1996791688946606e-05,
      "loss": 1.6925,
      "step": 398000
    },
    {
      "epoch": 30.410205484684134,
      "grad_norm": 7.954496383666992,
      "learning_rate": 1.1987243144144833e-05,
      "loss": 1.747,
      "step": 398100
    },
    {
      "epoch": 30.417844320525553,
      "grad_norm": 9.398475646972656,
      "learning_rate": 1.197769459934306e-05,
      "loss": 1.7414,
      "step": 398200
    },
    {
      "epoch": 30.42548315636697,
      "grad_norm": 8.290878295898438,
      "learning_rate": 1.1968146054541289e-05,
      "loss": 1.7936,
      "step": 398300
    },
    {
      "epoch": 30.433121992208388,
      "grad_norm": 7.34360933303833,
      "learning_rate": 1.1958597509739516e-05,
      "loss": 1.732,
      "step": 398400
    },
    {
      "epoch": 30.440760828049804,
      "grad_norm": 8.839238166809082,
      "learning_rate": 1.1949048964937743e-05,
      "loss": 1.7209,
      "step": 398500
    },
    {
      "epoch": 30.448399663891223,
      "grad_norm": 5.35386848449707,
      "learning_rate": 1.1939500420135972e-05,
      "loss": 1.6077,
      "step": 398600
    },
    {
      "epoch": 30.456038499732642,
      "grad_norm": 7.7104268074035645,
      "learning_rate": 1.19299518753342e-05,
      "loss": 1.6931,
      "step": 398700
    },
    {
      "epoch": 30.463677335574058,
      "grad_norm": 8.525594711303711,
      "learning_rate": 1.1920403330532427e-05,
      "loss": 1.7227,
      "step": 398800
    },
    {
      "epoch": 30.471316171415477,
      "grad_norm": 8.132527351379395,
      "learning_rate": 1.1910854785730654e-05,
      "loss": 1.7696,
      "step": 398900
    },
    {
      "epoch": 30.478955007256893,
      "grad_norm": 10.970449447631836,
      "learning_rate": 1.1901306240928883e-05,
      "loss": 1.6988,
      "step": 399000
    },
    {
      "epoch": 30.486593843098312,
      "grad_norm": 7.011094093322754,
      "learning_rate": 1.189175769612711e-05,
      "loss": 1.7262,
      "step": 399100
    },
    {
      "epoch": 30.49423267893973,
      "grad_norm": 8.676105499267578,
      "learning_rate": 1.1882209151325339e-05,
      "loss": 1.7242,
      "step": 399200
    },
    {
      "epoch": 30.501871514781147,
      "grad_norm": 6.151576042175293,
      "learning_rate": 1.1872660606523567e-05,
      "loss": 1.7177,
      "step": 399300
    },
    {
      "epoch": 30.509510350622566,
      "grad_norm": 7.993706226348877,
      "learning_rate": 1.1863112061721794e-05,
      "loss": 1.6559,
      "step": 399400
    },
    {
      "epoch": 30.517149186463982,
      "grad_norm": 7.457546234130859,
      "learning_rate": 1.1853563516920021e-05,
      "loss": 1.7063,
      "step": 399500
    },
    {
      "epoch": 30.5247880223054,
      "grad_norm": 6.229737758636475,
      "learning_rate": 1.184401497211825e-05,
      "loss": 1.7036,
      "step": 399600
    },
    {
      "epoch": 30.532426858146817,
      "grad_norm": 9.782356262207031,
      "learning_rate": 1.1834466427316477e-05,
      "loss": 1.7324,
      "step": 399700
    },
    {
      "epoch": 30.540065693988236,
      "grad_norm": 8.683356285095215,
      "learning_rate": 1.1824917882514706e-05,
      "loss": 1.7773,
      "step": 399800
    },
    {
      "epoch": 30.547704529829655,
      "grad_norm": 6.969141483306885,
      "learning_rate": 1.1815369337712935e-05,
      "loss": 1.6368,
      "step": 399900
    },
    {
      "epoch": 30.55534336567107,
      "grad_norm": 8.567267417907715,
      "learning_rate": 1.1805820792911162e-05,
      "loss": 1.8306,
      "step": 400000
    },
    {
      "epoch": 30.56298220151249,
      "grad_norm": 8.237399101257324,
      "learning_rate": 1.1796272248109389e-05,
      "loss": 1.728,
      "step": 400100
    },
    {
      "epoch": 30.570621037353906,
      "grad_norm": 8.116959571838379,
      "learning_rate": 1.1786723703307616e-05,
      "loss": 1.6072,
      "step": 400200
    },
    {
      "epoch": 30.578259873195325,
      "grad_norm": 5.988696098327637,
      "learning_rate": 1.1777175158505844e-05,
      "loss": 1.8098,
      "step": 400300
    },
    {
      "epoch": 30.585898709036744,
      "grad_norm": 7.596585273742676,
      "learning_rate": 1.1767626613704071e-05,
      "loss": 1.7675,
      "step": 400400
    },
    {
      "epoch": 30.59353754487816,
      "grad_norm": 7.390631675720215,
      "learning_rate": 1.17580780689023e-05,
      "loss": 1.6375,
      "step": 400500
    },
    {
      "epoch": 30.60117638071958,
      "grad_norm": 7.2702155113220215,
      "learning_rate": 1.1748529524100529e-05,
      "loss": 1.7008,
      "step": 400600
    },
    {
      "epoch": 30.608815216560995,
      "grad_norm": 9.59580135345459,
      "learning_rate": 1.1738980979298756e-05,
      "loss": 1.7484,
      "step": 400700
    },
    {
      "epoch": 30.616454052402414,
      "grad_norm": 8.567334175109863,
      "learning_rate": 1.1729432434496983e-05,
      "loss": 1.6845,
      "step": 400800
    },
    {
      "epoch": 30.624092888243833,
      "grad_norm": 7.079248428344727,
      "learning_rate": 1.1719883889695211e-05,
      "loss": 1.7597,
      "step": 400900
    },
    {
      "epoch": 30.63173172408525,
      "grad_norm": 8.123993873596191,
      "learning_rate": 1.1710335344893438e-05,
      "loss": 1.8318,
      "step": 401000
    },
    {
      "epoch": 30.639370559926668,
      "grad_norm": 8.706298828125,
      "learning_rate": 1.1700786800091667e-05,
      "loss": 1.7218,
      "step": 401100
    },
    {
      "epoch": 30.647009395768084,
      "grad_norm": 10.437071800231934,
      "learning_rate": 1.1691238255289896e-05,
      "loss": 1.7105,
      "step": 401200
    },
    {
      "epoch": 30.654648231609503,
      "grad_norm": 8.568779945373535,
      "learning_rate": 1.1681689710488123e-05,
      "loss": 1.7659,
      "step": 401300
    },
    {
      "epoch": 30.66228706745092,
      "grad_norm": 9.179929733276367,
      "learning_rate": 1.167214116568635e-05,
      "loss": 1.6967,
      "step": 401400
    },
    {
      "epoch": 30.669925903292338,
      "grad_norm": 7.172037601470947,
      "learning_rate": 1.1662592620884578e-05,
      "loss": 1.7457,
      "step": 401500
    },
    {
      "epoch": 30.677564739133757,
      "grad_norm": 9.54345417022705,
      "learning_rate": 1.1653044076082805e-05,
      "loss": 1.6851,
      "step": 401600
    },
    {
      "epoch": 30.685203574975173,
      "grad_norm": 7.162028789520264,
      "learning_rate": 1.1643495531281032e-05,
      "loss": 1.6835,
      "step": 401700
    },
    {
      "epoch": 30.692842410816592,
      "grad_norm": 7.266986846923828,
      "learning_rate": 1.1633946986479261e-05,
      "loss": 1.7392,
      "step": 401800
    },
    {
      "epoch": 30.700481246658008,
      "grad_norm": 8.876776695251465,
      "learning_rate": 1.162439844167749e-05,
      "loss": 1.7721,
      "step": 401900
    },
    {
      "epoch": 30.708120082499427,
      "grad_norm": 6.623115539550781,
      "learning_rate": 1.1614849896875717e-05,
      "loss": 1.698,
      "step": 402000
    },
    {
      "epoch": 30.715758918340846,
      "grad_norm": 7.514226913452148,
      "learning_rate": 1.1605301352073944e-05,
      "loss": 1.7143,
      "step": 402100
    },
    {
      "epoch": 30.723397754182262,
      "grad_norm": 7.763564586639404,
      "learning_rate": 1.1595752807272172e-05,
      "loss": 1.7417,
      "step": 402200
    },
    {
      "epoch": 30.73103659002368,
      "grad_norm": 8.097814559936523,
      "learning_rate": 1.15862042624704e-05,
      "loss": 1.6929,
      "step": 402300
    },
    {
      "epoch": 30.738675425865097,
      "grad_norm": 8.641863822937012,
      "learning_rate": 1.1576655717668628e-05,
      "loss": 1.7521,
      "step": 402400
    },
    {
      "epoch": 30.746314261706516,
      "grad_norm": 7.424339771270752,
      "learning_rate": 1.1567107172866857e-05,
      "loss": 1.7933,
      "step": 402500
    },
    {
      "epoch": 30.753953097547935,
      "grad_norm": 6.390678882598877,
      "learning_rate": 1.1557558628065084e-05,
      "loss": 1.798,
      "step": 402600
    },
    {
      "epoch": 30.76159193338935,
      "grad_norm": 8.019241333007812,
      "learning_rate": 1.154801008326331e-05,
      "loss": 1.6778,
      "step": 402700
    },
    {
      "epoch": 30.76923076923077,
      "grad_norm": 9.12142276763916,
      "learning_rate": 1.153846153846154e-05,
      "loss": 1.8201,
      "step": 402800
    },
    {
      "epoch": 30.776869605072186,
      "grad_norm": 7.532639980316162,
      "learning_rate": 1.1528912993659767e-05,
      "loss": 1.6574,
      "step": 402900
    },
    {
      "epoch": 30.784508440913605,
      "grad_norm": 7.073253154754639,
      "learning_rate": 1.1519364448857994e-05,
      "loss": 1.7555,
      "step": 403000
    },
    {
      "epoch": 30.792147276755024,
      "grad_norm": 12.459864616394043,
      "learning_rate": 1.1509815904056222e-05,
      "loss": 1.7574,
      "step": 403100
    },
    {
      "epoch": 30.79978611259644,
      "grad_norm": 9.962451934814453,
      "learning_rate": 1.1500267359254451e-05,
      "loss": 1.6961,
      "step": 403200
    },
    {
      "epoch": 30.80742494843786,
      "grad_norm": 8.632837295532227,
      "learning_rate": 1.1490718814452678e-05,
      "loss": 1.7205,
      "step": 403300
    },
    {
      "epoch": 30.815063784279275,
      "grad_norm": 8.004520416259766,
      "learning_rate": 1.1481170269650905e-05,
      "loss": 1.7944,
      "step": 403400
    },
    {
      "epoch": 30.822702620120694,
      "grad_norm": 3.9972634315490723,
      "learning_rate": 1.1471621724849134e-05,
      "loss": 1.6806,
      "step": 403500
    },
    {
      "epoch": 30.830341455962113,
      "grad_norm": 8.027082443237305,
      "learning_rate": 1.146207318004736e-05,
      "loss": 1.7483,
      "step": 403600
    },
    {
      "epoch": 30.83798029180353,
      "grad_norm": 8.021754264831543,
      "learning_rate": 1.145252463524559e-05,
      "loss": 1.7562,
      "step": 403700
    },
    {
      "epoch": 30.845619127644948,
      "grad_norm": 6.436664581298828,
      "learning_rate": 1.1442976090443818e-05,
      "loss": 1.7095,
      "step": 403800
    },
    {
      "epoch": 30.853257963486364,
      "grad_norm": 8.036518096923828,
      "learning_rate": 1.1433427545642045e-05,
      "loss": 1.7296,
      "step": 403900
    },
    {
      "epoch": 30.860896799327783,
      "grad_norm": 9.389032363891602,
      "learning_rate": 1.1423879000840272e-05,
      "loss": 1.6973,
      "step": 404000
    },
    {
      "epoch": 30.8685356351692,
      "grad_norm": 8.130361557006836,
      "learning_rate": 1.14143304560385e-05,
      "loss": 1.6844,
      "step": 404100
    },
    {
      "epoch": 30.876174471010618,
      "grad_norm": 3.5910046100616455,
      "learning_rate": 1.1404781911236728e-05,
      "loss": 1.7283,
      "step": 404200
    },
    {
      "epoch": 30.883813306852037,
      "grad_norm": 9.683473587036133,
      "learning_rate": 1.1395233366434956e-05,
      "loss": 1.7395,
      "step": 404300
    },
    {
      "epoch": 30.891452142693453,
      "grad_norm": 6.87666654586792,
      "learning_rate": 1.1385684821633185e-05,
      "loss": 1.6801,
      "step": 404400
    },
    {
      "epoch": 30.899090978534872,
      "grad_norm": 8.815841674804688,
      "learning_rate": 1.1376136276831412e-05,
      "loss": 1.7965,
      "step": 404500
    },
    {
      "epoch": 30.906729814376288,
      "grad_norm": 7.074377059936523,
      "learning_rate": 1.1366587732029639e-05,
      "loss": 1.7686,
      "step": 404600
    },
    {
      "epoch": 30.914368650217707,
      "grad_norm": 6.053150177001953,
      "learning_rate": 1.1357039187227866e-05,
      "loss": 1.7611,
      "step": 404700
    },
    {
      "epoch": 30.922007486059126,
      "grad_norm": 7.560431957244873,
      "learning_rate": 1.1347490642426095e-05,
      "loss": 1.7445,
      "step": 404800
    },
    {
      "epoch": 30.92964632190054,
      "grad_norm": 7.610212802886963,
      "learning_rate": 1.1337942097624322e-05,
      "loss": 1.7544,
      "step": 404900
    },
    {
      "epoch": 30.93728515774196,
      "grad_norm": 7.691734313964844,
      "learning_rate": 1.132839355282255e-05,
      "loss": 1.7216,
      "step": 405000
    },
    {
      "epoch": 30.944923993583377,
      "grad_norm": 7.694296836853027,
      "learning_rate": 1.1318845008020779e-05,
      "loss": 1.6911,
      "step": 405100
    },
    {
      "epoch": 30.952562829424796,
      "grad_norm": 8.241561889648438,
      "learning_rate": 1.1309296463219006e-05,
      "loss": 1.716,
      "step": 405200
    },
    {
      "epoch": 30.960201665266215,
      "grad_norm": 5.540658950805664,
      "learning_rate": 1.1299747918417233e-05,
      "loss": 1.7619,
      "step": 405300
    },
    {
      "epoch": 30.96784050110763,
      "grad_norm": 6.986551761627197,
      "learning_rate": 1.1290199373615462e-05,
      "loss": 1.6743,
      "step": 405400
    },
    {
      "epoch": 30.97547933694905,
      "grad_norm": 6.632750034332275,
      "learning_rate": 1.1280650828813689e-05,
      "loss": 1.7888,
      "step": 405500
    },
    {
      "epoch": 30.983118172790466,
      "grad_norm": 7.5796122550964355,
      "learning_rate": 1.1271102284011918e-05,
      "loss": 1.7615,
      "step": 405600
    },
    {
      "epoch": 30.990757008631885,
      "grad_norm": 8.26540756225586,
      "learning_rate": 1.1261553739210146e-05,
      "loss": 1.638,
      "step": 405700
    },
    {
      "epoch": 30.9983958444733,
      "grad_norm": 8.0700044631958,
      "learning_rate": 1.1252005194408373e-05,
      "loss": 1.7584,
      "step": 405800
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.7960145473480225,
      "eval_runtime": 1.474,
      "eval_samples_per_second": 468.114,
      "eval_steps_per_second": 468.114,
      "step": 405821
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.488559365272522,
      "eval_runtime": 28.1054,
      "eval_samples_per_second": 465.783,
      "eval_steps_per_second": 465.783,
      "step": 405821
    },
    {
      "epoch": 31.00603468031472,
      "grad_norm": 9.214900016784668,
      "learning_rate": 1.12424566496066e-05,
      "loss": 1.7099,
      "step": 405900
    },
    {
      "epoch": 31.01367351615614,
      "grad_norm": 7.934974193572998,
      "learning_rate": 1.1232908104804827e-05,
      "loss": 1.8223,
      "step": 406000
    },
    {
      "epoch": 31.021312351997555,
      "grad_norm": 8.959312438964844,
      "learning_rate": 1.1223359560003056e-05,
      "loss": 1.6662,
      "step": 406100
    },
    {
      "epoch": 31.028951187838974,
      "grad_norm": 7.846477031707764,
      "learning_rate": 1.1213811015201283e-05,
      "loss": 1.7693,
      "step": 406200
    },
    {
      "epoch": 31.03659002368039,
      "grad_norm": 10.742258071899414,
      "learning_rate": 1.1204262470399512e-05,
      "loss": 1.7075,
      "step": 406300
    },
    {
      "epoch": 31.04422885952181,
      "grad_norm": 7.396917819976807,
      "learning_rate": 1.119471392559774e-05,
      "loss": 1.7578,
      "step": 406400
    },
    {
      "epoch": 31.051867695363228,
      "grad_norm": 7.015844821929932,
      "learning_rate": 1.1185165380795967e-05,
      "loss": 1.7544,
      "step": 406500
    },
    {
      "epoch": 31.059506531204644,
      "grad_norm": 8.022544860839844,
      "learning_rate": 1.1175616835994194e-05,
      "loss": 1.623,
      "step": 406600
    },
    {
      "epoch": 31.067145367046063,
      "grad_norm": 7.685300827026367,
      "learning_rate": 1.1166068291192423e-05,
      "loss": 1.6862,
      "step": 406700
    },
    {
      "epoch": 31.07478420288748,
      "grad_norm": 7.745612621307373,
      "learning_rate": 1.115651974639065e-05,
      "loss": 1.6843,
      "step": 406800
    },
    {
      "epoch": 31.082423038728898,
      "grad_norm": 6.413073539733887,
      "learning_rate": 1.1146971201588879e-05,
      "loss": 1.607,
      "step": 406900
    },
    {
      "epoch": 31.090061874570317,
      "grad_norm": 7.426509857177734,
      "learning_rate": 1.1137422656787107e-05,
      "loss": 1.8296,
      "step": 407000
    },
    {
      "epoch": 31.097700710411733,
      "grad_norm": 8.590378761291504,
      "learning_rate": 1.1127874111985334e-05,
      "loss": 1.6615,
      "step": 407100
    },
    {
      "epoch": 31.105339546253152,
      "grad_norm": 8.074493408203125,
      "learning_rate": 1.1118325567183561e-05,
      "loss": 1.7353,
      "step": 407200
    },
    {
      "epoch": 31.112978382094568,
      "grad_norm": 5.959928512573242,
      "learning_rate": 1.1108777022381788e-05,
      "loss": 1.6501,
      "step": 407300
    },
    {
      "epoch": 31.120617217935987,
      "grad_norm": 7.760273456573486,
      "learning_rate": 1.1099228477580017e-05,
      "loss": 1.7587,
      "step": 407400
    },
    {
      "epoch": 31.128256053777406,
      "grad_norm": 13.83344841003418,
      "learning_rate": 1.1089679932778246e-05,
      "loss": 1.693,
      "step": 407500
    },
    {
      "epoch": 31.13589488961882,
      "grad_norm": 8.099424362182617,
      "learning_rate": 1.1080131387976473e-05,
      "loss": 1.7691,
      "step": 407600
    },
    {
      "epoch": 31.14353372546024,
      "grad_norm": 5.4638471603393555,
      "learning_rate": 1.1070582843174701e-05,
      "loss": 1.7446,
      "step": 407700
    },
    {
      "epoch": 31.151172561301657,
      "grad_norm": 7.442184925079346,
      "learning_rate": 1.1061034298372928e-05,
      "loss": 1.7133,
      "step": 407800
    },
    {
      "epoch": 31.158811397143076,
      "grad_norm": 8.034246444702148,
      "learning_rate": 1.1051485753571155e-05,
      "loss": 1.7439,
      "step": 407900
    },
    {
      "epoch": 31.16645023298449,
      "grad_norm": 11.35301685333252,
      "learning_rate": 1.1041937208769384e-05,
      "loss": 1.7175,
      "step": 408000
    },
    {
      "epoch": 31.17408906882591,
      "grad_norm": 5.632691860198975,
      "learning_rate": 1.1032388663967611e-05,
      "loss": 1.6884,
      "step": 408100
    },
    {
      "epoch": 31.18172790466733,
      "grad_norm": 8.603410720825195,
      "learning_rate": 1.102284011916584e-05,
      "loss": 1.6996,
      "step": 408200
    },
    {
      "epoch": 31.189366740508746,
      "grad_norm": 11.037927627563477,
      "learning_rate": 1.1013291574364069e-05,
      "loss": 1.7464,
      "step": 408300
    },
    {
      "epoch": 31.197005576350165,
      "grad_norm": 6.044356822967529,
      "learning_rate": 1.1003743029562296e-05,
      "loss": 1.7757,
      "step": 408400
    },
    {
      "epoch": 31.20464441219158,
      "grad_norm": 4.903772830963135,
      "learning_rate": 1.0994194484760523e-05,
      "loss": 1.7951,
      "step": 408500
    },
    {
      "epoch": 31.212283248033,
      "grad_norm": 7.5295939445495605,
      "learning_rate": 1.0984645939958751e-05,
      "loss": 1.8066,
      "step": 408600
    },
    {
      "epoch": 31.21992208387442,
      "grad_norm": 5.939337730407715,
      "learning_rate": 1.0975097395156978e-05,
      "loss": 1.6758,
      "step": 408700
    },
    {
      "epoch": 31.227560919715835,
      "grad_norm": 9.66260051727295,
      "learning_rate": 1.0965548850355207e-05,
      "loss": 1.6342,
      "step": 408800
    },
    {
      "epoch": 31.235199755557254,
      "grad_norm": 10.880708694458008,
      "learning_rate": 1.0956000305553434e-05,
      "loss": 1.7267,
      "step": 408900
    },
    {
      "epoch": 31.24283859139867,
      "grad_norm": 7.838746070861816,
      "learning_rate": 1.0946451760751663e-05,
      "loss": 1.744,
      "step": 409000
    },
    {
      "epoch": 31.25047742724009,
      "grad_norm": 6.339502811431885,
      "learning_rate": 1.093690321594989e-05,
      "loss": 1.6955,
      "step": 409100
    },
    {
      "epoch": 31.258116263081508,
      "grad_norm": 5.911495208740234,
      "learning_rate": 1.0927354671148117e-05,
      "loss": 1.6228,
      "step": 409200
    },
    {
      "epoch": 31.265755098922924,
      "grad_norm": 8.567012786865234,
      "learning_rate": 1.0917806126346345e-05,
      "loss": 1.7171,
      "step": 409300
    },
    {
      "epoch": 31.273393934764343,
      "grad_norm": 9.176904678344727,
      "learning_rate": 1.0908257581544572e-05,
      "loss": 1.7356,
      "step": 409400
    },
    {
      "epoch": 31.28103277060576,
      "grad_norm": 7.523628234863281,
      "learning_rate": 1.0898709036742801e-05,
      "loss": 1.6404,
      "step": 409500
    },
    {
      "epoch": 31.288671606447178,
      "grad_norm": 8.429192543029785,
      "learning_rate": 1.088916049194103e-05,
      "loss": 1.7137,
      "step": 409600
    },
    {
      "epoch": 31.296310442288597,
      "grad_norm": 6.574237823486328,
      "learning_rate": 1.0879611947139257e-05,
      "loss": 1.7168,
      "step": 409700
    },
    {
      "epoch": 31.303949278130013,
      "grad_norm": 5.004671573638916,
      "learning_rate": 1.0870063402337484e-05,
      "loss": 1.7561,
      "step": 409800
    },
    {
      "epoch": 31.311588113971432,
      "grad_norm": 6.260571002960205,
      "learning_rate": 1.0860514857535712e-05,
      "loss": 1.6737,
      "step": 409900
    },
    {
      "epoch": 31.319226949812847,
      "grad_norm": 8.111743927001953,
      "learning_rate": 1.085096631273394e-05,
      "loss": 1.7388,
      "step": 410000
    },
    {
      "epoch": 31.326865785654267,
      "grad_norm": 5.99464750289917,
      "learning_rate": 1.0841417767932168e-05,
      "loss": 1.6843,
      "step": 410100
    },
    {
      "epoch": 31.334504621495682,
      "grad_norm": 5.761244773864746,
      "learning_rate": 1.0831869223130395e-05,
      "loss": 1.7289,
      "step": 410200
    },
    {
      "epoch": 31.3421434573371,
      "grad_norm": 10.060943603515625,
      "learning_rate": 1.0822320678328624e-05,
      "loss": 1.7062,
      "step": 410300
    },
    {
      "epoch": 31.34978229317852,
      "grad_norm": 7.677520751953125,
      "learning_rate": 1.081277213352685e-05,
      "loss": 1.7303,
      "step": 410400
    },
    {
      "epoch": 31.357421129019937,
      "grad_norm": 7.447815895080566,
      "learning_rate": 1.0803223588725078e-05,
      "loss": 1.8429,
      "step": 410500
    },
    {
      "epoch": 31.365059964861356,
      "grad_norm": 7.345260143280029,
      "learning_rate": 1.0793675043923306e-05,
      "loss": 1.6559,
      "step": 410600
    },
    {
      "epoch": 31.37269880070277,
      "grad_norm": 5.952193260192871,
      "learning_rate": 1.0784126499121535e-05,
      "loss": 1.7891,
      "step": 410700
    },
    {
      "epoch": 31.38033763654419,
      "grad_norm": 7.045895099639893,
      "learning_rate": 1.0774577954319762e-05,
      "loss": 1.7679,
      "step": 410800
    },
    {
      "epoch": 31.38797647238561,
      "grad_norm": 7.66488790512085,
      "learning_rate": 1.076502940951799e-05,
      "loss": 1.7166,
      "step": 410900
    },
    {
      "epoch": 31.395615308227026,
      "grad_norm": 5.869099140167236,
      "learning_rate": 1.0755480864716218e-05,
      "loss": 1.8019,
      "step": 411000
    },
    {
      "epoch": 31.403254144068445,
      "grad_norm": 9.597649574279785,
      "learning_rate": 1.0745932319914445e-05,
      "loss": 1.7354,
      "step": 411100
    },
    {
      "epoch": 31.41089297990986,
      "grad_norm": 9.896873474121094,
      "learning_rate": 1.0736383775112674e-05,
      "loss": 1.712,
      "step": 411200
    },
    {
      "epoch": 31.41853181575128,
      "grad_norm": 7.562180995941162,
      "learning_rate": 1.07268352303109e-05,
      "loss": 1.709,
      "step": 411300
    },
    {
      "epoch": 31.4261706515927,
      "grad_norm": 7.49486780166626,
      "learning_rate": 1.071728668550913e-05,
      "loss": 1.6868,
      "step": 411400
    },
    {
      "epoch": 31.433809487434115,
      "grad_norm": 12.497862815856934,
      "learning_rate": 1.0707738140707358e-05,
      "loss": 1.7226,
      "step": 411500
    },
    {
      "epoch": 31.441448323275534,
      "grad_norm": 6.241755485534668,
      "learning_rate": 1.0698189595905585e-05,
      "loss": 1.6232,
      "step": 411600
    },
    {
      "epoch": 31.44908715911695,
      "grad_norm": 7.54902458190918,
      "learning_rate": 1.0688641051103812e-05,
      "loss": 1.766,
      "step": 411700
    },
    {
      "epoch": 31.45672599495837,
      "grad_norm": 5.379391193389893,
      "learning_rate": 1.0679092506302039e-05,
      "loss": 1.7593,
      "step": 411800
    },
    {
      "epoch": 31.464364830799788,
      "grad_norm": 9.713838577270508,
      "learning_rate": 1.0669543961500268e-05,
      "loss": 1.7665,
      "step": 411900
    },
    {
      "epoch": 31.472003666641204,
      "grad_norm": 8.20556640625,
      "learning_rate": 1.0659995416698496e-05,
      "loss": 1.7397,
      "step": 412000
    },
    {
      "epoch": 31.479642502482623,
      "grad_norm": 7.558310031890869,
      "learning_rate": 1.0650446871896723e-05,
      "loss": 1.7689,
      "step": 412100
    },
    {
      "epoch": 31.48728133832404,
      "grad_norm": 6.631275653839111,
      "learning_rate": 1.0640898327094952e-05,
      "loss": 1.7407,
      "step": 412200
    },
    {
      "epoch": 31.494920174165458,
      "grad_norm": 7.146749019622803,
      "learning_rate": 1.0631349782293179e-05,
      "loss": 1.6669,
      "step": 412300
    },
    {
      "epoch": 31.502559010006873,
      "grad_norm": 6.494352340698242,
      "learning_rate": 1.0621801237491406e-05,
      "loss": 1.8298,
      "step": 412400
    },
    {
      "epoch": 31.510197845848293,
      "grad_norm": 8.661918640136719,
      "learning_rate": 1.0612252692689635e-05,
      "loss": 1.7007,
      "step": 412500
    },
    {
      "epoch": 31.51783668168971,
      "grad_norm": 8.701423645019531,
      "learning_rate": 1.0602704147887862e-05,
      "loss": 1.8559,
      "step": 412600
    },
    {
      "epoch": 31.525475517531127,
      "grad_norm": 10.31277084350586,
      "learning_rate": 1.059315560308609e-05,
      "loss": 1.742,
      "step": 412700
    },
    {
      "epoch": 31.533114353372547,
      "grad_norm": 5.711704730987549,
      "learning_rate": 1.0583607058284319e-05,
      "loss": 1.7825,
      "step": 412800
    },
    {
      "epoch": 31.540753189213962,
      "grad_norm": 6.45315408706665,
      "learning_rate": 1.0574058513482546e-05,
      "loss": 1.7143,
      "step": 412900
    },
    {
      "epoch": 31.54839202505538,
      "grad_norm": 8.423368453979492,
      "learning_rate": 1.0564509968680773e-05,
      "loss": 1.7675,
      "step": 413000
    },
    {
      "epoch": 31.5560308608968,
      "grad_norm": 7.6262102127075195,
      "learning_rate": 1.0554961423879e-05,
      "loss": 1.7294,
      "step": 413100
    },
    {
      "epoch": 31.563669696738216,
      "grad_norm": 6.079408645629883,
      "learning_rate": 1.0545412879077229e-05,
      "loss": 1.6732,
      "step": 413200
    },
    {
      "epoch": 31.571308532579636,
      "grad_norm": 6.926796913146973,
      "learning_rate": 1.0535864334275457e-05,
      "loss": 1.7838,
      "step": 413300
    },
    {
      "epoch": 31.57894736842105,
      "grad_norm": 9.230706214904785,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.6378,
      "step": 413400
    },
    {
      "epoch": 31.58658620426247,
      "grad_norm": 7.437566757202148,
      "learning_rate": 1.0516767244671913e-05,
      "loss": 1.7787,
      "step": 413500
    },
    {
      "epoch": 31.59422504010389,
      "grad_norm": 9.345224380493164,
      "learning_rate": 1.050721869987014e-05,
      "loss": 1.7178,
      "step": 413600
    },
    {
      "epoch": 31.601863875945305,
      "grad_norm": 9.691266059875488,
      "learning_rate": 1.0497670155068367e-05,
      "loss": 1.6988,
      "step": 413700
    },
    {
      "epoch": 31.609502711786725,
      "grad_norm": 7.238393306732178,
      "learning_rate": 1.0488121610266596e-05,
      "loss": 1.6999,
      "step": 413800
    },
    {
      "epoch": 31.61714154762814,
      "grad_norm": 7.648535251617432,
      "learning_rate": 1.0478573065464825e-05,
      "loss": 1.7504,
      "step": 413900
    },
    {
      "epoch": 31.62478038346956,
      "grad_norm": 7.68802547454834,
      "learning_rate": 1.0469024520663052e-05,
      "loss": 1.6891,
      "step": 414000
    },
    {
      "epoch": 31.632419219310975,
      "grad_norm": 5.013835430145264,
      "learning_rate": 1.045947597586128e-05,
      "loss": 1.6562,
      "step": 414100
    },
    {
      "epoch": 31.640058055152394,
      "grad_norm": 8.200159072875977,
      "learning_rate": 1.0449927431059507e-05,
      "loss": 1.8095,
      "step": 414200
    },
    {
      "epoch": 31.647696890993814,
      "grad_norm": 8.728631019592285,
      "learning_rate": 1.0440378886257734e-05,
      "loss": 1.6412,
      "step": 414300
    },
    {
      "epoch": 31.65533572683523,
      "grad_norm": 7.325597286224365,
      "learning_rate": 1.0430830341455963e-05,
      "loss": 1.7601,
      "step": 414400
    },
    {
      "epoch": 31.66297456267665,
      "grad_norm": 7.358796119689941,
      "learning_rate": 1.042128179665419e-05,
      "loss": 1.6685,
      "step": 414500
    },
    {
      "epoch": 31.670613398518064,
      "grad_norm": 6.272096633911133,
      "learning_rate": 1.0411733251852419e-05,
      "loss": 1.6944,
      "step": 414600
    },
    {
      "epoch": 31.678252234359483,
      "grad_norm": 8.032960891723633,
      "learning_rate": 1.0402184707050646e-05,
      "loss": 1.6927,
      "step": 414700
    },
    {
      "epoch": 31.685891070200903,
      "grad_norm": 5.0311689376831055,
      "learning_rate": 1.0392636162248874e-05,
      "loss": 1.7015,
      "step": 414800
    },
    {
      "epoch": 31.69352990604232,
      "grad_norm": 9.605707168579102,
      "learning_rate": 1.0383087617447101e-05,
      "loss": 1.7674,
      "step": 414900
    },
    {
      "epoch": 31.701168741883738,
      "grad_norm": 6.897519111633301,
      "learning_rate": 1.0373539072645328e-05,
      "loss": 1.722,
      "step": 415000
    },
    {
      "epoch": 31.708807577725153,
      "grad_norm": 7.361019611358643,
      "learning_rate": 1.0363990527843557e-05,
      "loss": 1.6306,
      "step": 415100
    },
    {
      "epoch": 31.716446413566572,
      "grad_norm": 9.59400463104248,
      "learning_rate": 1.0354441983041786e-05,
      "loss": 1.768,
      "step": 415200
    },
    {
      "epoch": 31.72408524940799,
      "grad_norm": 7.244360446929932,
      "learning_rate": 1.0344893438240013e-05,
      "loss": 1.8462,
      "step": 415300
    },
    {
      "epoch": 31.731724085249407,
      "grad_norm": 7.5842671394348145,
      "learning_rate": 1.0335344893438241e-05,
      "loss": 1.6517,
      "step": 415400
    },
    {
      "epoch": 31.739362921090827,
      "grad_norm": 7.1385273933410645,
      "learning_rate": 1.0325796348636468e-05,
      "loss": 1.621,
      "step": 415500
    },
    {
      "epoch": 31.747001756932242,
      "grad_norm": 10.846230506896973,
      "learning_rate": 1.0316247803834695e-05,
      "loss": 1.6395,
      "step": 415600
    },
    {
      "epoch": 31.75464059277366,
      "grad_norm": 8.309672355651855,
      "learning_rate": 1.0306699259032924e-05,
      "loss": 1.7806,
      "step": 415700
    },
    {
      "epoch": 31.76227942861508,
      "grad_norm": 7.050225734710693,
      "learning_rate": 1.0297150714231151e-05,
      "loss": 1.6734,
      "step": 415800
    },
    {
      "epoch": 31.769918264456496,
      "grad_norm": 7.052569389343262,
      "learning_rate": 1.028760216942938e-05,
      "loss": 1.6782,
      "step": 415900
    },
    {
      "epoch": 31.777557100297916,
      "grad_norm": 7.7615790367126465,
      "learning_rate": 1.0278053624627607e-05,
      "loss": 1.7121,
      "step": 416000
    },
    {
      "epoch": 31.78519593613933,
      "grad_norm": 8.337666511535645,
      "learning_rate": 1.0268505079825835e-05,
      "loss": 1.7817,
      "step": 416100
    },
    {
      "epoch": 31.79283477198075,
      "grad_norm": 8.957999229431152,
      "learning_rate": 1.0258956535024062e-05,
      "loss": 1.7614,
      "step": 416200
    },
    {
      "epoch": 31.80047360782217,
      "grad_norm": 5.6573615074157715,
      "learning_rate": 1.024940799022229e-05,
      "loss": 1.8046,
      "step": 416300
    },
    {
      "epoch": 31.808112443663585,
      "grad_norm": 9.92068862915039,
      "learning_rate": 1.0239859445420518e-05,
      "loss": 1.6247,
      "step": 416400
    },
    {
      "epoch": 31.815751279505005,
      "grad_norm": 7.405957221984863,
      "learning_rate": 1.0230310900618747e-05,
      "loss": 1.7563,
      "step": 416500
    },
    {
      "epoch": 31.82339011534642,
      "grad_norm": 10.026090621948242,
      "learning_rate": 1.0220762355816974e-05,
      "loss": 1.6322,
      "step": 416600
    },
    {
      "epoch": 31.83102895118784,
      "grad_norm": 9.101522445678711,
      "learning_rate": 1.0211213811015203e-05,
      "loss": 1.7494,
      "step": 416700
    },
    {
      "epoch": 31.838667787029255,
      "grad_norm": 9.015856742858887,
      "learning_rate": 1.020166526621343e-05,
      "loss": 1.7952,
      "step": 416800
    },
    {
      "epoch": 31.846306622870674,
      "grad_norm": 9.224283218383789,
      "learning_rate": 1.0192116721411657e-05,
      "loss": 1.748,
      "step": 416900
    },
    {
      "epoch": 31.853945458712094,
      "grad_norm": 5.8194708824157715,
      "learning_rate": 1.0182568176609885e-05,
      "loss": 1.7214,
      "step": 417000
    },
    {
      "epoch": 31.86158429455351,
      "grad_norm": 6.828210830688477,
      "learning_rate": 1.0173019631808114e-05,
      "loss": 1.6529,
      "step": 417100
    },
    {
      "epoch": 31.86922313039493,
      "grad_norm": 6.103082180023193,
      "learning_rate": 1.0163471087006341e-05,
      "loss": 1.6621,
      "step": 417200
    },
    {
      "epoch": 31.876861966236344,
      "grad_norm": 7.5549726486206055,
      "learning_rate": 1.015392254220457e-05,
      "loss": 1.803,
      "step": 417300
    },
    {
      "epoch": 31.884500802077763,
      "grad_norm": 7.507296562194824,
      "learning_rate": 1.0144373997402797e-05,
      "loss": 1.7015,
      "step": 417400
    },
    {
      "epoch": 31.892139637919183,
      "grad_norm": 8.287789344787598,
      "learning_rate": 1.0134825452601024e-05,
      "loss": 1.6701,
      "step": 417500
    },
    {
      "epoch": 31.8997784737606,
      "grad_norm": 9.946696281433105,
      "learning_rate": 1.012527690779925e-05,
      "loss": 1.7437,
      "step": 417600
    },
    {
      "epoch": 31.907417309602017,
      "grad_norm": 10.489898681640625,
      "learning_rate": 1.011572836299748e-05,
      "loss": 1.7042,
      "step": 417700
    },
    {
      "epoch": 31.915056145443433,
      "grad_norm": 14.155102729797363,
      "learning_rate": 1.0106179818195708e-05,
      "loss": 1.741,
      "step": 417800
    },
    {
      "epoch": 31.922694981284852,
      "grad_norm": 5.829087734222412,
      "learning_rate": 1.0096631273393935e-05,
      "loss": 1.745,
      "step": 417900
    },
    {
      "epoch": 31.93033381712627,
      "grad_norm": 5.719164848327637,
      "learning_rate": 1.0087082728592164e-05,
      "loss": 1.7186,
      "step": 418000
    },
    {
      "epoch": 31.937972652967687,
      "grad_norm": 8.2576265335083,
      "learning_rate": 1.007753418379039e-05,
      "loss": 1.7099,
      "step": 418100
    },
    {
      "epoch": 31.945611488809107,
      "grad_norm": 8.398260116577148,
      "learning_rate": 1.0067985638988618e-05,
      "loss": 1.7404,
      "step": 418200
    },
    {
      "epoch": 31.953250324650522,
      "grad_norm": 9.489703178405762,
      "learning_rate": 1.0058437094186846e-05,
      "loss": 1.6535,
      "step": 418300
    },
    {
      "epoch": 31.96088916049194,
      "grad_norm": 8.733772277832031,
      "learning_rate": 1.0048888549385075e-05,
      "loss": 1.826,
      "step": 418400
    },
    {
      "epoch": 31.968527996333357,
      "grad_norm": 7.480605125427246,
      "learning_rate": 1.0039340004583302e-05,
      "loss": 1.7061,
      "step": 418500
    },
    {
      "epoch": 31.976166832174776,
      "grad_norm": 7.845168113708496,
      "learning_rate": 1.002979145978153e-05,
      "loss": 1.7241,
      "step": 418600
    },
    {
      "epoch": 31.983805668016196,
      "grad_norm": 7.883364677429199,
      "learning_rate": 1.0020242914979758e-05,
      "loss": 1.5751,
      "step": 418700
    },
    {
      "epoch": 31.99144450385761,
      "grad_norm": 8.703290939331055,
      "learning_rate": 1.0010694370177985e-05,
      "loss": 1.7642,
      "step": 418800
    },
    {
      "epoch": 31.99908333969903,
      "grad_norm": 8.02507495880127,
      "learning_rate": 1.0001145825376212e-05,
      "loss": 1.8175,
      "step": 418900
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.7934424877166748,
      "eval_runtime": 1.4824,
      "eval_samples_per_second": 465.475,
      "eval_steps_per_second": 465.475,
      "step": 418912
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.4863914251327515,
      "eval_runtime": 27.9837,
      "eval_samples_per_second": 467.808,
      "eval_steps_per_second": 467.808,
      "step": 418912
    },
    {
      "epoch": 32.006722175540446,
      "grad_norm": 6.773228645324707,
      "learning_rate": 9.99159728057444e-06,
      "loss": 1.77,
      "step": 419000
    },
    {
      "epoch": 32.014361011381865,
      "grad_norm": 11.899232864379883,
      "learning_rate": 9.982048735772669e-06,
      "loss": 1.7355,
      "step": 419100
    },
    {
      "epoch": 32.021999847223285,
      "grad_norm": 9.637585639953613,
      "learning_rate": 9.972500190970896e-06,
      "loss": 1.6496,
      "step": 419200
    },
    {
      "epoch": 32.029638683064704,
      "grad_norm": 8.178444862365723,
      "learning_rate": 9.962951646169125e-06,
      "loss": 1.637,
      "step": 419300
    },
    {
      "epoch": 32.037277518906116,
      "grad_norm": 7.42239236831665,
      "learning_rate": 9.953403101367352e-06,
      "loss": 1.6938,
      "step": 419400
    },
    {
      "epoch": 32.044916354747535,
      "grad_norm": 9.562666893005371,
      "learning_rate": 9.943854556565579e-06,
      "loss": 1.6795,
      "step": 419500
    },
    {
      "epoch": 32.052555190588954,
      "grad_norm": 8.809279441833496,
      "learning_rate": 9.934306011763808e-06,
      "loss": 1.7984,
      "step": 419600
    },
    {
      "epoch": 32.06019402643037,
      "grad_norm": 6.331694602966309,
      "learning_rate": 9.924757466962036e-06,
      "loss": 1.7141,
      "step": 419700
    },
    {
      "epoch": 32.06783286227179,
      "grad_norm": 7.667140483856201,
      "learning_rate": 9.915208922160263e-06,
      "loss": 1.6304,
      "step": 419800
    },
    {
      "epoch": 32.075471698113205,
      "grad_norm": 7.4874348640441895,
      "learning_rate": 9.905660377358492e-06,
      "loss": 1.7494,
      "step": 419900
    },
    {
      "epoch": 32.083110533954624,
      "grad_norm": 8.715115547180176,
      "learning_rate": 9.896111832556719e-06,
      "loss": 1.7593,
      "step": 420000
    },
    {
      "epoch": 32.09074936979604,
      "grad_norm": 7.840766906738281,
      "learning_rate": 9.886563287754946e-06,
      "loss": 1.7201,
      "step": 420100
    },
    {
      "epoch": 32.09838820563746,
      "grad_norm": 8.181527137756348,
      "learning_rate": 9.877014742953175e-06,
      "loss": 1.6506,
      "step": 420200
    },
    {
      "epoch": 32.10602704147888,
      "grad_norm": 7.536813735961914,
      "learning_rate": 9.867466198151402e-06,
      "loss": 1.7085,
      "step": 420300
    },
    {
      "epoch": 32.113665877320294,
      "grad_norm": 9.564143180847168,
      "learning_rate": 9.85791765334963e-06,
      "loss": 1.7541,
      "step": 420400
    },
    {
      "epoch": 32.12130471316171,
      "grad_norm": 7.2967848777771,
      "learning_rate": 9.848369108547857e-06,
      "loss": 1.6958,
      "step": 420500
    },
    {
      "epoch": 32.12894354900313,
      "grad_norm": 10.235724449157715,
      "learning_rate": 9.838820563746086e-06,
      "loss": 1.7247,
      "step": 420600
    },
    {
      "epoch": 32.13658238484455,
      "grad_norm": 10.62395191192627,
      "learning_rate": 9.829272018944313e-06,
      "loss": 1.8239,
      "step": 420700
    },
    {
      "epoch": 32.14422122068597,
      "grad_norm": 7.081534385681152,
      "learning_rate": 9.81972347414254e-06,
      "loss": 1.6558,
      "step": 420800
    },
    {
      "epoch": 32.15186005652738,
      "grad_norm": 8.643457412719727,
      "learning_rate": 9.810174929340769e-06,
      "loss": 1.6692,
      "step": 420900
    },
    {
      "epoch": 32.1594988923688,
      "grad_norm": 11.625032424926758,
      "learning_rate": 9.800626384538997e-06,
      "loss": 1.8063,
      "step": 421000
    },
    {
      "epoch": 32.16713772821022,
      "grad_norm": 11.4324951171875,
      "learning_rate": 9.791077839737224e-06,
      "loss": 1.6384,
      "step": 421100
    },
    {
      "epoch": 32.17477656405164,
      "grad_norm": 7.436976432800293,
      "learning_rate": 9.781529294935453e-06,
      "loss": 1.7139,
      "step": 421200
    },
    {
      "epoch": 32.18241539989306,
      "grad_norm": 9.400775909423828,
      "learning_rate": 9.77198075013368e-06,
      "loss": 1.6904,
      "step": 421300
    },
    {
      "epoch": 32.19005423573447,
      "grad_norm": 6.20825719833374,
      "learning_rate": 9.762432205331907e-06,
      "loss": 1.7699,
      "step": 421400
    },
    {
      "epoch": 32.19769307157589,
      "grad_norm": 9.310179710388184,
      "learning_rate": 9.752883660530136e-06,
      "loss": 1.6245,
      "step": 421500
    },
    {
      "epoch": 32.20533190741731,
      "grad_norm": 6.713366508483887,
      "learning_rate": 9.743335115728364e-06,
      "loss": 1.8106,
      "step": 421600
    },
    {
      "epoch": 32.21297074325873,
      "grad_norm": 7.938657760620117,
      "learning_rate": 9.733786570926591e-06,
      "loss": 1.722,
      "step": 421700
    },
    {
      "epoch": 32.22060957910014,
      "grad_norm": 7.049314498901367,
      "learning_rate": 9.724238026124818e-06,
      "loss": 1.7112,
      "step": 421800
    },
    {
      "epoch": 32.22824841494156,
      "grad_norm": 5.835221767425537,
      "learning_rate": 9.714689481323047e-06,
      "loss": 1.7007,
      "step": 421900
    },
    {
      "epoch": 32.23588725078298,
      "grad_norm": 6.989078521728516,
      "learning_rate": 9.705140936521274e-06,
      "loss": 1.668,
      "step": 422000
    },
    {
      "epoch": 32.2435260866244,
      "grad_norm": 8.584261894226074,
      "learning_rate": 9.695592391719501e-06,
      "loss": 1.7048,
      "step": 422100
    },
    {
      "epoch": 32.25116492246582,
      "grad_norm": 10.6575345993042,
      "learning_rate": 9.68604384691773e-06,
      "loss": 1.7303,
      "step": 422200
    },
    {
      "epoch": 32.25880375830723,
      "grad_norm": 8.183161735534668,
      "learning_rate": 9.676495302115959e-06,
      "loss": 1.7114,
      "step": 422300
    },
    {
      "epoch": 32.26644259414865,
      "grad_norm": 9.276378631591797,
      "learning_rate": 9.666946757314186e-06,
      "loss": 1.7579,
      "step": 422400
    },
    {
      "epoch": 32.27408142999007,
      "grad_norm": 9.605245590209961,
      "learning_rate": 9.657398212512414e-06,
      "loss": 1.809,
      "step": 422500
    },
    {
      "epoch": 32.28172026583149,
      "grad_norm": 7.394964694976807,
      "learning_rate": 9.647849667710641e-06,
      "loss": 1.7544,
      "step": 422600
    },
    {
      "epoch": 32.28935910167291,
      "grad_norm": 5.79463529586792,
      "learning_rate": 9.638301122908868e-06,
      "loss": 1.8066,
      "step": 422700
    },
    {
      "epoch": 32.29699793751432,
      "grad_norm": 5.891967296600342,
      "learning_rate": 9.628752578107097e-06,
      "loss": 1.7576,
      "step": 422800
    },
    {
      "epoch": 32.30463677335574,
      "grad_norm": 6.934422969818115,
      "learning_rate": 9.619204033305326e-06,
      "loss": 1.5916,
      "step": 422900
    },
    {
      "epoch": 32.31227560919716,
      "grad_norm": 8.073822975158691,
      "learning_rate": 9.609655488503553e-06,
      "loss": 1.7296,
      "step": 423000
    },
    {
      "epoch": 32.31991444503858,
      "grad_norm": 6.696771144866943,
      "learning_rate": 9.600106943701781e-06,
      "loss": 1.7144,
      "step": 423100
    },
    {
      "epoch": 32.32755328088,
      "grad_norm": 6.430151462554932,
      "learning_rate": 9.590558398900008e-06,
      "loss": 1.7169,
      "step": 423200
    },
    {
      "epoch": 32.33519211672141,
      "grad_norm": 7.578967094421387,
      "learning_rate": 9.581009854098235e-06,
      "loss": 1.7665,
      "step": 423300
    },
    {
      "epoch": 32.34283095256283,
      "grad_norm": 6.0375494956970215,
      "learning_rate": 9.571461309296462e-06,
      "loss": 1.606,
      "step": 423400
    },
    {
      "epoch": 32.35046978840425,
      "grad_norm": 8.327462196350098,
      "learning_rate": 9.561912764494691e-06,
      "loss": 1.8172,
      "step": 423500
    },
    {
      "epoch": 32.358108624245666,
      "grad_norm": 7.558799743652344,
      "learning_rate": 9.55236421969292e-06,
      "loss": 1.7248,
      "step": 423600
    },
    {
      "epoch": 32.365747460087086,
      "grad_norm": 8.345011711120605,
      "learning_rate": 9.542815674891147e-06,
      "loss": 1.85,
      "step": 423700
    },
    {
      "epoch": 32.3733862959285,
      "grad_norm": 7.685854911804199,
      "learning_rate": 9.533267130089375e-06,
      "loss": 1.6952,
      "step": 423800
    },
    {
      "epoch": 32.38102513176992,
      "grad_norm": 6.828810691833496,
      "learning_rate": 9.523718585287602e-06,
      "loss": 1.6744,
      "step": 423900
    },
    {
      "epoch": 32.388663967611336,
      "grad_norm": 9.372674942016602,
      "learning_rate": 9.51417004048583e-06,
      "loss": 1.7352,
      "step": 424000
    },
    {
      "epoch": 32.396302803452755,
      "grad_norm": 6.328155040740967,
      "learning_rate": 9.504621495684058e-06,
      "loss": 1.8067,
      "step": 424100
    },
    {
      "epoch": 32.403941639294175,
      "grad_norm": 7.284165382385254,
      "learning_rate": 9.495072950882287e-06,
      "loss": 1.6561,
      "step": 424200
    },
    {
      "epoch": 32.41158047513559,
      "grad_norm": 7.978161811828613,
      "learning_rate": 9.485524406080514e-06,
      "loss": 1.7132,
      "step": 424300
    },
    {
      "epoch": 32.419219310977006,
      "grad_norm": 6.8212175369262695,
      "learning_rate": 9.475975861278742e-06,
      "loss": 1.706,
      "step": 424400
    },
    {
      "epoch": 32.426858146818425,
      "grad_norm": 9.088277816772461,
      "learning_rate": 9.46642731647697e-06,
      "loss": 1.7451,
      "step": 424500
    },
    {
      "epoch": 32.434496982659844,
      "grad_norm": 8.364681243896484,
      "learning_rate": 9.456878771675196e-06,
      "loss": 1.7103,
      "step": 424600
    },
    {
      "epoch": 32.442135818501264,
      "grad_norm": 8.086098670959473,
      "learning_rate": 9.447330226873425e-06,
      "loss": 1.761,
      "step": 424700
    },
    {
      "epoch": 32.449774654342676,
      "grad_norm": 6.31915283203125,
      "learning_rate": 9.437781682071654e-06,
      "loss": 1.7596,
      "step": 424800
    },
    {
      "epoch": 32.457413490184095,
      "grad_norm": 7.28623628616333,
      "learning_rate": 9.42823313726988e-06,
      "loss": 1.7005,
      "step": 424900
    },
    {
      "epoch": 32.465052326025514,
      "grad_norm": 7.709569931030273,
      "learning_rate": 9.418684592468108e-06,
      "loss": 1.8043,
      "step": 425000
    },
    {
      "epoch": 32.47269116186693,
      "grad_norm": 6.644814968109131,
      "learning_rate": 9.409136047666337e-06,
      "loss": 1.7531,
      "step": 425100
    },
    {
      "epoch": 32.48032999770835,
      "grad_norm": 8.4058198928833,
      "learning_rate": 9.399587502864564e-06,
      "loss": 1.6658,
      "step": 425200
    },
    {
      "epoch": 32.487968833549765,
      "grad_norm": 7.865488052368164,
      "learning_rate": 9.39003895806279e-06,
      "loss": 1.7376,
      "step": 425300
    },
    {
      "epoch": 32.495607669391184,
      "grad_norm": 8.929096221923828,
      "learning_rate": 9.38049041326102e-06,
      "loss": 1.7205,
      "step": 425400
    },
    {
      "epoch": 32.5032465052326,
      "grad_norm": 8.506664276123047,
      "learning_rate": 9.370941868459248e-06,
      "loss": 1.7417,
      "step": 425500
    },
    {
      "epoch": 32.51088534107402,
      "grad_norm": 4.754073143005371,
      "learning_rate": 9.361393323657475e-06,
      "loss": 1.6827,
      "step": 425600
    },
    {
      "epoch": 32.518524176915435,
      "grad_norm": 9.951692581176758,
      "learning_rate": 9.351844778855704e-06,
      "loss": 1.7239,
      "step": 425700
    },
    {
      "epoch": 32.526163012756854,
      "grad_norm": 6.988509178161621,
      "learning_rate": 9.34229623405393e-06,
      "loss": 1.6265,
      "step": 425800
    },
    {
      "epoch": 32.53380184859827,
      "grad_norm": 8.258201599121094,
      "learning_rate": 9.332747689252158e-06,
      "loss": 1.793,
      "step": 425900
    },
    {
      "epoch": 32.54144068443969,
      "grad_norm": 10.760197639465332,
      "learning_rate": 9.323199144450386e-06,
      "loss": 1.7928,
      "step": 426000
    },
    {
      "epoch": 32.54907952028111,
      "grad_norm": 7.0327301025390625,
      "learning_rate": 9.313650599648615e-06,
      "loss": 1.6914,
      "step": 426100
    },
    {
      "epoch": 32.55671835612252,
      "grad_norm": 8.081341743469238,
      "learning_rate": 9.304102054846842e-06,
      "loss": 1.7909,
      "step": 426200
    },
    {
      "epoch": 32.56435719196394,
      "grad_norm": 7.663844108581543,
      "learning_rate": 9.294553510045069e-06,
      "loss": 1.7241,
      "step": 426300
    },
    {
      "epoch": 32.57199602780536,
      "grad_norm": 11.195314407348633,
      "learning_rate": 9.285004965243298e-06,
      "loss": 1.6639,
      "step": 426400
    },
    {
      "epoch": 32.57963486364678,
      "grad_norm": 8.865312576293945,
      "learning_rate": 9.275456420441525e-06,
      "loss": 1.6082,
      "step": 426500
    },
    {
      "epoch": 32.5872736994882,
      "grad_norm": 7.108160972595215,
      "learning_rate": 9.265907875639752e-06,
      "loss": 1.7297,
      "step": 426600
    },
    {
      "epoch": 32.59491253532961,
      "grad_norm": 5.654353618621826,
      "learning_rate": 9.25635933083798e-06,
      "loss": 1.6496,
      "step": 426700
    },
    {
      "epoch": 32.60255137117103,
      "grad_norm": 7.848978519439697,
      "learning_rate": 9.246810786036209e-06,
      "loss": 1.7326,
      "step": 426800
    },
    {
      "epoch": 32.61019020701245,
      "grad_norm": 7.580340385437012,
      "learning_rate": 9.237262241234436e-06,
      "loss": 1.7581,
      "step": 426900
    },
    {
      "epoch": 32.61782904285387,
      "grad_norm": 7.345829486846924,
      "learning_rate": 9.227713696432665e-06,
      "loss": 1.7746,
      "step": 427000
    },
    {
      "epoch": 32.62546787869529,
      "grad_norm": 8.394654273986816,
      "learning_rate": 9.218165151630892e-06,
      "loss": 1.7222,
      "step": 427100
    },
    {
      "epoch": 32.6331067145367,
      "grad_norm": 11.404768943786621,
      "learning_rate": 9.208616606829119e-06,
      "loss": 1.8166,
      "step": 427200
    },
    {
      "epoch": 32.64074555037812,
      "grad_norm": 7.85352897644043,
      "learning_rate": 9.199068062027347e-06,
      "loss": 1.7412,
      "step": 427300
    },
    {
      "epoch": 32.64838438621954,
      "grad_norm": 7.406595230102539,
      "learning_rate": 9.189519517225576e-06,
      "loss": 1.6158,
      "step": 427400
    },
    {
      "epoch": 32.65602322206096,
      "grad_norm": 8.759032249450684,
      "learning_rate": 9.179970972423803e-06,
      "loss": 1.6738,
      "step": 427500
    },
    {
      "epoch": 32.66366205790238,
      "grad_norm": 7.948644638061523,
      "learning_rate": 9.17042242762203e-06,
      "loss": 1.7923,
      "step": 427600
    },
    {
      "epoch": 32.67130089374379,
      "grad_norm": 6.484379291534424,
      "learning_rate": 9.160873882820259e-06,
      "loss": 1.7343,
      "step": 427700
    },
    {
      "epoch": 32.67893972958521,
      "grad_norm": 5.891561508178711,
      "learning_rate": 9.151325338018486e-06,
      "loss": 1.6987,
      "step": 427800
    },
    {
      "epoch": 32.68657856542663,
      "grad_norm": 6.713890552520752,
      "learning_rate": 9.141776793216713e-06,
      "loss": 1.573,
      "step": 427900
    },
    {
      "epoch": 32.69421740126805,
      "grad_norm": 6.710124492645264,
      "learning_rate": 9.132228248414943e-06,
      "loss": 1.6988,
      "step": 428000
    },
    {
      "epoch": 32.70185623710947,
      "grad_norm": 9.266077995300293,
      "learning_rate": 9.12267970361317e-06,
      "loss": 1.7273,
      "step": 428100
    },
    {
      "epoch": 32.70949507295088,
      "grad_norm": 7.934900760650635,
      "learning_rate": 9.113131158811397e-06,
      "loss": 1.7289,
      "step": 428200
    },
    {
      "epoch": 32.7171339087923,
      "grad_norm": 6.9856181144714355,
      "learning_rate": 9.103582614009626e-06,
      "loss": 1.7647,
      "step": 428300
    },
    {
      "epoch": 32.72477274463372,
      "grad_norm": 8.625462532043457,
      "learning_rate": 9.094034069207853e-06,
      "loss": 1.8081,
      "step": 428400
    },
    {
      "epoch": 32.73241158047514,
      "grad_norm": 7.123507499694824,
      "learning_rate": 9.08448552440608e-06,
      "loss": 1.6843,
      "step": 428500
    },
    {
      "epoch": 32.740050416316556,
      "grad_norm": 8.851886749267578,
      "learning_rate": 9.074936979604309e-06,
      "loss": 1.6201,
      "step": 428600
    },
    {
      "epoch": 32.74768925215797,
      "grad_norm": 6.211804389953613,
      "learning_rate": 9.065388434802537e-06,
      "loss": 1.6757,
      "step": 428700
    },
    {
      "epoch": 32.75532808799939,
      "grad_norm": 8.699965476989746,
      "learning_rate": 9.055839890000764e-06,
      "loss": 1.7855,
      "step": 428800
    },
    {
      "epoch": 32.76296692384081,
      "grad_norm": 7.669370174407959,
      "learning_rate": 9.046291345198993e-06,
      "loss": 1.6806,
      "step": 428900
    },
    {
      "epoch": 32.770605759682226,
      "grad_norm": 10.685552597045898,
      "learning_rate": 9.03674280039722e-06,
      "loss": 1.6953,
      "step": 429000
    },
    {
      "epoch": 32.778244595523645,
      "grad_norm": 8.770719528198242,
      "learning_rate": 9.027194255595447e-06,
      "loss": 1.6079,
      "step": 429100
    },
    {
      "epoch": 32.78588343136506,
      "grad_norm": 7.2207159996032715,
      "learning_rate": 9.017645710793676e-06,
      "loss": 1.6826,
      "step": 429200
    },
    {
      "epoch": 32.79352226720648,
      "grad_norm": 7.242040157318115,
      "learning_rate": 9.008097165991904e-06,
      "loss": 1.7076,
      "step": 429300
    },
    {
      "epoch": 32.801161103047896,
      "grad_norm": 11.10108757019043,
      "learning_rate": 8.998548621190131e-06,
      "loss": 1.8405,
      "step": 429400
    },
    {
      "epoch": 32.808799938889315,
      "grad_norm": 7.096502304077148,
      "learning_rate": 8.989000076388358e-06,
      "loss": 1.6635,
      "step": 429500
    },
    {
      "epoch": 32.816438774730734,
      "grad_norm": 11.480671882629395,
      "learning_rate": 8.979451531586587e-06,
      "loss": 1.7116,
      "step": 429600
    },
    {
      "epoch": 32.82407761057215,
      "grad_norm": 8.237231254577637,
      "learning_rate": 8.969902986784814e-06,
      "loss": 1.6665,
      "step": 429700
    },
    {
      "epoch": 32.831716446413566,
      "grad_norm": 9.417778968811035,
      "learning_rate": 8.960354441983041e-06,
      "loss": 1.7487,
      "step": 429800
    },
    {
      "epoch": 32.839355282254985,
      "grad_norm": 8.826783180236816,
      "learning_rate": 8.95080589718127e-06,
      "loss": 1.7494,
      "step": 429900
    },
    {
      "epoch": 32.846994118096404,
      "grad_norm": 6.293108940124512,
      "learning_rate": 8.941257352379498e-06,
      "loss": 1.6744,
      "step": 430000
    },
    {
      "epoch": 32.85463295393782,
      "grad_norm": 7.451251983642578,
      "learning_rate": 8.931708807577725e-06,
      "loss": 1.717,
      "step": 430100
    },
    {
      "epoch": 32.862271789779236,
      "grad_norm": 7.526402950286865,
      "learning_rate": 8.922160262775954e-06,
      "loss": 1.8176,
      "step": 430200
    },
    {
      "epoch": 32.869910625620655,
      "grad_norm": 5.447211265563965,
      "learning_rate": 8.912611717974181e-06,
      "loss": 1.6516,
      "step": 430300
    },
    {
      "epoch": 32.877549461462074,
      "grad_norm": 9.356314659118652,
      "learning_rate": 8.903063173172408e-06,
      "loss": 1.7091,
      "step": 430400
    },
    {
      "epoch": 32.88518829730349,
      "grad_norm": 8.91724967956543,
      "learning_rate": 8.893514628370637e-06,
      "loss": 1.7101,
      "step": 430500
    },
    {
      "epoch": 32.892827133144905,
      "grad_norm": 8.091479301452637,
      "learning_rate": 8.883966083568866e-06,
      "loss": 1.7505,
      "step": 430600
    },
    {
      "epoch": 32.900465968986325,
      "grad_norm": 8.483158111572266,
      "learning_rate": 8.874417538767093e-06,
      "loss": 1.6872,
      "step": 430700
    },
    {
      "epoch": 32.908104804827744,
      "grad_norm": 7.751063346862793,
      "learning_rate": 8.86486899396532e-06,
      "loss": 1.7929,
      "step": 430800
    },
    {
      "epoch": 32.91574364066916,
      "grad_norm": 7.5677714347839355,
      "learning_rate": 8.855320449163548e-06,
      "loss": 1.6723,
      "step": 430900
    },
    {
      "epoch": 32.92338247651058,
      "grad_norm": 11.012239456176758,
      "learning_rate": 8.845771904361775e-06,
      "loss": 1.718,
      "step": 431000
    },
    {
      "epoch": 32.931021312351994,
      "grad_norm": 8.694672584533691,
      "learning_rate": 8.836223359560002e-06,
      "loss": 1.6163,
      "step": 431100
    },
    {
      "epoch": 32.938660148193414,
      "grad_norm": 8.45657730102539,
      "learning_rate": 8.826674814758233e-06,
      "loss": 1.735,
      "step": 431200
    },
    {
      "epoch": 32.94629898403483,
      "grad_norm": 8.933324813842773,
      "learning_rate": 8.81712626995646e-06,
      "loss": 1.6818,
      "step": 431300
    },
    {
      "epoch": 32.95393781987625,
      "grad_norm": 7.064707279205322,
      "learning_rate": 8.807577725154687e-06,
      "loss": 1.7097,
      "step": 431400
    },
    {
      "epoch": 32.96157665571767,
      "grad_norm": 8.879243850708008,
      "learning_rate": 8.798029180352915e-06,
      "loss": 1.8259,
      "step": 431500
    },
    {
      "epoch": 32.96921549155908,
      "grad_norm": 7.22133207321167,
      "learning_rate": 8.788480635551142e-06,
      "loss": 1.786,
      "step": 431600
    },
    {
      "epoch": 32.9768543274005,
      "grad_norm": 9.656820297241211,
      "learning_rate": 8.77893209074937e-06,
      "loss": 1.6853,
      "step": 431700
    },
    {
      "epoch": 32.98449316324192,
      "grad_norm": 4.988397121429443,
      "learning_rate": 8.769383545947598e-06,
      "loss": 1.6517,
      "step": 431800
    },
    {
      "epoch": 32.99213199908334,
      "grad_norm": 6.161432266235352,
      "learning_rate": 8.759835001145827e-06,
      "loss": 1.6985,
      "step": 431900
    },
    {
      "epoch": 32.99977083492476,
      "grad_norm": 9.37753963470459,
      "learning_rate": 8.750286456344054e-06,
      "loss": 1.8313,
      "step": 432000
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.7946261167526245,
      "eval_runtime": 1.4745,
      "eval_samples_per_second": 467.94,
      "eval_steps_per_second": 467.94,
      "step": 432003
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.48365318775177,
      "eval_runtime": 28.3902,
      "eval_samples_per_second": 461.109,
      "eval_steps_per_second": 461.109,
      "step": 432003
    },
    {
      "epoch": 33.00740967076617,
      "grad_norm": 8.64807415008545,
      "learning_rate": 8.74073791154228e-06,
      "loss": 1.677,
      "step": 432100
    },
    {
      "epoch": 33.01504850660759,
      "grad_norm": 8.499265670776367,
      "learning_rate": 8.73118936674051e-06,
      "loss": 1.7477,
      "step": 432200
    },
    {
      "epoch": 33.02268734244901,
      "grad_norm": 7.21552848815918,
      "learning_rate": 8.721640821938736e-06,
      "loss": 1.8111,
      "step": 432300
    },
    {
      "epoch": 33.03032617829043,
      "grad_norm": 7.0005621910095215,
      "learning_rate": 8.712092277136965e-06,
      "loss": 1.7242,
      "step": 432400
    },
    {
      "epoch": 33.03796501413185,
      "grad_norm": 7.630504131317139,
      "learning_rate": 8.702543732335194e-06,
      "loss": 1.6751,
      "step": 432500
    },
    {
      "epoch": 33.04560384997326,
      "grad_norm": 9.19616413116455,
      "learning_rate": 8.69299518753342e-06,
      "loss": 1.6991,
      "step": 432600
    },
    {
      "epoch": 33.05324268581468,
      "grad_norm": 9.007124900817871,
      "learning_rate": 8.683446642731648e-06,
      "loss": 1.6126,
      "step": 432700
    },
    {
      "epoch": 33.0608815216561,
      "grad_norm": 8.945286750793457,
      "learning_rate": 8.673898097929876e-06,
      "loss": 1.6816,
      "step": 432800
    },
    {
      "epoch": 33.06852035749752,
      "grad_norm": 7.753163814544678,
      "learning_rate": 8.664349553128103e-06,
      "loss": 1.7063,
      "step": 432900
    },
    {
      "epoch": 33.07615919333894,
      "grad_norm": 8.598923683166504,
      "learning_rate": 8.65480100832633e-06,
      "loss": 1.7607,
      "step": 433000
    },
    {
      "epoch": 33.08379802918035,
      "grad_norm": 7.688928604125977,
      "learning_rate": 8.645252463524559e-06,
      "loss": 1.7229,
      "step": 433100
    },
    {
      "epoch": 33.09143686502177,
      "grad_norm": 9.93098258972168,
      "learning_rate": 8.635703918722788e-06,
      "loss": 1.7458,
      "step": 433200
    },
    {
      "epoch": 33.09907570086319,
      "grad_norm": 6.987140655517578,
      "learning_rate": 8.626155373921015e-06,
      "loss": 1.6571,
      "step": 433300
    },
    {
      "epoch": 33.10671453670461,
      "grad_norm": 8.850019454956055,
      "learning_rate": 8.616606829119242e-06,
      "loss": 1.785,
      "step": 433400
    },
    {
      "epoch": 33.11435337254603,
      "grad_norm": 8.457772254943848,
      "learning_rate": 8.60705828431747e-06,
      "loss": 1.7711,
      "step": 433500
    },
    {
      "epoch": 33.12199220838744,
      "grad_norm": 7.561373710632324,
      "learning_rate": 8.597509739515698e-06,
      "loss": 1.6414,
      "step": 433600
    },
    {
      "epoch": 33.12963104422886,
      "grad_norm": 7.155788898468018,
      "learning_rate": 8.587961194713926e-06,
      "loss": 1.7189,
      "step": 433700
    },
    {
      "epoch": 33.13726988007028,
      "grad_norm": 10.201298713684082,
      "learning_rate": 8.578412649912155e-06,
      "loss": 1.7562,
      "step": 433800
    },
    {
      "epoch": 33.1449087159117,
      "grad_norm": 7.331350803375244,
      "learning_rate": 8.568864105110382e-06,
      "loss": 1.7681,
      "step": 433900
    },
    {
      "epoch": 33.152547551753116,
      "grad_norm": 7.03918981552124,
      "learning_rate": 8.559315560308609e-06,
      "loss": 1.6735,
      "step": 434000
    },
    {
      "epoch": 33.16018638759453,
      "grad_norm": 8.854860305786133,
      "learning_rate": 8.549767015506838e-06,
      "loss": 1.6912,
      "step": 434100
    },
    {
      "epoch": 33.16782522343595,
      "grad_norm": 7.878204345703125,
      "learning_rate": 8.540218470705065e-06,
      "loss": 1.6889,
      "step": 434200
    },
    {
      "epoch": 33.17546405927737,
      "grad_norm": 8.895607948303223,
      "learning_rate": 8.530669925903292e-06,
      "loss": 1.5701,
      "step": 434300
    },
    {
      "epoch": 33.183102895118786,
      "grad_norm": 7.6517157554626465,
      "learning_rate": 8.521121381101522e-06,
      "loss": 1.7401,
      "step": 434400
    },
    {
      "epoch": 33.1907417309602,
      "grad_norm": 6.321479797363281,
      "learning_rate": 8.511572836299749e-06,
      "loss": 1.7364,
      "step": 434500
    },
    {
      "epoch": 33.19838056680162,
      "grad_norm": 8.4672212600708,
      "learning_rate": 8.502024291497976e-06,
      "loss": 1.7103,
      "step": 434600
    },
    {
      "epoch": 33.20601940264304,
      "grad_norm": 5.94143009185791,
      "learning_rate": 8.492475746696203e-06,
      "loss": 1.693,
      "step": 434700
    },
    {
      "epoch": 33.213658238484456,
      "grad_norm": 9.50305461883545,
      "learning_rate": 8.482927201894432e-06,
      "loss": 1.7925,
      "step": 434800
    },
    {
      "epoch": 33.221297074325875,
      "grad_norm": 6.538076400756836,
      "learning_rate": 8.473378657092659e-06,
      "loss": 1.6934,
      "step": 434900
    },
    {
      "epoch": 33.22893591016729,
      "grad_norm": 7.454860210418701,
      "learning_rate": 8.463830112290887e-06,
      "loss": 1.702,
      "step": 435000
    },
    {
      "epoch": 33.236574746008706,
      "grad_norm": 7.427585124969482,
      "learning_rate": 8.454281567489116e-06,
      "loss": 1.7836,
      "step": 435100
    },
    {
      "epoch": 33.244213581850126,
      "grad_norm": 6.081295967102051,
      "learning_rate": 8.444733022687343e-06,
      "loss": 1.7402,
      "step": 435200
    },
    {
      "epoch": 33.251852417691545,
      "grad_norm": 8.90534496307373,
      "learning_rate": 8.43518447788557e-06,
      "loss": 1.7352,
      "step": 435300
    },
    {
      "epoch": 33.259491253532964,
      "grad_norm": 7.44382905960083,
      "learning_rate": 8.425635933083799e-06,
      "loss": 1.6758,
      "step": 435400
    },
    {
      "epoch": 33.267130089374376,
      "grad_norm": 8.442051887512207,
      "learning_rate": 8.416087388282026e-06,
      "loss": 1.739,
      "step": 435500
    },
    {
      "epoch": 33.274768925215795,
      "grad_norm": 9.022954940795898,
      "learning_rate": 8.406538843480254e-06,
      "loss": 1.7231,
      "step": 435600
    },
    {
      "epoch": 33.282407761057215,
      "grad_norm": 6.1196794509887695,
      "learning_rate": 8.396990298678483e-06,
      "loss": 1.7423,
      "step": 435700
    },
    {
      "epoch": 33.290046596898634,
      "grad_norm": 11.100679397583008,
      "learning_rate": 8.38744175387671e-06,
      "loss": 1.6571,
      "step": 435800
    },
    {
      "epoch": 33.29768543274005,
      "grad_norm": 9.262185096740723,
      "learning_rate": 8.377893209074937e-06,
      "loss": 1.8012,
      "step": 435900
    },
    {
      "epoch": 33.305324268581465,
      "grad_norm": 8.994210243225098,
      "learning_rate": 8.368344664273166e-06,
      "loss": 1.7975,
      "step": 436000
    },
    {
      "epoch": 33.312963104422884,
      "grad_norm": 8.29332447052002,
      "learning_rate": 8.358796119471393e-06,
      "loss": 1.7478,
      "step": 436100
    },
    {
      "epoch": 33.320601940264304,
      "grad_norm": 6.930624961853027,
      "learning_rate": 8.34924757466962e-06,
      "loss": 1.7042,
      "step": 436200
    },
    {
      "epoch": 33.32824077610572,
      "grad_norm": 8.418082237243652,
      "learning_rate": 8.339699029867849e-06,
      "loss": 1.6814,
      "step": 436300
    },
    {
      "epoch": 33.33587961194714,
      "grad_norm": 4.640406131744385,
      "learning_rate": 8.330150485066077e-06,
      "loss": 1.6135,
      "step": 436400
    },
    {
      "epoch": 33.343518447788554,
      "grad_norm": 7.767351150512695,
      "learning_rate": 8.320601940264304e-06,
      "loss": 1.635,
      "step": 436500
    },
    {
      "epoch": 33.35115728362997,
      "grad_norm": 10.050063133239746,
      "learning_rate": 8.311053395462531e-06,
      "loss": 1.7215,
      "step": 436600
    },
    {
      "epoch": 33.35879611947139,
      "grad_norm": 7.009403228759766,
      "learning_rate": 8.30150485066076e-06,
      "loss": 1.7569,
      "step": 436700
    },
    {
      "epoch": 33.36643495531281,
      "grad_norm": 8.22557258605957,
      "learning_rate": 8.291956305858987e-06,
      "loss": 1.7113,
      "step": 436800
    },
    {
      "epoch": 33.37407379115423,
      "grad_norm": 7.4804368019104,
      "learning_rate": 8.282407761057216e-06,
      "loss": 1.74,
      "step": 436900
    },
    {
      "epoch": 33.38171262699564,
      "grad_norm": 8.655803680419922,
      "learning_rate": 8.272859216255444e-06,
      "loss": 1.6888,
      "step": 437000
    },
    {
      "epoch": 33.38935146283706,
      "grad_norm": 7.279573440551758,
      "learning_rate": 8.263310671453671e-06,
      "loss": 1.7293,
      "step": 437100
    },
    {
      "epoch": 33.39699029867848,
      "grad_norm": 7.5804548263549805,
      "learning_rate": 8.253762126651898e-06,
      "loss": 1.6512,
      "step": 437200
    },
    {
      "epoch": 33.4046291345199,
      "grad_norm": 10.57466983795166,
      "learning_rate": 8.244213581850127e-06,
      "loss": 1.7998,
      "step": 437300
    },
    {
      "epoch": 33.41226797036132,
      "grad_norm": 6.69725227355957,
      "learning_rate": 8.234665037048354e-06,
      "loss": 1.7414,
      "step": 437400
    },
    {
      "epoch": 33.41990680620273,
      "grad_norm": 10.719881057739258,
      "learning_rate": 8.225116492246581e-06,
      "loss": 1.7749,
      "step": 437500
    },
    {
      "epoch": 33.42754564204415,
      "grad_norm": 7.612126350402832,
      "learning_rate": 8.21556794744481e-06,
      "loss": 1.7328,
      "step": 437600
    },
    {
      "epoch": 33.43518447788557,
      "grad_norm": 8.052774429321289,
      "learning_rate": 8.206019402643038e-06,
      "loss": 1.6413,
      "step": 437700
    },
    {
      "epoch": 33.44282331372699,
      "grad_norm": 7.1854248046875,
      "learning_rate": 8.196470857841265e-06,
      "loss": 1.7022,
      "step": 437800
    },
    {
      "epoch": 33.45046214956841,
      "grad_norm": 6.710805416107178,
      "learning_rate": 8.186922313039492e-06,
      "loss": 1.7547,
      "step": 437900
    },
    {
      "epoch": 33.45810098540982,
      "grad_norm": 7.0503621101379395,
      "learning_rate": 8.177373768237721e-06,
      "loss": 1.7425,
      "step": 438000
    },
    {
      "epoch": 33.46573982125124,
      "grad_norm": 7.60988712310791,
      "learning_rate": 8.167825223435948e-06,
      "loss": 1.7013,
      "step": 438100
    },
    {
      "epoch": 33.47337865709266,
      "grad_norm": 7.852294445037842,
      "learning_rate": 8.158276678634177e-06,
      "loss": 1.6796,
      "step": 438200
    },
    {
      "epoch": 33.48101749293408,
      "grad_norm": 8.19446849822998,
      "learning_rate": 8.148728133832405e-06,
      "loss": 1.5915,
      "step": 438300
    },
    {
      "epoch": 33.4886563287755,
      "grad_norm": 6.45480489730835,
      "learning_rate": 8.139179589030632e-06,
      "loss": 1.6721,
      "step": 438400
    },
    {
      "epoch": 33.49629516461691,
      "grad_norm": 7.662940979003906,
      "learning_rate": 8.12963104422886e-06,
      "loss": 1.6534,
      "step": 438500
    },
    {
      "epoch": 33.50393400045833,
      "grad_norm": 7.609344005584717,
      "learning_rate": 8.120082499427088e-06,
      "loss": 1.797,
      "step": 438600
    },
    {
      "epoch": 33.51157283629975,
      "grad_norm": 6.585292339324951,
      "learning_rate": 8.110533954625315e-06,
      "loss": 1.6658,
      "step": 438700
    },
    {
      "epoch": 33.51921167214117,
      "grad_norm": 10.45904541015625,
      "learning_rate": 8.100985409823544e-06,
      "loss": 1.7327,
      "step": 438800
    },
    {
      "epoch": 33.52685050798258,
      "grad_norm": 7.293821334838867,
      "learning_rate": 8.091436865021773e-06,
      "loss": 1.7153,
      "step": 438900
    },
    {
      "epoch": 33.534489343824,
      "grad_norm": 8.231914520263672,
      "learning_rate": 8.08188832022e-06,
      "loss": 1.7244,
      "step": 439000
    },
    {
      "epoch": 33.54212817966542,
      "grad_norm": 15.035093307495117,
      "learning_rate": 8.072339775418226e-06,
      "loss": 1.6694,
      "step": 439100
    },
    {
      "epoch": 33.54976701550684,
      "grad_norm": 7.7014641761779785,
      "learning_rate": 8.062791230616453e-06,
      "loss": 1.6346,
      "step": 439200
    },
    {
      "epoch": 33.55740585134826,
      "grad_norm": 8.519172668457031,
      "learning_rate": 8.053242685814682e-06,
      "loss": 1.6619,
      "step": 439300
    },
    {
      "epoch": 33.56504468718967,
      "grad_norm": 6.979835033416748,
      "learning_rate": 8.04369414101291e-06,
      "loss": 1.7408,
      "step": 439400
    },
    {
      "epoch": 33.57268352303109,
      "grad_norm": 8.946700096130371,
      "learning_rate": 8.034145596211138e-06,
      "loss": 1.746,
      "step": 439500
    },
    {
      "epoch": 33.58032235887251,
      "grad_norm": 9.305794715881348,
      "learning_rate": 8.024597051409367e-06,
      "loss": 1.7886,
      "step": 439600
    },
    {
      "epoch": 33.58796119471393,
      "grad_norm": 10.255887031555176,
      "learning_rate": 8.015048506607594e-06,
      "loss": 1.8369,
      "step": 439700
    },
    {
      "epoch": 33.595600030555346,
      "grad_norm": 10.477917671203613,
      "learning_rate": 8.00549996180582e-06,
      "loss": 1.6234,
      "step": 439800
    },
    {
      "epoch": 33.60323886639676,
      "grad_norm": 8.205561637878418,
      "learning_rate": 7.99595141700405e-06,
      "loss": 1.7279,
      "step": 439900
    },
    {
      "epoch": 33.61087770223818,
      "grad_norm": 7.340374946594238,
      "learning_rate": 7.986402872202276e-06,
      "loss": 1.7045,
      "step": 440000
    },
    {
      "epoch": 33.6185165380796,
      "grad_norm": 5.621270656585693,
      "learning_rate": 7.976854327400505e-06,
      "loss": 1.6549,
      "step": 440100
    },
    {
      "epoch": 33.626155373921016,
      "grad_norm": 6.061675548553467,
      "learning_rate": 7.967305782598734e-06,
      "loss": 1.7096,
      "step": 440200
    },
    {
      "epoch": 33.633794209762435,
      "grad_norm": 7.837016582489014,
      "learning_rate": 7.95775723779696e-06,
      "loss": 1.7615,
      "step": 440300
    },
    {
      "epoch": 33.64143304560385,
      "grad_norm": 7.689794063568115,
      "learning_rate": 7.948208692995188e-06,
      "loss": 1.6826,
      "step": 440400
    },
    {
      "epoch": 33.649071881445266,
      "grad_norm": 7.080601215362549,
      "learning_rate": 7.938660148193415e-06,
      "loss": 1.6803,
      "step": 440500
    },
    {
      "epoch": 33.656710717286686,
      "grad_norm": 10.30943489074707,
      "learning_rate": 7.929111603391643e-06,
      "loss": 1.7378,
      "step": 440600
    },
    {
      "epoch": 33.664349553128105,
      "grad_norm": 8.381146430969238,
      "learning_rate": 7.91956305858987e-06,
      "loss": 1.7834,
      "step": 440700
    },
    {
      "epoch": 33.671988388969524,
      "grad_norm": 8.34158706665039,
      "learning_rate": 7.910014513788099e-06,
      "loss": 1.6937,
      "step": 440800
    },
    {
      "epoch": 33.679627224810936,
      "grad_norm": 7.79941987991333,
      "learning_rate": 7.900465968986328e-06,
      "loss": 1.7914,
      "step": 440900
    },
    {
      "epoch": 33.687266060652355,
      "grad_norm": 7.893221378326416,
      "learning_rate": 7.890917424184555e-06,
      "loss": 1.7358,
      "step": 441000
    },
    {
      "epoch": 33.694904896493775,
      "grad_norm": 8.097691535949707,
      "learning_rate": 7.881368879382782e-06,
      "loss": 1.6716,
      "step": 441100
    },
    {
      "epoch": 33.702543732335194,
      "grad_norm": 9.596923828125,
      "learning_rate": 7.87182033458101e-06,
      "loss": 1.6889,
      "step": 441200
    },
    {
      "epoch": 33.71018256817661,
      "grad_norm": 6.95388650894165,
      "learning_rate": 7.862271789779237e-06,
      "loss": 1.7101,
      "step": 441300
    },
    {
      "epoch": 33.717821404018025,
      "grad_norm": 7.912573337554932,
      "learning_rate": 7.852723244977466e-06,
      "loss": 1.6546,
      "step": 441400
    },
    {
      "epoch": 33.725460239859444,
      "grad_norm": 9.323118209838867,
      "learning_rate": 7.843174700175695e-06,
      "loss": 1.649,
      "step": 441500
    },
    {
      "epoch": 33.73309907570086,
      "grad_norm": 8.062848091125488,
      "learning_rate": 7.833626155373922e-06,
      "loss": 1.7451,
      "step": 441600
    },
    {
      "epoch": 33.74073791154228,
      "grad_norm": 6.361479759216309,
      "learning_rate": 7.824077610572149e-06,
      "loss": 1.7286,
      "step": 441700
    },
    {
      "epoch": 33.7483767473837,
      "grad_norm": 8.375419616699219,
      "learning_rate": 7.814529065770377e-06,
      "loss": 1.6888,
      "step": 441800
    },
    {
      "epoch": 33.756015583225114,
      "grad_norm": 7.882444858551025,
      "learning_rate": 7.804980520968604e-06,
      "loss": 1.7129,
      "step": 441900
    },
    {
      "epoch": 33.76365441906653,
      "grad_norm": 8.867963790893555,
      "learning_rate": 7.795431976166833e-06,
      "loss": 1.7437,
      "step": 442000
    },
    {
      "epoch": 33.77129325490795,
      "grad_norm": 8.712754249572754,
      "learning_rate": 7.78588343136506e-06,
      "loss": 1.7168,
      "step": 442100
    },
    {
      "epoch": 33.77893209074937,
      "grad_norm": 8.128646850585938,
      "learning_rate": 7.776334886563289e-06,
      "loss": 1.8056,
      "step": 442200
    },
    {
      "epoch": 33.78657092659079,
      "grad_norm": 7.116497039794922,
      "learning_rate": 7.766786341761516e-06,
      "loss": 1.7344,
      "step": 442300
    },
    {
      "epoch": 33.7942097624322,
      "grad_norm": 7.259843349456787,
      "learning_rate": 7.757237796959743e-06,
      "loss": 1.7374,
      "step": 442400
    },
    {
      "epoch": 33.80184859827362,
      "grad_norm": 5.545843601226807,
      "learning_rate": 7.747689252157972e-06,
      "loss": 1.6824,
      "step": 442500
    },
    {
      "epoch": 33.80948743411504,
      "grad_norm": 7.579481601715088,
      "learning_rate": 7.738140707356199e-06,
      "loss": 1.697,
      "step": 442600
    },
    {
      "epoch": 33.81712626995646,
      "grad_norm": 6.949297904968262,
      "learning_rate": 7.728592162554427e-06,
      "loss": 1.7842,
      "step": 442700
    },
    {
      "epoch": 33.82476510579788,
      "grad_norm": 6.503594875335693,
      "learning_rate": 7.719043617752656e-06,
      "loss": 1.7231,
      "step": 442800
    },
    {
      "epoch": 33.83240394163929,
      "grad_norm": 7.530587673187256,
      "learning_rate": 7.709495072950883e-06,
      "loss": 1.7675,
      "step": 442900
    },
    {
      "epoch": 33.84004277748071,
      "grad_norm": 8.604641914367676,
      "learning_rate": 7.69994652814911e-06,
      "loss": 1.8057,
      "step": 443000
    },
    {
      "epoch": 33.84768161332213,
      "grad_norm": 7.263052940368652,
      "learning_rate": 7.690397983347339e-06,
      "loss": 1.8195,
      "step": 443100
    },
    {
      "epoch": 33.85532044916355,
      "grad_norm": 7.442298412322998,
      "learning_rate": 7.680849438545566e-06,
      "loss": 1.7415,
      "step": 443200
    },
    {
      "epoch": 33.86295928500496,
      "grad_norm": 4.655451774597168,
      "learning_rate": 7.671300893743794e-06,
      "loss": 1.6826,
      "step": 443300
    },
    {
      "epoch": 33.87059812084638,
      "grad_norm": 8.417617797851562,
      "learning_rate": 7.661752348942021e-06,
      "loss": 1.7162,
      "step": 443400
    },
    {
      "epoch": 33.8782369566878,
      "grad_norm": 6.306179523468018,
      "learning_rate": 7.65220380414025e-06,
      "loss": 1.6502,
      "step": 443500
    },
    {
      "epoch": 33.88587579252922,
      "grad_norm": 6.706884860992432,
      "learning_rate": 7.642655259338477e-06,
      "loss": 1.7129,
      "step": 443600
    },
    {
      "epoch": 33.89351462837064,
      "grad_norm": 7.978837490081787,
      "learning_rate": 7.633106714536704e-06,
      "loss": 1.7395,
      "step": 443700
    },
    {
      "epoch": 33.90115346421205,
      "grad_norm": 9.960436820983887,
      "learning_rate": 7.623558169734933e-06,
      "loss": 1.7033,
      "step": 443800
    },
    {
      "epoch": 33.90879230005347,
      "grad_norm": 10.816654205322266,
      "learning_rate": 7.6140096249331606e-06,
      "loss": 1.712,
      "step": 443900
    },
    {
      "epoch": 33.91643113589489,
      "grad_norm": 6.686206817626953,
      "learning_rate": 7.6044610801313876e-06,
      "loss": 1.7556,
      "step": 444000
    },
    {
      "epoch": 33.92406997173631,
      "grad_norm": 8.290252685546875,
      "learning_rate": 7.594912535329616e-06,
      "loss": 1.7748,
      "step": 444100
    },
    {
      "epoch": 33.93170880757773,
      "grad_norm": 9.58303451538086,
      "learning_rate": 7.585363990527844e-06,
      "loss": 1.6269,
      "step": 444200
    },
    {
      "epoch": 33.93934764341914,
      "grad_norm": 6.121604919433594,
      "learning_rate": 7.575815445726071e-06,
      "loss": 1.7244,
      "step": 444300
    },
    {
      "epoch": 33.94698647926056,
      "grad_norm": 9.174691200256348,
      "learning_rate": 7.5662669009243e-06,
      "loss": 1.7962,
      "step": 444400
    },
    {
      "epoch": 33.95462531510198,
      "grad_norm": 8.763126373291016,
      "learning_rate": 7.556718356122528e-06,
      "loss": 1.7287,
      "step": 444500
    },
    {
      "epoch": 33.9622641509434,
      "grad_norm": 7.384233474731445,
      "learning_rate": 7.547169811320755e-06,
      "loss": 1.7452,
      "step": 444600
    },
    {
      "epoch": 33.96990298678482,
      "grad_norm": 7.300814628601074,
      "learning_rate": 7.537621266518983e-06,
      "loss": 1.7337,
      "step": 444700
    },
    {
      "epoch": 33.97754182262623,
      "grad_norm": 7.740078449249268,
      "learning_rate": 7.528072721717211e-06,
      "loss": 1.6323,
      "step": 444800
    },
    {
      "epoch": 33.98518065846765,
      "grad_norm": 9.29552173614502,
      "learning_rate": 7.518524176915438e-06,
      "loss": 1.7057,
      "step": 444900
    },
    {
      "epoch": 33.99281949430907,
      "grad_norm": 9.868123054504395,
      "learning_rate": 7.508975632113666e-06,
      "loss": 1.6252,
      "step": 445000
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.795920729637146,
      "eval_runtime": 1.478,
      "eval_samples_per_second": 466.856,
      "eval_steps_per_second": 466.856,
      "step": 445094
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.4810707569122314,
      "eval_runtime": 27.8293,
      "eval_samples_per_second": 470.404,
      "eval_steps_per_second": 470.404,
      "step": 445094
    },
    {
      "epoch": 34.00045833015049,
      "grad_norm": 6.487915992736816,
      "learning_rate": 7.499427087311895e-06,
      "loss": 1.7011,
      "step": 445100
    },
    {
      "epoch": 34.008097165991906,
      "grad_norm": 7.447747707366943,
      "learning_rate": 7.489878542510122e-06,
      "loss": 1.7158,
      "step": 445200
    },
    {
      "epoch": 34.01573600183332,
      "grad_norm": 6.699521064758301,
      "learning_rate": 7.480329997708349e-06,
      "loss": 1.7226,
      "step": 445300
    },
    {
      "epoch": 34.02337483767474,
      "grad_norm": 8.845501899719238,
      "learning_rate": 7.470781452906577e-06,
      "loss": 1.6615,
      "step": 445400
    },
    {
      "epoch": 34.031013673516156,
      "grad_norm": 15.329330444335938,
      "learning_rate": 7.461232908104805e-06,
      "loss": 1.7073,
      "step": 445500
    },
    {
      "epoch": 34.038652509357576,
      "grad_norm": 7.893105983734131,
      "learning_rate": 7.451684363303032e-06,
      "loss": 1.6323,
      "step": 445600
    },
    {
      "epoch": 34.046291345198995,
      "grad_norm": 7.097867012023926,
      "learning_rate": 7.442135818501261e-06,
      "loss": 1.7317,
      "step": 445700
    },
    {
      "epoch": 34.05393018104041,
      "grad_norm": 7.403366565704346,
      "learning_rate": 7.432587273699489e-06,
      "loss": 1.7643,
      "step": 445800
    },
    {
      "epoch": 34.061569016881826,
      "grad_norm": 7.791531085968018,
      "learning_rate": 7.423038728897716e-06,
      "loss": 1.7153,
      "step": 445900
    },
    {
      "epoch": 34.069207852723245,
      "grad_norm": 6.404930114746094,
      "learning_rate": 7.4134901840959445e-06,
      "loss": 1.6709,
      "step": 446000
    },
    {
      "epoch": 34.076846688564665,
      "grad_norm": 8.266838073730469,
      "learning_rate": 7.403941639294172e-06,
      "loss": 1.6463,
      "step": 446100
    },
    {
      "epoch": 34.084485524406084,
      "grad_norm": 7.307079315185547,
      "learning_rate": 7.394393094492399e-06,
      "loss": 1.6515,
      "step": 446200
    },
    {
      "epoch": 34.092124360247496,
      "grad_norm": 9.10888957977295,
      "learning_rate": 7.384844549690627e-06,
      "loss": 1.597,
      "step": 446300
    },
    {
      "epoch": 34.099763196088915,
      "grad_norm": 8.266022682189941,
      "learning_rate": 7.375296004888856e-06,
      "loss": 1.7711,
      "step": 446400
    },
    {
      "epoch": 34.107402031930334,
      "grad_norm": 6.666872024536133,
      "learning_rate": 7.365747460087083e-06,
      "loss": 1.6678,
      "step": 446500
    },
    {
      "epoch": 34.115040867771754,
      "grad_norm": 8.801677703857422,
      "learning_rate": 7.356198915285311e-06,
      "loss": 1.6902,
      "step": 446600
    },
    {
      "epoch": 34.12267970361317,
      "grad_norm": 7.370937824249268,
      "learning_rate": 7.346650370483539e-06,
      "loss": 1.7568,
      "step": 446700
    },
    {
      "epoch": 34.130318539454585,
      "grad_norm": 7.894540786743164,
      "learning_rate": 7.337101825681766e-06,
      "loss": 1.7564,
      "step": 446800
    },
    {
      "epoch": 34.137957375296004,
      "grad_norm": 6.5296406745910645,
      "learning_rate": 7.327553280879993e-06,
      "loss": 1.7023,
      "step": 446900
    },
    {
      "epoch": 34.14559621113742,
      "grad_norm": 7.882818698883057,
      "learning_rate": 7.318004736078222e-06,
      "loss": 1.7339,
      "step": 447000
    },
    {
      "epoch": 34.15323504697884,
      "grad_norm": 6.345357894897461,
      "learning_rate": 7.30845619127645e-06,
      "loss": 1.6721,
      "step": 447100
    },
    {
      "epoch": 34.160873882820255,
      "grad_norm": 7.427247047424316,
      "learning_rate": 7.298907646474677e-06,
      "loss": 1.6637,
      "step": 447200
    },
    {
      "epoch": 34.168512718661674,
      "grad_norm": 7.936208248138428,
      "learning_rate": 7.289359101672906e-06,
      "loss": 1.7247,
      "step": 447300
    },
    {
      "epoch": 34.17615155450309,
      "grad_norm": 6.900907039642334,
      "learning_rate": 7.2798105568711335e-06,
      "loss": 1.7503,
      "step": 447400
    },
    {
      "epoch": 34.18379039034451,
      "grad_norm": 8.489643096923828,
      "learning_rate": 7.2702620120693605e-06,
      "loss": 1.724,
      "step": 447500
    },
    {
      "epoch": 34.19142922618593,
      "grad_norm": 6.009905815124512,
      "learning_rate": 7.260713467267589e-06,
      "loss": 1.705,
      "step": 447600
    },
    {
      "epoch": 34.199068062027344,
      "grad_norm": 7.942802429199219,
      "learning_rate": 7.251164922465817e-06,
      "loss": 1.591,
      "step": 447700
    },
    {
      "epoch": 34.20670689786876,
      "grad_norm": 7.412447929382324,
      "learning_rate": 7.241616377664044e-06,
      "loss": 1.6363,
      "step": 447800
    },
    {
      "epoch": 34.21434573371018,
      "grad_norm": 6.981125354766846,
      "learning_rate": 7.232067832862272e-06,
      "loss": 1.8206,
      "step": 447900
    },
    {
      "epoch": 34.2219845695516,
      "grad_norm": 8.160111427307129,
      "learning_rate": 7.2225192880605006e-06,
      "loss": 1.6696,
      "step": 448000
    },
    {
      "epoch": 34.22962340539302,
      "grad_norm": 10.688072204589844,
      "learning_rate": 7.2129707432587276e-06,
      "loss": 1.6548,
      "step": 448100
    },
    {
      "epoch": 34.23726224123443,
      "grad_norm": 8.544353485107422,
      "learning_rate": 7.2034221984569546e-06,
      "loss": 1.7406,
      "step": 448200
    },
    {
      "epoch": 34.24490107707585,
      "grad_norm": 6.203709125518799,
      "learning_rate": 7.193873653655184e-06,
      "loss": 1.6201,
      "step": 448300
    },
    {
      "epoch": 34.25253991291727,
      "grad_norm": 8.942601203918457,
      "learning_rate": 7.184325108853411e-06,
      "loss": 1.6893,
      "step": 448400
    },
    {
      "epoch": 34.26017874875869,
      "grad_norm": 9.464700698852539,
      "learning_rate": 7.174776564051638e-06,
      "loss": 1.858,
      "step": 448500
    },
    {
      "epoch": 34.26781758460011,
      "grad_norm": 8.258427619934082,
      "learning_rate": 7.165228019249867e-06,
      "loss": 1.7608,
      "step": 448600
    },
    {
      "epoch": 34.27545642044152,
      "grad_norm": 5.385390758514404,
      "learning_rate": 7.155679474448095e-06,
      "loss": 1.7431,
      "step": 448700
    },
    {
      "epoch": 34.28309525628294,
      "grad_norm": 6.925198554992676,
      "learning_rate": 7.146130929646322e-06,
      "loss": 1.7113,
      "step": 448800
    },
    {
      "epoch": 34.29073409212436,
      "grad_norm": 8.352745056152344,
      "learning_rate": 7.13658238484455e-06,
      "loss": 1.6909,
      "step": 448900
    },
    {
      "epoch": 34.29837292796578,
      "grad_norm": 8.866270065307617,
      "learning_rate": 7.127033840042778e-06,
      "loss": 1.5797,
      "step": 449000
    },
    {
      "epoch": 34.3060117638072,
      "grad_norm": 7.648944854736328,
      "learning_rate": 7.117485295241005e-06,
      "loss": 1.7777,
      "step": 449100
    },
    {
      "epoch": 34.31365059964861,
      "grad_norm": 6.895660877227783,
      "learning_rate": 7.107936750439233e-06,
      "loss": 1.7737,
      "step": 449200
    },
    {
      "epoch": 34.32128943549003,
      "grad_norm": 10.433123588562012,
      "learning_rate": 7.098388205637462e-06,
      "loss": 1.7215,
      "step": 449300
    },
    {
      "epoch": 34.32892827133145,
      "grad_norm": 7.917525768280029,
      "learning_rate": 7.088839660835689e-06,
      "loss": 1.8138,
      "step": 449400
    },
    {
      "epoch": 34.33656710717287,
      "grad_norm": 7.895618438720703,
      "learning_rate": 7.0792911160339166e-06,
      "loss": 1.7229,
      "step": 449500
    },
    {
      "epoch": 34.34420594301429,
      "grad_norm": 6.931880474090576,
      "learning_rate": 7.069742571232145e-06,
      "loss": 1.7321,
      "step": 449600
    },
    {
      "epoch": 34.3518447788557,
      "grad_norm": 8.440454483032227,
      "learning_rate": 7.060194026430372e-06,
      "loss": 1.6226,
      "step": 449700
    },
    {
      "epoch": 34.35948361469712,
      "grad_norm": 8.196157455444336,
      "learning_rate": 7.050645481628599e-06,
      "loss": 1.7202,
      "step": 449800
    },
    {
      "epoch": 34.36712245053854,
      "grad_norm": 8.288944244384766,
      "learning_rate": 7.041096936826829e-06,
      "loss": 1.6715,
      "step": 449900
    },
    {
      "epoch": 34.37476128637996,
      "grad_norm": 8.074138641357422,
      "learning_rate": 7.031548392025056e-06,
      "loss": 1.6928,
      "step": 450000
    },
    {
      "epoch": 34.38240012222138,
      "grad_norm": 8.605033874511719,
      "learning_rate": 7.021999847223283e-06,
      "loss": 1.7899,
      "step": 450100
    },
    {
      "epoch": 34.39003895806279,
      "grad_norm": 8.080626487731934,
      "learning_rate": 7.0124513024215115e-06,
      "loss": 1.7372,
      "step": 450200
    },
    {
      "epoch": 34.39767779390421,
      "grad_norm": 8.509275436401367,
      "learning_rate": 7.002902757619739e-06,
      "loss": 1.7406,
      "step": 450300
    },
    {
      "epoch": 34.40531662974563,
      "grad_norm": 6.890511512756348,
      "learning_rate": 6.993354212817966e-06,
      "loss": 1.7122,
      "step": 450400
    },
    {
      "epoch": 34.412955465587046,
      "grad_norm": 6.407515048980713,
      "learning_rate": 6.983805668016195e-06,
      "loss": 1.6986,
      "step": 450500
    },
    {
      "epoch": 34.420594301428466,
      "grad_norm": 10.606549263000488,
      "learning_rate": 6.974257123214423e-06,
      "loss": 1.684,
      "step": 450600
    },
    {
      "epoch": 34.42823313726988,
      "grad_norm": 8.978297233581543,
      "learning_rate": 6.96470857841265e-06,
      "loss": 1.6365,
      "step": 450700
    },
    {
      "epoch": 34.4358719731113,
      "grad_norm": 6.908784866333008,
      "learning_rate": 6.955160033610878e-06,
      "loss": 1.7034,
      "step": 450800
    },
    {
      "epoch": 34.443510808952716,
      "grad_norm": 7.9899983406066895,
      "learning_rate": 6.945611488809106e-06,
      "loss": 1.694,
      "step": 450900
    },
    {
      "epoch": 34.451149644794135,
      "grad_norm": 6.817541122436523,
      "learning_rate": 6.936062944007333e-06,
      "loss": 1.7489,
      "step": 451000
    },
    {
      "epoch": 34.458788480635555,
      "grad_norm": 6.903706073760986,
      "learning_rate": 6.926514399205561e-06,
      "loss": 1.7348,
      "step": 451100
    },
    {
      "epoch": 34.46642731647697,
      "grad_norm": 5.655760288238525,
      "learning_rate": 6.91696585440379e-06,
      "loss": 1.6576,
      "step": 451200
    },
    {
      "epoch": 34.474066152318386,
      "grad_norm": 6.4616217613220215,
      "learning_rate": 6.907417309602017e-06,
      "loss": 1.7378,
      "step": 451300
    },
    {
      "epoch": 34.481704988159805,
      "grad_norm": 7.2573161125183105,
      "learning_rate": 6.897868764800244e-06,
      "loss": 1.6496,
      "step": 451400
    },
    {
      "epoch": 34.489343824001224,
      "grad_norm": 7.287041187286377,
      "learning_rate": 6.8883202199984735e-06,
      "loss": 1.75,
      "step": 451500
    },
    {
      "epoch": 34.49698265984264,
      "grad_norm": 7.959404945373535,
      "learning_rate": 6.8787716751967005e-06,
      "loss": 1.6525,
      "step": 451600
    },
    {
      "epoch": 34.504621495684056,
      "grad_norm": 5.666585922241211,
      "learning_rate": 6.8692231303949275e-06,
      "loss": 1.6477,
      "step": 451700
    },
    {
      "epoch": 34.512260331525475,
      "grad_norm": 6.7903947830200195,
      "learning_rate": 6.859674585593156e-06,
      "loss": 1.7387,
      "step": 451800
    },
    {
      "epoch": 34.519899167366894,
      "grad_norm": 7.312031269073486,
      "learning_rate": 6.850126040791384e-06,
      "loss": 1.7776,
      "step": 451900
    },
    {
      "epoch": 34.52753800320831,
      "grad_norm": 8.271918296813965,
      "learning_rate": 6.840577495989611e-06,
      "loss": 1.7224,
      "step": 452000
    },
    {
      "epoch": 34.535176839049726,
      "grad_norm": 5.192245006561279,
      "learning_rate": 6.831028951187839e-06,
      "loss": 1.7035,
      "step": 452100
    },
    {
      "epoch": 34.542815674891145,
      "grad_norm": 7.825064182281494,
      "learning_rate": 6.8214804063860676e-06,
      "loss": 1.6919,
      "step": 452200
    },
    {
      "epoch": 34.550454510732564,
      "grad_norm": 8.406558990478516,
      "learning_rate": 6.8119318615842946e-06,
      "loss": 1.6868,
      "step": 452300
    },
    {
      "epoch": 34.55809334657398,
      "grad_norm": 9.22240161895752,
      "learning_rate": 6.802383316782522e-06,
      "loss": 1.7559,
      "step": 452400
    },
    {
      "epoch": 34.5657321824154,
      "grad_norm": 8.926549911499023,
      "learning_rate": 6.792834771980751e-06,
      "loss": 1.7425,
      "step": 452500
    },
    {
      "epoch": 34.573371018256815,
      "grad_norm": 9.714725494384766,
      "learning_rate": 6.783286227178978e-06,
      "loss": 1.6833,
      "step": 452600
    },
    {
      "epoch": 34.581009854098234,
      "grad_norm": 6.194821834564209,
      "learning_rate": 6.773737682377206e-06,
      "loss": 1.7647,
      "step": 452700
    },
    {
      "epoch": 34.58864868993965,
      "grad_norm": 9.386152267456055,
      "learning_rate": 6.764189137575435e-06,
      "loss": 1.7216,
      "step": 452800
    },
    {
      "epoch": 34.59628752578107,
      "grad_norm": 7.128841400146484,
      "learning_rate": 6.754640592773662e-06,
      "loss": 1.7061,
      "step": 452900
    },
    {
      "epoch": 34.60392636162249,
      "grad_norm": 10.204601287841797,
      "learning_rate": 6.745092047971889e-06,
      "loss": 1.6806,
      "step": 453000
    },
    {
      "epoch": 34.611565197463904,
      "grad_norm": 7.846474647521973,
      "learning_rate": 6.735543503170118e-06,
      "loss": 1.7406,
      "step": 453100
    },
    {
      "epoch": 34.61920403330532,
      "grad_norm": 5.891078472137451,
      "learning_rate": 6.725994958368345e-06,
      "loss": 1.7082,
      "step": 453200
    },
    {
      "epoch": 34.62684286914674,
      "grad_norm": 8.473398208618164,
      "learning_rate": 6.716446413566572e-06,
      "loss": 1.6717,
      "step": 453300
    },
    {
      "epoch": 34.63448170498816,
      "grad_norm": 9.923624992370605,
      "learning_rate": 6.706897868764801e-06,
      "loss": 1.7034,
      "step": 453400
    },
    {
      "epoch": 34.64212054082958,
      "grad_norm": 8.426238059997559,
      "learning_rate": 6.697349323963029e-06,
      "loss": 1.7248,
      "step": 453500
    },
    {
      "epoch": 34.64975937667099,
      "grad_norm": 7.615166187286377,
      "learning_rate": 6.687800779161256e-06,
      "loss": 1.7006,
      "step": 453600
    },
    {
      "epoch": 34.65739821251241,
      "grad_norm": 9.307135581970215,
      "learning_rate": 6.6782522343594836e-06,
      "loss": 1.7063,
      "step": 453700
    },
    {
      "epoch": 34.66503704835383,
      "grad_norm": 9.165987014770508,
      "learning_rate": 6.668703689557712e-06,
      "loss": 1.7321,
      "step": 453800
    },
    {
      "epoch": 34.67267588419525,
      "grad_norm": 6.389023780822754,
      "learning_rate": 6.659155144755939e-06,
      "loss": 1.7412,
      "step": 453900
    },
    {
      "epoch": 34.68031472003667,
      "grad_norm": 5.83396577835083,
      "learning_rate": 6.649606599954167e-06,
      "loss": 1.6814,
      "step": 454000
    },
    {
      "epoch": 34.68795355587808,
      "grad_norm": 7.809436798095703,
      "learning_rate": 6.640058055152396e-06,
      "loss": 1.6992,
      "step": 454100
    },
    {
      "epoch": 34.6955923917195,
      "grad_norm": 5.055444717407227,
      "learning_rate": 6.630509510350623e-06,
      "loss": 1.7314,
      "step": 454200
    },
    {
      "epoch": 34.70323122756092,
      "grad_norm": 5.5224432945251465,
      "learning_rate": 6.620960965548851e-06,
      "loss": 1.6791,
      "step": 454300
    },
    {
      "epoch": 34.71087006340234,
      "grad_norm": 9.042318344116211,
      "learning_rate": 6.611412420747079e-06,
      "loss": 1.8267,
      "step": 454400
    },
    {
      "epoch": 34.71850889924376,
      "grad_norm": 9.424921035766602,
      "learning_rate": 6.601863875945306e-06,
      "loss": 1.7588,
      "step": 454500
    },
    {
      "epoch": 34.72614773508517,
      "grad_norm": 10.439438819885254,
      "learning_rate": 6.592315331143533e-06,
      "loss": 1.8032,
      "step": 454600
    },
    {
      "epoch": 34.73378657092659,
      "grad_norm": 7.108526706695557,
      "learning_rate": 6.582766786341763e-06,
      "loss": 1.6873,
      "step": 454700
    },
    {
      "epoch": 34.74142540676801,
      "grad_norm": 9.153412818908691,
      "learning_rate": 6.57321824153999e-06,
      "loss": 1.7598,
      "step": 454800
    },
    {
      "epoch": 34.74906424260943,
      "grad_norm": 6.159564018249512,
      "learning_rate": 6.563669696738217e-06,
      "loss": 1.7624,
      "step": 454900
    },
    {
      "epoch": 34.75670307845085,
      "grad_norm": 9.048062324523926,
      "learning_rate": 6.554121151936445e-06,
      "loss": 1.5959,
      "step": 455000
    },
    {
      "epoch": 34.76434191429226,
      "grad_norm": 7.3868184089660645,
      "learning_rate": 6.544572607134673e-06,
      "loss": 1.7064,
      "step": 455100
    },
    {
      "epoch": 34.77198075013368,
      "grad_norm": 7.8154706954956055,
      "learning_rate": 6.5350240623329e-06,
      "loss": 1.7065,
      "step": 455200
    },
    {
      "epoch": 34.7796195859751,
      "grad_norm": 9.125395774841309,
      "learning_rate": 6.525475517531128e-06,
      "loss": 1.829,
      "step": 455300
    },
    {
      "epoch": 34.78725842181652,
      "grad_norm": 7.61521053314209,
      "learning_rate": 6.515926972729357e-06,
      "loss": 1.7931,
      "step": 455400
    },
    {
      "epoch": 34.79489725765794,
      "grad_norm": 8.085563659667969,
      "learning_rate": 6.506378427927584e-06,
      "loss": 1.6571,
      "step": 455500
    },
    {
      "epoch": 34.80253609349935,
      "grad_norm": 7.63947868347168,
      "learning_rate": 6.496829883125812e-06,
      "loss": 1.6507,
      "step": 455600
    },
    {
      "epoch": 34.81017492934077,
      "grad_norm": 8.063940048217773,
      "learning_rate": 6.4872813383240405e-06,
      "loss": 1.731,
      "step": 455700
    },
    {
      "epoch": 34.81781376518219,
      "grad_norm": 6.589540958404541,
      "learning_rate": 6.4777327935222675e-06,
      "loss": 1.6978,
      "step": 455800
    },
    {
      "epoch": 34.825452601023606,
      "grad_norm": 6.608764171600342,
      "learning_rate": 6.468184248720495e-06,
      "loss": 1.7749,
      "step": 455900
    },
    {
      "epoch": 34.83309143686502,
      "grad_norm": 6.6275739669799805,
      "learning_rate": 6.458635703918724e-06,
      "loss": 1.7086,
      "step": 456000
    },
    {
      "epoch": 34.84073027270644,
      "grad_norm": 7.824775218963623,
      "learning_rate": 6.449087159116951e-06,
      "loss": 1.6989,
      "step": 456100
    },
    {
      "epoch": 34.84836910854786,
      "grad_norm": 7.822872638702393,
      "learning_rate": 6.439538614315178e-06,
      "loss": 1.8222,
      "step": 456200
    },
    {
      "epoch": 34.856007944389276,
      "grad_norm": 6.992080211639404,
      "learning_rate": 6.4299900695134076e-06,
      "loss": 1.7646,
      "step": 456300
    },
    {
      "epoch": 34.863646780230695,
      "grad_norm": 6.532874584197998,
      "learning_rate": 6.4204415247116346e-06,
      "loss": 1.7208,
      "step": 456400
    },
    {
      "epoch": 34.87128561607211,
      "grad_norm": 8.503090858459473,
      "learning_rate": 6.4108929799098616e-06,
      "loss": 1.757,
      "step": 456500
    },
    {
      "epoch": 34.87892445191353,
      "grad_norm": 7.806046485900879,
      "learning_rate": 6.401344435108089e-06,
      "loss": 1.6791,
      "step": 456600
    },
    {
      "epoch": 34.886563287754946,
      "grad_norm": 12.186174392700195,
      "learning_rate": 6.391795890306318e-06,
      "loss": 1.7042,
      "step": 456700
    },
    {
      "epoch": 34.894202123596365,
      "grad_norm": 3.3169305324554443,
      "learning_rate": 6.382247345504545e-06,
      "loss": 1.6013,
      "step": 456800
    },
    {
      "epoch": 34.901840959437784,
      "grad_norm": 6.34982967376709,
      "learning_rate": 6.372698800702773e-06,
      "loss": 1.6306,
      "step": 456900
    },
    {
      "epoch": 34.909479795279196,
      "grad_norm": 5.522075176239014,
      "learning_rate": 6.363150255901002e-06,
      "loss": 1.7279,
      "step": 457000
    },
    {
      "epoch": 34.917118631120616,
      "grad_norm": 7.59018611907959,
      "learning_rate": 6.353601711099229e-06,
      "loss": 1.682,
      "step": 457100
    },
    {
      "epoch": 34.924757466962035,
      "grad_norm": 8.549399375915527,
      "learning_rate": 6.3440531662974565e-06,
      "loss": 1.7023,
      "step": 457200
    },
    {
      "epoch": 34.932396302803454,
      "grad_norm": 8.42109489440918,
      "learning_rate": 6.334504621495685e-06,
      "loss": 1.7288,
      "step": 457300
    },
    {
      "epoch": 34.94003513864487,
      "grad_norm": 5.85672664642334,
      "learning_rate": 6.324956076693912e-06,
      "loss": 1.6445,
      "step": 457400
    },
    {
      "epoch": 34.947673974486285,
      "grad_norm": 9.130066871643066,
      "learning_rate": 6.31540753189214e-06,
      "loss": 1.8231,
      "step": 457500
    },
    {
      "epoch": 34.955312810327705,
      "grad_norm": 10.160743713378906,
      "learning_rate": 6.305858987090369e-06,
      "loss": 1.7103,
      "step": 457600
    },
    {
      "epoch": 34.962951646169124,
      "grad_norm": 8.052643775939941,
      "learning_rate": 6.296310442288596e-06,
      "loss": 1.7633,
      "step": 457700
    },
    {
      "epoch": 34.97059048201054,
      "grad_norm": 9.121658325195312,
      "learning_rate": 6.286761897486823e-06,
      "loss": 1.7297,
      "step": 457800
    },
    {
      "epoch": 34.97822931785196,
      "grad_norm": 7.12194299697876,
      "learning_rate": 6.2772133526850506e-06,
      "loss": 1.7289,
      "step": 457900
    },
    {
      "epoch": 34.985868153693374,
      "grad_norm": 7.677619457244873,
      "learning_rate": 6.267664807883279e-06,
      "loss": 1.6764,
      "step": 458000
    },
    {
      "epoch": 34.993506989534794,
      "grad_norm": 7.819802761077881,
      "learning_rate": 6.258116263081506e-06,
      "loss": 1.7389,
      "step": 458100
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.7887924909591675,
      "eval_runtime": 1.4849,
      "eval_samples_per_second": 464.666,
      "eval_steps_per_second": 464.666,
      "step": 458185
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.4753509759902954,
      "eval_runtime": 27.8108,
      "eval_samples_per_second": 470.716,
      "eval_steps_per_second": 470.716,
      "step": 458185
    },
    {
      "epoch": 35.00114582537621,
      "grad_norm": 8.904838562011719,
      "learning_rate": 6.248567718279735e-06,
      "loss": 1.7555,
      "step": 458200
    },
    {
      "epoch": 35.00878466121763,
      "grad_norm": 9.023012161254883,
      "learning_rate": 6.239019173477962e-06,
      "loss": 1.7124,
      "step": 458300
    },
    {
      "epoch": 35.01642349705905,
      "grad_norm": 6.3404154777526855,
      "learning_rate": 6.22947062867619e-06,
      "loss": 1.6523,
      "step": 458400
    },
    {
      "epoch": 35.02406233290046,
      "grad_norm": 7.244789123535156,
      "learning_rate": 6.2199220838744185e-06,
      "loss": 1.6893,
      "step": 458500
    },
    {
      "epoch": 35.03170116874188,
      "grad_norm": 10.435162544250488,
      "learning_rate": 6.2103735390726455e-06,
      "loss": 1.6943,
      "step": 458600
    },
    {
      "epoch": 35.0393400045833,
      "grad_norm": 6.633112907409668,
      "learning_rate": 6.200824994270873e-06,
      "loss": 1.7516,
      "step": 458700
    },
    {
      "epoch": 35.04697884042472,
      "grad_norm": 7.2545647621154785,
      "learning_rate": 6.191276449469101e-06,
      "loss": 1.6786,
      "step": 458800
    },
    {
      "epoch": 35.05461767626614,
      "grad_norm": 7.802933692932129,
      "learning_rate": 6.181727904667329e-06,
      "loss": 1.7882,
      "step": 458900
    },
    {
      "epoch": 35.06225651210755,
      "grad_norm": 6.186181545257568,
      "learning_rate": 6.172179359865557e-06,
      "loss": 1.8129,
      "step": 459000
    },
    {
      "epoch": 35.06989534794897,
      "grad_norm": 7.298792362213135,
      "learning_rate": 6.162630815063785e-06,
      "loss": 1.6143,
      "step": 459100
    },
    {
      "epoch": 35.07753418379039,
      "grad_norm": 7.441481113433838,
      "learning_rate": 6.1530822702620126e-06,
      "loss": 1.6514,
      "step": 459200
    },
    {
      "epoch": 35.08517301963181,
      "grad_norm": 11.504510879516602,
      "learning_rate": 6.14353372546024e-06,
      "loss": 1.6754,
      "step": 459300
    },
    {
      "epoch": 35.09281185547323,
      "grad_norm": 5.858377456665039,
      "learning_rate": 6.133985180658467e-06,
      "loss": 1.7069,
      "step": 459400
    },
    {
      "epoch": 35.10045069131464,
      "grad_norm": 9.034358024597168,
      "learning_rate": 6.124436635856696e-06,
      "loss": 1.7059,
      "step": 459500
    },
    {
      "epoch": 35.10808952715606,
      "grad_norm": 9.066568374633789,
      "learning_rate": 6.114888091054923e-06,
      "loss": 1.6321,
      "step": 459600
    },
    {
      "epoch": 35.11572836299748,
      "grad_norm": 7.026242733001709,
      "learning_rate": 6.105339546253151e-06,
      "loss": 1.764,
      "step": 459700
    },
    {
      "epoch": 35.1233671988389,
      "grad_norm": 7.271480560302734,
      "learning_rate": 6.09579100145138e-06,
      "loss": 1.6926,
      "step": 459800
    },
    {
      "epoch": 35.13100603468031,
      "grad_norm": 6.350254058837891,
      "learning_rate": 6.086242456649607e-06,
      "loss": 1.7325,
      "step": 459900
    },
    {
      "epoch": 35.13864487052173,
      "grad_norm": 8.606087684631348,
      "learning_rate": 6.0766939118478345e-06,
      "loss": 1.7211,
      "step": 460000
    },
    {
      "epoch": 35.14628370636315,
      "grad_norm": 7.474998950958252,
      "learning_rate": 6.067145367046063e-06,
      "loss": 1.665,
      "step": 460100
    },
    {
      "epoch": 35.15392254220457,
      "grad_norm": 8.004066467285156,
      "learning_rate": 6.05759682224429e-06,
      "loss": 1.751,
      "step": 460200
    },
    {
      "epoch": 35.16156137804599,
      "grad_norm": 9.404953956604004,
      "learning_rate": 6.048048277442518e-06,
      "loss": 1.6632,
      "step": 460300
    },
    {
      "epoch": 35.1692002138874,
      "grad_norm": 8.679457664489746,
      "learning_rate": 6.038499732640746e-06,
      "loss": 1.6981,
      "step": 460400
    },
    {
      "epoch": 35.17683904972882,
      "grad_norm": 7.76074743270874,
      "learning_rate": 6.028951187838974e-06,
      "loss": 1.8216,
      "step": 460500
    },
    {
      "epoch": 35.18447788557024,
      "grad_norm": 6.479882717132568,
      "learning_rate": 6.0194026430372016e-06,
      "loss": 1.7899,
      "step": 460600
    },
    {
      "epoch": 35.19211672141166,
      "grad_norm": 9.925467491149902,
      "learning_rate": 6.009854098235429e-06,
      "loss": 1.6628,
      "step": 460700
    },
    {
      "epoch": 35.19975555725308,
      "grad_norm": 6.549010276794434,
      "learning_rate": 6.000305553433657e-06,
      "loss": 1.6136,
      "step": 460800
    },
    {
      "epoch": 35.20739439309449,
      "grad_norm": 7.058155059814453,
      "learning_rate": 5.990757008631885e-06,
      "loss": 1.6961,
      "step": 460900
    },
    {
      "epoch": 35.21503322893591,
      "grad_norm": 9.22316837310791,
      "learning_rate": 5.981208463830112e-06,
      "loss": 1.7302,
      "step": 461000
    },
    {
      "epoch": 35.22267206477733,
      "grad_norm": 11.529762268066406,
      "learning_rate": 5.971659919028341e-06,
      "loss": 1.7217,
      "step": 461100
    },
    {
      "epoch": 35.23031090061875,
      "grad_norm": 8.674453735351562,
      "learning_rate": 5.962111374226568e-06,
      "loss": 1.6989,
      "step": 461200
    },
    {
      "epoch": 35.237949736460166,
      "grad_norm": 7.826931476593018,
      "learning_rate": 5.952562829424796e-06,
      "loss": 1.7051,
      "step": 461300
    },
    {
      "epoch": 35.24558857230158,
      "grad_norm": 3.3368382453918457,
      "learning_rate": 5.943014284623024e-06,
      "loss": 1.7595,
      "step": 461400
    },
    {
      "epoch": 35.253227408143,
      "grad_norm": 8.663569450378418,
      "learning_rate": 5.933465739821251e-06,
      "loss": 1.7401,
      "step": 461500
    },
    {
      "epoch": 35.26086624398442,
      "grad_norm": 6.403210639953613,
      "learning_rate": 5.923917195019479e-06,
      "loss": 1.669,
      "step": 461600
    },
    {
      "epoch": 35.268505079825836,
      "grad_norm": 8.892086029052734,
      "learning_rate": 5.914368650217708e-06,
      "loss": 1.7041,
      "step": 461700
    },
    {
      "epoch": 35.276143915667255,
      "grad_norm": 8.278168678283691,
      "learning_rate": 5.904820105415935e-06,
      "loss": 1.7301,
      "step": 461800
    },
    {
      "epoch": 35.28378275150867,
      "grad_norm": 5.832798957824707,
      "learning_rate": 5.895271560614163e-06,
      "loss": 1.6247,
      "step": 461900
    },
    {
      "epoch": 35.29142158735009,
      "grad_norm": 9.51414966583252,
      "learning_rate": 5.8857230158123906e-06,
      "loss": 1.7301,
      "step": 462000
    },
    {
      "epoch": 35.299060423191506,
      "grad_norm": 6.985359191894531,
      "learning_rate": 5.876174471010618e-06,
      "loss": 1.7372,
      "step": 462100
    },
    {
      "epoch": 35.306699259032925,
      "grad_norm": 7.182845592498779,
      "learning_rate": 5.866625926208846e-06,
      "loss": 1.6992,
      "step": 462200
    },
    {
      "epoch": 35.314338094874344,
      "grad_norm": 6.489016532897949,
      "learning_rate": 5.857077381407074e-06,
      "loss": 1.7202,
      "step": 462300
    },
    {
      "epoch": 35.321976930715756,
      "grad_norm": 6.637661457061768,
      "learning_rate": 5.847528836605302e-06,
      "loss": 1.7247,
      "step": 462400
    },
    {
      "epoch": 35.329615766557176,
      "grad_norm": 9.944906234741211,
      "learning_rate": 5.837980291803529e-06,
      "loss": 1.7824,
      "step": 462500
    },
    {
      "epoch": 35.337254602398595,
      "grad_norm": 6.074448585510254,
      "learning_rate": 5.828431747001757e-06,
      "loss": 1.7721,
      "step": 462600
    },
    {
      "epoch": 35.344893438240014,
      "grad_norm": 9.836831092834473,
      "learning_rate": 5.8188832021999855e-06,
      "loss": 1.7141,
      "step": 462700
    },
    {
      "epoch": 35.35253227408143,
      "grad_norm": 9.06381893157959,
      "learning_rate": 5.8093346573982125e-06,
      "loss": 1.6677,
      "step": 462800
    },
    {
      "epoch": 35.360171109922845,
      "grad_norm": 7.716080665588379,
      "learning_rate": 5.79978611259644e-06,
      "loss": 1.7693,
      "step": 462900
    },
    {
      "epoch": 35.367809945764265,
      "grad_norm": 6.117785930633545,
      "learning_rate": 5.790237567794669e-06,
      "loss": 1.7098,
      "step": 463000
    },
    {
      "epoch": 35.375448781605684,
      "grad_norm": 7.0590362548828125,
      "learning_rate": 5.780689022992896e-06,
      "loss": 1.6714,
      "step": 463100
    },
    {
      "epoch": 35.3830876174471,
      "grad_norm": 5.732290267944336,
      "learning_rate": 5.771140478191124e-06,
      "loss": 1.7091,
      "step": 463200
    },
    {
      "epoch": 35.39072645328852,
      "grad_norm": 7.258004188537598,
      "learning_rate": 5.761591933389352e-06,
      "loss": 1.6726,
      "step": 463300
    },
    {
      "epoch": 35.398365289129934,
      "grad_norm": 8.107033729553223,
      "learning_rate": 5.7520433885875796e-06,
      "loss": 1.6426,
      "step": 463400
    },
    {
      "epoch": 35.40600412497135,
      "grad_norm": 8.22929859161377,
      "learning_rate": 5.742494843785807e-06,
      "loss": 1.666,
      "step": 463500
    },
    {
      "epoch": 35.41364296081277,
      "grad_norm": 8.010754585266113,
      "learning_rate": 5.732946298984035e-06,
      "loss": 1.6758,
      "step": 463600
    },
    {
      "epoch": 35.42128179665419,
      "grad_norm": 7.731673240661621,
      "learning_rate": 5.723397754182263e-06,
      "loss": 1.7077,
      "step": 463700
    },
    {
      "epoch": 35.42892063249561,
      "grad_norm": 9.279053688049316,
      "learning_rate": 5.713849209380491e-06,
      "loss": 1.7107,
      "step": 463800
    },
    {
      "epoch": 35.43655946833702,
      "grad_norm": 8.542547225952148,
      "learning_rate": 5.704300664578719e-06,
      "loss": 1.654,
      "step": 463900
    },
    {
      "epoch": 35.44419830417844,
      "grad_norm": 6.266664981842041,
      "learning_rate": 5.694752119776947e-06,
      "loss": 1.7072,
      "step": 464000
    },
    {
      "epoch": 35.45183714001986,
      "grad_norm": 8.484953880310059,
      "learning_rate": 5.685203574975174e-06,
      "loss": 1.7502,
      "step": 464100
    },
    {
      "epoch": 35.45947597586128,
      "grad_norm": 8.071483612060547,
      "learning_rate": 5.6756550301734015e-06,
      "loss": 1.7132,
      "step": 464200
    },
    {
      "epoch": 35.46711481170269,
      "grad_norm": 8.314319610595703,
      "learning_rate": 5.66610648537163e-06,
      "loss": 1.8393,
      "step": 464300
    },
    {
      "epoch": 35.47475364754411,
      "grad_norm": 7.114902019500732,
      "learning_rate": 5.656557940569857e-06,
      "loss": 1.6589,
      "step": 464400
    },
    {
      "epoch": 35.48239248338553,
      "grad_norm": 6.097214221954346,
      "learning_rate": 5.647009395768085e-06,
      "loss": 1.7257,
      "step": 464500
    },
    {
      "epoch": 35.49003131922695,
      "grad_norm": 10.429356575012207,
      "learning_rate": 5.637460850966314e-06,
      "loss": 1.6484,
      "step": 464600
    },
    {
      "epoch": 35.49767015506837,
      "grad_norm": 7.460084438323975,
      "learning_rate": 5.627912306164541e-06,
      "loss": 1.6581,
      "step": 464700
    },
    {
      "epoch": 35.50530899090978,
      "grad_norm": 7.989859580993652,
      "learning_rate": 5.6183637613627686e-06,
      "loss": 1.7078,
      "step": 464800
    },
    {
      "epoch": 35.5129478267512,
      "grad_norm": 9.004132270812988,
      "learning_rate": 5.608815216560996e-06,
      "loss": 1.6568,
      "step": 464900
    },
    {
      "epoch": 35.52058666259262,
      "grad_norm": 4.112893104553223,
      "learning_rate": 5.599266671759224e-06,
      "loss": 1.5899,
      "step": 465000
    },
    {
      "epoch": 35.52822549843404,
      "grad_norm": 8.812320709228516,
      "learning_rate": 5.589718126957452e-06,
      "loss": 1.7038,
      "step": 465100
    },
    {
      "epoch": 35.53586433427546,
      "grad_norm": 10.748072624206543,
      "learning_rate": 5.58016958215568e-06,
      "loss": 1.7578,
      "step": 465200
    },
    {
      "epoch": 35.54350317011687,
      "grad_norm": 5.964654922485352,
      "learning_rate": 5.570621037353908e-06,
      "loss": 1.6782,
      "step": 465300
    },
    {
      "epoch": 35.55114200595829,
      "grad_norm": 9.675697326660156,
      "learning_rate": 5.561072492552135e-06,
      "loss": 1.8171,
      "step": 465400
    },
    {
      "epoch": 35.55878084179971,
      "grad_norm": 7.6698899269104,
      "learning_rate": 5.5515239477503635e-06,
      "loss": 1.7253,
      "step": 465500
    },
    {
      "epoch": 35.56641967764113,
      "grad_norm": 7.128386497497559,
      "learning_rate": 5.541975402948591e-06,
      "loss": 1.7152,
      "step": 465600
    },
    {
      "epoch": 35.57405851348255,
      "grad_norm": 8.7159423828125,
      "learning_rate": 5.532426858146818e-06,
      "loss": 1.6694,
      "step": 465700
    },
    {
      "epoch": 35.58169734932396,
      "grad_norm": 9.831704139709473,
      "learning_rate": 5.522878313345046e-06,
      "loss": 1.7519,
      "step": 465800
    },
    {
      "epoch": 35.58933618516538,
      "grad_norm": 7.029451370239258,
      "learning_rate": 5.513329768543275e-06,
      "loss": 1.5893,
      "step": 465900
    },
    {
      "epoch": 35.5969750210068,
      "grad_norm": 7.822367191314697,
      "learning_rate": 5.503781223741502e-06,
      "loss": 1.6784,
      "step": 466000
    },
    {
      "epoch": 35.60461385684822,
      "grad_norm": 7.888941764831543,
      "learning_rate": 5.49423267893973e-06,
      "loss": 1.7635,
      "step": 466100
    },
    {
      "epoch": 35.61225269268964,
      "grad_norm": 8.81457233428955,
      "learning_rate": 5.4846841341379575e-06,
      "loss": 1.7088,
      "step": 466200
    },
    {
      "epoch": 35.61989152853105,
      "grad_norm": 8.918087005615234,
      "learning_rate": 5.475135589336185e-06,
      "loss": 1.7632,
      "step": 466300
    },
    {
      "epoch": 35.62753036437247,
      "grad_norm": 10.899434089660645,
      "learning_rate": 5.465587044534413e-06,
      "loss": 1.7596,
      "step": 466400
    },
    {
      "epoch": 35.63516920021389,
      "grad_norm": 7.169009208679199,
      "learning_rate": 5.456038499732641e-06,
      "loss": 1.6742,
      "step": 466500
    },
    {
      "epoch": 35.64280803605531,
      "grad_norm": 9.246179580688477,
      "learning_rate": 5.446489954930869e-06,
      "loss": 1.6759,
      "step": 466600
    },
    {
      "epoch": 35.650446871896726,
      "grad_norm": 9.232014656066895,
      "learning_rate": 5.436941410129097e-06,
      "loss": 1.7262,
      "step": 466700
    },
    {
      "epoch": 35.65808570773814,
      "grad_norm": 7.609327793121338,
      "learning_rate": 5.427392865327325e-06,
      "loss": 1.7263,
      "step": 466800
    },
    {
      "epoch": 35.66572454357956,
      "grad_norm": 9.580304145812988,
      "learning_rate": 5.4178443205255525e-06,
      "loss": 1.7338,
      "step": 466900
    },
    {
      "epoch": 35.67336337942098,
      "grad_norm": 8.782898902893066,
      "learning_rate": 5.4082957757237795e-06,
      "loss": 1.7896,
      "step": 467000
    },
    {
      "epoch": 35.681002215262396,
      "grad_norm": 7.034870147705078,
      "learning_rate": 5.398747230922007e-06,
      "loss": 1.6407,
      "step": 467100
    },
    {
      "epoch": 35.688641051103815,
      "grad_norm": 5.00610876083374,
      "learning_rate": 5.389198686120236e-06,
      "loss": 1.6912,
      "step": 467200
    },
    {
      "epoch": 35.69627988694523,
      "grad_norm": 8.426281929016113,
      "learning_rate": 5.379650141318463e-06,
      "loss": 1.7634,
      "step": 467300
    },
    {
      "epoch": 35.703918722786646,
      "grad_norm": 7.885986804962158,
      "learning_rate": 5.370101596516691e-06,
      "loss": 1.6803,
      "step": 467400
    },
    {
      "epoch": 35.711557558628066,
      "grad_norm": 7.682908535003662,
      "learning_rate": 5.360553051714919e-06,
      "loss": 1.6689,
      "step": 467500
    },
    {
      "epoch": 35.719196394469485,
      "grad_norm": 8.988033294677734,
      "learning_rate": 5.3510045069131465e-06,
      "loss": 1.7603,
      "step": 467600
    },
    {
      "epoch": 35.726835230310904,
      "grad_norm": 7.996571063995361,
      "learning_rate": 5.341455962111374e-06,
      "loss": 1.6215,
      "step": 467700
    },
    {
      "epoch": 35.734474066152316,
      "grad_norm": 7.848865032196045,
      "learning_rate": 5.331907417309602e-06,
      "loss": 1.8078,
      "step": 467800
    },
    {
      "epoch": 35.742112901993735,
      "grad_norm": 8.831120491027832,
      "learning_rate": 5.32235887250783e-06,
      "loss": 1.6454,
      "step": 467900
    },
    {
      "epoch": 35.749751737835155,
      "grad_norm": 6.696578502655029,
      "learning_rate": 5.312810327706058e-06,
      "loss": 1.7266,
      "step": 468000
    },
    {
      "epoch": 35.757390573676574,
      "grad_norm": 7.365776062011719,
      "learning_rate": 5.303261782904286e-06,
      "loss": 1.6608,
      "step": 468100
    },
    {
      "epoch": 35.76502940951799,
      "grad_norm": 7.704281806945801,
      "learning_rate": 5.293713238102514e-06,
      "loss": 1.7789,
      "step": 468200
    },
    {
      "epoch": 35.772668245359405,
      "grad_norm": 6.740980625152588,
      "learning_rate": 5.284164693300741e-06,
      "loss": 1.6707,
      "step": 468300
    },
    {
      "epoch": 35.780307081200824,
      "grad_norm": 8.985316276550293,
      "learning_rate": 5.274616148498969e-06,
      "loss": 1.6774,
      "step": 468400
    },
    {
      "epoch": 35.787945917042244,
      "grad_norm": 13.009049415588379,
      "learning_rate": 5.265067603697197e-06,
      "loss": 1.8115,
      "step": 468500
    },
    {
      "epoch": 35.79558475288366,
      "grad_norm": 6.623354911804199,
      "learning_rate": 5.255519058895424e-06,
      "loss": 1.6562,
      "step": 468600
    },
    {
      "epoch": 35.803223588725075,
      "grad_norm": 6.518723964691162,
      "learning_rate": 5.245970514093652e-06,
      "loss": 1.7356,
      "step": 468700
    },
    {
      "epoch": 35.810862424566494,
      "grad_norm": 7.106374263763428,
      "learning_rate": 5.236421969291881e-06,
      "loss": 1.7006,
      "step": 468800
    },
    {
      "epoch": 35.81850126040791,
      "grad_norm": 8.944886207580566,
      "learning_rate": 5.226873424490108e-06,
      "loss": 1.7221,
      "step": 468900
    },
    {
      "epoch": 35.82614009624933,
      "grad_norm": 7.86426305770874,
      "learning_rate": 5.2173248796883355e-06,
      "loss": 1.7154,
      "step": 469000
    },
    {
      "epoch": 35.83377893209075,
      "grad_norm": 7.709264755249023,
      "learning_rate": 5.207776334886563e-06,
      "loss": 1.7171,
      "step": 469100
    },
    {
      "epoch": 35.841417767932164,
      "grad_norm": 6.553750038146973,
      "learning_rate": 5.198227790084791e-06,
      "loss": 1.6957,
      "step": 469200
    },
    {
      "epoch": 35.84905660377358,
      "grad_norm": 8.214709281921387,
      "learning_rate": 5.188679245283019e-06,
      "loss": 1.6473,
      "step": 469300
    },
    {
      "epoch": 35.856695439615,
      "grad_norm": 5.4102091789245605,
      "learning_rate": 5.179130700481247e-06,
      "loss": 1.6929,
      "step": 469400
    },
    {
      "epoch": 35.86433427545642,
      "grad_norm": 5.969914436340332,
      "learning_rate": 5.169582155679475e-06,
      "loss": 1.6724,
      "step": 469500
    },
    {
      "epoch": 35.87197311129784,
      "grad_norm": 9.431700706481934,
      "learning_rate": 5.160033610877703e-06,
      "loss": 1.7202,
      "step": 469600
    },
    {
      "epoch": 35.87961194713925,
      "grad_norm": 6.848319053649902,
      "learning_rate": 5.1504850660759305e-06,
      "loss": 1.6495,
      "step": 469700
    },
    {
      "epoch": 35.88725078298067,
      "grad_norm": 4.037728786468506,
      "learning_rate": 5.140936521274158e-06,
      "loss": 1.6299,
      "step": 469800
    },
    {
      "epoch": 35.89488961882209,
      "grad_norm": 7.1390862464904785,
      "learning_rate": 5.131387976472385e-06,
      "loss": 1.7672,
      "step": 469900
    },
    {
      "epoch": 35.90252845466351,
      "grad_norm": 9.421347618103027,
      "learning_rate": 5.121839431670614e-06,
      "loss": 1.7953,
      "step": 470000
    },
    {
      "epoch": 35.91016729050493,
      "grad_norm": 8.573175430297852,
      "learning_rate": 5.112290886868842e-06,
      "loss": 1.8087,
      "step": 470100
    },
    {
      "epoch": 35.91780612634634,
      "grad_norm": 6.880861282348633,
      "learning_rate": 5.102742342067069e-06,
      "loss": 1.7413,
      "step": 470200
    },
    {
      "epoch": 35.92544496218776,
      "grad_norm": 6.947147846221924,
      "learning_rate": 5.093193797265297e-06,
      "loss": 1.83,
      "step": 470300
    },
    {
      "epoch": 35.93308379802918,
      "grad_norm": 6.48140287399292,
      "learning_rate": 5.0836452524635245e-06,
      "loss": 1.7262,
      "step": 470400
    },
    {
      "epoch": 35.9407226338706,
      "grad_norm": 6.73414945602417,
      "learning_rate": 5.074096707661752e-06,
      "loss": 1.7898,
      "step": 470500
    },
    {
      "epoch": 35.94836146971202,
      "grad_norm": 8.691387176513672,
      "learning_rate": 5.06454816285998e-06,
      "loss": 1.6846,
      "step": 470600
    },
    {
      "epoch": 35.95600030555343,
      "grad_norm": 10.274106979370117,
      "learning_rate": 5.054999618058208e-06,
      "loss": 1.7214,
      "step": 470700
    },
    {
      "epoch": 35.96363914139485,
      "grad_norm": 7.75604772567749,
      "learning_rate": 5.045451073256436e-06,
      "loss": 1.6959,
      "step": 470800
    },
    {
      "epoch": 35.97127797723627,
      "grad_norm": 8.249784469604492,
      "learning_rate": 5.035902528454664e-06,
      "loss": 1.7691,
      "step": 470900
    },
    {
      "epoch": 35.97891681307769,
      "grad_norm": 8.68730640411377,
      "learning_rate": 5.026353983652892e-06,
      "loss": 1.7967,
      "step": 471000
    },
    {
      "epoch": 35.98655564891911,
      "grad_norm": 6.785549640655518,
      "learning_rate": 5.0168054388511195e-06,
      "loss": 1.6751,
      "step": 471100
    },
    {
      "epoch": 35.99419448476052,
      "grad_norm": 5.346662521362305,
      "learning_rate": 5.0072568940493465e-06,
      "loss": 1.6852,
      "step": 471200
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.7932809591293335,
      "eval_runtime": 1.5069,
      "eval_samples_per_second": 457.902,
      "eval_steps_per_second": 457.902,
      "step": 471276
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.4768526554107666,
      "eval_runtime": 28.5629,
      "eval_samples_per_second": 458.322,
      "eval_steps_per_second": 458.322,
      "step": 471276
    },
    {
      "epoch": 36.00183332060194,
      "grad_norm": 9.247467041015625,
      "learning_rate": 4.997708349247575e-06,
      "loss": 1.7124,
      "step": 471300
    },
    {
      "epoch": 36.00947215644336,
      "grad_norm": 7.8812150955200195,
      "learning_rate": 4.988159804445803e-06,
      "loss": 1.7189,
      "step": 471400
    },
    {
      "epoch": 36.01711099228478,
      "grad_norm": 7.888108253479004,
      "learning_rate": 4.97861125964403e-06,
      "loss": 1.6969,
      "step": 471500
    },
    {
      "epoch": 36.0247498281262,
      "grad_norm": 10.263022422790527,
      "learning_rate": 4.969062714842259e-06,
      "loss": 1.6961,
      "step": 471600
    },
    {
      "epoch": 36.03238866396761,
      "grad_norm": 5.885705947875977,
      "learning_rate": 4.9595141700404865e-06,
      "loss": 1.6853,
      "step": 471700
    },
    {
      "epoch": 36.04002749980903,
      "grad_norm": 8.989577293395996,
      "learning_rate": 4.9499656252387135e-06,
      "loss": 1.7182,
      "step": 471800
    },
    {
      "epoch": 36.04766633565045,
      "grad_norm": 7.883768081665039,
      "learning_rate": 4.940417080436941e-06,
      "loss": 1.751,
      "step": 471900
    },
    {
      "epoch": 36.05530517149187,
      "grad_norm": 6.981677532196045,
      "learning_rate": 4.930868535635169e-06,
      "loss": 1.6471,
      "step": 472000
    },
    {
      "epoch": 36.062944007333286,
      "grad_norm": 9.689594268798828,
      "learning_rate": 4.921319990833397e-06,
      "loss": 1.7006,
      "step": 472100
    },
    {
      "epoch": 36.0705828431747,
      "grad_norm": 6.944420337677002,
      "learning_rate": 4.911771446031625e-06,
      "loss": 1.6799,
      "step": 472200
    },
    {
      "epoch": 36.07822167901612,
      "grad_norm": 8.320368766784668,
      "learning_rate": 4.902222901229853e-06,
      "loss": 1.7453,
      "step": 472300
    },
    {
      "epoch": 36.085860514857536,
      "grad_norm": 7.735958099365234,
      "learning_rate": 4.892674356428081e-06,
      "loss": 1.6754,
      "step": 472400
    },
    {
      "epoch": 36.093499350698956,
      "grad_norm": 5.125920295715332,
      "learning_rate": 4.8831258116263085e-06,
      "loss": 1.6584,
      "step": 472500
    },
    {
      "epoch": 36.10113818654037,
      "grad_norm": 9.074073791503906,
      "learning_rate": 4.873577266824536e-06,
      "loss": 1.7599,
      "step": 472600
    },
    {
      "epoch": 36.10877702238179,
      "grad_norm": 8.105402946472168,
      "learning_rate": 4.864028722022764e-06,
      "loss": 1.6844,
      "step": 472700
    },
    {
      "epoch": 36.116415858223206,
      "grad_norm": 10.92154598236084,
      "learning_rate": 4.854480177220991e-06,
      "loss": 1.6078,
      "step": 472800
    },
    {
      "epoch": 36.124054694064625,
      "grad_norm": 9.161687850952148,
      "learning_rate": 4.84493163241922e-06,
      "loss": 1.6251,
      "step": 472900
    },
    {
      "epoch": 36.131693529906045,
      "grad_norm": 4.451972007751465,
      "learning_rate": 4.835383087617448e-06,
      "loss": 1.6703,
      "step": 473000
    },
    {
      "epoch": 36.13933236574746,
      "grad_norm": 9.432035446166992,
      "learning_rate": 4.825834542815675e-06,
      "loss": 1.6845,
      "step": 473100
    },
    {
      "epoch": 36.146971201588876,
      "grad_norm": 6.727396488189697,
      "learning_rate": 4.816285998013903e-06,
      "loss": 1.7341,
      "step": 473200
    },
    {
      "epoch": 36.154610037430295,
      "grad_norm": 7.633157730102539,
      "learning_rate": 4.80673745321213e-06,
      "loss": 1.7761,
      "step": 473300
    },
    {
      "epoch": 36.162248873271714,
      "grad_norm": 8.526533126831055,
      "learning_rate": 4.797188908410358e-06,
      "loss": 1.7868,
      "step": 473400
    },
    {
      "epoch": 36.169887709113134,
      "grad_norm": 8.221612930297852,
      "learning_rate": 4.787640363608586e-06,
      "loss": 1.6378,
      "step": 473500
    },
    {
      "epoch": 36.177526544954546,
      "grad_norm": 8.486749649047852,
      "learning_rate": 4.778091818806814e-06,
      "loss": 1.7713,
      "step": 473600
    },
    {
      "epoch": 36.185165380795965,
      "grad_norm": 7.674193382263184,
      "learning_rate": 4.768543274005042e-06,
      "loss": 1.7181,
      "step": 473700
    },
    {
      "epoch": 36.192804216637384,
      "grad_norm": 6.073097229003906,
      "learning_rate": 4.75899472920327e-06,
      "loss": 1.7024,
      "step": 473800
    },
    {
      "epoch": 36.2004430524788,
      "grad_norm": 7.685568332672119,
      "learning_rate": 4.7494461844014975e-06,
      "loss": 1.6853,
      "step": 473900
    },
    {
      "epoch": 36.20808188832022,
      "grad_norm": 6.820272445678711,
      "learning_rate": 4.739897639599725e-06,
      "loss": 1.7278,
      "step": 474000
    },
    {
      "epoch": 36.215720724161635,
      "grad_norm": 9.475264549255371,
      "learning_rate": 4.730349094797952e-06,
      "loss": 1.6569,
      "step": 474100
    },
    {
      "epoch": 36.223359560003054,
      "grad_norm": 8.938688278198242,
      "learning_rate": 4.720800549996181e-06,
      "loss": 1.6278,
      "step": 474200
    },
    {
      "epoch": 36.23099839584447,
      "grad_norm": 8.78870677947998,
      "learning_rate": 4.711252005194409e-06,
      "loss": 1.7027,
      "step": 474300
    },
    {
      "epoch": 36.23863723168589,
      "grad_norm": 8.171055793762207,
      "learning_rate": 4.701703460392636e-06,
      "loss": 1.6715,
      "step": 474400
    },
    {
      "epoch": 36.24627606752731,
      "grad_norm": 6.287745475769043,
      "learning_rate": 4.6921549155908645e-06,
      "loss": 1.7668,
      "step": 474500
    },
    {
      "epoch": 36.253914903368724,
      "grad_norm": 9.140572547912598,
      "learning_rate": 4.682606370789092e-06,
      "loss": 1.7532,
      "step": 474600
    },
    {
      "epoch": 36.26155373921014,
      "grad_norm": 10.53403091430664,
      "learning_rate": 4.673057825987319e-06,
      "loss": 1.718,
      "step": 474700
    },
    {
      "epoch": 36.26919257505156,
      "grad_norm": 10.901103019714355,
      "learning_rate": 4.663509281185548e-06,
      "loss": 1.6945,
      "step": 474800
    },
    {
      "epoch": 36.27683141089298,
      "grad_norm": 7.220089912414551,
      "learning_rate": 4.653960736383775e-06,
      "loss": 1.702,
      "step": 474900
    },
    {
      "epoch": 36.2844702467344,
      "grad_norm": 6.846986770629883,
      "learning_rate": 4.644412191582003e-06,
      "loss": 1.8033,
      "step": 475000
    },
    {
      "epoch": 36.29210908257581,
      "grad_norm": 11.414048194885254,
      "learning_rate": 4.634863646780231e-06,
      "loss": 1.7866,
      "step": 475100
    },
    {
      "epoch": 36.29974791841723,
      "grad_norm": 6.893448829650879,
      "learning_rate": 4.625315101978459e-06,
      "loss": 1.6969,
      "step": 475200
    },
    {
      "epoch": 36.30738675425865,
      "grad_norm": 11.43517780303955,
      "learning_rate": 4.6157665571766865e-06,
      "loss": 1.7327,
      "step": 475300
    },
    {
      "epoch": 36.31502559010007,
      "grad_norm": 7.085757732391357,
      "learning_rate": 4.606218012374914e-06,
      "loss": 1.6956,
      "step": 475400
    },
    {
      "epoch": 36.32266442594149,
      "grad_norm": 7.808544158935547,
      "learning_rate": 4.596669467573142e-06,
      "loss": 1.6467,
      "step": 475500
    },
    {
      "epoch": 36.3303032617829,
      "grad_norm": 11.874519348144531,
      "learning_rate": 4.58712092277137e-06,
      "loss": 1.6857,
      "step": 475600
    },
    {
      "epoch": 36.33794209762432,
      "grad_norm": 5.632986068725586,
      "learning_rate": 4.577572377969597e-06,
      "loss": 1.6864,
      "step": 475700
    },
    {
      "epoch": 36.34558093346574,
      "grad_norm": 8.700308799743652,
      "learning_rate": 4.568023833167826e-06,
      "loss": 1.7113,
      "step": 475800
    },
    {
      "epoch": 36.35321976930716,
      "grad_norm": 8.04637336730957,
      "learning_rate": 4.5584752883660535e-06,
      "loss": 1.711,
      "step": 475900
    },
    {
      "epoch": 36.36085860514858,
      "grad_norm": 8.627327919006348,
      "learning_rate": 4.5489267435642805e-06,
      "loss": 1.7968,
      "step": 476000
    },
    {
      "epoch": 36.36849744098999,
      "grad_norm": 9.32357406616211,
      "learning_rate": 4.539378198762509e-06,
      "loss": 1.8031,
      "step": 476100
    },
    {
      "epoch": 36.37613627683141,
      "grad_norm": 4.735348224639893,
      "learning_rate": 4.529829653960736e-06,
      "loss": 1.6644,
      "step": 476200
    },
    {
      "epoch": 36.38377511267283,
      "grad_norm": 8.498575210571289,
      "learning_rate": 4.520281109158964e-06,
      "loss": 1.6336,
      "step": 476300
    },
    {
      "epoch": 36.39141394851425,
      "grad_norm": 6.959092140197754,
      "learning_rate": 4.510732564357193e-06,
      "loss": 1.5939,
      "step": 476400
    },
    {
      "epoch": 36.39905278435567,
      "grad_norm": 9.884432792663574,
      "learning_rate": 4.50118401955542e-06,
      "loss": 1.6252,
      "step": 476500
    },
    {
      "epoch": 36.40669162019708,
      "grad_norm": 10.582592010498047,
      "learning_rate": 4.491635474753648e-06,
      "loss": 1.7229,
      "step": 476600
    },
    {
      "epoch": 36.4143304560385,
      "grad_norm": 7.3445305824279785,
      "learning_rate": 4.4820869299518755e-06,
      "loss": 1.6266,
      "step": 476700
    },
    {
      "epoch": 36.42196929187992,
      "grad_norm": 11.169489860534668,
      "learning_rate": 4.472538385150103e-06,
      "loss": 1.7264,
      "step": 476800
    },
    {
      "epoch": 36.42960812772134,
      "grad_norm": 9.041547775268555,
      "learning_rate": 4.462989840348331e-06,
      "loss": 1.7946,
      "step": 476900
    },
    {
      "epoch": 36.43724696356275,
      "grad_norm": 8.13457202911377,
      "learning_rate": 4.453441295546559e-06,
      "loss": 1.6765,
      "step": 477000
    },
    {
      "epoch": 36.44488579940417,
      "grad_norm": 7.692326545715332,
      "learning_rate": 4.443892750744787e-06,
      "loss": 1.6484,
      "step": 477100
    },
    {
      "epoch": 36.45252463524559,
      "grad_norm": 7.062061309814453,
      "learning_rate": 4.434344205943015e-06,
      "loss": 1.7225,
      "step": 477200
    },
    {
      "epoch": 36.46016347108701,
      "grad_norm": 8.924153327941895,
      "learning_rate": 4.424795661141242e-06,
      "loss": 1.682,
      "step": 477300
    },
    {
      "epoch": 36.46780230692843,
      "grad_norm": 9.75976276397705,
      "learning_rate": 4.41524711633947e-06,
      "loss": 1.6768,
      "step": 477400
    },
    {
      "epoch": 36.47544114276984,
      "grad_norm": 7.539053916931152,
      "learning_rate": 4.405698571537698e-06,
      "loss": 1.6381,
      "step": 477500
    },
    {
      "epoch": 36.48307997861126,
      "grad_norm": 9.227590560913086,
      "learning_rate": 4.396150026735925e-06,
      "loss": 1.7186,
      "step": 477600
    },
    {
      "epoch": 36.49071881445268,
      "grad_norm": 7.698163986206055,
      "learning_rate": 4.386601481934154e-06,
      "loss": 1.6044,
      "step": 477700
    },
    {
      "epoch": 36.498357650294096,
      "grad_norm": 8.567232131958008,
      "learning_rate": 4.377052937132381e-06,
      "loss": 1.6579,
      "step": 477800
    },
    {
      "epoch": 36.505996486135516,
      "grad_norm": 7.873004913330078,
      "learning_rate": 4.367504392330609e-06,
      "loss": 1.7218,
      "step": 477900
    },
    {
      "epoch": 36.51363532197693,
      "grad_norm": 7.05356502532959,
      "learning_rate": 4.3579558475288375e-06,
      "loss": 1.7896,
      "step": 478000
    },
    {
      "epoch": 36.52127415781835,
      "grad_norm": 8.458148956298828,
      "learning_rate": 4.3484073027270645e-06,
      "loss": 1.6448,
      "step": 478100
    },
    {
      "epoch": 36.528912993659766,
      "grad_norm": 8.148224830627441,
      "learning_rate": 4.338858757925292e-06,
      "loss": 1.6742,
      "step": 478200
    },
    {
      "epoch": 36.536551829501185,
      "grad_norm": 7.542145252227783,
      "learning_rate": 4.32931021312352e-06,
      "loss": 1.7894,
      "step": 478300
    },
    {
      "epoch": 36.544190665342605,
      "grad_norm": 11.92346477508545,
      "learning_rate": 4.319761668321748e-06,
      "loss": 1.7264,
      "step": 478400
    },
    {
      "epoch": 36.55182950118402,
      "grad_norm": 6.561234951019287,
      "learning_rate": 4.310213123519976e-06,
      "loss": 1.727,
      "step": 478500
    },
    {
      "epoch": 36.559468337025436,
      "grad_norm": 8.14040470123291,
      "learning_rate": 4.300664578718204e-06,
      "loss": 1.7039,
      "step": 478600
    },
    {
      "epoch": 36.567107172866855,
      "grad_norm": 4.781500339508057,
      "learning_rate": 4.2911160339164315e-06,
      "loss": 1.6758,
      "step": 478700
    },
    {
      "epoch": 36.574746008708274,
      "grad_norm": 6.872020244598389,
      "learning_rate": 4.281567489114659e-06,
      "loss": 1.7045,
      "step": 478800
    },
    {
      "epoch": 36.58238484454969,
      "grad_norm": 6.829588413238525,
      "learning_rate": 4.272018944312886e-06,
      "loss": 1.749,
      "step": 478900
    },
    {
      "epoch": 36.590023680391106,
      "grad_norm": 7.67635440826416,
      "learning_rate": 4.262470399511115e-06,
      "loss": 1.7102,
      "step": 479000
    },
    {
      "epoch": 36.597662516232525,
      "grad_norm": 8.622926712036133,
      "learning_rate": 4.252921854709342e-06,
      "loss": 1.6492,
      "step": 479100
    },
    {
      "epoch": 36.605301352073944,
      "grad_norm": 7.549907684326172,
      "learning_rate": 4.24337330990757e-06,
      "loss": 1.6987,
      "step": 479200
    },
    {
      "epoch": 36.61294018791536,
      "grad_norm": 6.367252826690674,
      "learning_rate": 4.233824765105799e-06,
      "loss": 1.6722,
      "step": 479300
    },
    {
      "epoch": 36.62057902375678,
      "grad_norm": 8.589522361755371,
      "learning_rate": 4.224276220304026e-06,
      "loss": 1.7434,
      "step": 479400
    },
    {
      "epoch": 36.628217859598195,
      "grad_norm": 8.317294120788574,
      "learning_rate": 4.2147276755022535e-06,
      "loss": 1.7814,
      "step": 479500
    },
    {
      "epoch": 36.635856695439614,
      "grad_norm": 7.842792987823486,
      "learning_rate": 4.205179130700482e-06,
      "loss": 1.7383,
      "step": 479600
    },
    {
      "epoch": 36.64349553128103,
      "grad_norm": 8.7788667678833,
      "learning_rate": 4.195630585898709e-06,
      "loss": 1.781,
      "step": 479700
    },
    {
      "epoch": 36.65113436712245,
      "grad_norm": 9.148355484008789,
      "learning_rate": 4.186082041096937e-06,
      "loss": 1.8373,
      "step": 479800
    },
    {
      "epoch": 36.65877320296387,
      "grad_norm": 8.81178092956543,
      "learning_rate": 4.176533496295165e-06,
      "loss": 1.7166,
      "step": 479900
    },
    {
      "epoch": 36.666412038805284,
      "grad_norm": 8.406360626220703,
      "learning_rate": 4.166984951493393e-06,
      "loss": 1.6739,
      "step": 480000
    },
    {
      "epoch": 36.6740508746467,
      "grad_norm": 6.891680717468262,
      "learning_rate": 4.1574364066916205e-06,
      "loss": 1.704,
      "step": 480100
    },
    {
      "epoch": 36.68168971048812,
      "grad_norm": 6.853549957275391,
      "learning_rate": 4.147887861889848e-06,
      "loss": 1.7402,
      "step": 480200
    },
    {
      "epoch": 36.68932854632954,
      "grad_norm": 6.328033924102783,
      "learning_rate": 4.138339317088076e-06,
      "loss": 1.7462,
      "step": 480300
    },
    {
      "epoch": 36.69696738217096,
      "grad_norm": 8.208148002624512,
      "learning_rate": 4.128790772286304e-06,
      "loss": 1.7296,
      "step": 480400
    },
    {
      "epoch": 36.70460621801237,
      "grad_norm": 9.60728931427002,
      "learning_rate": 4.119242227484531e-06,
      "loss": 1.7644,
      "step": 480500
    },
    {
      "epoch": 36.71224505385379,
      "grad_norm": 7.810080528259277,
      "learning_rate": 4.10969368268276e-06,
      "loss": 1.687,
      "step": 480600
    },
    {
      "epoch": 36.71988388969521,
      "grad_norm": 11.11677074432373,
      "learning_rate": 4.100145137880987e-06,
      "loss": 1.6984,
      "step": 480700
    },
    {
      "epoch": 36.72752272553663,
      "grad_norm": 9.357799530029297,
      "learning_rate": 4.090596593079215e-06,
      "loss": 1.8024,
      "step": 480800
    },
    {
      "epoch": 36.73516156137805,
      "grad_norm": 8.053950309753418,
      "learning_rate": 4.081048048277443e-06,
      "loss": 1.6395,
      "step": 480900
    },
    {
      "epoch": 36.74280039721946,
      "grad_norm": 7.767838954925537,
      "learning_rate": 4.07149950347567e-06,
      "loss": 1.7972,
      "step": 481000
    },
    {
      "epoch": 36.75043923306088,
      "grad_norm": 5.488613128662109,
      "learning_rate": 4.061950958673898e-06,
      "loss": 1.6583,
      "step": 481100
    },
    {
      "epoch": 36.7580780689023,
      "grad_norm": 7.826075553894043,
      "learning_rate": 4.052402413872126e-06,
      "loss": 1.6739,
      "step": 481200
    },
    {
      "epoch": 36.76571690474372,
      "grad_norm": 9.481606483459473,
      "learning_rate": 4.042853869070354e-06,
      "loss": 1.7513,
      "step": 481300
    },
    {
      "epoch": 36.77335574058513,
      "grad_norm": 6.130575180053711,
      "learning_rate": 4.033305324268582e-06,
      "loss": 1.7136,
      "step": 481400
    },
    {
      "epoch": 36.78099457642655,
      "grad_norm": 6.727950096130371,
      "learning_rate": 4.0237567794668095e-06,
      "loss": 1.663,
      "step": 481500
    },
    {
      "epoch": 36.78863341226797,
      "grad_norm": 6.340071678161621,
      "learning_rate": 4.014208234665037e-06,
      "loss": 1.668,
      "step": 481600
    },
    {
      "epoch": 36.79627224810939,
      "grad_norm": 8.17136287689209,
      "learning_rate": 4.004659689863265e-06,
      "loss": 1.7688,
      "step": 481700
    },
    {
      "epoch": 36.80391108395081,
      "grad_norm": 8.80251407623291,
      "learning_rate": 3.995111145061493e-06,
      "loss": 1.7341,
      "step": 481800
    },
    {
      "epoch": 36.81154991979222,
      "grad_norm": 9.62891960144043,
      "learning_rate": 3.985562600259721e-06,
      "loss": 1.679,
      "step": 481900
    },
    {
      "epoch": 36.81918875563364,
      "grad_norm": 8.326203346252441,
      "learning_rate": 3.976014055457948e-06,
      "loss": 1.691,
      "step": 482000
    },
    {
      "epoch": 36.82682759147506,
      "grad_norm": 6.899759769439697,
      "learning_rate": 3.966465510656176e-06,
      "loss": 1.6845,
      "step": 482100
    },
    {
      "epoch": 36.83446642731648,
      "grad_norm": 9.06251049041748,
      "learning_rate": 3.9569169658544045e-06,
      "loss": 1.6428,
      "step": 482200
    },
    {
      "epoch": 36.8421052631579,
      "grad_norm": 7.639486312866211,
      "learning_rate": 3.9473684210526315e-06,
      "loss": 1.6761,
      "step": 482300
    },
    {
      "epoch": 36.84974409899931,
      "grad_norm": 8.53836441040039,
      "learning_rate": 3.937819876250859e-06,
      "loss": 1.6322,
      "step": 482400
    },
    {
      "epoch": 36.85738293484073,
      "grad_norm": 7.862280368804932,
      "learning_rate": 3.928271331449088e-06,
      "loss": 1.7636,
      "step": 482500
    },
    {
      "epoch": 36.86502177068215,
      "grad_norm": 6.810059547424316,
      "learning_rate": 3.918722786647315e-06,
      "loss": 1.6824,
      "step": 482600
    },
    {
      "epoch": 36.87266060652357,
      "grad_norm": 7.887315273284912,
      "learning_rate": 3.909174241845543e-06,
      "loss": 1.7079,
      "step": 482700
    },
    {
      "epoch": 36.880299442364986,
      "grad_norm": 6.4139404296875,
      "learning_rate": 3.899625697043771e-06,
      "loss": 1.6942,
      "step": 482800
    },
    {
      "epoch": 36.8879382782064,
      "grad_norm": 8.010963439941406,
      "learning_rate": 3.8900771522419985e-06,
      "loss": 1.6845,
      "step": 482900
    },
    {
      "epoch": 36.89557711404782,
      "grad_norm": 7.715710639953613,
      "learning_rate": 3.880528607440226e-06,
      "loss": 1.6344,
      "step": 483000
    },
    {
      "epoch": 36.90321594988924,
      "grad_norm": 7.406836032867432,
      "learning_rate": 3.870980062638454e-06,
      "loss": 1.7862,
      "step": 483100
    },
    {
      "epoch": 36.910854785730656,
      "grad_norm": 8.439155578613281,
      "learning_rate": 3.861431517836682e-06,
      "loss": 1.6856,
      "step": 483200
    },
    {
      "epoch": 36.918493621572075,
      "grad_norm": 6.302153587341309,
      "learning_rate": 3.85188297303491e-06,
      "loss": 1.6308,
      "step": 483300
    },
    {
      "epoch": 36.92613245741349,
      "grad_norm": 8.416727066040039,
      "learning_rate": 3.842334428233138e-06,
      "loss": 1.7967,
      "step": 483400
    },
    {
      "epoch": 36.93377129325491,
      "grad_norm": 10.231208801269531,
      "learning_rate": 3.832785883431366e-06,
      "loss": 1.7262,
      "step": 483500
    },
    {
      "epoch": 36.941410129096326,
      "grad_norm": 10.38955307006836,
      "learning_rate": 3.823237338629593e-06,
      "loss": 1.6757,
      "step": 483600
    },
    {
      "epoch": 36.949048964937745,
      "grad_norm": 9.845088005065918,
      "learning_rate": 3.813688793827821e-06,
      "loss": 1.7486,
      "step": 483700
    },
    {
      "epoch": 36.956687800779164,
      "grad_norm": 8.370071411132812,
      "learning_rate": 3.8041402490260487e-06,
      "loss": 1.6923,
      "step": 483800
    },
    {
      "epoch": 36.96432663662058,
      "grad_norm": 8.427978515625,
      "learning_rate": 3.794591704224276e-06,
      "loss": 1.8052,
      "step": 483900
    },
    {
      "epoch": 36.971965472461996,
      "grad_norm": 4.089268684387207,
      "learning_rate": 3.7850431594225044e-06,
      "loss": 1.7165,
      "step": 484000
    },
    {
      "epoch": 36.979604308303415,
      "grad_norm": 8.116243362426758,
      "learning_rate": 3.775494614620732e-06,
      "loss": 1.723,
      "step": 484100
    },
    {
      "epoch": 36.987243144144834,
      "grad_norm": 13.178255081176758,
      "learning_rate": 3.7659460698189597e-06,
      "loss": 1.6199,
      "step": 484200
    },
    {
      "epoch": 36.99488197998625,
      "grad_norm": 8.453262329101562,
      "learning_rate": 3.756397525017188e-06,
      "loss": 1.8015,
      "step": 484300
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.7890198230743408,
      "eval_runtime": 1.4936,
      "eval_samples_per_second": 461.958,
      "eval_steps_per_second": 461.958,
      "step": 484367
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.4726917743682861,
      "eval_runtime": 28.5645,
      "eval_samples_per_second": 458.296,
      "eval_steps_per_second": 458.296,
      "step": 484367
    },
    {
      "epoch": 37.002520815827666,
      "grad_norm": 8.635465621948242,
      "learning_rate": 3.746848980215415e-06,
      "loss": 1.764,
      "step": 484400
    },
    {
      "epoch": 37.010159651669085,
      "grad_norm": 7.2934346199035645,
      "learning_rate": 3.7373004354136432e-06,
      "loss": 1.7366,
      "step": 484500
    },
    {
      "epoch": 37.017798487510504,
      "grad_norm": 8.81678581237793,
      "learning_rate": 3.727751890611871e-06,
      "loss": 1.7625,
      "step": 484600
    },
    {
      "epoch": 37.02543732335192,
      "grad_norm": 7.520113945007324,
      "learning_rate": 3.7182033458100985e-06,
      "loss": 1.7057,
      "step": 484700
    },
    {
      "epoch": 37.03307615919334,
      "grad_norm": 8.065958023071289,
      "learning_rate": 3.7086548010083268e-06,
      "loss": 1.7027,
      "step": 484800
    },
    {
      "epoch": 37.040714995034755,
      "grad_norm": 7.3239521980285645,
      "learning_rate": 3.699106256206554e-06,
      "loss": 1.801,
      "step": 484900
    },
    {
      "epoch": 37.048353830876174,
      "grad_norm": 6.138584136962891,
      "learning_rate": 3.689557711404782e-06,
      "loss": 1.6997,
      "step": 485000
    },
    {
      "epoch": 37.05599266671759,
      "grad_norm": 6.817995071411133,
      "learning_rate": 3.6800091666030103e-06,
      "loss": 1.7035,
      "step": 485100
    },
    {
      "epoch": 37.06363150255901,
      "grad_norm": 8.558557510375977,
      "learning_rate": 3.6704606218012373e-06,
      "loss": 1.7651,
      "step": 485200
    },
    {
      "epoch": 37.071270338400424,
      "grad_norm": 8.245258331298828,
      "learning_rate": 3.6609120769994656e-06,
      "loss": 1.7454,
      "step": 485300
    },
    {
      "epoch": 37.07890917424184,
      "grad_norm": 7.39324951171875,
      "learning_rate": 3.6513635321976934e-06,
      "loss": 1.6606,
      "step": 485400
    },
    {
      "epoch": 37.08654801008326,
      "grad_norm": 7.671942710876465,
      "learning_rate": 3.641814987395921e-06,
      "loss": 1.7636,
      "step": 485500
    },
    {
      "epoch": 37.09418684592468,
      "grad_norm": 7.15172004699707,
      "learning_rate": 3.632266442594149e-06,
      "loss": 1.6929,
      "step": 485600
    },
    {
      "epoch": 37.1018256817661,
      "grad_norm": 8.218560218811035,
      "learning_rate": 3.6227178977923765e-06,
      "loss": 1.7384,
      "step": 485700
    },
    {
      "epoch": 37.10946451760751,
      "grad_norm": 7.839950084686279,
      "learning_rate": 3.6131693529906044e-06,
      "loss": 1.7672,
      "step": 485800
    },
    {
      "epoch": 37.11710335344893,
      "grad_norm": 6.845271587371826,
      "learning_rate": 3.6036208081888326e-06,
      "loss": 1.6359,
      "step": 485900
    },
    {
      "epoch": 37.12474218929035,
      "grad_norm": 7.282863616943359,
      "learning_rate": 3.5940722633870596e-06,
      "loss": 1.7371,
      "step": 486000
    },
    {
      "epoch": 37.13238102513177,
      "grad_norm": 7.715573310852051,
      "learning_rate": 3.584523718585288e-06,
      "loss": 1.7644,
      "step": 486100
    },
    {
      "epoch": 37.14001986097319,
      "grad_norm": 8.343070983886719,
      "learning_rate": 3.5749751737835158e-06,
      "loss": 1.7425,
      "step": 486200
    },
    {
      "epoch": 37.1476586968146,
      "grad_norm": 5.034505844116211,
      "learning_rate": 3.565426628981743e-06,
      "loss": 1.7159,
      "step": 486300
    },
    {
      "epoch": 37.15529753265602,
      "grad_norm": 7.398262023925781,
      "learning_rate": 3.5558780841799715e-06,
      "loss": 1.6419,
      "step": 486400
    },
    {
      "epoch": 37.16293636849744,
      "grad_norm": 3.7905704975128174,
      "learning_rate": 3.546329539378199e-06,
      "loss": 1.6581,
      "step": 486500
    },
    {
      "epoch": 37.17057520433886,
      "grad_norm": 4.5844502449035645,
      "learning_rate": 3.5367809945764267e-06,
      "loss": 1.7217,
      "step": 486600
    },
    {
      "epoch": 37.17821404018028,
      "grad_norm": 7.75318717956543,
      "learning_rate": 3.5272324497746546e-06,
      "loss": 1.7201,
      "step": 486700
    },
    {
      "epoch": 37.18585287602169,
      "grad_norm": 6.918208599090576,
      "learning_rate": 3.517683904972882e-06,
      "loss": 1.6617,
      "step": 486800
    },
    {
      "epoch": 37.19349171186311,
      "grad_norm": 6.230824947357178,
      "learning_rate": 3.5081353601711103e-06,
      "loss": 1.6901,
      "step": 486900
    },
    {
      "epoch": 37.20113054770453,
      "grad_norm": 7.168167591094971,
      "learning_rate": 3.4985868153693377e-06,
      "loss": 1.6325,
      "step": 487000
    },
    {
      "epoch": 37.20876938354595,
      "grad_norm": 8.600726127624512,
      "learning_rate": 3.4890382705675655e-06,
      "loss": 1.6406,
      "step": 487100
    },
    {
      "epoch": 37.21640821938737,
      "grad_norm": 9.509369850158691,
      "learning_rate": 3.479489725765794e-06,
      "loss": 1.63,
      "step": 487200
    },
    {
      "epoch": 37.22404705522878,
      "grad_norm": 7.521973609924316,
      "learning_rate": 3.4699411809640212e-06,
      "loss": 1.7072,
      "step": 487300
    },
    {
      "epoch": 37.2316858910702,
      "grad_norm": 7.32751989364624,
      "learning_rate": 3.460392636162249e-06,
      "loss": 1.6584,
      "step": 487400
    },
    {
      "epoch": 37.23932472691162,
      "grad_norm": 7.39549446105957,
      "learning_rate": 3.450844091360477e-06,
      "loss": 1.6717,
      "step": 487500
    },
    {
      "epoch": 37.24696356275304,
      "grad_norm": 8.419828414916992,
      "learning_rate": 3.4412955465587043e-06,
      "loss": 1.7084,
      "step": 487600
    },
    {
      "epoch": 37.25460239859446,
      "grad_norm": 8.40086841583252,
      "learning_rate": 3.4317470017569326e-06,
      "loss": 1.7344,
      "step": 487700
    },
    {
      "epoch": 37.26224123443587,
      "grad_norm": 7.748413562774658,
      "learning_rate": 3.42219845695516e-06,
      "loss": 1.7162,
      "step": 487800
    },
    {
      "epoch": 37.26988007027729,
      "grad_norm": 7.60930061340332,
      "learning_rate": 3.412649912153388e-06,
      "loss": 1.7085,
      "step": 487900
    },
    {
      "epoch": 37.27751890611871,
      "grad_norm": 6.919654846191406,
      "learning_rate": 3.403101367351616e-06,
      "loss": 1.6544,
      "step": 488000
    },
    {
      "epoch": 37.28515774196013,
      "grad_norm": 7.649588108062744,
      "learning_rate": 3.3935528225498436e-06,
      "loss": 1.7182,
      "step": 488100
    },
    {
      "epoch": 37.292796577801546,
      "grad_norm": 9.469913482666016,
      "learning_rate": 3.3840042777480714e-06,
      "loss": 1.6432,
      "step": 488200
    },
    {
      "epoch": 37.30043541364296,
      "grad_norm": 6.902965545654297,
      "learning_rate": 3.3744557329462993e-06,
      "loss": 1.6484,
      "step": 488300
    },
    {
      "epoch": 37.30807424948438,
      "grad_norm": 8.680594444274902,
      "learning_rate": 3.3649071881445267e-06,
      "loss": 1.7552,
      "step": 488400
    },
    {
      "epoch": 37.3157130853258,
      "grad_norm": 8.273760795593262,
      "learning_rate": 3.355358643342755e-06,
      "loss": 1.7495,
      "step": 488500
    },
    {
      "epoch": 37.323351921167216,
      "grad_norm": 6.187467575073242,
      "learning_rate": 3.3458100985409824e-06,
      "loss": 1.7491,
      "step": 488600
    },
    {
      "epoch": 37.330990757008635,
      "grad_norm": 8.849723815917969,
      "learning_rate": 3.3362615537392102e-06,
      "loss": 1.6974,
      "step": 488700
    },
    {
      "epoch": 37.33862959285005,
      "grad_norm": 8.460895538330078,
      "learning_rate": 3.3267130089374385e-06,
      "loss": 1.7615,
      "step": 488800
    },
    {
      "epoch": 37.34626842869147,
      "grad_norm": 6.920392036437988,
      "learning_rate": 3.317164464135666e-06,
      "loss": 1.7535,
      "step": 488900
    },
    {
      "epoch": 37.353907264532886,
      "grad_norm": 8.04329776763916,
      "learning_rate": 3.3076159193338938e-06,
      "loss": 1.6678,
      "step": 489000
    },
    {
      "epoch": 37.361546100374305,
      "grad_norm": 7.503138542175293,
      "learning_rate": 3.2980673745321216e-06,
      "loss": 1.7234,
      "step": 489100
    },
    {
      "epoch": 37.369184936215724,
      "grad_norm": 8.88205337524414,
      "learning_rate": 3.288518829730349e-06,
      "loss": 1.6482,
      "step": 489200
    },
    {
      "epoch": 37.376823772057136,
      "grad_norm": 6.9263200759887695,
      "learning_rate": 3.2789702849285773e-06,
      "loss": 1.6699,
      "step": 489300
    },
    {
      "epoch": 37.384462607898556,
      "grad_norm": 7.740715026855469,
      "learning_rate": 3.2694217401268047e-06,
      "loss": 1.7585,
      "step": 489400
    },
    {
      "epoch": 37.392101443739975,
      "grad_norm": 3.821493625640869,
      "learning_rate": 3.2598731953250326e-06,
      "loss": 1.6625,
      "step": 489500
    },
    {
      "epoch": 37.399740279581394,
      "grad_norm": 10.139732360839844,
      "learning_rate": 3.250324650523261e-06,
      "loss": 1.6397,
      "step": 489600
    },
    {
      "epoch": 37.407379115422806,
      "grad_norm": 7.638740062713623,
      "learning_rate": 3.240776105721488e-06,
      "loss": 1.8009,
      "step": 489700
    },
    {
      "epoch": 37.415017951264225,
      "grad_norm": 8.98145866394043,
      "learning_rate": 3.231227560919716e-06,
      "loss": 1.7254,
      "step": 489800
    },
    {
      "epoch": 37.422656787105645,
      "grad_norm": 7.696512222290039,
      "learning_rate": 3.2216790161179435e-06,
      "loss": 1.6699,
      "step": 489900
    },
    {
      "epoch": 37.430295622947064,
      "grad_norm": 8.479753494262695,
      "learning_rate": 3.2121304713161714e-06,
      "loss": 1.7272,
      "step": 490000
    },
    {
      "epoch": 37.43793445878848,
      "grad_norm": 6.185227394104004,
      "learning_rate": 3.2025819265143996e-06,
      "loss": 1.7732,
      "step": 490100
    },
    {
      "epoch": 37.445573294629895,
      "grad_norm": 8.501643180847168,
      "learning_rate": 3.193033381712627e-06,
      "loss": 1.6908,
      "step": 490200
    },
    {
      "epoch": 37.453212130471314,
      "grad_norm": 9.827361106872559,
      "learning_rate": 3.183484836910855e-06,
      "loss": 1.6838,
      "step": 490300
    },
    {
      "epoch": 37.460850966312734,
      "grad_norm": 7.634782314300537,
      "learning_rate": 3.173936292109083e-06,
      "loss": 1.7457,
      "step": 490400
    },
    {
      "epoch": 37.46848980215415,
      "grad_norm": 7.91249942779541,
      "learning_rate": 3.16438774730731e-06,
      "loss": 1.7086,
      "step": 490500
    },
    {
      "epoch": 37.47612863799557,
      "grad_norm": 8.02363109588623,
      "learning_rate": 3.1548392025055384e-06,
      "loss": 1.75,
      "step": 490600
    },
    {
      "epoch": 37.483767473836984,
      "grad_norm": 9.29882526397705,
      "learning_rate": 3.145290657703766e-06,
      "loss": 1.7349,
      "step": 490700
    },
    {
      "epoch": 37.4914063096784,
      "grad_norm": 8.161165237426758,
      "learning_rate": 3.1357421129019937e-06,
      "loss": 1.6626,
      "step": 490800
    },
    {
      "epoch": 37.49904514551982,
      "grad_norm": 9.11757755279541,
      "learning_rate": 3.126193568100222e-06,
      "loss": 1.6453,
      "step": 490900
    },
    {
      "epoch": 37.50668398136124,
      "grad_norm": 7.137901782989502,
      "learning_rate": 3.1166450232984494e-06,
      "loss": 1.7196,
      "step": 491000
    },
    {
      "epoch": 37.51432281720266,
      "grad_norm": 8.569232940673828,
      "learning_rate": 3.1070964784966773e-06,
      "loss": 1.7503,
      "step": 491100
    },
    {
      "epoch": 37.52196165304407,
      "grad_norm": 7.262247085571289,
      "learning_rate": 3.097547933694905e-06,
      "loss": 1.6725,
      "step": 491200
    },
    {
      "epoch": 37.52960048888549,
      "grad_norm": 8.701114654541016,
      "learning_rate": 3.0879993888931325e-06,
      "loss": 1.6521,
      "step": 491300
    },
    {
      "epoch": 37.53723932472691,
      "grad_norm": 6.658961296081543,
      "learning_rate": 3.078450844091361e-06,
      "loss": 1.7034,
      "step": 491400
    },
    {
      "epoch": 37.54487816056833,
      "grad_norm": 9.067111015319824,
      "learning_rate": 3.0689022992895886e-06,
      "loss": 1.7008,
      "step": 491500
    },
    {
      "epoch": 37.55251699640975,
      "grad_norm": 9.935589790344238,
      "learning_rate": 3.059353754487816e-06,
      "loss": 1.7131,
      "step": 491600
    },
    {
      "epoch": 37.56015583225116,
      "grad_norm": 10.94887638092041,
      "learning_rate": 3.049805209686044e-06,
      "loss": 1.6614,
      "step": 491700
    },
    {
      "epoch": 37.56779466809258,
      "grad_norm": 9.137449264526367,
      "learning_rate": 3.0402566648842718e-06,
      "loss": 1.7324,
      "step": 491800
    },
    {
      "epoch": 37.575433503934,
      "grad_norm": 5.707928657531738,
      "learning_rate": 3.0307081200824996e-06,
      "loss": 1.6866,
      "step": 491900
    },
    {
      "epoch": 37.58307233977542,
      "grad_norm": 6.536242961883545,
      "learning_rate": 3.0211595752807274e-06,
      "loss": 1.7641,
      "step": 492000
    },
    {
      "epoch": 37.59071117561684,
      "grad_norm": 7.694429397583008,
      "learning_rate": 3.011611030478955e-06,
      "loss": 1.7314,
      "step": 492100
    },
    {
      "epoch": 37.59835001145825,
      "grad_norm": 9.148061752319336,
      "learning_rate": 3.0020624856771827e-06,
      "loss": 1.7223,
      "step": 492200
    },
    {
      "epoch": 37.60598884729967,
      "grad_norm": 10.400238037109375,
      "learning_rate": 2.992513940875411e-06,
      "loss": 1.6732,
      "step": 492300
    },
    {
      "epoch": 37.61362768314109,
      "grad_norm": 9.40068531036377,
      "learning_rate": 2.9829653960736384e-06,
      "loss": 1.6479,
      "step": 492400
    },
    {
      "epoch": 37.62126651898251,
      "grad_norm": 7.051244735717773,
      "learning_rate": 2.9734168512718663e-06,
      "loss": 1.6613,
      "step": 492500
    },
    {
      "epoch": 37.62890535482393,
      "grad_norm": 11.652822494506836,
      "learning_rate": 2.963868306470094e-06,
      "loss": 1.6426,
      "step": 492600
    },
    {
      "epoch": 37.63654419066534,
      "grad_norm": 7.772698879241943,
      "learning_rate": 2.954319761668322e-06,
      "loss": 1.6439,
      "step": 492700
    },
    {
      "epoch": 37.64418302650676,
      "grad_norm": 7.720663070678711,
      "learning_rate": 2.94477121686655e-06,
      "loss": 1.6999,
      "step": 492800
    },
    {
      "epoch": 37.65182186234818,
      "grad_norm": 11.2998685836792,
      "learning_rate": 2.9352226720647772e-06,
      "loss": 1.6592,
      "step": 492900
    },
    {
      "epoch": 37.6594606981896,
      "grad_norm": 12.41891098022461,
      "learning_rate": 2.925674127263005e-06,
      "loss": 1.7666,
      "step": 493000
    },
    {
      "epoch": 37.66709953403102,
      "grad_norm": 9.454355239868164,
      "learning_rate": 2.9161255824612333e-06,
      "loss": 1.8148,
      "step": 493100
    },
    {
      "epoch": 37.67473836987243,
      "grad_norm": 6.036147117614746,
      "learning_rate": 2.9065770376594608e-06,
      "loss": 1.6609,
      "step": 493200
    },
    {
      "epoch": 37.68237720571385,
      "grad_norm": 6.635899066925049,
      "learning_rate": 2.8970284928576886e-06,
      "loss": 1.7808,
      "step": 493300
    },
    {
      "epoch": 37.69001604155527,
      "grad_norm": 7.405475616455078,
      "learning_rate": 2.8874799480559164e-06,
      "loss": 1.7743,
      "step": 493400
    },
    {
      "epoch": 37.69765487739669,
      "grad_norm": 7.8041791915893555,
      "learning_rate": 2.8779314032541443e-06,
      "loss": 1.8255,
      "step": 493500
    },
    {
      "epoch": 37.705293713238106,
      "grad_norm": 9.257128715515137,
      "learning_rate": 2.868382858452372e-06,
      "loss": 1.7253,
      "step": 493600
    },
    {
      "epoch": 37.71293254907952,
      "grad_norm": 8.290437698364258,
      "learning_rate": 2.8588343136505996e-06,
      "loss": 1.705,
      "step": 493700
    },
    {
      "epoch": 37.72057138492094,
      "grad_norm": 8.030271530151367,
      "learning_rate": 2.8492857688488274e-06,
      "loss": 1.6689,
      "step": 493800
    },
    {
      "epoch": 37.72821022076236,
      "grad_norm": 7.015763282775879,
      "learning_rate": 2.8397372240470557e-06,
      "loss": 1.6039,
      "step": 493900
    },
    {
      "epoch": 37.735849056603776,
      "grad_norm": 8.097901344299316,
      "learning_rate": 2.830188679245283e-06,
      "loss": 1.6663,
      "step": 494000
    },
    {
      "epoch": 37.74348789244519,
      "grad_norm": 7.437534332275391,
      "learning_rate": 2.820640134443511e-06,
      "loss": 1.7049,
      "step": 494100
    },
    {
      "epoch": 37.75112672828661,
      "grad_norm": 7.044539928436279,
      "learning_rate": 2.811091589641739e-06,
      "loss": 1.6938,
      "step": 494200
    },
    {
      "epoch": 37.758765564128026,
      "grad_norm": 6.885052680969238,
      "learning_rate": 2.8015430448399666e-06,
      "loss": 1.8173,
      "step": 494300
    },
    {
      "epoch": 37.766404399969446,
      "grad_norm": 7.688786506652832,
      "learning_rate": 2.7919945000381945e-06,
      "loss": 1.7464,
      "step": 494400
    },
    {
      "epoch": 37.774043235810865,
      "grad_norm": 6.864111423492432,
      "learning_rate": 2.782445955236422e-06,
      "loss": 1.6059,
      "step": 494500
    },
    {
      "epoch": 37.78168207165228,
      "grad_norm": 10.87602424621582,
      "learning_rate": 2.7728974104346498e-06,
      "loss": 1.7538,
      "step": 494600
    },
    {
      "epoch": 37.789320907493696,
      "grad_norm": 7.074096202850342,
      "learning_rate": 2.7633488656328776e-06,
      "loss": 1.6273,
      "step": 494700
    },
    {
      "epoch": 37.796959743335115,
      "grad_norm": 7.911192893981934,
      "learning_rate": 2.7538003208311054e-06,
      "loss": 1.7138,
      "step": 494800
    },
    {
      "epoch": 37.804598579176535,
      "grad_norm": 8.484387397766113,
      "learning_rate": 2.7442517760293333e-06,
      "loss": 1.6976,
      "step": 494900
    },
    {
      "epoch": 37.812237415017954,
      "grad_norm": 8.31277847290039,
      "learning_rate": 2.734703231227561e-06,
      "loss": 1.7407,
      "step": 495000
    },
    {
      "epoch": 37.819876250859366,
      "grad_norm": 6.893887519836426,
      "learning_rate": 2.7251546864257886e-06,
      "loss": 1.6684,
      "step": 495100
    },
    {
      "epoch": 37.827515086700785,
      "grad_norm": 8.256185531616211,
      "learning_rate": 2.715606141624017e-06,
      "loss": 1.6969,
      "step": 495200
    },
    {
      "epoch": 37.835153922542204,
      "grad_norm": 8.103343963623047,
      "learning_rate": 2.7060575968222443e-06,
      "loss": 1.7379,
      "step": 495300
    },
    {
      "epoch": 37.842792758383624,
      "grad_norm": 9.566895484924316,
      "learning_rate": 2.696509052020472e-06,
      "loss": 1.5676,
      "step": 495400
    },
    {
      "epoch": 37.85043159422504,
      "grad_norm": 7.174400806427002,
      "learning_rate": 2.6869605072187e-06,
      "loss": 1.6189,
      "step": 495500
    },
    {
      "epoch": 37.858070430066455,
      "grad_norm": 7.1376729011535645,
      "learning_rate": 2.677411962416928e-06,
      "loss": 1.6934,
      "step": 495600
    },
    {
      "epoch": 37.865709265907874,
      "grad_norm": 7.595621585845947,
      "learning_rate": 2.6678634176151556e-06,
      "loss": 1.7162,
      "step": 495700
    },
    {
      "epoch": 37.87334810174929,
      "grad_norm": 8.365471839904785,
      "learning_rate": 2.6583148728133835e-06,
      "loss": 1.6335,
      "step": 495800
    },
    {
      "epoch": 37.88098693759071,
      "grad_norm": 8.252325057983398,
      "learning_rate": 2.648766328011611e-06,
      "loss": 1.7229,
      "step": 495900
    },
    {
      "epoch": 37.88862577343213,
      "grad_norm": 6.886550426483154,
      "learning_rate": 2.639217783209839e-06,
      "loss": 1.6952,
      "step": 496000
    },
    {
      "epoch": 37.896264609273544,
      "grad_norm": 8.941803932189941,
      "learning_rate": 2.6296692384080666e-06,
      "loss": 1.7012,
      "step": 496100
    },
    {
      "epoch": 37.90390344511496,
      "grad_norm": 3.8959765434265137,
      "learning_rate": 2.6201206936062944e-06,
      "loss": 1.576,
      "step": 496200
    },
    {
      "epoch": 37.91154228095638,
      "grad_norm": 4.893716335296631,
      "learning_rate": 2.6105721488045223e-06,
      "loss": 1.7346,
      "step": 496300
    },
    {
      "epoch": 37.9191811167978,
      "grad_norm": 7.857240200042725,
      "learning_rate": 2.60102360400275e-06,
      "loss": 1.8595,
      "step": 496400
    },
    {
      "epoch": 37.92681995263922,
      "grad_norm": 10.103544235229492,
      "learning_rate": 2.591475059200978e-06,
      "loss": 1.7543,
      "step": 496500
    },
    {
      "epoch": 37.93445878848063,
      "grad_norm": 7.017517566680908,
      "learning_rate": 2.581926514399206e-06,
      "loss": 1.6511,
      "step": 496600
    },
    {
      "epoch": 37.94209762432205,
      "grad_norm": 7.093704700469971,
      "learning_rate": 2.5723779695974332e-06,
      "loss": 1.7277,
      "step": 496700
    },
    {
      "epoch": 37.94973646016347,
      "grad_norm": 7.538501262664795,
      "learning_rate": 2.5628294247956615e-06,
      "loss": 1.6214,
      "step": 496800
    },
    {
      "epoch": 37.95737529600489,
      "grad_norm": 6.256071090698242,
      "learning_rate": 2.553280879993889e-06,
      "loss": 1.6854,
      "step": 496900
    },
    {
      "epoch": 37.96501413184631,
      "grad_norm": 6.258879661560059,
      "learning_rate": 2.5437323351921168e-06,
      "loss": 1.6741,
      "step": 497000
    },
    {
      "epoch": 37.97265296768772,
      "grad_norm": 7.574193477630615,
      "learning_rate": 2.5341837903903446e-06,
      "loss": 1.7199,
      "step": 497100
    },
    {
      "epoch": 37.98029180352914,
      "grad_norm": 8.965775489807129,
      "learning_rate": 2.5246352455885725e-06,
      "loss": 1.7524,
      "step": 497200
    },
    {
      "epoch": 37.98793063937056,
      "grad_norm": 6.824267864227295,
      "learning_rate": 2.5150867007868003e-06,
      "loss": 1.5975,
      "step": 497300
    },
    {
      "epoch": 37.99556947521198,
      "grad_norm": 9.668853759765625,
      "learning_rate": 2.505538155985028e-06,
      "loss": 1.7488,
      "step": 497400
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.7923543453216553,
      "eval_runtime": 1.5581,
      "eval_samples_per_second": 442.835,
      "eval_steps_per_second": 442.835,
      "step": 497458
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.4731231927871704,
      "eval_runtime": 29.0567,
      "eval_samples_per_second": 450.532,
      "eval_steps_per_second": 450.532,
      "step": 497458
    },
    {
      "epoch": 38.0032083110534,
      "grad_norm": 6.831445693969727,
      "learning_rate": 2.4959896111832556e-06,
      "loss": 1.6914,
      "step": 497500
    },
    {
      "epoch": 38.01084714689481,
      "grad_norm": 9.983123779296875,
      "learning_rate": 2.4864410663814834e-06,
      "loss": 1.7503,
      "step": 497600
    },
    {
      "epoch": 38.01848598273623,
      "grad_norm": 7.493795394897461,
      "learning_rate": 2.4768925215797113e-06,
      "loss": 1.7403,
      "step": 497700
    },
    {
      "epoch": 38.02612481857765,
      "grad_norm": 7.197173118591309,
      "learning_rate": 2.467343976777939e-06,
      "loss": 1.6057,
      "step": 497800
    },
    {
      "epoch": 38.03376365441907,
      "grad_norm": 9.321176528930664,
      "learning_rate": 2.457795431976167e-06,
      "loss": 1.6652,
      "step": 497900
    },
    {
      "epoch": 38.04140249026048,
      "grad_norm": 14.137679100036621,
      "learning_rate": 2.4482468871743944e-06,
      "loss": 1.7644,
      "step": 498000
    },
    {
      "epoch": 38.0490413261019,
      "grad_norm": 9.179213523864746,
      "learning_rate": 2.4386983423726227e-06,
      "loss": 1.7069,
      "step": 498100
    },
    {
      "epoch": 38.05668016194332,
      "grad_norm": 6.607952117919922,
      "learning_rate": 2.4291497975708505e-06,
      "loss": 1.7335,
      "step": 498200
    },
    {
      "epoch": 38.06431899778474,
      "grad_norm": 7.433439254760742,
      "learning_rate": 2.419601252769078e-06,
      "loss": 1.6276,
      "step": 498300
    },
    {
      "epoch": 38.07195783362616,
      "grad_norm": 9.49668025970459,
      "learning_rate": 2.4100527079673058e-06,
      "loss": 1.7129,
      "step": 498400
    },
    {
      "epoch": 38.07959666946757,
      "grad_norm": 6.904598712921143,
      "learning_rate": 2.4005041631655336e-06,
      "loss": 1.7632,
      "step": 498500
    },
    {
      "epoch": 38.08723550530899,
      "grad_norm": 7.436203479766846,
      "learning_rate": 2.3909556183637615e-06,
      "loss": 1.6746,
      "step": 498600
    },
    {
      "epoch": 38.09487434115041,
      "grad_norm": 8.133389472961426,
      "learning_rate": 2.3814070735619893e-06,
      "loss": 1.7235,
      "step": 498700
    },
    {
      "epoch": 38.10251317699183,
      "grad_norm": 8.897799491882324,
      "learning_rate": 2.3718585287602167e-06,
      "loss": 1.7033,
      "step": 498800
    },
    {
      "epoch": 38.11015201283325,
      "grad_norm": 7.147581100463867,
      "learning_rate": 2.362309983958445e-06,
      "loss": 1.6899,
      "step": 498900
    },
    {
      "epoch": 38.11779084867466,
      "grad_norm": 10.621264457702637,
      "learning_rate": 2.352761439156673e-06,
      "loss": 1.7243,
      "step": 499000
    },
    {
      "epoch": 38.12542968451608,
      "grad_norm": 7.842775821685791,
      "learning_rate": 2.3432128943549003e-06,
      "loss": 1.7985,
      "step": 499100
    },
    {
      "epoch": 38.1330685203575,
      "grad_norm": 7.552323341369629,
      "learning_rate": 2.333664349553128e-06,
      "loss": 1.654,
      "step": 499200
    },
    {
      "epoch": 38.14070735619892,
      "grad_norm": 7.231265544891357,
      "learning_rate": 2.324115804751356e-06,
      "loss": 1.6484,
      "step": 499300
    },
    {
      "epoch": 38.148346192040336,
      "grad_norm": 5.49257755279541,
      "learning_rate": 2.314567259949584e-06,
      "loss": 1.6448,
      "step": 499400
    },
    {
      "epoch": 38.15598502788175,
      "grad_norm": 6.687881946563721,
      "learning_rate": 2.3050187151478117e-06,
      "loss": 1.5812,
      "step": 499500
    },
    {
      "epoch": 38.16362386372317,
      "grad_norm": 7.497600078582764,
      "learning_rate": 2.295470170346039e-06,
      "loss": 1.6382,
      "step": 499600
    },
    {
      "epoch": 38.171262699564586,
      "grad_norm": 8.157032012939453,
      "learning_rate": 2.2859216255442674e-06,
      "loss": 1.594,
      "step": 499700
    },
    {
      "epoch": 38.178901535406006,
      "grad_norm": 8.193249702453613,
      "learning_rate": 2.276373080742495e-06,
      "loss": 1.768,
      "step": 499800
    },
    {
      "epoch": 38.186540371247425,
      "grad_norm": 9.040303230285645,
      "learning_rate": 2.2668245359407226e-06,
      "loss": 1.673,
      "step": 499900
    },
    {
      "epoch": 38.19417920708884,
      "grad_norm": 9.582802772521973,
      "learning_rate": 2.2572759911389505e-06,
      "loss": 1.6735,
      "step": 500000
    },
    {
      "epoch": 38.201818042930256,
      "grad_norm": 9.496187210083008,
      "learning_rate": 2.2477274463371783e-06,
      "loss": 1.8184,
      "step": 500100
    },
    {
      "epoch": 38.209456878771675,
      "grad_norm": 7.420029640197754,
      "learning_rate": 2.238178901535406e-06,
      "loss": 1.7759,
      "step": 500200
    },
    {
      "epoch": 38.217095714613095,
      "grad_norm": 8.415616989135742,
      "learning_rate": 2.228630356733634e-06,
      "loss": 1.8223,
      "step": 500300
    },
    {
      "epoch": 38.224734550454514,
      "grad_norm": 10.499028205871582,
      "learning_rate": 2.2190818119318614e-06,
      "loss": 1.7384,
      "step": 500400
    },
    {
      "epoch": 38.232373386295926,
      "grad_norm": 8.776247024536133,
      "learning_rate": 2.2095332671300893e-06,
      "loss": 1.6952,
      "step": 500500
    },
    {
      "epoch": 38.240012222137345,
      "grad_norm": 8.851781845092773,
      "learning_rate": 2.1999847223283176e-06,
      "loss": 1.6411,
      "step": 500600
    },
    {
      "epoch": 38.247651057978764,
      "grad_norm": 8.679654121398926,
      "learning_rate": 2.190436177526545e-06,
      "loss": 1.6713,
      "step": 500700
    },
    {
      "epoch": 38.25528989382018,
      "grad_norm": 6.911263942718506,
      "learning_rate": 2.180887632724773e-06,
      "loss": 1.7972,
      "step": 500800
    },
    {
      "epoch": 38.2629287296616,
      "grad_norm": 6.529816150665283,
      "learning_rate": 2.1713390879230007e-06,
      "loss": 1.7075,
      "step": 500900
    },
    {
      "epoch": 38.270567565503015,
      "grad_norm": 8.856430053710938,
      "learning_rate": 2.1617905431212285e-06,
      "loss": 1.7981,
      "step": 501000
    },
    {
      "epoch": 38.278206401344434,
      "grad_norm": 6.549081802368164,
      "learning_rate": 2.1522419983194564e-06,
      "loss": 1.6841,
      "step": 501100
    },
    {
      "epoch": 38.28584523718585,
      "grad_norm": 8.137154579162598,
      "learning_rate": 2.1426934535176838e-06,
      "loss": 1.6819,
      "step": 501200
    },
    {
      "epoch": 38.29348407302727,
      "grad_norm": 6.559865474700928,
      "learning_rate": 2.1331449087159116e-06,
      "loss": 1.6655,
      "step": 501300
    },
    {
      "epoch": 38.30112290886869,
      "grad_norm": 8.72822380065918,
      "learning_rate": 2.12359636391414e-06,
      "loss": 1.7751,
      "step": 501400
    },
    {
      "epoch": 38.308761744710104,
      "grad_norm": 7.8680219650268555,
      "learning_rate": 2.1140478191123673e-06,
      "loss": 1.7133,
      "step": 501500
    },
    {
      "epoch": 38.31640058055152,
      "grad_norm": 7.742643356323242,
      "learning_rate": 2.104499274310595e-06,
      "loss": 1.6905,
      "step": 501600
    },
    {
      "epoch": 38.32403941639294,
      "grad_norm": 8.158849716186523,
      "learning_rate": 2.094950729508823e-06,
      "loss": 1.5386,
      "step": 501700
    },
    {
      "epoch": 38.33167825223436,
      "grad_norm": 6.425256729125977,
      "learning_rate": 2.085402184707051e-06,
      "loss": 1.6748,
      "step": 501800
    },
    {
      "epoch": 38.33931708807578,
      "grad_norm": 6.220701694488525,
      "learning_rate": 2.0758536399052787e-06,
      "loss": 1.7524,
      "step": 501900
    },
    {
      "epoch": 38.34695592391719,
      "grad_norm": 7.723898887634277,
      "learning_rate": 2.066305095103506e-06,
      "loss": 1.7263,
      "step": 502000
    },
    {
      "epoch": 38.35459475975861,
      "grad_norm": 6.003921985626221,
      "learning_rate": 2.056756550301734e-06,
      "loss": 1.7252,
      "step": 502100
    },
    {
      "epoch": 38.36223359560003,
      "grad_norm": 6.928465366363525,
      "learning_rate": 2.0472080054999622e-06,
      "loss": 1.7813,
      "step": 502200
    },
    {
      "epoch": 38.36987243144145,
      "grad_norm": 8.410594940185547,
      "learning_rate": 2.0376594606981897e-06,
      "loss": 1.7364,
      "step": 502300
    },
    {
      "epoch": 38.37751126728286,
      "grad_norm": 10.602404594421387,
      "learning_rate": 2.0281109158964175e-06,
      "loss": 1.71,
      "step": 502400
    },
    {
      "epoch": 38.38515010312428,
      "grad_norm": 9.515641212463379,
      "learning_rate": 2.0185623710946454e-06,
      "loss": 1.6835,
      "step": 502500
    },
    {
      "epoch": 38.3927889389657,
      "grad_norm": 8.299309730529785,
      "learning_rate": 2.009013826292873e-06,
      "loss": 1.744,
      "step": 502600
    },
    {
      "epoch": 38.40042777480712,
      "grad_norm": 6.249111175537109,
      "learning_rate": 1.999465281491101e-06,
      "loss": 1.649,
      "step": 502700
    },
    {
      "epoch": 38.40806661064854,
      "grad_norm": 7.131174564361572,
      "learning_rate": 1.9899167366893285e-06,
      "loss": 1.6784,
      "step": 502800
    },
    {
      "epoch": 38.41570544648995,
      "grad_norm": 9.25025749206543,
      "learning_rate": 1.9803681918875563e-06,
      "loss": 1.6379,
      "step": 502900
    },
    {
      "epoch": 38.42334428233137,
      "grad_norm": 7.561570644378662,
      "learning_rate": 1.970819647085784e-06,
      "loss": 1.7068,
      "step": 503000
    },
    {
      "epoch": 38.43098311817279,
      "grad_norm": 8.497097969055176,
      "learning_rate": 1.961271102284012e-06,
      "loss": 1.6726,
      "step": 503100
    },
    {
      "epoch": 38.43862195401421,
      "grad_norm": 9.014918327331543,
      "learning_rate": 1.95172255748224e-06,
      "loss": 1.7358,
      "step": 503200
    },
    {
      "epoch": 38.44626078985563,
      "grad_norm": 6.288944721221924,
      "learning_rate": 1.9421740126804677e-06,
      "loss": 1.6848,
      "step": 503300
    },
    {
      "epoch": 38.45389962569704,
      "grad_norm": 7.080864429473877,
      "learning_rate": 1.932625467878695e-06,
      "loss": 1.675,
      "step": 503400
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 8.492277145385742,
      "learning_rate": 1.9230769230769234e-06,
      "loss": 1.6181,
      "step": 503500
    },
    {
      "epoch": 38.46917729737988,
      "grad_norm": 6.445043563842773,
      "learning_rate": 1.913528378275151e-06,
      "loss": 1.6833,
      "step": 503600
    },
    {
      "epoch": 38.4768161332213,
      "grad_norm": 7.703553199768066,
      "learning_rate": 1.9039798334733787e-06,
      "loss": 1.6854,
      "step": 503700
    },
    {
      "epoch": 38.48445496906272,
      "grad_norm": 7.252519130706787,
      "learning_rate": 1.8944312886716063e-06,
      "loss": 1.775,
      "step": 503800
    },
    {
      "epoch": 38.49209380490413,
      "grad_norm": 5.954096794128418,
      "learning_rate": 1.8848827438698344e-06,
      "loss": 1.7073,
      "step": 503900
    },
    {
      "epoch": 38.49973264074555,
      "grad_norm": 9.121326446533203,
      "learning_rate": 1.8753341990680622e-06,
      "loss": 1.7689,
      "step": 504000
    },
    {
      "epoch": 38.50737147658697,
      "grad_norm": 11.28341293334961,
      "learning_rate": 1.8657856542662898e-06,
      "loss": 1.6957,
      "step": 504100
    },
    {
      "epoch": 38.51501031242839,
      "grad_norm": 8.294057846069336,
      "learning_rate": 1.8562371094645175e-06,
      "loss": 1.647,
      "step": 504200
    },
    {
      "epoch": 38.52264914826981,
      "grad_norm": 7.17072057723999,
      "learning_rate": 1.8466885646627455e-06,
      "loss": 1.6322,
      "step": 504300
    },
    {
      "epoch": 38.53028798411122,
      "grad_norm": 8.903419494628906,
      "learning_rate": 1.8371400198609734e-06,
      "loss": 1.6377,
      "step": 504400
    },
    {
      "epoch": 38.53792681995264,
      "grad_norm": 9.244599342346191,
      "learning_rate": 1.827591475059201e-06,
      "loss": 1.7449,
      "step": 504500
    },
    {
      "epoch": 38.54556565579406,
      "grad_norm": 7.704828262329102,
      "learning_rate": 1.8180429302574286e-06,
      "loss": 1.6942,
      "step": 504600
    },
    {
      "epoch": 38.553204491635476,
      "grad_norm": 8.213262557983398,
      "learning_rate": 1.8084943854556567e-06,
      "loss": 1.699,
      "step": 504700
    },
    {
      "epoch": 38.560843327476896,
      "grad_norm": 6.647755146026611,
      "learning_rate": 1.7989458406538845e-06,
      "loss": 1.6249,
      "step": 504800
    },
    {
      "epoch": 38.56848216331831,
      "grad_norm": 7.233497619628906,
      "learning_rate": 1.7893972958521122e-06,
      "loss": 1.6609,
      "step": 504900
    },
    {
      "epoch": 38.57612099915973,
      "grad_norm": 9.134361267089844,
      "learning_rate": 1.7798487510503398e-06,
      "loss": 1.6907,
      "step": 505000
    },
    {
      "epoch": 38.583759835001146,
      "grad_norm": 8.388372421264648,
      "learning_rate": 1.7703002062485679e-06,
      "loss": 1.6994,
      "step": 505100
    },
    {
      "epoch": 38.591398670842565,
      "grad_norm": 7.426808834075928,
      "learning_rate": 1.7607516614467957e-06,
      "loss": 1.7079,
      "step": 505200
    },
    {
      "epoch": 38.599037506683985,
      "grad_norm": 7.149953842163086,
      "learning_rate": 1.7512031166450234e-06,
      "loss": 1.7275,
      "step": 505300
    },
    {
      "epoch": 38.6066763425254,
      "grad_norm": 7.8475565910339355,
      "learning_rate": 1.741654571843251e-06,
      "loss": 1.6585,
      "step": 505400
    },
    {
      "epoch": 38.614315178366816,
      "grad_norm": 7.524656772613525,
      "learning_rate": 1.732106027041479e-06,
      "loss": 1.7192,
      "step": 505500
    },
    {
      "epoch": 38.621954014208235,
      "grad_norm": 6.457558631896973,
      "learning_rate": 1.722557482239707e-06,
      "loss": 1.7393,
      "step": 505600
    },
    {
      "epoch": 38.629592850049654,
      "grad_norm": 8.93562126159668,
      "learning_rate": 1.7130089374379345e-06,
      "loss": 1.75,
      "step": 505700
    },
    {
      "epoch": 38.637231685891074,
      "grad_norm": 8.183226585388184,
      "learning_rate": 1.7034603926361622e-06,
      "loss": 1.7204,
      "step": 505800
    },
    {
      "epoch": 38.644870521732486,
      "grad_norm": 6.319390296936035,
      "learning_rate": 1.69391184783439e-06,
      "loss": 1.6603,
      "step": 505900
    },
    {
      "epoch": 38.652509357573905,
      "grad_norm": 7.9699249267578125,
      "learning_rate": 1.684363303032618e-06,
      "loss": 1.7626,
      "step": 506000
    },
    {
      "epoch": 38.660148193415324,
      "grad_norm": 12.644378662109375,
      "learning_rate": 1.6748147582308457e-06,
      "loss": 1.7674,
      "step": 506100
    },
    {
      "epoch": 38.66778702925674,
      "grad_norm": 7.742186069488525,
      "learning_rate": 1.6652662134290733e-06,
      "loss": 1.7412,
      "step": 506200
    },
    {
      "epoch": 38.67542586509816,
      "grad_norm": 8.263651847839355,
      "learning_rate": 1.6557176686273012e-06,
      "loss": 1.6947,
      "step": 506300
    },
    {
      "epoch": 38.683064700939575,
      "grad_norm": 7.962770462036133,
      "learning_rate": 1.6461691238255292e-06,
      "loss": 1.6632,
      "step": 506400
    },
    {
      "epoch": 38.690703536780994,
      "grad_norm": 4.1552839279174805,
      "learning_rate": 1.6366205790237569e-06,
      "loss": 1.7521,
      "step": 506500
    },
    {
      "epoch": 38.69834237262241,
      "grad_norm": 5.757291793823242,
      "learning_rate": 1.6270720342219845e-06,
      "loss": 1.6795,
      "step": 506600
    },
    {
      "epoch": 38.70598120846383,
      "grad_norm": 6.837121486663818,
      "learning_rate": 1.6175234894202124e-06,
      "loss": 1.6761,
      "step": 506700
    },
    {
      "epoch": 38.713620044305245,
      "grad_norm": 9.154796600341797,
      "learning_rate": 1.6079749446184404e-06,
      "loss": 1.7451,
      "step": 506800
    },
    {
      "epoch": 38.721258880146664,
      "grad_norm": 9.89754581451416,
      "learning_rate": 1.598426399816668e-06,
      "loss": 1.7651,
      "step": 506900
    },
    {
      "epoch": 38.72889771598808,
      "grad_norm": 10.161665916442871,
      "learning_rate": 1.5888778550148957e-06,
      "loss": 1.689,
      "step": 507000
    },
    {
      "epoch": 38.7365365518295,
      "grad_norm": 8.268214225769043,
      "learning_rate": 1.5793293102131235e-06,
      "loss": 1.7003,
      "step": 507100
    },
    {
      "epoch": 38.74417538767092,
      "grad_norm": 4.97770357131958,
      "learning_rate": 1.5697807654113516e-06,
      "loss": 1.7519,
      "step": 507200
    },
    {
      "epoch": 38.75181422351233,
      "grad_norm": 8.604930877685547,
      "learning_rate": 1.5602322206095792e-06,
      "loss": 1.7343,
      "step": 507300
    },
    {
      "epoch": 38.75945305935375,
      "grad_norm": 6.829016208648682,
      "learning_rate": 1.5506836758078069e-06,
      "loss": 1.6833,
      "step": 507400
    },
    {
      "epoch": 38.76709189519517,
      "grad_norm": 8.279535293579102,
      "learning_rate": 1.541135131006035e-06,
      "loss": 1.803,
      "step": 507500
    },
    {
      "epoch": 38.77473073103659,
      "grad_norm": 6.666513919830322,
      "learning_rate": 1.5315865862042625e-06,
      "loss": 1.7894,
      "step": 507600
    },
    {
      "epoch": 38.78236956687801,
      "grad_norm": 8.351776123046875,
      "learning_rate": 1.5220380414024904e-06,
      "loss": 1.6558,
      "step": 507700
    },
    {
      "epoch": 38.79000840271942,
      "grad_norm": 8.450675010681152,
      "learning_rate": 1.512489496600718e-06,
      "loss": 1.8173,
      "step": 507800
    },
    {
      "epoch": 38.79764723856084,
      "grad_norm": 3.4309542179107666,
      "learning_rate": 1.502940951798946e-06,
      "loss": 1.7497,
      "step": 507900
    },
    {
      "epoch": 38.80528607440226,
      "grad_norm": 8.602299690246582,
      "learning_rate": 1.4933924069971737e-06,
      "loss": 1.6779,
      "step": 508000
    },
    {
      "epoch": 38.81292491024368,
      "grad_norm": 7.234574317932129,
      "learning_rate": 1.4838438621954016e-06,
      "loss": 1.6565,
      "step": 508100
    },
    {
      "epoch": 38.8205637460851,
      "grad_norm": 9.50208568572998,
      "learning_rate": 1.4742953173936292e-06,
      "loss": 1.6888,
      "step": 508200
    },
    {
      "epoch": 38.82820258192651,
      "grad_norm": 5.219991207122803,
      "learning_rate": 1.4647467725918573e-06,
      "loss": 1.6965,
      "step": 508300
    },
    {
      "epoch": 38.83584141776793,
      "grad_norm": 8.364800453186035,
      "learning_rate": 1.4551982277900849e-06,
      "loss": 1.673,
      "step": 508400
    },
    {
      "epoch": 38.84348025360935,
      "grad_norm": 9.398029327392578,
      "learning_rate": 1.4456496829883125e-06,
      "loss": 1.7366,
      "step": 508500
    },
    {
      "epoch": 38.85111908945077,
      "grad_norm": 8.752129554748535,
      "learning_rate": 1.4361011381865404e-06,
      "loss": 1.7328,
      "step": 508600
    },
    {
      "epoch": 38.85875792529219,
      "grad_norm": 10.573856353759766,
      "learning_rate": 1.4265525933847682e-06,
      "loss": 1.7265,
      "step": 508700
    },
    {
      "epoch": 38.8663967611336,
      "grad_norm": 8.359094619750977,
      "learning_rate": 1.417004048582996e-06,
      "loss": 1.7036,
      "step": 508800
    },
    {
      "epoch": 38.87403559697502,
      "grad_norm": 8.97648811340332,
      "learning_rate": 1.4074555037812237e-06,
      "loss": 1.67,
      "step": 508900
    },
    {
      "epoch": 38.88167443281644,
      "grad_norm": 6.3156514167785645,
      "learning_rate": 1.3979069589794515e-06,
      "loss": 1.6381,
      "step": 509000
    },
    {
      "epoch": 38.88931326865786,
      "grad_norm": 9.306825637817383,
      "learning_rate": 1.3883584141776794e-06,
      "loss": 1.7079,
      "step": 509100
    },
    {
      "epoch": 38.89695210449928,
      "grad_norm": 8.281187057495117,
      "learning_rate": 1.3788098693759072e-06,
      "loss": 1.7539,
      "step": 509200
    },
    {
      "epoch": 38.90459094034069,
      "grad_norm": 8.88880443572998,
      "learning_rate": 1.3692613245741349e-06,
      "loss": 1.6427,
      "step": 509300
    },
    {
      "epoch": 38.91222977618211,
      "grad_norm": 11.908183097839355,
      "learning_rate": 1.3597127797723627e-06,
      "loss": 1.6929,
      "step": 509400
    },
    {
      "epoch": 38.91986861202353,
      "grad_norm": 10.107247352600098,
      "learning_rate": 1.3501642349705906e-06,
      "loss": 1.7667,
      "step": 509500
    },
    {
      "epoch": 38.92750744786495,
      "grad_norm": 7.765848636627197,
      "learning_rate": 1.3406156901688184e-06,
      "loss": 1.6379,
      "step": 509600
    },
    {
      "epoch": 38.935146283706366,
      "grad_norm": 9.108724594116211,
      "learning_rate": 1.331067145367046e-06,
      "loss": 1.6772,
      "step": 509700
    },
    {
      "epoch": 38.94278511954778,
      "grad_norm": 6.959110260009766,
      "learning_rate": 1.3215186005652739e-06,
      "loss": 1.6473,
      "step": 509800
    },
    {
      "epoch": 38.9504239553892,
      "grad_norm": 9.303067207336426,
      "learning_rate": 1.3119700557635017e-06,
      "loss": 1.6285,
      "step": 509900
    },
    {
      "epoch": 38.95806279123062,
      "grad_norm": 6.2354302406311035,
      "learning_rate": 1.3024215109617296e-06,
      "loss": 1.6834,
      "step": 510000
    },
    {
      "epoch": 38.965701627072036,
      "grad_norm": 7.562057971954346,
      "learning_rate": 1.2928729661599572e-06,
      "loss": 1.7305,
      "step": 510100
    },
    {
      "epoch": 38.973340462913455,
      "grad_norm": 11.590299606323242,
      "learning_rate": 1.283324421358185e-06,
      "loss": 1.7616,
      "step": 510200
    },
    {
      "epoch": 38.98097929875487,
      "grad_norm": 7.658497333526611,
      "learning_rate": 1.273775876556413e-06,
      "loss": 1.6268,
      "step": 510300
    },
    {
      "epoch": 38.98861813459629,
      "grad_norm": 8.494640350341797,
      "learning_rate": 1.2642273317546408e-06,
      "loss": 1.6588,
      "step": 510400
    },
    {
      "epoch": 38.996256970437706,
      "grad_norm": 7.271934509277344,
      "learning_rate": 1.2546787869528684e-06,
      "loss": 1.7471,
      "step": 510500
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.7902768850326538,
      "eval_runtime": 3.0006,
      "eval_samples_per_second": 229.957,
      "eval_steps_per_second": 229.957,
      "step": 510549
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.4709571599960327,
      "eval_runtime": 56.7947,
      "eval_samples_per_second": 230.497,
      "eval_steps_per_second": 230.497,
      "step": 510549
    },
    {
      "epoch": 39.003895806279125,
      "grad_norm": 8.131792068481445,
      "learning_rate": 1.2451302421510962e-06,
      "loss": 1.6884,
      "step": 510600
    },
    {
      "epoch": 39.01153464212054,
      "grad_norm": 8.059809684753418,
      "learning_rate": 1.235581697349324e-06,
      "loss": 1.694,
      "step": 510700
    },
    {
      "epoch": 39.01917347796196,
      "grad_norm": 9.056391716003418,
      "learning_rate": 1.226033152547552e-06,
      "loss": 1.694,
      "step": 510800
    },
    {
      "epoch": 39.026812313803376,
      "grad_norm": 8.21518611907959,
      "learning_rate": 1.2164846077457796e-06,
      "loss": 1.6776,
      "step": 510900
    },
    {
      "epoch": 39.034451149644795,
      "grad_norm": 6.909281253814697,
      "learning_rate": 1.2069360629440074e-06,
      "loss": 1.5985,
      "step": 511000
    },
    {
      "epoch": 39.042089985486214,
      "grad_norm": 10.524345397949219,
      "learning_rate": 1.1973875181422353e-06,
      "loss": 1.7126,
      "step": 511100
    },
    {
      "epoch": 39.049728821327626,
      "grad_norm": 7.289944648742676,
      "learning_rate": 1.1878389733404629e-06,
      "loss": 1.6131,
      "step": 511200
    },
    {
      "epoch": 39.057367657169046,
      "grad_norm": 8.483798027038574,
      "learning_rate": 1.1782904285386907e-06,
      "loss": 1.672,
      "step": 511300
    },
    {
      "epoch": 39.065006493010465,
      "grad_norm": 7.578226089477539,
      "learning_rate": 1.1687418837369184e-06,
      "loss": 1.7764,
      "step": 511400
    },
    {
      "epoch": 39.072645328851884,
      "grad_norm": 9.903704643249512,
      "learning_rate": 1.1591933389351464e-06,
      "loss": 1.7355,
      "step": 511500
    },
    {
      "epoch": 39.0802841646933,
      "grad_norm": 7.69304084777832,
      "learning_rate": 1.149644794133374e-06,
      "loss": 1.6716,
      "step": 511600
    },
    {
      "epoch": 39.087923000534715,
      "grad_norm": 6.711968898773193,
      "learning_rate": 1.140096249331602e-06,
      "loss": 1.6983,
      "step": 511700
    },
    {
      "epoch": 39.095561836376135,
      "grad_norm": 9.802549362182617,
      "learning_rate": 1.1305477045298295e-06,
      "loss": 1.7234,
      "step": 511800
    },
    {
      "epoch": 39.103200672217554,
      "grad_norm": 5.914157867431641,
      "learning_rate": 1.1209991597280576e-06,
      "loss": 1.7598,
      "step": 511900
    },
    {
      "epoch": 39.11083950805897,
      "grad_norm": 6.6261773109436035,
      "learning_rate": 1.1114506149262852e-06,
      "loss": 1.57,
      "step": 512000
    },
    {
      "epoch": 39.11847834390039,
      "grad_norm": 8.320330619812012,
      "learning_rate": 1.101902070124513e-06,
      "loss": 1.7325,
      "step": 512100
    },
    {
      "epoch": 39.126117179741804,
      "grad_norm": 7.378388404846191,
      "learning_rate": 1.0923535253227407e-06,
      "loss": 1.7916,
      "step": 512200
    },
    {
      "epoch": 39.133756015583224,
      "grad_norm": 11.301615715026855,
      "learning_rate": 1.0828049805209688e-06,
      "loss": 1.6985,
      "step": 512300
    },
    {
      "epoch": 39.14139485142464,
      "grad_norm": 13.91987133026123,
      "learning_rate": 1.0732564357191964e-06,
      "loss": 1.7406,
      "step": 512400
    },
    {
      "epoch": 39.14903368726606,
      "grad_norm": 9.751972198486328,
      "learning_rate": 1.0637078909174243e-06,
      "loss": 1.8077,
      "step": 512500
    },
    {
      "epoch": 39.15667252310748,
      "grad_norm": 10.44469165802002,
      "learning_rate": 1.0541593461156519e-06,
      "loss": 1.7248,
      "step": 512600
    },
    {
      "epoch": 39.16431135894889,
      "grad_norm": 5.973923206329346,
      "learning_rate": 1.04461080131388e-06,
      "loss": 1.6096,
      "step": 512700
    },
    {
      "epoch": 39.17195019479031,
      "grad_norm": 7.068875312805176,
      "learning_rate": 1.0350622565121076e-06,
      "loss": 1.7378,
      "step": 512800
    },
    {
      "epoch": 39.17958903063173,
      "grad_norm": 9.6737699508667,
      "learning_rate": 1.0255137117103354e-06,
      "loss": 1.7132,
      "step": 512900
    },
    {
      "epoch": 39.18722786647315,
      "grad_norm": 8.764172554016113,
      "learning_rate": 1.015965166908563e-06,
      "loss": 1.6616,
      "step": 513000
    },
    {
      "epoch": 39.19486670231457,
      "grad_norm": 6.731708526611328,
      "learning_rate": 1.0064166221067911e-06,
      "loss": 1.7298,
      "step": 513100
    },
    {
      "epoch": 39.20250553815598,
      "grad_norm": 5.425012588500977,
      "learning_rate": 9.968680773050188e-07,
      "loss": 1.6629,
      "step": 513200
    },
    {
      "epoch": 39.2101443739974,
      "grad_norm": 6.539101600646973,
      "learning_rate": 9.873195325032466e-07,
      "loss": 1.7278,
      "step": 513300
    },
    {
      "epoch": 39.21778320983882,
      "grad_norm": 5.916723728179932,
      "learning_rate": 9.777709877014742e-07,
      "loss": 1.6628,
      "step": 513400
    },
    {
      "epoch": 39.22542204568024,
      "grad_norm": 9.8322172164917,
      "learning_rate": 9.682224428997023e-07,
      "loss": 1.6805,
      "step": 513500
    },
    {
      "epoch": 39.23306088152166,
      "grad_norm": 9.884664535522461,
      "learning_rate": 9.5867389809793e-07,
      "loss": 1.6867,
      "step": 513600
    },
    {
      "epoch": 39.24069971736307,
      "grad_norm": 8.611526489257812,
      "learning_rate": 9.491253532961578e-07,
      "loss": 1.7201,
      "step": 513700
    },
    {
      "epoch": 39.24833855320449,
      "grad_norm": 7.763745307922363,
      "learning_rate": 9.395768084943855e-07,
      "loss": 1.678,
      "step": 513800
    },
    {
      "epoch": 39.25597738904591,
      "grad_norm": 8.427167892456055,
      "learning_rate": 9.300282636926133e-07,
      "loss": 1.6562,
      "step": 513900
    },
    {
      "epoch": 39.26361622488733,
      "grad_norm": 6.757754802703857,
      "learning_rate": 9.204797188908411e-07,
      "loss": 1.7477,
      "step": 514000
    },
    {
      "epoch": 39.27125506072875,
      "grad_norm": 9.111894607543945,
      "learning_rate": 9.109311740890688e-07,
      "loss": 1.7258,
      "step": 514100
    },
    {
      "epoch": 39.27889389657016,
      "grad_norm": 9.009037017822266,
      "learning_rate": 9.013826292872967e-07,
      "loss": 1.7113,
      "step": 514200
    },
    {
      "epoch": 39.28653273241158,
      "grad_norm": 8.82954216003418,
      "learning_rate": 8.918340844855244e-07,
      "loss": 1.6276,
      "step": 514300
    },
    {
      "epoch": 39.294171568253,
      "grad_norm": 8.94398021697998,
      "learning_rate": 8.822855396837523e-07,
      "loss": 1.797,
      "step": 514400
    },
    {
      "epoch": 39.30181040409442,
      "grad_norm": 9.699676513671875,
      "learning_rate": 8.7273699488198e-07,
      "loss": 1.6708,
      "step": 514500
    },
    {
      "epoch": 39.30944923993584,
      "grad_norm": 8.974857330322266,
      "learning_rate": 8.631884500802079e-07,
      "loss": 1.7689,
      "step": 514600
    },
    {
      "epoch": 39.31708807577725,
      "grad_norm": 6.904298305511475,
      "learning_rate": 8.536399052784356e-07,
      "loss": 1.7128,
      "step": 514700
    },
    {
      "epoch": 39.32472691161867,
      "grad_norm": 8.04434871673584,
      "learning_rate": 8.440913604766634e-07,
      "loss": 1.7653,
      "step": 514800
    },
    {
      "epoch": 39.33236574746009,
      "grad_norm": 8.935805320739746,
      "learning_rate": 8.345428156748912e-07,
      "loss": 1.7194,
      "step": 514900
    },
    {
      "epoch": 39.34000458330151,
      "grad_norm": 6.828797817230225,
      "learning_rate": 8.24994270873119e-07,
      "loss": 1.729,
      "step": 515000
    },
    {
      "epoch": 39.34764341914292,
      "grad_norm": 7.639486312866211,
      "learning_rate": 8.154457260713468e-07,
      "loss": 1.6742,
      "step": 515100
    },
    {
      "epoch": 39.35528225498434,
      "grad_norm": 6.863253593444824,
      "learning_rate": 8.058971812695746e-07,
      "loss": 1.6956,
      "step": 515200
    },
    {
      "epoch": 39.36292109082576,
      "grad_norm": 6.342118263244629,
      "learning_rate": 7.963486364678024e-07,
      "loss": 1.6146,
      "step": 515300
    },
    {
      "epoch": 39.37055992666718,
      "grad_norm": 7.786365509033203,
      "learning_rate": 7.868000916660302e-07,
      "loss": 1.7313,
      "step": 515400
    },
    {
      "epoch": 39.378198762508596,
      "grad_norm": 6.569422245025635,
      "learning_rate": 7.772515468642579e-07,
      "loss": 1.6977,
      "step": 515500
    },
    {
      "epoch": 39.38583759835001,
      "grad_norm": 7.849484443664551,
      "learning_rate": 7.677030020624857e-07,
      "loss": 1.7335,
      "step": 515600
    },
    {
      "epoch": 39.39347643419143,
      "grad_norm": 9.415453910827637,
      "learning_rate": 7.581544572607135e-07,
      "loss": 1.7098,
      "step": 515700
    },
    {
      "epoch": 39.40111527003285,
      "grad_norm": 8.128416061401367,
      "learning_rate": 7.486059124589413e-07,
      "loss": 1.6786,
      "step": 515800
    },
    {
      "epoch": 39.408754105874266,
      "grad_norm": 8.523662567138672,
      "learning_rate": 7.390573676571691e-07,
      "loss": 1.6497,
      "step": 515900
    },
    {
      "epoch": 39.416392941715685,
      "grad_norm": 6.886565208435059,
      "learning_rate": 7.295088228553969e-07,
      "loss": 1.6837,
      "step": 516000
    },
    {
      "epoch": 39.4240317775571,
      "grad_norm": 7.576806545257568,
      "learning_rate": 7.199602780536246e-07,
      "loss": 1.7018,
      "step": 516100
    },
    {
      "epoch": 39.431670613398516,
      "grad_norm": 9.831673622131348,
      "learning_rate": 7.104117332518524e-07,
      "loss": 1.6708,
      "step": 516200
    },
    {
      "epoch": 39.439309449239936,
      "grad_norm": 8.818913459777832,
      "learning_rate": 7.008631884500802e-07,
      "loss": 1.6464,
      "step": 516300
    },
    {
      "epoch": 39.446948285081355,
      "grad_norm": 10.019186973571777,
      "learning_rate": 6.91314643648308e-07,
      "loss": 1.7156,
      "step": 516400
    },
    {
      "epoch": 39.454587120922774,
      "grad_norm": 10.384149551391602,
      "learning_rate": 6.817660988465358e-07,
      "loss": 1.6334,
      "step": 516500
    },
    {
      "epoch": 39.462225956764186,
      "grad_norm": 7.68766975402832,
      "learning_rate": 6.722175540447636e-07,
      "loss": 1.6605,
      "step": 516600
    },
    {
      "epoch": 39.469864792605605,
      "grad_norm": 6.278019905090332,
      "learning_rate": 6.626690092429914e-07,
      "loss": 1.7081,
      "step": 516700
    },
    {
      "epoch": 39.477503628447025,
      "grad_norm": 8.598372459411621,
      "learning_rate": 6.531204644412192e-07,
      "loss": 1.7057,
      "step": 516800
    },
    {
      "epoch": 39.485142464288444,
      "grad_norm": 9.940869331359863,
      "learning_rate": 6.435719196394469e-07,
      "loss": 1.6085,
      "step": 516900
    },
    {
      "epoch": 39.49278130012986,
      "grad_norm": 7.132571220397949,
      "learning_rate": 6.340233748376748e-07,
      "loss": 1.5571,
      "step": 517000
    },
    {
      "epoch": 39.500420135971275,
      "grad_norm": 8.414863586425781,
      "learning_rate": 6.244748300359025e-07,
      "loss": 1.7312,
      "step": 517100
    },
    {
      "epoch": 39.508058971812694,
      "grad_norm": 7.650960922241211,
      "learning_rate": 6.149262852341304e-07,
      "loss": 1.776,
      "step": 517200
    },
    {
      "epoch": 39.515697807654114,
      "grad_norm": 10.227174758911133,
      "learning_rate": 6.053777404323581e-07,
      "loss": 1.7248,
      "step": 517300
    },
    {
      "epoch": 39.52333664349553,
      "grad_norm": 11.041851997375488,
      "learning_rate": 5.95829195630586e-07,
      "loss": 1.746,
      "step": 517400
    },
    {
      "epoch": 39.53097547933695,
      "grad_norm": 8.408859252929688,
      "learning_rate": 5.862806508288137e-07,
      "loss": 1.6755,
      "step": 517500
    },
    {
      "epoch": 39.538614315178364,
      "grad_norm": 7.832177639007568,
      "learning_rate": 5.767321060270415e-07,
      "loss": 1.6676,
      "step": 517600
    },
    {
      "epoch": 39.54625315101978,
      "grad_norm": 6.50880241394043,
      "learning_rate": 5.671835612252693e-07,
      "loss": 1.6897,
      "step": 517700
    },
    {
      "epoch": 39.5538919868612,
      "grad_norm": 9.386752128601074,
      "learning_rate": 5.576350164234971e-07,
      "loss": 1.787,
      "step": 517800
    },
    {
      "epoch": 39.56153082270262,
      "grad_norm": 7.7343010902404785,
      "learning_rate": 5.480864716217249e-07,
      "loss": 1.6648,
      "step": 517900
    },
    {
      "epoch": 39.56916965854404,
      "grad_norm": 7.756143569946289,
      "learning_rate": 5.385379268199526e-07,
      "loss": 1.688,
      "step": 518000
    },
    {
      "epoch": 39.57680849438545,
      "grad_norm": 4.573485851287842,
      "learning_rate": 5.289893820181805e-07,
      "loss": 1.6045,
      "step": 518100
    },
    {
      "epoch": 39.58444733022687,
      "grad_norm": 7.700098514556885,
      "learning_rate": 5.194408372164082e-07,
      "loss": 1.628,
      "step": 518200
    },
    {
      "epoch": 39.59208616606829,
      "grad_norm": 7.409614086151123,
      "learning_rate": 5.09892292414636e-07,
      "loss": 1.74,
      "step": 518300
    },
    {
      "epoch": 39.59972500190971,
      "grad_norm": 6.565759181976318,
      "learning_rate": 5.003437476128638e-07,
      "loss": 1.7462,
      "step": 518400
    },
    {
      "epoch": 39.60736383775113,
      "grad_norm": 6.038341522216797,
      "learning_rate": 4.907952028110916e-07,
      "loss": 1.6863,
      "step": 518500
    },
    {
      "epoch": 39.61500267359254,
      "grad_norm": 10.605080604553223,
      "learning_rate": 4.812466580093194e-07,
      "loss": 1.7263,
      "step": 518600
    },
    {
      "epoch": 39.62264150943396,
      "grad_norm": 7.219996452331543,
      "learning_rate": 4.7169811320754717e-07,
      "loss": 1.7855,
      "step": 518700
    },
    {
      "epoch": 39.63028034527538,
      "grad_norm": 11.166370391845703,
      "learning_rate": 4.6214956840577496e-07,
      "loss": 1.7595,
      "step": 518800
    },
    {
      "epoch": 39.6379191811168,
      "grad_norm": 8.689704895019531,
      "learning_rate": 4.5260102360400275e-07,
      "loss": 1.6955,
      "step": 518900
    },
    {
      "epoch": 39.64555801695822,
      "grad_norm": 5.355472087860107,
      "learning_rate": 4.4305247880223054e-07,
      "loss": 1.7136,
      "step": 519000
    },
    {
      "epoch": 39.65319685279963,
      "grad_norm": 6.885904312133789,
      "learning_rate": 4.3350393400045834e-07,
      "loss": 1.6561,
      "step": 519100
    },
    {
      "epoch": 39.66083568864105,
      "grad_norm": 8.281734466552734,
      "learning_rate": 4.2395538919868613e-07,
      "loss": 1.6129,
      "step": 519200
    },
    {
      "epoch": 39.66847452448247,
      "grad_norm": 7.357802867889404,
      "learning_rate": 4.144068443969139e-07,
      "loss": 1.74,
      "step": 519300
    },
    {
      "epoch": 39.67611336032389,
      "grad_norm": 6.878678798675537,
      "learning_rate": 4.048582995951417e-07,
      "loss": 1.6172,
      "step": 519400
    },
    {
      "epoch": 39.6837521961653,
      "grad_norm": 7.43673038482666,
      "learning_rate": 3.953097547933695e-07,
      "loss": 1.7409,
      "step": 519500
    },
    {
      "epoch": 39.69139103200672,
      "grad_norm": 8.532906532287598,
      "learning_rate": 3.857612099915973e-07,
      "loss": 1.7682,
      "step": 519600
    },
    {
      "epoch": 39.69902986784814,
      "grad_norm": 7.577236652374268,
      "learning_rate": 3.762126651898251e-07,
      "loss": 1.739,
      "step": 519700
    },
    {
      "epoch": 39.70666870368956,
      "grad_norm": 6.4519944190979,
      "learning_rate": 3.666641203880529e-07,
      "loss": 1.7231,
      "step": 519800
    },
    {
      "epoch": 39.71430753953098,
      "grad_norm": 7.212090969085693,
      "learning_rate": 3.571155755862807e-07,
      "loss": 1.5499,
      "step": 519900
    },
    {
      "epoch": 39.72194637537239,
      "grad_norm": 9.521596908569336,
      "learning_rate": 3.475670307845084e-07,
      "loss": 1.6874,
      "step": 520000
    },
    {
      "epoch": 39.72958521121381,
      "grad_norm": 7.048783302307129,
      "learning_rate": 3.380184859827362e-07,
      "loss": 1.6911,
      "step": 520100
    },
    {
      "epoch": 39.73722404705523,
      "grad_norm": 7.88380765914917,
      "learning_rate": 3.28469941180964e-07,
      "loss": 1.6376,
      "step": 520200
    },
    {
      "epoch": 39.74486288289665,
      "grad_norm": 9.971758842468262,
      "learning_rate": 3.189213963791918e-07,
      "loss": 1.7179,
      "step": 520300
    },
    {
      "epoch": 39.75250171873807,
      "grad_norm": 8.277036666870117,
      "learning_rate": 3.093728515774196e-07,
      "loss": 1.7747,
      "step": 520400
    },
    {
      "epoch": 39.76014055457948,
      "grad_norm": 5.970338344573975,
      "learning_rate": 2.998243067756474e-07,
      "loss": 1.6258,
      "step": 520500
    },
    {
      "epoch": 39.7677793904209,
      "grad_norm": 6.895204067230225,
      "learning_rate": 2.902757619738752e-07,
      "loss": 1.6901,
      "step": 520600
    },
    {
      "epoch": 39.77541822626232,
      "grad_norm": 8.98222827911377,
      "learning_rate": 2.80727217172103e-07,
      "loss": 1.6429,
      "step": 520700
    },
    {
      "epoch": 39.78305706210374,
      "grad_norm": 7.018857479095459,
      "learning_rate": 2.7117867237033077e-07,
      "loss": 1.6857,
      "step": 520800
    },
    {
      "epoch": 39.790695897945156,
      "grad_norm": 6.874022960662842,
      "learning_rate": 2.6163012756855856e-07,
      "loss": 1.76,
      "step": 520900
    },
    {
      "epoch": 39.79833473378657,
      "grad_norm": 6.724330425262451,
      "learning_rate": 2.5208158276678636e-07,
      "loss": 1.6502,
      "step": 521000
    },
    {
      "epoch": 39.80597356962799,
      "grad_norm": 7.574769020080566,
      "learning_rate": 2.4253303796501415e-07,
      "loss": 1.6841,
      "step": 521100
    },
    {
      "epoch": 39.81361240546941,
      "grad_norm": 7.9737162590026855,
      "learning_rate": 2.3298449316324194e-07,
      "loss": 1.6699,
      "step": 521200
    },
    {
      "epoch": 39.821251241310826,
      "grad_norm": 9.945906639099121,
      "learning_rate": 2.2343594836146974e-07,
      "loss": 1.6922,
      "step": 521300
    },
    {
      "epoch": 39.828890077152245,
      "grad_norm": 5.952361106872559,
      "learning_rate": 2.138874035596975e-07,
      "loss": 1.6921,
      "step": 521400
    },
    {
      "epoch": 39.83652891299366,
      "grad_norm": 10.09557056427002,
      "learning_rate": 2.043388587579253e-07,
      "loss": 1.67,
      "step": 521500
    },
    {
      "epoch": 39.844167748835076,
      "grad_norm": 8.408365249633789,
      "learning_rate": 1.947903139561531e-07,
      "loss": 1.8163,
      "step": 521600
    },
    {
      "epoch": 39.851806584676496,
      "grad_norm": 5.060699462890625,
      "learning_rate": 1.8524176915438088e-07,
      "loss": 1.6321,
      "step": 521700
    },
    {
      "epoch": 39.859445420517915,
      "grad_norm": 7.667184352874756,
      "learning_rate": 1.7569322435260867e-07,
      "loss": 1.717,
      "step": 521800
    },
    {
      "epoch": 39.867084256359334,
      "grad_norm": 8.075865745544434,
      "learning_rate": 1.6614467955083647e-07,
      "loss": 1.7198,
      "step": 521900
    },
    {
      "epoch": 39.874723092200746,
      "grad_norm": 7.857492446899414,
      "learning_rate": 1.5659613474906426e-07,
      "loss": 1.7324,
      "step": 522000
    },
    {
      "epoch": 39.882361928042165,
      "grad_norm": 6.768502712249756,
      "learning_rate": 1.4704758994729205e-07,
      "loss": 1.8093,
      "step": 522100
    },
    {
      "epoch": 39.890000763883585,
      "grad_norm": 6.439661502838135,
      "learning_rate": 1.3749904514551985e-07,
      "loss": 1.7382,
      "step": 522200
    },
    {
      "epoch": 39.897639599725004,
      "grad_norm": 6.879478454589844,
      "learning_rate": 1.2795050034374764e-07,
      "loss": 1.7715,
      "step": 522300
    },
    {
      "epoch": 39.90527843556642,
      "grad_norm": 8.871479034423828,
      "learning_rate": 1.184019555419754e-07,
      "loss": 1.6752,
      "step": 522400
    },
    {
      "epoch": 39.912917271407835,
      "grad_norm": 8.686076164245605,
      "learning_rate": 1.0885341074020319e-07,
      "loss": 1.7067,
      "step": 522500
    },
    {
      "epoch": 39.920556107249254,
      "grad_norm": 6.9540696144104,
      "learning_rate": 9.930486593843098e-08,
      "loss": 1.6893,
      "step": 522600
    },
    {
      "epoch": 39.92819494309067,
      "grad_norm": 9.116412162780762,
      "learning_rate": 8.975632113665877e-08,
      "loss": 1.7061,
      "step": 522700
    },
    {
      "epoch": 39.93583377893209,
      "grad_norm": 7.657554626464844,
      "learning_rate": 8.020777633488657e-08,
      "loss": 1.7306,
      "step": 522800
    },
    {
      "epoch": 39.94347261477351,
      "grad_norm": 8.04134750366211,
      "learning_rate": 7.065923153311435e-08,
      "loss": 1.7243,
      "step": 522900
    },
    {
      "epoch": 39.951111450614924,
      "grad_norm": 11.853042602539062,
      "learning_rate": 6.111068673134214e-08,
      "loss": 1.6138,
      "step": 523000
    },
    {
      "epoch": 39.95875028645634,
      "grad_norm": 5.546648979187012,
      "learning_rate": 5.156214192956994e-08,
      "loss": 1.6641,
      "step": 523100
    },
    {
      "epoch": 39.96638912229776,
      "grad_norm": 9.0502347946167,
      "learning_rate": 4.2013597127797726e-08,
      "loss": 1.6481,
      "step": 523200
    },
    {
      "epoch": 39.97402795813918,
      "grad_norm": 9.363941192626953,
      "learning_rate": 3.246505232602551e-08,
      "loss": 1.7264,
      "step": 523300
    },
    {
      "epoch": 39.981666793980594,
      "grad_norm": 7.139658451080322,
      "learning_rate": 2.2916507524253306e-08,
      "loss": 1.6658,
      "step": 523400
    },
    {
      "epoch": 39.98930562982201,
      "grad_norm": 7.539555549621582,
      "learning_rate": 1.3367962722481094e-08,
      "loss": 1.676,
      "step": 523500
    },
    {
      "epoch": 39.99694446566343,
      "grad_norm": 6.412676811218262,
      "learning_rate": 3.819417920708884e-09,
      "loss": 1.7926,
      "step": 523600
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.7898088693618774,
      "eval_runtime": 3.0447,
      "eval_samples_per_second": 226.623,
      "eval_steps_per_second": 226.623,
      "step": 523640
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.4702228307724,
      "eval_runtime": 34.8582,
      "eval_samples_per_second": 375.55,
      "eval_steps_per_second": 375.55,
      "step": 523640
    }
  ],
  "logging_steps": 100,
  "max_steps": 523640,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 324032788684800.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
