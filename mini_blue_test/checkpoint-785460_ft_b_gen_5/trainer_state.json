{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 60.0,
  "eval_steps": 500,
  "global_step": 785460,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007638835841417768,
      "grad_norm": 11.003396987915039,
      "learning_rate": 4.9993634303465486e-05,
      "loss": 4.7987,
      "step": 100
    },
    {
      "epoch": 0.015277671682835536,
      "grad_norm": 8.900592803955078,
      "learning_rate": 4.998726860693097e-05,
      "loss": 3.3992,
      "step": 200
    },
    {
      "epoch": 0.022916507524253303,
      "grad_norm": 5.975394248962402,
      "learning_rate": 4.998090291039646e-05,
      "loss": 3.0005,
      "step": 300
    },
    {
      "epoch": 0.030555343365671072,
      "grad_norm": 5.7895307540893555,
      "learning_rate": 4.997453721386194e-05,
      "loss": 2.9206,
      "step": 400
    },
    {
      "epoch": 0.03819417920708884,
      "grad_norm": 7.866730690002441,
      "learning_rate": 4.996817151732743e-05,
      "loss": 2.8266,
      "step": 500
    },
    {
      "epoch": 0.045833015048506606,
      "grad_norm": 6.887518882751465,
      "learning_rate": 4.996180582079291e-05,
      "loss": 2.8767,
      "step": 600
    },
    {
      "epoch": 0.05347185088992438,
      "grad_norm": 7.774272918701172,
      "learning_rate": 4.99554401242584e-05,
      "loss": 2.7593,
      "step": 700
    },
    {
      "epoch": 0.061110686731342144,
      "grad_norm": 6.494357109069824,
      "learning_rate": 4.9949074427723884e-05,
      "loss": 2.7513,
      "step": 800
    },
    {
      "epoch": 0.06874952257275992,
      "grad_norm": 5.112570762634277,
      "learning_rate": 4.994270873118937e-05,
      "loss": 2.58,
      "step": 900
    },
    {
      "epoch": 0.07638835841417768,
      "grad_norm": 7.6285014152526855,
      "learning_rate": 4.993634303465485e-05,
      "loss": 2.6628,
      "step": 1000
    },
    {
      "epoch": 0.08402719425559545,
      "grad_norm": 5.320619106292725,
      "learning_rate": 4.9929977338120335e-05,
      "loss": 2.7452,
      "step": 1100
    },
    {
      "epoch": 0.09166603009701321,
      "grad_norm": 4.718387603759766,
      "learning_rate": 4.9923611641585825e-05,
      "loss": 2.6365,
      "step": 1200
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 4.754127502441406,
      "learning_rate": 4.991724594505131e-05,
      "loss": 2.6765,
      "step": 1300
    },
    {
      "epoch": 0.10694370177984876,
      "grad_norm": 5.459957599639893,
      "learning_rate": 4.991088024851679e-05,
      "loss": 2.5782,
      "step": 1400
    },
    {
      "epoch": 0.11458253762126652,
      "grad_norm": 4.47595739364624,
      "learning_rate": 4.9904514551982276e-05,
      "loss": 2.5001,
      "step": 1500
    },
    {
      "epoch": 0.12222137346268429,
      "grad_norm": 7.569705963134766,
      "learning_rate": 4.9898148855447766e-05,
      "loss": 2.64,
      "step": 1600
    },
    {
      "epoch": 0.12986020930410205,
      "grad_norm": 4.624007225036621,
      "learning_rate": 4.989178315891325e-05,
      "loss": 2.4831,
      "step": 1700
    },
    {
      "epoch": 0.13749904514551983,
      "grad_norm": 5.883856773376465,
      "learning_rate": 4.988541746237873e-05,
      "loss": 2.5157,
      "step": 1800
    },
    {
      "epoch": 0.14513788098693758,
      "grad_norm": 6.40806770324707,
      "learning_rate": 4.9879051765844223e-05,
      "loss": 2.506,
      "step": 1900
    },
    {
      "epoch": 0.15277671682835536,
      "grad_norm": 4.2202959060668945,
      "learning_rate": 4.987268606930971e-05,
      "loss": 2.5831,
      "step": 2000
    },
    {
      "epoch": 0.16041555266977311,
      "grad_norm": 5.745185852050781,
      "learning_rate": 4.986632037277519e-05,
      "loss": 2.4737,
      "step": 2100
    },
    {
      "epoch": 0.1680543885111909,
      "grad_norm": 5.628921985626221,
      "learning_rate": 4.985995467624068e-05,
      "loss": 2.4374,
      "step": 2200
    },
    {
      "epoch": 0.17569322435260867,
      "grad_norm": 6.821033477783203,
      "learning_rate": 4.9853588979706164e-05,
      "loss": 2.4101,
      "step": 2300
    },
    {
      "epoch": 0.18333206019402642,
      "grad_norm": 5.130759239196777,
      "learning_rate": 4.984722328317165e-05,
      "loss": 2.5305,
      "step": 2400
    },
    {
      "epoch": 0.1909708960354442,
      "grad_norm": 6.336085796356201,
      "learning_rate": 4.984085758663713e-05,
      "loss": 2.4942,
      "step": 2500
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 5.207876682281494,
      "learning_rate": 4.983449189010262e-05,
      "loss": 2.4839,
      "step": 2600
    },
    {
      "epoch": 0.20624856771827974,
      "grad_norm": 6.232301235198975,
      "learning_rate": 4.9828126193568105e-05,
      "loss": 2.4191,
      "step": 2700
    },
    {
      "epoch": 0.21388740355969751,
      "grad_norm": 4.579092502593994,
      "learning_rate": 4.982176049703359e-05,
      "loss": 2.4134,
      "step": 2800
    },
    {
      "epoch": 0.22152623940111527,
      "grad_norm": 4.447245121002197,
      "learning_rate": 4.981539480049907e-05,
      "loss": 2.3616,
      "step": 2900
    },
    {
      "epoch": 0.22916507524253305,
      "grad_norm": 4.76075553894043,
      "learning_rate": 4.980902910396456e-05,
      "loss": 2.4775,
      "step": 3000
    },
    {
      "epoch": 0.2368039110839508,
      "grad_norm": 3.732889413833618,
      "learning_rate": 4.9802663407430046e-05,
      "loss": 2.4313,
      "step": 3100
    },
    {
      "epoch": 0.24444274692536858,
      "grad_norm": 4.529886722564697,
      "learning_rate": 4.979629771089553e-05,
      "loss": 2.3847,
      "step": 3200
    },
    {
      "epoch": 0.2520815827667863,
      "grad_norm": 5.568953990936279,
      "learning_rate": 4.978993201436101e-05,
      "loss": 2.4514,
      "step": 3300
    },
    {
      "epoch": 0.2597204186082041,
      "grad_norm": 4.899194717407227,
      "learning_rate": 4.97835663178265e-05,
      "loss": 2.4084,
      "step": 3400
    },
    {
      "epoch": 0.2673592544496219,
      "grad_norm": 5.895698547363281,
      "learning_rate": 4.977720062129199e-05,
      "loss": 2.3911,
      "step": 3500
    },
    {
      "epoch": 0.27499809029103967,
      "grad_norm": 5.283671855926514,
      "learning_rate": 4.977083492475747e-05,
      "loss": 2.3701,
      "step": 3600
    },
    {
      "epoch": 0.2826369261324574,
      "grad_norm": 4.288642406463623,
      "learning_rate": 4.9764469228222954e-05,
      "loss": 2.3728,
      "step": 3700
    },
    {
      "epoch": 0.29027576197387517,
      "grad_norm": 5.616775989532471,
      "learning_rate": 4.975810353168844e-05,
      "loss": 2.4251,
      "step": 3800
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 6.505110263824463,
      "learning_rate": 4.975173783515393e-05,
      "loss": 2.3596,
      "step": 3900
    },
    {
      "epoch": 0.3055534336567107,
      "grad_norm": 5.286049842834473,
      "learning_rate": 4.974537213861941e-05,
      "loss": 2.4107,
      "step": 4000
    },
    {
      "epoch": 0.3131922694981285,
      "grad_norm": 6.731076717376709,
      "learning_rate": 4.9739006442084895e-05,
      "loss": 2.3262,
      "step": 4100
    },
    {
      "epoch": 0.32083110533954623,
      "grad_norm": 4.681948661804199,
      "learning_rate": 4.973264074555038e-05,
      "loss": 2.4641,
      "step": 4200
    },
    {
      "epoch": 0.328469941180964,
      "grad_norm": 4.909715175628662,
      "learning_rate": 4.972627504901586e-05,
      "loss": 2.2768,
      "step": 4300
    },
    {
      "epoch": 0.3361087770223818,
      "grad_norm": 6.321661949157715,
      "learning_rate": 4.971990935248135e-05,
      "loss": 2.4101,
      "step": 4400
    },
    {
      "epoch": 0.34374761286379957,
      "grad_norm": 4.7248711585998535,
      "learning_rate": 4.9713543655946836e-05,
      "loss": 2.339,
      "step": 4500
    },
    {
      "epoch": 0.35138644870521735,
      "grad_norm": 5.027790069580078,
      "learning_rate": 4.970717795941232e-05,
      "loss": 2.3546,
      "step": 4600
    },
    {
      "epoch": 0.35902528454663507,
      "grad_norm": 4.733400344848633,
      "learning_rate": 4.97008122628778e-05,
      "loss": 2.4172,
      "step": 4700
    },
    {
      "epoch": 0.36666412038805285,
      "grad_norm": 5.185433387756348,
      "learning_rate": 4.969444656634329e-05,
      "loss": 2.417,
      "step": 4800
    },
    {
      "epoch": 0.37430295622947063,
      "grad_norm": 6.965174198150635,
      "learning_rate": 4.968808086980878e-05,
      "loss": 2.4498,
      "step": 4900
    },
    {
      "epoch": 0.3819417920708884,
      "grad_norm": 5.102669715881348,
      "learning_rate": 4.968171517327426e-05,
      "loss": 2.3239,
      "step": 5000
    },
    {
      "epoch": 0.3895806279123062,
      "grad_norm": 5.339658737182617,
      "learning_rate": 4.9675349476739744e-05,
      "loss": 2.3836,
      "step": 5100
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 6.19642972946167,
      "learning_rate": 4.966898378020523e-05,
      "loss": 2.3158,
      "step": 5200
    },
    {
      "epoch": 0.4048582995951417,
      "grad_norm": 3.971287727355957,
      "learning_rate": 4.966261808367072e-05,
      "loss": 2.3134,
      "step": 5300
    },
    {
      "epoch": 0.41249713543655947,
      "grad_norm": 4.2142133712768555,
      "learning_rate": 4.96562523871362e-05,
      "loss": 2.2675,
      "step": 5400
    },
    {
      "epoch": 0.42013597127797725,
      "grad_norm": 5.646320343017578,
      "learning_rate": 4.9649886690601685e-05,
      "loss": 2.4329,
      "step": 5500
    },
    {
      "epoch": 0.42777480711939503,
      "grad_norm": 5.131170749664307,
      "learning_rate": 4.9643520994067175e-05,
      "loss": 2.249,
      "step": 5600
    },
    {
      "epoch": 0.43541364296081275,
      "grad_norm": 4.929803848266602,
      "learning_rate": 4.963715529753266e-05,
      "loss": 2.2427,
      "step": 5700
    },
    {
      "epoch": 0.44305247880223053,
      "grad_norm": 4.086399078369141,
      "learning_rate": 4.963078960099814e-05,
      "loss": 2.3378,
      "step": 5800
    },
    {
      "epoch": 0.4506913146436483,
      "grad_norm": 4.7853779792785645,
      "learning_rate": 4.962442390446363e-05,
      "loss": 2.3175,
      "step": 5900
    },
    {
      "epoch": 0.4583301504850661,
      "grad_norm": 5.705160140991211,
      "learning_rate": 4.9618058207929116e-05,
      "loss": 2.3939,
      "step": 6000
    },
    {
      "epoch": 0.46596898632648387,
      "grad_norm": 4.324826240539551,
      "learning_rate": 4.96116925113946e-05,
      "loss": 2.3565,
      "step": 6100
    },
    {
      "epoch": 0.4736078221679016,
      "grad_norm": 5.057748794555664,
      "learning_rate": 4.960532681486009e-05,
      "loss": 2.3273,
      "step": 6200
    },
    {
      "epoch": 0.4812466580093194,
      "grad_norm": 5.834131717681885,
      "learning_rate": 4.9598961118325574e-05,
      "loss": 2.3206,
      "step": 6300
    },
    {
      "epoch": 0.48888549385073715,
      "grad_norm": 5.706160545349121,
      "learning_rate": 4.959259542179106e-05,
      "loss": 2.3716,
      "step": 6400
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 5.269311904907227,
      "learning_rate": 4.958622972525654e-05,
      "loss": 2.2627,
      "step": 6500
    },
    {
      "epoch": 0.5041631655335727,
      "grad_norm": 5.729904651641846,
      "learning_rate": 4.9579864028722024e-05,
      "loss": 2.1671,
      "step": 6600
    },
    {
      "epoch": 0.5118020013749904,
      "grad_norm": 3.571481466293335,
      "learning_rate": 4.9573498332187515e-05,
      "loss": 2.2704,
      "step": 6700
    },
    {
      "epoch": 0.5194408372164082,
      "grad_norm": 6.264553546905518,
      "learning_rate": 4.9567132635653e-05,
      "loss": 2.2505,
      "step": 6800
    },
    {
      "epoch": 0.527079673057826,
      "grad_norm": 4.309935092926025,
      "learning_rate": 4.956076693911848e-05,
      "loss": 2.2722,
      "step": 6900
    },
    {
      "epoch": 0.5347185088992438,
      "grad_norm": 6.211354732513428,
      "learning_rate": 4.9554401242583965e-05,
      "loss": 2.2451,
      "step": 7000
    },
    {
      "epoch": 0.5423573447406616,
      "grad_norm": 5.986732006072998,
      "learning_rate": 4.9548035546049455e-05,
      "loss": 2.3056,
      "step": 7100
    },
    {
      "epoch": 0.5499961805820793,
      "grad_norm": 5.57463264465332,
      "learning_rate": 4.954166984951494e-05,
      "loss": 2.2282,
      "step": 7200
    },
    {
      "epoch": 0.5576350164234971,
      "grad_norm": 4.415623188018799,
      "learning_rate": 4.953530415298042e-05,
      "loss": 2.2761,
      "step": 7300
    },
    {
      "epoch": 0.5652738522649148,
      "grad_norm": 5.856958389282227,
      "learning_rate": 4.9528938456445906e-05,
      "loss": 2.2874,
      "step": 7400
    },
    {
      "epoch": 0.5729126881063326,
      "grad_norm": 4.492795944213867,
      "learning_rate": 4.952257275991139e-05,
      "loss": 2.3064,
      "step": 7500
    },
    {
      "epoch": 0.5805515239477503,
      "grad_norm": 4.8552069664001465,
      "learning_rate": 4.951620706337688e-05,
      "loss": 2.2689,
      "step": 7600
    },
    {
      "epoch": 0.5881903597891681,
      "grad_norm": 4.370059490203857,
      "learning_rate": 4.9509841366842363e-05,
      "loss": 2.3461,
      "step": 7700
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 5.262638568878174,
      "learning_rate": 4.950347567030785e-05,
      "loss": 2.2502,
      "step": 7800
    },
    {
      "epoch": 0.6034680314720037,
      "grad_norm": 6.289999485015869,
      "learning_rate": 4.949710997377333e-05,
      "loss": 2.3907,
      "step": 7900
    },
    {
      "epoch": 0.6111068673134215,
      "grad_norm": 5.156611442565918,
      "learning_rate": 4.9490744277238814e-05,
      "loss": 2.2792,
      "step": 8000
    },
    {
      "epoch": 0.6187457031548392,
      "grad_norm": 8.065458297729492,
      "learning_rate": 4.9484378580704304e-05,
      "loss": 2.3101,
      "step": 8100
    },
    {
      "epoch": 0.626384538996257,
      "grad_norm": 4.8647589683532715,
      "learning_rate": 4.947801288416979e-05,
      "loss": 2.4493,
      "step": 8200
    },
    {
      "epoch": 0.6340233748376748,
      "grad_norm": 5.609063625335693,
      "learning_rate": 4.947164718763527e-05,
      "loss": 2.2455,
      "step": 8300
    },
    {
      "epoch": 0.6416622106790925,
      "grad_norm": 4.696290493011475,
      "learning_rate": 4.9465281491100755e-05,
      "loss": 2.3689,
      "step": 8400
    },
    {
      "epoch": 0.6493010465205102,
      "grad_norm": 4.671238899230957,
      "learning_rate": 4.9458915794566245e-05,
      "loss": 2.3385,
      "step": 8500
    },
    {
      "epoch": 0.656939882361928,
      "grad_norm": 5.878252983093262,
      "learning_rate": 4.945255009803173e-05,
      "loss": 2.373,
      "step": 8600
    },
    {
      "epoch": 0.6645787182033458,
      "grad_norm": 5.9969258308410645,
      "learning_rate": 4.944618440149721e-05,
      "loss": 2.2998,
      "step": 8700
    },
    {
      "epoch": 0.6722175540447636,
      "grad_norm": 5.083381652832031,
      "learning_rate": 4.9439818704962696e-05,
      "loss": 2.3673,
      "step": 8800
    },
    {
      "epoch": 0.6798563898861814,
      "grad_norm": 6.138138771057129,
      "learning_rate": 4.943345300842818e-05,
      "loss": 2.1902,
      "step": 8900
    },
    {
      "epoch": 0.6874952257275991,
      "grad_norm": 4.921735763549805,
      "learning_rate": 4.942708731189367e-05,
      "loss": 2.2429,
      "step": 9000
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 5.999809741973877,
      "learning_rate": 4.942072161535915e-05,
      "loss": 2.3055,
      "step": 9100
    },
    {
      "epoch": 0.7027728974104347,
      "grad_norm": 5.198227882385254,
      "learning_rate": 4.941435591882464e-05,
      "loss": 2.2199,
      "step": 9200
    },
    {
      "epoch": 0.7104117332518525,
      "grad_norm": 5.976837158203125,
      "learning_rate": 4.940799022229012e-05,
      "loss": 2.1992,
      "step": 9300
    },
    {
      "epoch": 0.7180505690932701,
      "grad_norm": 5.504315376281738,
      "learning_rate": 4.940162452575561e-05,
      "loss": 2.0797,
      "step": 9400
    },
    {
      "epoch": 0.7256894049346879,
      "grad_norm": 5.1509175300598145,
      "learning_rate": 4.9395258829221094e-05,
      "loss": 2.2534,
      "step": 9500
    },
    {
      "epoch": 0.7333282407761057,
      "grad_norm": 5.14268684387207,
      "learning_rate": 4.938889313268658e-05,
      "loss": 2.3399,
      "step": 9600
    },
    {
      "epoch": 0.7409670766175235,
      "grad_norm": 5.848530292510986,
      "learning_rate": 4.938252743615207e-05,
      "loss": 2.2494,
      "step": 9700
    },
    {
      "epoch": 0.7486059124589413,
      "grad_norm": 5.223510265350342,
      "learning_rate": 4.937616173961755e-05,
      "loss": 2.1326,
      "step": 9800
    },
    {
      "epoch": 0.756244748300359,
      "grad_norm": 4.658531665802002,
      "learning_rate": 4.936979604308304e-05,
      "loss": 2.2067,
      "step": 9900
    },
    {
      "epoch": 0.7638835841417768,
      "grad_norm": 3.829406976699829,
      "learning_rate": 4.9363430346548525e-05,
      "loss": 2.1967,
      "step": 10000
    },
    {
      "epoch": 0.7715224199831946,
      "grad_norm": 5.2110748291015625,
      "learning_rate": 4.935706465001401e-05,
      "loss": 2.1784,
      "step": 10100
    },
    {
      "epoch": 0.7791612558246124,
      "grad_norm": 5.96177339553833,
      "learning_rate": 4.935069895347949e-05,
      "loss": 2.2164,
      "step": 10200
    },
    {
      "epoch": 0.78680009166603,
      "grad_norm": 5.595090866088867,
      "learning_rate": 4.9344333256944976e-05,
      "loss": 2.1664,
      "step": 10300
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 5.02117919921875,
      "learning_rate": 4.9337967560410466e-05,
      "loss": 2.2654,
      "step": 10400
    },
    {
      "epoch": 0.8020777633488656,
      "grad_norm": 4.5138983726501465,
      "learning_rate": 4.933160186387595e-05,
      "loss": 2.321,
      "step": 10500
    },
    {
      "epoch": 0.8097165991902834,
      "grad_norm": 4.387924671173096,
      "learning_rate": 4.9325236167341433e-05,
      "loss": 2.2242,
      "step": 10600
    },
    {
      "epoch": 0.8173554350317012,
      "grad_norm": 5.320981025695801,
      "learning_rate": 4.931887047080692e-05,
      "loss": 2.3197,
      "step": 10700
    },
    {
      "epoch": 0.8249942708731189,
      "grad_norm": 4.889512538909912,
      "learning_rate": 4.931250477427241e-05,
      "loss": 2.1835,
      "step": 10800
    },
    {
      "epoch": 0.8326331067145367,
      "grad_norm": 4.650567531585693,
      "learning_rate": 4.930613907773789e-05,
      "loss": 2.2562,
      "step": 10900
    },
    {
      "epoch": 0.8402719425559545,
      "grad_norm": 5.706696510314941,
      "learning_rate": 4.9299773381203374e-05,
      "loss": 2.2688,
      "step": 11000
    },
    {
      "epoch": 0.8479107783973723,
      "grad_norm": 8.170299530029297,
      "learning_rate": 4.929340768466886e-05,
      "loss": 2.1902,
      "step": 11100
    },
    {
      "epoch": 0.8555496142387901,
      "grad_norm": 4.71286678314209,
      "learning_rate": 4.928704198813434e-05,
      "loss": 2.2445,
      "step": 11200
    },
    {
      "epoch": 0.8631884500802077,
      "grad_norm": 5.262633800506592,
      "learning_rate": 4.928067629159983e-05,
      "loss": 2.2422,
      "step": 11300
    },
    {
      "epoch": 0.8708272859216255,
      "grad_norm": 6.1787004470825195,
      "learning_rate": 4.9274310595065315e-05,
      "loss": 2.2594,
      "step": 11400
    },
    {
      "epoch": 0.8784661217630433,
      "grad_norm": 5.776530742645264,
      "learning_rate": 4.92679448985308e-05,
      "loss": 2.2076,
      "step": 11500
    },
    {
      "epoch": 0.8861049576044611,
      "grad_norm": 5.155461311340332,
      "learning_rate": 4.926157920199628e-05,
      "loss": 2.2628,
      "step": 11600
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 5.225574970245361,
      "learning_rate": 4.925521350546177e-05,
      "loss": 2.2179,
      "step": 11700
    },
    {
      "epoch": 0.9013826292872966,
      "grad_norm": 5.672508716583252,
      "learning_rate": 4.9248847808927256e-05,
      "loss": 2.3103,
      "step": 11800
    },
    {
      "epoch": 0.9090214651287144,
      "grad_norm": 6.074906826019287,
      "learning_rate": 4.924248211239274e-05,
      "loss": 2.3162,
      "step": 11900
    },
    {
      "epoch": 0.9166603009701322,
      "grad_norm": 5.03071403503418,
      "learning_rate": 4.923611641585822e-05,
      "loss": 2.2034,
      "step": 12000
    },
    {
      "epoch": 0.92429913681155,
      "grad_norm": 4.268101692199707,
      "learning_rate": 4.922975071932371e-05,
      "loss": 2.2248,
      "step": 12100
    },
    {
      "epoch": 0.9319379726529677,
      "grad_norm": 5.669069766998291,
      "learning_rate": 4.92233850227892e-05,
      "loss": 2.3092,
      "step": 12200
    },
    {
      "epoch": 0.9395768084943854,
      "grad_norm": 4.495509147644043,
      "learning_rate": 4.921701932625468e-05,
      "loss": 2.1965,
      "step": 12300
    },
    {
      "epoch": 0.9472156443358032,
      "grad_norm": 7.0822954177856445,
      "learning_rate": 4.9210653629720164e-05,
      "loss": 2.207,
      "step": 12400
    },
    {
      "epoch": 0.954854480177221,
      "grad_norm": 4.783316135406494,
      "learning_rate": 4.920428793318565e-05,
      "loss": 2.2018,
      "step": 12500
    },
    {
      "epoch": 0.9624933160186387,
      "grad_norm": 6.816341876983643,
      "learning_rate": 4.919792223665114e-05,
      "loss": 2.1585,
      "step": 12600
    },
    {
      "epoch": 0.9701321518600565,
      "grad_norm": 4.57766056060791,
      "learning_rate": 4.919155654011662e-05,
      "loss": 2.1525,
      "step": 12700
    },
    {
      "epoch": 0.9777709877014743,
      "grad_norm": 4.292228698730469,
      "learning_rate": 4.9185190843582105e-05,
      "loss": 2.1817,
      "step": 12800
    },
    {
      "epoch": 0.9854098235428921,
      "grad_norm": 6.101471424102783,
      "learning_rate": 4.917882514704759e-05,
      "loss": 2.1908,
      "step": 12900
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 4.360482692718506,
      "learning_rate": 4.917245945051307e-05,
      "loss": 2.1565,
      "step": 13000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.1020638942718506,
      "eval_runtime": 3.3368,
      "eval_samples_per_second": 206.787,
      "eval_steps_per_second": 206.787,
      "step": 13091
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9957842826843262,
      "eval_runtime": 59.7947,
      "eval_samples_per_second": 218.932,
      "eval_steps_per_second": 218.932,
      "step": 13091
    },
    {
      "epoch": 1.0006874952257276,
      "grad_norm": 4.266941547393799,
      "learning_rate": 4.916609375397856e-05,
      "loss": 2.2963,
      "step": 13100
    },
    {
      "epoch": 1.0083263310671453,
      "grad_norm": 4.117493152618408,
      "learning_rate": 4.9159728057444046e-05,
      "loss": 2.2097,
      "step": 13200
    },
    {
      "epoch": 1.0159651669085632,
      "grad_norm": 5.203220367431641,
      "learning_rate": 4.915336236090953e-05,
      "loss": 2.1886,
      "step": 13300
    },
    {
      "epoch": 1.0236040027499809,
      "grad_norm": 5.174717903137207,
      "learning_rate": 4.914699666437502e-05,
      "loss": 2.2313,
      "step": 13400
    },
    {
      "epoch": 1.0312428385913988,
      "grad_norm": 5.837007522583008,
      "learning_rate": 4.9140630967840503e-05,
      "loss": 2.2045,
      "step": 13500
    },
    {
      "epoch": 1.0388816744328164,
      "grad_norm": 5.140186309814453,
      "learning_rate": 4.913426527130599e-05,
      "loss": 2.1467,
      "step": 13600
    },
    {
      "epoch": 1.046520510274234,
      "grad_norm": 4.459240436553955,
      "learning_rate": 4.912789957477148e-05,
      "loss": 2.1501,
      "step": 13700
    },
    {
      "epoch": 1.054159346115652,
      "grad_norm": 3.984614849090576,
      "learning_rate": 4.912153387823696e-05,
      "loss": 2.1225,
      "step": 13800
    },
    {
      "epoch": 1.0617981819570697,
      "grad_norm": 4.381406784057617,
      "learning_rate": 4.9115168181702444e-05,
      "loss": 2.213,
      "step": 13900
    },
    {
      "epoch": 1.0694370177984875,
      "grad_norm": 4.571962356567383,
      "learning_rate": 4.9108802485167935e-05,
      "loss": 2.1812,
      "step": 14000
    },
    {
      "epoch": 1.0770758536399052,
      "grad_norm": 4.495993614196777,
      "learning_rate": 4.910243678863342e-05,
      "loss": 2.1612,
      "step": 14100
    },
    {
      "epoch": 1.084714689481323,
      "grad_norm": 5.194322109222412,
      "learning_rate": 4.90960710920989e-05,
      "loss": 2.1773,
      "step": 14200
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 6.22868537902832,
      "learning_rate": 4.9089705395564385e-05,
      "loss": 2.156,
      "step": 14300
    },
    {
      "epoch": 1.0999923611641587,
      "grad_norm": 6.081085681915283,
      "learning_rate": 4.908333969902987e-05,
      "loss": 2.1497,
      "step": 14400
    },
    {
      "epoch": 1.1076311970055763,
      "grad_norm": 5.961086750030518,
      "learning_rate": 4.907697400249536e-05,
      "loss": 2.1399,
      "step": 14500
    },
    {
      "epoch": 1.1152700328469942,
      "grad_norm": 5.200144290924072,
      "learning_rate": 4.907060830596084e-05,
      "loss": 2.1204,
      "step": 14600
    },
    {
      "epoch": 1.1229088686884119,
      "grad_norm": 5.688019752502441,
      "learning_rate": 4.9064242609426326e-05,
      "loss": 2.1568,
      "step": 14700
    },
    {
      "epoch": 1.1305477045298296,
      "grad_norm": 4.145893573760986,
      "learning_rate": 4.905787691289181e-05,
      "loss": 2.1535,
      "step": 14800
    },
    {
      "epoch": 1.1381865403712474,
      "grad_norm": 4.054906368255615,
      "learning_rate": 4.90515112163573e-05,
      "loss": 2.1917,
      "step": 14900
    },
    {
      "epoch": 1.1458253762126651,
      "grad_norm": 6.779367923736572,
      "learning_rate": 4.9045145519822784e-05,
      "loss": 2.2658,
      "step": 15000
    },
    {
      "epoch": 1.153464212054083,
      "grad_norm": 3.653660535812378,
      "learning_rate": 4.903877982328827e-05,
      "loss": 2.2033,
      "step": 15100
    },
    {
      "epoch": 1.1611030478955007,
      "grad_norm": 4.047735691070557,
      "learning_rate": 4.903241412675375e-05,
      "loss": 2.1292,
      "step": 15200
    },
    {
      "epoch": 1.1687418837369186,
      "grad_norm": 5.862399578094482,
      "learning_rate": 4.9026048430219234e-05,
      "loss": 2.1728,
      "step": 15300
    },
    {
      "epoch": 1.1763807195783362,
      "grad_norm": 4.829909324645996,
      "learning_rate": 4.9019682733684725e-05,
      "loss": 2.1782,
      "step": 15400
    },
    {
      "epoch": 1.1840195554197541,
      "grad_norm": 4.674121379852295,
      "learning_rate": 4.901331703715021e-05,
      "loss": 2.1537,
      "step": 15500
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 5.156543731689453,
      "learning_rate": 4.900695134061569e-05,
      "loss": 2.2121,
      "step": 15600
    },
    {
      "epoch": 1.1992972271025897,
      "grad_norm": 4.994520664215088,
      "learning_rate": 4.9000585644081175e-05,
      "loss": 2.1037,
      "step": 15700
    },
    {
      "epoch": 1.2069360629440073,
      "grad_norm": 4.793277740478516,
      "learning_rate": 4.8994219947546665e-05,
      "loss": 2.2251,
      "step": 15800
    },
    {
      "epoch": 1.214574898785425,
      "grad_norm": 5.018063068389893,
      "learning_rate": 4.898785425101215e-05,
      "loss": 2.1234,
      "step": 15900
    },
    {
      "epoch": 1.222213734626843,
      "grad_norm": 6.0472540855407715,
      "learning_rate": 4.898148855447763e-05,
      "loss": 2.2256,
      "step": 16000
    },
    {
      "epoch": 1.2298525704682606,
      "grad_norm": 4.678605556488037,
      "learning_rate": 4.8975122857943116e-05,
      "loss": 2.179,
      "step": 16100
    },
    {
      "epoch": 1.2374914063096785,
      "grad_norm": 6.100599765777588,
      "learning_rate": 4.89687571614086e-05,
      "loss": 2.1707,
      "step": 16200
    },
    {
      "epoch": 1.2451302421510961,
      "grad_norm": 5.2731475830078125,
      "learning_rate": 4.896239146487409e-05,
      "loss": 2.281,
      "step": 16300
    },
    {
      "epoch": 1.252769077992514,
      "grad_norm": 6.650087833404541,
      "learning_rate": 4.8956025768339573e-05,
      "loss": 2.2748,
      "step": 16400
    },
    {
      "epoch": 1.2604079138339317,
      "grad_norm": 5.361921787261963,
      "learning_rate": 4.894966007180506e-05,
      "loss": 2.1837,
      "step": 16500
    },
    {
      "epoch": 1.2680467496753494,
      "grad_norm": 5.769726276397705,
      "learning_rate": 4.894329437527054e-05,
      "loss": 2.1696,
      "step": 16600
    },
    {
      "epoch": 1.2756855855167673,
      "grad_norm": 4.066039085388184,
      "learning_rate": 4.8936928678736024e-05,
      "loss": 2.1457,
      "step": 16700
    },
    {
      "epoch": 1.2833244213581851,
      "grad_norm": 5.172082424163818,
      "learning_rate": 4.8930562982201514e-05,
      "loss": 2.1773,
      "step": 16800
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 5.9817094802856445,
      "learning_rate": 4.8924197285667e-05,
      "loss": 2.1951,
      "step": 16900
    },
    {
      "epoch": 1.2986020930410205,
      "grad_norm": 6.422817707061768,
      "learning_rate": 4.891783158913248e-05,
      "loss": 2.1133,
      "step": 17000
    },
    {
      "epoch": 1.3062409288824384,
      "grad_norm": 5.376896381378174,
      "learning_rate": 4.891146589259797e-05,
      "loss": 2.1469,
      "step": 17100
    },
    {
      "epoch": 1.313879764723856,
      "grad_norm": 3.4142510890960693,
      "learning_rate": 4.8905100196063455e-05,
      "loss": 2.1766,
      "step": 17200
    },
    {
      "epoch": 1.321518600565274,
      "grad_norm": 4.558021068572998,
      "learning_rate": 4.889873449952894e-05,
      "loss": 2.1528,
      "step": 17300
    },
    {
      "epoch": 1.3291574364066916,
      "grad_norm": 4.238364219665527,
      "learning_rate": 4.889236880299443e-05,
      "loss": 2.1989,
      "step": 17400
    },
    {
      "epoch": 1.3367962722481095,
      "grad_norm": 4.637102127075195,
      "learning_rate": 4.888600310645991e-05,
      "loss": 2.0712,
      "step": 17500
    },
    {
      "epoch": 1.3444351080895272,
      "grad_norm": 4.062856197357178,
      "learning_rate": 4.8879637409925396e-05,
      "loss": 2.0617,
      "step": 17600
    },
    {
      "epoch": 1.3520739439309448,
      "grad_norm": 3.9050891399383545,
      "learning_rate": 4.8873271713390887e-05,
      "loss": 2.1914,
      "step": 17700
    },
    {
      "epoch": 1.3597127797723627,
      "grad_norm": 5.718042373657227,
      "learning_rate": 4.886690601685637e-05,
      "loss": 2.1699,
      "step": 17800
    },
    {
      "epoch": 1.3673516156137804,
      "grad_norm": 4.354116916656494,
      "learning_rate": 4.8860540320321854e-05,
      "loss": 2.2103,
      "step": 17900
    },
    {
      "epoch": 1.3749904514551983,
      "grad_norm": 4.899850845336914,
      "learning_rate": 4.885417462378734e-05,
      "loss": 2.1215,
      "step": 18000
    },
    {
      "epoch": 1.382629287296616,
      "grad_norm": 10.652284622192383,
      "learning_rate": 4.884780892725283e-05,
      "loss": 2.2907,
      "step": 18100
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 5.675821781158447,
      "learning_rate": 4.884144323071831e-05,
      "loss": 2.1683,
      "step": 18200
    },
    {
      "epoch": 1.3979069589794515,
      "grad_norm": 4.043569087982178,
      "learning_rate": 4.8835077534183795e-05,
      "loss": 2.1464,
      "step": 18300
    },
    {
      "epoch": 1.4055457948208692,
      "grad_norm": 4.624657154083252,
      "learning_rate": 4.882871183764928e-05,
      "loss": 2.1238,
      "step": 18400
    },
    {
      "epoch": 1.413184630662287,
      "grad_norm": 4.769337177276611,
      "learning_rate": 4.882234614111476e-05,
      "loss": 2.1673,
      "step": 18500
    },
    {
      "epoch": 1.420823466503705,
      "grad_norm": 5.787594795227051,
      "learning_rate": 4.881598044458025e-05,
      "loss": 2.1006,
      "step": 18600
    },
    {
      "epoch": 1.4284623023451226,
      "grad_norm": 4.485126972198486,
      "learning_rate": 4.8809614748045735e-05,
      "loss": 2.0529,
      "step": 18700
    },
    {
      "epoch": 1.4361011381865403,
      "grad_norm": 5.658504962921143,
      "learning_rate": 4.880324905151122e-05,
      "loss": 2.0698,
      "step": 18800
    },
    {
      "epoch": 1.4437399740279582,
      "grad_norm": 5.069826602935791,
      "learning_rate": 4.87968833549767e-05,
      "loss": 2.1246,
      "step": 18900
    },
    {
      "epoch": 1.4513788098693758,
      "grad_norm": 4.87768030166626,
      "learning_rate": 4.8790517658442186e-05,
      "loss": 2.1206,
      "step": 19000
    },
    {
      "epoch": 1.4590176457107937,
      "grad_norm": 5.155447959899902,
      "learning_rate": 4.8784151961907676e-05,
      "loss": 2.1964,
      "step": 19100
    },
    {
      "epoch": 1.4666564815522114,
      "grad_norm": 5.651037216186523,
      "learning_rate": 4.877778626537316e-05,
      "loss": 2.1596,
      "step": 19200
    },
    {
      "epoch": 1.4742953173936293,
      "grad_norm": 6.657303810119629,
      "learning_rate": 4.8771420568838643e-05,
      "loss": 2.079,
      "step": 19300
    },
    {
      "epoch": 1.481934153235047,
      "grad_norm": 4.782303810119629,
      "learning_rate": 4.876505487230413e-05,
      "loss": 2.1995,
      "step": 19400
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 4.925703048706055,
      "learning_rate": 4.875868917576962e-05,
      "loss": 2.1218,
      "step": 19500
    },
    {
      "epoch": 1.4972118249178825,
      "grad_norm": 6.810848712921143,
      "learning_rate": 4.87523234792351e-05,
      "loss": 2.1854,
      "step": 19600
    },
    {
      "epoch": 1.5048506607593004,
      "grad_norm": 6.130570888519287,
      "learning_rate": 4.8745957782700584e-05,
      "loss": 2.0251,
      "step": 19700
    },
    {
      "epoch": 1.512489496600718,
      "grad_norm": 5.918476581573486,
      "learning_rate": 4.873959208616607e-05,
      "loss": 2.0872,
      "step": 19800
    },
    {
      "epoch": 1.5201283324421357,
      "grad_norm": 7.562942028045654,
      "learning_rate": 4.873322638963155e-05,
      "loss": 2.2041,
      "step": 19900
    },
    {
      "epoch": 1.5277671682835536,
      "grad_norm": 5.463088035583496,
      "learning_rate": 4.872686069309704e-05,
      "loss": 2.1747,
      "step": 20000
    },
    {
      "epoch": 1.5354060041249713,
      "grad_norm": 6.329188823699951,
      "learning_rate": 4.8720494996562525e-05,
      "loss": 2.1182,
      "step": 20100
    },
    {
      "epoch": 1.543044839966389,
      "grad_norm": 4.3021111488342285,
      "learning_rate": 4.871412930002801e-05,
      "loss": 2.1375,
      "step": 20200
    },
    {
      "epoch": 1.5506836758078069,
      "grad_norm": 5.480384826660156,
      "learning_rate": 4.870776360349349e-05,
      "loss": 2.1221,
      "step": 20300
    },
    {
      "epoch": 1.5583225116492248,
      "grad_norm": 4.1673197746276855,
      "learning_rate": 4.870139790695898e-05,
      "loss": 2.1011,
      "step": 20400
    },
    {
      "epoch": 1.5659613474906424,
      "grad_norm": 4.677631855010986,
      "learning_rate": 4.8695032210424466e-05,
      "loss": 2.1229,
      "step": 20500
    },
    {
      "epoch": 1.57360018333206,
      "grad_norm": 4.346617221832275,
      "learning_rate": 4.868866651388995e-05,
      "loss": 2.1143,
      "step": 20600
    },
    {
      "epoch": 1.581239019173478,
      "grad_norm": 5.942032814025879,
      "learning_rate": 4.868230081735543e-05,
      "loss": 2.1784,
      "step": 20700
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 5.14401388168335,
      "learning_rate": 4.867593512082092e-05,
      "loss": 2.1496,
      "step": 20800
    },
    {
      "epoch": 1.5965166908563135,
      "grad_norm": 4.923608779907227,
      "learning_rate": 4.866956942428641e-05,
      "loss": 2.1213,
      "step": 20900
    },
    {
      "epoch": 1.6041555266977312,
      "grad_norm": 4.917679309844971,
      "learning_rate": 4.866320372775189e-05,
      "loss": 2.1582,
      "step": 21000
    },
    {
      "epoch": 1.611794362539149,
      "grad_norm": 6.510858535766602,
      "learning_rate": 4.865683803121738e-05,
      "loss": 2.2582,
      "step": 21100
    },
    {
      "epoch": 1.6194331983805668,
      "grad_norm": 4.243424415588379,
      "learning_rate": 4.8650472334682865e-05,
      "loss": 2.1764,
      "step": 21200
    },
    {
      "epoch": 1.6270720342219844,
      "grad_norm": 6.301328659057617,
      "learning_rate": 4.864410663814835e-05,
      "loss": 2.158,
      "step": 21300
    },
    {
      "epoch": 1.6347108700634023,
      "grad_norm": 5.111812114715576,
      "learning_rate": 4.863774094161384e-05,
      "loss": 2.1786,
      "step": 21400
    },
    {
      "epoch": 1.6423497059048202,
      "grad_norm": 7.983246326446533,
      "learning_rate": 4.863137524507932e-05,
      "loss": 2.1528,
      "step": 21500
    },
    {
      "epoch": 1.6499885417462379,
      "grad_norm": 5.050661563873291,
      "learning_rate": 4.8625009548544805e-05,
      "loss": 2.0533,
      "step": 21600
    },
    {
      "epoch": 1.6576273775876555,
      "grad_norm": 6.025188446044922,
      "learning_rate": 4.861864385201029e-05,
      "loss": 2.1889,
      "step": 21700
    },
    {
      "epoch": 1.6652662134290734,
      "grad_norm": 4.917980194091797,
      "learning_rate": 4.861227815547578e-05,
      "loss": 2.1376,
      "step": 21800
    },
    {
      "epoch": 1.6729050492704913,
      "grad_norm": 4.371609210968018,
      "learning_rate": 4.860591245894126e-05,
      "loss": 2.0667,
      "step": 21900
    },
    {
      "epoch": 1.680543885111909,
      "grad_norm": 6.1302690505981445,
      "learning_rate": 4.8599546762406746e-05,
      "loss": 2.1436,
      "step": 22000
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 5.644843578338623,
      "learning_rate": 4.859318106587223e-05,
      "loss": 2.0392,
      "step": 22100
    },
    {
      "epoch": 1.6958215567947446,
      "grad_norm": 5.595630645751953,
      "learning_rate": 4.8586815369337713e-05,
      "loss": 2.0955,
      "step": 22200
    },
    {
      "epoch": 1.7034603926361622,
      "grad_norm": 4.8115081787109375,
      "learning_rate": 4.8580449672803204e-05,
      "loss": 2.0765,
      "step": 22300
    },
    {
      "epoch": 1.71109922847758,
      "grad_norm": 3.4439241886138916,
      "learning_rate": 4.857408397626869e-05,
      "loss": 2.1073,
      "step": 22400
    },
    {
      "epoch": 1.7187380643189978,
      "grad_norm": 4.72652530670166,
      "learning_rate": 4.856771827973417e-05,
      "loss": 2.1051,
      "step": 22500
    },
    {
      "epoch": 1.7263769001604157,
      "grad_norm": 4.072229385375977,
      "learning_rate": 4.8561352583199654e-05,
      "loss": 2.1477,
      "step": 22600
    },
    {
      "epoch": 1.7340157360018333,
      "grad_norm": 4.202186107635498,
      "learning_rate": 4.8554986886665145e-05,
      "loss": 2.2049,
      "step": 22700
    },
    {
      "epoch": 1.741654571843251,
      "grad_norm": 4.197848320007324,
      "learning_rate": 4.854862119013063e-05,
      "loss": 2.1878,
      "step": 22800
    },
    {
      "epoch": 1.749293407684669,
      "grad_norm": 6.1550822257995605,
      "learning_rate": 4.854225549359611e-05,
      "loss": 2.0561,
      "step": 22900
    },
    {
      "epoch": 1.7569322435260868,
      "grad_norm": 5.276869773864746,
      "learning_rate": 4.8535889797061595e-05,
      "loss": 2.1004,
      "step": 23000
    },
    {
      "epoch": 1.7645710793675042,
      "grad_norm": 12.029422760009766,
      "learning_rate": 4.852952410052708e-05,
      "loss": 2.1107,
      "step": 23100
    },
    {
      "epoch": 1.7722099152089221,
      "grad_norm": 4.983710765838623,
      "learning_rate": 4.852315840399257e-05,
      "loss": 2.131,
      "step": 23200
    },
    {
      "epoch": 1.77984875105034,
      "grad_norm": 4.1371002197265625,
      "learning_rate": 4.851679270745805e-05,
      "loss": 2.1985,
      "step": 23300
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 5.068281650543213,
      "learning_rate": 4.8510427010923536e-05,
      "loss": 2.1359,
      "step": 23400
    },
    {
      "epoch": 1.7951264227331754,
      "grad_norm": 4.510528564453125,
      "learning_rate": 4.850406131438902e-05,
      "loss": 2.0488,
      "step": 23500
    },
    {
      "epoch": 1.8027652585745932,
      "grad_norm": 5.442463397979736,
      "learning_rate": 4.849769561785451e-05,
      "loss": 2.1733,
      "step": 23600
    },
    {
      "epoch": 1.8104040944160111,
      "grad_norm": 6.8283257484436035,
      "learning_rate": 4.8491329921319994e-05,
      "loss": 2.1572,
      "step": 23700
    },
    {
      "epoch": 1.8180429302574288,
      "grad_norm": 5.184465408325195,
      "learning_rate": 4.848496422478548e-05,
      "loss": 2.1329,
      "step": 23800
    },
    {
      "epoch": 1.8256817660988465,
      "grad_norm": 6.569472789764404,
      "learning_rate": 4.847859852825096e-05,
      "loss": 2.0786,
      "step": 23900
    },
    {
      "epoch": 1.8333206019402644,
      "grad_norm": 4.285816192626953,
      "learning_rate": 4.8472232831716444e-05,
      "loss": 2.1484,
      "step": 24000
    },
    {
      "epoch": 1.840959437781682,
      "grad_norm": 6.099305629730225,
      "learning_rate": 4.8465867135181935e-05,
      "loss": 2.0387,
      "step": 24100
    },
    {
      "epoch": 1.8485982736230997,
      "grad_norm": 5.492764472961426,
      "learning_rate": 4.845950143864742e-05,
      "loss": 2.1993,
      "step": 24200
    },
    {
      "epoch": 1.8562371094645176,
      "grad_norm": 4.292374134063721,
      "learning_rate": 4.84531357421129e-05,
      "loss": 1.9753,
      "step": 24300
    },
    {
      "epoch": 1.8638759453059355,
      "grad_norm": 6.739704132080078,
      "learning_rate": 4.8446770045578385e-05,
      "loss": 2.1097,
      "step": 24400
    },
    {
      "epoch": 1.8715147811473531,
      "grad_norm": 4.284154891967773,
      "learning_rate": 4.8440404349043875e-05,
      "loss": 2.1677,
      "step": 24500
    },
    {
      "epoch": 1.8791536169887708,
      "grad_norm": 4.1605095863342285,
      "learning_rate": 4.843403865250936e-05,
      "loss": 2.2365,
      "step": 24600
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 8.439342498779297,
      "learning_rate": 4.842767295597484e-05,
      "loss": 2.1714,
      "step": 24700
    },
    {
      "epoch": 1.8944312886716066,
      "grad_norm": 5.864032745361328,
      "learning_rate": 4.8421307259440326e-05,
      "loss": 2.1274,
      "step": 24800
    },
    {
      "epoch": 1.9020701245130243,
      "grad_norm": 5.383835315704346,
      "learning_rate": 4.8414941562905816e-05,
      "loss": 2.0716,
      "step": 24900
    },
    {
      "epoch": 1.909708960354442,
      "grad_norm": 5.177967548370361,
      "learning_rate": 4.84085758663713e-05,
      "loss": 2.0403,
      "step": 25000
    },
    {
      "epoch": 1.9173477961958598,
      "grad_norm": 5.032341957092285,
      "learning_rate": 4.840221016983679e-05,
      "loss": 2.1198,
      "step": 25100
    },
    {
      "epoch": 1.9249866320372775,
      "grad_norm": 5.435822486877441,
      "learning_rate": 4.8395844473302274e-05,
      "loss": 2.1566,
      "step": 25200
    },
    {
      "epoch": 1.9326254678786952,
      "grad_norm": 4.061986923217773,
      "learning_rate": 4.838947877676776e-05,
      "loss": 2.0638,
      "step": 25300
    },
    {
      "epoch": 1.940264303720113,
      "grad_norm": 5.794314384460449,
      "learning_rate": 4.838311308023324e-05,
      "loss": 2.227,
      "step": 25400
    },
    {
      "epoch": 1.947903139561531,
      "grad_norm": 5.677358627319336,
      "learning_rate": 4.837674738369873e-05,
      "loss": 2.1671,
      "step": 25500
    },
    {
      "epoch": 1.9555419754029486,
      "grad_norm": 4.265524387359619,
      "learning_rate": 4.8370381687164215e-05,
      "loss": 2.1347,
      "step": 25600
    },
    {
      "epoch": 1.9631808112443663,
      "grad_norm": 5.251564025878906,
      "learning_rate": 4.83640159906297e-05,
      "loss": 2.1373,
      "step": 25700
    },
    {
      "epoch": 1.9708196470857842,
      "grad_norm": 5.316563606262207,
      "learning_rate": 4.835765029409518e-05,
      "loss": 2.1637,
      "step": 25800
    },
    {
      "epoch": 1.978458482927202,
      "grad_norm": 6.519346714019775,
      "learning_rate": 4.835128459756067e-05,
      "loss": 2.0343,
      "step": 25900
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 6.586745262145996,
      "learning_rate": 4.8344918901026156e-05,
      "loss": 2.068,
      "step": 26000
    },
    {
      "epoch": 1.9937361546100374,
      "grad_norm": 4.336559772491455,
      "learning_rate": 4.833855320449164e-05,
      "loss": 2.2537,
      "step": 26100
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.020484209060669,
      "eval_runtime": 3.1424,
      "eval_samples_per_second": 219.577,
      "eval_steps_per_second": 219.577,
      "step": 26182
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8968026638031006,
      "eval_runtime": 58.4097,
      "eval_samples_per_second": 224.124,
      "eval_steps_per_second": 224.124,
      "step": 26182
    },
    {
      "epoch": 2.0013749904514553,
      "grad_norm": 5.239777565002441,
      "learning_rate": 4.833218750795712e-05,
      "loss": 2.096,
      "step": 26200
    },
    {
      "epoch": 2.009013826292873,
      "grad_norm": 9.349827766418457,
      "learning_rate": 4.8325821811422606e-05,
      "loss": 2.0455,
      "step": 26300
    },
    {
      "epoch": 2.0166526621342906,
      "grad_norm": 5.598684310913086,
      "learning_rate": 4.8319456114888097e-05,
      "loss": 2.0727,
      "step": 26400
    },
    {
      "epoch": 2.0242914979757085,
      "grad_norm": 4.535653114318848,
      "learning_rate": 4.831309041835358e-05,
      "loss": 2.0427,
      "step": 26500
    },
    {
      "epoch": 2.0319303338171264,
      "grad_norm": 5.4314422607421875,
      "learning_rate": 4.8306724721819064e-05,
      "loss": 2.0167,
      "step": 26600
    },
    {
      "epoch": 2.039569169658544,
      "grad_norm": 6.214211463928223,
      "learning_rate": 4.830035902528455e-05,
      "loss": 1.9405,
      "step": 26700
    },
    {
      "epoch": 2.0472080054999617,
      "grad_norm": 7.109127521514893,
      "learning_rate": 4.829399332875004e-05,
      "loss": 1.9976,
      "step": 26800
    },
    {
      "epoch": 2.0548468413413796,
      "grad_norm": 4.740929126739502,
      "learning_rate": 4.828762763221552e-05,
      "loss": 2.1325,
      "step": 26900
    },
    {
      "epoch": 2.0624856771827975,
      "grad_norm": 6.149768352508545,
      "learning_rate": 4.8281261935681005e-05,
      "loss": 2.1174,
      "step": 27000
    },
    {
      "epoch": 2.070124513024215,
      "grad_norm": 5.684814453125,
      "learning_rate": 4.827489623914649e-05,
      "loss": 2.0005,
      "step": 27100
    },
    {
      "epoch": 2.077763348865633,
      "grad_norm": 5.925021648406982,
      "learning_rate": 4.826853054261197e-05,
      "loss": 2.1668,
      "step": 27200
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 6.058468341827393,
      "learning_rate": 4.826216484607746e-05,
      "loss": 2.141,
      "step": 27300
    },
    {
      "epoch": 2.093041020548468,
      "grad_norm": 4.875472068786621,
      "learning_rate": 4.8255799149542945e-05,
      "loss": 2.1204,
      "step": 27400
    },
    {
      "epoch": 2.100679856389886,
      "grad_norm": 6.417136192321777,
      "learning_rate": 4.824943345300843e-05,
      "loss": 2.1333,
      "step": 27500
    },
    {
      "epoch": 2.108318692231304,
      "grad_norm": 5.101943016052246,
      "learning_rate": 4.824306775647391e-05,
      "loss": 2.035,
      "step": 27600
    },
    {
      "epoch": 2.115957528072722,
      "grad_norm": 9.323779106140137,
      "learning_rate": 4.8236702059939396e-05,
      "loss": 2.1425,
      "step": 27700
    },
    {
      "epoch": 2.1235963639141393,
      "grad_norm": 6.243385314941406,
      "learning_rate": 4.8230336363404886e-05,
      "loss": 2.1066,
      "step": 27800
    },
    {
      "epoch": 2.131235199755557,
      "grad_norm": 5.291448593139648,
      "learning_rate": 4.822397066687037e-05,
      "loss": 2.1073,
      "step": 27900
    },
    {
      "epoch": 2.138874035596975,
      "grad_norm": 5.441615104675293,
      "learning_rate": 4.8217604970335853e-05,
      "loss": 1.9885,
      "step": 28000
    },
    {
      "epoch": 2.146512871438393,
      "grad_norm": 4.194351673126221,
      "learning_rate": 4.821123927380134e-05,
      "loss": 2.0316,
      "step": 28100
    },
    {
      "epoch": 2.1541517072798104,
      "grad_norm": 4.211719036102295,
      "learning_rate": 4.820487357726683e-05,
      "loss": 2.057,
      "step": 28200
    },
    {
      "epoch": 2.1617905431212283,
      "grad_norm": 5.979389190673828,
      "learning_rate": 4.819850788073231e-05,
      "loss": 2.09,
      "step": 28300
    },
    {
      "epoch": 2.169429378962646,
      "grad_norm": 4.7104268074035645,
      "learning_rate": 4.8192142184197794e-05,
      "loss": 2.0834,
      "step": 28400
    },
    {
      "epoch": 2.1770682148040637,
      "grad_norm": 5.146792411804199,
      "learning_rate": 4.818577648766328e-05,
      "loss": 2.1638,
      "step": 28500
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 5.3610920906066895,
      "learning_rate": 4.817941079112877e-05,
      "loss": 2.107,
      "step": 28600
    },
    {
      "epoch": 2.1923458864868994,
      "grad_norm": 6.014113903045654,
      "learning_rate": 4.817304509459425e-05,
      "loss": 2.1409,
      "step": 28700
    },
    {
      "epoch": 2.1999847223283173,
      "grad_norm": 6.0757317543029785,
      "learning_rate": 4.8166679398059735e-05,
      "loss": 2.0646,
      "step": 28800
    },
    {
      "epoch": 2.2076235581697348,
      "grad_norm": 3.780609369277954,
      "learning_rate": 4.8160313701525226e-05,
      "loss": 2.0984,
      "step": 28900
    },
    {
      "epoch": 2.2152623940111527,
      "grad_norm": 4.89990758895874,
      "learning_rate": 4.815394800499071e-05,
      "loss": 2.0923,
      "step": 29000
    },
    {
      "epoch": 2.2229012298525705,
      "grad_norm": 3.7411105632781982,
      "learning_rate": 4.81475823084562e-05,
      "loss": 2.0936,
      "step": 29100
    },
    {
      "epoch": 2.2305400656939884,
      "grad_norm": 4.921263694763184,
      "learning_rate": 4.814121661192168e-05,
      "loss": 2.1132,
      "step": 29200
    },
    {
      "epoch": 2.238178901535406,
      "grad_norm": 4.462282657623291,
      "learning_rate": 4.8134850915387167e-05,
      "loss": 2.0996,
      "step": 29300
    },
    {
      "epoch": 2.2458177373768238,
      "grad_norm": 5.4401702880859375,
      "learning_rate": 4.812848521885265e-05,
      "loss": 2.0292,
      "step": 29400
    },
    {
      "epoch": 2.2534565732182417,
      "grad_norm": 8.745314598083496,
      "learning_rate": 4.8122119522318134e-05,
      "loss": 2.0795,
      "step": 29500
    },
    {
      "epoch": 2.261095409059659,
      "grad_norm": 5.076837062835693,
      "learning_rate": 4.8115753825783624e-05,
      "loss": 2.1051,
      "step": 29600
    },
    {
      "epoch": 2.268734244901077,
      "grad_norm": 4.635329246520996,
      "learning_rate": 4.810938812924911e-05,
      "loss": 2.0986,
      "step": 29700
    },
    {
      "epoch": 2.276373080742495,
      "grad_norm": 5.220306873321533,
      "learning_rate": 4.810302243271459e-05,
      "loss": 2.1423,
      "step": 29800
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 5.391396522521973,
      "learning_rate": 4.8096656736180075e-05,
      "loss": 2.0505,
      "step": 29900
    },
    {
      "epoch": 2.2916507524253302,
      "grad_norm": 4.772590637207031,
      "learning_rate": 4.8090291039645565e-05,
      "loss": 1.9732,
      "step": 30000
    },
    {
      "epoch": 2.299289588266748,
      "grad_norm": 5.256459712982178,
      "learning_rate": 4.808392534311105e-05,
      "loss": 2.064,
      "step": 30100
    },
    {
      "epoch": 2.306928424108166,
      "grad_norm": 8.409823417663574,
      "learning_rate": 4.807755964657653e-05,
      "loss": 2.1115,
      "step": 30200
    },
    {
      "epoch": 2.314567259949584,
      "grad_norm": 4.639118671417236,
      "learning_rate": 4.8071193950042015e-05,
      "loss": 2.0753,
      "step": 30300
    },
    {
      "epoch": 2.3222060957910013,
      "grad_norm": 5.826315402984619,
      "learning_rate": 4.80648282535075e-05,
      "loss": 2.1097,
      "step": 30400
    },
    {
      "epoch": 2.3298449316324192,
      "grad_norm": 6.313990116119385,
      "learning_rate": 4.805846255697299e-05,
      "loss": 2.1108,
      "step": 30500
    },
    {
      "epoch": 2.337483767473837,
      "grad_norm": 6.015549659729004,
      "learning_rate": 4.805209686043847e-05,
      "loss": 2.0456,
      "step": 30600
    },
    {
      "epoch": 2.3451226033152546,
      "grad_norm": 4.639851093292236,
      "learning_rate": 4.8045731163903956e-05,
      "loss": 2.098,
      "step": 30700
    },
    {
      "epoch": 2.3527614391566725,
      "grad_norm": 4.938508987426758,
      "learning_rate": 4.803936546736944e-05,
      "loss": 2.1979,
      "step": 30800
    },
    {
      "epoch": 2.3604002749980904,
      "grad_norm": 4.838897705078125,
      "learning_rate": 4.8032999770834923e-05,
      "loss": 2.0043,
      "step": 30900
    },
    {
      "epoch": 2.3680391108395082,
      "grad_norm": 4.856192111968994,
      "learning_rate": 4.8026634074300414e-05,
      "loss": 2.019,
      "step": 31000
    },
    {
      "epoch": 2.3756779466809257,
      "grad_norm": 5.483775615692139,
      "learning_rate": 4.80202683777659e-05,
      "loss": 2.0729,
      "step": 31100
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 4.7839250564575195,
      "learning_rate": 4.801390268123138e-05,
      "loss": 2.123,
      "step": 31200
    },
    {
      "epoch": 2.3909556183637615,
      "grad_norm": 4.284111499786377,
      "learning_rate": 4.8007536984696864e-05,
      "loss": 2.0884,
      "step": 31300
    },
    {
      "epoch": 2.3985944542051794,
      "grad_norm": 5.394376277923584,
      "learning_rate": 4.8001171288162355e-05,
      "loss": 2.037,
      "step": 31400
    },
    {
      "epoch": 2.406233290046597,
      "grad_norm": 6.648130416870117,
      "learning_rate": 4.799480559162784e-05,
      "loss": 2.0557,
      "step": 31500
    },
    {
      "epoch": 2.4138721258880147,
      "grad_norm": 6.490131378173828,
      "learning_rate": 4.798843989509332e-05,
      "loss": 2.0922,
      "step": 31600
    },
    {
      "epoch": 2.4215109617294326,
      "grad_norm": 5.608483791351318,
      "learning_rate": 4.7982074198558805e-05,
      "loss": 2.0661,
      "step": 31700
    },
    {
      "epoch": 2.42914979757085,
      "grad_norm": 4.925465106964111,
      "learning_rate": 4.797570850202429e-05,
      "loss": 2.1197,
      "step": 31800
    },
    {
      "epoch": 2.436788633412268,
      "grad_norm": 4.274147033691406,
      "learning_rate": 4.796934280548978e-05,
      "loss": 2.0894,
      "step": 31900
    },
    {
      "epoch": 2.444427469253686,
      "grad_norm": 5.028322696685791,
      "learning_rate": 4.796297710895526e-05,
      "loss": 2.1839,
      "step": 32000
    },
    {
      "epoch": 2.4520663050951033,
      "grad_norm": 5.157658100128174,
      "learning_rate": 4.7956611412420746e-05,
      "loss": 2.0122,
      "step": 32100
    },
    {
      "epoch": 2.459705140936521,
      "grad_norm": 5.35587739944458,
      "learning_rate": 4.795024571588623e-05,
      "loss": 2.1586,
      "step": 32200
    },
    {
      "epoch": 2.467343976777939,
      "grad_norm": 4.257548809051514,
      "learning_rate": 4.794388001935172e-05,
      "loss": 2.0823,
      "step": 32300
    },
    {
      "epoch": 2.474982812619357,
      "grad_norm": 6.824571132659912,
      "learning_rate": 4.7937514322817204e-05,
      "loss": 2.0326,
      "step": 32400
    },
    {
      "epoch": 2.482621648460775,
      "grad_norm": 4.206033229827881,
      "learning_rate": 4.793114862628269e-05,
      "loss": 2.0533,
      "step": 32500
    },
    {
      "epoch": 2.4902604843021923,
      "grad_norm": 7.227175235748291,
      "learning_rate": 4.792478292974818e-05,
      "loss": 2.0373,
      "step": 32600
    },
    {
      "epoch": 2.49789932014361,
      "grad_norm": 4.7988152503967285,
      "learning_rate": 4.791841723321366e-05,
      "loss": 2.1041,
      "step": 32700
    },
    {
      "epoch": 2.505538155985028,
      "grad_norm": 4.865692615509033,
      "learning_rate": 4.7912051536679145e-05,
      "loss": 2.0838,
      "step": 32800
    },
    {
      "epoch": 2.5131769918264455,
      "grad_norm": 4.131801128387451,
      "learning_rate": 4.7905685840144635e-05,
      "loss": 2.0647,
      "step": 32900
    },
    {
      "epoch": 2.5208158276678634,
      "grad_norm": 6.135159969329834,
      "learning_rate": 4.789932014361012e-05,
      "loss": 2.1144,
      "step": 33000
    },
    {
      "epoch": 2.5284546635092813,
      "grad_norm": 4.225569248199463,
      "learning_rate": 4.78929544470756e-05,
      "loss": 1.9846,
      "step": 33100
    },
    {
      "epoch": 2.5360934993506987,
      "grad_norm": 5.735033988952637,
      "learning_rate": 4.7886588750541085e-05,
      "loss": 1.9852,
      "step": 33200
    },
    {
      "epoch": 2.5437323351921166,
      "grad_norm": 5.599527835845947,
      "learning_rate": 4.7880223054006576e-05,
      "loss": 2.0807,
      "step": 33300
    },
    {
      "epoch": 2.5513711710335345,
      "grad_norm": 5.068694591522217,
      "learning_rate": 4.787385735747206e-05,
      "loss": 2.0887,
      "step": 33400
    },
    {
      "epoch": 2.5590100068749524,
      "grad_norm": 4.8097429275512695,
      "learning_rate": 4.786749166093754e-05,
      "loss": 2.0133,
      "step": 33500
    },
    {
      "epoch": 2.5666488427163703,
      "grad_norm": 4.6799750328063965,
      "learning_rate": 4.7861125964403026e-05,
      "loss": 2.1557,
      "step": 33600
    },
    {
      "epoch": 2.5742876785577877,
      "grad_norm": 5.470669269561768,
      "learning_rate": 4.785476026786852e-05,
      "loss": 2.0459,
      "step": 33700
    },
    {
      "epoch": 2.5819265143992056,
      "grad_norm": 6.791591167449951,
      "learning_rate": 4.7848394571334e-05,
      "loss": 2.0902,
      "step": 33800
    },
    {
      "epoch": 2.5895653502406235,
      "grad_norm": 4.428032398223877,
      "learning_rate": 4.7842028874799484e-05,
      "loss": 2.0803,
      "step": 33900
    },
    {
      "epoch": 2.597204186082041,
      "grad_norm": 5.734211444854736,
      "learning_rate": 4.783566317826497e-05,
      "loss": 2.0473,
      "step": 34000
    },
    {
      "epoch": 2.604843021923459,
      "grad_norm": 5.6737823486328125,
      "learning_rate": 4.782929748173045e-05,
      "loss": 2.0911,
      "step": 34100
    },
    {
      "epoch": 2.6124818577648767,
      "grad_norm": 4.2421650886535645,
      "learning_rate": 4.782293178519594e-05,
      "loss": 2.0211,
      "step": 34200
    },
    {
      "epoch": 2.620120693606294,
      "grad_norm": 4.961748123168945,
      "learning_rate": 4.7816566088661425e-05,
      "loss": 2.0429,
      "step": 34300
    },
    {
      "epoch": 2.627759529447712,
      "grad_norm": 6.896844863891602,
      "learning_rate": 4.781020039212691e-05,
      "loss": 1.9485,
      "step": 34400
    },
    {
      "epoch": 2.63539836528913,
      "grad_norm": 4.352271556854248,
      "learning_rate": 4.780383469559239e-05,
      "loss": 2.006,
      "step": 34500
    },
    {
      "epoch": 2.643037201130548,
      "grad_norm": 5.78906774520874,
      "learning_rate": 4.779746899905788e-05,
      "loss": 2.0499,
      "step": 34600
    },
    {
      "epoch": 2.6506760369719657,
      "grad_norm": 5.523979663848877,
      "learning_rate": 4.7791103302523366e-05,
      "loss": 1.9969,
      "step": 34700
    },
    {
      "epoch": 2.658314872813383,
      "grad_norm": 3.9258153438568115,
      "learning_rate": 4.778473760598885e-05,
      "loss": 2.108,
      "step": 34800
    },
    {
      "epoch": 2.665953708654801,
      "grad_norm": 6.43876314163208,
      "learning_rate": 4.777837190945433e-05,
      "loss": 2.1521,
      "step": 34900
    },
    {
      "epoch": 2.673592544496219,
      "grad_norm": 4.416392803192139,
      "learning_rate": 4.7772006212919816e-05,
      "loss": 2.1582,
      "step": 35000
    },
    {
      "epoch": 2.6812313803376364,
      "grad_norm": 5.201959609985352,
      "learning_rate": 4.7765640516385307e-05,
      "loss": 2.1457,
      "step": 35100
    },
    {
      "epoch": 2.6888702161790543,
      "grad_norm": 4.056188106536865,
      "learning_rate": 4.775927481985079e-05,
      "loss": 2.0516,
      "step": 35200
    },
    {
      "epoch": 2.696509052020472,
      "grad_norm": 5.854379653930664,
      "learning_rate": 4.7752909123316274e-05,
      "loss": 2.0607,
      "step": 35300
    },
    {
      "epoch": 2.7041478878618896,
      "grad_norm": 5.085122585296631,
      "learning_rate": 4.774654342678176e-05,
      "loss": 2.0034,
      "step": 35400
    },
    {
      "epoch": 2.7117867237033075,
      "grad_norm": 3.8859007358551025,
      "learning_rate": 4.774017773024725e-05,
      "loss": 2.1394,
      "step": 35500
    },
    {
      "epoch": 2.7194255595447254,
      "grad_norm": 4.854066848754883,
      "learning_rate": 4.773381203371273e-05,
      "loss": 2.1363,
      "step": 35600
    },
    {
      "epoch": 2.7270643953861433,
      "grad_norm": 4.935229301452637,
      "learning_rate": 4.7727446337178215e-05,
      "loss": 2.0532,
      "step": 35700
    },
    {
      "epoch": 2.7347032312275608,
      "grad_norm": 4.085946559906006,
      "learning_rate": 4.77210806406437e-05,
      "loss": 2.0394,
      "step": 35800
    },
    {
      "epoch": 2.7423420670689787,
      "grad_norm": 4.631667613983154,
      "learning_rate": 4.771471494410918e-05,
      "loss": 2.0588,
      "step": 35900
    },
    {
      "epoch": 2.7499809029103965,
      "grad_norm": 4.287802696228027,
      "learning_rate": 4.770834924757467e-05,
      "loss": 1.9409,
      "step": 36000
    },
    {
      "epoch": 2.7576197387518144,
      "grad_norm": 4.994728088378906,
      "learning_rate": 4.7701983551040155e-05,
      "loss": 1.987,
      "step": 36100
    },
    {
      "epoch": 2.765258574593232,
      "grad_norm": 4.268820285797119,
      "learning_rate": 4.769561785450564e-05,
      "loss": 1.9888,
      "step": 36200
    },
    {
      "epoch": 2.7728974104346498,
      "grad_norm": 5.370898246765137,
      "learning_rate": 4.768925215797113e-05,
      "loss": 2.0366,
      "step": 36300
    },
    {
      "epoch": 2.7805362462760677,
      "grad_norm": 7.054059028625488,
      "learning_rate": 4.768288646143661e-05,
      "loss": 2.0979,
      "step": 36400
    },
    {
      "epoch": 2.788175082117485,
      "grad_norm": 6.168173313140869,
      "learning_rate": 4.7676520764902096e-05,
      "loss": 2.0609,
      "step": 36500
    },
    {
      "epoch": 2.795813917958903,
      "grad_norm": 4.5591325759887695,
      "learning_rate": 4.767015506836759e-05,
      "loss": 2.0574,
      "step": 36600
    },
    {
      "epoch": 2.803452753800321,
      "grad_norm": 4.750640869140625,
      "learning_rate": 4.766378937183307e-05,
      "loss": 2.1063,
      "step": 36700
    },
    {
      "epoch": 2.8110915896417383,
      "grad_norm": 4.647895336151123,
      "learning_rate": 4.7657423675298554e-05,
      "loss": 2.0155,
      "step": 36800
    },
    {
      "epoch": 2.818730425483156,
      "grad_norm": 5.193914890289307,
      "learning_rate": 4.7651057978764044e-05,
      "loss": 1.9837,
      "step": 36900
    },
    {
      "epoch": 2.826369261324574,
      "grad_norm": 5.381611347198486,
      "learning_rate": 4.764469228222953e-05,
      "loss": 2.0764,
      "step": 37000
    },
    {
      "epoch": 2.834008097165992,
      "grad_norm": 5.065334796905518,
      "learning_rate": 4.763832658569501e-05,
      "loss": 1.9949,
      "step": 37100
    },
    {
      "epoch": 2.84164693300741,
      "grad_norm": 7.530979156494141,
      "learning_rate": 4.7631960889160495e-05,
      "loss": 2.0981,
      "step": 37200
    },
    {
      "epoch": 2.8492857688488273,
      "grad_norm": 6.000520706176758,
      "learning_rate": 4.762559519262598e-05,
      "loss": 2.0499,
      "step": 37300
    },
    {
      "epoch": 2.8569246046902452,
      "grad_norm": 5.795202732086182,
      "learning_rate": 4.761922949609147e-05,
      "loss": 2.1414,
      "step": 37400
    },
    {
      "epoch": 2.864563440531663,
      "grad_norm": 5.080810546875,
      "learning_rate": 4.761286379955695e-05,
      "loss": 1.9555,
      "step": 37500
    },
    {
      "epoch": 2.8722022763730806,
      "grad_norm": 4.061729431152344,
      "learning_rate": 4.7606498103022436e-05,
      "loss": 2.1304,
      "step": 37600
    },
    {
      "epoch": 2.8798411122144985,
      "grad_norm": 3.5828006267547607,
      "learning_rate": 4.760013240648792e-05,
      "loss": 2.0668,
      "step": 37700
    },
    {
      "epoch": 2.8874799480559163,
      "grad_norm": 5.215569496154785,
      "learning_rate": 4.759376670995341e-05,
      "loss": 2.0469,
      "step": 37800
    },
    {
      "epoch": 2.895118783897334,
      "grad_norm": 4.157955169677734,
      "learning_rate": 4.758740101341889e-05,
      "loss": 2.0211,
      "step": 37900
    },
    {
      "epoch": 2.9027576197387517,
      "grad_norm": 5.153299808502197,
      "learning_rate": 4.7581035316884377e-05,
      "loss": 1.9759,
      "step": 38000
    },
    {
      "epoch": 2.9103964555801696,
      "grad_norm": 5.149206161499023,
      "learning_rate": 4.757466962034986e-05,
      "loss": 2.1049,
      "step": 38100
    },
    {
      "epoch": 2.9180352914215875,
      "grad_norm": 5.8140130043029785,
      "learning_rate": 4.7568303923815344e-05,
      "loss": 2.1687,
      "step": 38200
    },
    {
      "epoch": 2.9256741272630054,
      "grad_norm": 4.380126476287842,
      "learning_rate": 4.7561938227280834e-05,
      "loss": 2.0356,
      "step": 38300
    },
    {
      "epoch": 2.933312963104423,
      "grad_norm": 4.340345859527588,
      "learning_rate": 4.755557253074632e-05,
      "loss": 2.0534,
      "step": 38400
    },
    {
      "epoch": 2.9409517989458407,
      "grad_norm": 4.6894989013671875,
      "learning_rate": 4.75492068342118e-05,
      "loss": 2.12,
      "step": 38500
    },
    {
      "epoch": 2.9485906347872586,
      "grad_norm": 6.527030944824219,
      "learning_rate": 4.7542841137677285e-05,
      "loss": 2.0311,
      "step": 38600
    },
    {
      "epoch": 2.956229470628676,
      "grad_norm": 5.363827705383301,
      "learning_rate": 4.7536475441142775e-05,
      "loss": 2.0215,
      "step": 38700
    },
    {
      "epoch": 2.963868306470094,
      "grad_norm": 4.951590061187744,
      "learning_rate": 4.753010974460826e-05,
      "loss": 1.9832,
      "step": 38800
    },
    {
      "epoch": 2.971507142311512,
      "grad_norm": 4.7511067390441895,
      "learning_rate": 4.752374404807374e-05,
      "loss": 2.1989,
      "step": 38900
    },
    {
      "epoch": 2.9791459781529293,
      "grad_norm": 5.65178918838501,
      "learning_rate": 4.7517378351539225e-05,
      "loss": 2.1729,
      "step": 39000
    },
    {
      "epoch": 2.986784813994347,
      "grad_norm": 6.222956657409668,
      "learning_rate": 4.751101265500471e-05,
      "loss": 1.9993,
      "step": 39100
    },
    {
      "epoch": 2.994423649835765,
      "grad_norm": 4.718719482421875,
      "learning_rate": 4.75046469584702e-05,
      "loss": 2.103,
      "step": 39200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9709210395812988,
      "eval_runtime": 3.0013,
      "eval_samples_per_second": 229.904,
      "eval_steps_per_second": 229.904,
      "step": 39273
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.8291467428207397,
      "eval_runtime": 57.746,
      "eval_samples_per_second": 226.7,
      "eval_steps_per_second": 226.7,
      "step": 39273
    },
    {
      "epoch": 3.002062485677183,
      "grad_norm": 6.920961380004883,
      "learning_rate": 4.749828126193568e-05,
      "loss": 2.0947,
      "step": 39300
    },
    {
      "epoch": 3.0097013215186004,
      "grad_norm": 4.873175621032715,
      "learning_rate": 4.7491915565401166e-05,
      "loss": 1.9453,
      "step": 39400
    },
    {
      "epoch": 3.0173401573600183,
      "grad_norm": 7.457477569580078,
      "learning_rate": 4.748554986886665e-05,
      "loss": 2.0029,
      "step": 39500
    },
    {
      "epoch": 3.024978993201436,
      "grad_norm": 5.837228298187256,
      "learning_rate": 4.7479184172332133e-05,
      "loss": 2.0186,
      "step": 39600
    },
    {
      "epoch": 3.032617829042854,
      "grad_norm": 11.411619186401367,
      "learning_rate": 4.7472818475797624e-05,
      "loss": 2.059,
      "step": 39700
    },
    {
      "epoch": 3.0402566648842715,
      "grad_norm": 4.735198497772217,
      "learning_rate": 4.746645277926311e-05,
      "loss": 1.9568,
      "step": 39800
    },
    {
      "epoch": 3.0478955007256894,
      "grad_norm": 3.876016616821289,
      "learning_rate": 4.746008708272859e-05,
      "loss": 1.9365,
      "step": 39900
    },
    {
      "epoch": 3.0555343365671073,
      "grad_norm": 6.221607208251953,
      "learning_rate": 4.7453721386194074e-05,
      "loss": 1.9899,
      "step": 40000
    },
    {
      "epoch": 3.0631731724085247,
      "grad_norm": 6.019495010375977,
      "learning_rate": 4.7447355689659565e-05,
      "loss": 2.0742,
      "step": 40100
    },
    {
      "epoch": 3.0708120082499426,
      "grad_norm": 4.211643218994141,
      "learning_rate": 4.744098999312505e-05,
      "loss": 2.1225,
      "step": 40200
    },
    {
      "epoch": 3.0784508440913605,
      "grad_norm": 4.1621270179748535,
      "learning_rate": 4.743462429659054e-05,
      "loss": 1.9934,
      "step": 40300
    },
    {
      "epoch": 3.0860896799327784,
      "grad_norm": 8.324265480041504,
      "learning_rate": 4.742825860005602e-05,
      "loss": 2.1573,
      "step": 40400
    },
    {
      "epoch": 3.093728515774196,
      "grad_norm": 5.026970863342285,
      "learning_rate": 4.7421892903521506e-05,
      "loss": 1.9929,
      "step": 40500
    },
    {
      "epoch": 3.1013673516156137,
      "grad_norm": 4.514750003814697,
      "learning_rate": 4.7415527206986996e-05,
      "loss": 2.082,
      "step": 40600
    },
    {
      "epoch": 3.1090061874570316,
      "grad_norm": 5.714109420776367,
      "learning_rate": 4.740916151045248e-05,
      "loss": 2.0858,
      "step": 40700
    },
    {
      "epoch": 3.1166450232984495,
      "grad_norm": 4.341569423675537,
      "learning_rate": 4.740279581391796e-05,
      "loss": 1.9853,
      "step": 40800
    },
    {
      "epoch": 3.124283859139867,
      "grad_norm": 7.402037620544434,
      "learning_rate": 4.7396430117383447e-05,
      "loss": 2.0766,
      "step": 40900
    },
    {
      "epoch": 3.131922694981285,
      "grad_norm": 4.035668849945068,
      "learning_rate": 4.739006442084894e-05,
      "loss": 2.0204,
      "step": 41000
    },
    {
      "epoch": 3.1395615308227027,
      "grad_norm": 7.0034613609313965,
      "learning_rate": 4.738369872431442e-05,
      "loss": 1.9795,
      "step": 41100
    },
    {
      "epoch": 3.14720036666412,
      "grad_norm": 5.146919250488281,
      "learning_rate": 4.7377333027779904e-05,
      "loss": 2.0825,
      "step": 41200
    },
    {
      "epoch": 3.154839202505538,
      "grad_norm": 6.133081912994385,
      "learning_rate": 4.737096733124539e-05,
      "loss": 2.0455,
      "step": 41300
    },
    {
      "epoch": 3.162478038346956,
      "grad_norm": 5.0865888595581055,
      "learning_rate": 4.736460163471087e-05,
      "loss": 2.0393,
      "step": 41400
    },
    {
      "epoch": 3.170116874188374,
      "grad_norm": 4.154977798461914,
      "learning_rate": 4.735823593817636e-05,
      "loss": 1.9669,
      "step": 41500
    },
    {
      "epoch": 3.1777557100297913,
      "grad_norm": 7.2478437423706055,
      "learning_rate": 4.7351870241641845e-05,
      "loss": 1.9831,
      "step": 41600
    },
    {
      "epoch": 3.185394545871209,
      "grad_norm": 6.258958339691162,
      "learning_rate": 4.734550454510733e-05,
      "loss": 1.9492,
      "step": 41700
    },
    {
      "epoch": 3.193033381712627,
      "grad_norm": 5.562872886657715,
      "learning_rate": 4.733913884857281e-05,
      "loss": 1.9849,
      "step": 41800
    },
    {
      "epoch": 3.200672217554045,
      "grad_norm": 6.222262859344482,
      "learning_rate": 4.7332773152038295e-05,
      "loss": 1.985,
      "step": 41900
    },
    {
      "epoch": 3.2083110533954624,
      "grad_norm": 7.17245626449585,
      "learning_rate": 4.7326407455503786e-05,
      "loss": 2.0292,
      "step": 42000
    },
    {
      "epoch": 3.2159498892368803,
      "grad_norm": 6.926325798034668,
      "learning_rate": 4.732004175896927e-05,
      "loss": 1.9187,
      "step": 42100
    },
    {
      "epoch": 3.223588725078298,
      "grad_norm": 4.941972255706787,
      "learning_rate": 4.731367606243475e-05,
      "loss": 2.0928,
      "step": 42200
    },
    {
      "epoch": 3.2312275609197156,
      "grad_norm": 5.363863468170166,
      "learning_rate": 4.7307310365900236e-05,
      "loss": 2.0439,
      "step": 42300
    },
    {
      "epoch": 3.2388663967611335,
      "grad_norm": 4.903682231903076,
      "learning_rate": 4.730094466936573e-05,
      "loss": 2.0426,
      "step": 42400
    },
    {
      "epoch": 3.2465052326025514,
      "grad_norm": 4.857275009155273,
      "learning_rate": 4.729457897283121e-05,
      "loss": 2.0729,
      "step": 42500
    },
    {
      "epoch": 3.2541440684439693,
      "grad_norm": 6.666849136352539,
      "learning_rate": 4.7288213276296694e-05,
      "loss": 1.931,
      "step": 42600
    },
    {
      "epoch": 3.2617829042853868,
      "grad_norm": 5.084465503692627,
      "learning_rate": 4.728184757976218e-05,
      "loss": 2.0547,
      "step": 42700
    },
    {
      "epoch": 3.2694217401268046,
      "grad_norm": 7.100191593170166,
      "learning_rate": 4.727548188322766e-05,
      "loss": 2.0215,
      "step": 42800
    },
    {
      "epoch": 3.2770605759682225,
      "grad_norm": 6.69305944442749,
      "learning_rate": 4.726911618669315e-05,
      "loss": 1.9749,
      "step": 42900
    },
    {
      "epoch": 3.2846994118096404,
      "grad_norm": 6.8603925704956055,
      "learning_rate": 4.7262750490158635e-05,
      "loss": 2.0229,
      "step": 43000
    },
    {
      "epoch": 3.292338247651058,
      "grad_norm": 4.830540657043457,
      "learning_rate": 4.725638479362412e-05,
      "loss": 2.0083,
      "step": 43100
    },
    {
      "epoch": 3.2999770834924758,
      "grad_norm": 6.379141330718994,
      "learning_rate": 4.72500190970896e-05,
      "loss": 2.0338,
      "step": 43200
    },
    {
      "epoch": 3.3076159193338937,
      "grad_norm": 4.356451511383057,
      "learning_rate": 4.724365340055509e-05,
      "loss": 2.0289,
      "step": 43300
    },
    {
      "epoch": 3.315254755175311,
      "grad_norm": 4.9136857986450195,
      "learning_rate": 4.7237287704020576e-05,
      "loss": 2.0895,
      "step": 43400
    },
    {
      "epoch": 3.322893591016729,
      "grad_norm": 4.277096748352051,
      "learning_rate": 4.723092200748606e-05,
      "loss": 1.9687,
      "step": 43500
    },
    {
      "epoch": 3.330532426858147,
      "grad_norm": 6.533212184906006,
      "learning_rate": 4.722455631095154e-05,
      "loss": 2.019,
      "step": 43600
    },
    {
      "epoch": 3.3381712626995648,
      "grad_norm": 6.22859525680542,
      "learning_rate": 4.7218190614417026e-05,
      "loss": 1.9602,
      "step": 43700
    },
    {
      "epoch": 3.345810098540982,
      "grad_norm": 3.645887851715088,
      "learning_rate": 4.7211824917882517e-05,
      "loss": 2.0555,
      "step": 43800
    },
    {
      "epoch": 3.3534489343824,
      "grad_norm": 5.759725093841553,
      "learning_rate": 4.7205459221348e-05,
      "loss": 2.0545,
      "step": 43900
    },
    {
      "epoch": 3.361087770223818,
      "grad_norm": 4.942416191101074,
      "learning_rate": 4.7199093524813484e-05,
      "loss": 2.0499,
      "step": 44000
    },
    {
      "epoch": 3.368726606065236,
      "grad_norm": 4.43211030960083,
      "learning_rate": 4.7192727828278974e-05,
      "loss": 1.8967,
      "step": 44100
    },
    {
      "epoch": 3.3763654419066533,
      "grad_norm": 5.230855464935303,
      "learning_rate": 4.718636213174446e-05,
      "loss": 2.001,
      "step": 44200
    },
    {
      "epoch": 3.384004277748071,
      "grad_norm": 5.503210067749023,
      "learning_rate": 4.717999643520995e-05,
      "loss": 2.0419,
      "step": 44300
    },
    {
      "epoch": 3.391643113589489,
      "grad_norm": 4.483783721923828,
      "learning_rate": 4.717363073867543e-05,
      "loss": 2.0752,
      "step": 44400
    },
    {
      "epoch": 3.3992819494309066,
      "grad_norm": 5.690219879150391,
      "learning_rate": 4.7167265042140915e-05,
      "loss": 2.1148,
      "step": 44500
    },
    {
      "epoch": 3.4069207852723244,
      "grad_norm": 5.363310813903809,
      "learning_rate": 4.71608993456064e-05,
      "loss": 2.0573,
      "step": 44600
    },
    {
      "epoch": 3.4145596211137423,
      "grad_norm": 4.754331588745117,
      "learning_rate": 4.715453364907189e-05,
      "loss": 2.0236,
      "step": 44700
    },
    {
      "epoch": 3.42219845695516,
      "grad_norm": 5.659271717071533,
      "learning_rate": 4.714816795253737e-05,
      "loss": 2.0388,
      "step": 44800
    },
    {
      "epoch": 3.4298372927965777,
      "grad_norm": 4.505490303039551,
      "learning_rate": 4.7141802256002856e-05,
      "loss": 2.0266,
      "step": 44900
    },
    {
      "epoch": 3.4374761286379956,
      "grad_norm": 5.492023944854736,
      "learning_rate": 4.713543655946834e-05,
      "loss": 2.0895,
      "step": 45000
    },
    {
      "epoch": 3.4451149644794135,
      "grad_norm": 6.5337700843811035,
      "learning_rate": 4.712907086293382e-05,
      "loss": 2.077,
      "step": 45100
    },
    {
      "epoch": 3.4527538003208313,
      "grad_norm": 4.201380729675293,
      "learning_rate": 4.712270516639931e-05,
      "loss": 2.1013,
      "step": 45200
    },
    {
      "epoch": 3.460392636162249,
      "grad_norm": 4.292972564697266,
      "learning_rate": 4.71163394698648e-05,
      "loss": 2.0033,
      "step": 45300
    },
    {
      "epoch": 3.4680314720036667,
      "grad_norm": 6.537202835083008,
      "learning_rate": 4.710997377333028e-05,
      "loss": 2.0022,
      "step": 45400
    },
    {
      "epoch": 3.4756703078450846,
      "grad_norm": 5.648343563079834,
      "learning_rate": 4.7103608076795764e-05,
      "loss": 2.0766,
      "step": 45500
    },
    {
      "epoch": 3.483309143686502,
      "grad_norm": 3.9729013442993164,
      "learning_rate": 4.7097242380261254e-05,
      "loss": 2.1069,
      "step": 45600
    },
    {
      "epoch": 3.49094797952792,
      "grad_norm": 4.85319709777832,
      "learning_rate": 4.709087668372674e-05,
      "loss": 2.039,
      "step": 45700
    },
    {
      "epoch": 3.498586815369338,
      "grad_norm": 7.199388027191162,
      "learning_rate": 4.708451098719222e-05,
      "loss": 1.9975,
      "step": 45800
    },
    {
      "epoch": 3.5062256512107552,
      "grad_norm": 6.2629828453063965,
      "learning_rate": 4.7078145290657705e-05,
      "loss": 1.8664,
      "step": 45900
    },
    {
      "epoch": 3.513864487052173,
      "grad_norm": 5.076056957244873,
      "learning_rate": 4.707177959412319e-05,
      "loss": 1.9562,
      "step": 46000
    },
    {
      "epoch": 3.521503322893591,
      "grad_norm": 6.042113304138184,
      "learning_rate": 4.706541389758868e-05,
      "loss": 2.0108,
      "step": 46100
    },
    {
      "epoch": 3.529142158735009,
      "grad_norm": 6.344681262969971,
      "learning_rate": 4.705904820105416e-05,
      "loss": 2.0762,
      "step": 46200
    },
    {
      "epoch": 3.536780994576427,
      "grad_norm": 5.069186687469482,
      "learning_rate": 4.7052682504519646e-05,
      "loss": 2.0522,
      "step": 46300
    },
    {
      "epoch": 3.5444198304178443,
      "grad_norm": 4.118061065673828,
      "learning_rate": 4.704631680798513e-05,
      "loss": 2.0448,
      "step": 46400
    },
    {
      "epoch": 3.552058666259262,
      "grad_norm": 4.52597713470459,
      "learning_rate": 4.703995111145062e-05,
      "loss": 1.9035,
      "step": 46500
    },
    {
      "epoch": 3.55969750210068,
      "grad_norm": 4.247780799865723,
      "learning_rate": 4.70335854149161e-05,
      "loss": 1.971,
      "step": 46600
    },
    {
      "epoch": 3.5673363379420975,
      "grad_norm": 5.884444236755371,
      "learning_rate": 4.7027219718381587e-05,
      "loss": 1.9424,
      "step": 46700
    },
    {
      "epoch": 3.5749751737835154,
      "grad_norm": 6.794717311859131,
      "learning_rate": 4.702085402184707e-05,
      "loss": 2.0194,
      "step": 46800
    },
    {
      "epoch": 3.5826140096249333,
      "grad_norm": 5.13367223739624,
      "learning_rate": 4.7014488325312554e-05,
      "loss": 2.0346,
      "step": 46900
    },
    {
      "epoch": 3.5902528454663507,
      "grad_norm": 4.690126895904541,
      "learning_rate": 4.7008122628778044e-05,
      "loss": 2.1053,
      "step": 47000
    },
    {
      "epoch": 3.5978916813077686,
      "grad_norm": 4.214417457580566,
      "learning_rate": 4.700175693224353e-05,
      "loss": 2.0128,
      "step": 47100
    },
    {
      "epoch": 3.6055305171491865,
      "grad_norm": 5.6644086837768555,
      "learning_rate": 4.699539123570901e-05,
      "loss": 1.9833,
      "step": 47200
    },
    {
      "epoch": 3.6131693529906044,
      "grad_norm": 5.890137672424316,
      "learning_rate": 4.6989025539174495e-05,
      "loss": 2.0551,
      "step": 47300
    },
    {
      "epoch": 3.6208081888320223,
      "grad_norm": 7.127089977264404,
      "learning_rate": 4.6982659842639985e-05,
      "loss": 2.0127,
      "step": 47400
    },
    {
      "epoch": 3.6284470246734397,
      "grad_norm": 5.129510879516602,
      "learning_rate": 4.697629414610547e-05,
      "loss": 1.9963,
      "step": 47500
    },
    {
      "epoch": 3.6360858605148576,
      "grad_norm": 4.292200088500977,
      "learning_rate": 4.696992844957095e-05,
      "loss": 2.0177,
      "step": 47600
    },
    {
      "epoch": 3.6437246963562755,
      "grad_norm": 6.82318639755249,
      "learning_rate": 4.6963562753036435e-05,
      "loss": 1.9799,
      "step": 47700
    },
    {
      "epoch": 3.651363532197693,
      "grad_norm": 5.414224147796631,
      "learning_rate": 4.6957197056501926e-05,
      "loss": 2.0639,
      "step": 47800
    },
    {
      "epoch": 3.659002368039111,
      "grad_norm": 6.823721885681152,
      "learning_rate": 4.695083135996741e-05,
      "loss": 2.0426,
      "step": 47900
    },
    {
      "epoch": 3.6666412038805287,
      "grad_norm": 5.086898326873779,
      "learning_rate": 4.694446566343289e-05,
      "loss": 2.0559,
      "step": 48000
    },
    {
      "epoch": 3.674280039721946,
      "grad_norm": 6.089254379272461,
      "learning_rate": 4.693809996689838e-05,
      "loss": 2.0366,
      "step": 48100
    },
    {
      "epoch": 3.681918875563364,
      "grad_norm": 5.277979373931885,
      "learning_rate": 4.693173427036387e-05,
      "loss": 2.1064,
      "step": 48200
    },
    {
      "epoch": 3.689557711404782,
      "grad_norm": 5.24579381942749,
      "learning_rate": 4.692536857382935e-05,
      "loss": 2.0082,
      "step": 48300
    },
    {
      "epoch": 3.6971965472462,
      "grad_norm": 4.998936653137207,
      "learning_rate": 4.691900287729484e-05,
      "loss": 2.0655,
      "step": 48400
    },
    {
      "epoch": 3.7048353830876173,
      "grad_norm": 4.973114967346191,
      "learning_rate": 4.6912637180760324e-05,
      "loss": 2.0698,
      "step": 48500
    },
    {
      "epoch": 3.712474218929035,
      "grad_norm": 4.945659160614014,
      "learning_rate": 4.690627148422581e-05,
      "loss": 1.9837,
      "step": 48600
    },
    {
      "epoch": 3.720113054770453,
      "grad_norm": 4.230228424072266,
      "learning_rate": 4.689990578769129e-05,
      "loss": 2.0618,
      "step": 48700
    },
    {
      "epoch": 3.727751890611871,
      "grad_norm": 4.634078502655029,
      "learning_rate": 4.689354009115678e-05,
      "loss": 1.9996,
      "step": 48800
    },
    {
      "epoch": 3.7353907264532884,
      "grad_norm": 4.939635276794434,
      "learning_rate": 4.6887174394622265e-05,
      "loss": 2.0065,
      "step": 48900
    },
    {
      "epoch": 3.7430295622947063,
      "grad_norm": 3.9844164848327637,
      "learning_rate": 4.688080869808775e-05,
      "loss": 2.0877,
      "step": 49000
    },
    {
      "epoch": 3.750668398136124,
      "grad_norm": 4.829095363616943,
      "learning_rate": 4.687444300155323e-05,
      "loss": 1.9613,
      "step": 49100
    },
    {
      "epoch": 3.7583072339775416,
      "grad_norm": 4.53411340713501,
      "learning_rate": 4.6868077305018716e-05,
      "loss": 1.9969,
      "step": 49200
    },
    {
      "epoch": 3.7659460698189595,
      "grad_norm": 6.00852632522583,
      "learning_rate": 4.6861711608484206e-05,
      "loss": 2.0438,
      "step": 49300
    },
    {
      "epoch": 3.7735849056603774,
      "grad_norm": 4.730018138885498,
      "learning_rate": 4.685534591194969e-05,
      "loss": 1.9591,
      "step": 49400
    },
    {
      "epoch": 3.781223741501795,
      "grad_norm": 6.662684917449951,
      "learning_rate": 4.684898021541517e-05,
      "loss": 1.9345,
      "step": 49500
    },
    {
      "epoch": 3.7888625773432127,
      "grad_norm": 6.456526279449463,
      "learning_rate": 4.6842614518880657e-05,
      "loss": 2.0955,
      "step": 49600
    },
    {
      "epoch": 3.7965014131846306,
      "grad_norm": 5.238156318664551,
      "learning_rate": 4.683624882234615e-05,
      "loss": 2.0363,
      "step": 49700
    },
    {
      "epoch": 3.8041402490260485,
      "grad_norm": 6.548165798187256,
      "learning_rate": 4.682988312581163e-05,
      "loss": 1.9782,
      "step": 49800
    },
    {
      "epoch": 3.8117790848674664,
      "grad_norm": 4.221246719360352,
      "learning_rate": 4.6823517429277114e-05,
      "loss": 1.9875,
      "step": 49900
    },
    {
      "epoch": 3.819417920708884,
      "grad_norm": 4.650042533874512,
      "learning_rate": 4.68171517327426e-05,
      "loss": 1.9442,
      "step": 50000
    },
    {
      "epoch": 3.8270567565503018,
      "grad_norm": 4.816494941711426,
      "learning_rate": 4.681078603620808e-05,
      "loss": 1.9474,
      "step": 50100
    },
    {
      "epoch": 3.8346955923917196,
      "grad_norm": 6.769612789154053,
      "learning_rate": 4.680442033967357e-05,
      "loss": 1.9586,
      "step": 50200
    },
    {
      "epoch": 3.842334428233137,
      "grad_norm": 4.5933074951171875,
      "learning_rate": 4.6798054643139055e-05,
      "loss": 1.9871,
      "step": 50300
    },
    {
      "epoch": 3.849973264074555,
      "grad_norm": 3.1836040019989014,
      "learning_rate": 4.679168894660454e-05,
      "loss": 1.9427,
      "step": 50400
    },
    {
      "epoch": 3.857612099915973,
      "grad_norm": 4.239912986755371,
      "learning_rate": 4.678532325007002e-05,
      "loss": 2.0044,
      "step": 50500
    },
    {
      "epoch": 3.8652509357573903,
      "grad_norm": 4.208387851715088,
      "learning_rate": 4.6778957553535505e-05,
      "loss": 1.9598,
      "step": 50600
    },
    {
      "epoch": 3.872889771598808,
      "grad_norm": 4.513430595397949,
      "learning_rate": 4.6772591857000996e-05,
      "loss": 2.0254,
      "step": 50700
    },
    {
      "epoch": 3.880528607440226,
      "grad_norm": 5.529609203338623,
      "learning_rate": 4.676622616046648e-05,
      "loss": 2.0653,
      "step": 50800
    },
    {
      "epoch": 3.888167443281644,
      "grad_norm": 4.370026111602783,
      "learning_rate": 4.675986046393196e-05,
      "loss": 2.0514,
      "step": 50900
    },
    {
      "epoch": 3.895806279123062,
      "grad_norm": 4.222750663757324,
      "learning_rate": 4.6753494767397446e-05,
      "loss": 2.0412,
      "step": 51000
    },
    {
      "epoch": 3.9034451149644793,
      "grad_norm": 7.344798564910889,
      "learning_rate": 4.674712907086294e-05,
      "loss": 1.9799,
      "step": 51100
    },
    {
      "epoch": 3.911083950805897,
      "grad_norm": 5.259044647216797,
      "learning_rate": 4.674076337432842e-05,
      "loss": 1.966,
      "step": 51200
    },
    {
      "epoch": 3.918722786647315,
      "grad_norm": 3.6799917221069336,
      "learning_rate": 4.6734397677793904e-05,
      "loss": 2.0642,
      "step": 51300
    },
    {
      "epoch": 3.9263616224887326,
      "grad_norm": 5.969090461730957,
      "learning_rate": 4.672803198125939e-05,
      "loss": 1.9558,
      "step": 51400
    },
    {
      "epoch": 3.9340004583301504,
      "grad_norm": 5.064054489135742,
      "learning_rate": 4.672166628472488e-05,
      "loss": 2.1197,
      "step": 51500
    },
    {
      "epoch": 3.9416392941715683,
      "grad_norm": 6.3181071281433105,
      "learning_rate": 4.671530058819036e-05,
      "loss": 2.0303,
      "step": 51600
    },
    {
      "epoch": 3.9492781300129858,
      "grad_norm": 5.344417095184326,
      "learning_rate": 4.6708934891655845e-05,
      "loss": 1.8803,
      "step": 51700
    },
    {
      "epoch": 3.9569169658544037,
      "grad_norm": 4.359766960144043,
      "learning_rate": 4.6702569195121335e-05,
      "loss": 1.9698,
      "step": 51800
    },
    {
      "epoch": 3.9645558016958216,
      "grad_norm": 4.679858207702637,
      "learning_rate": 4.669620349858682e-05,
      "loss": 2.0283,
      "step": 51900
    },
    {
      "epoch": 3.9721946375372394,
      "grad_norm": 4.576117992401123,
      "learning_rate": 4.66898378020523e-05,
      "loss": 2.0621,
      "step": 52000
    },
    {
      "epoch": 3.9798334733786573,
      "grad_norm": 5.5069661140441895,
      "learning_rate": 4.668347210551779e-05,
      "loss": 2.0308,
      "step": 52100
    },
    {
      "epoch": 3.987472309220075,
      "grad_norm": 4.196009635925293,
      "learning_rate": 4.6677106408983276e-05,
      "loss": 1.9756,
      "step": 52200
    },
    {
      "epoch": 3.9951111450614927,
      "grad_norm": 4.011051654815674,
      "learning_rate": 4.667074071244876e-05,
      "loss": 2.0461,
      "step": 52300
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9394561052322388,
      "eval_runtime": 3.0358,
      "eval_samples_per_second": 227.291,
      "eval_steps_per_second": 227.291,
      "step": 52364
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.7849482297897339,
      "eval_runtime": 57.072,
      "eval_samples_per_second": 229.377,
      "eval_steps_per_second": 229.377,
      "step": 52364
    },
    {
      "epoch": 4.002749980902911,
      "grad_norm": 5.968033313751221,
      "learning_rate": 4.666437501591424e-05,
      "loss": 1.9884,
      "step": 52400
    },
    {
      "epoch": 4.010388816744328,
      "grad_norm": 7.054060935974121,
      "learning_rate": 4.665800931937973e-05,
      "loss": 2.0565,
      "step": 52500
    },
    {
      "epoch": 4.018027652585746,
      "grad_norm": 5.600791931152344,
      "learning_rate": 4.665164362284522e-05,
      "loss": 1.9513,
      "step": 52600
    },
    {
      "epoch": 4.025666488427164,
      "grad_norm": 5.166355133056641,
      "learning_rate": 4.66452779263107e-05,
      "loss": 1.971,
      "step": 52700
    },
    {
      "epoch": 4.033305324268581,
      "grad_norm": 4.879295825958252,
      "learning_rate": 4.6638912229776184e-05,
      "loss": 1.9426,
      "step": 52800
    },
    {
      "epoch": 4.04094416011,
      "grad_norm": 5.286499500274658,
      "learning_rate": 4.6632546533241674e-05,
      "loss": 2.0056,
      "step": 52900
    },
    {
      "epoch": 4.048582995951417,
      "grad_norm": 5.955382823944092,
      "learning_rate": 4.662618083670716e-05,
      "loss": 1.9771,
      "step": 53000
    },
    {
      "epoch": 4.0562218317928345,
      "grad_norm": 4.8634843826293945,
      "learning_rate": 4.661981514017264e-05,
      "loss": 1.9652,
      "step": 53100
    },
    {
      "epoch": 4.063860667634253,
      "grad_norm": 5.226178169250488,
      "learning_rate": 4.6613449443638125e-05,
      "loss": 1.9844,
      "step": 53200
    },
    {
      "epoch": 4.07149950347567,
      "grad_norm": 4.338829517364502,
      "learning_rate": 4.660708374710361e-05,
      "loss": 2.0236,
      "step": 53300
    },
    {
      "epoch": 4.079138339317088,
      "grad_norm": 5.355194091796875,
      "learning_rate": 4.66007180505691e-05,
      "loss": 2.0338,
      "step": 53400
    },
    {
      "epoch": 4.086777175158506,
      "grad_norm": 6.502950191497803,
      "learning_rate": 4.659435235403458e-05,
      "loss": 2.0164,
      "step": 53500
    },
    {
      "epoch": 4.0944160109999235,
      "grad_norm": 5.423729419708252,
      "learning_rate": 4.6587986657500066e-05,
      "loss": 1.9264,
      "step": 53600
    },
    {
      "epoch": 4.102054846841342,
      "grad_norm": 5.438634872436523,
      "learning_rate": 4.658162096096555e-05,
      "loss": 2.0169,
      "step": 53700
    },
    {
      "epoch": 4.109693682682759,
      "grad_norm": 4.417952060699463,
      "learning_rate": 4.657525526443103e-05,
      "loss": 2.0047,
      "step": 53800
    },
    {
      "epoch": 4.117332518524177,
      "grad_norm": 5.2608489990234375,
      "learning_rate": 4.656888956789652e-05,
      "loss": 2.0877,
      "step": 53900
    },
    {
      "epoch": 4.124971354365595,
      "grad_norm": 5.583528995513916,
      "learning_rate": 4.656252387136201e-05,
      "loss": 2.073,
      "step": 54000
    },
    {
      "epoch": 4.1326101902070125,
      "grad_norm": 4.132389068603516,
      "learning_rate": 4.655615817482749e-05,
      "loss": 1.9521,
      "step": 54100
    },
    {
      "epoch": 4.14024902604843,
      "grad_norm": 6.9280853271484375,
      "learning_rate": 4.6549792478292974e-05,
      "loss": 1.9756,
      "step": 54200
    },
    {
      "epoch": 4.147887861889848,
      "grad_norm": 4.941770076751709,
      "learning_rate": 4.6543426781758464e-05,
      "loss": 1.9286,
      "step": 54300
    },
    {
      "epoch": 4.155526697731266,
      "grad_norm": 6.611552715301514,
      "learning_rate": 4.653706108522395e-05,
      "loss": 1.9628,
      "step": 54400
    },
    {
      "epoch": 4.163165533572683,
      "grad_norm": 6.190732955932617,
      "learning_rate": 4.653069538868943e-05,
      "loss": 1.911,
      "step": 54500
    },
    {
      "epoch": 4.1708043694141015,
      "grad_norm": 4.024224758148193,
      "learning_rate": 4.6524329692154915e-05,
      "loss": 1.9925,
      "step": 54600
    },
    {
      "epoch": 4.178443205255519,
      "grad_norm": 4.375622272491455,
      "learning_rate": 4.65179639956204e-05,
      "loss": 1.894,
      "step": 54700
    },
    {
      "epoch": 4.186082041096936,
      "grad_norm": 5.078554153442383,
      "learning_rate": 4.651159829908589e-05,
      "loss": 1.9658,
      "step": 54800
    },
    {
      "epoch": 4.193720876938355,
      "grad_norm": 4.605544090270996,
      "learning_rate": 4.650523260255137e-05,
      "loss": 1.9603,
      "step": 54900
    },
    {
      "epoch": 4.201359712779772,
      "grad_norm": 4.555361270904541,
      "learning_rate": 4.6498866906016856e-05,
      "loss": 1.9499,
      "step": 55000
    },
    {
      "epoch": 4.2089985486211905,
      "grad_norm": 6.414746284484863,
      "learning_rate": 4.649250120948234e-05,
      "loss": 2.0738,
      "step": 55100
    },
    {
      "epoch": 4.216637384462608,
      "grad_norm": 5.1826677322387695,
      "learning_rate": 4.648613551294783e-05,
      "loss": 1.9639,
      "step": 55200
    },
    {
      "epoch": 4.224276220304025,
      "grad_norm": 5.959232330322266,
      "learning_rate": 4.647976981641331e-05,
      "loss": 2.0122,
      "step": 55300
    },
    {
      "epoch": 4.231915056145444,
      "grad_norm": 4.505661964416504,
      "learning_rate": 4.6473404119878797e-05,
      "loss": 2.0443,
      "step": 55400
    },
    {
      "epoch": 4.239553891986861,
      "grad_norm": 3.938676595687866,
      "learning_rate": 4.646703842334429e-05,
      "loss": 2.0405,
      "step": 55500
    },
    {
      "epoch": 4.247192727828279,
      "grad_norm": 5.607074737548828,
      "learning_rate": 4.646067272680977e-05,
      "loss": 2.0118,
      "step": 55600
    },
    {
      "epoch": 4.254831563669697,
      "grad_norm": 5.613372325897217,
      "learning_rate": 4.6454307030275254e-05,
      "loss": 1.9073,
      "step": 55700
    },
    {
      "epoch": 4.262470399511114,
      "grad_norm": 5.298267841339111,
      "learning_rate": 4.6447941333740744e-05,
      "loss": 1.9277,
      "step": 55800
    },
    {
      "epoch": 4.270109235352532,
      "grad_norm": 4.565837383270264,
      "learning_rate": 4.644157563720623e-05,
      "loss": 2.0176,
      "step": 55900
    },
    {
      "epoch": 4.27774807119395,
      "grad_norm": 6.341546535491943,
      "learning_rate": 4.643520994067171e-05,
      "loss": 1.9266,
      "step": 56000
    },
    {
      "epoch": 4.285386907035368,
      "grad_norm": 6.417647361755371,
      "learning_rate": 4.64288442441372e-05,
      "loss": 2.0479,
      "step": 56100
    },
    {
      "epoch": 4.293025742876786,
      "grad_norm": 6.20062255859375,
      "learning_rate": 4.6422478547602685e-05,
      "loss": 2.1239,
      "step": 56200
    },
    {
      "epoch": 4.300664578718203,
      "grad_norm": 3.6899893283843994,
      "learning_rate": 4.641611285106817e-05,
      "loss": 2.0268,
      "step": 56300
    },
    {
      "epoch": 4.308303414559621,
      "grad_norm": 5.1761908531188965,
      "learning_rate": 4.640974715453365e-05,
      "loss": 1.984,
      "step": 56400
    },
    {
      "epoch": 4.315942250401039,
      "grad_norm": 6.955145359039307,
      "learning_rate": 4.6403381457999136e-05,
      "loss": 2.035,
      "step": 56500
    },
    {
      "epoch": 4.323581086242457,
      "grad_norm": 4.993860244750977,
      "learning_rate": 4.6397015761464626e-05,
      "loss": 1.9003,
      "step": 56600
    },
    {
      "epoch": 4.331219922083874,
      "grad_norm": 5.303086757659912,
      "learning_rate": 4.639065006493011e-05,
      "loss": 1.9433,
      "step": 56700
    },
    {
      "epoch": 4.338858757925292,
      "grad_norm": 3.908095598220825,
      "learning_rate": 4.638428436839559e-05,
      "loss": 1.8941,
      "step": 56800
    },
    {
      "epoch": 4.34649759376671,
      "grad_norm": 5.210081100463867,
      "learning_rate": 4.637791867186108e-05,
      "loss": 2.042,
      "step": 56900
    },
    {
      "epoch": 4.354136429608127,
      "grad_norm": 5.154153823852539,
      "learning_rate": 4.637155297532656e-05,
      "loss": 2.04,
      "step": 57000
    },
    {
      "epoch": 4.361775265449546,
      "grad_norm": 5.123912334442139,
      "learning_rate": 4.636518727879205e-05,
      "loss": 2.0043,
      "step": 57100
    },
    {
      "epoch": 4.369414101290963,
      "grad_norm": 6.965680122375488,
      "learning_rate": 4.6358821582257534e-05,
      "loss": 2.0313,
      "step": 57200
    },
    {
      "epoch": 4.377052937132381,
      "grad_norm": 5.040268898010254,
      "learning_rate": 4.635245588572302e-05,
      "loss": 1.9983,
      "step": 57300
    },
    {
      "epoch": 4.384691772973799,
      "grad_norm": 3.7932517528533936,
      "learning_rate": 4.63460901891885e-05,
      "loss": 1.9932,
      "step": 57400
    },
    {
      "epoch": 4.392330608815216,
      "grad_norm": 3.825294256210327,
      "learning_rate": 4.633972449265399e-05,
      "loss": 2.0002,
      "step": 57500
    },
    {
      "epoch": 4.399969444656635,
      "grad_norm": 5.374707221984863,
      "learning_rate": 4.6333358796119475e-05,
      "loss": 1.995,
      "step": 57600
    },
    {
      "epoch": 4.407608280498052,
      "grad_norm": 4.59088659286499,
      "learning_rate": 4.632699309958496e-05,
      "loss": 1.9639,
      "step": 57700
    },
    {
      "epoch": 4.4152471163394695,
      "grad_norm": 5.084624290466309,
      "learning_rate": 4.632062740305044e-05,
      "loss": 1.9342,
      "step": 57800
    },
    {
      "epoch": 4.422885952180888,
      "grad_norm": 3.7594170570373535,
      "learning_rate": 4.6314261706515926e-05,
      "loss": 2.1288,
      "step": 57900
    },
    {
      "epoch": 4.430524788022305,
      "grad_norm": 6.3802056312561035,
      "learning_rate": 4.6307896009981416e-05,
      "loss": 1.9768,
      "step": 58000
    },
    {
      "epoch": 4.438163623863723,
      "grad_norm": 5.944986343383789,
      "learning_rate": 4.63015303134469e-05,
      "loss": 2.0226,
      "step": 58100
    },
    {
      "epoch": 4.445802459705141,
      "grad_norm": 5.617832183837891,
      "learning_rate": 4.629516461691238e-05,
      "loss": 1.9474,
      "step": 58200
    },
    {
      "epoch": 4.4534412955465585,
      "grad_norm": 4.800806045532227,
      "learning_rate": 4.6288798920377867e-05,
      "loss": 2.032,
      "step": 58300
    },
    {
      "epoch": 4.461080131387977,
      "grad_norm": 4.38847541809082,
      "learning_rate": 4.628243322384336e-05,
      "loss": 1.9484,
      "step": 58400
    },
    {
      "epoch": 4.468718967229394,
      "grad_norm": 8.25306224822998,
      "learning_rate": 4.627606752730884e-05,
      "loss": 1.9083,
      "step": 58500
    },
    {
      "epoch": 4.476357803070812,
      "grad_norm": 6.641019821166992,
      "learning_rate": 4.6269701830774324e-05,
      "loss": 1.9959,
      "step": 58600
    },
    {
      "epoch": 4.48399663891223,
      "grad_norm": 4.589031219482422,
      "learning_rate": 4.626333613423981e-05,
      "loss": 1.8212,
      "step": 58700
    },
    {
      "epoch": 4.4916354747536476,
      "grad_norm": 10.745789527893066,
      "learning_rate": 4.625697043770529e-05,
      "loss": 1.9517,
      "step": 58800
    },
    {
      "epoch": 4.499274310595065,
      "grad_norm": 6.371495246887207,
      "learning_rate": 4.625060474117078e-05,
      "loss": 1.9699,
      "step": 58900
    },
    {
      "epoch": 4.506913146436483,
      "grad_norm": 4.918023586273193,
      "learning_rate": 4.6244239044636265e-05,
      "loss": 2.0231,
      "step": 59000
    },
    {
      "epoch": 4.514551982277901,
      "grad_norm": 4.435523986816406,
      "learning_rate": 4.623787334810175e-05,
      "loss": 2.0127,
      "step": 59100
    },
    {
      "epoch": 4.522190818119318,
      "grad_norm": 5.766720771789551,
      "learning_rate": 4.623150765156723e-05,
      "loss": 1.9302,
      "step": 59200
    },
    {
      "epoch": 4.529829653960737,
      "grad_norm": 4.266794204711914,
      "learning_rate": 4.622514195503272e-05,
      "loss": 1.9793,
      "step": 59300
    },
    {
      "epoch": 4.537468489802154,
      "grad_norm": 5.347245693206787,
      "learning_rate": 4.6218776258498206e-05,
      "loss": 2.0052,
      "step": 59400
    },
    {
      "epoch": 4.545107325643572,
      "grad_norm": 5.425571918487549,
      "learning_rate": 4.6212410561963696e-05,
      "loss": 1.9339,
      "step": 59500
    },
    {
      "epoch": 4.55274616148499,
      "grad_norm": 5.835730075836182,
      "learning_rate": 4.620604486542918e-05,
      "loss": 1.9614,
      "step": 59600
    },
    {
      "epoch": 4.560384997326407,
      "grad_norm": 6.820795059204102,
      "learning_rate": 4.619967916889466e-05,
      "loss": 1.9292,
      "step": 59700
    },
    {
      "epoch": 4.568023833167826,
      "grad_norm": 5.6686577796936035,
      "learning_rate": 4.6193313472360153e-05,
      "loss": 2.0331,
      "step": 59800
    },
    {
      "epoch": 4.575662669009243,
      "grad_norm": 5.918038845062256,
      "learning_rate": 4.618694777582564e-05,
      "loss": 1.9312,
      "step": 59900
    },
    {
      "epoch": 4.5833015048506605,
      "grad_norm": 6.079443454742432,
      "learning_rate": 4.618058207929112e-05,
      "loss": 2.0031,
      "step": 60000
    },
    {
      "epoch": 4.590940340692079,
      "grad_norm": 5.707193851470947,
      "learning_rate": 4.6174216382756604e-05,
      "loss": 2.0667,
      "step": 60100
    },
    {
      "epoch": 4.598579176533496,
      "grad_norm": 4.819544792175293,
      "learning_rate": 4.616785068622209e-05,
      "loss": 1.8487,
      "step": 60200
    },
    {
      "epoch": 4.606218012374914,
      "grad_norm": 5.257479667663574,
      "learning_rate": 4.616148498968758e-05,
      "loss": 1.9049,
      "step": 60300
    },
    {
      "epoch": 4.613856848216332,
      "grad_norm": 5.6171417236328125,
      "learning_rate": 4.615511929315306e-05,
      "loss": 1.9475,
      "step": 60400
    },
    {
      "epoch": 4.6214956840577495,
      "grad_norm": 5.813770294189453,
      "learning_rate": 4.6148753596618545e-05,
      "loss": 1.9915,
      "step": 60500
    },
    {
      "epoch": 4.629134519899168,
      "grad_norm": 3.7017946243286133,
      "learning_rate": 4.614238790008403e-05,
      "loss": 1.9217,
      "step": 60600
    },
    {
      "epoch": 4.636773355740585,
      "grad_norm": 4.644604206085205,
      "learning_rate": 4.613602220354952e-05,
      "loss": 1.8863,
      "step": 60700
    },
    {
      "epoch": 4.644412191582003,
      "grad_norm": 6.8807053565979,
      "learning_rate": 4.6129656507015e-05,
      "loss": 1.8832,
      "step": 60800
    },
    {
      "epoch": 4.652051027423421,
      "grad_norm": 4.899267673492432,
      "learning_rate": 4.6123290810480486e-05,
      "loss": 2.0201,
      "step": 60900
    },
    {
      "epoch": 4.6596898632648385,
      "grad_norm": 3.996875286102295,
      "learning_rate": 4.611692511394597e-05,
      "loss": 2.0484,
      "step": 61000
    },
    {
      "epoch": 4.667328699106256,
      "grad_norm": 5.476218223571777,
      "learning_rate": 4.611055941741145e-05,
      "loss": 2.0731,
      "step": 61100
    },
    {
      "epoch": 4.674967534947674,
      "grad_norm": 4.5578083992004395,
      "learning_rate": 4.610419372087694e-05,
      "loss": 1.9276,
      "step": 61200
    },
    {
      "epoch": 4.682606370789092,
      "grad_norm": 4.542959213256836,
      "learning_rate": 4.609782802434243e-05,
      "loss": 2.0011,
      "step": 61300
    },
    {
      "epoch": 4.690245206630509,
      "grad_norm": 4.646629333496094,
      "learning_rate": 4.609146232780791e-05,
      "loss": 1.9862,
      "step": 61400
    },
    {
      "epoch": 4.6978840424719275,
      "grad_norm": 3.342839002609253,
      "learning_rate": 4.6085096631273394e-05,
      "loss": 1.9154,
      "step": 61500
    },
    {
      "epoch": 4.705522878313345,
      "grad_norm": 6.490499496459961,
      "learning_rate": 4.6078730934738884e-05,
      "loss": 1.9746,
      "step": 61600
    },
    {
      "epoch": 4.713161714154763,
      "grad_norm": 5.257203578948975,
      "learning_rate": 4.607236523820437e-05,
      "loss": 1.9506,
      "step": 61700
    },
    {
      "epoch": 4.720800549996181,
      "grad_norm": 5.024448394775391,
      "learning_rate": 4.606599954166985e-05,
      "loss": 1.9206,
      "step": 61800
    },
    {
      "epoch": 4.728439385837598,
      "grad_norm": 5.991445064544678,
      "learning_rate": 4.6059633845135335e-05,
      "loss": 1.9803,
      "step": 61900
    },
    {
      "epoch": 4.7360782216790165,
      "grad_norm": 7.969479560852051,
      "learning_rate": 4.605326814860082e-05,
      "loss": 1.9792,
      "step": 62000
    },
    {
      "epoch": 4.743717057520434,
      "grad_norm": 6.069757461547852,
      "learning_rate": 4.604690245206631e-05,
      "loss": 2.0104,
      "step": 62100
    },
    {
      "epoch": 4.751355893361851,
      "grad_norm": 5.458887100219727,
      "learning_rate": 4.604053675553179e-05,
      "loss": 1.9505,
      "step": 62200
    },
    {
      "epoch": 4.75899472920327,
      "grad_norm": 4.88452672958374,
      "learning_rate": 4.6034171058997276e-05,
      "loss": 2.0568,
      "step": 62300
    },
    {
      "epoch": 4.766633565044687,
      "grad_norm": 4.410565376281738,
      "learning_rate": 4.602780536246276e-05,
      "loss": 1.914,
      "step": 62400
    },
    {
      "epoch": 4.774272400886105,
      "grad_norm": 4.190004348754883,
      "learning_rate": 4.602143966592824e-05,
      "loss": 1.9409,
      "step": 62500
    },
    {
      "epoch": 4.781911236727523,
      "grad_norm": 5.347715377807617,
      "learning_rate": 4.601507396939373e-05,
      "loss": 1.9586,
      "step": 62600
    },
    {
      "epoch": 4.78955007256894,
      "grad_norm": 4.3668131828308105,
      "learning_rate": 4.600870827285922e-05,
      "loss": 1.8453,
      "step": 62700
    },
    {
      "epoch": 4.797188908410359,
      "grad_norm": 6.341158390045166,
      "learning_rate": 4.60023425763247e-05,
      "loss": 1.962,
      "step": 62800
    },
    {
      "epoch": 4.804827744251776,
      "grad_norm": 3.948110342025757,
      "learning_rate": 4.5995976879790184e-05,
      "loss": 1.8645,
      "step": 62900
    },
    {
      "epoch": 4.812466580093194,
      "grad_norm": 4.195403099060059,
      "learning_rate": 4.5989611183255674e-05,
      "loss": 2.0641,
      "step": 63000
    },
    {
      "epoch": 4.820105415934612,
      "grad_norm": 4.517614364624023,
      "learning_rate": 4.598324548672116e-05,
      "loss": 2.0867,
      "step": 63100
    },
    {
      "epoch": 4.827744251776029,
      "grad_norm": 5.445192813873291,
      "learning_rate": 4.597687979018664e-05,
      "loss": 1.9522,
      "step": 63200
    },
    {
      "epoch": 4.835383087617447,
      "grad_norm": 6.436577320098877,
      "learning_rate": 4.597051409365213e-05,
      "loss": 2.0056,
      "step": 63300
    },
    {
      "epoch": 4.843021923458865,
      "grad_norm": 6.167810916900635,
      "learning_rate": 4.5964148397117615e-05,
      "loss": 2.1072,
      "step": 63400
    },
    {
      "epoch": 4.850660759300283,
      "grad_norm": 8.345593452453613,
      "learning_rate": 4.59577827005831e-05,
      "loss": 2.0202,
      "step": 63500
    },
    {
      "epoch": 4.8582995951417,
      "grad_norm": 5.288193702697754,
      "learning_rate": 4.595141700404859e-05,
      "loss": 1.9672,
      "step": 63600
    },
    {
      "epoch": 4.865938430983118,
      "grad_norm": 6.5141425132751465,
      "learning_rate": 4.594505130751407e-05,
      "loss": 2.0216,
      "step": 63700
    },
    {
      "epoch": 4.873577266824536,
      "grad_norm": 4.371536731719971,
      "learning_rate": 4.5938685610979556e-05,
      "loss": 2.0604,
      "step": 63800
    },
    {
      "epoch": 4.881216102665954,
      "grad_norm": 5.45941162109375,
      "learning_rate": 4.5932319914445046e-05,
      "loss": 1.9635,
      "step": 63900
    },
    {
      "epoch": 4.888854938507372,
      "grad_norm": 6.135395050048828,
      "learning_rate": 4.592595421791053e-05,
      "loss": 2.03,
      "step": 64000
    },
    {
      "epoch": 4.896493774348789,
      "grad_norm": 5.999543190002441,
      "learning_rate": 4.591958852137601e-05,
      "loss": 2.027,
      "step": 64100
    },
    {
      "epoch": 4.9041326101902065,
      "grad_norm": 4.519680500030518,
      "learning_rate": 4.59132228248415e-05,
      "loss": 1.9429,
      "step": 64200
    },
    {
      "epoch": 4.911771446031625,
      "grad_norm": 5.376014232635498,
      "learning_rate": 4.590685712830698e-05,
      "loss": 2.0703,
      "step": 64300
    },
    {
      "epoch": 4.919410281873042,
      "grad_norm": 5.949662685394287,
      "learning_rate": 4.590049143177247e-05,
      "loss": 1.9921,
      "step": 64400
    },
    {
      "epoch": 4.927049117714461,
      "grad_norm": 4.4464497566223145,
      "learning_rate": 4.5894125735237954e-05,
      "loss": 2.0386,
      "step": 64500
    },
    {
      "epoch": 4.934687953555878,
      "grad_norm": 6.31113862991333,
      "learning_rate": 4.588776003870344e-05,
      "loss": 1.9639,
      "step": 64600
    },
    {
      "epoch": 4.9423267893972955,
      "grad_norm": 4.842045783996582,
      "learning_rate": 4.588139434216892e-05,
      "loss": 1.97,
      "step": 64700
    },
    {
      "epoch": 4.949965625238714,
      "grad_norm": 5.735503196716309,
      "learning_rate": 4.587502864563441e-05,
      "loss": 1.9933,
      "step": 64800
    },
    {
      "epoch": 4.957604461080131,
      "grad_norm": 5.26561975479126,
      "learning_rate": 4.5868662949099895e-05,
      "loss": 2.0812,
      "step": 64900
    },
    {
      "epoch": 4.96524329692155,
      "grad_norm": 5.26536750793457,
      "learning_rate": 4.586229725256538e-05,
      "loss": 1.9652,
      "step": 65000
    },
    {
      "epoch": 4.972882132762967,
      "grad_norm": 4.055658340454102,
      "learning_rate": 4.585593155603086e-05,
      "loss": 1.9782,
      "step": 65100
    },
    {
      "epoch": 4.9805209686043845,
      "grad_norm": 4.685146331787109,
      "learning_rate": 4.5849565859496346e-05,
      "loss": 1.9564,
      "step": 65200
    },
    {
      "epoch": 4.988159804445802,
      "grad_norm": 4.63474702835083,
      "learning_rate": 4.5843200162961836e-05,
      "loss": 1.9549,
      "step": 65300
    },
    {
      "epoch": 4.99579864028722,
      "grad_norm": 8.4915189743042,
      "learning_rate": 4.583683446642732e-05,
      "loss": 2.0123,
      "step": 65400
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.9182157516479492,
      "eval_runtime": 3.0307,
      "eval_samples_per_second": 227.672,
      "eval_steps_per_second": 227.672,
      "step": 65455
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7551203966140747,
      "eval_runtime": 57.4922,
      "eval_samples_per_second": 227.7,
      "eval_steps_per_second": 227.7,
      "step": 65455
    },
    {
      "epoch": 5.003437476128638,
      "grad_norm": 6.137702465057373,
      "learning_rate": 4.58304687698928e-05,
      "loss": 2.0214,
      "step": 65500
    },
    {
      "epoch": 5.011076311970056,
      "grad_norm": 4.753273010253906,
      "learning_rate": 4.582410307335829e-05,
      "loss": 1.9009,
      "step": 65600
    },
    {
      "epoch": 5.0187151478114735,
      "grad_norm": 4.957821369171143,
      "learning_rate": 4.581773737682377e-05,
      "loss": 1.9908,
      "step": 65700
    },
    {
      "epoch": 5.026353983652891,
      "grad_norm": 4.568780422210693,
      "learning_rate": 4.581137168028926e-05,
      "loss": 1.9197,
      "step": 65800
    },
    {
      "epoch": 5.033992819494309,
      "grad_norm": 7.433024883270264,
      "learning_rate": 4.5805005983754744e-05,
      "loss": 1.9642,
      "step": 65900
    },
    {
      "epoch": 5.041631655335727,
      "grad_norm": 5.09379768371582,
      "learning_rate": 4.579864028722023e-05,
      "loss": 2.1104,
      "step": 66000
    },
    {
      "epoch": 5.049270491177144,
      "grad_norm": 4.953510284423828,
      "learning_rate": 4.579227459068571e-05,
      "loss": 1.9214,
      "step": 66100
    },
    {
      "epoch": 5.0569093270185625,
      "grad_norm": 4.331299781799316,
      "learning_rate": 4.57859088941512e-05,
      "loss": 1.9283,
      "step": 66200
    },
    {
      "epoch": 5.06454816285998,
      "grad_norm": 5.812013149261475,
      "learning_rate": 4.5779543197616685e-05,
      "loss": 1.9552,
      "step": 66300
    },
    {
      "epoch": 5.072186998701398,
      "grad_norm": 4.646178722381592,
      "learning_rate": 4.577317750108217e-05,
      "loss": 1.8975,
      "step": 66400
    },
    {
      "epoch": 5.079825834542816,
      "grad_norm": 5.573010444641113,
      "learning_rate": 4.576681180454765e-05,
      "loss": 2.061,
      "step": 66500
    },
    {
      "epoch": 5.087464670384233,
      "grad_norm": 7.208837509155273,
      "learning_rate": 4.5760446108013136e-05,
      "loss": 2.0014,
      "step": 66600
    },
    {
      "epoch": 5.095103506225652,
      "grad_norm": 6.60245943069458,
      "learning_rate": 4.5754080411478626e-05,
      "loss": 1.8657,
      "step": 66700
    },
    {
      "epoch": 5.102742342067069,
      "grad_norm": 5.491392135620117,
      "learning_rate": 4.574771471494411e-05,
      "loss": 2.0405,
      "step": 66800
    },
    {
      "epoch": 5.1103811779084864,
      "grad_norm": 5.405097484588623,
      "learning_rate": 4.574134901840959e-05,
      "loss": 1.946,
      "step": 66900
    },
    {
      "epoch": 5.118020013749905,
      "grad_norm": 4.340205192565918,
      "learning_rate": 4.573498332187508e-05,
      "loss": 1.8688,
      "step": 67000
    },
    {
      "epoch": 5.125658849591322,
      "grad_norm": 6.297792911529541,
      "learning_rate": 4.572861762534057e-05,
      "loss": 1.9069,
      "step": 67100
    },
    {
      "epoch": 5.13329768543274,
      "grad_norm": 4.246395111083984,
      "learning_rate": 4.572225192880605e-05,
      "loss": 1.9784,
      "step": 67200
    },
    {
      "epoch": 5.140936521274158,
      "grad_norm": 5.896659851074219,
      "learning_rate": 4.571588623227154e-05,
      "loss": 1.9865,
      "step": 67300
    },
    {
      "epoch": 5.1485753571155755,
      "grad_norm": 4.17970609664917,
      "learning_rate": 4.5709520535737024e-05,
      "loss": 1.9236,
      "step": 67400
    },
    {
      "epoch": 5.156214192956993,
      "grad_norm": 5.000166416168213,
      "learning_rate": 4.570315483920251e-05,
      "loss": 2.0424,
      "step": 67500
    },
    {
      "epoch": 5.163853028798411,
      "grad_norm": 4.6702880859375,
      "learning_rate": 4.5696789142668e-05,
      "loss": 1.9265,
      "step": 67600
    },
    {
      "epoch": 5.171491864639829,
      "grad_norm": 5.210743427276611,
      "learning_rate": 4.569042344613348e-05,
      "loss": 2.0642,
      "step": 67700
    },
    {
      "epoch": 5.179130700481247,
      "grad_norm": 5.381895065307617,
      "learning_rate": 4.5684057749598965e-05,
      "loss": 2.0521,
      "step": 67800
    },
    {
      "epoch": 5.1867695363226645,
      "grad_norm": 4.628066062927246,
      "learning_rate": 4.567769205306445e-05,
      "loss": 1.9628,
      "step": 67900
    },
    {
      "epoch": 5.194408372164082,
      "grad_norm": 4.6269683837890625,
      "learning_rate": 4.567132635652993e-05,
      "loss": 1.8891,
      "step": 68000
    },
    {
      "epoch": 5.2020472080055,
      "grad_norm": 4.674765586853027,
      "learning_rate": 4.566496065999542e-05,
      "loss": 1.9843,
      "step": 68100
    },
    {
      "epoch": 5.209686043846918,
      "grad_norm": 5.925546646118164,
      "learning_rate": 4.5658594963460906e-05,
      "loss": 1.9646,
      "step": 68200
    },
    {
      "epoch": 5.217324879688335,
      "grad_norm": 4.727754592895508,
      "learning_rate": 4.565222926692639e-05,
      "loss": 1.9732,
      "step": 68300
    },
    {
      "epoch": 5.2249637155297535,
      "grad_norm": 5.026760578155518,
      "learning_rate": 4.564586357039187e-05,
      "loss": 1.989,
      "step": 68400
    },
    {
      "epoch": 5.232602551371171,
      "grad_norm": 5.211971282958984,
      "learning_rate": 4.5639497873857363e-05,
      "loss": 1.9859,
      "step": 68500
    },
    {
      "epoch": 5.240241387212588,
      "grad_norm": 6.910489559173584,
      "learning_rate": 4.563313217732285e-05,
      "loss": 1.9408,
      "step": 68600
    },
    {
      "epoch": 5.247880223054007,
      "grad_norm": 6.91651725769043,
      "learning_rate": 4.562676648078833e-05,
      "loss": 1.8975,
      "step": 68700
    },
    {
      "epoch": 5.255519058895424,
      "grad_norm": 4.443906307220459,
      "learning_rate": 4.5620400784253814e-05,
      "loss": 1.9849,
      "step": 68800
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 5.428321361541748,
      "learning_rate": 4.56140350877193e-05,
      "loss": 1.8847,
      "step": 68900
    },
    {
      "epoch": 5.27079673057826,
      "grad_norm": 5.2514448165893555,
      "learning_rate": 4.560766939118479e-05,
      "loss": 1.9909,
      "step": 69000
    },
    {
      "epoch": 5.278435566419677,
      "grad_norm": 5.702043533325195,
      "learning_rate": 4.560130369465027e-05,
      "loss": 1.9139,
      "step": 69100
    },
    {
      "epoch": 5.286074402261096,
      "grad_norm": 5.4538469314575195,
      "learning_rate": 4.5594937998115755e-05,
      "loss": 1.986,
      "step": 69200
    },
    {
      "epoch": 5.293713238102513,
      "grad_norm": 5.525846481323242,
      "learning_rate": 4.558857230158124e-05,
      "loss": 1.8631,
      "step": 69300
    },
    {
      "epoch": 5.301352073943931,
      "grad_norm": 6.695493698120117,
      "learning_rate": 4.558220660504673e-05,
      "loss": 1.9244,
      "step": 69400
    },
    {
      "epoch": 5.308990909785349,
      "grad_norm": 4.024606704711914,
      "learning_rate": 4.557584090851221e-05,
      "loss": 1.9416,
      "step": 69500
    },
    {
      "epoch": 5.316629745626766,
      "grad_norm": 5.410897254943848,
      "learning_rate": 4.5569475211977696e-05,
      "loss": 1.9311,
      "step": 69600
    },
    {
      "epoch": 5.324268581468184,
      "grad_norm": 7.742481708526611,
      "learning_rate": 4.556310951544318e-05,
      "loss": 1.9801,
      "step": 69700
    },
    {
      "epoch": 5.331907417309602,
      "grad_norm": 4.748840808868408,
      "learning_rate": 4.555674381890866e-05,
      "loss": 1.9291,
      "step": 69800
    },
    {
      "epoch": 5.33954625315102,
      "grad_norm": 6.385909557342529,
      "learning_rate": 4.555037812237415e-05,
      "loss": 1.987,
      "step": 69900
    },
    {
      "epoch": 5.347185088992438,
      "grad_norm": 5.871299743652344,
      "learning_rate": 4.554401242583964e-05,
      "loss": 2.03,
      "step": 70000
    },
    {
      "epoch": 5.354823924833855,
      "grad_norm": 6.5986504554748535,
      "learning_rate": 4.553764672930512e-05,
      "loss": 2.0329,
      "step": 70100
    },
    {
      "epoch": 5.362462760675273,
      "grad_norm": 4.866158485412598,
      "learning_rate": 4.5531281032770604e-05,
      "loss": 1.9489,
      "step": 70200
    },
    {
      "epoch": 5.370101596516691,
      "grad_norm": 4.433085918426514,
      "learning_rate": 4.5524915336236094e-05,
      "loss": 1.9712,
      "step": 70300
    },
    {
      "epoch": 5.377740432358109,
      "grad_norm": 4.234788417816162,
      "learning_rate": 4.551854963970158e-05,
      "loss": 1.9446,
      "step": 70400
    },
    {
      "epoch": 5.385379268199526,
      "grad_norm": 4.877241134643555,
      "learning_rate": 4.551218394316706e-05,
      "loss": 1.9647,
      "step": 70500
    },
    {
      "epoch": 5.393018104040944,
      "grad_norm": 5.693511962890625,
      "learning_rate": 4.5505818246632545e-05,
      "loss": 1.9095,
      "step": 70600
    },
    {
      "epoch": 5.400656939882362,
      "grad_norm": 4.840832710266113,
      "learning_rate": 4.5499452550098035e-05,
      "loss": 1.8799,
      "step": 70700
    },
    {
      "epoch": 5.408295775723779,
      "grad_norm": 5.265841007232666,
      "learning_rate": 4.549308685356352e-05,
      "loss": 1.934,
      "step": 70800
    },
    {
      "epoch": 5.415934611565198,
      "grad_norm": 4.25891637802124,
      "learning_rate": 4.5486721157029e-05,
      "loss": 1.9413,
      "step": 70900
    },
    {
      "epoch": 5.423573447406615,
      "grad_norm": 6.699929237365723,
      "learning_rate": 4.548035546049449e-05,
      "loss": 1.9456,
      "step": 71000
    },
    {
      "epoch": 5.431212283248033,
      "grad_norm": 4.75304651260376,
      "learning_rate": 4.5473989763959976e-05,
      "loss": 1.9512,
      "step": 71100
    },
    {
      "epoch": 5.438851119089451,
      "grad_norm": 6.148152828216553,
      "learning_rate": 4.546762406742546e-05,
      "loss": 1.9624,
      "step": 71200
    },
    {
      "epoch": 5.446489954930868,
      "grad_norm": 3.9012699127197266,
      "learning_rate": 4.546125837089095e-05,
      "loss": 2.0012,
      "step": 71300
    },
    {
      "epoch": 5.454128790772287,
      "grad_norm": 5.309199333190918,
      "learning_rate": 4.5454892674356433e-05,
      "loss": 2.0269,
      "step": 71400
    },
    {
      "epoch": 5.461767626613704,
      "grad_norm": 6.914174556732178,
      "learning_rate": 4.544852697782192e-05,
      "loss": 1.8907,
      "step": 71500
    },
    {
      "epoch": 5.4694064624551215,
      "grad_norm": 6.744208812713623,
      "learning_rate": 4.54421612812874e-05,
      "loss": 1.9424,
      "step": 71600
    },
    {
      "epoch": 5.47704529829654,
      "grad_norm": 4.66702938079834,
      "learning_rate": 4.543579558475289e-05,
      "loss": 1.9437,
      "step": 71700
    },
    {
      "epoch": 5.484684134137957,
      "grad_norm": 8.35803508758545,
      "learning_rate": 4.5429429888218374e-05,
      "loss": 1.9971,
      "step": 71800
    },
    {
      "epoch": 5.492322969979375,
      "grad_norm": 5.199077606201172,
      "learning_rate": 4.542306419168386e-05,
      "loss": 1.9484,
      "step": 71900
    },
    {
      "epoch": 5.499961805820793,
      "grad_norm": 5.661782264709473,
      "learning_rate": 4.541669849514934e-05,
      "loss": 1.9173,
      "step": 72000
    },
    {
      "epoch": 5.5076006416622105,
      "grad_norm": 4.649659156799316,
      "learning_rate": 4.5410332798614825e-05,
      "loss": 1.9204,
      "step": 72100
    },
    {
      "epoch": 5.515239477503629,
      "grad_norm": 6.155493259429932,
      "learning_rate": 4.5403967102080315e-05,
      "loss": 2.0059,
      "step": 72200
    },
    {
      "epoch": 5.522878313345046,
      "grad_norm": 5.007844924926758,
      "learning_rate": 4.53976014055458e-05,
      "loss": 1.8812,
      "step": 72300
    },
    {
      "epoch": 5.530517149186464,
      "grad_norm": 5.220801830291748,
      "learning_rate": 4.539123570901128e-05,
      "loss": 1.9241,
      "step": 72400
    },
    {
      "epoch": 5.538155985027882,
      "grad_norm": 4.875824451446533,
      "learning_rate": 4.5384870012476766e-05,
      "loss": 1.9677,
      "step": 72500
    },
    {
      "epoch": 5.5457948208692995,
      "grad_norm": 6.7142205238342285,
      "learning_rate": 4.5378504315942256e-05,
      "loss": 1.9686,
      "step": 72600
    },
    {
      "epoch": 5.553433656710717,
      "grad_norm": 4.6450324058532715,
      "learning_rate": 4.537213861940774e-05,
      "loss": 2.0025,
      "step": 72700
    },
    {
      "epoch": 5.561072492552135,
      "grad_norm": 4.493157386779785,
      "learning_rate": 4.536577292287322e-05,
      "loss": 1.8157,
      "step": 72800
    },
    {
      "epoch": 5.568711328393553,
      "grad_norm": 5.36734676361084,
      "learning_rate": 4.535940722633871e-05,
      "loss": 1.8603,
      "step": 72900
    },
    {
      "epoch": 5.57635016423497,
      "grad_norm": 4.256871223449707,
      "learning_rate": 4.535304152980419e-05,
      "loss": 2.0113,
      "step": 73000
    },
    {
      "epoch": 5.5839890000763885,
      "grad_norm": 5.686335563659668,
      "learning_rate": 4.534667583326968e-05,
      "loss": 1.9383,
      "step": 73100
    },
    {
      "epoch": 5.591627835917806,
      "grad_norm": 4.419832229614258,
      "learning_rate": 4.5340310136735164e-05,
      "loss": 1.9504,
      "step": 73200
    },
    {
      "epoch": 5.599266671759224,
      "grad_norm": 6.313498020172119,
      "learning_rate": 4.533394444020065e-05,
      "loss": 1.9673,
      "step": 73300
    },
    {
      "epoch": 5.606905507600642,
      "grad_norm": 5.56048059463501,
      "learning_rate": 4.532757874366613e-05,
      "loss": 1.92,
      "step": 73400
    },
    {
      "epoch": 5.614544343442059,
      "grad_norm": 5.982112407684326,
      "learning_rate": 4.532121304713162e-05,
      "loss": 1.9381,
      "step": 73500
    },
    {
      "epoch": 5.6221831792834775,
      "grad_norm": 5.965380668640137,
      "learning_rate": 4.5314847350597105e-05,
      "loss": 1.9128,
      "step": 73600
    },
    {
      "epoch": 5.629822015124895,
      "grad_norm": 4.5175933837890625,
      "learning_rate": 4.530848165406259e-05,
      "loss": 2.0334,
      "step": 73700
    },
    {
      "epoch": 5.637460850966312,
      "grad_norm": 3.9150547981262207,
      "learning_rate": 4.530211595752807e-05,
      "loss": 1.9342,
      "step": 73800
    },
    {
      "epoch": 5.645099686807731,
      "grad_norm": 5.926520824432373,
      "learning_rate": 4.5295750260993556e-05,
      "loss": 1.9091,
      "step": 73900
    },
    {
      "epoch": 5.652738522649148,
      "grad_norm": 7.1926445960998535,
      "learning_rate": 4.5289384564459046e-05,
      "loss": 2.0075,
      "step": 74000
    },
    {
      "epoch": 5.660377358490566,
      "grad_norm": 5.4150166511535645,
      "learning_rate": 4.528301886792453e-05,
      "loss": 1.9182,
      "step": 74100
    },
    {
      "epoch": 5.668016194331984,
      "grad_norm": 4.129126071929932,
      "learning_rate": 4.527665317139001e-05,
      "loss": 1.9827,
      "step": 74200
    },
    {
      "epoch": 5.6756550301734014,
      "grad_norm": 6.892459392547607,
      "learning_rate": 4.52702874748555e-05,
      "loss": 2.0083,
      "step": 74300
    },
    {
      "epoch": 5.68329386601482,
      "grad_norm": 5.517465114593506,
      "learning_rate": 4.526392177832098e-05,
      "loss": 1.966,
      "step": 74400
    },
    {
      "epoch": 5.690932701856237,
      "grad_norm": 4.873973369598389,
      "learning_rate": 4.525755608178647e-05,
      "loss": 1.9927,
      "step": 74500
    },
    {
      "epoch": 5.698571537697655,
      "grad_norm": 4.352402210235596,
      "learning_rate": 4.5251190385251954e-05,
      "loss": 2.0126,
      "step": 74600
    },
    {
      "epoch": 5.706210373539073,
      "grad_norm": 4.469357967376709,
      "learning_rate": 4.524482468871744e-05,
      "loss": 1.9753,
      "step": 74700
    },
    {
      "epoch": 5.7138492093804905,
      "grad_norm": 7.5295562744140625,
      "learning_rate": 4.523845899218293e-05,
      "loss": 1.8737,
      "step": 74800
    },
    {
      "epoch": 5.721488045221908,
      "grad_norm": 5.772067546844482,
      "learning_rate": 4.523209329564841e-05,
      "loss": 2.0063,
      "step": 74900
    },
    {
      "epoch": 5.729126881063326,
      "grad_norm": 4.096925735473633,
      "learning_rate": 4.52257275991139e-05,
      "loss": 1.9087,
      "step": 75000
    },
    {
      "epoch": 5.736765716904744,
      "grad_norm": 7.565279483795166,
      "learning_rate": 4.5219361902579385e-05,
      "loss": 1.9367,
      "step": 75100
    },
    {
      "epoch": 5.744404552746161,
      "grad_norm": 9.746256828308105,
      "learning_rate": 4.521299620604487e-05,
      "loss": 1.9723,
      "step": 75200
    },
    {
      "epoch": 5.7520433885875795,
      "grad_norm": 6.366628646850586,
      "learning_rate": 4.520663050951035e-05,
      "loss": 1.9139,
      "step": 75300
    },
    {
      "epoch": 5.759682224428997,
      "grad_norm": 4.834724426269531,
      "learning_rate": 4.520026481297584e-05,
      "loss": 1.9831,
      "step": 75400
    },
    {
      "epoch": 5.767321060270415,
      "grad_norm": 4.890036582946777,
      "learning_rate": 4.5193899116441326e-05,
      "loss": 1.9392,
      "step": 75500
    },
    {
      "epoch": 5.774959896111833,
      "grad_norm": 5.9210004806518555,
      "learning_rate": 4.518753341990681e-05,
      "loss": 1.9871,
      "step": 75600
    },
    {
      "epoch": 5.78259873195325,
      "grad_norm": 5.184570789337158,
      "learning_rate": 4.518116772337229e-05,
      "loss": 2.0821,
      "step": 75700
    },
    {
      "epoch": 5.7902375677946685,
      "grad_norm": 4.012991428375244,
      "learning_rate": 4.5174802026837784e-05,
      "loss": 1.8902,
      "step": 75800
    },
    {
      "epoch": 5.797876403636086,
      "grad_norm": 3.7665839195251465,
      "learning_rate": 4.516843633030327e-05,
      "loss": 1.8136,
      "step": 75900
    },
    {
      "epoch": 5.805515239477503,
      "grad_norm": 5.176202774047852,
      "learning_rate": 4.516207063376875e-05,
      "loss": 2.0282,
      "step": 76000
    },
    {
      "epoch": 5.813154075318922,
      "grad_norm": 4.583958625793457,
      "learning_rate": 4.5155704937234234e-05,
      "loss": 1.9784,
      "step": 76100
    },
    {
      "epoch": 5.820792911160339,
      "grad_norm": 4.774069786071777,
      "learning_rate": 4.514933924069972e-05,
      "loss": 1.922,
      "step": 76200
    },
    {
      "epoch": 5.828431747001757,
      "grad_norm": 5.541167736053467,
      "learning_rate": 4.514297354416521e-05,
      "loss": 1.9009,
      "step": 76300
    },
    {
      "epoch": 5.836070582843175,
      "grad_norm": 4.450122356414795,
      "learning_rate": 4.513660784763069e-05,
      "loss": 1.9604,
      "step": 76400
    },
    {
      "epoch": 5.843709418684592,
      "grad_norm": 4.056705474853516,
      "learning_rate": 4.5130242151096175e-05,
      "loss": 2.0338,
      "step": 76500
    },
    {
      "epoch": 5.851348254526011,
      "grad_norm": 5.2177324295043945,
      "learning_rate": 4.512387645456166e-05,
      "loss": 1.9823,
      "step": 76600
    },
    {
      "epoch": 5.858987090367428,
      "grad_norm": 3.5976459980010986,
      "learning_rate": 4.511751075802714e-05,
      "loss": 1.9674,
      "step": 76700
    },
    {
      "epoch": 5.866625926208846,
      "grad_norm": 5.894534111022949,
      "learning_rate": 4.511114506149263e-05,
      "loss": 1.9051,
      "step": 76800
    },
    {
      "epoch": 5.874264762050263,
      "grad_norm": 5.045036315917969,
      "learning_rate": 4.5104779364958116e-05,
      "loss": 2.0245,
      "step": 76900
    },
    {
      "epoch": 5.881903597891681,
      "grad_norm": 4.626689910888672,
      "learning_rate": 4.50984136684236e-05,
      "loss": 1.8974,
      "step": 77000
    },
    {
      "epoch": 5.889542433733099,
      "grad_norm": 5.840139389038086,
      "learning_rate": 4.509204797188908e-05,
      "loss": 1.9333,
      "step": 77100
    },
    {
      "epoch": 5.897181269574517,
      "grad_norm": 6.163106441497803,
      "learning_rate": 4.5085682275354573e-05,
      "loss": 1.972,
      "step": 77200
    },
    {
      "epoch": 5.904820105415935,
      "grad_norm": 5.054567813873291,
      "learning_rate": 4.507931657882006e-05,
      "loss": 1.8799,
      "step": 77300
    },
    {
      "epoch": 5.912458941257352,
      "grad_norm": 3.715471029281616,
      "learning_rate": 4.507295088228554e-05,
      "loss": 1.9239,
      "step": 77400
    },
    {
      "epoch": 5.92009777709877,
      "grad_norm": 6.020042419433594,
      "learning_rate": 4.5066585185751024e-05,
      "loss": 1.9973,
      "step": 77500
    },
    {
      "epoch": 5.927736612940188,
      "grad_norm": 4.718035697937012,
      "learning_rate": 4.506021948921651e-05,
      "loss": 1.9184,
      "step": 77600
    },
    {
      "epoch": 5.935375448781606,
      "grad_norm": 6.004112720489502,
      "learning_rate": 4.5053853792682e-05,
      "loss": 1.9164,
      "step": 77700
    },
    {
      "epoch": 5.943014284623024,
      "grad_norm": 4.50485372543335,
      "learning_rate": 4.504748809614748e-05,
      "loss": 1.9029,
      "step": 77800
    },
    {
      "epoch": 5.950653120464441,
      "grad_norm": 5.192349910736084,
      "learning_rate": 4.5041122399612965e-05,
      "loss": 1.973,
      "step": 77900
    },
    {
      "epoch": 5.9582919563058585,
      "grad_norm": 9.964667320251465,
      "learning_rate": 4.503475670307845e-05,
      "loss": 1.9762,
      "step": 78000
    },
    {
      "epoch": 5.965930792147277,
      "grad_norm": 6.773638725280762,
      "learning_rate": 4.502839100654394e-05,
      "loss": 2.0127,
      "step": 78100
    },
    {
      "epoch": 5.973569627988694,
      "grad_norm": 6.9240031242370605,
      "learning_rate": 4.502202531000942e-05,
      "loss": 1.9725,
      "step": 78200
    },
    {
      "epoch": 5.981208463830113,
      "grad_norm": 5.077742576599121,
      "learning_rate": 4.5015659613474906e-05,
      "loss": 1.9704,
      "step": 78300
    },
    {
      "epoch": 5.98884729967153,
      "grad_norm": 6.7643723487854,
      "learning_rate": 4.500929391694039e-05,
      "loss": 1.9033,
      "step": 78400
    },
    {
      "epoch": 5.9964861355129475,
      "grad_norm": 6.140349388122559,
      "learning_rate": 4.500292822040588e-05,
      "loss": 1.9047,
      "step": 78500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.9135161638259888,
      "eval_runtime": 3.015,
      "eval_samples_per_second": 228.857,
      "eval_steps_per_second": 228.857,
      "step": 78546
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.733034610748291,
      "eval_runtime": 57.9731,
      "eval_samples_per_second": 225.812,
      "eval_steps_per_second": 225.812,
      "step": 78546
    },
    {
      "epoch": 6.004124971354366,
      "grad_norm": 5.904068946838379,
      "learning_rate": 4.499656252387136e-05,
      "loss": 1.9149,
      "step": 78600
    },
    {
      "epoch": 6.011763807195783,
      "grad_norm": 6.0260009765625,
      "learning_rate": 4.499019682733685e-05,
      "loss": 1.9242,
      "step": 78700
    },
    {
      "epoch": 6.019402643037201,
      "grad_norm": 4.907913684844971,
      "learning_rate": 4.498383113080234e-05,
      "loss": 1.7993,
      "step": 78800
    },
    {
      "epoch": 6.027041478878619,
      "grad_norm": 5.064624309539795,
      "learning_rate": 4.497746543426782e-05,
      "loss": 1.9019,
      "step": 78900
    },
    {
      "epoch": 6.0346803147200365,
      "grad_norm": 7.9353203773498535,
      "learning_rate": 4.497109973773331e-05,
      "loss": 1.927,
      "step": 79000
    },
    {
      "epoch": 6.042319150561455,
      "grad_norm": 4.443105697631836,
      "learning_rate": 4.4964734041198795e-05,
      "loss": 2.0084,
      "step": 79100
    },
    {
      "epoch": 6.049957986402872,
      "grad_norm": 4.915706634521484,
      "learning_rate": 4.495836834466428e-05,
      "loss": 1.9974,
      "step": 79200
    },
    {
      "epoch": 6.05759682224429,
      "grad_norm": 4.066573619842529,
      "learning_rate": 4.495200264812976e-05,
      "loss": 1.9866,
      "step": 79300
    },
    {
      "epoch": 6.065235658085708,
      "grad_norm": 5.437056541442871,
      "learning_rate": 4.4945636951595245e-05,
      "loss": 1.9389,
      "step": 79400
    },
    {
      "epoch": 6.0728744939271255,
      "grad_norm": 5.100918769836426,
      "learning_rate": 4.4939271255060735e-05,
      "loss": 1.9254,
      "step": 79500
    },
    {
      "epoch": 6.080513329768543,
      "grad_norm": 4.816897392272949,
      "learning_rate": 4.493290555852622e-05,
      "loss": 1.9281,
      "step": 79600
    },
    {
      "epoch": 6.088152165609961,
      "grad_norm": 6.231194019317627,
      "learning_rate": 4.49265398619917e-05,
      "loss": 1.9053,
      "step": 79700
    },
    {
      "epoch": 6.095791001451379,
      "grad_norm": 6.0748820304870605,
      "learning_rate": 4.4920174165457186e-05,
      "loss": 1.9582,
      "step": 79800
    },
    {
      "epoch": 6.103429837292796,
      "grad_norm": 4.4607462882995605,
      "learning_rate": 4.491380846892267e-05,
      "loss": 1.8958,
      "step": 79900
    },
    {
      "epoch": 6.1110686731342145,
      "grad_norm": 5.6619954109191895,
      "learning_rate": 4.490744277238816e-05,
      "loss": 1.999,
      "step": 80000
    },
    {
      "epoch": 6.118707508975632,
      "grad_norm": 3.6705126762390137,
      "learning_rate": 4.4901077075853643e-05,
      "loss": 1.9709,
      "step": 80100
    },
    {
      "epoch": 6.126346344817049,
      "grad_norm": 5.345674991607666,
      "learning_rate": 4.489471137931913e-05,
      "loss": 1.9225,
      "step": 80200
    },
    {
      "epoch": 6.133985180658468,
      "grad_norm": 5.591727256774902,
      "learning_rate": 4.488834568278461e-05,
      "loss": 1.8223,
      "step": 80300
    },
    {
      "epoch": 6.141624016499885,
      "grad_norm": 5.213663101196289,
      "learning_rate": 4.48819799862501e-05,
      "loss": 1.955,
      "step": 80400
    },
    {
      "epoch": 6.1492628523413035,
      "grad_norm": 4.702120780944824,
      "learning_rate": 4.4875614289715584e-05,
      "loss": 1.8721,
      "step": 80500
    },
    {
      "epoch": 6.156901688182721,
      "grad_norm": 5.206714630126953,
      "learning_rate": 4.486924859318107e-05,
      "loss": 1.9,
      "step": 80600
    },
    {
      "epoch": 6.164540524024138,
      "grad_norm": 5.208995819091797,
      "learning_rate": 4.486288289664655e-05,
      "loss": 1.8488,
      "step": 80700
    },
    {
      "epoch": 6.172179359865557,
      "grad_norm": 6.3213372230529785,
      "learning_rate": 4.4856517200112035e-05,
      "loss": 1.9035,
      "step": 80800
    },
    {
      "epoch": 6.179818195706974,
      "grad_norm": 4.823122024536133,
      "learning_rate": 4.4850151503577525e-05,
      "loss": 1.9726,
      "step": 80900
    },
    {
      "epoch": 6.187457031548392,
      "grad_norm": 5.2066731452941895,
      "learning_rate": 4.484378580704301e-05,
      "loss": 1.9628,
      "step": 81000
    },
    {
      "epoch": 6.19509586738981,
      "grad_norm": 5.329166412353516,
      "learning_rate": 4.483742011050849e-05,
      "loss": 2.0136,
      "step": 81100
    },
    {
      "epoch": 6.202734703231227,
      "grad_norm": 5.170571327209473,
      "learning_rate": 4.4831054413973976e-05,
      "loss": 1.985,
      "step": 81200
    },
    {
      "epoch": 6.210373539072645,
      "grad_norm": 7.5756144523620605,
      "learning_rate": 4.4824688717439466e-05,
      "loss": 1.9316,
      "step": 81300
    },
    {
      "epoch": 6.218012374914063,
      "grad_norm": 5.165488243103027,
      "learning_rate": 4.481832302090495e-05,
      "loss": 1.9175,
      "step": 81400
    },
    {
      "epoch": 6.225651210755481,
      "grad_norm": 5.0296478271484375,
      "learning_rate": 4.481195732437043e-05,
      "loss": 1.936,
      "step": 81500
    },
    {
      "epoch": 6.233290046596899,
      "grad_norm": 4.202127933502197,
      "learning_rate": 4.480559162783592e-05,
      "loss": 2.0359,
      "step": 81600
    },
    {
      "epoch": 6.2409288824383164,
      "grad_norm": 6.015239238739014,
      "learning_rate": 4.47992259313014e-05,
      "loss": 1.9006,
      "step": 81700
    },
    {
      "epoch": 6.248567718279734,
      "grad_norm": 3.763878583908081,
      "learning_rate": 4.479286023476689e-05,
      "loss": 1.8705,
      "step": 81800
    },
    {
      "epoch": 6.256206554121152,
      "grad_norm": 4.468059539794922,
      "learning_rate": 4.4786494538232374e-05,
      "loss": 1.8353,
      "step": 81900
    },
    {
      "epoch": 6.26384538996257,
      "grad_norm": 4.922019004821777,
      "learning_rate": 4.478012884169786e-05,
      "loss": 1.9731,
      "step": 82000
    },
    {
      "epoch": 6.271484225803987,
      "grad_norm": 6.819455623626709,
      "learning_rate": 4.477376314516334e-05,
      "loss": 1.9796,
      "step": 82100
    },
    {
      "epoch": 6.2791230616454055,
      "grad_norm": 5.424646854400635,
      "learning_rate": 4.476739744862883e-05,
      "loss": 2.0242,
      "step": 82200
    },
    {
      "epoch": 6.286761897486823,
      "grad_norm": 3.5880019664764404,
      "learning_rate": 4.4761031752094315e-05,
      "loss": 1.8438,
      "step": 82300
    },
    {
      "epoch": 6.29440073332824,
      "grad_norm": 5.634782314300537,
      "learning_rate": 4.47546660555598e-05,
      "loss": 1.9263,
      "step": 82400
    },
    {
      "epoch": 6.302039569169659,
      "grad_norm": 7.1964030265808105,
      "learning_rate": 4.474830035902529e-05,
      "loss": 1.854,
      "step": 82500
    },
    {
      "epoch": 6.309678405011076,
      "grad_norm": 5.158989906311035,
      "learning_rate": 4.474193466249077e-05,
      "loss": 1.9153,
      "step": 82600
    },
    {
      "epoch": 6.3173172408524945,
      "grad_norm": 7.915912628173828,
      "learning_rate": 4.4735568965956256e-05,
      "loss": 1.9643,
      "step": 82700
    },
    {
      "epoch": 6.324956076693912,
      "grad_norm": 7.484498500823975,
      "learning_rate": 4.4729203269421746e-05,
      "loss": 1.9716,
      "step": 82800
    },
    {
      "epoch": 6.332594912535329,
      "grad_norm": 4.917417049407959,
      "learning_rate": 4.472283757288723e-05,
      "loss": 1.8269,
      "step": 82900
    },
    {
      "epoch": 6.340233748376748,
      "grad_norm": 5.3463873863220215,
      "learning_rate": 4.4716471876352713e-05,
      "loss": 2.0003,
      "step": 83000
    },
    {
      "epoch": 6.347872584218165,
      "grad_norm": 5.176912307739258,
      "learning_rate": 4.47101061798182e-05,
      "loss": 1.8567,
      "step": 83100
    },
    {
      "epoch": 6.355511420059583,
      "grad_norm": 5.340311527252197,
      "learning_rate": 4.470374048328369e-05,
      "loss": 1.9113,
      "step": 83200
    },
    {
      "epoch": 6.363150255901001,
      "grad_norm": 5.24639368057251,
      "learning_rate": 4.469737478674917e-05,
      "loss": 1.9787,
      "step": 83300
    },
    {
      "epoch": 6.370789091742418,
      "grad_norm": 4.88355827331543,
      "learning_rate": 4.4691009090214654e-05,
      "loss": 1.9723,
      "step": 83400
    },
    {
      "epoch": 6.378427927583836,
      "grad_norm": 5.595099449157715,
      "learning_rate": 4.468464339368014e-05,
      "loss": 1.9588,
      "step": 83500
    },
    {
      "epoch": 6.386066763425254,
      "grad_norm": 4.738694667816162,
      "learning_rate": 4.467827769714563e-05,
      "loss": 1.9614,
      "step": 83600
    },
    {
      "epoch": 6.393705599266672,
      "grad_norm": 4.146310806274414,
      "learning_rate": 4.467191200061111e-05,
      "loss": 1.892,
      "step": 83700
    },
    {
      "epoch": 6.40134443510809,
      "grad_norm": 5.572367191314697,
      "learning_rate": 4.4665546304076595e-05,
      "loss": 1.9182,
      "step": 83800
    },
    {
      "epoch": 6.408983270949507,
      "grad_norm": 5.506856441497803,
      "learning_rate": 4.465918060754208e-05,
      "loss": 1.9682,
      "step": 83900
    },
    {
      "epoch": 6.416622106790925,
      "grad_norm": 6.118284225463867,
      "learning_rate": 4.465281491100756e-05,
      "loss": 1.9191,
      "step": 84000
    },
    {
      "epoch": 6.424260942632343,
      "grad_norm": 6.122117042541504,
      "learning_rate": 4.464644921447305e-05,
      "loss": 2.0094,
      "step": 84100
    },
    {
      "epoch": 6.431899778473761,
      "grad_norm": 5.517603874206543,
      "learning_rate": 4.4640083517938536e-05,
      "loss": 1.9283,
      "step": 84200
    },
    {
      "epoch": 6.439538614315178,
      "grad_norm": 5.055347442626953,
      "learning_rate": 4.463371782140402e-05,
      "loss": 1.9767,
      "step": 84300
    },
    {
      "epoch": 6.447177450156596,
      "grad_norm": 3.9393062591552734,
      "learning_rate": 4.46273521248695e-05,
      "loss": 1.8851,
      "step": 84400
    },
    {
      "epoch": 6.454816285998014,
      "grad_norm": 3.4724857807159424,
      "learning_rate": 4.4620986428334994e-05,
      "loss": 1.9186,
      "step": 84500
    },
    {
      "epoch": 6.462455121839431,
      "grad_norm": 3.792233943939209,
      "learning_rate": 4.461462073180048e-05,
      "loss": 1.9493,
      "step": 84600
    },
    {
      "epoch": 6.47009395768085,
      "grad_norm": 5.279947757720947,
      "learning_rate": 4.460825503526596e-05,
      "loss": 1.9512,
      "step": 84700
    },
    {
      "epoch": 6.477732793522267,
      "grad_norm": 7.234067916870117,
      "learning_rate": 4.4601889338731444e-05,
      "loss": 1.9075,
      "step": 84800
    },
    {
      "epoch": 6.485371629363685,
      "grad_norm": 5.501047611236572,
      "learning_rate": 4.459552364219693e-05,
      "loss": 1.9555,
      "step": 84900
    },
    {
      "epoch": 6.493010465205103,
      "grad_norm": 4.053994178771973,
      "learning_rate": 4.458915794566242e-05,
      "loss": 1.8989,
      "step": 85000
    },
    {
      "epoch": 6.50064930104652,
      "grad_norm": 5.6861572265625,
      "learning_rate": 4.45827922491279e-05,
      "loss": 1.9661,
      "step": 85100
    },
    {
      "epoch": 6.508288136887939,
      "grad_norm": 6.408703804016113,
      "learning_rate": 4.4576426552593385e-05,
      "loss": 2.0018,
      "step": 85200
    },
    {
      "epoch": 6.515926972729356,
      "grad_norm": 6.104319095611572,
      "learning_rate": 4.457006085605887e-05,
      "loss": 1.8136,
      "step": 85300
    },
    {
      "epoch": 6.5235658085707735,
      "grad_norm": 4.595305919647217,
      "learning_rate": 4.456369515952435e-05,
      "loss": 1.951,
      "step": 85400
    },
    {
      "epoch": 6.531204644412192,
      "grad_norm": 3.769369602203369,
      "learning_rate": 4.455732946298984e-05,
      "loss": 1.91,
      "step": 85500
    },
    {
      "epoch": 6.538843480253609,
      "grad_norm": 5.757225036621094,
      "learning_rate": 4.4550963766455326e-05,
      "loss": 1.9998,
      "step": 85600
    },
    {
      "epoch": 6.546482316095027,
      "grad_norm": 5.402205944061279,
      "learning_rate": 4.454459806992081e-05,
      "loss": 1.9171,
      "step": 85700
    },
    {
      "epoch": 6.554121151936445,
      "grad_norm": 4.131413459777832,
      "learning_rate": 4.453823237338629e-05,
      "loss": 1.9733,
      "step": 85800
    },
    {
      "epoch": 6.5617599877778625,
      "grad_norm": 6.339912414550781,
      "learning_rate": 4.4531866676851783e-05,
      "loss": 1.8762,
      "step": 85900
    },
    {
      "epoch": 6.569398823619281,
      "grad_norm": 7.006033897399902,
      "learning_rate": 4.452550098031727e-05,
      "loss": 1.9104,
      "step": 86000
    },
    {
      "epoch": 6.577037659460698,
      "grad_norm": 5.7486090660095215,
      "learning_rate": 4.451913528378275e-05,
      "loss": 1.839,
      "step": 86100
    },
    {
      "epoch": 6.584676495302116,
      "grad_norm": 9.521084785461426,
      "learning_rate": 4.451276958724824e-05,
      "loss": 1.9537,
      "step": 86200
    },
    {
      "epoch": 6.592315331143534,
      "grad_norm": 5.861858367919922,
      "learning_rate": 4.4506403890713724e-05,
      "loss": 1.8451,
      "step": 86300
    },
    {
      "epoch": 6.5999541669849515,
      "grad_norm": 5.300544261932373,
      "learning_rate": 4.450003819417921e-05,
      "loss": 1.8812,
      "step": 86400
    },
    {
      "epoch": 6.607593002826369,
      "grad_norm": 4.4408183097839355,
      "learning_rate": 4.44936724976447e-05,
      "loss": 1.9617,
      "step": 86500
    },
    {
      "epoch": 6.615231838667787,
      "grad_norm": 5.140110492706299,
      "learning_rate": 4.448730680111018e-05,
      "loss": 1.9114,
      "step": 86600
    },
    {
      "epoch": 6.622870674509205,
      "grad_norm": 7.422388076782227,
      "learning_rate": 4.4480941104575665e-05,
      "loss": 1.899,
      "step": 86700
    },
    {
      "epoch": 6.630509510350622,
      "grad_norm": 4.995553493499756,
      "learning_rate": 4.4474575408041156e-05,
      "loss": 1.9986,
      "step": 86800
    },
    {
      "epoch": 6.6381483461920405,
      "grad_norm": 5.366636753082275,
      "learning_rate": 4.446820971150664e-05,
      "loss": 1.9875,
      "step": 86900
    },
    {
      "epoch": 6.645787182033458,
      "grad_norm": 4.7010393142700195,
      "learning_rate": 4.446184401497212e-05,
      "loss": 1.9342,
      "step": 87000
    },
    {
      "epoch": 6.653426017874876,
      "grad_norm": 5.077617168426514,
      "learning_rate": 4.4455478318437606e-05,
      "loss": 1.9643,
      "step": 87100
    },
    {
      "epoch": 6.661064853716294,
      "grad_norm": 5.201131343841553,
      "learning_rate": 4.444911262190309e-05,
      "loss": 1.8797,
      "step": 87200
    },
    {
      "epoch": 6.668703689557711,
      "grad_norm": 4.063190460205078,
      "learning_rate": 4.444274692536858e-05,
      "loss": 2.0364,
      "step": 87300
    },
    {
      "epoch": 6.6763425253991295,
      "grad_norm": 5.14696741104126,
      "learning_rate": 4.4436381228834064e-05,
      "loss": 1.9559,
      "step": 87400
    },
    {
      "epoch": 6.683981361240547,
      "grad_norm": 5.87091588973999,
      "learning_rate": 4.443001553229955e-05,
      "loss": 1.8704,
      "step": 87500
    },
    {
      "epoch": 6.691620197081964,
      "grad_norm": 4.455717086791992,
      "learning_rate": 4.442364983576503e-05,
      "loss": 1.9506,
      "step": 87600
    },
    {
      "epoch": 6.699259032923383,
      "grad_norm": 6.018988609313965,
      "learning_rate": 4.441728413923052e-05,
      "loss": 1.8189,
      "step": 87700
    },
    {
      "epoch": 6.7068978687648,
      "grad_norm": 5.894256114959717,
      "learning_rate": 4.4410918442696004e-05,
      "loss": 1.9612,
      "step": 87800
    },
    {
      "epoch": 6.714536704606218,
      "grad_norm": 5.1913275718688965,
      "learning_rate": 4.440455274616149e-05,
      "loss": 1.879,
      "step": 87900
    },
    {
      "epoch": 6.722175540447636,
      "grad_norm": 4.610140800476074,
      "learning_rate": 4.439818704962697e-05,
      "loss": 1.9666,
      "step": 88000
    },
    {
      "epoch": 6.729814376289053,
      "grad_norm": 4.625627040863037,
      "learning_rate": 4.4391821353092455e-05,
      "loss": 1.9739,
      "step": 88100
    },
    {
      "epoch": 6.737453212130472,
      "grad_norm": 5.246644496917725,
      "learning_rate": 4.4385455656557945e-05,
      "loss": 1.9468,
      "step": 88200
    },
    {
      "epoch": 6.745092047971889,
      "grad_norm": 4.565964698791504,
      "learning_rate": 4.437908996002343e-05,
      "loss": 1.8917,
      "step": 88300
    },
    {
      "epoch": 6.752730883813307,
      "grad_norm": 7.794886589050293,
      "learning_rate": 4.437272426348891e-05,
      "loss": 1.8908,
      "step": 88400
    },
    {
      "epoch": 6.760369719654725,
      "grad_norm": 5.121796131134033,
      "learning_rate": 4.4366358566954396e-05,
      "loss": 1.8969,
      "step": 88500
    },
    {
      "epoch": 6.768008555496142,
      "grad_norm": 5.417040824890137,
      "learning_rate": 4.435999287041988e-05,
      "loss": 1.8477,
      "step": 88600
    },
    {
      "epoch": 6.77564739133756,
      "grad_norm": 7.016406536102295,
      "learning_rate": 4.435362717388537e-05,
      "loss": 1.9778,
      "step": 88700
    },
    {
      "epoch": 6.783286227178978,
      "grad_norm": 5.732724189758301,
      "learning_rate": 4.4347261477350853e-05,
      "loss": 1.8579,
      "step": 88800
    },
    {
      "epoch": 6.790925063020396,
      "grad_norm": 4.907149791717529,
      "learning_rate": 4.434089578081634e-05,
      "loss": 1.9975,
      "step": 88900
    },
    {
      "epoch": 6.798563898861813,
      "grad_norm": 4.811365127563477,
      "learning_rate": 4.433453008428182e-05,
      "loss": 1.8997,
      "step": 89000
    },
    {
      "epoch": 6.8062027347032314,
      "grad_norm": 4.583732604980469,
      "learning_rate": 4.432816438774731e-05,
      "loss": 1.951,
      "step": 89100
    },
    {
      "epoch": 6.813841570544649,
      "grad_norm": 4.723849773406982,
      "learning_rate": 4.4321798691212794e-05,
      "loss": 1.8945,
      "step": 89200
    },
    {
      "epoch": 6.821480406386067,
      "grad_norm": 7.965198040008545,
      "learning_rate": 4.431543299467828e-05,
      "loss": 1.9254,
      "step": 89300
    },
    {
      "epoch": 6.829119242227485,
      "grad_norm": 5.1803460121154785,
      "learning_rate": 4.430906729814376e-05,
      "loss": 1.8998,
      "step": 89400
    },
    {
      "epoch": 6.836758078068902,
      "grad_norm": 5.056241512298584,
      "learning_rate": 4.4302701601609245e-05,
      "loss": 1.8805,
      "step": 89500
    },
    {
      "epoch": 6.84439691391032,
      "grad_norm": 5.304034233093262,
      "learning_rate": 4.4296335905074735e-05,
      "loss": 1.8852,
      "step": 89600
    },
    {
      "epoch": 6.852035749751738,
      "grad_norm": 5.227508544921875,
      "learning_rate": 4.428997020854022e-05,
      "loss": 1.895,
      "step": 89700
    },
    {
      "epoch": 6.859674585593155,
      "grad_norm": 7.0499162673950195,
      "learning_rate": 4.42836045120057e-05,
      "loss": 1.9938,
      "step": 89800
    },
    {
      "epoch": 6.867313421434574,
      "grad_norm": 5.108076572418213,
      "learning_rate": 4.4277238815471186e-05,
      "loss": 2.0123,
      "step": 89900
    },
    {
      "epoch": 6.874952257275991,
      "grad_norm": 5.709650039672852,
      "learning_rate": 4.4270873118936676e-05,
      "loss": 1.9283,
      "step": 90000
    },
    {
      "epoch": 6.882591093117409,
      "grad_norm": 6.560221195220947,
      "learning_rate": 4.426450742240216e-05,
      "loss": 1.9795,
      "step": 90100
    },
    {
      "epoch": 6.890229928958827,
      "grad_norm": 4.747638702392578,
      "learning_rate": 4.425814172586765e-05,
      "loss": 1.9334,
      "step": 90200
    },
    {
      "epoch": 6.897868764800244,
      "grad_norm": 5.530727386474609,
      "learning_rate": 4.4251776029333134e-05,
      "loss": 1.8906,
      "step": 90300
    },
    {
      "epoch": 6.905507600641663,
      "grad_norm": 6.235299587249756,
      "learning_rate": 4.424541033279862e-05,
      "loss": 1.87,
      "step": 90400
    },
    {
      "epoch": 6.91314643648308,
      "grad_norm": 5.641899585723877,
      "learning_rate": 4.423904463626411e-05,
      "loss": 1.9412,
      "step": 90500
    },
    {
      "epoch": 6.920785272324498,
      "grad_norm": 4.567913055419922,
      "learning_rate": 4.423267893972959e-05,
      "loss": 2.0381,
      "step": 90600
    },
    {
      "epoch": 6.928424108165915,
      "grad_norm": 5.884411811828613,
      "learning_rate": 4.4226313243195074e-05,
      "loss": 1.832,
      "step": 90700
    },
    {
      "epoch": 6.936062944007333,
      "grad_norm": 7.638720512390137,
      "learning_rate": 4.421994754666056e-05,
      "loss": 2.0769,
      "step": 90800
    },
    {
      "epoch": 6.943701779848751,
      "grad_norm": 5.136369705200195,
      "learning_rate": 4.421358185012604e-05,
      "loss": 1.9503,
      "step": 90900
    },
    {
      "epoch": 6.951340615690169,
      "grad_norm": 3.418663501739502,
      "learning_rate": 4.420721615359153e-05,
      "loss": 1.9373,
      "step": 91000
    },
    {
      "epoch": 6.958979451531587,
      "grad_norm": 7.792577743530273,
      "learning_rate": 4.4200850457057015e-05,
      "loss": 1.8528,
      "step": 91100
    },
    {
      "epoch": 6.966618287373004,
      "grad_norm": 4.392591953277588,
      "learning_rate": 4.41944847605225e-05,
      "loss": 2.0193,
      "step": 91200
    },
    {
      "epoch": 6.974257123214422,
      "grad_norm": 5.989616870880127,
      "learning_rate": 4.418811906398798e-05,
      "loss": 1.8526,
      "step": 91300
    },
    {
      "epoch": 6.98189595905584,
      "grad_norm": 5.205139636993408,
      "learning_rate": 4.418175336745347e-05,
      "loss": 1.9493,
      "step": 91400
    },
    {
      "epoch": 6.989534794897258,
      "grad_norm": 4.314513683319092,
      "learning_rate": 4.4175387670918956e-05,
      "loss": 1.8668,
      "step": 91500
    },
    {
      "epoch": 6.997173630738676,
      "grad_norm": 3.5105326175689697,
      "learning_rate": 4.416902197438444e-05,
      "loss": 1.931,
      "step": 91600
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8962945938110352,
      "eval_runtime": 1.5919,
      "eval_samples_per_second": 433.454,
      "eval_steps_per_second": 433.454,
      "step": 91637
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.7074999809265137,
      "eval_runtime": 57.6513,
      "eval_samples_per_second": 227.072,
      "eval_steps_per_second": 227.072,
      "step": 91637
    },
    {
      "epoch": 7.004812466580093,
      "grad_norm": 4.7653889656066895,
      "learning_rate": 4.4162656277849923e-05,
      "loss": 1.892,
      "step": 91700
    },
    {
      "epoch": 7.012451302421511,
      "grad_norm": 4.855531692504883,
      "learning_rate": 4.415629058131541e-05,
      "loss": 1.8565,
      "step": 91800
    },
    {
      "epoch": 7.020090138262929,
      "grad_norm": 4.058398723602295,
      "learning_rate": 4.41499248847809e-05,
      "loss": 1.8917,
      "step": 91900
    },
    {
      "epoch": 7.027728974104346,
      "grad_norm": 5.318028450012207,
      "learning_rate": 4.414355918824638e-05,
      "loss": 1.9413,
      "step": 92000
    },
    {
      "epoch": 7.035367809945765,
      "grad_norm": 5.230833053588867,
      "learning_rate": 4.4137193491711864e-05,
      "loss": 1.8438,
      "step": 92100
    },
    {
      "epoch": 7.043006645787182,
      "grad_norm": 5.3205037117004395,
      "learning_rate": 4.413082779517735e-05,
      "loss": 2.0406,
      "step": 92200
    },
    {
      "epoch": 7.0506454816285995,
      "grad_norm": 4.438368797302246,
      "learning_rate": 4.412446209864284e-05,
      "loss": 1.9283,
      "step": 92300
    },
    {
      "epoch": 7.058284317470018,
      "grad_norm": 4.247425079345703,
      "learning_rate": 4.411809640210832e-05,
      "loss": 2.0039,
      "step": 92400
    },
    {
      "epoch": 7.065923153311435,
      "grad_norm": 6.024202823638916,
      "learning_rate": 4.4111730705573805e-05,
      "loss": 1.7512,
      "step": 92500
    },
    {
      "epoch": 7.073561989152853,
      "grad_norm": 4.6915717124938965,
      "learning_rate": 4.410536500903929e-05,
      "loss": 2.0237,
      "step": 92600
    },
    {
      "epoch": 7.081200824994271,
      "grad_norm": 5.808121681213379,
      "learning_rate": 4.409899931250477e-05,
      "loss": 1.9182,
      "step": 92700
    },
    {
      "epoch": 7.0888396608356885,
      "grad_norm": 6.564473628997803,
      "learning_rate": 4.409263361597026e-05,
      "loss": 1.9463,
      "step": 92800
    },
    {
      "epoch": 7.096478496677107,
      "grad_norm": 5.151823043823242,
      "learning_rate": 4.4086267919435746e-05,
      "loss": 1.8734,
      "step": 92900
    },
    {
      "epoch": 7.104117332518524,
      "grad_norm": 4.320760250091553,
      "learning_rate": 4.407990222290123e-05,
      "loss": 1.9472,
      "step": 93000
    },
    {
      "epoch": 7.111756168359942,
      "grad_norm": 5.771154403686523,
      "learning_rate": 4.407353652636671e-05,
      "loss": 1.8414,
      "step": 93100
    },
    {
      "epoch": 7.11939500420136,
      "grad_norm": 4.273876190185547,
      "learning_rate": 4.4067170829832204e-05,
      "loss": 1.9505,
      "step": 93200
    },
    {
      "epoch": 7.1270338400427775,
      "grad_norm": 4.12385892868042,
      "learning_rate": 4.406080513329769e-05,
      "loss": 1.8843,
      "step": 93300
    },
    {
      "epoch": 7.134672675884195,
      "grad_norm": 6.5187788009643555,
      "learning_rate": 4.405443943676317e-05,
      "loss": 1.8593,
      "step": 93400
    },
    {
      "epoch": 7.142311511725613,
      "grad_norm": 6.3286542892456055,
      "learning_rate": 4.4048073740228654e-05,
      "loss": 1.9735,
      "step": 93500
    },
    {
      "epoch": 7.149950347567031,
      "grad_norm": 4.717532157897949,
      "learning_rate": 4.404170804369414e-05,
      "loss": 1.8883,
      "step": 93600
    },
    {
      "epoch": 7.157589183408448,
      "grad_norm": 5.632380485534668,
      "learning_rate": 4.403534234715963e-05,
      "loss": 1.9289,
      "step": 93700
    },
    {
      "epoch": 7.1652280192498665,
      "grad_norm": 4.77441930770874,
      "learning_rate": 4.402897665062511e-05,
      "loss": 1.8785,
      "step": 93800
    },
    {
      "epoch": 7.172866855091284,
      "grad_norm": 5.279762268066406,
      "learning_rate": 4.4022610954090595e-05,
      "loss": 1.8203,
      "step": 93900
    },
    {
      "epoch": 7.180505690932701,
      "grad_norm": 5.7512617111206055,
      "learning_rate": 4.4016245257556085e-05,
      "loss": 1.868,
      "step": 94000
    },
    {
      "epoch": 7.18814452677412,
      "grad_norm": 4.989966869354248,
      "learning_rate": 4.400987956102157e-05,
      "loss": 1.9327,
      "step": 94100
    },
    {
      "epoch": 7.195783362615537,
      "grad_norm": 5.404567718505859,
      "learning_rate": 4.400351386448706e-05,
      "loss": 1.93,
      "step": 94200
    },
    {
      "epoch": 7.2034221984569555,
      "grad_norm": 5.418544292449951,
      "learning_rate": 4.399714816795254e-05,
      "loss": 1.8721,
      "step": 94300
    },
    {
      "epoch": 7.211061034298373,
      "grad_norm": 4.974229335784912,
      "learning_rate": 4.3990782471418026e-05,
      "loss": 1.8656,
      "step": 94400
    },
    {
      "epoch": 7.21869987013979,
      "grad_norm": 8.701114654541016,
      "learning_rate": 4.398441677488351e-05,
      "loss": 1.8352,
      "step": 94500
    },
    {
      "epoch": 7.226338705981209,
      "grad_norm": 4.673770427703857,
      "learning_rate": 4.3978051078349e-05,
      "loss": 1.9011,
      "step": 94600
    },
    {
      "epoch": 7.233977541822626,
      "grad_norm": 4.676106929779053,
      "learning_rate": 4.3971685381814484e-05,
      "loss": 1.9247,
      "step": 94700
    },
    {
      "epoch": 7.241616377664044,
      "grad_norm": 5.793699264526367,
      "learning_rate": 4.396531968527997e-05,
      "loss": 2.0065,
      "step": 94800
    },
    {
      "epoch": 7.249255213505462,
      "grad_norm": 4.6568708419799805,
      "learning_rate": 4.395895398874545e-05,
      "loss": 2.0023,
      "step": 94900
    },
    {
      "epoch": 7.256894049346879,
      "grad_norm": 6.698300361633301,
      "learning_rate": 4.3952588292210934e-05,
      "loss": 1.8786,
      "step": 95000
    },
    {
      "epoch": 7.264532885188297,
      "grad_norm": 5.2395853996276855,
      "learning_rate": 4.3946222595676425e-05,
      "loss": 1.8901,
      "step": 95100
    },
    {
      "epoch": 7.272171721029715,
      "grad_norm": 4.139072895050049,
      "learning_rate": 4.393985689914191e-05,
      "loss": 1.9092,
      "step": 95200
    },
    {
      "epoch": 7.279810556871133,
      "grad_norm": 5.80139684677124,
      "learning_rate": 4.393349120260739e-05,
      "loss": 2.0812,
      "step": 95300
    },
    {
      "epoch": 7.287449392712551,
      "grad_norm": 9.564288139343262,
      "learning_rate": 4.3927125506072875e-05,
      "loss": 1.8202,
      "step": 95400
    },
    {
      "epoch": 7.295088228553968,
      "grad_norm": 8.030281066894531,
      "learning_rate": 4.3920759809538366e-05,
      "loss": 1.9285,
      "step": 95500
    },
    {
      "epoch": 7.302727064395386,
      "grad_norm": 4.824806213378906,
      "learning_rate": 4.391439411300385e-05,
      "loss": 2.0351,
      "step": 95600
    },
    {
      "epoch": 7.310365900236804,
      "grad_norm": 5.361957550048828,
      "learning_rate": 4.390802841646933e-05,
      "loss": 1.9883,
      "step": 95700
    },
    {
      "epoch": 7.318004736078222,
      "grad_norm": 5.453330039978027,
      "learning_rate": 4.3901662719934816e-05,
      "loss": 2.0092,
      "step": 95800
    },
    {
      "epoch": 7.325643571919639,
      "grad_norm": 4.493899345397949,
      "learning_rate": 4.38952970234003e-05,
      "loss": 1.9585,
      "step": 95900
    },
    {
      "epoch": 7.333282407761057,
      "grad_norm": 4.283451080322266,
      "learning_rate": 4.388893132686579e-05,
      "loss": 1.9465,
      "step": 96000
    },
    {
      "epoch": 7.340921243602475,
      "grad_norm": 8.159506797790527,
      "learning_rate": 4.3882565630331274e-05,
      "loss": 1.8712,
      "step": 96100
    },
    {
      "epoch": 7.348560079443892,
      "grad_norm": 6.003170013427734,
      "learning_rate": 4.387619993379676e-05,
      "loss": 1.9259,
      "step": 96200
    },
    {
      "epoch": 7.356198915285311,
      "grad_norm": 7.913665771484375,
      "learning_rate": 4.386983423726224e-05,
      "loss": 1.8768,
      "step": 96300
    },
    {
      "epoch": 7.363837751126728,
      "grad_norm": 4.541495323181152,
      "learning_rate": 4.386346854072773e-05,
      "loss": 1.8338,
      "step": 96400
    },
    {
      "epoch": 7.3714765869681464,
      "grad_norm": 4.365513324737549,
      "learning_rate": 4.3857102844193214e-05,
      "loss": 1.8813,
      "step": 96500
    },
    {
      "epoch": 7.379115422809564,
      "grad_norm": 5.859654903411865,
      "learning_rate": 4.38507371476587e-05,
      "loss": 1.8927,
      "step": 96600
    },
    {
      "epoch": 7.386754258650981,
      "grad_norm": 4.638923168182373,
      "learning_rate": 4.384437145112418e-05,
      "loss": 1.8894,
      "step": 96700
    },
    {
      "epoch": 7.3943930944924,
      "grad_norm": 5.74122428894043,
      "learning_rate": 4.3838005754589665e-05,
      "loss": 1.9264,
      "step": 96800
    },
    {
      "epoch": 7.402031930333817,
      "grad_norm": 4.224832534790039,
      "learning_rate": 4.3831640058055155e-05,
      "loss": 1.8673,
      "step": 96900
    },
    {
      "epoch": 7.409670766175235,
      "grad_norm": 5.935471534729004,
      "learning_rate": 4.382527436152064e-05,
      "loss": 1.9697,
      "step": 97000
    },
    {
      "epoch": 7.417309602016653,
      "grad_norm": 6.379003524780273,
      "learning_rate": 4.381890866498612e-05,
      "loss": 1.9122,
      "step": 97100
    },
    {
      "epoch": 7.42494843785807,
      "grad_norm": 6.713342189788818,
      "learning_rate": 4.3812542968451606e-05,
      "loss": 1.9968,
      "step": 97200
    },
    {
      "epoch": 7.432587273699488,
      "grad_norm": 5.2732768058776855,
      "learning_rate": 4.380617727191709e-05,
      "loss": 1.8055,
      "step": 97300
    },
    {
      "epoch": 7.440226109540906,
      "grad_norm": 4.959253311157227,
      "learning_rate": 4.379981157538258e-05,
      "loss": 1.9007,
      "step": 97400
    },
    {
      "epoch": 7.447864945382324,
      "grad_norm": 5.911795616149902,
      "learning_rate": 4.3793445878848063e-05,
      "loss": 1.9011,
      "step": 97500
    },
    {
      "epoch": 7.455503781223742,
      "grad_norm": 5.375742435455322,
      "learning_rate": 4.378708018231355e-05,
      "loss": 1.8192,
      "step": 97600
    },
    {
      "epoch": 7.463142617065159,
      "grad_norm": 6.375746250152588,
      "learning_rate": 4.378071448577904e-05,
      "loss": 1.9659,
      "step": 97700
    },
    {
      "epoch": 7.470781452906577,
      "grad_norm": 5.067616939544678,
      "learning_rate": 4.377434878924452e-05,
      "loss": 1.93,
      "step": 97800
    },
    {
      "epoch": 7.478420288747995,
      "grad_norm": 3.695413112640381,
      "learning_rate": 4.3767983092710004e-05,
      "loss": 1.8962,
      "step": 97900
    },
    {
      "epoch": 7.486059124589413,
      "grad_norm": 5.808437347412109,
      "learning_rate": 4.3761617396175495e-05,
      "loss": 1.883,
      "step": 98000
    },
    {
      "epoch": 7.49369796043083,
      "grad_norm": 5.333313465118408,
      "learning_rate": 4.375525169964098e-05,
      "loss": 1.8558,
      "step": 98100
    },
    {
      "epoch": 7.501336796272248,
      "grad_norm": 6.168153285980225,
      "learning_rate": 4.374888600310646e-05,
      "loss": 1.8555,
      "step": 98200
    },
    {
      "epoch": 7.508975632113666,
      "grad_norm": 4.02331018447876,
      "learning_rate": 4.374252030657195e-05,
      "loss": 1.8518,
      "step": 98300
    },
    {
      "epoch": 7.516614467955083,
      "grad_norm": 4.9560370445251465,
      "learning_rate": 4.3736154610037436e-05,
      "loss": 1.9192,
      "step": 98400
    },
    {
      "epoch": 7.524253303796502,
      "grad_norm": 4.425736427307129,
      "learning_rate": 4.372978891350292e-05,
      "loss": 1.8993,
      "step": 98500
    },
    {
      "epoch": 7.531892139637919,
      "grad_norm": 6.167379379272461,
      "learning_rate": 4.37234232169684e-05,
      "loss": 1.9307,
      "step": 98600
    },
    {
      "epoch": 7.539530975479337,
      "grad_norm": 4.8537797927856445,
      "learning_rate": 4.371705752043389e-05,
      "loss": 1.8839,
      "step": 98700
    },
    {
      "epoch": 7.547169811320755,
      "grad_norm": 5.662596225738525,
      "learning_rate": 4.3710691823899376e-05,
      "loss": 1.9198,
      "step": 98800
    },
    {
      "epoch": 7.554808647162172,
      "grad_norm": 5.409363269805908,
      "learning_rate": 4.370432612736486e-05,
      "loss": 1.8837,
      "step": 98900
    },
    {
      "epoch": 7.562447483003591,
      "grad_norm": 4.958245754241943,
      "learning_rate": 4.3697960430830344e-05,
      "loss": 1.8822,
      "step": 99000
    },
    {
      "epoch": 7.570086318845008,
      "grad_norm": 7.68130350112915,
      "learning_rate": 4.369159473429583e-05,
      "loss": 1.8123,
      "step": 99100
    },
    {
      "epoch": 7.5777251546864255,
      "grad_norm": 5.989340305328369,
      "learning_rate": 4.368522903776132e-05,
      "loss": 1.9037,
      "step": 99200
    },
    {
      "epoch": 7.585363990527844,
      "grad_norm": 5.917642116546631,
      "learning_rate": 4.36788633412268e-05,
      "loss": 1.9177,
      "step": 99300
    },
    {
      "epoch": 7.593002826369261,
      "grad_norm": 6.259093284606934,
      "learning_rate": 4.3672497644692284e-05,
      "loss": 1.9912,
      "step": 99400
    },
    {
      "epoch": 7.600641662210679,
      "grad_norm": 5.739859580993652,
      "learning_rate": 4.366613194815777e-05,
      "loss": 1.862,
      "step": 99500
    },
    {
      "epoch": 7.608280498052097,
      "grad_norm": 6.05273962020874,
      "learning_rate": 4.365976625162325e-05,
      "loss": 1.8767,
      "step": 99600
    },
    {
      "epoch": 7.6159193338935145,
      "grad_norm": 4.461911201477051,
      "learning_rate": 4.365340055508874e-05,
      "loss": 1.8589,
      "step": 99700
    },
    {
      "epoch": 7.623558169734933,
      "grad_norm": 5.674760341644287,
      "learning_rate": 4.3647034858554225e-05,
      "loss": 1.9453,
      "step": 99800
    },
    {
      "epoch": 7.63119700557635,
      "grad_norm": 5.4024457931518555,
      "learning_rate": 4.364066916201971e-05,
      "loss": 1.9271,
      "step": 99900
    },
    {
      "epoch": 7.638835841417768,
      "grad_norm": 4.326831817626953,
      "learning_rate": 4.363430346548519e-05,
      "loss": 2.0027,
      "step": 100000
    },
    {
      "epoch": 7.646474677259186,
      "grad_norm": 4.974251747131348,
      "learning_rate": 4.362793776895068e-05,
      "loss": 1.9286,
      "step": 100100
    },
    {
      "epoch": 7.6541135131006035,
      "grad_norm": 4.804560661315918,
      "learning_rate": 4.3621572072416166e-05,
      "loss": 1.8856,
      "step": 100200
    },
    {
      "epoch": 7.661752348942021,
      "grad_norm": 5.244510650634766,
      "learning_rate": 4.361520637588165e-05,
      "loss": 1.9513,
      "step": 100300
    },
    {
      "epoch": 7.669391184783439,
      "grad_norm": 5.475290775299072,
      "learning_rate": 4.3608840679347133e-05,
      "loss": 1.9121,
      "step": 100400
    },
    {
      "epoch": 7.677030020624857,
      "grad_norm": 5.876389503479004,
      "learning_rate": 4.360247498281262e-05,
      "loss": 1.9292,
      "step": 100500
    },
    {
      "epoch": 7.684668856466274,
      "grad_norm": 4.047474384307861,
      "learning_rate": 4.359610928627811e-05,
      "loss": 1.8228,
      "step": 100600
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 5.195046901702881,
      "learning_rate": 4.358974358974359e-05,
      "loss": 1.8195,
      "step": 100700
    },
    {
      "epoch": 7.69994652814911,
      "grad_norm": 5.427616119384766,
      "learning_rate": 4.3583377893209074e-05,
      "loss": 1.9268,
      "step": 100800
    },
    {
      "epoch": 7.707585363990528,
      "grad_norm": 5.58164119720459,
      "learning_rate": 4.357701219667456e-05,
      "loss": 1.9513,
      "step": 100900
    },
    {
      "epoch": 7.715224199831946,
      "grad_norm": 7.171389579772949,
      "learning_rate": 4.357064650014005e-05,
      "loss": 1.9107,
      "step": 101000
    },
    {
      "epoch": 7.722863035673363,
      "grad_norm": 4.580379486083984,
      "learning_rate": 4.356428080360553e-05,
      "loss": 1.9696,
      "step": 101100
    },
    {
      "epoch": 7.7305018715147815,
      "grad_norm": 4.123369216918945,
      "learning_rate": 4.3557915107071015e-05,
      "loss": 1.8758,
      "step": 101200
    },
    {
      "epoch": 7.738140707356199,
      "grad_norm": 5.483968734741211,
      "learning_rate": 4.35515494105365e-05,
      "loss": 1.8744,
      "step": 101300
    },
    {
      "epoch": 7.745779543197616,
      "grad_norm": 5.096212387084961,
      "learning_rate": 4.354518371400199e-05,
      "loss": 1.944,
      "step": 101400
    },
    {
      "epoch": 7.753418379039035,
      "grad_norm": 6.531468391418457,
      "learning_rate": 4.353881801746747e-05,
      "loss": 1.8565,
      "step": 101500
    },
    {
      "epoch": 7.761057214880452,
      "grad_norm": 4.961297512054443,
      "learning_rate": 4.3532452320932956e-05,
      "loss": 1.7934,
      "step": 101600
    },
    {
      "epoch": 7.76869605072187,
      "grad_norm": 5.943989276885986,
      "learning_rate": 4.3526086624398446e-05,
      "loss": 1.9727,
      "step": 101700
    },
    {
      "epoch": 7.776334886563288,
      "grad_norm": 6.423222541809082,
      "learning_rate": 4.351972092786393e-05,
      "loss": 1.8788,
      "step": 101800
    },
    {
      "epoch": 7.783973722404705,
      "grad_norm": 5.675902366638184,
      "learning_rate": 4.3513355231329414e-05,
      "loss": 1.9214,
      "step": 101900
    },
    {
      "epoch": 7.791612558246124,
      "grad_norm": 5.880340576171875,
      "learning_rate": 4.3506989534794904e-05,
      "loss": 1.8544,
      "step": 102000
    },
    {
      "epoch": 7.799251394087541,
      "grad_norm": 5.277304172515869,
      "learning_rate": 4.350062383826039e-05,
      "loss": 1.8977,
      "step": 102100
    },
    {
      "epoch": 7.806890229928959,
      "grad_norm": 4.63175630569458,
      "learning_rate": 4.349425814172587e-05,
      "loss": 1.92,
      "step": 102200
    },
    {
      "epoch": 7.814529065770377,
      "grad_norm": 4.5592546463012695,
      "learning_rate": 4.3487892445191354e-05,
      "loss": 1.9482,
      "step": 102300
    },
    {
      "epoch": 7.822167901611794,
      "grad_norm": 4.465346336364746,
      "learning_rate": 4.3481526748656845e-05,
      "loss": 1.9138,
      "step": 102400
    },
    {
      "epoch": 7.829806737453212,
      "grad_norm": 4.813109397888184,
      "learning_rate": 4.347516105212233e-05,
      "loss": 1.8024,
      "step": 102500
    },
    {
      "epoch": 7.83744557329463,
      "grad_norm": 4.7985334396362305,
      "learning_rate": 4.346879535558781e-05,
      "loss": 1.9515,
      "step": 102600
    },
    {
      "epoch": 7.845084409136048,
      "grad_norm": 4.826459884643555,
      "learning_rate": 4.3462429659053295e-05,
      "loss": 1.9558,
      "step": 102700
    },
    {
      "epoch": 7.852723244977465,
      "grad_norm": 6.383771896362305,
      "learning_rate": 4.345606396251878e-05,
      "loss": 1.8676,
      "step": 102800
    },
    {
      "epoch": 7.860362080818883,
      "grad_norm": 5.79874324798584,
      "learning_rate": 4.344969826598427e-05,
      "loss": 1.8275,
      "step": 102900
    },
    {
      "epoch": 7.868000916660301,
      "grad_norm": 4.769158363342285,
      "learning_rate": 4.344333256944975e-05,
      "loss": 1.8732,
      "step": 103000
    },
    {
      "epoch": 7.875639752501719,
      "grad_norm": 5.05056095123291,
      "learning_rate": 4.3436966872915236e-05,
      "loss": 1.9047,
      "step": 103100
    },
    {
      "epoch": 7.883278588343137,
      "grad_norm": 3.457874298095703,
      "learning_rate": 4.343060117638072e-05,
      "loss": 1.9956,
      "step": 103200
    },
    {
      "epoch": 7.890917424184554,
      "grad_norm": 5.513462066650391,
      "learning_rate": 4.342423547984621e-05,
      "loss": 1.8719,
      "step": 103300
    },
    {
      "epoch": 7.8985562600259716,
      "grad_norm": 3.9101736545562744,
      "learning_rate": 4.3417869783311694e-05,
      "loss": 1.9815,
      "step": 103400
    },
    {
      "epoch": 7.90619509586739,
      "grad_norm": 4.772362232208252,
      "learning_rate": 4.341150408677718e-05,
      "loss": 1.9457,
      "step": 103500
    },
    {
      "epoch": 7.913833931708807,
      "grad_norm": 4.7447733879089355,
      "learning_rate": 4.340513839024266e-05,
      "loss": 1.9304,
      "step": 103600
    },
    {
      "epoch": 7.921472767550226,
      "grad_norm": 4.142722129821777,
      "learning_rate": 4.3398772693708144e-05,
      "loss": 1.9367,
      "step": 103700
    },
    {
      "epoch": 7.929111603391643,
      "grad_norm": 5.845932960510254,
      "learning_rate": 4.3392406997173635e-05,
      "loss": 1.8774,
      "step": 103800
    },
    {
      "epoch": 7.936750439233061,
      "grad_norm": 4.8506951332092285,
      "learning_rate": 4.338604130063912e-05,
      "loss": 1.8117,
      "step": 103900
    },
    {
      "epoch": 7.944389275074479,
      "grad_norm": 5.925570964813232,
      "learning_rate": 4.33796756041046e-05,
      "loss": 1.9702,
      "step": 104000
    },
    {
      "epoch": 7.952028110915896,
      "grad_norm": 5.126280307769775,
      "learning_rate": 4.3373309907570085e-05,
      "loss": 1.8942,
      "step": 104100
    },
    {
      "epoch": 7.959666946757315,
      "grad_norm": 5.213882923126221,
      "learning_rate": 4.3366944211035576e-05,
      "loss": 2.0165,
      "step": 104200
    },
    {
      "epoch": 7.967305782598732,
      "grad_norm": 6.405158042907715,
      "learning_rate": 4.336057851450106e-05,
      "loss": 1.9437,
      "step": 104300
    },
    {
      "epoch": 7.97494461844015,
      "grad_norm": 5.662927627563477,
      "learning_rate": 4.335421281796654e-05,
      "loss": 1.9743,
      "step": 104400
    },
    {
      "epoch": 7.982583454281567,
      "grad_norm": 5.570472717285156,
      "learning_rate": 4.3347847121432026e-05,
      "loss": 1.9107,
      "step": 104500
    },
    {
      "epoch": 7.990222290122985,
      "grad_norm": 3.8273117542266846,
      "learning_rate": 4.334148142489751e-05,
      "loss": 1.9419,
      "step": 104600
    },
    {
      "epoch": 7.997861125964403,
      "grad_norm": 3.7786877155303955,
      "learning_rate": 4.3335115728363e-05,
      "loss": 1.922,
      "step": 104700
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8718880414962769,
      "eval_runtime": 1.4883,
      "eval_samples_per_second": 463.621,
      "eval_steps_per_second": 463.621,
      "step": 104728
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.6798288822174072,
      "eval_runtime": 27.8367,
      "eval_samples_per_second": 470.279,
      "eval_steps_per_second": 470.279,
      "step": 104728
    },
    {
      "epoch": 8.005499961805821,
      "grad_norm": 5.270654678344727,
      "learning_rate": 4.3328750031828484e-05,
      "loss": 1.9149,
      "step": 104800
    },
    {
      "epoch": 8.013138797647239,
      "grad_norm": 4.946557521820068,
      "learning_rate": 4.332238433529397e-05,
      "loss": 1.8872,
      "step": 104900
    },
    {
      "epoch": 8.020777633488656,
      "grad_norm": 4.3522210121154785,
      "learning_rate": 4.331601863875945e-05,
      "loss": 1.8392,
      "step": 105000
    },
    {
      "epoch": 8.028416469330073,
      "grad_norm": 5.269619464874268,
      "learning_rate": 4.330965294222494e-05,
      "loss": 1.8631,
      "step": 105100
    },
    {
      "epoch": 8.036055305171493,
      "grad_norm": 5.059521675109863,
      "learning_rate": 4.3303287245690424e-05,
      "loss": 1.8951,
      "step": 105200
    },
    {
      "epoch": 8.04369414101291,
      "grad_norm": 5.2276763916015625,
      "learning_rate": 4.329692154915591e-05,
      "loss": 1.9141,
      "step": 105300
    },
    {
      "epoch": 8.051332976854328,
      "grad_norm": 3.7625601291656494,
      "learning_rate": 4.32905558526214e-05,
      "loss": 1.9025,
      "step": 105400
    },
    {
      "epoch": 8.058971812695745,
      "grad_norm": 4.366011619567871,
      "learning_rate": 4.328419015608688e-05,
      "loss": 1.9357,
      "step": 105500
    },
    {
      "epoch": 8.066610648537162,
      "grad_norm": 3.1189708709716797,
      "learning_rate": 4.3277824459552365e-05,
      "loss": 1.8728,
      "step": 105600
    },
    {
      "epoch": 8.07424948437858,
      "grad_norm": 4.425981521606445,
      "learning_rate": 4.3271458763017856e-05,
      "loss": 1.948,
      "step": 105700
    },
    {
      "epoch": 8.08188832022,
      "grad_norm": 5.1177077293396,
      "learning_rate": 4.326509306648334e-05,
      "loss": 1.8593,
      "step": 105800
    },
    {
      "epoch": 8.089527156061417,
      "grad_norm": 4.510285377502441,
      "learning_rate": 4.325872736994882e-05,
      "loss": 1.854,
      "step": 105900
    },
    {
      "epoch": 8.097165991902834,
      "grad_norm": 5.514438629150391,
      "learning_rate": 4.3252361673414306e-05,
      "loss": 1.881,
      "step": 106000
    },
    {
      "epoch": 8.104804827744251,
      "grad_norm": 4.948558330535889,
      "learning_rate": 4.32459959768798e-05,
      "loss": 1.918,
      "step": 106100
    },
    {
      "epoch": 8.112443663585669,
      "grad_norm": 4.215891361236572,
      "learning_rate": 4.323963028034528e-05,
      "loss": 1.8664,
      "step": 106200
    },
    {
      "epoch": 8.120082499427088,
      "grad_norm": 6.1273274421691895,
      "learning_rate": 4.3233264583810764e-05,
      "loss": 1.855,
      "step": 106300
    },
    {
      "epoch": 8.127721335268506,
      "grad_norm": 5.22762393951416,
      "learning_rate": 4.322689888727625e-05,
      "loss": 1.8482,
      "step": 106400
    },
    {
      "epoch": 8.135360171109923,
      "grad_norm": 8.188591003417969,
      "learning_rate": 4.322053319074174e-05,
      "loss": 1.8166,
      "step": 106500
    },
    {
      "epoch": 8.14299900695134,
      "grad_norm": 5.32204532623291,
      "learning_rate": 4.321416749420722e-05,
      "loss": 1.7979,
      "step": 106600
    },
    {
      "epoch": 8.150637842792758,
      "grad_norm": 5.350155353546143,
      "learning_rate": 4.3207801797672705e-05,
      "loss": 1.9813,
      "step": 106700
    },
    {
      "epoch": 8.158276678634175,
      "grad_norm": 3.8183765411376953,
      "learning_rate": 4.320143610113819e-05,
      "loss": 1.9645,
      "step": 106800
    },
    {
      "epoch": 8.165915514475595,
      "grad_norm": 4.968862056732178,
      "learning_rate": 4.319507040460367e-05,
      "loss": 1.8635,
      "step": 106900
    },
    {
      "epoch": 8.173554350317012,
      "grad_norm": 5.656919956207275,
      "learning_rate": 4.318870470806916e-05,
      "loss": 2.0025,
      "step": 107000
    },
    {
      "epoch": 8.18119318615843,
      "grad_norm": 6.096869945526123,
      "learning_rate": 4.3182339011534646e-05,
      "loss": 1.8753,
      "step": 107100
    },
    {
      "epoch": 8.188832021999847,
      "grad_norm": 8.388071060180664,
      "learning_rate": 4.317597331500013e-05,
      "loss": 1.8611,
      "step": 107200
    },
    {
      "epoch": 8.196470857841264,
      "grad_norm": 4.8184285163879395,
      "learning_rate": 4.316960761846561e-05,
      "loss": 1.8884,
      "step": 107300
    },
    {
      "epoch": 8.204109693682684,
      "grad_norm": 4.7639875411987305,
      "learning_rate": 4.31632419219311e-05,
      "loss": 1.9208,
      "step": 107400
    },
    {
      "epoch": 8.211748529524101,
      "grad_norm": 5.566692352294922,
      "learning_rate": 4.3156876225396586e-05,
      "loss": 1.8648,
      "step": 107500
    },
    {
      "epoch": 8.219387365365519,
      "grad_norm": 5.037610054016113,
      "learning_rate": 4.315051052886207e-05,
      "loss": 1.9211,
      "step": 107600
    },
    {
      "epoch": 8.227026201206936,
      "grad_norm": 5.3707075119018555,
      "learning_rate": 4.3144144832327554e-05,
      "loss": 1.93,
      "step": 107700
    },
    {
      "epoch": 8.234665037048353,
      "grad_norm": 7.694821357727051,
      "learning_rate": 4.313777913579304e-05,
      "loss": 1.9116,
      "step": 107800
    },
    {
      "epoch": 8.24230387288977,
      "grad_norm": 5.332866668701172,
      "learning_rate": 4.313141343925853e-05,
      "loss": 1.8601,
      "step": 107900
    },
    {
      "epoch": 8.24994270873119,
      "grad_norm": 5.214315891265869,
      "learning_rate": 4.312504774272401e-05,
      "loss": 1.9375,
      "step": 108000
    },
    {
      "epoch": 8.257581544572608,
      "grad_norm": 5.172670364379883,
      "learning_rate": 4.3118682046189494e-05,
      "loss": 1.8342,
      "step": 108100
    },
    {
      "epoch": 8.265220380414025,
      "grad_norm": 6.525402545928955,
      "learning_rate": 4.311231634965498e-05,
      "loss": 1.8332,
      "step": 108200
    },
    {
      "epoch": 8.272859216255442,
      "grad_norm": 6.8758015632629395,
      "learning_rate": 4.310595065312046e-05,
      "loss": 1.8509,
      "step": 108300
    },
    {
      "epoch": 8.28049805209686,
      "grad_norm": 5.262033462524414,
      "learning_rate": 4.309958495658595e-05,
      "loss": 1.9445,
      "step": 108400
    },
    {
      "epoch": 8.288136887938279,
      "grad_norm": 6.810665130615234,
      "learning_rate": 4.3093219260051435e-05,
      "loss": 1.9246,
      "step": 108500
    },
    {
      "epoch": 8.295775723779697,
      "grad_norm": 5.814034938812256,
      "learning_rate": 4.308685356351692e-05,
      "loss": 1.9502,
      "step": 108600
    },
    {
      "epoch": 8.303414559621114,
      "grad_norm": 6.799792289733887,
      "learning_rate": 4.30804878669824e-05,
      "loss": 1.7996,
      "step": 108700
    },
    {
      "epoch": 8.311053395462531,
      "grad_norm": 4.702723503112793,
      "learning_rate": 4.307412217044789e-05,
      "loss": 1.9034,
      "step": 108800
    },
    {
      "epoch": 8.318692231303949,
      "grad_norm": 4.239006519317627,
      "learning_rate": 4.3067756473913376e-05,
      "loss": 2.0055,
      "step": 108900
    },
    {
      "epoch": 8.326331067145366,
      "grad_norm": 4.6683197021484375,
      "learning_rate": 4.306139077737886e-05,
      "loss": 1.9763,
      "step": 109000
    },
    {
      "epoch": 8.333969902986786,
      "grad_norm": 4.511746883392334,
      "learning_rate": 4.3055025080844343e-05,
      "loss": 1.9618,
      "step": 109100
    },
    {
      "epoch": 8.341608738828203,
      "grad_norm": 7.426597595214844,
      "learning_rate": 4.3048659384309834e-05,
      "loss": 1.8957,
      "step": 109200
    },
    {
      "epoch": 8.34924757466962,
      "grad_norm": 4.637439250946045,
      "learning_rate": 4.304229368777532e-05,
      "loss": 1.831,
      "step": 109300
    },
    {
      "epoch": 8.356886410511038,
      "grad_norm": 4.543435573577881,
      "learning_rate": 4.303592799124081e-05,
      "loss": 1.8952,
      "step": 109400
    },
    {
      "epoch": 8.364525246352455,
      "grad_norm": 6.754870414733887,
      "learning_rate": 4.302956229470629e-05,
      "loss": 1.8041,
      "step": 109500
    },
    {
      "epoch": 8.372164082193873,
      "grad_norm": 4.394623279571533,
      "learning_rate": 4.3023196598171775e-05,
      "loss": 1.9327,
      "step": 109600
    },
    {
      "epoch": 8.379802918035292,
      "grad_norm": 5.713695049285889,
      "learning_rate": 4.3016830901637265e-05,
      "loss": 1.9897,
      "step": 109700
    },
    {
      "epoch": 8.38744175387671,
      "grad_norm": 5.062060832977295,
      "learning_rate": 4.301046520510275e-05,
      "loss": 1.917,
      "step": 109800
    },
    {
      "epoch": 8.395080589718127,
      "grad_norm": 5.121266841888428,
      "learning_rate": 4.300409950856823e-05,
      "loss": 1.8255,
      "step": 109900
    },
    {
      "epoch": 8.402719425559544,
      "grad_norm": 5.544003486633301,
      "learning_rate": 4.2997733812033716e-05,
      "loss": 1.8715,
      "step": 110000
    },
    {
      "epoch": 8.410358261400962,
      "grad_norm": 4.804728031158447,
      "learning_rate": 4.29913681154992e-05,
      "loss": 1.9552,
      "step": 110100
    },
    {
      "epoch": 8.417997097242381,
      "grad_norm": 4.843710422515869,
      "learning_rate": 4.298500241896469e-05,
      "loss": 1.8607,
      "step": 110200
    },
    {
      "epoch": 8.425635933083798,
      "grad_norm": 4.634023666381836,
      "learning_rate": 4.297863672243017e-05,
      "loss": 1.9281,
      "step": 110300
    },
    {
      "epoch": 8.433274768925216,
      "grad_norm": 5.014491081237793,
      "learning_rate": 4.2972271025895656e-05,
      "loss": 1.9246,
      "step": 110400
    },
    {
      "epoch": 8.440913604766633,
      "grad_norm": 5.605496883392334,
      "learning_rate": 4.296590532936114e-05,
      "loss": 1.9128,
      "step": 110500
    },
    {
      "epoch": 8.44855244060805,
      "grad_norm": 5.464090347290039,
      "learning_rate": 4.295953963282663e-05,
      "loss": 1.9441,
      "step": 110600
    },
    {
      "epoch": 8.45619127644947,
      "grad_norm": 7.384422779083252,
      "learning_rate": 4.2953173936292114e-05,
      "loss": 1.8795,
      "step": 110700
    },
    {
      "epoch": 8.463830112290887,
      "grad_norm": 5.164448261260986,
      "learning_rate": 4.29468082397576e-05,
      "loss": 1.8843,
      "step": 110800
    },
    {
      "epoch": 8.471468948132305,
      "grad_norm": 6.099098205566406,
      "learning_rate": 4.294044254322308e-05,
      "loss": 1.9346,
      "step": 110900
    },
    {
      "epoch": 8.479107783973722,
      "grad_norm": 4.833465099334717,
      "learning_rate": 4.2934076846688564e-05,
      "loss": 1.9435,
      "step": 111000
    },
    {
      "epoch": 8.48674661981514,
      "grad_norm": 5.668520450592041,
      "learning_rate": 4.2927711150154055e-05,
      "loss": 1.9468,
      "step": 111100
    },
    {
      "epoch": 8.494385455656557,
      "grad_norm": 5.388899326324463,
      "learning_rate": 4.292134545361954e-05,
      "loss": 1.8507,
      "step": 111200
    },
    {
      "epoch": 8.502024291497976,
      "grad_norm": 4.410058975219727,
      "learning_rate": 4.291497975708502e-05,
      "loss": 1.8755,
      "step": 111300
    },
    {
      "epoch": 8.509663127339394,
      "grad_norm": 4.905378818511963,
      "learning_rate": 4.2908614060550505e-05,
      "loss": 1.9219,
      "step": 111400
    },
    {
      "epoch": 8.517301963180811,
      "grad_norm": 5.721988201141357,
      "learning_rate": 4.290224836401599e-05,
      "loss": 1.829,
      "step": 111500
    },
    {
      "epoch": 8.524940799022229,
      "grad_norm": 6.329893112182617,
      "learning_rate": 4.289588266748148e-05,
      "loss": 1.9464,
      "step": 111600
    },
    {
      "epoch": 8.532579634863646,
      "grad_norm": 5.330678939819336,
      "learning_rate": 4.288951697094696e-05,
      "loss": 1.856,
      "step": 111700
    },
    {
      "epoch": 8.540218470705064,
      "grad_norm": 5.432226181030273,
      "learning_rate": 4.2883151274412446e-05,
      "loss": 1.887,
      "step": 111800
    },
    {
      "epoch": 8.547857306546483,
      "grad_norm": 3.975597381591797,
      "learning_rate": 4.287678557787793e-05,
      "loss": 1.9367,
      "step": 111900
    },
    {
      "epoch": 8.5554961423879,
      "grad_norm": 3.053105592727661,
      "learning_rate": 4.287041988134342e-05,
      "loss": 1.8536,
      "step": 112000
    },
    {
      "epoch": 8.563134978229318,
      "grad_norm": 5.949962615966797,
      "learning_rate": 4.2864054184808904e-05,
      "loss": 1.9218,
      "step": 112100
    },
    {
      "epoch": 8.570773814070735,
      "grad_norm": 9.353867530822754,
      "learning_rate": 4.285768848827439e-05,
      "loss": 1.9554,
      "step": 112200
    },
    {
      "epoch": 8.578412649912153,
      "grad_norm": 4.005197525024414,
      "learning_rate": 4.285132279173987e-05,
      "loss": 1.8836,
      "step": 112300
    },
    {
      "epoch": 8.586051485753572,
      "grad_norm": 4.726653099060059,
      "learning_rate": 4.2844957095205354e-05,
      "loss": 1.9071,
      "step": 112400
    },
    {
      "epoch": 8.59369032159499,
      "grad_norm": 4.980913162231445,
      "learning_rate": 4.2838591398670845e-05,
      "loss": 1.8281,
      "step": 112500
    },
    {
      "epoch": 8.601329157436407,
      "grad_norm": 4.320165157318115,
      "learning_rate": 4.283222570213633e-05,
      "loss": 1.8911,
      "step": 112600
    },
    {
      "epoch": 8.608967993277824,
      "grad_norm": 6.119589805603027,
      "learning_rate": 4.282586000560181e-05,
      "loss": 1.8204,
      "step": 112700
    },
    {
      "epoch": 8.616606829119242,
      "grad_norm": 6.76011848449707,
      "learning_rate": 4.2819494309067295e-05,
      "loss": 1.8455,
      "step": 112800
    },
    {
      "epoch": 8.624245664960661,
      "grad_norm": 6.199556350708008,
      "learning_rate": 4.2813128612532786e-05,
      "loss": 2.0238,
      "step": 112900
    },
    {
      "epoch": 8.631884500802078,
      "grad_norm": 4.1849212646484375,
      "learning_rate": 4.280676291599827e-05,
      "loss": 1.8008,
      "step": 113000
    },
    {
      "epoch": 8.639523336643496,
      "grad_norm": 6.2360310554504395,
      "learning_rate": 4.280039721946375e-05,
      "loss": 1.9318,
      "step": 113100
    },
    {
      "epoch": 8.647162172484913,
      "grad_norm": 4.270270824432373,
      "learning_rate": 4.279403152292924e-05,
      "loss": 1.8548,
      "step": 113200
    },
    {
      "epoch": 8.65480100832633,
      "grad_norm": 8.328608512878418,
      "learning_rate": 4.2787665826394726e-05,
      "loss": 1.9622,
      "step": 113300
    },
    {
      "epoch": 8.662439844167748,
      "grad_norm": 4.418557167053223,
      "learning_rate": 4.278130012986022e-05,
      "loss": 1.9366,
      "step": 113400
    },
    {
      "epoch": 8.670078680009167,
      "grad_norm": 6.416481971740723,
      "learning_rate": 4.27749344333257e-05,
      "loss": 1.9561,
      "step": 113500
    },
    {
      "epoch": 8.677717515850585,
      "grad_norm": 5.529240131378174,
      "learning_rate": 4.2768568736791184e-05,
      "loss": 1.9566,
      "step": 113600
    },
    {
      "epoch": 8.685356351692002,
      "grad_norm": 5.1254777908325195,
      "learning_rate": 4.276220304025667e-05,
      "loss": 1.9529,
      "step": 113700
    },
    {
      "epoch": 8.69299518753342,
      "grad_norm": 5.881425380706787,
      "learning_rate": 4.275583734372216e-05,
      "loss": 1.8761,
      "step": 113800
    },
    {
      "epoch": 8.700634023374837,
      "grad_norm": 6.415513515472412,
      "learning_rate": 4.274947164718764e-05,
      "loss": 1.8711,
      "step": 113900
    },
    {
      "epoch": 8.708272859216255,
      "grad_norm": 3.862842082977295,
      "learning_rate": 4.2743105950653125e-05,
      "loss": 1.9843,
      "step": 114000
    },
    {
      "epoch": 8.715911695057674,
      "grad_norm": 5.449680805206299,
      "learning_rate": 4.273674025411861e-05,
      "loss": 1.9385,
      "step": 114100
    },
    {
      "epoch": 8.723550530899091,
      "grad_norm": 4.512577533721924,
      "learning_rate": 4.273037455758409e-05,
      "loss": 1.9058,
      "step": 114200
    },
    {
      "epoch": 8.731189366740509,
      "grad_norm": 6.179937839508057,
      "learning_rate": 4.272400886104958e-05,
      "loss": 1.9299,
      "step": 114300
    },
    {
      "epoch": 8.738828202581926,
      "grad_norm": 5.923374652862549,
      "learning_rate": 4.2717643164515066e-05,
      "loss": 1.8792,
      "step": 114400
    },
    {
      "epoch": 8.746467038423344,
      "grad_norm": 5.310499668121338,
      "learning_rate": 4.271127746798055e-05,
      "loss": 1.8644,
      "step": 114500
    },
    {
      "epoch": 8.754105874264763,
      "grad_norm": 5.017524242401123,
      "learning_rate": 4.270491177144603e-05,
      "loss": 1.8629,
      "step": 114600
    },
    {
      "epoch": 8.76174471010618,
      "grad_norm": 5.148873329162598,
      "learning_rate": 4.2698546074911516e-05,
      "loss": 1.8403,
      "step": 114700
    },
    {
      "epoch": 8.769383545947598,
      "grad_norm": 5.112626075744629,
      "learning_rate": 4.269218037837701e-05,
      "loss": 1.8619,
      "step": 114800
    },
    {
      "epoch": 8.777022381789015,
      "grad_norm": 6.733065128326416,
      "learning_rate": 4.268581468184249e-05,
      "loss": 1.8578,
      "step": 114900
    },
    {
      "epoch": 8.784661217630433,
      "grad_norm": 6.080535411834717,
      "learning_rate": 4.2679448985307974e-05,
      "loss": 1.7998,
      "step": 115000
    },
    {
      "epoch": 8.79230005347185,
      "grad_norm": 5.466644763946533,
      "learning_rate": 4.267308328877346e-05,
      "loss": 1.8645,
      "step": 115100
    },
    {
      "epoch": 8.79993888931327,
      "grad_norm": 5.728806495666504,
      "learning_rate": 4.266671759223895e-05,
      "loss": 1.8612,
      "step": 115200
    },
    {
      "epoch": 8.807577725154687,
      "grad_norm": 5.089931011199951,
      "learning_rate": 4.266035189570443e-05,
      "loss": 1.8827,
      "step": 115300
    },
    {
      "epoch": 8.815216560996104,
      "grad_norm": 4.771980285644531,
      "learning_rate": 4.2653986199169915e-05,
      "loss": 1.9086,
      "step": 115400
    },
    {
      "epoch": 8.822855396837522,
      "grad_norm": 6.004822254180908,
      "learning_rate": 4.26476205026354e-05,
      "loss": 1.8794,
      "step": 115500
    },
    {
      "epoch": 8.830494232678939,
      "grad_norm": 4.689841270446777,
      "learning_rate": 4.264125480610088e-05,
      "loss": 1.8501,
      "step": 115600
    },
    {
      "epoch": 8.838133068520358,
      "grad_norm": 3.596522092819214,
      "learning_rate": 4.263488910956637e-05,
      "loss": 1.8801,
      "step": 115700
    },
    {
      "epoch": 8.845771904361776,
      "grad_norm": 4.697671890258789,
      "learning_rate": 4.2628523413031856e-05,
      "loss": 1.8618,
      "step": 115800
    },
    {
      "epoch": 8.853410740203193,
      "grad_norm": 5.788748264312744,
      "learning_rate": 4.262215771649734e-05,
      "loss": 1.9204,
      "step": 115900
    },
    {
      "epoch": 8.86104957604461,
      "grad_norm": 5.513919353485107,
      "learning_rate": 4.261579201996282e-05,
      "loss": 1.9164,
      "step": 116000
    },
    {
      "epoch": 8.868688411886028,
      "grad_norm": 4.739734172821045,
      "learning_rate": 4.260942632342831e-05,
      "loss": 1.9028,
      "step": 116100
    },
    {
      "epoch": 8.876327247727446,
      "grad_norm": 7.67496395111084,
      "learning_rate": 4.2603060626893796e-05,
      "loss": 1.831,
      "step": 116200
    },
    {
      "epoch": 8.883966083568865,
      "grad_norm": 4.9640727043151855,
      "learning_rate": 4.259669493035928e-05,
      "loss": 1.8664,
      "step": 116300
    },
    {
      "epoch": 8.891604919410282,
      "grad_norm": 5.4027814865112305,
      "learning_rate": 4.2590329233824764e-05,
      "loss": 1.8494,
      "step": 116400
    },
    {
      "epoch": 8.8992437552517,
      "grad_norm": 4.507627487182617,
      "learning_rate": 4.258396353729025e-05,
      "loss": 1.8805,
      "step": 116500
    },
    {
      "epoch": 8.906882591093117,
      "grad_norm": 5.848571300506592,
      "learning_rate": 4.257759784075574e-05,
      "loss": 1.9302,
      "step": 116600
    },
    {
      "epoch": 8.914521426934535,
      "grad_norm": 5.990423679351807,
      "learning_rate": 4.257123214422122e-05,
      "loss": 1.8938,
      "step": 116700
    },
    {
      "epoch": 8.922160262775954,
      "grad_norm": 4.367282867431641,
      "learning_rate": 4.2564866447686704e-05,
      "loss": 1.8086,
      "step": 116800
    },
    {
      "epoch": 8.929799098617371,
      "grad_norm": 5.5064167976379395,
      "learning_rate": 4.2558500751152195e-05,
      "loss": 1.8886,
      "step": 116900
    },
    {
      "epoch": 8.937437934458789,
      "grad_norm": 6.838388442993164,
      "learning_rate": 4.255213505461768e-05,
      "loss": 1.838,
      "step": 117000
    },
    {
      "epoch": 8.945076770300206,
      "grad_norm": 4.506416320800781,
      "learning_rate": 4.254576935808316e-05,
      "loss": 1.904,
      "step": 117100
    },
    {
      "epoch": 8.952715606141624,
      "grad_norm": 3.9996120929718018,
      "learning_rate": 4.253940366154865e-05,
      "loss": 1.7918,
      "step": 117200
    },
    {
      "epoch": 8.960354441983041,
      "grad_norm": 4.379598617553711,
      "learning_rate": 4.2533037965014136e-05,
      "loss": 1.8622,
      "step": 117300
    },
    {
      "epoch": 8.96799327782446,
      "grad_norm": 3.2533717155456543,
      "learning_rate": 4.252667226847962e-05,
      "loss": 1.7841,
      "step": 117400
    },
    {
      "epoch": 8.975632113665878,
      "grad_norm": 3.846233367919922,
      "learning_rate": 4.252030657194511e-05,
      "loss": 1.9264,
      "step": 117500
    },
    {
      "epoch": 8.983270949507295,
      "grad_norm": 8.09168815612793,
      "learning_rate": 4.251394087541059e-05,
      "loss": 1.8725,
      "step": 117600
    },
    {
      "epoch": 8.990909785348713,
      "grad_norm": 5.255865573883057,
      "learning_rate": 4.250757517887608e-05,
      "loss": 1.8942,
      "step": 117700
    },
    {
      "epoch": 8.99854862119013,
      "grad_norm": 5.121086120605469,
      "learning_rate": 4.250120948234156e-05,
      "loss": 1.8887,
      "step": 117800
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8682140111923218,
      "eval_runtime": 1.535,
      "eval_samples_per_second": 449.513,
      "eval_steps_per_second": 449.513,
      "step": 117819
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.6651471853256226,
      "eval_runtime": 28.503,
      "eval_samples_per_second": 459.285,
      "eval_steps_per_second": 459.285,
      "step": 117819
    },
    {
      "epoch": 9.00618745703155,
      "grad_norm": 4.876704216003418,
      "learning_rate": 4.2494843785807044e-05,
      "loss": 1.8035,
      "step": 117900
    },
    {
      "epoch": 9.013826292872967,
      "grad_norm": 4.577346324920654,
      "learning_rate": 4.2488478089272534e-05,
      "loss": 1.8996,
      "step": 118000
    },
    {
      "epoch": 9.021465128714384,
      "grad_norm": 5.6141228675842285,
      "learning_rate": 4.248211239273802e-05,
      "loss": 1.8014,
      "step": 118100
    },
    {
      "epoch": 9.029103964555802,
      "grad_norm": 6.518258571624756,
      "learning_rate": 4.24757466962035e-05,
      "loss": 1.921,
      "step": 118200
    },
    {
      "epoch": 9.036742800397219,
      "grad_norm": 4.674749374389648,
      "learning_rate": 4.2469380999668985e-05,
      "loss": 1.8157,
      "step": 118300
    },
    {
      "epoch": 9.044381636238636,
      "grad_norm": 4.873442649841309,
      "learning_rate": 4.2463015303134475e-05,
      "loss": 1.8154,
      "step": 118400
    },
    {
      "epoch": 9.052020472080056,
      "grad_norm": 4.952631950378418,
      "learning_rate": 4.245664960659996e-05,
      "loss": 1.907,
      "step": 118500
    },
    {
      "epoch": 9.059659307921473,
      "grad_norm": 5.643988132476807,
      "learning_rate": 4.245028391006544e-05,
      "loss": 1.8643,
      "step": 118600
    },
    {
      "epoch": 9.06729814376289,
      "grad_norm": 6.6830058097839355,
      "learning_rate": 4.2443918213530926e-05,
      "loss": 1.9238,
      "step": 118700
    },
    {
      "epoch": 9.074936979604308,
      "grad_norm": 4.975009441375732,
      "learning_rate": 4.243755251699641e-05,
      "loss": 1.778,
      "step": 118800
    },
    {
      "epoch": 9.082575815445725,
      "grad_norm": 4.591828346252441,
      "learning_rate": 4.24311868204619e-05,
      "loss": 1.9934,
      "step": 118900
    },
    {
      "epoch": 9.090214651287145,
      "grad_norm": 4.263481616973877,
      "learning_rate": 4.242482112392738e-05,
      "loss": 1.7981,
      "step": 119000
    },
    {
      "epoch": 9.097853487128562,
      "grad_norm": 5.380435466766357,
      "learning_rate": 4.2418455427392866e-05,
      "loss": 1.9083,
      "step": 119100
    },
    {
      "epoch": 9.10549232296998,
      "grad_norm": 4.7316389083862305,
      "learning_rate": 4.241208973085835e-05,
      "loss": 1.8886,
      "step": 119200
    },
    {
      "epoch": 9.113131158811397,
      "grad_norm": 4.634594440460205,
      "learning_rate": 4.240572403432384e-05,
      "loss": 1.8824,
      "step": 119300
    },
    {
      "epoch": 9.120769994652814,
      "grad_norm": 4.770493030548096,
      "learning_rate": 4.2399358337789324e-05,
      "loss": 1.8836,
      "step": 119400
    },
    {
      "epoch": 9.128408830494232,
      "grad_norm": 5.084421634674072,
      "learning_rate": 4.239299264125481e-05,
      "loss": 1.7968,
      "step": 119500
    },
    {
      "epoch": 9.136047666335651,
      "grad_norm": 6.036217212677002,
      "learning_rate": 4.238662694472029e-05,
      "loss": 1.8353,
      "step": 119600
    },
    {
      "epoch": 9.143686502177069,
      "grad_norm": 7.6096062660217285,
      "learning_rate": 4.2380261248185774e-05,
      "loss": 1.8335,
      "step": 119700
    },
    {
      "epoch": 9.151325338018486,
      "grad_norm": 4.0164899826049805,
      "learning_rate": 4.2373895551651265e-05,
      "loss": 1.7784,
      "step": 119800
    },
    {
      "epoch": 9.158964173859903,
      "grad_norm": 3.4492576122283936,
      "learning_rate": 4.236752985511675e-05,
      "loss": 1.9038,
      "step": 119900
    },
    {
      "epoch": 9.166603009701321,
      "grad_norm": 5.298243999481201,
      "learning_rate": 4.236116415858223e-05,
      "loss": 1.9726,
      "step": 120000
    },
    {
      "epoch": 9.17424184554274,
      "grad_norm": 5.004743576049805,
      "learning_rate": 4.2354798462047715e-05,
      "loss": 1.8218,
      "step": 120100
    },
    {
      "epoch": 9.181880681384158,
      "grad_norm": 6.579277038574219,
      "learning_rate": 4.23484327655132e-05,
      "loss": 1.744,
      "step": 120200
    },
    {
      "epoch": 9.189519517225575,
      "grad_norm": 5.390781879425049,
      "learning_rate": 4.234206706897869e-05,
      "loss": 1.8585,
      "step": 120300
    },
    {
      "epoch": 9.197158353066992,
      "grad_norm": 5.818085670471191,
      "learning_rate": 4.233570137244417e-05,
      "loss": 1.9077,
      "step": 120400
    },
    {
      "epoch": 9.20479718890841,
      "grad_norm": 4.688218116760254,
      "learning_rate": 4.2329335675909656e-05,
      "loss": 1.8426,
      "step": 120500
    },
    {
      "epoch": 9.212436024749827,
      "grad_norm": 4.102455139160156,
      "learning_rate": 4.232296997937515e-05,
      "loss": 1.9115,
      "step": 120600
    },
    {
      "epoch": 9.220074860591247,
      "grad_norm": 5.028476715087891,
      "learning_rate": 4.231660428284063e-05,
      "loss": 1.8519,
      "step": 120700
    },
    {
      "epoch": 9.227713696432664,
      "grad_norm": 5.7236175537109375,
      "learning_rate": 4.2310238586306114e-05,
      "loss": 1.8259,
      "step": 120800
    },
    {
      "epoch": 9.235352532274081,
      "grad_norm": 4.159270286560059,
      "learning_rate": 4.2303872889771604e-05,
      "loss": 1.7875,
      "step": 120900
    },
    {
      "epoch": 9.242991368115499,
      "grad_norm": 4.629364967346191,
      "learning_rate": 4.229750719323709e-05,
      "loss": 1.8971,
      "step": 121000
    },
    {
      "epoch": 9.250630203956916,
      "grad_norm": 6.041938304901123,
      "learning_rate": 4.229114149670257e-05,
      "loss": 1.8409,
      "step": 121100
    },
    {
      "epoch": 9.258269039798336,
      "grad_norm": 5.031254768371582,
      "learning_rate": 4.228477580016806e-05,
      "loss": 1.8539,
      "step": 121200
    },
    {
      "epoch": 9.265907875639753,
      "grad_norm": 4.93757963180542,
      "learning_rate": 4.2278410103633545e-05,
      "loss": 1.9024,
      "step": 121300
    },
    {
      "epoch": 9.27354671148117,
      "grad_norm": 7.047270774841309,
      "learning_rate": 4.227204440709903e-05,
      "loss": 1.8724,
      "step": 121400
    },
    {
      "epoch": 9.281185547322588,
      "grad_norm": 5.0850019454956055,
      "learning_rate": 4.226567871056451e-05,
      "loss": 1.8598,
      "step": 121500
    },
    {
      "epoch": 9.288824383164005,
      "grad_norm": 4.694998264312744,
      "learning_rate": 4.225931301403e-05,
      "loss": 1.9395,
      "step": 121600
    },
    {
      "epoch": 9.296463219005423,
      "grad_norm": 4.709160327911377,
      "learning_rate": 4.2252947317495486e-05,
      "loss": 1.7967,
      "step": 121700
    },
    {
      "epoch": 9.304102054846842,
      "grad_norm": 4.925492286682129,
      "learning_rate": 4.224658162096097e-05,
      "loss": 1.934,
      "step": 121800
    },
    {
      "epoch": 9.31174089068826,
      "grad_norm": 7.049617290496826,
      "learning_rate": 4.224021592442645e-05,
      "loss": 1.8881,
      "step": 121900
    },
    {
      "epoch": 9.319379726529677,
      "grad_norm": 6.343888759613037,
      "learning_rate": 4.2233850227891936e-05,
      "loss": 1.7886,
      "step": 122000
    },
    {
      "epoch": 9.327018562371094,
      "grad_norm": 5.368983745574951,
      "learning_rate": 4.222748453135743e-05,
      "loss": 1.892,
      "step": 122100
    },
    {
      "epoch": 9.334657398212512,
      "grad_norm": 4.923095226287842,
      "learning_rate": 4.222111883482291e-05,
      "loss": 1.8053,
      "step": 122200
    },
    {
      "epoch": 9.34229623405393,
      "grad_norm": 5.2665581703186035,
      "learning_rate": 4.2214753138288394e-05,
      "loss": 1.9152,
      "step": 122300
    },
    {
      "epoch": 9.349935069895349,
      "grad_norm": 5.551938056945801,
      "learning_rate": 4.220838744175388e-05,
      "loss": 1.9695,
      "step": 122400
    },
    {
      "epoch": 9.357573905736766,
      "grad_norm": 4.851152420043945,
      "learning_rate": 4.220202174521937e-05,
      "loss": 1.9227,
      "step": 122500
    },
    {
      "epoch": 9.365212741578183,
      "grad_norm": 5.662990093231201,
      "learning_rate": 4.219565604868485e-05,
      "loss": 1.9838,
      "step": 122600
    },
    {
      "epoch": 9.3728515774196,
      "grad_norm": 4.841716289520264,
      "learning_rate": 4.2189290352150335e-05,
      "loss": 1.9343,
      "step": 122700
    },
    {
      "epoch": 9.380490413261018,
      "grad_norm": 5.302859306335449,
      "learning_rate": 4.218292465561582e-05,
      "loss": 1.9362,
      "step": 122800
    },
    {
      "epoch": 9.388129249102438,
      "grad_norm": 6.306928634643555,
      "learning_rate": 4.21765589590813e-05,
      "loss": 1.9534,
      "step": 122900
    },
    {
      "epoch": 9.395768084943855,
      "grad_norm": 5.243575096130371,
      "learning_rate": 4.217019326254679e-05,
      "loss": 1.8098,
      "step": 123000
    },
    {
      "epoch": 9.403406920785272,
      "grad_norm": 5.853952407836914,
      "learning_rate": 4.2163827566012276e-05,
      "loss": 1.8379,
      "step": 123100
    },
    {
      "epoch": 9.41104575662669,
      "grad_norm": 3.315833330154419,
      "learning_rate": 4.215746186947776e-05,
      "loss": 1.8783,
      "step": 123200
    },
    {
      "epoch": 9.418684592468107,
      "grad_norm": 5.901674270629883,
      "learning_rate": 4.215109617294324e-05,
      "loss": 1.8781,
      "step": 123300
    },
    {
      "epoch": 9.426323428309527,
      "grad_norm": 5.911020278930664,
      "learning_rate": 4.2144730476408726e-05,
      "loss": 1.8562,
      "step": 123400
    },
    {
      "epoch": 9.433962264150944,
      "grad_norm": 5.382162094116211,
      "learning_rate": 4.213836477987422e-05,
      "loss": 1.9325,
      "step": 123500
    },
    {
      "epoch": 9.441601099992361,
      "grad_norm": 4.641994953155518,
      "learning_rate": 4.21319990833397e-05,
      "loss": 1.8539,
      "step": 123600
    },
    {
      "epoch": 9.449239935833779,
      "grad_norm": 5.543191432952881,
      "learning_rate": 4.2125633386805184e-05,
      "loss": 1.8122,
      "step": 123700
    },
    {
      "epoch": 9.456878771675196,
      "grad_norm": 5.760679721832275,
      "learning_rate": 4.211926769027067e-05,
      "loss": 1.8395,
      "step": 123800
    },
    {
      "epoch": 9.464517607516614,
      "grad_norm": 4.609614372253418,
      "learning_rate": 4.211290199373616e-05,
      "loss": 1.9207,
      "step": 123900
    },
    {
      "epoch": 9.472156443358033,
      "grad_norm": 6.910390377044678,
      "learning_rate": 4.210653629720164e-05,
      "loss": 1.8921,
      "step": 124000
    },
    {
      "epoch": 9.47979527919945,
      "grad_norm": 5.661477088928223,
      "learning_rate": 4.2100170600667125e-05,
      "loss": 1.8407,
      "step": 124100
    },
    {
      "epoch": 9.487434115040868,
      "grad_norm": 5.075596809387207,
      "learning_rate": 4.209380490413261e-05,
      "loss": 1.9265,
      "step": 124200
    },
    {
      "epoch": 9.495072950882285,
      "grad_norm": 8.298215866088867,
      "learning_rate": 4.208743920759809e-05,
      "loss": 1.8817,
      "step": 124300
    },
    {
      "epoch": 9.502711786723703,
      "grad_norm": 5.470483303070068,
      "learning_rate": 4.208107351106358e-05,
      "loss": 1.8689,
      "step": 124400
    },
    {
      "epoch": 9.51035062256512,
      "grad_norm": 4.71275520324707,
      "learning_rate": 4.2074707814529066e-05,
      "loss": 1.8318,
      "step": 124500
    },
    {
      "epoch": 9.51798945840654,
      "grad_norm": 6.651081562042236,
      "learning_rate": 4.2068342117994556e-05,
      "loss": 1.8326,
      "step": 124600
    },
    {
      "epoch": 9.525628294247957,
      "grad_norm": 5.828207969665527,
      "learning_rate": 4.206197642146004e-05,
      "loss": 1.9193,
      "step": 124700
    },
    {
      "epoch": 9.533267130089374,
      "grad_norm": 3.998602867126465,
      "learning_rate": 4.205561072492552e-05,
      "loss": 1.9134,
      "step": 124800
    },
    {
      "epoch": 9.540905965930792,
      "grad_norm": 7.818151473999023,
      "learning_rate": 4.204924502839101e-05,
      "loss": 1.7708,
      "step": 124900
    },
    {
      "epoch": 9.54854480177221,
      "grad_norm": 4.606949806213379,
      "learning_rate": 4.20428793318565e-05,
      "loss": 1.8859,
      "step": 125000
    },
    {
      "epoch": 9.556183637613628,
      "grad_norm": 4.709493637084961,
      "learning_rate": 4.203651363532198e-05,
      "loss": 1.8971,
      "step": 125100
    },
    {
      "epoch": 9.563822473455046,
      "grad_norm": 5.737884998321533,
      "learning_rate": 4.2030147938787464e-05,
      "loss": 1.8805,
      "step": 125200
    },
    {
      "epoch": 9.571461309296463,
      "grad_norm": 4.542520046234131,
      "learning_rate": 4.2023782242252954e-05,
      "loss": 1.8677,
      "step": 125300
    },
    {
      "epoch": 9.57910014513788,
      "grad_norm": 5.496220111846924,
      "learning_rate": 4.201741654571844e-05,
      "loss": 1.8209,
      "step": 125400
    },
    {
      "epoch": 9.586738980979298,
      "grad_norm": 5.959680080413818,
      "learning_rate": 4.201105084918392e-05,
      "loss": 1.9372,
      "step": 125500
    },
    {
      "epoch": 9.594377816820717,
      "grad_norm": 5.4805731773376465,
      "learning_rate": 4.2004685152649405e-05,
      "loss": 1.8438,
      "step": 125600
    },
    {
      "epoch": 9.602016652662135,
      "grad_norm": 4.435595512390137,
      "learning_rate": 4.199831945611489e-05,
      "loss": 1.9546,
      "step": 125700
    },
    {
      "epoch": 9.609655488503552,
      "grad_norm": 5.016106605529785,
      "learning_rate": 4.199195375958038e-05,
      "loss": 1.904,
      "step": 125800
    },
    {
      "epoch": 9.61729432434497,
      "grad_norm": 3.99311900138855,
      "learning_rate": 4.198558806304586e-05,
      "loss": 1.8968,
      "step": 125900
    },
    {
      "epoch": 9.624933160186387,
      "grad_norm": 6.1620707511901855,
      "learning_rate": 4.1979222366511346e-05,
      "loss": 1.7323,
      "step": 126000
    },
    {
      "epoch": 9.632571996027805,
      "grad_norm": 5.217924118041992,
      "learning_rate": 4.197285666997683e-05,
      "loss": 1.8148,
      "step": 126100
    },
    {
      "epoch": 9.640210831869224,
      "grad_norm": 5.210739612579346,
      "learning_rate": 4.196649097344232e-05,
      "loss": 1.947,
      "step": 126200
    },
    {
      "epoch": 9.647849667710641,
      "grad_norm": 4.119085788726807,
      "learning_rate": 4.19601252769078e-05,
      "loss": 1.8302,
      "step": 126300
    },
    {
      "epoch": 9.655488503552059,
      "grad_norm": 6.8705315589904785,
      "learning_rate": 4.195375958037329e-05,
      "loss": 1.8705,
      "step": 126400
    },
    {
      "epoch": 9.663127339393476,
      "grad_norm": 4.17591667175293,
      "learning_rate": 4.194739388383877e-05,
      "loss": 1.8664,
      "step": 126500
    },
    {
      "epoch": 9.670766175234894,
      "grad_norm": 5.9550957679748535,
      "learning_rate": 4.1941028187304254e-05,
      "loss": 1.966,
      "step": 126600
    },
    {
      "epoch": 9.678405011076311,
      "grad_norm": 5.031418323516846,
      "learning_rate": 4.1934662490769744e-05,
      "loss": 1.8793,
      "step": 126700
    },
    {
      "epoch": 9.68604384691773,
      "grad_norm": 4.082370281219482,
      "learning_rate": 4.192829679423523e-05,
      "loss": 1.9938,
      "step": 126800
    },
    {
      "epoch": 9.693682682759148,
      "grad_norm": 7.381651401519775,
      "learning_rate": 4.192193109770071e-05,
      "loss": 1.8809,
      "step": 126900
    },
    {
      "epoch": 9.701321518600565,
      "grad_norm": 5.146286964416504,
      "learning_rate": 4.1915565401166195e-05,
      "loss": 1.9256,
      "step": 127000
    },
    {
      "epoch": 9.708960354441983,
      "grad_norm": 5.585173606872559,
      "learning_rate": 4.1909199704631685e-05,
      "loss": 1.889,
      "step": 127100
    },
    {
      "epoch": 9.7165991902834,
      "grad_norm": 5.712351322174072,
      "learning_rate": 4.190283400809717e-05,
      "loss": 1.8816,
      "step": 127200
    },
    {
      "epoch": 9.72423802612482,
      "grad_norm": 6.949220180511475,
      "learning_rate": 4.189646831156265e-05,
      "loss": 1.9192,
      "step": 127300
    },
    {
      "epoch": 9.731876861966237,
      "grad_norm": 7.271574974060059,
      "learning_rate": 4.1890102615028136e-05,
      "loss": 1.9265,
      "step": 127400
    },
    {
      "epoch": 9.739515697807654,
      "grad_norm": 5.091915130615234,
      "learning_rate": 4.188373691849362e-05,
      "loss": 1.8336,
      "step": 127500
    },
    {
      "epoch": 9.747154533649072,
      "grad_norm": 5.6979899406433105,
      "learning_rate": 4.187737122195911e-05,
      "loss": 1.8763,
      "step": 127600
    },
    {
      "epoch": 9.75479336949049,
      "grad_norm": 5.278911590576172,
      "learning_rate": 4.187100552542459e-05,
      "loss": 1.8718,
      "step": 127700
    },
    {
      "epoch": 9.762432205331907,
      "grad_norm": 4.8035736083984375,
      "learning_rate": 4.1864639828890076e-05,
      "loss": 1.7647,
      "step": 127800
    },
    {
      "epoch": 9.770071041173326,
      "grad_norm": 5.348084449768066,
      "learning_rate": 4.185827413235556e-05,
      "loss": 1.8905,
      "step": 127900
    },
    {
      "epoch": 9.777709877014743,
      "grad_norm": 9.652613639831543,
      "learning_rate": 4.185190843582105e-05,
      "loss": 1.9547,
      "step": 128000
    },
    {
      "epoch": 9.78534871285616,
      "grad_norm": 5.687643527984619,
      "learning_rate": 4.1845542739286534e-05,
      "loss": 1.9183,
      "step": 128100
    },
    {
      "epoch": 9.792987548697578,
      "grad_norm": 4.483093738555908,
      "learning_rate": 4.183917704275202e-05,
      "loss": 1.8474,
      "step": 128200
    },
    {
      "epoch": 9.800626384538996,
      "grad_norm": 4.2661967277526855,
      "learning_rate": 4.18328113462175e-05,
      "loss": 1.8743,
      "step": 128300
    },
    {
      "epoch": 9.808265220380415,
      "grad_norm": 5.443153381347656,
      "learning_rate": 4.182644564968299e-05,
      "loss": 1.8934,
      "step": 128400
    },
    {
      "epoch": 9.815904056221832,
      "grad_norm": 4.895206451416016,
      "learning_rate": 4.1820079953148475e-05,
      "loss": 1.85,
      "step": 128500
    },
    {
      "epoch": 9.82354289206325,
      "grad_norm": 5.329528331756592,
      "learning_rate": 4.181371425661396e-05,
      "loss": 1.8914,
      "step": 128600
    },
    {
      "epoch": 9.831181727904667,
      "grad_norm": 5.231879711151123,
      "learning_rate": 4.180734856007945e-05,
      "loss": 1.8985,
      "step": 128700
    },
    {
      "epoch": 9.838820563746085,
      "grad_norm": 4.843310356140137,
      "learning_rate": 4.180098286354493e-05,
      "loss": 1.8222,
      "step": 128800
    },
    {
      "epoch": 9.846459399587502,
      "grad_norm": 4.029630661010742,
      "learning_rate": 4.1794617167010416e-05,
      "loss": 1.8532,
      "step": 128900
    },
    {
      "epoch": 9.854098235428921,
      "grad_norm": 6.558986186981201,
      "learning_rate": 4.1788251470475906e-05,
      "loss": 1.8581,
      "step": 129000
    },
    {
      "epoch": 9.861737071270339,
      "grad_norm": 3.8280370235443115,
      "learning_rate": 4.178188577394139e-05,
      "loss": 1.8576,
      "step": 129100
    },
    {
      "epoch": 9.869375907111756,
      "grad_norm": 3.973668098449707,
      "learning_rate": 4.177552007740687e-05,
      "loss": 1.8402,
      "step": 129200
    },
    {
      "epoch": 9.877014742953174,
      "grad_norm": 4.2843122482299805,
      "learning_rate": 4.176915438087236e-05,
      "loss": 1.974,
      "step": 129300
    },
    {
      "epoch": 9.884653578794591,
      "grad_norm": 5.337863445281982,
      "learning_rate": 4.176278868433785e-05,
      "loss": 1.9647,
      "step": 129400
    },
    {
      "epoch": 9.89229241463601,
      "grad_norm": 4.8928704261779785,
      "learning_rate": 4.175642298780333e-05,
      "loss": 1.7653,
      "step": 129500
    },
    {
      "epoch": 9.899931250477428,
      "grad_norm": 6.218359470367432,
      "learning_rate": 4.1750057291268814e-05,
      "loss": 1.8837,
      "step": 129600
    },
    {
      "epoch": 9.907570086318845,
      "grad_norm": 4.917843818664551,
      "learning_rate": 4.17436915947343e-05,
      "loss": 1.9098,
      "step": 129700
    },
    {
      "epoch": 9.915208922160263,
      "grad_norm": 4.470651626586914,
      "learning_rate": 4.173732589819978e-05,
      "loss": 1.9445,
      "step": 129800
    },
    {
      "epoch": 9.92284775800168,
      "grad_norm": 6.626383304595947,
      "learning_rate": 4.173096020166527e-05,
      "loss": 1.8335,
      "step": 129900
    },
    {
      "epoch": 9.930486593843098,
      "grad_norm": 4.804535388946533,
      "learning_rate": 4.1724594505130755e-05,
      "loss": 1.9758,
      "step": 130000
    },
    {
      "epoch": 9.938125429684517,
      "grad_norm": 5.388918399810791,
      "learning_rate": 4.171822880859624e-05,
      "loss": 1.9383,
      "step": 130100
    },
    {
      "epoch": 9.945764265525934,
      "grad_norm": 5.366269111633301,
      "learning_rate": 4.171186311206172e-05,
      "loss": 1.7891,
      "step": 130200
    },
    {
      "epoch": 9.953403101367352,
      "grad_norm": 5.391961574554443,
      "learning_rate": 4.170549741552721e-05,
      "loss": 1.7587,
      "step": 130300
    },
    {
      "epoch": 9.961041937208769,
      "grad_norm": 6.334890842437744,
      "learning_rate": 4.1699131718992696e-05,
      "loss": 1.9505,
      "step": 130400
    },
    {
      "epoch": 9.968680773050187,
      "grad_norm": 5.848935127258301,
      "learning_rate": 4.169276602245818e-05,
      "loss": 1.8868,
      "step": 130500
    },
    {
      "epoch": 9.976319608891606,
      "grad_norm": 5.798586368560791,
      "learning_rate": 4.168640032592366e-05,
      "loss": 1.8989,
      "step": 130600
    },
    {
      "epoch": 9.983958444733023,
      "grad_norm": 4.251210689544678,
      "learning_rate": 4.1680034629389146e-05,
      "loss": 1.8534,
      "step": 130700
    },
    {
      "epoch": 9.99159728057444,
      "grad_norm": 4.301126956939697,
      "learning_rate": 4.167366893285464e-05,
      "loss": 1.9002,
      "step": 130800
    },
    {
      "epoch": 9.999236116415858,
      "grad_norm": 4.404673099517822,
      "learning_rate": 4.166730323632012e-05,
      "loss": 1.8323,
      "step": 130900
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8609400987625122,
      "eval_runtime": 1.5156,
      "eval_samples_per_second": 455.268,
      "eval_steps_per_second": 455.268,
      "step": 130910
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.6505169868469238,
      "eval_runtime": 28.1245,
      "eval_samples_per_second": 465.466,
      "eval_steps_per_second": 465.466,
      "step": 130910
    },
    {
      "epoch": 10.006874952257276,
      "grad_norm": 6.706752777099609,
      "learning_rate": 4.1660937539785604e-05,
      "loss": 1.7926,
      "step": 131000
    },
    {
      "epoch": 10.014513788098693,
      "grad_norm": 6.633245468139648,
      "learning_rate": 4.165457184325109e-05,
      "loss": 1.779,
      "step": 131100
    },
    {
      "epoch": 10.022152623940112,
      "grad_norm": 4.234188079833984,
      "learning_rate": 4.164820614671657e-05,
      "loss": 1.8688,
      "step": 131200
    },
    {
      "epoch": 10.02979145978153,
      "grad_norm": 4.446294784545898,
      "learning_rate": 4.164184045018206e-05,
      "loss": 1.7641,
      "step": 131300
    },
    {
      "epoch": 10.037430295622947,
      "grad_norm": 4.486045837402344,
      "learning_rate": 4.1635474753647545e-05,
      "loss": 1.8978,
      "step": 131400
    },
    {
      "epoch": 10.045069131464365,
      "grad_norm": 6.434033393859863,
      "learning_rate": 4.162910905711303e-05,
      "loss": 1.8012,
      "step": 131500
    },
    {
      "epoch": 10.052707967305782,
      "grad_norm": 4.881091117858887,
      "learning_rate": 4.162274336057851e-05,
      "loss": 1.9118,
      "step": 131600
    },
    {
      "epoch": 10.060346803147201,
      "grad_norm": 5.291952133178711,
      "learning_rate": 4.1616377664044e-05,
      "loss": 1.8439,
      "step": 131700
    },
    {
      "epoch": 10.067985638988619,
      "grad_norm": 4.870322227478027,
      "learning_rate": 4.1610011967509486e-05,
      "loss": 1.9357,
      "step": 131800
    },
    {
      "epoch": 10.075624474830036,
      "grad_norm": 5.011058807373047,
      "learning_rate": 4.160364627097497e-05,
      "loss": 1.7822,
      "step": 131900
    },
    {
      "epoch": 10.083263310671454,
      "grad_norm": 5.80817174911499,
      "learning_rate": 4.159728057444045e-05,
      "loss": 1.848,
      "step": 132000
    },
    {
      "epoch": 10.090902146512871,
      "grad_norm": 6.6157732009887695,
      "learning_rate": 4.159091487790594e-05,
      "loss": 1.8538,
      "step": 132100
    },
    {
      "epoch": 10.098540982354288,
      "grad_norm": 4.5814208984375,
      "learning_rate": 4.158454918137143e-05,
      "loss": 1.9095,
      "step": 132200
    },
    {
      "epoch": 10.106179818195708,
      "grad_norm": 6.971556663513184,
      "learning_rate": 4.157818348483691e-05,
      "loss": 1.8294,
      "step": 132300
    },
    {
      "epoch": 10.113818654037125,
      "grad_norm": 5.425800323486328,
      "learning_rate": 4.15718177883024e-05,
      "loss": 1.8543,
      "step": 132400
    },
    {
      "epoch": 10.121457489878543,
      "grad_norm": 6.554374694824219,
      "learning_rate": 4.1565452091767884e-05,
      "loss": 1.8416,
      "step": 132500
    },
    {
      "epoch": 10.12909632571996,
      "grad_norm": 5.202113151550293,
      "learning_rate": 4.155908639523337e-05,
      "loss": 1.8951,
      "step": 132600
    },
    {
      "epoch": 10.136735161561377,
      "grad_norm": 4.425475597381592,
      "learning_rate": 4.155272069869886e-05,
      "loss": 1.7914,
      "step": 132700
    },
    {
      "epoch": 10.144373997402797,
      "grad_norm": 4.710936546325684,
      "learning_rate": 4.154635500216434e-05,
      "loss": 1.8135,
      "step": 132800
    },
    {
      "epoch": 10.152012833244214,
      "grad_norm": 4.732395648956299,
      "learning_rate": 4.1539989305629825e-05,
      "loss": 1.8298,
      "step": 132900
    },
    {
      "epoch": 10.159651669085632,
      "grad_norm": 6.240360736846924,
      "learning_rate": 4.153362360909531e-05,
      "loss": 1.9237,
      "step": 133000
    },
    {
      "epoch": 10.167290504927049,
      "grad_norm": 4.416039943695068,
      "learning_rate": 4.15272579125608e-05,
      "loss": 1.8519,
      "step": 133100
    },
    {
      "epoch": 10.174929340768466,
      "grad_norm": 5.785648345947266,
      "learning_rate": 4.152089221602628e-05,
      "loss": 1.849,
      "step": 133200
    },
    {
      "epoch": 10.182568176609884,
      "grad_norm": 5.714556694030762,
      "learning_rate": 4.1514526519491766e-05,
      "loss": 1.8252,
      "step": 133300
    },
    {
      "epoch": 10.190207012451303,
      "grad_norm": 5.023358345031738,
      "learning_rate": 4.150816082295725e-05,
      "loss": 1.8693,
      "step": 133400
    },
    {
      "epoch": 10.19784584829272,
      "grad_norm": 4.774245262145996,
      "learning_rate": 4.150179512642274e-05,
      "loss": 1.8365,
      "step": 133500
    },
    {
      "epoch": 10.205484684134138,
      "grad_norm": 5.231987476348877,
      "learning_rate": 4.149542942988822e-05,
      "loss": 1.9784,
      "step": 133600
    },
    {
      "epoch": 10.213123519975555,
      "grad_norm": 8.737183570861816,
      "learning_rate": 4.148906373335371e-05,
      "loss": 1.8856,
      "step": 133700
    },
    {
      "epoch": 10.220762355816973,
      "grad_norm": 3.9530694484710693,
      "learning_rate": 4.148269803681919e-05,
      "loss": 1.7864,
      "step": 133800
    },
    {
      "epoch": 10.228401191658392,
      "grad_norm": 4.6878180503845215,
      "learning_rate": 4.1476332340284674e-05,
      "loss": 1.8359,
      "step": 133900
    },
    {
      "epoch": 10.23604002749981,
      "grad_norm": 8.272879600524902,
      "learning_rate": 4.1469966643750164e-05,
      "loss": 1.8554,
      "step": 134000
    },
    {
      "epoch": 10.243678863341227,
      "grad_norm": 4.511852741241455,
      "learning_rate": 4.146360094721565e-05,
      "loss": 1.8915,
      "step": 134100
    },
    {
      "epoch": 10.251317699182644,
      "grad_norm": 4.114581108093262,
      "learning_rate": 4.145723525068113e-05,
      "loss": 1.8519,
      "step": 134200
    },
    {
      "epoch": 10.258956535024062,
      "grad_norm": 4.496776103973389,
      "learning_rate": 4.1450869554146615e-05,
      "loss": 1.9006,
      "step": 134300
    },
    {
      "epoch": 10.26659537086548,
      "grad_norm": 5.465458393096924,
      "learning_rate": 4.14445038576121e-05,
      "loss": 1.8957,
      "step": 134400
    },
    {
      "epoch": 10.274234206706899,
      "grad_norm": 4.327534198760986,
      "learning_rate": 4.143813816107759e-05,
      "loss": 1.9952,
      "step": 134500
    },
    {
      "epoch": 10.281873042548316,
      "grad_norm": 4.981780529022217,
      "learning_rate": 4.143177246454307e-05,
      "loss": 1.8758,
      "step": 134600
    },
    {
      "epoch": 10.289511878389733,
      "grad_norm": 5.00352668762207,
      "learning_rate": 4.1425406768008556e-05,
      "loss": 1.8673,
      "step": 134700
    },
    {
      "epoch": 10.297150714231151,
      "grad_norm": 3.951958179473877,
      "learning_rate": 4.141904107147404e-05,
      "loss": 1.8883,
      "step": 134800
    },
    {
      "epoch": 10.304789550072568,
      "grad_norm": 4.300780296325684,
      "learning_rate": 4.141267537493953e-05,
      "loss": 1.8061,
      "step": 134900
    },
    {
      "epoch": 10.312428385913986,
      "grad_norm": 5.41746711730957,
      "learning_rate": 4.140630967840501e-05,
      "loss": 1.8939,
      "step": 135000
    },
    {
      "epoch": 10.320067221755405,
      "grad_norm": 6.924165725708008,
      "learning_rate": 4.13999439818705e-05,
      "loss": 1.8618,
      "step": 135100
    },
    {
      "epoch": 10.327706057596822,
      "grad_norm": 6.015490531921387,
      "learning_rate": 4.139357828533598e-05,
      "loss": 1.8787,
      "step": 135200
    },
    {
      "epoch": 10.33534489343824,
      "grad_norm": 4.9992780685424805,
      "learning_rate": 4.1387212588801464e-05,
      "loss": 1.8006,
      "step": 135300
    },
    {
      "epoch": 10.342983729279657,
      "grad_norm": 4.854994297027588,
      "learning_rate": 4.1380846892266954e-05,
      "loss": 1.8732,
      "step": 135400
    },
    {
      "epoch": 10.350622565121075,
      "grad_norm": 6.1212873458862305,
      "learning_rate": 4.137448119573244e-05,
      "loss": 1.7294,
      "step": 135500
    },
    {
      "epoch": 10.358261400962494,
      "grad_norm": 5.907104969024658,
      "learning_rate": 4.136811549919792e-05,
      "loss": 1.7557,
      "step": 135600
    },
    {
      "epoch": 10.365900236803911,
      "grad_norm": 5.25717830657959,
      "learning_rate": 4.1361749802663405e-05,
      "loss": 1.8498,
      "step": 135700
    },
    {
      "epoch": 10.373539072645329,
      "grad_norm": 5.197629928588867,
      "learning_rate": 4.1355384106128895e-05,
      "loss": 1.906,
      "step": 135800
    },
    {
      "epoch": 10.381177908486746,
      "grad_norm": 5.263003826141357,
      "learning_rate": 4.134901840959438e-05,
      "loss": 1.8158,
      "step": 135900
    },
    {
      "epoch": 10.388816744328164,
      "grad_norm": 6.090213298797607,
      "learning_rate": 4.134265271305986e-05,
      "loss": 1.8921,
      "step": 136000
    },
    {
      "epoch": 10.396455580169583,
      "grad_norm": 5.418923377990723,
      "learning_rate": 4.133628701652535e-05,
      "loss": 1.8988,
      "step": 136100
    },
    {
      "epoch": 10.404094416011,
      "grad_norm": 5.29148006439209,
      "learning_rate": 4.1329921319990836e-05,
      "loss": 1.8728,
      "step": 136200
    },
    {
      "epoch": 10.411733251852418,
      "grad_norm": 4.665146827697754,
      "learning_rate": 4.132355562345632e-05,
      "loss": 1.858,
      "step": 136300
    },
    {
      "epoch": 10.419372087693835,
      "grad_norm": 5.786968231201172,
      "learning_rate": 4.131718992692181e-05,
      "loss": 1.9216,
      "step": 136400
    },
    {
      "epoch": 10.427010923535253,
      "grad_norm": 5.112220287322998,
      "learning_rate": 4.131082423038729e-05,
      "loss": 1.8881,
      "step": 136500
    },
    {
      "epoch": 10.43464975937667,
      "grad_norm": 4.835946559906006,
      "learning_rate": 4.130445853385278e-05,
      "loss": 1.9062,
      "step": 136600
    },
    {
      "epoch": 10.44228859521809,
      "grad_norm": 3.890803098678589,
      "learning_rate": 4.129809283731827e-05,
      "loss": 1.8538,
      "step": 136700
    },
    {
      "epoch": 10.449927431059507,
      "grad_norm": 5.880978584289551,
      "learning_rate": 4.129172714078375e-05,
      "loss": 1.8582,
      "step": 136800
    },
    {
      "epoch": 10.457566266900924,
      "grad_norm": 4.826455593109131,
      "learning_rate": 4.1285361444249234e-05,
      "loss": 1.7866,
      "step": 136900
    },
    {
      "epoch": 10.465205102742342,
      "grad_norm": 4.935458660125732,
      "learning_rate": 4.127899574771472e-05,
      "loss": 1.8268,
      "step": 137000
    },
    {
      "epoch": 10.47284393858376,
      "grad_norm": 5.532260894775391,
      "learning_rate": 4.12726300511802e-05,
      "loss": 1.8422,
      "step": 137100
    },
    {
      "epoch": 10.480482774425177,
      "grad_norm": 5.067915439605713,
      "learning_rate": 4.126626435464569e-05,
      "loss": 1.872,
      "step": 137200
    },
    {
      "epoch": 10.488121610266596,
      "grad_norm": 5.032346248626709,
      "learning_rate": 4.1259898658111175e-05,
      "loss": 1.8261,
      "step": 137300
    },
    {
      "epoch": 10.495760446108013,
      "grad_norm": 6.992937088012695,
      "learning_rate": 4.125353296157666e-05,
      "loss": 1.755,
      "step": 137400
    },
    {
      "epoch": 10.50339928194943,
      "grad_norm": 6.736015319824219,
      "learning_rate": 4.124716726504214e-05,
      "loss": 1.8402,
      "step": 137500
    },
    {
      "epoch": 10.511038117790848,
      "grad_norm": 6.218776702880859,
      "learning_rate": 4.1240801568507626e-05,
      "loss": 1.8865,
      "step": 137600
    },
    {
      "epoch": 10.518676953632266,
      "grad_norm": 6.146563529968262,
      "learning_rate": 4.1234435871973116e-05,
      "loss": 1.8355,
      "step": 137700
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 6.670068740844727,
      "learning_rate": 4.12280701754386e-05,
      "loss": 1.861,
      "step": 137800
    },
    {
      "epoch": 10.533954625315102,
      "grad_norm": 4.4622111320495605,
      "learning_rate": 4.122170447890408e-05,
      "loss": 1.8498,
      "step": 137900
    },
    {
      "epoch": 10.54159346115652,
      "grad_norm": 4.456503868103027,
      "learning_rate": 4.1215338782369567e-05,
      "loss": 1.7545,
      "step": 138000
    },
    {
      "epoch": 10.549232296997937,
      "grad_norm": 4.566029071807861,
      "learning_rate": 4.120897308583506e-05,
      "loss": 1.8538,
      "step": 138100
    },
    {
      "epoch": 10.556871132839355,
      "grad_norm": 6.164745330810547,
      "learning_rate": 4.120260738930054e-05,
      "loss": 1.9064,
      "step": 138200
    },
    {
      "epoch": 10.564509968680774,
      "grad_norm": 4.868379592895508,
      "learning_rate": 4.1196241692766024e-05,
      "loss": 1.8803,
      "step": 138300
    },
    {
      "epoch": 10.572148804522191,
      "grad_norm": 6.58086633682251,
      "learning_rate": 4.118987599623151e-05,
      "loss": 2.0003,
      "step": 138400
    },
    {
      "epoch": 10.579787640363609,
      "grad_norm": 4.3243408203125,
      "learning_rate": 4.118351029969699e-05,
      "loss": 1.8038,
      "step": 138500
    },
    {
      "epoch": 10.587426476205026,
      "grad_norm": 6.067200660705566,
      "learning_rate": 4.117714460316248e-05,
      "loss": 1.7198,
      "step": 138600
    },
    {
      "epoch": 10.595065312046444,
      "grad_norm": 5.443403244018555,
      "learning_rate": 4.1170778906627965e-05,
      "loss": 1.912,
      "step": 138700
    },
    {
      "epoch": 10.602704147887861,
      "grad_norm": 5.971498012542725,
      "learning_rate": 4.116441321009345e-05,
      "loss": 1.9144,
      "step": 138800
    },
    {
      "epoch": 10.61034298372928,
      "grad_norm": 5.211988925933838,
      "learning_rate": 4.115804751355893e-05,
      "loss": 1.9106,
      "step": 138900
    },
    {
      "epoch": 10.617981819570698,
      "grad_norm": 6.812435150146484,
      "learning_rate": 4.115168181702442e-05,
      "loss": 1.8705,
      "step": 139000
    },
    {
      "epoch": 10.625620655412115,
      "grad_norm": 4.841569423675537,
      "learning_rate": 4.1145316120489906e-05,
      "loss": 1.9087,
      "step": 139100
    },
    {
      "epoch": 10.633259491253533,
      "grad_norm": 4.803815841674805,
      "learning_rate": 4.113895042395539e-05,
      "loss": 1.8929,
      "step": 139200
    },
    {
      "epoch": 10.64089832709495,
      "grad_norm": 4.176607608795166,
      "learning_rate": 4.113258472742087e-05,
      "loss": 1.9216,
      "step": 139300
    },
    {
      "epoch": 10.648537162936368,
      "grad_norm": 5.03771448135376,
      "learning_rate": 4.1126219030886356e-05,
      "loss": 1.8806,
      "step": 139400
    },
    {
      "epoch": 10.656175998777787,
      "grad_norm": 4.333748817443848,
      "learning_rate": 4.111985333435185e-05,
      "loss": 1.8053,
      "step": 139500
    },
    {
      "epoch": 10.663814834619204,
      "grad_norm": 7.166929244995117,
      "learning_rate": 4.111348763781733e-05,
      "loss": 1.9275,
      "step": 139600
    },
    {
      "epoch": 10.671453670460622,
      "grad_norm": 6.710169315338135,
      "learning_rate": 4.1107121941282814e-05,
      "loss": 1.8683,
      "step": 139700
    },
    {
      "epoch": 10.67909250630204,
      "grad_norm": 5.458677291870117,
      "learning_rate": 4.1100756244748304e-05,
      "loss": 1.8942,
      "step": 139800
    },
    {
      "epoch": 10.686731342143457,
      "grad_norm": 6.161677360534668,
      "learning_rate": 4.109439054821379e-05,
      "loss": 1.8538,
      "step": 139900
    },
    {
      "epoch": 10.694370177984876,
      "grad_norm": 9.385972023010254,
      "learning_rate": 4.108802485167927e-05,
      "loss": 1.8729,
      "step": 140000
    },
    {
      "epoch": 10.702009013826293,
      "grad_norm": 6.973222255706787,
      "learning_rate": 4.108165915514476e-05,
      "loss": 1.8448,
      "step": 140100
    },
    {
      "epoch": 10.70964784966771,
      "grad_norm": 4.954450607299805,
      "learning_rate": 4.1075293458610245e-05,
      "loss": 1.8255,
      "step": 140200
    },
    {
      "epoch": 10.717286685509128,
      "grad_norm": 6.767470359802246,
      "learning_rate": 4.106892776207573e-05,
      "loss": 1.9229,
      "step": 140300
    },
    {
      "epoch": 10.724925521350546,
      "grad_norm": 5.627338886260986,
      "learning_rate": 4.106256206554122e-05,
      "loss": 1.8237,
      "step": 140400
    },
    {
      "epoch": 10.732564357191965,
      "grad_norm": 4.911874294281006,
      "learning_rate": 4.10561963690067e-05,
      "loss": 1.8315,
      "step": 140500
    },
    {
      "epoch": 10.740203193033382,
      "grad_norm": 6.691699504852295,
      "learning_rate": 4.1049830672472186e-05,
      "loss": 1.8739,
      "step": 140600
    },
    {
      "epoch": 10.7478420288748,
      "grad_norm": 3.7525007724761963,
      "learning_rate": 4.104346497593767e-05,
      "loss": 1.9145,
      "step": 140700
    },
    {
      "epoch": 10.755480864716217,
      "grad_norm": 6.60238790512085,
      "learning_rate": 4.103709927940315e-05,
      "loss": 1.8315,
      "step": 140800
    },
    {
      "epoch": 10.763119700557635,
      "grad_norm": 5.26823091506958,
      "learning_rate": 4.103073358286864e-05,
      "loss": 1.8659,
      "step": 140900
    },
    {
      "epoch": 10.770758536399052,
      "grad_norm": 5.843709945678711,
      "learning_rate": 4.102436788633413e-05,
      "loss": 1.881,
      "step": 141000
    },
    {
      "epoch": 10.778397372240471,
      "grad_norm": 4.0984625816345215,
      "learning_rate": 4.101800218979961e-05,
      "loss": 1.8526,
      "step": 141100
    },
    {
      "epoch": 10.786036208081889,
      "grad_norm": 5.549156665802002,
      "learning_rate": 4.1011636493265094e-05,
      "loss": 1.8557,
      "step": 141200
    },
    {
      "epoch": 10.793675043923306,
      "grad_norm": 5.893410682678223,
      "learning_rate": 4.1005270796730584e-05,
      "loss": 1.8548,
      "step": 141300
    },
    {
      "epoch": 10.801313879764724,
      "grad_norm": 6.072484016418457,
      "learning_rate": 4.099890510019607e-05,
      "loss": 1.8696,
      "step": 141400
    },
    {
      "epoch": 10.808952715606141,
      "grad_norm": 4.432455539703369,
      "learning_rate": 4.099253940366155e-05,
      "loss": 1.755,
      "step": 141500
    },
    {
      "epoch": 10.816591551447559,
      "grad_norm": 4.701449394226074,
      "learning_rate": 4.0986173707127035e-05,
      "loss": 1.857,
      "step": 141600
    },
    {
      "epoch": 10.824230387288978,
      "grad_norm": 5.208026885986328,
      "learning_rate": 4.097980801059252e-05,
      "loss": 1.8903,
      "step": 141700
    },
    {
      "epoch": 10.831869223130395,
      "grad_norm": 4.4475016593933105,
      "learning_rate": 4.097344231405801e-05,
      "loss": 1.8656,
      "step": 141800
    },
    {
      "epoch": 10.839508058971813,
      "grad_norm": 4.188937187194824,
      "learning_rate": 4.096707661752349e-05,
      "loss": 1.8684,
      "step": 141900
    },
    {
      "epoch": 10.84714689481323,
      "grad_norm": 6.510696887969971,
      "learning_rate": 4.0960710920988976e-05,
      "loss": 1.8509,
      "step": 142000
    },
    {
      "epoch": 10.854785730654648,
      "grad_norm": 5.493051052093506,
      "learning_rate": 4.095434522445446e-05,
      "loss": 1.9587,
      "step": 142100
    },
    {
      "epoch": 10.862424566496067,
      "grad_norm": 5.087826728820801,
      "learning_rate": 4.094797952791995e-05,
      "loss": 1.9188,
      "step": 142200
    },
    {
      "epoch": 10.870063402337484,
      "grad_norm": 4.634488105773926,
      "learning_rate": 4.094161383138543e-05,
      "loss": 1.8679,
      "step": 142300
    },
    {
      "epoch": 10.877702238178902,
      "grad_norm": 4.747011661529541,
      "learning_rate": 4.093524813485092e-05,
      "loss": 1.8257,
      "step": 142400
    },
    {
      "epoch": 10.88534107402032,
      "grad_norm": 4.825387477874756,
      "learning_rate": 4.09288824383164e-05,
      "loss": 1.869,
      "step": 142500
    },
    {
      "epoch": 10.892979909861737,
      "grad_norm": 5.451625823974609,
      "learning_rate": 4.0922516741781884e-05,
      "loss": 1.8404,
      "step": 142600
    },
    {
      "epoch": 10.900618745703154,
      "grad_norm": 5.485662460327148,
      "learning_rate": 4.0916151045247374e-05,
      "loss": 1.8049,
      "step": 142700
    },
    {
      "epoch": 10.908257581544573,
      "grad_norm": 5.30764102935791,
      "learning_rate": 4.090978534871286e-05,
      "loss": 1.8324,
      "step": 142800
    },
    {
      "epoch": 10.91589641738599,
      "grad_norm": 6.033719062805176,
      "learning_rate": 4.090341965217834e-05,
      "loss": 1.9052,
      "step": 142900
    },
    {
      "epoch": 10.923535253227408,
      "grad_norm": 5.545665264129639,
      "learning_rate": 4.0897053955643825e-05,
      "loss": 1.8285,
      "step": 143000
    },
    {
      "epoch": 10.931174089068826,
      "grad_norm": 3.4788780212402344,
      "learning_rate": 4.089068825910931e-05,
      "loss": 1.7912,
      "step": 143100
    },
    {
      "epoch": 10.938812924910243,
      "grad_norm": 5.170256614685059,
      "learning_rate": 4.08843225625748e-05,
      "loss": 1.9421,
      "step": 143200
    },
    {
      "epoch": 10.946451760751662,
      "grad_norm": 4.952393531799316,
      "learning_rate": 4.087795686604028e-05,
      "loss": 1.8416,
      "step": 143300
    },
    {
      "epoch": 10.95409059659308,
      "grad_norm": 6.264578819274902,
      "learning_rate": 4.0871591169505766e-05,
      "loss": 1.8663,
      "step": 143400
    },
    {
      "epoch": 10.961729432434497,
      "grad_norm": 5.530966758728027,
      "learning_rate": 4.086522547297125e-05,
      "loss": 1.8676,
      "step": 143500
    },
    {
      "epoch": 10.969368268275915,
      "grad_norm": 5.250735759735107,
      "learning_rate": 4.085885977643674e-05,
      "loss": 1.8761,
      "step": 143600
    },
    {
      "epoch": 10.977007104117332,
      "grad_norm": 3.5249621868133545,
      "learning_rate": 4.085249407990222e-05,
      "loss": 1.9205,
      "step": 143700
    },
    {
      "epoch": 10.98464593995875,
      "grad_norm": 6.176119327545166,
      "learning_rate": 4.0846128383367707e-05,
      "loss": 1.8676,
      "step": 143800
    },
    {
      "epoch": 10.992284775800169,
      "grad_norm": 6.261103630065918,
      "learning_rate": 4.08397626868332e-05,
      "loss": 1.8649,
      "step": 143900
    },
    {
      "epoch": 10.999923611641586,
      "grad_norm": 5.860236644744873,
      "learning_rate": 4.083339699029868e-05,
      "loss": 1.7846,
      "step": 144000
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.8539578914642334,
      "eval_runtime": 1.4978,
      "eval_samples_per_second": 460.676,
      "eval_steps_per_second": 460.676,
      "step": 144001
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.6357455253601074,
      "eval_runtime": 27.8347,
      "eval_samples_per_second": 470.312,
      "eval_steps_per_second": 470.312,
      "step": 144001
    },
    {
      "epoch": 11.007562447483004,
      "grad_norm": 6.030253887176514,
      "learning_rate": 4.082703129376417e-05,
      "loss": 1.8429,
      "step": 144100
    },
    {
      "epoch": 11.015201283324421,
      "grad_norm": 5.655215740203857,
      "learning_rate": 4.0820665597229654e-05,
      "loss": 1.8778,
      "step": 144200
    },
    {
      "epoch": 11.022840119165839,
      "grad_norm": 3.8039684295654297,
      "learning_rate": 4.081429990069514e-05,
      "loss": 1.8906,
      "step": 144300
    },
    {
      "epoch": 11.030478955007258,
      "grad_norm": 4.9105448722839355,
      "learning_rate": 4.080793420416062e-05,
      "loss": 1.7694,
      "step": 144400
    },
    {
      "epoch": 11.038117790848675,
      "grad_norm": 5.525341510772705,
      "learning_rate": 4.080156850762611e-05,
      "loss": 1.8905,
      "step": 144500
    },
    {
      "epoch": 11.045756626690093,
      "grad_norm": 7.52587366104126,
      "learning_rate": 4.0795202811091595e-05,
      "loss": 1.7941,
      "step": 144600
    },
    {
      "epoch": 11.05339546253151,
      "grad_norm": 4.799932479858398,
      "learning_rate": 4.078883711455708e-05,
      "loss": 1.9202,
      "step": 144700
    },
    {
      "epoch": 11.061034298372928,
      "grad_norm": 6.075981140136719,
      "learning_rate": 4.078247141802256e-05,
      "loss": 1.8425,
      "step": 144800
    },
    {
      "epoch": 11.068673134214345,
      "grad_norm": 5.232790470123291,
      "learning_rate": 4.0776105721488046e-05,
      "loss": 1.8084,
      "step": 144900
    },
    {
      "epoch": 11.076311970055764,
      "grad_norm": 6.094871997833252,
      "learning_rate": 4.0769740024953536e-05,
      "loss": 1.8544,
      "step": 145000
    },
    {
      "epoch": 11.083950805897182,
      "grad_norm": 6.991426467895508,
      "learning_rate": 4.076337432841902e-05,
      "loss": 1.8595,
      "step": 145100
    },
    {
      "epoch": 11.091589641738599,
      "grad_norm": 5.294029712677002,
      "learning_rate": 4.07570086318845e-05,
      "loss": 1.8966,
      "step": 145200
    },
    {
      "epoch": 11.099228477580017,
      "grad_norm": 4.626865386962891,
      "learning_rate": 4.075064293534999e-05,
      "loss": 1.8435,
      "step": 145300
    },
    {
      "epoch": 11.106867313421434,
      "grad_norm": 5.750087738037109,
      "learning_rate": 4.074427723881548e-05,
      "loss": 1.9175,
      "step": 145400
    },
    {
      "epoch": 11.114506149262853,
      "grad_norm": 6.642927646636963,
      "learning_rate": 4.073791154228096e-05,
      "loss": 1.8167,
      "step": 145500
    },
    {
      "epoch": 11.12214498510427,
      "grad_norm": 5.722772598266602,
      "learning_rate": 4.0731545845746444e-05,
      "loss": 1.9113,
      "step": 145600
    },
    {
      "epoch": 11.129783820945688,
      "grad_norm": 5.835316181182861,
      "learning_rate": 4.072518014921193e-05,
      "loss": 1.8509,
      "step": 145700
    },
    {
      "epoch": 11.137422656787106,
      "grad_norm": 6.429225444793701,
      "learning_rate": 4.071881445267741e-05,
      "loss": 1.7761,
      "step": 145800
    },
    {
      "epoch": 11.145061492628523,
      "grad_norm": 5.024006366729736,
      "learning_rate": 4.07124487561429e-05,
      "loss": 1.9028,
      "step": 145900
    },
    {
      "epoch": 11.15270032846994,
      "grad_norm": 5.545507907867432,
      "learning_rate": 4.0706083059608385e-05,
      "loss": 1.8375,
      "step": 146000
    },
    {
      "epoch": 11.16033916431136,
      "grad_norm": 4.964239120483398,
      "learning_rate": 4.069971736307387e-05,
      "loss": 1.8269,
      "step": 146100
    },
    {
      "epoch": 11.167978000152777,
      "grad_norm": 4.984625339508057,
      "learning_rate": 4.069335166653935e-05,
      "loss": 1.8605,
      "step": 146200
    },
    {
      "epoch": 11.175616835994195,
      "grad_norm": 4.768532752990723,
      "learning_rate": 4.0686985970004836e-05,
      "loss": 1.8014,
      "step": 146300
    },
    {
      "epoch": 11.183255671835612,
      "grad_norm": 6.9097394943237305,
      "learning_rate": 4.0680620273470326e-05,
      "loss": 1.7984,
      "step": 146400
    },
    {
      "epoch": 11.19089450767703,
      "grad_norm": 6.477261066436768,
      "learning_rate": 4.067425457693581e-05,
      "loss": 1.9215,
      "step": 146500
    },
    {
      "epoch": 11.198533343518449,
      "grad_norm": 5.811918258666992,
      "learning_rate": 4.066788888040129e-05,
      "loss": 1.8741,
      "step": 146600
    },
    {
      "epoch": 11.206172179359866,
      "grad_norm": 6.403720855712891,
      "learning_rate": 4.0661523183866777e-05,
      "loss": 1.97,
      "step": 146700
    },
    {
      "epoch": 11.213811015201284,
      "grad_norm": 5.967005729675293,
      "learning_rate": 4.065515748733227e-05,
      "loss": 1.8487,
      "step": 146800
    },
    {
      "epoch": 11.221449851042701,
      "grad_norm": 4.308151721954346,
      "learning_rate": 4.064879179079775e-05,
      "loss": 1.7647,
      "step": 146900
    },
    {
      "epoch": 11.229088686884118,
      "grad_norm": 6.793034553527832,
      "learning_rate": 4.0642426094263234e-05,
      "loss": 1.7498,
      "step": 147000
    },
    {
      "epoch": 11.236727522725536,
      "grad_norm": 4.175665855407715,
      "learning_rate": 4.063606039772872e-05,
      "loss": 1.8488,
      "step": 147100
    },
    {
      "epoch": 11.244366358566955,
      "grad_norm": 4.68775749206543,
      "learning_rate": 4.06296947011942e-05,
      "loss": 1.7717,
      "step": 147200
    },
    {
      "epoch": 11.252005194408373,
      "grad_norm": 5.180821418762207,
      "learning_rate": 4.062332900465969e-05,
      "loss": 1.8166,
      "step": 147300
    },
    {
      "epoch": 11.25964403024979,
      "grad_norm": 3.584824562072754,
      "learning_rate": 4.0616963308125175e-05,
      "loss": 1.8969,
      "step": 147400
    },
    {
      "epoch": 11.267282866091207,
      "grad_norm": 4.1817193031311035,
      "learning_rate": 4.061059761159066e-05,
      "loss": 1.7764,
      "step": 147500
    },
    {
      "epoch": 11.274921701932625,
      "grad_norm": 5.5145368576049805,
      "learning_rate": 4.060423191505615e-05,
      "loss": 1.8132,
      "step": 147600
    },
    {
      "epoch": 11.282560537774042,
      "grad_norm": 5.605390548706055,
      "learning_rate": 4.059786621852163e-05,
      "loss": 1.7968,
      "step": 147700
    },
    {
      "epoch": 11.290199373615462,
      "grad_norm": 6.512953758239746,
      "learning_rate": 4.0591500521987116e-05,
      "loss": 1.827,
      "step": 147800
    },
    {
      "epoch": 11.297838209456879,
      "grad_norm": 4.446318626403809,
      "learning_rate": 4.0585134825452606e-05,
      "loss": 1.9707,
      "step": 147900
    },
    {
      "epoch": 11.305477045298296,
      "grad_norm": 5.156891345977783,
      "learning_rate": 4.057876912891809e-05,
      "loss": 1.9218,
      "step": 148000
    },
    {
      "epoch": 11.313115881139714,
      "grad_norm": 9.18498420715332,
      "learning_rate": 4.057240343238357e-05,
      "loss": 1.8539,
      "step": 148100
    },
    {
      "epoch": 11.320754716981131,
      "grad_norm": 4.518728256225586,
      "learning_rate": 4.0566037735849064e-05,
      "loss": 1.7514,
      "step": 148200
    },
    {
      "epoch": 11.32839355282255,
      "grad_norm": 5.928748607635498,
      "learning_rate": 4.055967203931455e-05,
      "loss": 1.8641,
      "step": 148300
    },
    {
      "epoch": 11.336032388663968,
      "grad_norm": 4.54873514175415,
      "learning_rate": 4.055330634278003e-05,
      "loss": 1.8798,
      "step": 148400
    },
    {
      "epoch": 11.343671224505385,
      "grad_norm": 5.19312047958374,
      "learning_rate": 4.0546940646245514e-05,
      "loss": 1.8585,
      "step": 148500
    },
    {
      "epoch": 11.351310060346803,
      "grad_norm": 4.415255546569824,
      "learning_rate": 4.0540574949711e-05,
      "loss": 1.8626,
      "step": 148600
    },
    {
      "epoch": 11.35894889618822,
      "grad_norm": 5.41362190246582,
      "learning_rate": 4.053420925317649e-05,
      "loss": 1.787,
      "step": 148700
    },
    {
      "epoch": 11.36658773202964,
      "grad_norm": 6.010000228881836,
      "learning_rate": 4.052784355664197e-05,
      "loss": 1.8603,
      "step": 148800
    },
    {
      "epoch": 11.374226567871057,
      "grad_norm": 9.882304191589355,
      "learning_rate": 4.0521477860107455e-05,
      "loss": 1.8745,
      "step": 148900
    },
    {
      "epoch": 11.381865403712474,
      "grad_norm": 4.978533744812012,
      "learning_rate": 4.051511216357294e-05,
      "loss": 1.8155,
      "step": 149000
    },
    {
      "epoch": 11.389504239553892,
      "grad_norm": 6.97443151473999,
      "learning_rate": 4.050874646703843e-05,
      "loss": 1.7664,
      "step": 149100
    },
    {
      "epoch": 11.39714307539531,
      "grad_norm": 5.12738037109375,
      "learning_rate": 4.050238077050391e-05,
      "loss": 1.9095,
      "step": 149200
    },
    {
      "epoch": 11.404781911236727,
      "grad_norm": 5.117856025695801,
      "learning_rate": 4.0496015073969396e-05,
      "loss": 1.8564,
      "step": 149300
    },
    {
      "epoch": 11.412420747078146,
      "grad_norm": 4.121090412139893,
      "learning_rate": 4.048964937743488e-05,
      "loss": 1.8645,
      "step": 149400
    },
    {
      "epoch": 11.420059582919563,
      "grad_norm": 4.225499629974365,
      "learning_rate": 4.048328368090036e-05,
      "loss": 1.7734,
      "step": 149500
    },
    {
      "epoch": 11.427698418760981,
      "grad_norm": 6.54106330871582,
      "learning_rate": 4.047691798436585e-05,
      "loss": 1.8297,
      "step": 149600
    },
    {
      "epoch": 11.435337254602398,
      "grad_norm": 4.592362880706787,
      "learning_rate": 4.047055228783134e-05,
      "loss": 1.9601,
      "step": 149700
    },
    {
      "epoch": 11.442976090443816,
      "grad_norm": 4.278637409210205,
      "learning_rate": 4.046418659129682e-05,
      "loss": 1.889,
      "step": 149800
    },
    {
      "epoch": 11.450614926285233,
      "grad_norm": 5.17339563369751,
      "learning_rate": 4.0457820894762304e-05,
      "loss": 1.8438,
      "step": 149900
    },
    {
      "epoch": 11.458253762126652,
      "grad_norm": 5.995450973510742,
      "learning_rate": 4.0451455198227794e-05,
      "loss": 1.8067,
      "step": 150000
    },
    {
      "epoch": 11.46589259796807,
      "grad_norm": 4.450592994689941,
      "learning_rate": 4.044508950169328e-05,
      "loss": 1.8528,
      "step": 150100
    },
    {
      "epoch": 11.473531433809487,
      "grad_norm": 5.719061374664307,
      "learning_rate": 4.043872380515876e-05,
      "loss": 1.7929,
      "step": 150200
    },
    {
      "epoch": 11.481170269650905,
      "grad_norm": 4.258737564086914,
      "learning_rate": 4.0432358108624245e-05,
      "loss": 1.8131,
      "step": 150300
    },
    {
      "epoch": 11.488809105492322,
      "grad_norm": 5.429071426391602,
      "learning_rate": 4.042599241208973e-05,
      "loss": 1.8391,
      "step": 150400
    },
    {
      "epoch": 11.496447941333741,
      "grad_norm": 4.997654914855957,
      "learning_rate": 4.041962671555522e-05,
      "loss": 1.8221,
      "step": 150500
    },
    {
      "epoch": 11.504086777175159,
      "grad_norm": 4.6886515617370605,
      "learning_rate": 4.04132610190207e-05,
      "loss": 1.9159,
      "step": 150600
    },
    {
      "epoch": 11.511725613016576,
      "grad_norm": 3.546384334564209,
      "learning_rate": 4.0406895322486186e-05,
      "loss": 1.8415,
      "step": 150700
    },
    {
      "epoch": 11.519364448857994,
      "grad_norm": 5.589832782745361,
      "learning_rate": 4.040052962595167e-05,
      "loss": 1.8681,
      "step": 150800
    },
    {
      "epoch": 11.527003284699411,
      "grad_norm": 5.190148830413818,
      "learning_rate": 4.039416392941716e-05,
      "loss": 1.8155,
      "step": 150900
    },
    {
      "epoch": 11.53464212054083,
      "grad_norm": 5.655953884124756,
      "learning_rate": 4.038779823288264e-05,
      "loss": 1.8286,
      "step": 151000
    },
    {
      "epoch": 11.542280956382248,
      "grad_norm": 6.620731830596924,
      "learning_rate": 4.038143253634813e-05,
      "loss": 1.815,
      "step": 151100
    },
    {
      "epoch": 11.549919792223665,
      "grad_norm": 5.195694923400879,
      "learning_rate": 4.037506683981361e-05,
      "loss": 1.7829,
      "step": 151200
    },
    {
      "epoch": 11.557558628065083,
      "grad_norm": 5.218443870544434,
      "learning_rate": 4.03687011432791e-05,
      "loss": 1.8958,
      "step": 151300
    },
    {
      "epoch": 11.5651974639065,
      "grad_norm": 5.8651838302612305,
      "learning_rate": 4.0362335446744584e-05,
      "loss": 1.7923,
      "step": 151400
    },
    {
      "epoch": 11.572836299747918,
      "grad_norm": 5.541748046875,
      "learning_rate": 4.035596975021007e-05,
      "loss": 1.9159,
      "step": 151500
    },
    {
      "epoch": 11.580475135589337,
      "grad_norm": 5.533249855041504,
      "learning_rate": 4.034960405367556e-05,
      "loss": 1.7824,
      "step": 151600
    },
    {
      "epoch": 11.588113971430754,
      "grad_norm": 5.546606063842773,
      "learning_rate": 4.034323835714104e-05,
      "loss": 1.7917,
      "step": 151700
    },
    {
      "epoch": 11.595752807272172,
      "grad_norm": 3.7614402770996094,
      "learning_rate": 4.0336872660606525e-05,
      "loss": 1.92,
      "step": 151800
    },
    {
      "epoch": 11.60339164311359,
      "grad_norm": 6.133822441101074,
      "learning_rate": 4.0330506964072015e-05,
      "loss": 1.824,
      "step": 151900
    },
    {
      "epoch": 11.611030478955007,
      "grad_norm": 4.941623210906982,
      "learning_rate": 4.03241412675375e-05,
      "loss": 1.7785,
      "step": 152000
    },
    {
      "epoch": 11.618669314796424,
      "grad_norm": 4.42753791809082,
      "learning_rate": 4.031777557100298e-05,
      "loss": 1.9185,
      "step": 152100
    },
    {
      "epoch": 11.626308150637843,
      "grad_norm": 8.931780815124512,
      "learning_rate": 4.0311409874468466e-05,
      "loss": 1.924,
      "step": 152200
    },
    {
      "epoch": 11.63394698647926,
      "grad_norm": 6.032554626464844,
      "learning_rate": 4.0305044177933956e-05,
      "loss": 1.8695,
      "step": 152300
    },
    {
      "epoch": 11.641585822320678,
      "grad_norm": 4.886072635650635,
      "learning_rate": 4.029867848139944e-05,
      "loss": 1.7983,
      "step": 152400
    },
    {
      "epoch": 11.649224658162096,
      "grad_norm": 4.721553325653076,
      "learning_rate": 4.029231278486492e-05,
      "loss": 1.7938,
      "step": 152500
    },
    {
      "epoch": 11.656863494003513,
      "grad_norm": 3.890550374984741,
      "learning_rate": 4.028594708833041e-05,
      "loss": 1.861,
      "step": 152600
    },
    {
      "epoch": 11.664502329844932,
      "grad_norm": 5.862076282501221,
      "learning_rate": 4.027958139179589e-05,
      "loss": 1.7812,
      "step": 152700
    },
    {
      "epoch": 11.67214116568635,
      "grad_norm": 9.1677885055542,
      "learning_rate": 4.027321569526138e-05,
      "loss": 1.9017,
      "step": 152800
    },
    {
      "epoch": 11.679780001527767,
      "grad_norm": 4.61500358581543,
      "learning_rate": 4.0266849998726864e-05,
      "loss": 1.923,
      "step": 152900
    },
    {
      "epoch": 11.687418837369185,
      "grad_norm": 6.499555587768555,
      "learning_rate": 4.026048430219235e-05,
      "loss": 1.9487,
      "step": 153000
    },
    {
      "epoch": 11.695057673210602,
      "grad_norm": 3.868093252182007,
      "learning_rate": 4.025411860565783e-05,
      "loss": 1.8295,
      "step": 153100
    },
    {
      "epoch": 11.702696509052021,
      "grad_norm": 5.215762138366699,
      "learning_rate": 4.024775290912332e-05,
      "loss": 1.8713,
      "step": 153200
    },
    {
      "epoch": 11.710335344893439,
      "grad_norm": 7.369318008422852,
      "learning_rate": 4.0241387212588805e-05,
      "loss": 1.819,
      "step": 153300
    },
    {
      "epoch": 11.717974180734856,
      "grad_norm": 4.3237128257751465,
      "learning_rate": 4.023502151605429e-05,
      "loss": 1.9286,
      "step": 153400
    },
    {
      "epoch": 11.725613016576274,
      "grad_norm": 5.489984035491943,
      "learning_rate": 4.022865581951977e-05,
      "loss": 1.8803,
      "step": 153500
    },
    {
      "epoch": 11.733251852417691,
      "grad_norm": 7.887074947357178,
      "learning_rate": 4.0222290122985256e-05,
      "loss": 1.8295,
      "step": 153600
    },
    {
      "epoch": 11.740890688259109,
      "grad_norm": 5.443108081817627,
      "learning_rate": 4.0215924426450746e-05,
      "loss": 1.8786,
      "step": 153700
    },
    {
      "epoch": 11.748529524100528,
      "grad_norm": 4.322096824645996,
      "learning_rate": 4.020955872991623e-05,
      "loss": 1.7924,
      "step": 153800
    },
    {
      "epoch": 11.756168359941945,
      "grad_norm": 5.2919392585754395,
      "learning_rate": 4.020319303338171e-05,
      "loss": 1.8533,
      "step": 153900
    },
    {
      "epoch": 11.763807195783363,
      "grad_norm": 5.377161026000977,
      "learning_rate": 4.01968273368472e-05,
      "loss": 1.7938,
      "step": 154000
    },
    {
      "epoch": 11.77144603162478,
      "grad_norm": 5.039187908172607,
      "learning_rate": 4.019046164031269e-05,
      "loss": 1.8258,
      "step": 154100
    },
    {
      "epoch": 11.779084867466198,
      "grad_norm": 6.513119220733643,
      "learning_rate": 4.018409594377817e-05,
      "loss": 1.8902,
      "step": 154200
    },
    {
      "epoch": 11.786723703307615,
      "grad_norm": 6.352049827575684,
      "learning_rate": 4.0177730247243654e-05,
      "loss": 1.8457,
      "step": 154300
    },
    {
      "epoch": 11.794362539149034,
      "grad_norm": 5.0807108879089355,
      "learning_rate": 4.017136455070914e-05,
      "loss": 1.8845,
      "step": 154400
    },
    {
      "epoch": 11.802001374990452,
      "grad_norm": 4.730208396911621,
      "learning_rate": 4.016499885417462e-05,
      "loss": 1.8476,
      "step": 154500
    },
    {
      "epoch": 11.80964021083187,
      "grad_norm": 5.828777313232422,
      "learning_rate": 4.015863315764011e-05,
      "loss": 1.8356,
      "step": 154600
    },
    {
      "epoch": 11.817279046673287,
      "grad_norm": 5.245401382446289,
      "learning_rate": 4.0152267461105595e-05,
      "loss": 1.9099,
      "step": 154700
    },
    {
      "epoch": 11.824917882514704,
      "grad_norm": 6.56407356262207,
      "learning_rate": 4.014590176457108e-05,
      "loss": 1.8854,
      "step": 154800
    },
    {
      "epoch": 11.832556718356123,
      "grad_norm": 5.518560409545898,
      "learning_rate": 4.013953606803656e-05,
      "loss": 1.8667,
      "step": 154900
    },
    {
      "epoch": 11.84019555419754,
      "grad_norm": 6.274546146392822,
      "learning_rate": 4.0133170371502046e-05,
      "loss": 1.9017,
      "step": 155000
    },
    {
      "epoch": 11.847834390038958,
      "grad_norm": 5.631549835205078,
      "learning_rate": 4.0126804674967536e-05,
      "loss": 1.9445,
      "step": 155100
    },
    {
      "epoch": 11.855473225880376,
      "grad_norm": 4.802612781524658,
      "learning_rate": 4.012043897843302e-05,
      "loss": 1.7471,
      "step": 155200
    },
    {
      "epoch": 11.863112061721793,
      "grad_norm": 5.760357856750488,
      "learning_rate": 4.011407328189851e-05,
      "loss": 1.8632,
      "step": 155300
    },
    {
      "epoch": 11.87075089756321,
      "grad_norm": 5.6391425132751465,
      "learning_rate": 4.010770758536399e-05,
      "loss": 1.8056,
      "step": 155400
    },
    {
      "epoch": 11.87838973340463,
      "grad_norm": 5.161311626434326,
      "learning_rate": 4.010134188882948e-05,
      "loss": 1.8837,
      "step": 155500
    },
    {
      "epoch": 11.886028569246047,
      "grad_norm": 4.777400493621826,
      "learning_rate": 4.009497619229497e-05,
      "loss": 1.8177,
      "step": 155600
    },
    {
      "epoch": 11.893667405087465,
      "grad_norm": 4.977406024932861,
      "learning_rate": 4.008861049576045e-05,
      "loss": 1.8445,
      "step": 155700
    },
    {
      "epoch": 11.901306240928882,
      "grad_norm": 5.845963478088379,
      "learning_rate": 4.0082244799225934e-05,
      "loss": 1.8455,
      "step": 155800
    },
    {
      "epoch": 11.9089450767703,
      "grad_norm": 4.666486740112305,
      "learning_rate": 4.007587910269142e-05,
      "loss": 1.8451,
      "step": 155900
    },
    {
      "epoch": 11.916583912611719,
      "grad_norm": 4.661685943603516,
      "learning_rate": 4.006951340615691e-05,
      "loss": 1.8686,
      "step": 156000
    },
    {
      "epoch": 11.924222748453136,
      "grad_norm": 5.532107830047607,
      "learning_rate": 4.006314770962239e-05,
      "loss": 1.9327,
      "step": 156100
    },
    {
      "epoch": 11.931861584294554,
      "grad_norm": 5.096184253692627,
      "learning_rate": 4.0056782013087875e-05,
      "loss": 1.9185,
      "step": 156200
    },
    {
      "epoch": 11.939500420135971,
      "grad_norm": 4.326228618621826,
      "learning_rate": 4.005041631655336e-05,
      "loss": 1.8407,
      "step": 156300
    },
    {
      "epoch": 11.947139255977389,
      "grad_norm": 5.29226541519165,
      "learning_rate": 4.004405062001885e-05,
      "loss": 1.889,
      "step": 156400
    },
    {
      "epoch": 11.954778091818806,
      "grad_norm": 3.6224963665008545,
      "learning_rate": 4.003768492348433e-05,
      "loss": 1.7874,
      "step": 156500
    },
    {
      "epoch": 11.962416927660225,
      "grad_norm": 5.3562541007995605,
      "learning_rate": 4.0031319226949816e-05,
      "loss": 1.8577,
      "step": 156600
    },
    {
      "epoch": 11.970055763501643,
      "grad_norm": 6.1767897605896,
      "learning_rate": 4.00249535304153e-05,
      "loss": 1.7607,
      "step": 156700
    },
    {
      "epoch": 11.97769459934306,
      "grad_norm": 4.895781993865967,
      "learning_rate": 4.001858783388078e-05,
      "loss": 1.9094,
      "step": 156800
    },
    {
      "epoch": 11.985333435184478,
      "grad_norm": 5.535770893096924,
      "learning_rate": 4.0012222137346274e-05,
      "loss": 1.8291,
      "step": 156900
    },
    {
      "epoch": 11.992972271025895,
      "grad_norm": 5.5770134925842285,
      "learning_rate": 4.000585644081176e-05,
      "loss": 1.7517,
      "step": 157000
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.8469613790512085,
      "eval_runtime": 1.4976,
      "eval_samples_per_second": 460.751,
      "eval_steps_per_second": 460.751,
      "step": 157092
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.6212811470031738,
      "eval_runtime": 27.747,
      "eval_samples_per_second": 471.799,
      "eval_steps_per_second": 471.799,
      "step": 157092
    },
    {
      "epoch": 12.000611106867314,
      "grad_norm": 6.446471214294434,
      "learning_rate": 3.999949074427724e-05,
      "loss": 1.779,
      "step": 157100
    },
    {
      "epoch": 12.008249942708732,
      "grad_norm": 6.937873840332031,
      "learning_rate": 3.9993125047742724e-05,
      "loss": 1.7363,
      "step": 157200
    },
    {
      "epoch": 12.01588877855015,
      "grad_norm": 6.368455410003662,
      "learning_rate": 3.998675935120821e-05,
      "loss": 1.8567,
      "step": 157300
    },
    {
      "epoch": 12.023527614391567,
      "grad_norm": 6.562458038330078,
      "learning_rate": 3.99803936546737e-05,
      "loss": 1.875,
      "step": 157400
    },
    {
      "epoch": 12.031166450232984,
      "grad_norm": 4.698304653167725,
      "learning_rate": 3.997402795813918e-05,
      "loss": 1.8899,
      "step": 157500
    },
    {
      "epoch": 12.038805286074401,
      "grad_norm": 5.560490608215332,
      "learning_rate": 3.9967662261604665e-05,
      "loss": 1.9167,
      "step": 157600
    },
    {
      "epoch": 12.04644412191582,
      "grad_norm": 4.694397926330566,
      "learning_rate": 3.996129656507015e-05,
      "loss": 1.8702,
      "step": 157700
    },
    {
      "epoch": 12.054082957757238,
      "grad_norm": 5.583373069763184,
      "learning_rate": 3.995493086853564e-05,
      "loss": 1.8997,
      "step": 157800
    },
    {
      "epoch": 12.061721793598656,
      "grad_norm": 4.995743751525879,
      "learning_rate": 3.994856517200112e-05,
      "loss": 1.8321,
      "step": 157900
    },
    {
      "epoch": 12.069360629440073,
      "grad_norm": 5.5294976234436035,
      "learning_rate": 3.9942199475466606e-05,
      "loss": 1.8136,
      "step": 158000
    },
    {
      "epoch": 12.07699946528149,
      "grad_norm": 4.909212112426758,
      "learning_rate": 3.993583377893209e-05,
      "loss": 1.8265,
      "step": 158100
    },
    {
      "epoch": 12.08463830112291,
      "grad_norm": 5.640551567077637,
      "learning_rate": 3.992946808239757e-05,
      "loss": 1.857,
      "step": 158200
    },
    {
      "epoch": 12.092277136964327,
      "grad_norm": 5.105077743530273,
      "learning_rate": 3.992310238586306e-05,
      "loss": 1.7173,
      "step": 158300
    },
    {
      "epoch": 12.099915972805745,
      "grad_norm": 5.5635504722595215,
      "learning_rate": 3.991673668932855e-05,
      "loss": 1.9075,
      "step": 158400
    },
    {
      "epoch": 12.107554808647162,
      "grad_norm": 8.100696563720703,
      "learning_rate": 3.991037099279403e-05,
      "loss": 1.7911,
      "step": 158500
    },
    {
      "epoch": 12.11519364448858,
      "grad_norm": 5.242899417877197,
      "learning_rate": 3.9904005296259514e-05,
      "loss": 1.8511,
      "step": 158600
    },
    {
      "epoch": 12.122832480329997,
      "grad_norm": 5.246454238891602,
      "learning_rate": 3.9897639599725004e-05,
      "loss": 1.7688,
      "step": 158700
    },
    {
      "epoch": 12.130471316171416,
      "grad_norm": 4.741710186004639,
      "learning_rate": 3.989127390319049e-05,
      "loss": 1.8953,
      "step": 158800
    },
    {
      "epoch": 12.138110152012834,
      "grad_norm": 6.620207786560059,
      "learning_rate": 3.988490820665597e-05,
      "loss": 1.8512,
      "step": 158900
    },
    {
      "epoch": 12.145748987854251,
      "grad_norm": 7.531042098999023,
      "learning_rate": 3.9878542510121455e-05,
      "loss": 1.807,
      "step": 159000
    },
    {
      "epoch": 12.153387823695669,
      "grad_norm": 5.834083557128906,
      "learning_rate": 3.9872176813586945e-05,
      "loss": 1.9312,
      "step": 159100
    },
    {
      "epoch": 12.161026659537086,
      "grad_norm": 5.8931097984313965,
      "learning_rate": 3.986581111705243e-05,
      "loss": 1.9144,
      "step": 159200
    },
    {
      "epoch": 12.168665495378505,
      "grad_norm": 2.757404327392578,
      "learning_rate": 3.985944542051792e-05,
      "loss": 1.8646,
      "step": 159300
    },
    {
      "epoch": 12.176304331219923,
      "grad_norm": 5.269337177276611,
      "learning_rate": 3.98530797239834e-05,
      "loss": 1.8185,
      "step": 159400
    },
    {
      "epoch": 12.18394316706134,
      "grad_norm": 5.746139049530029,
      "learning_rate": 3.9846714027448886e-05,
      "loss": 1.7212,
      "step": 159500
    },
    {
      "epoch": 12.191582002902758,
      "grad_norm": 4.418362140655518,
      "learning_rate": 3.9840348330914376e-05,
      "loss": 1.8409,
      "step": 159600
    },
    {
      "epoch": 12.199220838744175,
      "grad_norm": 5.664613246917725,
      "learning_rate": 3.983398263437986e-05,
      "loss": 1.8059,
      "step": 159700
    },
    {
      "epoch": 12.206859674585592,
      "grad_norm": 3.708869695663452,
      "learning_rate": 3.9827616937845344e-05,
      "loss": 1.7687,
      "step": 159800
    },
    {
      "epoch": 12.214498510427012,
      "grad_norm": 4.982434272766113,
      "learning_rate": 3.982125124131083e-05,
      "loss": 1.8214,
      "step": 159900
    },
    {
      "epoch": 12.222137346268429,
      "grad_norm": 4.958555221557617,
      "learning_rate": 3.981488554477631e-05,
      "loss": 1.8036,
      "step": 160000
    },
    {
      "epoch": 12.229776182109847,
      "grad_norm": 5.28061580657959,
      "learning_rate": 3.98085198482418e-05,
      "loss": 1.8608,
      "step": 160100
    },
    {
      "epoch": 12.237415017951264,
      "grad_norm": 4.689594745635986,
      "learning_rate": 3.9802154151707284e-05,
      "loss": 1.8789,
      "step": 160200
    },
    {
      "epoch": 12.245053853792681,
      "grad_norm": 7.768669128417969,
      "learning_rate": 3.979578845517277e-05,
      "loss": 1.8668,
      "step": 160300
    },
    {
      "epoch": 12.252692689634099,
      "grad_norm": 5.500535011291504,
      "learning_rate": 3.978942275863825e-05,
      "loss": 1.7922,
      "step": 160400
    },
    {
      "epoch": 12.260331525475518,
      "grad_norm": 4.391575813293457,
      "learning_rate": 3.9783057062103735e-05,
      "loss": 1.8751,
      "step": 160500
    },
    {
      "epoch": 12.267970361316936,
      "grad_norm": 5.849666595458984,
      "learning_rate": 3.9776691365569225e-05,
      "loss": 1.8163,
      "step": 160600
    },
    {
      "epoch": 12.275609197158353,
      "grad_norm": 6.558366298675537,
      "learning_rate": 3.977032566903471e-05,
      "loss": 1.9049,
      "step": 160700
    },
    {
      "epoch": 12.28324803299977,
      "grad_norm": 8.351210594177246,
      "learning_rate": 3.976395997250019e-05,
      "loss": 1.851,
      "step": 160800
    },
    {
      "epoch": 12.290886868841188,
      "grad_norm": 7.03640079498291,
      "learning_rate": 3.9757594275965676e-05,
      "loss": 1.7753,
      "step": 160900
    },
    {
      "epoch": 12.298525704682607,
      "grad_norm": 4.762546062469482,
      "learning_rate": 3.9751228579431166e-05,
      "loss": 1.819,
      "step": 161000
    },
    {
      "epoch": 12.306164540524025,
      "grad_norm": 6.663815021514893,
      "learning_rate": 3.974486288289665e-05,
      "loss": 1.9163,
      "step": 161100
    },
    {
      "epoch": 12.313803376365442,
      "grad_norm": 5.230507850646973,
      "learning_rate": 3.973849718636213e-05,
      "loss": 1.7637,
      "step": 161200
    },
    {
      "epoch": 12.32144221220686,
      "grad_norm": 4.3060102462768555,
      "learning_rate": 3.973213148982762e-05,
      "loss": 1.8756,
      "step": 161300
    },
    {
      "epoch": 12.329081048048277,
      "grad_norm": 6.094742774963379,
      "learning_rate": 3.97257657932931e-05,
      "loss": 1.8112,
      "step": 161400
    },
    {
      "epoch": 12.336719883889696,
      "grad_norm": 4.695478439331055,
      "learning_rate": 3.971940009675859e-05,
      "loss": 1.7727,
      "step": 161500
    },
    {
      "epoch": 12.344358719731114,
      "grad_norm": 7.324253559112549,
      "learning_rate": 3.9713034400224074e-05,
      "loss": 1.8049,
      "step": 161600
    },
    {
      "epoch": 12.351997555572531,
      "grad_norm": 3.4897048473358154,
      "learning_rate": 3.970666870368956e-05,
      "loss": 1.8534,
      "step": 161700
    },
    {
      "epoch": 12.359636391413948,
      "grad_norm": 6.1406354904174805,
      "learning_rate": 3.970030300715504e-05,
      "loss": 1.8032,
      "step": 161800
    },
    {
      "epoch": 12.367275227255366,
      "grad_norm": 4.379632472991943,
      "learning_rate": 3.969393731062053e-05,
      "loss": 1.8784,
      "step": 161900
    },
    {
      "epoch": 12.374914063096783,
      "grad_norm": 6.165738582611084,
      "learning_rate": 3.9687571614086015e-05,
      "loss": 1.83,
      "step": 162000
    },
    {
      "epoch": 12.382552898938203,
      "grad_norm": 4.913509368896484,
      "learning_rate": 3.96812059175515e-05,
      "loss": 1.8082,
      "step": 162100
    },
    {
      "epoch": 12.39019173477962,
      "grad_norm": 5.010658264160156,
      "learning_rate": 3.967484022101698e-05,
      "loss": 1.8597,
      "step": 162200
    },
    {
      "epoch": 12.397830570621037,
      "grad_norm": 6.6100053787231445,
      "learning_rate": 3.9668474524482466e-05,
      "loss": 1.8863,
      "step": 162300
    },
    {
      "epoch": 12.405469406462455,
      "grad_norm": 5.544520378112793,
      "learning_rate": 3.9662108827947956e-05,
      "loss": 1.8145,
      "step": 162400
    },
    {
      "epoch": 12.413108242303872,
      "grad_norm": 5.741589546203613,
      "learning_rate": 3.965574313141344e-05,
      "loss": 1.7974,
      "step": 162500
    },
    {
      "epoch": 12.42074707814529,
      "grad_norm": 4.959589958190918,
      "learning_rate": 3.964937743487892e-05,
      "loss": 1.8893,
      "step": 162600
    },
    {
      "epoch": 12.428385913986709,
      "grad_norm": 5.093733787536621,
      "learning_rate": 3.964301173834441e-05,
      "loss": 1.8394,
      "step": 162700
    },
    {
      "epoch": 12.436024749828126,
      "grad_norm": 6.2189812660217285,
      "learning_rate": 3.96366460418099e-05,
      "loss": 1.829,
      "step": 162800
    },
    {
      "epoch": 12.443663585669544,
      "grad_norm": 5.373911380767822,
      "learning_rate": 3.963028034527538e-05,
      "loss": 1.8018,
      "step": 162900
    },
    {
      "epoch": 12.451302421510961,
      "grad_norm": 4.991939067840576,
      "learning_rate": 3.9623914648740864e-05,
      "loss": 1.8524,
      "step": 163000
    },
    {
      "epoch": 12.458941257352379,
      "grad_norm": 6.630722999572754,
      "learning_rate": 3.9617548952206354e-05,
      "loss": 1.8787,
      "step": 163100
    },
    {
      "epoch": 12.466580093193798,
      "grad_norm": 6.456304550170898,
      "learning_rate": 3.961118325567184e-05,
      "loss": 1.8613,
      "step": 163200
    },
    {
      "epoch": 12.474218929035215,
      "grad_norm": 5.200156211853027,
      "learning_rate": 3.960481755913733e-05,
      "loss": 1.925,
      "step": 163300
    },
    {
      "epoch": 12.481857764876633,
      "grad_norm": 5.307696342468262,
      "learning_rate": 3.959845186260281e-05,
      "loss": 1.9304,
      "step": 163400
    },
    {
      "epoch": 12.48949660071805,
      "grad_norm": 4.766269207000732,
      "learning_rate": 3.9592086166068295e-05,
      "loss": 1.8694,
      "step": 163500
    },
    {
      "epoch": 12.497135436559468,
      "grad_norm": 5.057775020599365,
      "learning_rate": 3.958572046953378e-05,
      "loss": 1.8212,
      "step": 163600
    },
    {
      "epoch": 12.504774272400887,
      "grad_norm": 5.245304584503174,
      "learning_rate": 3.957935477299926e-05,
      "loss": 1.9223,
      "step": 163700
    },
    {
      "epoch": 12.512413108242304,
      "grad_norm": 4.888546943664551,
      "learning_rate": 3.957298907646475e-05,
      "loss": 1.8011,
      "step": 163800
    },
    {
      "epoch": 12.520051944083722,
      "grad_norm": 5.5530900955200195,
      "learning_rate": 3.9566623379930236e-05,
      "loss": 1.7968,
      "step": 163900
    },
    {
      "epoch": 12.52769077992514,
      "grad_norm": 6.620009422302246,
      "learning_rate": 3.956025768339572e-05,
      "loss": 1.8045,
      "step": 164000
    },
    {
      "epoch": 12.535329615766557,
      "grad_norm": 4.518826007843018,
      "learning_rate": 3.95538919868612e-05,
      "loss": 1.8414,
      "step": 164100
    },
    {
      "epoch": 12.542968451607974,
      "grad_norm": 4.861342906951904,
      "learning_rate": 3.9547526290326694e-05,
      "loss": 1.9824,
      "step": 164200
    },
    {
      "epoch": 12.550607287449393,
      "grad_norm": 5.843341827392578,
      "learning_rate": 3.954116059379218e-05,
      "loss": 1.846,
      "step": 164300
    },
    {
      "epoch": 12.558246123290811,
      "grad_norm": 5.181055545806885,
      "learning_rate": 3.953479489725766e-05,
      "loss": 1.7606,
      "step": 164400
    },
    {
      "epoch": 12.565884959132228,
      "grad_norm": 5.452077388763428,
      "learning_rate": 3.9528429200723144e-05,
      "loss": 1.7518,
      "step": 164500
    },
    {
      "epoch": 12.573523794973646,
      "grad_norm": 4.543633460998535,
      "learning_rate": 3.952206350418863e-05,
      "loss": 1.7883,
      "step": 164600
    },
    {
      "epoch": 12.581162630815063,
      "grad_norm": 5.109367847442627,
      "learning_rate": 3.951569780765412e-05,
      "loss": 1.7921,
      "step": 164700
    },
    {
      "epoch": 12.58880146665648,
      "grad_norm": 5.788227081298828,
      "learning_rate": 3.95093321111196e-05,
      "loss": 1.8598,
      "step": 164800
    },
    {
      "epoch": 12.5964403024979,
      "grad_norm": 4.588552951812744,
      "learning_rate": 3.9502966414585085e-05,
      "loss": 1.8663,
      "step": 164900
    },
    {
      "epoch": 12.604079138339317,
      "grad_norm": 7.505429267883301,
      "learning_rate": 3.949660071805057e-05,
      "loss": 1.803,
      "step": 165000
    },
    {
      "epoch": 12.611717974180735,
      "grad_norm": 4.905646800994873,
      "learning_rate": 3.949023502151606e-05,
      "loss": 1.9381,
      "step": 165100
    },
    {
      "epoch": 12.619356810022152,
      "grad_norm": 8.043947219848633,
      "learning_rate": 3.948386932498154e-05,
      "loss": 1.8313,
      "step": 165200
    },
    {
      "epoch": 12.62699564586357,
      "grad_norm": 5.417477130889893,
      "learning_rate": 3.9477503628447026e-05,
      "loss": 1.7542,
      "step": 165300
    },
    {
      "epoch": 12.634634481704989,
      "grad_norm": 5.851044654846191,
      "learning_rate": 3.947113793191251e-05,
      "loss": 1.744,
      "step": 165400
    },
    {
      "epoch": 12.642273317546406,
      "grad_norm": 8.200261116027832,
      "learning_rate": 3.946477223537799e-05,
      "loss": 1.8566,
      "step": 165500
    },
    {
      "epoch": 12.649912153387824,
      "grad_norm": 4.5037407875061035,
      "learning_rate": 3.9458406538843484e-05,
      "loss": 1.7579,
      "step": 165600
    },
    {
      "epoch": 12.657550989229241,
      "grad_norm": 4.681352138519287,
      "learning_rate": 3.945204084230897e-05,
      "loss": 1.8765,
      "step": 165700
    },
    {
      "epoch": 12.665189825070659,
      "grad_norm": 5.384182929992676,
      "learning_rate": 3.944567514577445e-05,
      "loss": 1.8365,
      "step": 165800
    },
    {
      "epoch": 12.672828660912078,
      "grad_norm": 5.625169277191162,
      "learning_rate": 3.9439309449239934e-05,
      "loss": 1.7993,
      "step": 165900
    },
    {
      "epoch": 12.680467496753495,
      "grad_norm": 4.858057498931885,
      "learning_rate": 3.943294375270542e-05,
      "loss": 1.8149,
      "step": 166000
    },
    {
      "epoch": 12.688106332594913,
      "grad_norm": 4.839702606201172,
      "learning_rate": 3.942657805617091e-05,
      "loss": 1.8976,
      "step": 166100
    },
    {
      "epoch": 12.69574516843633,
      "grad_norm": 6.837968349456787,
      "learning_rate": 3.942021235963639e-05,
      "loss": 1.8143,
      "step": 166200
    },
    {
      "epoch": 12.703384004277748,
      "grad_norm": 5.195735454559326,
      "learning_rate": 3.9413846663101875e-05,
      "loss": 1.9244,
      "step": 166300
    },
    {
      "epoch": 12.711022840119165,
      "grad_norm": 7.582294940948486,
      "learning_rate": 3.940748096656736e-05,
      "loss": 1.7518,
      "step": 166400
    },
    {
      "epoch": 12.718661675960584,
      "grad_norm": 6.663910865783691,
      "learning_rate": 3.940111527003285e-05,
      "loss": 1.7915,
      "step": 166500
    },
    {
      "epoch": 12.726300511802002,
      "grad_norm": 6.5524797439575195,
      "learning_rate": 3.939474957349833e-05,
      "loss": 1.7922,
      "step": 166600
    },
    {
      "epoch": 12.73393934764342,
      "grad_norm": 4.916337013244629,
      "learning_rate": 3.9388383876963816e-05,
      "loss": 1.891,
      "step": 166700
    },
    {
      "epoch": 12.741578183484837,
      "grad_norm": 9.977198600769043,
      "learning_rate": 3.9382018180429306e-05,
      "loss": 1.9445,
      "step": 166800
    },
    {
      "epoch": 12.749217019326254,
      "grad_norm": 5.125825881958008,
      "learning_rate": 3.937565248389479e-05,
      "loss": 1.9162,
      "step": 166900
    },
    {
      "epoch": 12.756855855167672,
      "grad_norm": 5.176908493041992,
      "learning_rate": 3.936928678736027e-05,
      "loss": 1.7908,
      "step": 167000
    },
    {
      "epoch": 12.76449469100909,
      "grad_norm": 5.712869644165039,
      "learning_rate": 3.9362921090825764e-05,
      "loss": 1.8473,
      "step": 167100
    },
    {
      "epoch": 12.772133526850508,
      "grad_norm": 4.488892078399658,
      "learning_rate": 3.935655539429125e-05,
      "loss": 1.828,
      "step": 167200
    },
    {
      "epoch": 12.779772362691926,
      "grad_norm": 7.156550407409668,
      "learning_rate": 3.935018969775673e-05,
      "loss": 1.8961,
      "step": 167300
    },
    {
      "epoch": 12.787411198533343,
      "grad_norm": 6.4242844581604,
      "learning_rate": 3.934382400122222e-05,
      "loss": 1.8443,
      "step": 167400
    },
    {
      "epoch": 12.79505003437476,
      "grad_norm": 5.203643798828125,
      "learning_rate": 3.9337458304687705e-05,
      "loss": 1.8096,
      "step": 167500
    },
    {
      "epoch": 12.80268887021618,
      "grad_norm": 5.302122592926025,
      "learning_rate": 3.933109260815319e-05,
      "loss": 1.7553,
      "step": 167600
    },
    {
      "epoch": 12.810327706057597,
      "grad_norm": 6.4332499504089355,
      "learning_rate": 3.932472691161867e-05,
      "loss": 1.7421,
      "step": 167700
    },
    {
      "epoch": 12.817966541899015,
      "grad_norm": 4.010054111480713,
      "learning_rate": 3.9318361215084155e-05,
      "loss": 1.8791,
      "step": 167800
    },
    {
      "epoch": 12.825605377740432,
      "grad_norm": 6.880874156951904,
      "learning_rate": 3.9311995518549646e-05,
      "loss": 1.7721,
      "step": 167900
    },
    {
      "epoch": 12.83324421358185,
      "grad_norm": 5.477826118469238,
      "learning_rate": 3.930562982201513e-05,
      "loss": 1.8105,
      "step": 168000
    },
    {
      "epoch": 12.840883049423267,
      "grad_norm": 7.416146755218506,
      "learning_rate": 3.929926412548061e-05,
      "loss": 1.827,
      "step": 168100
    },
    {
      "epoch": 12.848521885264686,
      "grad_norm": 6.035399913787842,
      "learning_rate": 3.9292898428946096e-05,
      "loss": 1.7753,
      "step": 168200
    },
    {
      "epoch": 12.856160721106104,
      "grad_norm": 4.521886348724365,
      "learning_rate": 3.9286532732411586e-05,
      "loss": 1.9199,
      "step": 168300
    },
    {
      "epoch": 12.863799556947521,
      "grad_norm": 5.655632972717285,
      "learning_rate": 3.928016703587707e-05,
      "loss": 1.78,
      "step": 168400
    },
    {
      "epoch": 12.871438392788939,
      "grad_norm": 7.335537910461426,
      "learning_rate": 3.9273801339342554e-05,
      "loss": 1.8052,
      "step": 168500
    },
    {
      "epoch": 12.879077228630356,
      "grad_norm": 5.508925914764404,
      "learning_rate": 3.926743564280804e-05,
      "loss": 1.9103,
      "step": 168600
    },
    {
      "epoch": 12.886716064471775,
      "grad_norm": 5.600693702697754,
      "learning_rate": 3.926106994627352e-05,
      "loss": 1.8443,
      "step": 168700
    },
    {
      "epoch": 12.894354900313193,
      "grad_norm": 5.512646198272705,
      "learning_rate": 3.925470424973901e-05,
      "loss": 1.841,
      "step": 168800
    },
    {
      "epoch": 12.90199373615461,
      "grad_norm": 4.811711311340332,
      "learning_rate": 3.9248338553204494e-05,
      "loss": 1.8208,
      "step": 168900
    },
    {
      "epoch": 12.909632571996028,
      "grad_norm": 5.073237419128418,
      "learning_rate": 3.924197285666998e-05,
      "loss": 1.931,
      "step": 169000
    },
    {
      "epoch": 12.917271407837445,
      "grad_norm": 6.742221355438232,
      "learning_rate": 3.923560716013546e-05,
      "loss": 1.8712,
      "step": 169100
    },
    {
      "epoch": 12.924910243678863,
      "grad_norm": 4.652886867523193,
      "learning_rate": 3.9229241463600945e-05,
      "loss": 1.7682,
      "step": 169200
    },
    {
      "epoch": 12.932549079520282,
      "grad_norm": 5.555724620819092,
      "learning_rate": 3.9222875767066435e-05,
      "loss": 1.8659,
      "step": 169300
    },
    {
      "epoch": 12.9401879153617,
      "grad_norm": 3.8980870246887207,
      "learning_rate": 3.921651007053192e-05,
      "loss": 1.7882,
      "step": 169400
    },
    {
      "epoch": 12.947826751203117,
      "grad_norm": 6.398043632507324,
      "learning_rate": 3.92101443739974e-05,
      "loss": 1.8342,
      "step": 169500
    },
    {
      "epoch": 12.955465587044534,
      "grad_norm": 5.837795734405518,
      "learning_rate": 3.9203778677462886e-05,
      "loss": 1.8676,
      "step": 169600
    },
    {
      "epoch": 12.963104422885952,
      "grad_norm": 5.450746059417725,
      "learning_rate": 3.9197412980928376e-05,
      "loss": 1.9542,
      "step": 169700
    },
    {
      "epoch": 12.97074325872737,
      "grad_norm": 4.844111442565918,
      "learning_rate": 3.919104728439386e-05,
      "loss": 1.7783,
      "step": 169800
    },
    {
      "epoch": 12.978382094568788,
      "grad_norm": 5.79696798324585,
      "learning_rate": 3.918468158785934e-05,
      "loss": 1.8943,
      "step": 169900
    },
    {
      "epoch": 12.986020930410206,
      "grad_norm": 3.2237069606781006,
      "learning_rate": 3.917831589132483e-05,
      "loss": 1.8763,
      "step": 170000
    },
    {
      "epoch": 12.993659766251623,
      "grad_norm": 6.092180252075195,
      "learning_rate": 3.917195019479031e-05,
      "loss": 1.8557,
      "step": 170100
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.8346210718154907,
      "eval_runtime": 1.4492,
      "eval_samples_per_second": 476.139,
      "eval_steps_per_second": 476.139,
      "step": 170183
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.6046264171600342,
      "eval_runtime": 27.7224,
      "eval_samples_per_second": 472.218,
      "eval_steps_per_second": 472.218,
      "step": 170183
    },
    {
      "epoch": 13.00129860209304,
      "grad_norm": 4.710206985473633,
      "learning_rate": 3.91655844982558e-05,
      "loss": 1.8719,
      "step": 170200
    },
    {
      "epoch": 13.008937437934458,
      "grad_norm": 5.5999650955200195,
      "learning_rate": 3.9159218801721284e-05,
      "loss": 1.8772,
      "step": 170300
    },
    {
      "epoch": 13.016576273775877,
      "grad_norm": 6.711345672607422,
      "learning_rate": 3.915285310518677e-05,
      "loss": 1.7298,
      "step": 170400
    },
    {
      "epoch": 13.024215109617295,
      "grad_norm": 4.070184230804443,
      "learning_rate": 3.914648740865226e-05,
      "loss": 1.7396,
      "step": 170500
    },
    {
      "epoch": 13.031853945458712,
      "grad_norm": 4.501736640930176,
      "learning_rate": 3.914012171211774e-05,
      "loss": 1.8953,
      "step": 170600
    },
    {
      "epoch": 13.03949278130013,
      "grad_norm": 5.509273052215576,
      "learning_rate": 3.9133756015583225e-05,
      "loss": 1.8211,
      "step": 170700
    },
    {
      "epoch": 13.047131617141547,
      "grad_norm": 8.860713005065918,
      "learning_rate": 3.9127390319048716e-05,
      "loss": 1.8208,
      "step": 170800
    },
    {
      "epoch": 13.054770452982966,
      "grad_norm": 5.008581638336182,
      "learning_rate": 3.91210246225142e-05,
      "loss": 1.8519,
      "step": 170900
    },
    {
      "epoch": 13.062409288824384,
      "grad_norm": 5.309434413909912,
      "learning_rate": 3.911465892597968e-05,
      "loss": 1.7746,
      "step": 171000
    },
    {
      "epoch": 13.070048124665801,
      "grad_norm": 5.902771472930908,
      "learning_rate": 3.910829322944517e-05,
      "loss": 1.8547,
      "step": 171100
    },
    {
      "epoch": 13.077686960507219,
      "grad_norm": 5.892754077911377,
      "learning_rate": 3.9101927532910656e-05,
      "loss": 1.8876,
      "step": 171200
    },
    {
      "epoch": 13.085325796348636,
      "grad_norm": 5.286036014556885,
      "learning_rate": 3.909556183637614e-05,
      "loss": 1.7303,
      "step": 171300
    },
    {
      "epoch": 13.092964632190053,
      "grad_norm": 6.227453231811523,
      "learning_rate": 3.9089196139841624e-05,
      "loss": 1.8601,
      "step": 171400
    },
    {
      "epoch": 13.100603468031473,
      "grad_norm": 5.977067470550537,
      "learning_rate": 3.9082830443307114e-05,
      "loss": 1.8012,
      "step": 171500
    },
    {
      "epoch": 13.10824230387289,
      "grad_norm": 5.409896373748779,
      "learning_rate": 3.90764647467726e-05,
      "loss": 1.869,
      "step": 171600
    },
    {
      "epoch": 13.115881139714308,
      "grad_norm": 7.711591720581055,
      "learning_rate": 3.907009905023808e-05,
      "loss": 1.819,
      "step": 171700
    },
    {
      "epoch": 13.123519975555725,
      "grad_norm": 4.699853897094727,
      "learning_rate": 3.9063733353703564e-05,
      "loss": 1.9059,
      "step": 171800
    },
    {
      "epoch": 13.131158811397142,
      "grad_norm": 4.474802494049072,
      "learning_rate": 3.905736765716905e-05,
      "loss": 1.7561,
      "step": 171900
    },
    {
      "epoch": 13.138797647238562,
      "grad_norm": 7.114630699157715,
      "learning_rate": 3.905100196063454e-05,
      "loss": 1.8819,
      "step": 172000
    },
    {
      "epoch": 13.14643648307998,
      "grad_norm": 6.156732559204102,
      "learning_rate": 3.904463626410002e-05,
      "loss": 1.7631,
      "step": 172100
    },
    {
      "epoch": 13.154075318921397,
      "grad_norm": 4.05747127532959,
      "learning_rate": 3.9038270567565505e-05,
      "loss": 1.8846,
      "step": 172200
    },
    {
      "epoch": 13.161714154762814,
      "grad_norm": 5.037477970123291,
      "learning_rate": 3.903190487103099e-05,
      "loss": 1.8027,
      "step": 172300
    },
    {
      "epoch": 13.169352990604231,
      "grad_norm": 6.606853485107422,
      "learning_rate": 3.902553917449647e-05,
      "loss": 1.9276,
      "step": 172400
    },
    {
      "epoch": 13.176991826445649,
      "grad_norm": 4.962330341339111,
      "learning_rate": 3.901917347796196e-05,
      "loss": 1.7929,
      "step": 172500
    },
    {
      "epoch": 13.184630662287068,
      "grad_norm": 4.831428050994873,
      "learning_rate": 3.9012807781427446e-05,
      "loss": 1.7778,
      "step": 172600
    },
    {
      "epoch": 13.192269498128486,
      "grad_norm": 6.088496685028076,
      "learning_rate": 3.900644208489293e-05,
      "loss": 1.7791,
      "step": 172700
    },
    {
      "epoch": 13.199908333969903,
      "grad_norm": 5.375566005706787,
      "learning_rate": 3.900007638835841e-05,
      "loss": 1.8351,
      "step": 172800
    },
    {
      "epoch": 13.20754716981132,
      "grad_norm": 5.804463863372803,
      "learning_rate": 3.8993710691823904e-05,
      "loss": 1.8108,
      "step": 172900
    },
    {
      "epoch": 13.215186005652738,
      "grad_norm": 4.150753498077393,
      "learning_rate": 3.898734499528939e-05,
      "loss": 1.7999,
      "step": 173000
    },
    {
      "epoch": 13.222824841494157,
      "grad_norm": 6.214064598083496,
      "learning_rate": 3.898097929875487e-05,
      "loss": 1.8381,
      "step": 173100
    },
    {
      "epoch": 13.230463677335575,
      "grad_norm": 3.949186086654663,
      "learning_rate": 3.8974613602220354e-05,
      "loss": 1.8481,
      "step": 173200
    },
    {
      "epoch": 13.238102513176992,
      "grad_norm": 5.253249168395996,
      "learning_rate": 3.896824790568584e-05,
      "loss": 1.8619,
      "step": 173300
    },
    {
      "epoch": 13.24574134901841,
      "grad_norm": 5.110404014587402,
      "learning_rate": 3.896188220915133e-05,
      "loss": 1.8945,
      "step": 173400
    },
    {
      "epoch": 13.253380184859827,
      "grad_norm": 5.89371395111084,
      "learning_rate": 3.895551651261681e-05,
      "loss": 1.8073,
      "step": 173500
    },
    {
      "epoch": 13.261019020701244,
      "grad_norm": 4.608572959899902,
      "learning_rate": 3.8949150816082295e-05,
      "loss": 1.8004,
      "step": 173600
    },
    {
      "epoch": 13.268657856542664,
      "grad_norm": 6.230780601501465,
      "learning_rate": 3.894278511954778e-05,
      "loss": 1.8985,
      "step": 173700
    },
    {
      "epoch": 13.276296692384081,
      "grad_norm": 5.944443702697754,
      "learning_rate": 3.893641942301327e-05,
      "loss": 1.8824,
      "step": 173800
    },
    {
      "epoch": 13.283935528225499,
      "grad_norm": 5.567529201507568,
      "learning_rate": 3.893005372647875e-05,
      "loss": 1.8698,
      "step": 173900
    },
    {
      "epoch": 13.291574364066916,
      "grad_norm": 4.314164161682129,
      "learning_rate": 3.8923688029944236e-05,
      "loss": 1.8142,
      "step": 174000
    },
    {
      "epoch": 13.299213199908333,
      "grad_norm": 4.211754322052002,
      "learning_rate": 3.891732233340972e-05,
      "loss": 1.7301,
      "step": 174100
    },
    {
      "epoch": 13.306852035749753,
      "grad_norm": 6.045474529266357,
      "learning_rate": 3.89109566368752e-05,
      "loss": 1.9284,
      "step": 174200
    },
    {
      "epoch": 13.31449087159117,
      "grad_norm": 3.9187841415405273,
      "learning_rate": 3.8904590940340694e-05,
      "loss": 1.8346,
      "step": 174300
    },
    {
      "epoch": 13.322129707432588,
      "grad_norm": 3.6297755241394043,
      "learning_rate": 3.889822524380618e-05,
      "loss": 1.7326,
      "step": 174400
    },
    {
      "epoch": 13.329768543274005,
      "grad_norm": 7.889037609100342,
      "learning_rate": 3.889185954727167e-05,
      "loss": 1.8967,
      "step": 174500
    },
    {
      "epoch": 13.337407379115422,
      "grad_norm": 4.859457969665527,
      "learning_rate": 3.888549385073715e-05,
      "loss": 1.8462,
      "step": 174600
    },
    {
      "epoch": 13.34504621495684,
      "grad_norm": 6.745675086975098,
      "learning_rate": 3.8879128154202634e-05,
      "loss": 1.7153,
      "step": 174700
    },
    {
      "epoch": 13.352685050798259,
      "grad_norm": 5.049821376800537,
      "learning_rate": 3.8872762457668125e-05,
      "loss": 1.8184,
      "step": 174800
    },
    {
      "epoch": 13.360323886639677,
      "grad_norm": 6.910056114196777,
      "learning_rate": 3.886639676113361e-05,
      "loss": 1.748,
      "step": 174900
    },
    {
      "epoch": 13.367962722481094,
      "grad_norm": 6.78286075592041,
      "learning_rate": 3.886003106459909e-05,
      "loss": 1.8274,
      "step": 175000
    },
    {
      "epoch": 13.375601558322511,
      "grad_norm": 4.89032506942749,
      "learning_rate": 3.8853665368064575e-05,
      "loss": 1.8118,
      "step": 175100
    },
    {
      "epoch": 13.383240394163929,
      "grad_norm": 5.168984889984131,
      "learning_rate": 3.8847299671530066e-05,
      "loss": 1.8406,
      "step": 175200
    },
    {
      "epoch": 13.390879230005346,
      "grad_norm": 4.4897847175598145,
      "learning_rate": 3.884093397499555e-05,
      "loss": 1.7785,
      "step": 175300
    },
    {
      "epoch": 13.398518065846766,
      "grad_norm": 5.122321605682373,
      "learning_rate": 3.883456827846103e-05,
      "loss": 1.8798,
      "step": 175400
    },
    {
      "epoch": 13.406156901688183,
      "grad_norm": 4.502192497253418,
      "learning_rate": 3.8828202581926516e-05,
      "loss": 1.8589,
      "step": 175500
    },
    {
      "epoch": 13.4137957375296,
      "grad_norm": 5.588714122772217,
      "learning_rate": 3.8821836885392e-05,
      "loss": 1.8273,
      "step": 175600
    },
    {
      "epoch": 13.421434573371018,
      "grad_norm": 4.726004123687744,
      "learning_rate": 3.881547118885749e-05,
      "loss": 1.8608,
      "step": 175700
    },
    {
      "epoch": 13.429073409212435,
      "grad_norm": 4.31279182434082,
      "learning_rate": 3.8809105492322974e-05,
      "loss": 1.7725,
      "step": 175800
    },
    {
      "epoch": 13.436712245053855,
      "grad_norm": 4.69873046875,
      "learning_rate": 3.880273979578846e-05,
      "loss": 1.8733,
      "step": 175900
    },
    {
      "epoch": 13.444351080895272,
      "grad_norm": 5.004831790924072,
      "learning_rate": 3.879637409925394e-05,
      "loss": 1.7875,
      "step": 176000
    },
    {
      "epoch": 13.45198991673669,
      "grad_norm": 5.47604513168335,
      "learning_rate": 3.879000840271943e-05,
      "loss": 1.8237,
      "step": 176100
    },
    {
      "epoch": 13.459628752578107,
      "grad_norm": 4.749556541442871,
      "learning_rate": 3.8783642706184915e-05,
      "loss": 1.8113,
      "step": 176200
    },
    {
      "epoch": 13.467267588419524,
      "grad_norm": 4.837276458740234,
      "learning_rate": 3.87772770096504e-05,
      "loss": 1.7316,
      "step": 176300
    },
    {
      "epoch": 13.474906424260944,
      "grad_norm": 7.043848514556885,
      "learning_rate": 3.877091131311588e-05,
      "loss": 1.7399,
      "step": 176400
    },
    {
      "epoch": 13.482545260102361,
      "grad_norm": 5.399111747741699,
      "learning_rate": 3.8764545616581365e-05,
      "loss": 1.7862,
      "step": 176500
    },
    {
      "epoch": 13.490184095943778,
      "grad_norm": 5.233197212219238,
      "learning_rate": 3.8758179920046856e-05,
      "loss": 1.8488,
      "step": 176600
    },
    {
      "epoch": 13.497822931785196,
      "grad_norm": 6.1001200675964355,
      "learning_rate": 3.875181422351234e-05,
      "loss": 1.8463,
      "step": 176700
    },
    {
      "epoch": 13.505461767626613,
      "grad_norm": 4.528559684753418,
      "learning_rate": 3.874544852697782e-05,
      "loss": 1.8163,
      "step": 176800
    },
    {
      "epoch": 13.51310060346803,
      "grad_norm": 4.924741744995117,
      "learning_rate": 3.8739082830443306e-05,
      "loss": 1.8756,
      "step": 176900
    },
    {
      "epoch": 13.52073943930945,
      "grad_norm": 5.743013381958008,
      "learning_rate": 3.8732717133908796e-05,
      "loss": 1.823,
      "step": 177000
    },
    {
      "epoch": 13.528378275150867,
      "grad_norm": 3.9376959800720215,
      "learning_rate": 3.872635143737428e-05,
      "loss": 1.733,
      "step": 177100
    },
    {
      "epoch": 13.536017110992285,
      "grad_norm": 4.902726650238037,
      "learning_rate": 3.8719985740839764e-05,
      "loss": 1.8084,
      "step": 177200
    },
    {
      "epoch": 13.543655946833702,
      "grad_norm": 5.2610883712768555,
      "learning_rate": 3.871362004430525e-05,
      "loss": 1.8696,
      "step": 177300
    },
    {
      "epoch": 13.55129478267512,
      "grad_norm": 4.738827705383301,
      "learning_rate": 3.870725434777073e-05,
      "loss": 1.8335,
      "step": 177400
    },
    {
      "epoch": 13.558933618516537,
      "grad_norm": 5.505366802215576,
      "learning_rate": 3.870088865123622e-05,
      "loss": 2.0076,
      "step": 177500
    },
    {
      "epoch": 13.566572454357956,
      "grad_norm": 5.9900078773498535,
      "learning_rate": 3.8694522954701704e-05,
      "loss": 1.8111,
      "step": 177600
    },
    {
      "epoch": 13.574211290199374,
      "grad_norm": 6.127885341644287,
      "learning_rate": 3.868815725816719e-05,
      "loss": 1.7449,
      "step": 177700
    },
    {
      "epoch": 13.581850126040791,
      "grad_norm": 6.379360198974609,
      "learning_rate": 3.868179156163267e-05,
      "loss": 1.8077,
      "step": 177800
    },
    {
      "epoch": 13.589488961882209,
      "grad_norm": 5.929272651672363,
      "learning_rate": 3.8675425865098155e-05,
      "loss": 1.8281,
      "step": 177900
    },
    {
      "epoch": 13.597127797723626,
      "grad_norm": 4.587331771850586,
      "learning_rate": 3.8669060168563645e-05,
      "loss": 1.9296,
      "step": 178000
    },
    {
      "epoch": 13.604766633565045,
      "grad_norm": 4.986667633056641,
      "learning_rate": 3.866269447202913e-05,
      "loss": 1.8236,
      "step": 178100
    },
    {
      "epoch": 13.612405469406463,
      "grad_norm": 6.045711040496826,
      "learning_rate": 3.865632877549461e-05,
      "loss": 1.7974,
      "step": 178200
    },
    {
      "epoch": 13.62004430524788,
      "grad_norm": 9.765433311462402,
      "learning_rate": 3.86499630789601e-05,
      "loss": 1.8248,
      "step": 178300
    },
    {
      "epoch": 13.627683141089298,
      "grad_norm": 5.639453411102295,
      "learning_rate": 3.8643597382425586e-05,
      "loss": 1.8552,
      "step": 178400
    },
    {
      "epoch": 13.635321976930715,
      "grad_norm": 4.109952926635742,
      "learning_rate": 3.8637231685891077e-05,
      "loss": 1.8393,
      "step": 178500
    },
    {
      "epoch": 13.642960812772134,
      "grad_norm": 4.501670837402344,
      "learning_rate": 3.863086598935656e-05,
      "loss": 1.79,
      "step": 178600
    },
    {
      "epoch": 13.650599648613552,
      "grad_norm": 4.360686779022217,
      "learning_rate": 3.8624500292822044e-05,
      "loss": 1.8092,
      "step": 178700
    },
    {
      "epoch": 13.65823848445497,
      "grad_norm": 4.342092990875244,
      "learning_rate": 3.861813459628753e-05,
      "loss": 1.8303,
      "step": 178800
    },
    {
      "epoch": 13.665877320296387,
      "grad_norm": 5.415624618530273,
      "learning_rate": 3.861176889975302e-05,
      "loss": 1.8555,
      "step": 178900
    },
    {
      "epoch": 13.673516156137804,
      "grad_norm": 6.0785346031188965,
      "learning_rate": 3.86054032032185e-05,
      "loss": 1.812,
      "step": 179000
    },
    {
      "epoch": 13.681154991979222,
      "grad_norm": 4.136346340179443,
      "learning_rate": 3.8599037506683985e-05,
      "loss": 1.7801,
      "step": 179100
    },
    {
      "epoch": 13.688793827820641,
      "grad_norm": 4.863166809082031,
      "learning_rate": 3.859267181014947e-05,
      "loss": 1.8222,
      "step": 179200
    },
    {
      "epoch": 13.696432663662058,
      "grad_norm": 6.38881778717041,
      "learning_rate": 3.858630611361496e-05,
      "loss": 1.7847,
      "step": 179300
    },
    {
      "epoch": 13.704071499503476,
      "grad_norm": 5.215724468231201,
      "learning_rate": 3.857994041708044e-05,
      "loss": 1.7792,
      "step": 179400
    },
    {
      "epoch": 13.711710335344893,
      "grad_norm": 5.543735504150391,
      "learning_rate": 3.8573574720545926e-05,
      "loss": 1.8883,
      "step": 179500
    },
    {
      "epoch": 13.71934917118631,
      "grad_norm": 6.448500156402588,
      "learning_rate": 3.856720902401141e-05,
      "loss": 1.803,
      "step": 179600
    },
    {
      "epoch": 13.726988007027728,
      "grad_norm": 4.821537017822266,
      "learning_rate": 3.856084332747689e-05,
      "loss": 1.7728,
      "step": 179700
    },
    {
      "epoch": 13.734626842869147,
      "grad_norm": 6.10214900970459,
      "learning_rate": 3.855447763094238e-05,
      "loss": 1.853,
      "step": 179800
    },
    {
      "epoch": 13.742265678710565,
      "grad_norm": 6.424217224121094,
      "learning_rate": 3.8548111934407866e-05,
      "loss": 1.7414,
      "step": 179900
    },
    {
      "epoch": 13.749904514551982,
      "grad_norm": 6.114694595336914,
      "learning_rate": 3.854174623787335e-05,
      "loss": 1.9561,
      "step": 180000
    },
    {
      "epoch": 13.7575433503934,
      "grad_norm": 4.783621788024902,
      "learning_rate": 3.8535380541338834e-05,
      "loss": 1.7813,
      "step": 180100
    },
    {
      "epoch": 13.765182186234817,
      "grad_norm": 5.52407693862915,
      "learning_rate": 3.8529014844804324e-05,
      "loss": 1.8589,
      "step": 180200
    },
    {
      "epoch": 13.772821022076236,
      "grad_norm": 7.3548407554626465,
      "learning_rate": 3.852264914826981e-05,
      "loss": 1.8267,
      "step": 180300
    },
    {
      "epoch": 13.780459857917654,
      "grad_norm": 6.94813871383667,
      "learning_rate": 3.851628345173529e-05,
      "loss": 1.8362,
      "step": 180400
    },
    {
      "epoch": 13.788098693759071,
      "grad_norm": 6.700806140899658,
      "learning_rate": 3.8509917755200774e-05,
      "loss": 1.792,
      "step": 180500
    },
    {
      "epoch": 13.795737529600489,
      "grad_norm": 3.817150831222534,
      "learning_rate": 3.850355205866626e-05,
      "loss": 1.795,
      "step": 180600
    },
    {
      "epoch": 13.803376365441906,
      "grad_norm": 4.6931586265563965,
      "learning_rate": 3.849718636213175e-05,
      "loss": 1.8411,
      "step": 180700
    },
    {
      "epoch": 13.811015201283324,
      "grad_norm": 5.775790691375732,
      "learning_rate": 3.849082066559723e-05,
      "loss": 1.7098,
      "step": 180800
    },
    {
      "epoch": 13.818654037124743,
      "grad_norm": 5.6896467208862305,
      "learning_rate": 3.8484454969062715e-05,
      "loss": 1.831,
      "step": 180900
    },
    {
      "epoch": 13.82629287296616,
      "grad_norm": 5.450008392333984,
      "learning_rate": 3.84780892725282e-05,
      "loss": 1.9091,
      "step": 181000
    },
    {
      "epoch": 13.833931708807578,
      "grad_norm": 7.530760288238525,
      "learning_rate": 3.847172357599368e-05,
      "loss": 1.8324,
      "step": 181100
    },
    {
      "epoch": 13.841570544648995,
      "grad_norm": 4.785487651824951,
      "learning_rate": 3.846535787945917e-05,
      "loss": 1.8797,
      "step": 181200
    },
    {
      "epoch": 13.849209380490413,
      "grad_norm": 4.853565216064453,
      "learning_rate": 3.8458992182924656e-05,
      "loss": 1.879,
      "step": 181300
    },
    {
      "epoch": 13.856848216331832,
      "grad_norm": 4.605937957763672,
      "learning_rate": 3.845262648639014e-05,
      "loss": 1.8521,
      "step": 181400
    },
    {
      "epoch": 13.86448705217325,
      "grad_norm": 5.345425128936768,
      "learning_rate": 3.844626078985562e-05,
      "loss": 1.872,
      "step": 181500
    },
    {
      "epoch": 13.872125888014667,
      "grad_norm": 5.282697677612305,
      "learning_rate": 3.8439895093321114e-05,
      "loss": 1.8849,
      "step": 181600
    },
    {
      "epoch": 13.879764723856084,
      "grad_norm": 3.2926459312438965,
      "learning_rate": 3.84335293967866e-05,
      "loss": 1.8555,
      "step": 181700
    },
    {
      "epoch": 13.887403559697502,
      "grad_norm": 5.843127727508545,
      "learning_rate": 3.842716370025208e-05,
      "loss": 1.827,
      "step": 181800
    },
    {
      "epoch": 13.895042395538919,
      "grad_norm": 5.289483070373535,
      "learning_rate": 3.8420798003717564e-05,
      "loss": 1.738,
      "step": 181900
    },
    {
      "epoch": 13.902681231380338,
      "grad_norm": 5.25545072555542,
      "learning_rate": 3.8414432307183055e-05,
      "loss": 1.8423,
      "step": 182000
    },
    {
      "epoch": 13.910320067221756,
      "grad_norm": 10.602232933044434,
      "learning_rate": 3.840806661064854e-05,
      "loss": 1.7595,
      "step": 182100
    },
    {
      "epoch": 13.917958903063173,
      "grad_norm": 5.641664028167725,
      "learning_rate": 3.840170091411402e-05,
      "loss": 1.8823,
      "step": 182200
    },
    {
      "epoch": 13.92559773890459,
      "grad_norm": 4.001131057739258,
      "learning_rate": 3.839533521757951e-05,
      "loss": 1.7974,
      "step": 182300
    },
    {
      "epoch": 13.933236574746008,
      "grad_norm": 5.148408889770508,
      "learning_rate": 3.8388969521044996e-05,
      "loss": 1.7854,
      "step": 182400
    },
    {
      "epoch": 13.940875410587427,
      "grad_norm": 4.885071277618408,
      "learning_rate": 3.838260382451048e-05,
      "loss": 1.8881,
      "step": 182500
    },
    {
      "epoch": 13.948514246428845,
      "grad_norm": 4.743511199951172,
      "learning_rate": 3.837623812797597e-05,
      "loss": 1.7586,
      "step": 182600
    },
    {
      "epoch": 13.956153082270262,
      "grad_norm": 5.580963134765625,
      "learning_rate": 3.836987243144145e-05,
      "loss": 1.8966,
      "step": 182700
    },
    {
      "epoch": 13.96379191811168,
      "grad_norm": 5.801124572753906,
      "learning_rate": 3.8363506734906936e-05,
      "loss": 1.8324,
      "step": 182800
    },
    {
      "epoch": 13.971430753953097,
      "grad_norm": 5.366735458374023,
      "learning_rate": 3.835714103837242e-05,
      "loss": 1.8587,
      "step": 182900
    },
    {
      "epoch": 13.979069589794515,
      "grad_norm": 5.632201671600342,
      "learning_rate": 3.835077534183791e-05,
      "loss": 1.7905,
      "step": 183000
    },
    {
      "epoch": 13.986708425635934,
      "grad_norm": 4.217787742614746,
      "learning_rate": 3.8344409645303394e-05,
      "loss": 1.8891,
      "step": 183100
    },
    {
      "epoch": 13.994347261477351,
      "grad_norm": 3.9812886714935303,
      "learning_rate": 3.833804394876888e-05,
      "loss": 1.779,
      "step": 183200
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8287259340286255,
      "eval_runtime": 1.4499,
      "eval_samples_per_second": 475.891,
      "eval_steps_per_second": 475.891,
      "step": 183274
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.5927059650421143,
      "eval_runtime": 27.6265,
      "eval_samples_per_second": 473.857,
      "eval_steps_per_second": 473.857,
      "step": 183274
    },
    {
      "epoch": 14.001986097318769,
      "grad_norm": 5.490603446960449,
      "learning_rate": 3.833167825223436e-05,
      "loss": 1.7895,
      "step": 183300
    },
    {
      "epoch": 14.009624933160186,
      "grad_norm": 6.505168914794922,
      "learning_rate": 3.8325312555699844e-05,
      "loss": 1.7756,
      "step": 183400
    },
    {
      "epoch": 14.017263769001604,
      "grad_norm": 6.371565818786621,
      "learning_rate": 3.8318946859165335e-05,
      "loss": 1.8107,
      "step": 183500
    },
    {
      "epoch": 14.024902604843023,
      "grad_norm": 5.5305094718933105,
      "learning_rate": 3.831258116263082e-05,
      "loss": 1.8007,
      "step": 183600
    },
    {
      "epoch": 14.03254144068444,
      "grad_norm": 5.38185453414917,
      "learning_rate": 3.83062154660963e-05,
      "loss": 1.8315,
      "step": 183700
    },
    {
      "epoch": 14.040180276525858,
      "grad_norm": 6.248636722564697,
      "learning_rate": 3.8299849769561785e-05,
      "loss": 1.8257,
      "step": 183800
    },
    {
      "epoch": 14.047819112367275,
      "grad_norm": 5.194202423095703,
      "learning_rate": 3.8293484073027276e-05,
      "loss": 1.8107,
      "step": 183900
    },
    {
      "epoch": 14.055457948208693,
      "grad_norm": 5.187681198120117,
      "learning_rate": 3.828711837649276e-05,
      "loss": 1.834,
      "step": 184000
    },
    {
      "epoch": 14.06309678405011,
      "grad_norm": 4.915846824645996,
      "learning_rate": 3.828075267995824e-05,
      "loss": 1.8442,
      "step": 184100
    },
    {
      "epoch": 14.07073561989153,
      "grad_norm": 4.641064167022705,
      "learning_rate": 3.8274386983423726e-05,
      "loss": 1.7722,
      "step": 184200
    },
    {
      "epoch": 14.078374455732947,
      "grad_norm": 6.200819492340088,
      "learning_rate": 3.826802128688921e-05,
      "loss": 1.8368,
      "step": 184300
    },
    {
      "epoch": 14.086013291574364,
      "grad_norm": 5.604488849639893,
      "learning_rate": 3.82616555903547e-05,
      "loss": 1.8727,
      "step": 184400
    },
    {
      "epoch": 14.093652127415782,
      "grad_norm": 5.6115617752075195,
      "learning_rate": 3.8255289893820184e-05,
      "loss": 1.7278,
      "step": 184500
    },
    {
      "epoch": 14.101290963257199,
      "grad_norm": 6.565742015838623,
      "learning_rate": 3.824892419728567e-05,
      "loss": 1.7818,
      "step": 184600
    },
    {
      "epoch": 14.108929799098618,
      "grad_norm": 4.73701810836792,
      "learning_rate": 3.824255850075115e-05,
      "loss": 1.8311,
      "step": 184700
    },
    {
      "epoch": 14.116568634940036,
      "grad_norm": 4.981948375701904,
      "learning_rate": 3.823619280421664e-05,
      "loss": 1.8223,
      "step": 184800
    },
    {
      "epoch": 14.124207470781453,
      "grad_norm": 6.508930206298828,
      "learning_rate": 3.8229827107682125e-05,
      "loss": 1.8208,
      "step": 184900
    },
    {
      "epoch": 14.13184630662287,
      "grad_norm": 4.822388172149658,
      "learning_rate": 3.822346141114761e-05,
      "loss": 1.8499,
      "step": 185000
    },
    {
      "epoch": 14.139485142464288,
      "grad_norm": 4.289942264556885,
      "learning_rate": 3.821709571461309e-05,
      "loss": 1.7654,
      "step": 185100
    },
    {
      "epoch": 14.147123978305705,
      "grad_norm": 4.669174671173096,
      "learning_rate": 3.8210730018078575e-05,
      "loss": 1.9273,
      "step": 185200
    },
    {
      "epoch": 14.154762814147125,
      "grad_norm": 4.900228023529053,
      "learning_rate": 3.8204364321544066e-05,
      "loss": 1.8675,
      "step": 185300
    },
    {
      "epoch": 14.162401649988542,
      "grad_norm": 5.668473720550537,
      "learning_rate": 3.819799862500955e-05,
      "loss": 1.8147,
      "step": 185400
    },
    {
      "epoch": 14.17004048582996,
      "grad_norm": 5.702951908111572,
      "learning_rate": 3.819163292847503e-05,
      "loss": 1.8422,
      "step": 185500
    },
    {
      "epoch": 14.177679321671377,
      "grad_norm": 5.098916530609131,
      "learning_rate": 3.8185267231940516e-05,
      "loss": 1.7803,
      "step": 185600
    },
    {
      "epoch": 14.185318157512794,
      "grad_norm": 7.644272804260254,
      "learning_rate": 3.8178901535406006e-05,
      "loss": 1.8324,
      "step": 185700
    },
    {
      "epoch": 14.192956993354214,
      "grad_norm": 4.382500648498535,
      "learning_rate": 3.817253583887149e-05,
      "loss": 1.7987,
      "step": 185800
    },
    {
      "epoch": 14.200595829195631,
      "grad_norm": 4.963811874389648,
      "learning_rate": 3.8166170142336974e-05,
      "loss": 1.7711,
      "step": 185900
    },
    {
      "epoch": 14.208234665037049,
      "grad_norm": 5.207120895385742,
      "learning_rate": 3.8159804445802464e-05,
      "loss": 1.87,
      "step": 186000
    },
    {
      "epoch": 14.215873500878466,
      "grad_norm": 4.673795700073242,
      "learning_rate": 3.815343874926795e-05,
      "loss": 1.8308,
      "step": 186100
    },
    {
      "epoch": 14.223512336719883,
      "grad_norm": 5.392840385437012,
      "learning_rate": 3.814707305273343e-05,
      "loss": 1.8006,
      "step": 186200
    },
    {
      "epoch": 14.231151172561301,
      "grad_norm": 5.5638747215271,
      "learning_rate": 3.814070735619892e-05,
      "loss": 1.7174,
      "step": 186300
    },
    {
      "epoch": 14.23879000840272,
      "grad_norm": 4.9491095542907715,
      "learning_rate": 3.8134341659664405e-05,
      "loss": 1.8673,
      "step": 186400
    },
    {
      "epoch": 14.246428844244138,
      "grad_norm": 5.081693649291992,
      "learning_rate": 3.812797596312989e-05,
      "loss": 1.7707,
      "step": 186500
    },
    {
      "epoch": 14.254067680085555,
      "grad_norm": 5.1293110847473145,
      "learning_rate": 3.812161026659537e-05,
      "loss": 1.8335,
      "step": 186600
    },
    {
      "epoch": 14.261706515926972,
      "grad_norm": 3.936084032058716,
      "learning_rate": 3.811524457006086e-05,
      "loss": 1.8514,
      "step": 186700
    },
    {
      "epoch": 14.26934535176839,
      "grad_norm": 4.887711524963379,
      "learning_rate": 3.8108878873526346e-05,
      "loss": 1.9133,
      "step": 186800
    },
    {
      "epoch": 14.27698418760981,
      "grad_norm": 4.717027187347412,
      "learning_rate": 3.810251317699183e-05,
      "loss": 1.8462,
      "step": 186900
    },
    {
      "epoch": 14.284623023451227,
      "grad_norm": 4.999364376068115,
      "learning_rate": 3.809614748045731e-05,
      "loss": 1.8391,
      "step": 187000
    },
    {
      "epoch": 14.292261859292644,
      "grad_norm": 7.579647064208984,
      "learning_rate": 3.80897817839228e-05,
      "loss": 1.8122,
      "step": 187100
    },
    {
      "epoch": 14.299900695134061,
      "grad_norm": 4.562612056732178,
      "learning_rate": 3.8083416087388287e-05,
      "loss": 1.8077,
      "step": 187200
    },
    {
      "epoch": 14.307539530975479,
      "grad_norm": 7.683398246765137,
      "learning_rate": 3.807705039085377e-05,
      "loss": 1.7964,
      "step": 187300
    },
    {
      "epoch": 14.315178366816896,
      "grad_norm": 5.16595983505249,
      "learning_rate": 3.8070684694319254e-05,
      "loss": 1.8201,
      "step": 187400
    },
    {
      "epoch": 14.322817202658316,
      "grad_norm": 5.9960150718688965,
      "learning_rate": 3.806431899778474e-05,
      "loss": 1.8344,
      "step": 187500
    },
    {
      "epoch": 14.330456038499733,
      "grad_norm": 5.857773303985596,
      "learning_rate": 3.805795330125023e-05,
      "loss": 1.8634,
      "step": 187600
    },
    {
      "epoch": 14.33809487434115,
      "grad_norm": 5.251617908477783,
      "learning_rate": 3.805158760471571e-05,
      "loss": 1.7539,
      "step": 187700
    },
    {
      "epoch": 14.345733710182568,
      "grad_norm": 4.093522071838379,
      "learning_rate": 3.8045221908181195e-05,
      "loss": 1.8045,
      "step": 187800
    },
    {
      "epoch": 14.353372546023985,
      "grad_norm": 5.308021545410156,
      "learning_rate": 3.803885621164668e-05,
      "loss": 1.8271,
      "step": 187900
    },
    {
      "epoch": 14.361011381865403,
      "grad_norm": 6.34128475189209,
      "learning_rate": 3.803249051511217e-05,
      "loss": 1.7766,
      "step": 188000
    },
    {
      "epoch": 14.368650217706822,
      "grad_norm": 5.029150485992432,
      "learning_rate": 3.802612481857765e-05,
      "loss": 1.8043,
      "step": 188100
    },
    {
      "epoch": 14.37628905354824,
      "grad_norm": 4.191378116607666,
      "learning_rate": 3.8019759122043136e-05,
      "loss": 1.8281,
      "step": 188200
    },
    {
      "epoch": 14.383927889389657,
      "grad_norm": 5.190170764923096,
      "learning_rate": 3.801339342550862e-05,
      "loss": 1.8026,
      "step": 188300
    },
    {
      "epoch": 14.391566725231074,
      "grad_norm": 5.154567718505859,
      "learning_rate": 3.80070277289741e-05,
      "loss": 1.8719,
      "step": 188400
    },
    {
      "epoch": 14.399205561072492,
      "grad_norm": 4.43347692489624,
      "learning_rate": 3.800066203243959e-05,
      "loss": 1.8883,
      "step": 188500
    },
    {
      "epoch": 14.406844396913911,
      "grad_norm": 7.719357013702393,
      "learning_rate": 3.7994296335905076e-05,
      "loss": 1.8398,
      "step": 188600
    },
    {
      "epoch": 14.414483232755329,
      "grad_norm": 7.901889324188232,
      "learning_rate": 3.798793063937056e-05,
      "loss": 1.8449,
      "step": 188700
    },
    {
      "epoch": 14.422122068596746,
      "grad_norm": 4.781552314758301,
      "learning_rate": 3.7981564942836043e-05,
      "loss": 1.7059,
      "step": 188800
    },
    {
      "epoch": 14.429760904438163,
      "grad_norm": 6.478456974029541,
      "learning_rate": 3.797519924630153e-05,
      "loss": 1.7369,
      "step": 188900
    },
    {
      "epoch": 14.43739974027958,
      "grad_norm": 4.713522911071777,
      "learning_rate": 3.796883354976702e-05,
      "loss": 1.7899,
      "step": 189000
    },
    {
      "epoch": 14.445038576121,
      "grad_norm": 5.548066139221191,
      "learning_rate": 3.79624678532325e-05,
      "loss": 1.8004,
      "step": 189100
    },
    {
      "epoch": 14.452677411962418,
      "grad_norm": 7.757017612457275,
      "learning_rate": 3.7956102156697984e-05,
      "loss": 1.9098,
      "step": 189200
    },
    {
      "epoch": 14.460316247803835,
      "grad_norm": 4.637407302856445,
      "learning_rate": 3.794973646016347e-05,
      "loss": 1.7673,
      "step": 189300
    },
    {
      "epoch": 14.467955083645252,
      "grad_norm": 4.190675258636475,
      "learning_rate": 3.794337076362896e-05,
      "loss": 1.8932,
      "step": 189400
    },
    {
      "epoch": 14.47559391948667,
      "grad_norm": 4.250411033630371,
      "learning_rate": 3.793700506709444e-05,
      "loss": 1.8756,
      "step": 189500
    },
    {
      "epoch": 14.483232755328087,
      "grad_norm": 5.060983657836914,
      "learning_rate": 3.7930639370559925e-05,
      "loss": 1.6892,
      "step": 189600
    },
    {
      "epoch": 14.490871591169507,
      "grad_norm": 5.885972023010254,
      "learning_rate": 3.7924273674025416e-05,
      "loss": 1.9231,
      "step": 189700
    },
    {
      "epoch": 14.498510427010924,
      "grad_norm": 6.152629375457764,
      "learning_rate": 3.79179079774909e-05,
      "loss": 1.8201,
      "step": 189800
    },
    {
      "epoch": 14.506149262852341,
      "grad_norm": 6.008790016174316,
      "learning_rate": 3.791154228095638e-05,
      "loss": 1.7366,
      "step": 189900
    },
    {
      "epoch": 14.513788098693759,
      "grad_norm": 5.317386150360107,
      "learning_rate": 3.790517658442187e-05,
      "loss": 1.7853,
      "step": 190000
    },
    {
      "epoch": 14.521426934535176,
      "grad_norm": 5.231252670288086,
      "learning_rate": 3.7898810887887357e-05,
      "loss": 1.8029,
      "step": 190100
    },
    {
      "epoch": 14.529065770376594,
      "grad_norm": 6.336948394775391,
      "learning_rate": 3.789244519135284e-05,
      "loss": 1.8214,
      "step": 190200
    },
    {
      "epoch": 14.536704606218013,
      "grad_norm": 5.783840179443359,
      "learning_rate": 3.788607949481833e-05,
      "loss": 1.7506,
      "step": 190300
    },
    {
      "epoch": 14.54434344205943,
      "grad_norm": 6.304731845855713,
      "learning_rate": 3.7879713798283814e-05,
      "loss": 1.8191,
      "step": 190400
    },
    {
      "epoch": 14.551982277900848,
      "grad_norm": 5.418978691101074,
      "learning_rate": 3.78733481017493e-05,
      "loss": 1.8035,
      "step": 190500
    },
    {
      "epoch": 14.559621113742265,
      "grad_norm": 4.740294933319092,
      "learning_rate": 3.786698240521478e-05,
      "loss": 1.8217,
      "step": 190600
    },
    {
      "epoch": 14.567259949583683,
      "grad_norm": 4.893802642822266,
      "learning_rate": 3.7860616708680265e-05,
      "loss": 1.8014,
      "step": 190700
    },
    {
      "epoch": 14.574898785425102,
      "grad_norm": 5.015142917633057,
      "learning_rate": 3.7854251012145755e-05,
      "loss": 1.7347,
      "step": 190800
    },
    {
      "epoch": 14.58253762126652,
      "grad_norm": 4.5399603843688965,
      "learning_rate": 3.784788531561124e-05,
      "loss": 1.827,
      "step": 190900
    },
    {
      "epoch": 14.590176457107937,
      "grad_norm": 5.1302995681762695,
      "learning_rate": 3.784151961907672e-05,
      "loss": 1.7877,
      "step": 191000
    },
    {
      "epoch": 14.597815292949354,
      "grad_norm": 5.797058582305908,
      "learning_rate": 3.7835153922542205e-05,
      "loss": 1.7854,
      "step": 191100
    },
    {
      "epoch": 14.605454128790772,
      "grad_norm": 3.830613136291504,
      "learning_rate": 3.7828788226007696e-05,
      "loss": 1.7892,
      "step": 191200
    },
    {
      "epoch": 14.613092964632191,
      "grad_norm": 5.229764938354492,
      "learning_rate": 3.782242252947318e-05,
      "loss": 1.7876,
      "step": 191300
    },
    {
      "epoch": 14.620731800473608,
      "grad_norm": 5.584578514099121,
      "learning_rate": 3.781605683293866e-05,
      "loss": 1.7226,
      "step": 191400
    },
    {
      "epoch": 14.628370636315026,
      "grad_norm": 8.651413917541504,
      "learning_rate": 3.7809691136404146e-05,
      "loss": 1.8838,
      "step": 191500
    },
    {
      "epoch": 14.636009472156443,
      "grad_norm": 4.929304122924805,
      "learning_rate": 3.780332543986963e-05,
      "loss": 1.7218,
      "step": 191600
    },
    {
      "epoch": 14.64364830799786,
      "grad_norm": 5.8300700187683105,
      "learning_rate": 3.779695974333512e-05,
      "loss": 1.8282,
      "step": 191700
    },
    {
      "epoch": 14.651287143839278,
      "grad_norm": 4.6842360496521,
      "learning_rate": 3.7790594046800604e-05,
      "loss": 1.8777,
      "step": 191800
    },
    {
      "epoch": 14.658925979680697,
      "grad_norm": 5.350640296936035,
      "learning_rate": 3.778422835026609e-05,
      "loss": 1.8726,
      "step": 191900
    },
    {
      "epoch": 14.666564815522115,
      "grad_norm": 4.355090618133545,
      "learning_rate": 3.777786265373157e-05,
      "loss": 1.7497,
      "step": 192000
    },
    {
      "epoch": 14.674203651363532,
      "grad_norm": 3.750788688659668,
      "learning_rate": 3.7771496957197054e-05,
      "loss": 1.871,
      "step": 192100
    },
    {
      "epoch": 14.68184248720495,
      "grad_norm": 4.496485710144043,
      "learning_rate": 3.7765131260662545e-05,
      "loss": 1.8205,
      "step": 192200
    },
    {
      "epoch": 14.689481323046367,
      "grad_norm": 5.622904300689697,
      "learning_rate": 3.775876556412803e-05,
      "loss": 1.7606,
      "step": 192300
    },
    {
      "epoch": 14.697120158887785,
      "grad_norm": 5.947119235992432,
      "learning_rate": 3.775239986759351e-05,
      "loss": 1.9271,
      "step": 192400
    },
    {
      "epoch": 14.704758994729204,
      "grad_norm": 6.1795973777771,
      "learning_rate": 3.7746034171058995e-05,
      "loss": 1.8707,
      "step": 192500
    },
    {
      "epoch": 14.712397830570621,
      "grad_norm": 4.434571743011475,
      "learning_rate": 3.7739668474524486e-05,
      "loss": 1.8248,
      "step": 192600
    },
    {
      "epoch": 14.720036666412039,
      "grad_norm": 4.753141403198242,
      "learning_rate": 3.773330277798997e-05,
      "loss": 1.8471,
      "step": 192700
    },
    {
      "epoch": 14.727675502253456,
      "grad_norm": 5.927921772003174,
      "learning_rate": 3.772693708145545e-05,
      "loss": 1.7638,
      "step": 192800
    },
    {
      "epoch": 14.735314338094874,
      "grad_norm": 5.577388763427734,
      "learning_rate": 3.7720571384920936e-05,
      "loss": 1.8095,
      "step": 192900
    },
    {
      "epoch": 14.742953173936293,
      "grad_norm": 4.66100549697876,
      "learning_rate": 3.771420568838642e-05,
      "loss": 1.6878,
      "step": 193000
    },
    {
      "epoch": 14.75059200977771,
      "grad_norm": 11.991456985473633,
      "learning_rate": 3.770783999185191e-05,
      "loss": 1.8457,
      "step": 193100
    },
    {
      "epoch": 14.758230845619128,
      "grad_norm": 5.0383620262146,
      "learning_rate": 3.7701474295317394e-05,
      "loss": 1.8142,
      "step": 193200
    },
    {
      "epoch": 14.765869681460545,
      "grad_norm": 4.097818851470947,
      "learning_rate": 3.769510859878288e-05,
      "loss": 1.7829,
      "step": 193300
    },
    {
      "epoch": 14.773508517301963,
      "grad_norm": 4.973057746887207,
      "learning_rate": 3.768874290224836e-05,
      "loss": 1.9168,
      "step": 193400
    },
    {
      "epoch": 14.78114735314338,
      "grad_norm": 4.260682106018066,
      "learning_rate": 3.768237720571385e-05,
      "loss": 1.8579,
      "step": 193500
    },
    {
      "epoch": 14.7887861889848,
      "grad_norm": 4.144012451171875,
      "learning_rate": 3.7676011509179335e-05,
      "loss": 1.8273,
      "step": 193600
    },
    {
      "epoch": 14.796425024826217,
      "grad_norm": 6.231870174407959,
      "learning_rate": 3.7669645812644825e-05,
      "loss": 1.8688,
      "step": 193700
    },
    {
      "epoch": 14.804063860667634,
      "grad_norm": 4.208641529083252,
      "learning_rate": 3.766328011611031e-05,
      "loss": 1.8076,
      "step": 193800
    },
    {
      "epoch": 14.811702696509052,
      "grad_norm": 6.333381175994873,
      "learning_rate": 3.765691441957579e-05,
      "loss": 1.793,
      "step": 193900
    },
    {
      "epoch": 14.81934153235047,
      "grad_norm": 5.651453018188477,
      "learning_rate": 3.765054872304128e-05,
      "loss": 1.8707,
      "step": 194000
    },
    {
      "epoch": 14.826980368191888,
      "grad_norm": 5.172697067260742,
      "learning_rate": 3.7644183026506766e-05,
      "loss": 1.871,
      "step": 194100
    },
    {
      "epoch": 14.834619204033306,
      "grad_norm": 5.249651908874512,
      "learning_rate": 3.763781732997225e-05,
      "loss": 1.8459,
      "step": 194200
    },
    {
      "epoch": 14.842258039874723,
      "grad_norm": 5.136164665222168,
      "learning_rate": 3.763145163343773e-05,
      "loss": 1.8357,
      "step": 194300
    },
    {
      "epoch": 14.84989687571614,
      "grad_norm": 5.944066524505615,
      "learning_rate": 3.762508593690322e-05,
      "loss": 1.8163,
      "step": 194400
    },
    {
      "epoch": 14.857535711557558,
      "grad_norm": 5.657197952270508,
      "learning_rate": 3.761872024036871e-05,
      "loss": 1.6931,
      "step": 194500
    },
    {
      "epoch": 14.865174547398976,
      "grad_norm": 5.769853115081787,
      "learning_rate": 3.761235454383419e-05,
      "loss": 1.8376,
      "step": 194600
    },
    {
      "epoch": 14.872813383240395,
      "grad_norm": 4.033280372619629,
      "learning_rate": 3.7605988847299674e-05,
      "loss": 1.804,
      "step": 194700
    },
    {
      "epoch": 14.880452219081812,
      "grad_norm": 4.384440898895264,
      "learning_rate": 3.759962315076516e-05,
      "loss": 1.772,
      "step": 194800
    },
    {
      "epoch": 14.88809105492323,
      "grad_norm": 4.74094820022583,
      "learning_rate": 3.759325745423065e-05,
      "loss": 1.869,
      "step": 194900
    },
    {
      "epoch": 14.895729890764647,
      "grad_norm": 6.552380561828613,
      "learning_rate": 3.758689175769613e-05,
      "loss": 1.911,
      "step": 195000
    },
    {
      "epoch": 14.903368726606065,
      "grad_norm": 5.6379828453063965,
      "learning_rate": 3.7580526061161615e-05,
      "loss": 1.8533,
      "step": 195100
    },
    {
      "epoch": 14.911007562447484,
      "grad_norm": 3.6140952110290527,
      "learning_rate": 3.75741603646271e-05,
      "loss": 1.7954,
      "step": 195200
    },
    {
      "epoch": 14.918646398288901,
      "grad_norm": 4.672685623168945,
      "learning_rate": 3.756779466809258e-05,
      "loss": 1.8723,
      "step": 195300
    },
    {
      "epoch": 14.926285234130319,
      "grad_norm": 4.311548709869385,
      "learning_rate": 3.756142897155807e-05,
      "loss": 1.8373,
      "step": 195400
    },
    {
      "epoch": 14.933924069971736,
      "grad_norm": 6.652279376983643,
      "learning_rate": 3.7555063275023556e-05,
      "loss": 1.7928,
      "step": 195500
    },
    {
      "epoch": 14.941562905813154,
      "grad_norm": 10.010393142700195,
      "learning_rate": 3.754869757848904e-05,
      "loss": 1.8177,
      "step": 195600
    },
    {
      "epoch": 14.949201741654571,
      "grad_norm": 5.627156734466553,
      "learning_rate": 3.754233188195452e-05,
      "loss": 1.8105,
      "step": 195700
    },
    {
      "epoch": 14.95684057749599,
      "grad_norm": 5.494160175323486,
      "learning_rate": 3.753596618542001e-05,
      "loss": 1.8425,
      "step": 195800
    },
    {
      "epoch": 14.964479413337408,
      "grad_norm": 3.520433187484741,
      "learning_rate": 3.7529600488885497e-05,
      "loss": 1.8315,
      "step": 195900
    },
    {
      "epoch": 14.972118249178825,
      "grad_norm": 6.9137187004089355,
      "learning_rate": 3.752323479235098e-05,
      "loss": 1.767,
      "step": 196000
    },
    {
      "epoch": 14.979757085020243,
      "grad_norm": 4.630965709686279,
      "learning_rate": 3.7516869095816464e-05,
      "loss": 1.796,
      "step": 196100
    },
    {
      "epoch": 14.98739592086166,
      "grad_norm": 4.575767993927002,
      "learning_rate": 3.751050339928195e-05,
      "loss": 1.771,
      "step": 196200
    },
    {
      "epoch": 14.99503475670308,
      "grad_norm": 4.693465232849121,
      "learning_rate": 3.750413770274744e-05,
      "loss": 1.8283,
      "step": 196300
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.8209341764450073,
      "eval_runtime": 1.4716,
      "eval_samples_per_second": 468.87,
      "eval_steps_per_second": 468.87,
      "step": 196365
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.58086097240448,
      "eval_runtime": 27.7042,
      "eval_samples_per_second": 472.527,
      "eval_steps_per_second": 472.527,
      "step": 196365
    },
    {
      "epoch": 15.002673592544497,
      "grad_norm": 5.255756855010986,
      "learning_rate": 3.749777200621292e-05,
      "loss": 1.7588,
      "step": 196400
    },
    {
      "epoch": 15.010312428385914,
      "grad_norm": 4.324917793273926,
      "learning_rate": 3.7491406309678405e-05,
      "loss": 1.788,
      "step": 196500
    },
    {
      "epoch": 15.017951264227332,
      "grad_norm": 4.555196285247803,
      "learning_rate": 3.748504061314389e-05,
      "loss": 1.8341,
      "step": 196600
    },
    {
      "epoch": 15.025590100068749,
      "grad_norm": 5.68308687210083,
      "learning_rate": 3.747867491660938e-05,
      "loss": 1.7426,
      "step": 196700
    },
    {
      "epoch": 15.033228935910167,
      "grad_norm": 4.953063488006592,
      "learning_rate": 3.747230922007486e-05,
      "loss": 1.7751,
      "step": 196800
    },
    {
      "epoch": 15.040867771751586,
      "grad_norm": 7.961254119873047,
      "learning_rate": 3.7465943523540345e-05,
      "loss": 1.7959,
      "step": 196900
    },
    {
      "epoch": 15.048506607593003,
      "grad_norm": 4.704087734222412,
      "learning_rate": 3.745957782700583e-05,
      "loss": 1.8591,
      "step": 197000
    },
    {
      "epoch": 15.05614544343442,
      "grad_norm": 4.461294651031494,
      "learning_rate": 3.745321213047131e-05,
      "loss": 1.8395,
      "step": 197100
    },
    {
      "epoch": 15.063784279275838,
      "grad_norm": 5.225213527679443,
      "learning_rate": 3.74468464339368e-05,
      "loss": 1.7581,
      "step": 197200
    },
    {
      "epoch": 15.071423115117256,
      "grad_norm": 5.13359260559082,
      "learning_rate": 3.7440480737402286e-05,
      "loss": 1.7634,
      "step": 197300
    },
    {
      "epoch": 15.079061950958675,
      "grad_norm": 4.67808198928833,
      "learning_rate": 3.743411504086777e-05,
      "loss": 1.8679,
      "step": 197400
    },
    {
      "epoch": 15.086700786800092,
      "grad_norm": 5.170900821685791,
      "learning_rate": 3.742774934433326e-05,
      "loss": 1.7789,
      "step": 197500
    },
    {
      "epoch": 15.09433962264151,
      "grad_norm": 5.243434429168701,
      "learning_rate": 3.7421383647798744e-05,
      "loss": 1.8146,
      "step": 197600
    },
    {
      "epoch": 15.101978458482927,
      "grad_norm": 5.680035591125488,
      "learning_rate": 3.741501795126423e-05,
      "loss": 1.7415,
      "step": 197700
    },
    {
      "epoch": 15.109617294324345,
      "grad_norm": 4.442081928253174,
      "learning_rate": 3.740865225472972e-05,
      "loss": 1.804,
      "step": 197800
    },
    {
      "epoch": 15.117256130165762,
      "grad_norm": 4.946157455444336,
      "learning_rate": 3.74022865581952e-05,
      "loss": 1.784,
      "step": 197900
    },
    {
      "epoch": 15.124894966007181,
      "grad_norm": 4.242211818695068,
      "learning_rate": 3.7395920861660685e-05,
      "loss": 1.8036,
      "step": 198000
    },
    {
      "epoch": 15.132533801848599,
      "grad_norm": 6.3387041091918945,
      "learning_rate": 3.7389555165126175e-05,
      "loss": 1.7476,
      "step": 198100
    },
    {
      "epoch": 15.140172637690016,
      "grad_norm": 5.226761341094971,
      "learning_rate": 3.738318946859166e-05,
      "loss": 1.8688,
      "step": 198200
    },
    {
      "epoch": 15.147811473531434,
      "grad_norm": 4.97035551071167,
      "learning_rate": 3.737682377205714e-05,
      "loss": 1.8363,
      "step": 198300
    },
    {
      "epoch": 15.155450309372851,
      "grad_norm": 5.087646007537842,
      "learning_rate": 3.7370458075522626e-05,
      "loss": 1.7639,
      "step": 198400
    },
    {
      "epoch": 15.16308914521427,
      "grad_norm": 6.271244525909424,
      "learning_rate": 3.736409237898811e-05,
      "loss": 1.783,
      "step": 198500
    },
    {
      "epoch": 15.170727981055688,
      "grad_norm": 4.104372978210449,
      "learning_rate": 3.73577266824536e-05,
      "loss": 1.7772,
      "step": 198600
    },
    {
      "epoch": 15.178366816897105,
      "grad_norm": 3.8299832344055176,
      "learning_rate": 3.735136098591908e-05,
      "loss": 1.8156,
      "step": 198700
    },
    {
      "epoch": 15.186005652738523,
      "grad_norm": 6.4630913734436035,
      "learning_rate": 3.7344995289384567e-05,
      "loss": 1.8587,
      "step": 198800
    },
    {
      "epoch": 15.19364448857994,
      "grad_norm": 4.134709358215332,
      "learning_rate": 3.733862959285005e-05,
      "loss": 1.85,
      "step": 198900
    },
    {
      "epoch": 15.201283324421357,
      "grad_norm": 6.5867695808410645,
      "learning_rate": 3.733226389631554e-05,
      "loss": 1.8918,
      "step": 199000
    },
    {
      "epoch": 15.208922160262777,
      "grad_norm": 5.325658321380615,
      "learning_rate": 3.7325898199781024e-05,
      "loss": 1.9076,
      "step": 199100
    },
    {
      "epoch": 15.216560996104194,
      "grad_norm": 6.219529151916504,
      "learning_rate": 3.731953250324651e-05,
      "loss": 1.813,
      "step": 199200
    },
    {
      "epoch": 15.224199831945612,
      "grad_norm": 4.629575729370117,
      "learning_rate": 3.731316680671199e-05,
      "loss": 1.7855,
      "step": 199300
    },
    {
      "epoch": 15.231838667787029,
      "grad_norm": 4.169185638427734,
      "learning_rate": 3.7306801110177475e-05,
      "loss": 1.7472,
      "step": 199400
    },
    {
      "epoch": 15.239477503628446,
      "grad_norm": 5.210055351257324,
      "learning_rate": 3.7300435413642965e-05,
      "loss": 1.7742,
      "step": 199500
    },
    {
      "epoch": 15.247116339469866,
      "grad_norm": 6.873137474060059,
      "learning_rate": 3.729406971710845e-05,
      "loss": 1.7721,
      "step": 199600
    },
    {
      "epoch": 15.254755175311283,
      "grad_norm": 6.076546669006348,
      "learning_rate": 3.728770402057393e-05,
      "loss": 1.7874,
      "step": 199700
    },
    {
      "epoch": 15.2623940111527,
      "grad_norm": 5.285336971282959,
      "learning_rate": 3.7281338324039415e-05,
      "loss": 1.8143,
      "step": 199800
    },
    {
      "epoch": 15.270032846994118,
      "grad_norm": 4.880945205688477,
      "learning_rate": 3.7274972627504906e-05,
      "loss": 1.7259,
      "step": 199900
    },
    {
      "epoch": 15.277671682835535,
      "grad_norm": 4.156857490539551,
      "learning_rate": 3.726860693097039e-05,
      "loss": 1.7669,
      "step": 200000
    },
    {
      "epoch": 15.285310518676953,
      "grad_norm": 5.9545979499816895,
      "learning_rate": 3.726224123443587e-05,
      "loss": 1.786,
      "step": 200100
    },
    {
      "epoch": 15.292949354518372,
      "grad_norm": 7.08278226852417,
      "learning_rate": 3.7255875537901356e-05,
      "loss": 1.6828,
      "step": 200200
    },
    {
      "epoch": 15.30058819035979,
      "grad_norm": 5.437515735626221,
      "learning_rate": 3.724950984136684e-05,
      "loss": 1.7092,
      "step": 200300
    },
    {
      "epoch": 15.308227026201207,
      "grad_norm": 5.599575042724609,
      "learning_rate": 3.724314414483233e-05,
      "loss": 1.742,
      "step": 200400
    },
    {
      "epoch": 15.315865862042624,
      "grad_norm": 6.081447601318359,
      "learning_rate": 3.7236778448297814e-05,
      "loss": 1.8013,
      "step": 200500
    },
    {
      "epoch": 15.323504697884042,
      "grad_norm": 3.6270947456359863,
      "learning_rate": 3.72304127517633e-05,
      "loss": 1.6922,
      "step": 200600
    },
    {
      "epoch": 15.33114353372546,
      "grad_norm": 4.992918014526367,
      "learning_rate": 3.722404705522878e-05,
      "loss": 1.8813,
      "step": 200700
    },
    {
      "epoch": 15.338782369566879,
      "grad_norm": 6.2418646812438965,
      "learning_rate": 3.7217681358694264e-05,
      "loss": 1.842,
      "step": 200800
    },
    {
      "epoch": 15.346421205408296,
      "grad_norm": 5.169105529785156,
      "learning_rate": 3.7211315662159755e-05,
      "loss": 1.7969,
      "step": 200900
    },
    {
      "epoch": 15.354060041249713,
      "grad_norm": 4.839015483856201,
      "learning_rate": 3.720494996562524e-05,
      "loss": 1.8124,
      "step": 201000
    },
    {
      "epoch": 15.361698877091131,
      "grad_norm": 6.220402717590332,
      "learning_rate": 3.719858426909072e-05,
      "loss": 1.7647,
      "step": 201100
    },
    {
      "epoch": 15.369337712932548,
      "grad_norm": 5.099370956420898,
      "learning_rate": 3.719221857255621e-05,
      "loss": 1.8361,
      "step": 201200
    },
    {
      "epoch": 15.376976548773968,
      "grad_norm": 7.413557052612305,
      "learning_rate": 3.7185852876021696e-05,
      "loss": 1.9831,
      "step": 201300
    },
    {
      "epoch": 15.384615384615385,
      "grad_norm": 6.13831901550293,
      "learning_rate": 3.717948717948718e-05,
      "loss": 1.7961,
      "step": 201400
    },
    {
      "epoch": 15.392254220456802,
      "grad_norm": 6.4407243728637695,
      "learning_rate": 3.717312148295267e-05,
      "loss": 1.8807,
      "step": 201500
    },
    {
      "epoch": 15.39989305629822,
      "grad_norm": 6.143129348754883,
      "learning_rate": 3.716675578641815e-05,
      "loss": 1.7629,
      "step": 201600
    },
    {
      "epoch": 15.407531892139637,
      "grad_norm": 5.959409236907959,
      "learning_rate": 3.7160390089883637e-05,
      "loss": 1.7886,
      "step": 201700
    },
    {
      "epoch": 15.415170727981057,
      "grad_norm": 9.532820701599121,
      "learning_rate": 3.715402439334913e-05,
      "loss": 1.8723,
      "step": 201800
    },
    {
      "epoch": 15.422809563822474,
      "grad_norm": 6.231055736541748,
      "learning_rate": 3.714765869681461e-05,
      "loss": 1.896,
      "step": 201900
    },
    {
      "epoch": 15.430448399663891,
      "grad_norm": 6.3134074211120605,
      "learning_rate": 3.7141293000280094e-05,
      "loss": 1.8646,
      "step": 202000
    },
    {
      "epoch": 15.438087235505309,
      "grad_norm": 3.957463502883911,
      "learning_rate": 3.713492730374558e-05,
      "loss": 1.8281,
      "step": 202100
    },
    {
      "epoch": 15.445726071346726,
      "grad_norm": 8.176911354064941,
      "learning_rate": 3.712856160721107e-05,
      "loss": 1.8348,
      "step": 202200
    },
    {
      "epoch": 15.453364907188144,
      "grad_norm": 5.682509899139404,
      "learning_rate": 3.712219591067655e-05,
      "loss": 1.8439,
      "step": 202300
    },
    {
      "epoch": 15.461003743029563,
      "grad_norm": 5.143177032470703,
      "learning_rate": 3.7115830214142035e-05,
      "loss": 1.7207,
      "step": 202400
    },
    {
      "epoch": 15.46864257887098,
      "grad_norm": 5.383087635040283,
      "learning_rate": 3.710946451760752e-05,
      "loss": 1.7646,
      "step": 202500
    },
    {
      "epoch": 15.476281414712398,
      "grad_norm": 6.114420413970947,
      "learning_rate": 3.7103098821073e-05,
      "loss": 1.8739,
      "step": 202600
    },
    {
      "epoch": 15.483920250553815,
      "grad_norm": 5.8337554931640625,
      "learning_rate": 3.709673312453849e-05,
      "loss": 1.8253,
      "step": 202700
    },
    {
      "epoch": 15.491559086395233,
      "grad_norm": 4.5322113037109375,
      "learning_rate": 3.7090367428003976e-05,
      "loss": 1.7912,
      "step": 202800
    },
    {
      "epoch": 15.49919792223665,
      "grad_norm": 5.633886337280273,
      "learning_rate": 3.708400173146946e-05,
      "loss": 1.8137,
      "step": 202900
    },
    {
      "epoch": 15.50683675807807,
      "grad_norm": 5.361466884613037,
      "learning_rate": 3.707763603493494e-05,
      "loss": 1.7853,
      "step": 203000
    },
    {
      "epoch": 15.514475593919487,
      "grad_norm": 5.3398261070251465,
      "learning_rate": 3.707127033840043e-05,
      "loss": 1.7807,
      "step": 203100
    },
    {
      "epoch": 15.522114429760904,
      "grad_norm": 4.433064937591553,
      "learning_rate": 3.706490464186592e-05,
      "loss": 1.7991,
      "step": 203200
    },
    {
      "epoch": 15.529753265602322,
      "grad_norm": 4.154852867126465,
      "learning_rate": 3.70585389453314e-05,
      "loss": 1.8458,
      "step": 203300
    },
    {
      "epoch": 15.53739210144374,
      "grad_norm": 4.387753009796143,
      "learning_rate": 3.7052173248796884e-05,
      "loss": 1.7402,
      "step": 203400
    },
    {
      "epoch": 15.545030937285159,
      "grad_norm": 5.346729278564453,
      "learning_rate": 3.704580755226237e-05,
      "loss": 1.8046,
      "step": 203500
    },
    {
      "epoch": 15.552669773126576,
      "grad_norm": 4.654165267944336,
      "learning_rate": 3.703944185572786e-05,
      "loss": 1.8228,
      "step": 203600
    },
    {
      "epoch": 15.560308608967993,
      "grad_norm": 3.8038642406463623,
      "learning_rate": 3.703307615919334e-05,
      "loss": 1.8132,
      "step": 203700
    },
    {
      "epoch": 15.56794744480941,
      "grad_norm": 4.39298677444458,
      "learning_rate": 3.7026710462658825e-05,
      "loss": 1.8259,
      "step": 203800
    },
    {
      "epoch": 15.575586280650828,
      "grad_norm": 4.2919111251831055,
      "learning_rate": 3.702034476612431e-05,
      "loss": 1.9188,
      "step": 203900
    },
    {
      "epoch": 15.583225116492248,
      "grad_norm": 4.9936957359313965,
      "learning_rate": 3.701397906958979e-05,
      "loss": 1.7636,
      "step": 204000
    },
    {
      "epoch": 15.590863952333665,
      "grad_norm": 5.231042861938477,
      "learning_rate": 3.700761337305528e-05,
      "loss": 1.7258,
      "step": 204100
    },
    {
      "epoch": 15.598502788175082,
      "grad_norm": 4.33424711227417,
      "learning_rate": 3.7001247676520766e-05,
      "loss": 1.7559,
      "step": 204200
    },
    {
      "epoch": 15.6061416240165,
      "grad_norm": 5.159147262573242,
      "learning_rate": 3.699488197998625e-05,
      "loss": 1.8602,
      "step": 204300
    },
    {
      "epoch": 15.613780459857917,
      "grad_norm": 4.209063529968262,
      "learning_rate": 3.698851628345173e-05,
      "loss": 1.8712,
      "step": 204400
    },
    {
      "epoch": 15.621419295699335,
      "grad_norm": 7.893857479095459,
      "learning_rate": 3.698215058691722e-05,
      "loss": 1.8186,
      "step": 204500
    },
    {
      "epoch": 15.629058131540754,
      "grad_norm": 5.832058906555176,
      "learning_rate": 3.6975784890382707e-05,
      "loss": 1.7671,
      "step": 204600
    },
    {
      "epoch": 15.636696967382171,
      "grad_norm": 6.4496846199035645,
      "learning_rate": 3.696941919384819e-05,
      "loss": 1.8025,
      "step": 204700
    },
    {
      "epoch": 15.644335803223589,
      "grad_norm": 5.713418960571289,
      "learning_rate": 3.6963053497313674e-05,
      "loss": 1.8202,
      "step": 204800
    },
    {
      "epoch": 15.651974639065006,
      "grad_norm": 5.890693664550781,
      "learning_rate": 3.6956687800779164e-05,
      "loss": 1.8109,
      "step": 204900
    },
    {
      "epoch": 15.659613474906424,
      "grad_norm": 7.109642028808594,
      "learning_rate": 3.695032210424465e-05,
      "loss": 1.7224,
      "step": 205000
    },
    {
      "epoch": 15.667252310747841,
      "grad_norm": 5.257635593414307,
      "learning_rate": 3.694395640771013e-05,
      "loss": 1.7848,
      "step": 205100
    },
    {
      "epoch": 15.67489114658926,
      "grad_norm": 5.117490768432617,
      "learning_rate": 3.693759071117562e-05,
      "loss": 1.8102,
      "step": 205200
    },
    {
      "epoch": 15.682529982430678,
      "grad_norm": 3.6975290775299072,
      "learning_rate": 3.6931225014641105e-05,
      "loss": 1.9182,
      "step": 205300
    },
    {
      "epoch": 15.690168818272095,
      "grad_norm": 8.120601654052734,
      "learning_rate": 3.692485931810659e-05,
      "loss": 1.7624,
      "step": 205400
    },
    {
      "epoch": 15.697807654113513,
      "grad_norm": 3.8267087936401367,
      "learning_rate": 3.691849362157208e-05,
      "loss": 1.7969,
      "step": 205500
    },
    {
      "epoch": 15.70544648995493,
      "grad_norm": 5.0041704177856445,
      "learning_rate": 3.691212792503756e-05,
      "loss": 1.8517,
      "step": 205600
    },
    {
      "epoch": 15.71308532579635,
      "grad_norm": 4.9835968017578125,
      "learning_rate": 3.6905762228503046e-05,
      "loss": 1.7954,
      "step": 205700
    },
    {
      "epoch": 15.720724161637767,
      "grad_norm": 4.104423999786377,
      "learning_rate": 3.689939653196853e-05,
      "loss": 1.8693,
      "step": 205800
    },
    {
      "epoch": 15.728362997479184,
      "grad_norm": 5.5035719871521,
      "learning_rate": 3.689303083543402e-05,
      "loss": 1.8352,
      "step": 205900
    },
    {
      "epoch": 15.736001833320602,
      "grad_norm": 5.911996364593506,
      "learning_rate": 3.68866651388995e-05,
      "loss": 1.8014,
      "step": 206000
    },
    {
      "epoch": 15.74364066916202,
      "grad_norm": 3.260732889175415,
      "learning_rate": 3.688029944236499e-05,
      "loss": 1.7453,
      "step": 206100
    },
    {
      "epoch": 15.751279505003437,
      "grad_norm": 5.039870262145996,
      "learning_rate": 3.687393374583047e-05,
      "loss": 1.7973,
      "step": 206200
    },
    {
      "epoch": 15.758918340844856,
      "grad_norm": 5.627043724060059,
      "learning_rate": 3.6867568049295954e-05,
      "loss": 1.7517,
      "step": 206300
    },
    {
      "epoch": 15.766557176686273,
      "grad_norm": 5.297060489654541,
      "learning_rate": 3.6861202352761444e-05,
      "loss": 1.8882,
      "step": 206400
    },
    {
      "epoch": 15.77419601252769,
      "grad_norm": 5.5022077560424805,
      "learning_rate": 3.685483665622693e-05,
      "loss": 1.7849,
      "step": 206500
    },
    {
      "epoch": 15.781834848369108,
      "grad_norm": 5.027629852294922,
      "learning_rate": 3.684847095969241e-05,
      "loss": 1.7143,
      "step": 206600
    },
    {
      "epoch": 15.789473684210526,
      "grad_norm": 5.3006815910339355,
      "learning_rate": 3.6842105263157895e-05,
      "loss": 1.8442,
      "step": 206700
    },
    {
      "epoch": 15.797112520051945,
      "grad_norm": 6.566426753997803,
      "learning_rate": 3.6835739566623385e-05,
      "loss": 1.7847,
      "step": 206800
    },
    {
      "epoch": 15.804751355893362,
      "grad_norm": 6.3089399337768555,
      "learning_rate": 3.682937387008887e-05,
      "loss": 1.8284,
      "step": 206900
    },
    {
      "epoch": 15.81239019173478,
      "grad_norm": 4.612377166748047,
      "learning_rate": 3.682300817355435e-05,
      "loss": 1.8593,
      "step": 207000
    },
    {
      "epoch": 15.820029027576197,
      "grad_norm": 5.728493690490723,
      "learning_rate": 3.6816642477019836e-05,
      "loss": 1.8158,
      "step": 207100
    },
    {
      "epoch": 15.827667863417615,
      "grad_norm": 4.825331211090088,
      "learning_rate": 3.681027678048532e-05,
      "loss": 1.7594,
      "step": 207200
    },
    {
      "epoch": 15.835306699259032,
      "grad_norm": 4.461496829986572,
      "learning_rate": 3.680391108395081e-05,
      "loss": 1.8916,
      "step": 207300
    },
    {
      "epoch": 15.842945535100451,
      "grad_norm": 5.115851402282715,
      "learning_rate": 3.679754538741629e-05,
      "loss": 1.7267,
      "step": 207400
    },
    {
      "epoch": 15.850584370941869,
      "grad_norm": 5.518476486206055,
      "learning_rate": 3.6791179690881777e-05,
      "loss": 1.9241,
      "step": 207500
    },
    {
      "epoch": 15.858223206783286,
      "grad_norm": 4.839174747467041,
      "learning_rate": 3.678481399434726e-05,
      "loss": 1.8134,
      "step": 207600
    },
    {
      "epoch": 15.865862042624704,
      "grad_norm": 5.663079261779785,
      "learning_rate": 3.677844829781275e-05,
      "loss": 1.8506,
      "step": 207700
    },
    {
      "epoch": 15.873500878466121,
      "grad_norm": 4.38652229309082,
      "learning_rate": 3.6772082601278234e-05,
      "loss": 1.75,
      "step": 207800
    },
    {
      "epoch": 15.88113971430754,
      "grad_norm": 5.450532913208008,
      "learning_rate": 3.676571690474372e-05,
      "loss": 1.8874,
      "step": 207900
    },
    {
      "epoch": 15.888778550148958,
      "grad_norm": 7.140590667724609,
      "learning_rate": 3.67593512082092e-05,
      "loss": 1.8122,
      "step": 208000
    },
    {
      "epoch": 15.896417385990375,
      "grad_norm": 5.644489765167236,
      "learning_rate": 3.6752985511674685e-05,
      "loss": 1.8766,
      "step": 208100
    },
    {
      "epoch": 15.904056221831793,
      "grad_norm": 5.771294593811035,
      "learning_rate": 3.6746619815140175e-05,
      "loss": 1.8063,
      "step": 208200
    },
    {
      "epoch": 15.91169505767321,
      "grad_norm": 5.661468505859375,
      "learning_rate": 3.674025411860566e-05,
      "loss": 1.8316,
      "step": 208300
    },
    {
      "epoch": 15.919333893514628,
      "grad_norm": 4.225802421569824,
      "learning_rate": 3.673388842207114e-05,
      "loss": 1.7697,
      "step": 208400
    },
    {
      "epoch": 15.926972729356047,
      "grad_norm": 4.956818103790283,
      "learning_rate": 3.6727522725536625e-05,
      "loss": 1.7666,
      "step": 208500
    },
    {
      "epoch": 15.934611565197464,
      "grad_norm": 4.472984790802002,
      "learning_rate": 3.6721157029002116e-05,
      "loss": 1.7815,
      "step": 208600
    },
    {
      "epoch": 15.942250401038882,
      "grad_norm": 4.550168991088867,
      "learning_rate": 3.67147913324676e-05,
      "loss": 1.9254,
      "step": 208700
    },
    {
      "epoch": 15.9498892368803,
      "grad_norm": 5.102032661437988,
      "learning_rate": 3.670842563593308e-05,
      "loss": 1.8344,
      "step": 208800
    },
    {
      "epoch": 15.957528072721717,
      "grad_norm": 4.15730619430542,
      "learning_rate": 3.6702059939398566e-05,
      "loss": 1.7264,
      "step": 208900
    },
    {
      "epoch": 15.965166908563136,
      "grad_norm": 4.473861217498779,
      "learning_rate": 3.669569424286406e-05,
      "loss": 1.8904,
      "step": 209000
    },
    {
      "epoch": 15.972805744404553,
      "grad_norm": 5.486338138580322,
      "learning_rate": 3.668932854632954e-05,
      "loss": 1.834,
      "step": 209100
    },
    {
      "epoch": 15.98044458024597,
      "grad_norm": 6.485126495361328,
      "learning_rate": 3.668296284979503e-05,
      "loss": 1.7434,
      "step": 209200
    },
    {
      "epoch": 15.988083416087388,
      "grad_norm": 4.639641761779785,
      "learning_rate": 3.6676597153260514e-05,
      "loss": 1.7905,
      "step": 209300
    },
    {
      "epoch": 15.995722251928806,
      "grad_norm": 5.3485260009765625,
      "learning_rate": 3.6670231456726e-05,
      "loss": 1.8806,
      "step": 209400
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.8179645538330078,
      "eval_runtime": 1.4747,
      "eval_samples_per_second": 467.892,
      "eval_steps_per_second": 467.892,
      "step": 209456
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.5737272500991821,
      "eval_runtime": 32.3869,
      "eval_samples_per_second": 404.207,
      "eval_steps_per_second": 404.207,
      "step": 209456
    },
    {
      "epoch": 16.003361087770223,
      "grad_norm": 6.176117420196533,
      "learning_rate": 3.666386576019148e-05,
      "loss": 1.8411,
      "step": 209500
    },
    {
      "epoch": 16.010999923611642,
      "grad_norm": 5.646403789520264,
      "learning_rate": 3.665750006365697e-05,
      "loss": 1.8061,
      "step": 209600
    },
    {
      "epoch": 16.018638759453058,
      "grad_norm": 3.870994806289673,
      "learning_rate": 3.6651134367122455e-05,
      "loss": 1.7937,
      "step": 209700
    },
    {
      "epoch": 16.026277595294477,
      "grad_norm": 6.515573978424072,
      "learning_rate": 3.664476867058794e-05,
      "loss": 1.7795,
      "step": 209800
    },
    {
      "epoch": 16.033916431135896,
      "grad_norm": 5.7642316818237305,
      "learning_rate": 3.663840297405342e-05,
      "loss": 1.746,
      "step": 209900
    },
    {
      "epoch": 16.041555266977312,
      "grad_norm": 5.407458782196045,
      "learning_rate": 3.663203727751891e-05,
      "loss": 1.7686,
      "step": 210000
    },
    {
      "epoch": 16.04919410281873,
      "grad_norm": 5.707695960998535,
      "learning_rate": 3.6625671580984396e-05,
      "loss": 1.7588,
      "step": 210100
    },
    {
      "epoch": 16.056832938660147,
      "grad_norm": 5.005987167358398,
      "learning_rate": 3.661930588444988e-05,
      "loss": 1.8693,
      "step": 210200
    },
    {
      "epoch": 16.064471774501566,
      "grad_norm": 6.757454872131348,
      "learning_rate": 3.661294018791536e-05,
      "loss": 1.7062,
      "step": 210300
    },
    {
      "epoch": 16.072110610342985,
      "grad_norm": 5.462362289428711,
      "learning_rate": 3.6606574491380847e-05,
      "loss": 1.7278,
      "step": 210400
    },
    {
      "epoch": 16.0797494461844,
      "grad_norm": 4.6559062004089355,
      "learning_rate": 3.660020879484634e-05,
      "loss": 1.8102,
      "step": 210500
    },
    {
      "epoch": 16.08738828202582,
      "grad_norm": 5.227852821350098,
      "learning_rate": 3.659384309831182e-05,
      "loss": 1.85,
      "step": 210600
    },
    {
      "epoch": 16.095027117867236,
      "grad_norm": 4.938114643096924,
      "learning_rate": 3.6587477401777304e-05,
      "loss": 1.8416,
      "step": 210700
    },
    {
      "epoch": 16.102665953708655,
      "grad_norm": 7.346367835998535,
      "learning_rate": 3.658111170524279e-05,
      "loss": 1.6836,
      "step": 210800
    },
    {
      "epoch": 16.11030478955007,
      "grad_norm": 4.520803928375244,
      "learning_rate": 3.657474600870828e-05,
      "loss": 1.8035,
      "step": 210900
    },
    {
      "epoch": 16.11794362539149,
      "grad_norm": 5.918234348297119,
      "learning_rate": 3.656838031217376e-05,
      "loss": 1.77,
      "step": 211000
    },
    {
      "epoch": 16.12558246123291,
      "grad_norm": 4.038640022277832,
      "learning_rate": 3.6562014615639245e-05,
      "loss": 1.8387,
      "step": 211100
    },
    {
      "epoch": 16.133221297074325,
      "grad_norm": 5.548118591308594,
      "learning_rate": 3.655564891910473e-05,
      "loss": 1.7777,
      "step": 211200
    },
    {
      "epoch": 16.140860132915744,
      "grad_norm": 5.266526222229004,
      "learning_rate": 3.654928322257021e-05,
      "loss": 1.8148,
      "step": 211300
    },
    {
      "epoch": 16.14849896875716,
      "grad_norm": 5.453585624694824,
      "learning_rate": 3.65429175260357e-05,
      "loss": 1.8489,
      "step": 211400
    },
    {
      "epoch": 16.15613780459858,
      "grad_norm": 7.698526382446289,
      "learning_rate": 3.6536551829501186e-05,
      "loss": 1.7784,
      "step": 211500
    },
    {
      "epoch": 16.16377664044,
      "grad_norm": 5.432257652282715,
      "learning_rate": 3.653018613296667e-05,
      "loss": 1.8099,
      "step": 211600
    },
    {
      "epoch": 16.171415476281414,
      "grad_norm": 4.2519211769104,
      "learning_rate": 3.652382043643215e-05,
      "loss": 1.7815,
      "step": 211700
    },
    {
      "epoch": 16.179054312122833,
      "grad_norm": 6.352433204650879,
      "learning_rate": 3.651745473989764e-05,
      "loss": 1.7222,
      "step": 211800
    },
    {
      "epoch": 16.18669314796425,
      "grad_norm": 5.5824384689331055,
      "learning_rate": 3.651108904336313e-05,
      "loss": 1.8371,
      "step": 211900
    },
    {
      "epoch": 16.194331983805668,
      "grad_norm": 4.649415969848633,
      "learning_rate": 3.650472334682861e-05,
      "loss": 1.7703,
      "step": 212000
    },
    {
      "epoch": 16.201970819647087,
      "grad_norm": 5.667125225067139,
      "learning_rate": 3.6498357650294094e-05,
      "loss": 1.8699,
      "step": 212100
    },
    {
      "epoch": 16.209609655488503,
      "grad_norm": 5.64997673034668,
      "learning_rate": 3.649199195375958e-05,
      "loss": 1.8025,
      "step": 212200
    },
    {
      "epoch": 16.217248491329922,
      "grad_norm": 5.781940937042236,
      "learning_rate": 3.648562625722507e-05,
      "loss": 1.7775,
      "step": 212300
    },
    {
      "epoch": 16.224887327171338,
      "grad_norm": 5.29103946685791,
      "learning_rate": 3.647926056069055e-05,
      "loss": 1.789,
      "step": 212400
    },
    {
      "epoch": 16.232526163012757,
      "grad_norm": 5.783700942993164,
      "learning_rate": 3.6472894864156035e-05,
      "loss": 1.7792,
      "step": 212500
    },
    {
      "epoch": 16.240164998854176,
      "grad_norm": 4.379150867462158,
      "learning_rate": 3.646652916762152e-05,
      "loss": 1.7174,
      "step": 212600
    },
    {
      "epoch": 16.247803834695592,
      "grad_norm": 5.139261722564697,
      "learning_rate": 3.646016347108701e-05,
      "loss": 1.7841,
      "step": 212700
    },
    {
      "epoch": 16.25544267053701,
      "grad_norm": 5.724588871002197,
      "learning_rate": 3.645379777455249e-05,
      "loss": 1.8527,
      "step": 212800
    },
    {
      "epoch": 16.263081506378427,
      "grad_norm": 4.473426818847656,
      "learning_rate": 3.6447432078017976e-05,
      "loss": 1.7004,
      "step": 212900
    },
    {
      "epoch": 16.270720342219846,
      "grad_norm": 4.725573539733887,
      "learning_rate": 3.6441066381483466e-05,
      "loss": 1.8483,
      "step": 213000
    },
    {
      "epoch": 16.27835917806126,
      "grad_norm": 5.75770378112793,
      "learning_rate": 3.643470068494895e-05,
      "loss": 1.793,
      "step": 213100
    },
    {
      "epoch": 16.28599801390268,
      "grad_norm": 5.746504783630371,
      "learning_rate": 3.642833498841444e-05,
      "loss": 1.7314,
      "step": 213200
    },
    {
      "epoch": 16.2936368497441,
      "grad_norm": 4.978047847747803,
      "learning_rate": 3.642196929187992e-05,
      "loss": 1.8294,
      "step": 213300
    },
    {
      "epoch": 16.301275685585516,
      "grad_norm": 4.217164993286133,
      "learning_rate": 3.641560359534541e-05,
      "loss": 1.8192,
      "step": 213400
    },
    {
      "epoch": 16.308914521426935,
      "grad_norm": 5.591315269470215,
      "learning_rate": 3.640923789881089e-05,
      "loss": 1.6969,
      "step": 213500
    },
    {
      "epoch": 16.31655335726835,
      "grad_norm": 8.342040061950684,
      "learning_rate": 3.6402872202276374e-05,
      "loss": 1.7632,
      "step": 213600
    },
    {
      "epoch": 16.32419219310977,
      "grad_norm": 5.853829383850098,
      "learning_rate": 3.6396506505741864e-05,
      "loss": 1.7716,
      "step": 213700
    },
    {
      "epoch": 16.33183102895119,
      "grad_norm": 5.431941032409668,
      "learning_rate": 3.639014080920735e-05,
      "loss": 1.7393,
      "step": 213800
    },
    {
      "epoch": 16.339469864792605,
      "grad_norm": 5.731469631195068,
      "learning_rate": 3.638377511267283e-05,
      "loss": 1.8943,
      "step": 213900
    },
    {
      "epoch": 16.347108700634024,
      "grad_norm": 3.8490543365478516,
      "learning_rate": 3.6377409416138315e-05,
      "loss": 1.8825,
      "step": 214000
    },
    {
      "epoch": 16.35474753647544,
      "grad_norm": 6.93500280380249,
      "learning_rate": 3.6371043719603805e-05,
      "loss": 1.8571,
      "step": 214100
    },
    {
      "epoch": 16.36238637231686,
      "grad_norm": 6.234709739685059,
      "learning_rate": 3.636467802306929e-05,
      "loss": 1.7494,
      "step": 214200
    },
    {
      "epoch": 16.370025208158278,
      "grad_norm": 3.389967679977417,
      "learning_rate": 3.635831232653477e-05,
      "loss": 1.8188,
      "step": 214300
    },
    {
      "epoch": 16.377664043999694,
      "grad_norm": 5.3453474044799805,
      "learning_rate": 3.6351946630000256e-05,
      "loss": 1.861,
      "step": 214400
    },
    {
      "epoch": 16.385302879841113,
      "grad_norm": 5.789317607879639,
      "learning_rate": 3.634558093346574e-05,
      "loss": 1.746,
      "step": 214500
    },
    {
      "epoch": 16.39294171568253,
      "grad_norm": 5.671370029449463,
      "learning_rate": 3.633921523693123e-05,
      "loss": 1.8557,
      "step": 214600
    },
    {
      "epoch": 16.400580551523948,
      "grad_norm": 6.92789363861084,
      "learning_rate": 3.633284954039671e-05,
      "loss": 1.7923,
      "step": 214700
    },
    {
      "epoch": 16.408219387365367,
      "grad_norm": 5.4073991775512695,
      "learning_rate": 3.63264838438622e-05,
      "loss": 1.7962,
      "step": 214800
    },
    {
      "epoch": 16.415858223206783,
      "grad_norm": 5.029530048370361,
      "learning_rate": 3.632011814732768e-05,
      "loss": 1.8522,
      "step": 214900
    },
    {
      "epoch": 16.423497059048202,
      "grad_norm": 4.907399654388428,
      "learning_rate": 3.6313752450793164e-05,
      "loss": 1.7985,
      "step": 215000
    },
    {
      "epoch": 16.431135894889618,
      "grad_norm": 4.412604331970215,
      "learning_rate": 3.6307386754258654e-05,
      "loss": 1.749,
      "step": 215100
    },
    {
      "epoch": 16.438774730731037,
      "grad_norm": 6.382293224334717,
      "learning_rate": 3.630102105772414e-05,
      "loss": 1.8523,
      "step": 215200
    },
    {
      "epoch": 16.446413566572453,
      "grad_norm": 5.447974681854248,
      "learning_rate": 3.629465536118962e-05,
      "loss": 1.8515,
      "step": 215300
    },
    {
      "epoch": 16.454052402413872,
      "grad_norm": 4.916696071624756,
      "learning_rate": 3.6288289664655105e-05,
      "loss": 1.8878,
      "step": 215400
    },
    {
      "epoch": 16.46169123825529,
      "grad_norm": 4.776894569396973,
      "learning_rate": 3.6281923968120595e-05,
      "loss": 1.8157,
      "step": 215500
    },
    {
      "epoch": 16.469330074096707,
      "grad_norm": 5.1199541091918945,
      "learning_rate": 3.627555827158608e-05,
      "loss": 1.8218,
      "step": 215600
    },
    {
      "epoch": 16.476968909938126,
      "grad_norm": 7.283223628997803,
      "learning_rate": 3.626919257505156e-05,
      "loss": 1.7245,
      "step": 215700
    },
    {
      "epoch": 16.48460774577954,
      "grad_norm": 7.0128493309021,
      "learning_rate": 3.6262826878517046e-05,
      "loss": 1.7866,
      "step": 215800
    },
    {
      "epoch": 16.49224658162096,
      "grad_norm": 4.441654682159424,
      "learning_rate": 3.625646118198253e-05,
      "loss": 1.8835,
      "step": 215900
    },
    {
      "epoch": 16.49988541746238,
      "grad_norm": 6.346991062164307,
      "learning_rate": 3.625009548544802e-05,
      "loss": 1.7917,
      "step": 216000
    },
    {
      "epoch": 16.507524253303796,
      "grad_norm": 4.532811164855957,
      "learning_rate": 3.62437297889135e-05,
      "loss": 1.767,
      "step": 216100
    },
    {
      "epoch": 16.515163089145215,
      "grad_norm": 4.20575475692749,
      "learning_rate": 3.6237364092378987e-05,
      "loss": 1.7532,
      "step": 216200
    },
    {
      "epoch": 16.52280192498663,
      "grad_norm": 3.4635279178619385,
      "learning_rate": 3.623099839584447e-05,
      "loss": 1.9222,
      "step": 216300
    },
    {
      "epoch": 16.53044076082805,
      "grad_norm": 4.601718902587891,
      "learning_rate": 3.622463269930996e-05,
      "loss": 1.8274,
      "step": 216400
    },
    {
      "epoch": 16.53807959666947,
      "grad_norm": 6.431770324707031,
      "learning_rate": 3.6218267002775444e-05,
      "loss": 1.8217,
      "step": 216500
    },
    {
      "epoch": 16.545718432510885,
      "grad_norm": 6.11761474609375,
      "learning_rate": 3.621190130624093e-05,
      "loss": 1.7878,
      "step": 216600
    },
    {
      "epoch": 16.553357268352304,
      "grad_norm": 4.552426338195801,
      "learning_rate": 3.620553560970642e-05,
      "loss": 1.9567,
      "step": 216700
    },
    {
      "epoch": 16.56099610419372,
      "grad_norm": 5.457888126373291,
      "learning_rate": 3.61991699131719e-05,
      "loss": 1.7416,
      "step": 216800
    },
    {
      "epoch": 16.56863494003514,
      "grad_norm": 4.9559245109558105,
      "learning_rate": 3.6192804216637385e-05,
      "loss": 1.703,
      "step": 216900
    },
    {
      "epoch": 16.576273775876558,
      "grad_norm": 4.845005512237549,
      "learning_rate": 3.6186438520102875e-05,
      "loss": 1.7104,
      "step": 217000
    },
    {
      "epoch": 16.583912611717974,
      "grad_norm": 7.101718902587891,
      "learning_rate": 3.618007282356836e-05,
      "loss": 1.7373,
      "step": 217100
    },
    {
      "epoch": 16.591551447559393,
      "grad_norm": 5.461655616760254,
      "learning_rate": 3.617370712703384e-05,
      "loss": 1.8202,
      "step": 217200
    },
    {
      "epoch": 16.59919028340081,
      "grad_norm": 3.7318975925445557,
      "learning_rate": 3.616734143049933e-05,
      "loss": 1.7949,
      "step": 217300
    },
    {
      "epoch": 16.606829119242228,
      "grad_norm": 5.580477237701416,
      "learning_rate": 3.6160975733964816e-05,
      "loss": 1.8018,
      "step": 217400
    },
    {
      "epoch": 16.614467955083644,
      "grad_norm": 5.106924057006836,
      "learning_rate": 3.61546100374303e-05,
      "loss": 1.7963,
      "step": 217500
    },
    {
      "epoch": 16.622106790925063,
      "grad_norm": 4.730681419372559,
      "learning_rate": 3.614824434089578e-05,
      "loss": 1.8513,
      "step": 217600
    },
    {
      "epoch": 16.629745626766482,
      "grad_norm": 6.956821441650391,
      "learning_rate": 3.614187864436127e-05,
      "loss": 1.9192,
      "step": 217700
    },
    {
      "epoch": 16.637384462607898,
      "grad_norm": 5.3766961097717285,
      "learning_rate": 3.613551294782676e-05,
      "loss": 1.9404,
      "step": 217800
    },
    {
      "epoch": 16.645023298449317,
      "grad_norm": 5.795951843261719,
      "learning_rate": 3.612914725129224e-05,
      "loss": 1.7796,
      "step": 217900
    },
    {
      "epoch": 16.652662134290733,
      "grad_norm": 4.902532577514648,
      "learning_rate": 3.6122781554757724e-05,
      "loss": 1.8355,
      "step": 218000
    },
    {
      "epoch": 16.660300970132152,
      "grad_norm": 5.635190963745117,
      "learning_rate": 3.611641585822321e-05,
      "loss": 1.7569,
      "step": 218100
    },
    {
      "epoch": 16.66793980597357,
      "grad_norm": 6.644406318664551,
      "learning_rate": 3.611005016168869e-05,
      "loss": 1.8041,
      "step": 218200
    },
    {
      "epoch": 16.675578641814987,
      "grad_norm": 7.612132549285889,
      "learning_rate": 3.610368446515418e-05,
      "loss": 1.6663,
      "step": 218300
    },
    {
      "epoch": 16.683217477656406,
      "grad_norm": 4.493908405303955,
      "learning_rate": 3.6097318768619665e-05,
      "loss": 1.7437,
      "step": 218400
    },
    {
      "epoch": 16.69085631349782,
      "grad_norm": 5.365228652954102,
      "learning_rate": 3.609095307208515e-05,
      "loss": 1.795,
      "step": 218500
    },
    {
      "epoch": 16.69849514933924,
      "grad_norm": 4.007399082183838,
      "learning_rate": 3.608458737555063e-05,
      "loss": 1.749,
      "step": 218600
    },
    {
      "epoch": 16.70613398518066,
      "grad_norm": 7.654324054718018,
      "learning_rate": 3.607822167901612e-05,
      "loss": 1.7729,
      "step": 218700
    },
    {
      "epoch": 16.713772821022076,
      "grad_norm": 5.535685062408447,
      "learning_rate": 3.6071855982481606e-05,
      "loss": 1.9277,
      "step": 218800
    },
    {
      "epoch": 16.721411656863495,
      "grad_norm": 5.5941596031188965,
      "learning_rate": 3.606549028594709e-05,
      "loss": 1.858,
      "step": 218900
    },
    {
      "epoch": 16.72905049270491,
      "grad_norm": 4.702564239501953,
      "learning_rate": 3.605912458941257e-05,
      "loss": 1.7586,
      "step": 219000
    },
    {
      "epoch": 16.73668932854633,
      "grad_norm": 4.306202411651611,
      "learning_rate": 3.6052758892878057e-05,
      "loss": 1.7799,
      "step": 219100
    },
    {
      "epoch": 16.744328164387746,
      "grad_norm": 4.845029354095459,
      "learning_rate": 3.604639319634355e-05,
      "loss": 1.8002,
      "step": 219200
    },
    {
      "epoch": 16.751967000229165,
      "grad_norm": 4.734576225280762,
      "learning_rate": 3.604002749980903e-05,
      "loss": 1.7808,
      "step": 219300
    },
    {
      "epoch": 16.759605836070584,
      "grad_norm": 6.146760940551758,
      "learning_rate": 3.6033661803274514e-05,
      "loss": 1.8538,
      "step": 219400
    },
    {
      "epoch": 16.767244671912,
      "grad_norm": 5.8753132820129395,
      "learning_rate": 3.602729610674e-05,
      "loss": 1.8403,
      "step": 219500
    },
    {
      "epoch": 16.77488350775342,
      "grad_norm": 5.121211051940918,
      "learning_rate": 3.602093041020549e-05,
      "loss": 1.7406,
      "step": 219600
    },
    {
      "epoch": 16.782522343594835,
      "grad_norm": 4.218024253845215,
      "learning_rate": 3.601456471367097e-05,
      "loss": 1.8251,
      "step": 219700
    },
    {
      "epoch": 16.790161179436254,
      "grad_norm": 5.431924819946289,
      "learning_rate": 3.6008199017136455e-05,
      "loss": 1.7766,
      "step": 219800
    },
    {
      "epoch": 16.797800015277673,
      "grad_norm": 6.882935523986816,
      "learning_rate": 3.600183332060194e-05,
      "loss": 1.7954,
      "step": 219900
    },
    {
      "epoch": 16.80543885111909,
      "grad_norm": 4.978346824645996,
      "learning_rate": 3.599546762406742e-05,
      "loss": 1.8812,
      "step": 220000
    },
    {
      "epoch": 16.813077686960508,
      "grad_norm": 4.12976598739624,
      "learning_rate": 3.598910192753291e-05,
      "loss": 1.8439,
      "step": 220100
    },
    {
      "epoch": 16.820716522801924,
      "grad_norm": 4.567799091339111,
      "learning_rate": 3.5982736230998396e-05,
      "loss": 1.7718,
      "step": 220200
    },
    {
      "epoch": 16.828355358643343,
      "grad_norm": 5.2771220207214355,
      "learning_rate": 3.597637053446388e-05,
      "loss": 1.8452,
      "step": 220300
    },
    {
      "epoch": 16.835994194484762,
      "grad_norm": 4.520233631134033,
      "learning_rate": 3.597000483792937e-05,
      "loss": 1.8375,
      "step": 220400
    },
    {
      "epoch": 16.843633030326178,
      "grad_norm": 4.649586200714111,
      "learning_rate": 3.596363914139485e-05,
      "loss": 1.8543,
      "step": 220500
    },
    {
      "epoch": 16.851271866167597,
      "grad_norm": 4.496204376220703,
      "learning_rate": 3.595727344486034e-05,
      "loss": 1.7778,
      "step": 220600
    },
    {
      "epoch": 16.858910702009013,
      "grad_norm": 4.309364318847656,
      "learning_rate": 3.595090774832583e-05,
      "loss": 1.8139,
      "step": 220700
    },
    {
      "epoch": 16.86654953785043,
      "grad_norm": 4.651418209075928,
      "learning_rate": 3.594454205179131e-05,
      "loss": 1.6995,
      "step": 220800
    },
    {
      "epoch": 16.87418837369185,
      "grad_norm": 6.379353046417236,
      "learning_rate": 3.5938176355256794e-05,
      "loss": 1.7822,
      "step": 220900
    },
    {
      "epoch": 16.881827209533267,
      "grad_norm": 4.163841724395752,
      "learning_rate": 3.5931810658722284e-05,
      "loss": 1.8281,
      "step": 221000
    },
    {
      "epoch": 16.889466045374686,
      "grad_norm": 5.655233383178711,
      "learning_rate": 3.592544496218777e-05,
      "loss": 1.7914,
      "step": 221100
    },
    {
      "epoch": 16.8971048812161,
      "grad_norm": 4.505074977874756,
      "learning_rate": 3.591907926565325e-05,
      "loss": 1.874,
      "step": 221200
    },
    {
      "epoch": 16.90474371705752,
      "grad_norm": 5.8737568855285645,
      "learning_rate": 3.5912713569118735e-05,
      "loss": 1.8347,
      "step": 221300
    },
    {
      "epoch": 16.91238255289894,
      "grad_norm": 5.983016014099121,
      "learning_rate": 3.590634787258422e-05,
      "loss": 1.7923,
      "step": 221400
    },
    {
      "epoch": 16.920021388740356,
      "grad_norm": 5.7603278160095215,
      "learning_rate": 3.589998217604971e-05,
      "loss": 1.8161,
      "step": 221500
    },
    {
      "epoch": 16.927660224581775,
      "grad_norm": 5.518553733825684,
      "learning_rate": 3.589361647951519e-05,
      "loss": 1.8911,
      "step": 221600
    },
    {
      "epoch": 16.93529906042319,
      "grad_norm": 7.010253429412842,
      "learning_rate": 3.5887250782980676e-05,
      "loss": 1.762,
      "step": 221700
    },
    {
      "epoch": 16.94293789626461,
      "grad_norm": 6.315733432769775,
      "learning_rate": 3.588088508644616e-05,
      "loss": 1.7923,
      "step": 221800
    },
    {
      "epoch": 16.950576732106025,
      "grad_norm": 4.284601211547852,
      "learning_rate": 3.587451938991165e-05,
      "loss": 1.7821,
      "step": 221900
    },
    {
      "epoch": 16.958215567947445,
      "grad_norm": 4.181111812591553,
      "learning_rate": 3.586815369337713e-05,
      "loss": 1.7163,
      "step": 222000
    },
    {
      "epoch": 16.965854403788864,
      "grad_norm": 5.4520158767700195,
      "learning_rate": 3.586178799684262e-05,
      "loss": 1.6913,
      "step": 222100
    },
    {
      "epoch": 16.97349323963028,
      "grad_norm": 5.710373878479004,
      "learning_rate": 3.58554223003081e-05,
      "loss": 1.8148,
      "step": 222200
    },
    {
      "epoch": 16.9811320754717,
      "grad_norm": 4.9659647941589355,
      "learning_rate": 3.5849056603773584e-05,
      "loss": 1.7484,
      "step": 222300
    },
    {
      "epoch": 16.988770911313114,
      "grad_norm": 4.753424644470215,
      "learning_rate": 3.5842690907239074e-05,
      "loss": 1.7576,
      "step": 222400
    },
    {
      "epoch": 16.996409747154534,
      "grad_norm": 4.351564884185791,
      "learning_rate": 3.583632521070456e-05,
      "loss": 1.7253,
      "step": 222500
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.8213446140289307,
      "eval_runtime": 3.0161,
      "eval_samples_per_second": 228.77,
      "eval_steps_per_second": 228.77,
      "step": 222547
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.569947361946106,
      "eval_runtime": 57.414,
      "eval_samples_per_second": 228.011,
      "eval_steps_per_second": 228.011,
      "step": 222547
    },
    {
      "epoch": 17.004048582995953,
      "grad_norm": 6.209295272827148,
      "learning_rate": 3.582995951417004e-05,
      "loss": 1.8168,
      "step": 222600
    },
    {
      "epoch": 17.01168741883737,
      "grad_norm": 6.58927059173584,
      "learning_rate": 3.5823593817635525e-05,
      "loss": 1.8478,
      "step": 222700
    },
    {
      "epoch": 17.019326254678788,
      "grad_norm": 4.73169469833374,
      "learning_rate": 3.5817228121101015e-05,
      "loss": 1.7314,
      "step": 222800
    },
    {
      "epoch": 17.026965090520203,
      "grad_norm": 5.938333034515381,
      "learning_rate": 3.58108624245665e-05,
      "loss": 1.7842,
      "step": 222900
    },
    {
      "epoch": 17.034603926361623,
      "grad_norm": 6.587980270385742,
      "learning_rate": 3.580449672803198e-05,
      "loss": 1.7536,
      "step": 223000
    },
    {
      "epoch": 17.042242762203042,
      "grad_norm": 4.90803337097168,
      "learning_rate": 3.5798131031497466e-05,
      "loss": 1.8019,
      "step": 223100
    },
    {
      "epoch": 17.049881598044458,
      "grad_norm": 5.35658073425293,
      "learning_rate": 3.579176533496295e-05,
      "loss": 1.6889,
      "step": 223200
    },
    {
      "epoch": 17.057520433885877,
      "grad_norm": 5.863280773162842,
      "learning_rate": 3.578539963842844e-05,
      "loss": 1.7707,
      "step": 223300
    },
    {
      "epoch": 17.065159269727292,
      "grad_norm": 4.84030818939209,
      "learning_rate": 3.577903394189392e-05,
      "loss": 1.7051,
      "step": 223400
    },
    {
      "epoch": 17.07279810556871,
      "grad_norm": 6.738310813903809,
      "learning_rate": 3.577266824535941e-05,
      "loss": 1.7817,
      "step": 223500
    },
    {
      "epoch": 17.080436941410127,
      "grad_norm": 4.490910053253174,
      "learning_rate": 3.576630254882489e-05,
      "loss": 1.769,
      "step": 223600
    },
    {
      "epoch": 17.088075777251547,
      "grad_norm": 6.467977046966553,
      "learning_rate": 3.5759936852290374e-05,
      "loss": 1.8113,
      "step": 223700
    },
    {
      "epoch": 17.095714613092966,
      "grad_norm": 5.097722053527832,
      "learning_rate": 3.5753571155755864e-05,
      "loss": 1.7397,
      "step": 223800
    },
    {
      "epoch": 17.10335344893438,
      "grad_norm": 5.787806987762451,
      "learning_rate": 3.574720545922135e-05,
      "loss": 1.719,
      "step": 223900
    },
    {
      "epoch": 17.1109922847758,
      "grad_norm": 5.340574264526367,
      "learning_rate": 3.574083976268683e-05,
      "loss": 1.8233,
      "step": 224000
    },
    {
      "epoch": 17.118631120617216,
      "grad_norm": 6.637300491333008,
      "learning_rate": 3.5734474066152315e-05,
      "loss": 1.7887,
      "step": 224100
    },
    {
      "epoch": 17.126269956458636,
      "grad_norm": 5.320420742034912,
      "learning_rate": 3.5728108369617805e-05,
      "loss": 1.7743,
      "step": 224200
    },
    {
      "epoch": 17.133908792300055,
      "grad_norm": 3.396406888961792,
      "learning_rate": 3.572174267308329e-05,
      "loss": 1.7857,
      "step": 224300
    },
    {
      "epoch": 17.14154762814147,
      "grad_norm": 4.533192157745361,
      "learning_rate": 3.571537697654878e-05,
      "loss": 1.792,
      "step": 224400
    },
    {
      "epoch": 17.14918646398289,
      "grad_norm": 6.12370491027832,
      "learning_rate": 3.570901128001426e-05,
      "loss": 1.7997,
      "step": 224500
    },
    {
      "epoch": 17.156825299824305,
      "grad_norm": 5.729208946228027,
      "learning_rate": 3.5702645583479746e-05,
      "loss": 1.8035,
      "step": 224600
    },
    {
      "epoch": 17.164464135665725,
      "grad_norm": 4.10065221786499,
      "learning_rate": 3.5696279886945236e-05,
      "loss": 1.7414,
      "step": 224700
    },
    {
      "epoch": 17.172102971507144,
      "grad_norm": 4.522495746612549,
      "learning_rate": 3.568991419041072e-05,
      "loss": 1.7812,
      "step": 224800
    },
    {
      "epoch": 17.17974180734856,
      "grad_norm": 7.039263725280762,
      "learning_rate": 3.56835484938762e-05,
      "loss": 1.773,
      "step": 224900
    },
    {
      "epoch": 17.18738064318998,
      "grad_norm": 6.116881370544434,
      "learning_rate": 3.567718279734169e-05,
      "loss": 1.8211,
      "step": 225000
    },
    {
      "epoch": 17.195019479031394,
      "grad_norm": 7.262289524078369,
      "learning_rate": 3.567081710080718e-05,
      "loss": 1.7179,
      "step": 225100
    },
    {
      "epoch": 17.202658314872814,
      "grad_norm": 5.291660308837891,
      "learning_rate": 3.566445140427266e-05,
      "loss": 1.8214,
      "step": 225200
    },
    {
      "epoch": 17.210297150714233,
      "grad_norm": 4.657587051391602,
      "learning_rate": 3.5658085707738144e-05,
      "loss": 1.8357,
      "step": 225300
    },
    {
      "epoch": 17.21793598655565,
      "grad_norm": 6.882526397705078,
      "learning_rate": 3.565172001120363e-05,
      "loss": 1.7081,
      "step": 225400
    },
    {
      "epoch": 17.225574822397068,
      "grad_norm": 4.617794036865234,
      "learning_rate": 3.564535431466911e-05,
      "loss": 1.762,
      "step": 225500
    },
    {
      "epoch": 17.233213658238483,
      "grad_norm": 5.425500869750977,
      "learning_rate": 3.56389886181346e-05,
      "loss": 1.7663,
      "step": 225600
    },
    {
      "epoch": 17.240852494079903,
      "grad_norm": 5.492351055145264,
      "learning_rate": 3.5632622921600085e-05,
      "loss": 1.7451,
      "step": 225700
    },
    {
      "epoch": 17.24849132992132,
      "grad_norm": 5.644015789031982,
      "learning_rate": 3.562625722506557e-05,
      "loss": 1.7468,
      "step": 225800
    },
    {
      "epoch": 17.256130165762738,
      "grad_norm": 5.845173358917236,
      "learning_rate": 3.561989152853105e-05,
      "loss": 1.8291,
      "step": 225900
    },
    {
      "epoch": 17.263769001604157,
      "grad_norm": 5.008636474609375,
      "learning_rate": 3.561352583199654e-05,
      "loss": 1.7562,
      "step": 226000
    },
    {
      "epoch": 17.271407837445572,
      "grad_norm": 5.9178547859191895,
      "learning_rate": 3.5607160135462026e-05,
      "loss": 1.7938,
      "step": 226100
    },
    {
      "epoch": 17.27904667328699,
      "grad_norm": 5.76621150970459,
      "learning_rate": 3.560079443892751e-05,
      "loss": 1.7954,
      "step": 226200
    },
    {
      "epoch": 17.286685509128407,
      "grad_norm": 6.167857646942139,
      "learning_rate": 3.559442874239299e-05,
      "loss": 1.7434,
      "step": 226300
    },
    {
      "epoch": 17.294324344969827,
      "grad_norm": 7.040863990783691,
      "learning_rate": 3.558806304585848e-05,
      "loss": 1.7642,
      "step": 226400
    },
    {
      "epoch": 17.301963180811246,
      "grad_norm": 5.053186893463135,
      "learning_rate": 3.558169734932397e-05,
      "loss": 1.7745,
      "step": 226500
    },
    {
      "epoch": 17.30960201665266,
      "grad_norm": 5.187387943267822,
      "learning_rate": 3.557533165278945e-05,
      "loss": 1.7981,
      "step": 226600
    },
    {
      "epoch": 17.31724085249408,
      "grad_norm": 7.234341621398926,
      "learning_rate": 3.5568965956254934e-05,
      "loss": 1.8147,
      "step": 226700
    },
    {
      "epoch": 17.324879688335496,
      "grad_norm": 6.334123134613037,
      "learning_rate": 3.556260025972042e-05,
      "loss": 1.8221,
      "step": 226800
    },
    {
      "epoch": 17.332518524176916,
      "grad_norm": 4.833451271057129,
      "learning_rate": 3.55562345631859e-05,
      "loss": 1.7891,
      "step": 226900
    },
    {
      "epoch": 17.340157360018335,
      "grad_norm": 5.140151023864746,
      "learning_rate": 3.554986886665139e-05,
      "loss": 1.8173,
      "step": 227000
    },
    {
      "epoch": 17.34779619585975,
      "grad_norm": 4.456250190734863,
      "learning_rate": 3.5543503170116875e-05,
      "loss": 1.7797,
      "step": 227100
    },
    {
      "epoch": 17.35543503170117,
      "grad_norm": 5.302767276763916,
      "learning_rate": 3.553713747358236e-05,
      "loss": 1.7997,
      "step": 227200
    },
    {
      "epoch": 17.363073867542585,
      "grad_norm": 5.588650226593018,
      "learning_rate": 3.553077177704784e-05,
      "loss": 1.8166,
      "step": 227300
    },
    {
      "epoch": 17.370712703384005,
      "grad_norm": 6.133257865905762,
      "learning_rate": 3.552440608051333e-05,
      "loss": 1.7948,
      "step": 227400
    },
    {
      "epoch": 17.378351539225424,
      "grad_norm": 6.10227108001709,
      "learning_rate": 3.5518040383978816e-05,
      "loss": 1.734,
      "step": 227500
    },
    {
      "epoch": 17.38599037506684,
      "grad_norm": 5.596081733703613,
      "learning_rate": 3.55116746874443e-05,
      "loss": 1.849,
      "step": 227600
    },
    {
      "epoch": 17.39362921090826,
      "grad_norm": 5.5310258865356445,
      "learning_rate": 3.550530899090978e-05,
      "loss": 1.8296,
      "step": 227700
    },
    {
      "epoch": 17.401268046749674,
      "grad_norm": 6.3030171394348145,
      "learning_rate": 3.5498943294375267e-05,
      "loss": 1.7037,
      "step": 227800
    },
    {
      "epoch": 17.408906882591094,
      "grad_norm": 5.516815662384033,
      "learning_rate": 3.549257759784076e-05,
      "loss": 1.7852,
      "step": 227900
    },
    {
      "epoch": 17.41654571843251,
      "grad_norm": 5.975398063659668,
      "learning_rate": 3.548621190130624e-05,
      "loss": 1.8633,
      "step": 228000
    },
    {
      "epoch": 17.42418455427393,
      "grad_norm": 8.11268424987793,
      "learning_rate": 3.5479846204771724e-05,
      "loss": 1.8262,
      "step": 228100
    },
    {
      "epoch": 17.431823390115348,
      "grad_norm": 4.646697521209717,
      "learning_rate": 3.5473480508237214e-05,
      "loss": 1.8217,
      "step": 228200
    },
    {
      "epoch": 17.439462225956763,
      "grad_norm": 4.631312370300293,
      "learning_rate": 3.54671148117027e-05,
      "loss": 1.7235,
      "step": 228300
    },
    {
      "epoch": 17.447101061798183,
      "grad_norm": 4.169549942016602,
      "learning_rate": 3.546074911516819e-05,
      "loss": 1.7938,
      "step": 228400
    },
    {
      "epoch": 17.454739897639598,
      "grad_norm": 3.980212926864624,
      "learning_rate": 3.545438341863367e-05,
      "loss": 1.7259,
      "step": 228500
    },
    {
      "epoch": 17.462378733481017,
      "grad_norm": 4.632042407989502,
      "learning_rate": 3.5448017722099155e-05,
      "loss": 1.7405,
      "step": 228600
    },
    {
      "epoch": 17.470017569322437,
      "grad_norm": 4.710415363311768,
      "learning_rate": 3.544165202556464e-05,
      "loss": 1.7762,
      "step": 228700
    },
    {
      "epoch": 17.477656405163852,
      "grad_norm": 4.526761531829834,
      "learning_rate": 3.543528632903013e-05,
      "loss": 1.8175,
      "step": 228800
    },
    {
      "epoch": 17.48529524100527,
      "grad_norm": 4.147746562957764,
      "learning_rate": 3.542892063249561e-05,
      "loss": 1.7729,
      "step": 228900
    },
    {
      "epoch": 17.492934076846687,
      "grad_norm": 7.664320945739746,
      "learning_rate": 3.5422554935961096e-05,
      "loss": 1.7829,
      "step": 229000
    },
    {
      "epoch": 17.500572912688106,
      "grad_norm": 4.3108744621276855,
      "learning_rate": 3.541618923942658e-05,
      "loss": 1.8161,
      "step": 229100
    },
    {
      "epoch": 17.508211748529526,
      "grad_norm": 5.2262043952941895,
      "learning_rate": 3.540982354289207e-05,
      "loss": 1.7249,
      "step": 229200
    },
    {
      "epoch": 17.51585058437094,
      "grad_norm": 2.6805732250213623,
      "learning_rate": 3.5403457846357553e-05,
      "loss": 1.7336,
      "step": 229300
    },
    {
      "epoch": 17.52348942021236,
      "grad_norm": 5.699448585510254,
      "learning_rate": 3.539709214982304e-05,
      "loss": 1.8424,
      "step": 229400
    },
    {
      "epoch": 17.531128256053776,
      "grad_norm": 5.434723377227783,
      "learning_rate": 3.539072645328852e-05,
      "loss": 1.7964,
      "step": 229500
    },
    {
      "epoch": 17.538767091895195,
      "grad_norm": 6.514330863952637,
      "learning_rate": 3.5384360756754004e-05,
      "loss": 1.8798,
      "step": 229600
    },
    {
      "epoch": 17.546405927736615,
      "grad_norm": 3.9669525623321533,
      "learning_rate": 3.5377995060219494e-05,
      "loss": 1.7342,
      "step": 229700
    },
    {
      "epoch": 17.55404476357803,
      "grad_norm": 4.7098493576049805,
      "learning_rate": 3.537162936368498e-05,
      "loss": 1.7812,
      "step": 229800
    },
    {
      "epoch": 17.56168359941945,
      "grad_norm": 5.317114353179932,
      "learning_rate": 3.536526366715046e-05,
      "loss": 1.7852,
      "step": 229900
    },
    {
      "epoch": 17.569322435260865,
      "grad_norm": 4.97015905380249,
      "learning_rate": 3.5358897970615945e-05,
      "loss": 1.7505,
      "step": 230000
    },
    {
      "epoch": 17.576961271102284,
      "grad_norm": 7.421313285827637,
      "learning_rate": 3.535253227408143e-05,
      "loss": 1.7683,
      "step": 230100
    },
    {
      "epoch": 17.5846001069437,
      "grad_norm": 5.6089301109313965,
      "learning_rate": 3.534616657754692e-05,
      "loss": 1.8126,
      "step": 230200
    },
    {
      "epoch": 17.59223894278512,
      "grad_norm": 5.512014389038086,
      "learning_rate": 3.53398008810124e-05,
      "loss": 1.7357,
      "step": 230300
    },
    {
      "epoch": 17.59987777862654,
      "grad_norm": 4.510561943054199,
      "learning_rate": 3.5333435184477886e-05,
      "loss": 1.7918,
      "step": 230400
    },
    {
      "epoch": 17.607516614467954,
      "grad_norm": 3.8973288536071777,
      "learning_rate": 3.532706948794337e-05,
      "loss": 1.9269,
      "step": 230500
    },
    {
      "epoch": 17.615155450309373,
      "grad_norm": 6.229835510253906,
      "learning_rate": 3.532070379140886e-05,
      "loss": 1.7847,
      "step": 230600
    },
    {
      "epoch": 17.62279428615079,
      "grad_norm": 4.164919853210449,
      "learning_rate": 3.531433809487434e-05,
      "loss": 1.8441,
      "step": 230700
    },
    {
      "epoch": 17.63043312199221,
      "grad_norm": 6.75883150100708,
      "learning_rate": 3.530797239833983e-05,
      "loss": 1.7758,
      "step": 230800
    },
    {
      "epoch": 17.638071957833628,
      "grad_norm": 5.650472640991211,
      "learning_rate": 3.530160670180531e-05,
      "loss": 1.7833,
      "step": 230900
    },
    {
      "epoch": 17.645710793675043,
      "grad_norm": 4.948305606842041,
      "learning_rate": 3.5295241005270794e-05,
      "loss": 1.7654,
      "step": 231000
    },
    {
      "epoch": 17.653349629516462,
      "grad_norm": 5.916745662689209,
      "learning_rate": 3.5288875308736284e-05,
      "loss": 1.8402,
      "step": 231100
    },
    {
      "epoch": 17.660988465357878,
      "grad_norm": 8.243706703186035,
      "learning_rate": 3.528250961220177e-05,
      "loss": 1.8569,
      "step": 231200
    },
    {
      "epoch": 17.668627301199297,
      "grad_norm": 5.373175621032715,
      "learning_rate": 3.527614391566725e-05,
      "loss": 1.8352,
      "step": 231300
    },
    {
      "epoch": 17.676266137040717,
      "grad_norm": 5.594158172607422,
      "learning_rate": 3.5269778219132735e-05,
      "loss": 1.8097,
      "step": 231400
    },
    {
      "epoch": 17.683904972882132,
      "grad_norm": 6.134895324707031,
      "learning_rate": 3.5263412522598225e-05,
      "loss": 1.7529,
      "step": 231500
    },
    {
      "epoch": 17.69154380872355,
      "grad_norm": 4.594460964202881,
      "learning_rate": 3.525704682606371e-05,
      "loss": 1.7946,
      "step": 231600
    },
    {
      "epoch": 17.699182644564967,
      "grad_norm": 7.394469738006592,
      "learning_rate": 3.525068112952919e-05,
      "loss": 1.8077,
      "step": 231700
    },
    {
      "epoch": 17.706821480406386,
      "grad_norm": 5.380090236663818,
      "learning_rate": 3.5244315432994676e-05,
      "loss": 1.9073,
      "step": 231800
    },
    {
      "epoch": 17.714460316247802,
      "grad_norm": 5.681201934814453,
      "learning_rate": 3.5237949736460166e-05,
      "loss": 1.7487,
      "step": 231900
    },
    {
      "epoch": 17.72209915208922,
      "grad_norm": 4.650277614593506,
      "learning_rate": 3.523158403992565e-05,
      "loss": 1.8062,
      "step": 232000
    },
    {
      "epoch": 17.72973798793064,
      "grad_norm": 4.76792573928833,
      "learning_rate": 3.522521834339113e-05,
      "loss": 1.8161,
      "step": 232100
    },
    {
      "epoch": 17.737376823772056,
      "grad_norm": 5.41840124130249,
      "learning_rate": 3.5218852646856623e-05,
      "loss": 1.8338,
      "step": 232200
    },
    {
      "epoch": 17.745015659613475,
      "grad_norm": 4.394642353057861,
      "learning_rate": 3.521248695032211e-05,
      "loss": 1.8326,
      "step": 232300
    },
    {
      "epoch": 17.75265449545489,
      "grad_norm": 6.671281814575195,
      "learning_rate": 3.520612125378759e-05,
      "loss": 1.8974,
      "step": 232400
    },
    {
      "epoch": 17.76029333129631,
      "grad_norm": 3.660806894302368,
      "learning_rate": 3.519975555725308e-05,
      "loss": 1.8583,
      "step": 232500
    },
    {
      "epoch": 17.76793216713773,
      "grad_norm": 5.374760627746582,
      "learning_rate": 3.5193389860718564e-05,
      "loss": 1.8173,
      "step": 232600
    },
    {
      "epoch": 17.775571002979145,
      "grad_norm": 5.9194746017456055,
      "learning_rate": 3.518702416418405e-05,
      "loss": 1.7815,
      "step": 232700
    },
    {
      "epoch": 17.783209838820564,
      "grad_norm": 5.139471054077148,
      "learning_rate": 3.518065846764953e-05,
      "loss": 1.7888,
      "step": 232800
    },
    {
      "epoch": 17.79084867466198,
      "grad_norm": 4.157825469970703,
      "learning_rate": 3.517429277111502e-05,
      "loss": 1.8148,
      "step": 232900
    },
    {
      "epoch": 17.7984875105034,
      "grad_norm": 5.151037693023682,
      "learning_rate": 3.5167927074580505e-05,
      "loss": 1.9196,
      "step": 233000
    },
    {
      "epoch": 17.80612634634482,
      "grad_norm": 5.274181842803955,
      "learning_rate": 3.516156137804599e-05,
      "loss": 1.7416,
      "step": 233100
    },
    {
      "epoch": 17.813765182186234,
      "grad_norm": 7.596693515777588,
      "learning_rate": 3.515519568151147e-05,
      "loss": 1.7556,
      "step": 233200
    },
    {
      "epoch": 17.821404018027653,
      "grad_norm": 5.8674821853637695,
      "learning_rate": 3.5148829984976956e-05,
      "loss": 1.7872,
      "step": 233300
    },
    {
      "epoch": 17.82904285386907,
      "grad_norm": 5.949047088623047,
      "learning_rate": 3.5142464288442446e-05,
      "loss": 1.8424,
      "step": 233400
    },
    {
      "epoch": 17.83668168971049,
      "grad_norm": 7.33682107925415,
      "learning_rate": 3.513609859190793e-05,
      "loss": 1.8274,
      "step": 233500
    },
    {
      "epoch": 17.844320525551908,
      "grad_norm": 7.33632230758667,
      "learning_rate": 3.512973289537341e-05,
      "loss": 1.7485,
      "step": 233600
    },
    {
      "epoch": 17.851959361393323,
      "grad_norm": 5.723560810089111,
      "learning_rate": 3.51233671988389e-05,
      "loss": 1.804,
      "step": 233700
    },
    {
      "epoch": 17.859598197234742,
      "grad_norm": 3.2741007804870605,
      "learning_rate": 3.511700150230439e-05,
      "loss": 1.769,
      "step": 233800
    },
    {
      "epoch": 17.867237033076158,
      "grad_norm": 5.705412864685059,
      "learning_rate": 3.511063580576987e-05,
      "loss": 1.7267,
      "step": 233900
    },
    {
      "epoch": 17.874875868917577,
      "grad_norm": 7.936476707458496,
      "learning_rate": 3.5104270109235354e-05,
      "loss": 1.7798,
      "step": 234000
    },
    {
      "epoch": 17.882514704758997,
      "grad_norm": 6.277868270874023,
      "learning_rate": 3.509790441270084e-05,
      "loss": 1.7708,
      "step": 234100
    },
    {
      "epoch": 17.890153540600412,
      "grad_norm": 6.526451110839844,
      "learning_rate": 3.509153871616632e-05,
      "loss": 1.9158,
      "step": 234200
    },
    {
      "epoch": 17.89779237644183,
      "grad_norm": 5.49283504486084,
      "learning_rate": 3.508517301963181e-05,
      "loss": 1.7716,
      "step": 234300
    },
    {
      "epoch": 17.905431212283247,
      "grad_norm": 4.950990200042725,
      "learning_rate": 3.5078807323097295e-05,
      "loss": 1.8694,
      "step": 234400
    },
    {
      "epoch": 17.913070048124666,
      "grad_norm": 4.799422264099121,
      "learning_rate": 3.507244162656278e-05,
      "loss": 1.7035,
      "step": 234500
    },
    {
      "epoch": 17.920708883966082,
      "grad_norm": 3.6858863830566406,
      "learning_rate": 3.506607593002826e-05,
      "loss": 1.7608,
      "step": 234600
    },
    {
      "epoch": 17.9283477198075,
      "grad_norm": 5.42033576965332,
      "learning_rate": 3.505971023349375e-05,
      "loss": 1.7734,
      "step": 234700
    },
    {
      "epoch": 17.93598655564892,
      "grad_norm": 4.868868827819824,
      "learning_rate": 3.5053344536959236e-05,
      "loss": 1.7918,
      "step": 234800
    },
    {
      "epoch": 17.943625391490336,
      "grad_norm": 4.71698522567749,
      "learning_rate": 3.504697884042472e-05,
      "loss": 1.8347,
      "step": 234900
    },
    {
      "epoch": 17.951264227331755,
      "grad_norm": 4.184746265411377,
      "learning_rate": 3.50406131438902e-05,
      "loss": 1.7982,
      "step": 235000
    },
    {
      "epoch": 17.95890306317317,
      "grad_norm": 5.350677013397217,
      "learning_rate": 3.503424744735569e-05,
      "loss": 1.819,
      "step": 235100
    },
    {
      "epoch": 17.96654189901459,
      "grad_norm": 4.704218864440918,
      "learning_rate": 3.502788175082118e-05,
      "loss": 1.8828,
      "step": 235200
    },
    {
      "epoch": 17.97418073485601,
      "grad_norm": 5.6900248527526855,
      "learning_rate": 3.502151605428666e-05,
      "loss": 1.8578,
      "step": 235300
    },
    {
      "epoch": 17.981819570697425,
      "grad_norm": 4.533700466156006,
      "learning_rate": 3.5015150357752144e-05,
      "loss": 1.7363,
      "step": 235400
    },
    {
      "epoch": 17.989458406538844,
      "grad_norm": 6.779026508331299,
      "learning_rate": 3.500878466121763e-05,
      "loss": 1.8151,
      "step": 235500
    },
    {
      "epoch": 17.99709724238026,
      "grad_norm": 6.680508136749268,
      "learning_rate": 3.500241896468312e-05,
      "loss": 1.8569,
      "step": 235600
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.8084125518798828,
      "eval_runtime": 3.0511,
      "eval_samples_per_second": 226.148,
      "eval_steps_per_second": 226.148,
      "step": 235638
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.5549949407577515,
      "eval_runtime": 53.4725,
      "eval_samples_per_second": 244.817,
      "eval_steps_per_second": 244.817,
      "step": 235638
    },
    {
      "epoch": 18.00473607822168,
      "grad_norm": 6.079131126403809,
      "learning_rate": 3.49960532681486e-05,
      "loss": 1.6833,
      "step": 235700
    },
    {
      "epoch": 18.0123749140631,
      "grad_norm": 4.730120658874512,
      "learning_rate": 3.4989687571614085e-05,
      "loss": 1.8649,
      "step": 235800
    },
    {
      "epoch": 18.020013749904514,
      "grad_norm": 5.850187301635742,
      "learning_rate": 3.4983321875079575e-05,
      "loss": 1.79,
      "step": 235900
    },
    {
      "epoch": 18.027652585745933,
      "grad_norm": 6.98518705368042,
      "learning_rate": 3.497695617854506e-05,
      "loss": 1.814,
      "step": 236000
    },
    {
      "epoch": 18.03529142158735,
      "grad_norm": 5.298699855804443,
      "learning_rate": 3.497059048201054e-05,
      "loss": 1.8498,
      "step": 236100
    },
    {
      "epoch": 18.042930257428768,
      "grad_norm": 6.789780139923096,
      "learning_rate": 3.496422478547603e-05,
      "loss": 1.7138,
      "step": 236200
    },
    {
      "epoch": 18.050569093270184,
      "grad_norm": 5.483675956726074,
      "learning_rate": 3.4957859088941516e-05,
      "loss": 1.8042,
      "step": 236300
    },
    {
      "epoch": 18.058207929111603,
      "grad_norm": 5.0924153327941895,
      "learning_rate": 3.4951493392407e-05,
      "loss": 1.7484,
      "step": 236400
    },
    {
      "epoch": 18.065846764953022,
      "grad_norm": 6.208898544311523,
      "learning_rate": 3.494512769587248e-05,
      "loss": 1.7596,
      "step": 236500
    },
    {
      "epoch": 18.073485600794438,
      "grad_norm": 4.973999500274658,
      "learning_rate": 3.4938761999337974e-05,
      "loss": 1.6648,
      "step": 236600
    },
    {
      "epoch": 18.081124436635857,
      "grad_norm": 5.56007194519043,
      "learning_rate": 3.493239630280346e-05,
      "loss": 1.7574,
      "step": 236700
    },
    {
      "epoch": 18.088763272477273,
      "grad_norm": 4.377530574798584,
      "learning_rate": 3.492603060626894e-05,
      "loss": 1.8584,
      "step": 236800
    },
    {
      "epoch": 18.096402108318692,
      "grad_norm": 5.250387668609619,
      "learning_rate": 3.4919664909734424e-05,
      "loss": 1.735,
      "step": 236900
    },
    {
      "epoch": 18.10404094416011,
      "grad_norm": 4.836373329162598,
      "learning_rate": 3.4913299213199915e-05,
      "loss": 1.8117,
      "step": 237000
    },
    {
      "epoch": 18.111679780001527,
      "grad_norm": 5.084224700927734,
      "learning_rate": 3.49069335166654e-05,
      "loss": 1.7955,
      "step": 237100
    },
    {
      "epoch": 18.119318615842946,
      "grad_norm": 5.452240943908691,
      "learning_rate": 3.490056782013088e-05,
      "loss": 1.7718,
      "step": 237200
    },
    {
      "epoch": 18.126957451684362,
      "grad_norm": 5.065482139587402,
      "learning_rate": 3.4894202123596365e-05,
      "loss": 1.7148,
      "step": 237300
    },
    {
      "epoch": 18.13459628752578,
      "grad_norm": 8.969667434692383,
      "learning_rate": 3.488783642706185e-05,
      "loss": 1.6653,
      "step": 237400
    },
    {
      "epoch": 18.1422351233672,
      "grad_norm": 5.605371475219727,
      "learning_rate": 3.488147073052734e-05,
      "loss": 1.7727,
      "step": 237500
    },
    {
      "epoch": 18.149873959208616,
      "grad_norm": 4.391564846038818,
      "learning_rate": 3.487510503399282e-05,
      "loss": 1.7227,
      "step": 237600
    },
    {
      "epoch": 18.157512795050035,
      "grad_norm": 5.367001056671143,
      "learning_rate": 3.4868739337458306e-05,
      "loss": 1.7406,
      "step": 237700
    },
    {
      "epoch": 18.16515163089145,
      "grad_norm": 5.733941078186035,
      "learning_rate": 3.486237364092379e-05,
      "loss": 1.8419,
      "step": 237800
    },
    {
      "epoch": 18.17279046673287,
      "grad_norm": 6.584256172180176,
      "learning_rate": 3.485600794438927e-05,
      "loss": 1.8415,
      "step": 237900
    },
    {
      "epoch": 18.18042930257429,
      "grad_norm": 6.929720878601074,
      "learning_rate": 3.4849642247854763e-05,
      "loss": 1.7758,
      "step": 238000
    },
    {
      "epoch": 18.188068138415705,
      "grad_norm": 4.129446506500244,
      "learning_rate": 3.484327655132025e-05,
      "loss": 1.7296,
      "step": 238100
    },
    {
      "epoch": 18.195706974257124,
      "grad_norm": 5.180588245391846,
      "learning_rate": 3.483691085478573e-05,
      "loss": 1.7712,
      "step": 238200
    },
    {
      "epoch": 18.20334581009854,
      "grad_norm": 4.921306133270264,
      "learning_rate": 3.4830545158251214e-05,
      "loss": 1.8611,
      "step": 238300
    },
    {
      "epoch": 18.21098464593996,
      "grad_norm": 4.938034534454346,
      "learning_rate": 3.4824179461716704e-05,
      "loss": 1.7597,
      "step": 238400
    },
    {
      "epoch": 18.218623481781375,
      "grad_norm": 4.7085747718811035,
      "learning_rate": 3.481781376518219e-05,
      "loss": 1.7637,
      "step": 238500
    },
    {
      "epoch": 18.226262317622794,
      "grad_norm": 6.466650485992432,
      "learning_rate": 3.481144806864767e-05,
      "loss": 1.8173,
      "step": 238600
    },
    {
      "epoch": 18.233901153464213,
      "grad_norm": 6.024188995361328,
      "learning_rate": 3.4805082372113155e-05,
      "loss": 1.8245,
      "step": 238700
    },
    {
      "epoch": 18.24153998930563,
      "grad_norm": 4.823842525482178,
      "learning_rate": 3.479871667557864e-05,
      "loss": 1.7017,
      "step": 238800
    },
    {
      "epoch": 18.249178825147048,
      "grad_norm": 5.033896446228027,
      "learning_rate": 3.479235097904413e-05,
      "loss": 1.8001,
      "step": 238900
    },
    {
      "epoch": 18.256817660988464,
      "grad_norm": 4.241402626037598,
      "learning_rate": 3.478598528250961e-05,
      "loss": 1.6988,
      "step": 239000
    },
    {
      "epoch": 18.264456496829883,
      "grad_norm": 4.7537126541137695,
      "learning_rate": 3.4779619585975096e-05,
      "loss": 1.8327,
      "step": 239100
    },
    {
      "epoch": 18.272095332671302,
      "grad_norm": 5.566995620727539,
      "learning_rate": 3.477325388944058e-05,
      "loss": 1.8019,
      "step": 239200
    },
    {
      "epoch": 18.279734168512718,
      "grad_norm": 5.175579071044922,
      "learning_rate": 3.476688819290607e-05,
      "loss": 1.7675,
      "step": 239300
    },
    {
      "epoch": 18.287373004354137,
      "grad_norm": 6.724055767059326,
      "learning_rate": 3.476052249637155e-05,
      "loss": 1.819,
      "step": 239400
    },
    {
      "epoch": 18.295011840195553,
      "grad_norm": 4.955374240875244,
      "learning_rate": 3.475415679983704e-05,
      "loss": 1.8862,
      "step": 239500
    },
    {
      "epoch": 18.302650676036972,
      "grad_norm": 5.729724407196045,
      "learning_rate": 3.474779110330253e-05,
      "loss": 1.7054,
      "step": 239600
    },
    {
      "epoch": 18.31028951187839,
      "grad_norm": 5.078611850738525,
      "learning_rate": 3.474142540676801e-05,
      "loss": 1.8012,
      "step": 239700
    },
    {
      "epoch": 18.317928347719807,
      "grad_norm": 4.1910529136657715,
      "learning_rate": 3.4735059710233494e-05,
      "loss": 1.8349,
      "step": 239800
    },
    {
      "epoch": 18.325567183561226,
      "grad_norm": 5.841139316558838,
      "learning_rate": 3.4728694013698985e-05,
      "loss": 1.7705,
      "step": 239900
    },
    {
      "epoch": 18.333206019402642,
      "grad_norm": 5.260806560516357,
      "learning_rate": 3.472232831716447e-05,
      "loss": 1.779,
      "step": 240000
    },
    {
      "epoch": 18.34084485524406,
      "grad_norm": 4.432966232299805,
      "learning_rate": 3.471596262062995e-05,
      "loss": 1.8058,
      "step": 240100
    },
    {
      "epoch": 18.34848369108548,
      "grad_norm": 6.370722770690918,
      "learning_rate": 3.470959692409544e-05,
      "loss": 1.7406,
      "step": 240200
    },
    {
      "epoch": 18.356122526926896,
      "grad_norm": 5.4333815574646,
      "learning_rate": 3.4703231227560925e-05,
      "loss": 1.8172,
      "step": 240300
    },
    {
      "epoch": 18.363761362768315,
      "grad_norm": 5.6797566413879395,
      "learning_rate": 3.469686553102641e-05,
      "loss": 1.8444,
      "step": 240400
    },
    {
      "epoch": 18.37140019860973,
      "grad_norm": 7.491749286651611,
      "learning_rate": 3.469049983449189e-05,
      "loss": 1.7501,
      "step": 240500
    },
    {
      "epoch": 18.37903903445115,
      "grad_norm": 4.648353099822998,
      "learning_rate": 3.4684134137957376e-05,
      "loss": 1.7627,
      "step": 240600
    },
    {
      "epoch": 18.386677870292566,
      "grad_norm": 5.233119010925293,
      "learning_rate": 3.4677768441422866e-05,
      "loss": 1.759,
      "step": 240700
    },
    {
      "epoch": 18.394316706133985,
      "grad_norm": 5.787045001983643,
      "learning_rate": 3.467140274488835e-05,
      "loss": 1.7736,
      "step": 240800
    },
    {
      "epoch": 18.401955541975404,
      "grad_norm": 4.500095367431641,
      "learning_rate": 3.4665037048353833e-05,
      "loss": 1.8065,
      "step": 240900
    },
    {
      "epoch": 18.40959437781682,
      "grad_norm": 6.476553440093994,
      "learning_rate": 3.465867135181932e-05,
      "loss": 1.8425,
      "step": 241000
    },
    {
      "epoch": 18.41723321365824,
      "grad_norm": 5.719386100769043,
      "learning_rate": 3.46523056552848e-05,
      "loss": 1.7931,
      "step": 241100
    },
    {
      "epoch": 18.424872049499655,
      "grad_norm": 5.2783379554748535,
      "learning_rate": 3.464593995875029e-05,
      "loss": 1.7083,
      "step": 241200
    },
    {
      "epoch": 18.432510885341074,
      "grad_norm": 5.427654266357422,
      "learning_rate": 3.4639574262215774e-05,
      "loss": 1.7686,
      "step": 241300
    },
    {
      "epoch": 18.440149721182493,
      "grad_norm": 5.670726776123047,
      "learning_rate": 3.463320856568126e-05,
      "loss": 1.7808,
      "step": 241400
    },
    {
      "epoch": 18.44778855702391,
      "grad_norm": 5.300136566162109,
      "learning_rate": 3.462684286914674e-05,
      "loss": 1.7544,
      "step": 241500
    },
    {
      "epoch": 18.455427392865328,
      "grad_norm": 4.241015434265137,
      "learning_rate": 3.462047717261223e-05,
      "loss": 1.7099,
      "step": 241600
    },
    {
      "epoch": 18.463066228706744,
      "grad_norm": 4.628862380981445,
      "learning_rate": 3.4614111476077715e-05,
      "loss": 1.803,
      "step": 241700
    },
    {
      "epoch": 18.470705064548163,
      "grad_norm": 5.597343921661377,
      "learning_rate": 3.46077457795432e-05,
      "loss": 1.6733,
      "step": 241800
    },
    {
      "epoch": 18.478343900389582,
      "grad_norm": 5.603865623474121,
      "learning_rate": 3.460138008300868e-05,
      "loss": 1.7202,
      "step": 241900
    },
    {
      "epoch": 18.485982736230998,
      "grad_norm": 6.913980484008789,
      "learning_rate": 3.4595014386474166e-05,
      "loss": 1.8033,
      "step": 242000
    },
    {
      "epoch": 18.493621572072417,
      "grad_norm": 6.517436981201172,
      "learning_rate": 3.4588648689939656e-05,
      "loss": 1.8468,
      "step": 242100
    },
    {
      "epoch": 18.501260407913833,
      "grad_norm": 3.715691089630127,
      "learning_rate": 3.458228299340514e-05,
      "loss": 1.8017,
      "step": 242200
    },
    {
      "epoch": 18.508899243755252,
      "grad_norm": 3.5050857067108154,
      "learning_rate": 3.457591729687062e-05,
      "loss": 1.8555,
      "step": 242300
    },
    {
      "epoch": 18.51653807959667,
      "grad_norm": 4.204541206359863,
      "learning_rate": 3.456955160033611e-05,
      "loss": 1.8239,
      "step": 242400
    },
    {
      "epoch": 18.524176915438087,
      "grad_norm": 5.340000152587891,
      "learning_rate": 3.45631859038016e-05,
      "loss": 1.7037,
      "step": 242500
    },
    {
      "epoch": 18.531815751279506,
      "grad_norm": 4.798318386077881,
      "learning_rate": 3.455682020726708e-05,
      "loss": 1.8476,
      "step": 242600
    },
    {
      "epoch": 18.53945458712092,
      "grad_norm": 6.095418453216553,
      "learning_rate": 3.4550454510732564e-05,
      "loss": 1.8085,
      "step": 242700
    },
    {
      "epoch": 18.54709342296234,
      "grad_norm": 5.151298522949219,
      "learning_rate": 3.454408881419805e-05,
      "loss": 1.7721,
      "step": 242800
    },
    {
      "epoch": 18.554732258803757,
      "grad_norm": 5.711555480957031,
      "learning_rate": 3.453772311766353e-05,
      "loss": 1.7872,
      "step": 242900
    },
    {
      "epoch": 18.562371094645176,
      "grad_norm": 5.496012210845947,
      "learning_rate": 3.453135742112902e-05,
      "loss": 1.8621,
      "step": 243000
    },
    {
      "epoch": 18.570009930486595,
      "grad_norm": 3.8634965419769287,
      "learning_rate": 3.4524991724594505e-05,
      "loss": 1.7892,
      "step": 243100
    },
    {
      "epoch": 18.57764876632801,
      "grad_norm": 5.014801025390625,
      "learning_rate": 3.451862602805999e-05,
      "loss": 1.8427,
      "step": 243200
    },
    {
      "epoch": 18.58528760216943,
      "grad_norm": 5.3952460289001465,
      "learning_rate": 3.451226033152547e-05,
      "loss": 1.935,
      "step": 243300
    },
    {
      "epoch": 18.592926438010846,
      "grad_norm": 7.735008239746094,
      "learning_rate": 3.450589463499096e-05,
      "loss": 1.766,
      "step": 243400
    },
    {
      "epoch": 18.600565273852265,
      "grad_norm": 4.641214370727539,
      "learning_rate": 3.4499528938456446e-05,
      "loss": 1.8007,
      "step": 243500
    },
    {
      "epoch": 18.608204109693684,
      "grad_norm": 4.622593879699707,
      "learning_rate": 3.4493163241921936e-05,
      "loss": 1.8742,
      "step": 243600
    },
    {
      "epoch": 18.6158429455351,
      "grad_norm": 6.158785343170166,
      "learning_rate": 3.448679754538742e-05,
      "loss": 1.711,
      "step": 243700
    },
    {
      "epoch": 18.62348178137652,
      "grad_norm": 9.043652534484863,
      "learning_rate": 3.4480431848852903e-05,
      "loss": 1.8002,
      "step": 243800
    },
    {
      "epoch": 18.631120617217935,
      "grad_norm": 4.629923343658447,
      "learning_rate": 3.4474066152318394e-05,
      "loss": 1.7785,
      "step": 243900
    },
    {
      "epoch": 18.638759453059354,
      "grad_norm": 4.217726230621338,
      "learning_rate": 3.446770045578388e-05,
      "loss": 1.7691,
      "step": 244000
    },
    {
      "epoch": 18.646398288900773,
      "grad_norm": 3.452016592025757,
      "learning_rate": 3.446133475924936e-05,
      "loss": 1.8026,
      "step": 244100
    },
    {
      "epoch": 18.65403712474219,
      "grad_norm": 5.246174335479736,
      "learning_rate": 3.4454969062714844e-05,
      "loss": 1.7425,
      "step": 244200
    },
    {
      "epoch": 18.661675960583608,
      "grad_norm": 4.343186378479004,
      "learning_rate": 3.444860336618033e-05,
      "loss": 1.8896,
      "step": 244300
    },
    {
      "epoch": 18.669314796425024,
      "grad_norm": 5.409232139587402,
      "learning_rate": 3.444223766964582e-05,
      "loss": 1.809,
      "step": 244400
    },
    {
      "epoch": 18.676953632266443,
      "grad_norm": 5.810929775238037,
      "learning_rate": 3.44358719731113e-05,
      "loss": 1.781,
      "step": 244500
    },
    {
      "epoch": 18.68459246810786,
      "grad_norm": 5.102533340454102,
      "learning_rate": 3.4429506276576785e-05,
      "loss": 1.7008,
      "step": 244600
    },
    {
      "epoch": 18.692231303949278,
      "grad_norm": 5.277497291564941,
      "learning_rate": 3.442314058004227e-05,
      "loss": 1.8073,
      "step": 244700
    },
    {
      "epoch": 18.699870139790697,
      "grad_norm": 4.600644111633301,
      "learning_rate": 3.441677488350776e-05,
      "loss": 1.7218,
      "step": 244800
    },
    {
      "epoch": 18.707508975632113,
      "grad_norm": 4.787283420562744,
      "learning_rate": 3.441040918697324e-05,
      "loss": 1.837,
      "step": 244900
    },
    {
      "epoch": 18.715147811473532,
      "grad_norm": 4.978477954864502,
      "learning_rate": 3.4404043490438726e-05,
      "loss": 1.8121,
      "step": 245000
    },
    {
      "epoch": 18.722786647314948,
      "grad_norm": 4.3326897621154785,
      "learning_rate": 3.439767779390421e-05,
      "loss": 1.7357,
      "step": 245100
    },
    {
      "epoch": 18.730425483156367,
      "grad_norm": 5.903501987457275,
      "learning_rate": 3.439131209736969e-05,
      "loss": 1.761,
      "step": 245200
    },
    {
      "epoch": 18.738064318997786,
      "grad_norm": 11.951603889465332,
      "learning_rate": 3.4384946400835184e-05,
      "loss": 1.8139,
      "step": 245300
    },
    {
      "epoch": 18.7457031548392,
      "grad_norm": 5.038351058959961,
      "learning_rate": 3.437858070430067e-05,
      "loss": 1.8332,
      "step": 245400
    },
    {
      "epoch": 18.75334199068062,
      "grad_norm": 6.018987655639648,
      "learning_rate": 3.437221500776615e-05,
      "loss": 1.7494,
      "step": 245500
    },
    {
      "epoch": 18.760980826522037,
      "grad_norm": 6.004837512969971,
      "learning_rate": 3.4365849311231634e-05,
      "loss": 1.7995,
      "step": 245600
    },
    {
      "epoch": 18.768619662363456,
      "grad_norm": 6.279009819030762,
      "learning_rate": 3.4359483614697125e-05,
      "loss": 1.787,
      "step": 245700
    },
    {
      "epoch": 18.776258498204875,
      "grad_norm": 4.344686508178711,
      "learning_rate": 3.435311791816261e-05,
      "loss": 1.7531,
      "step": 245800
    },
    {
      "epoch": 18.78389733404629,
      "grad_norm": 5.498958110809326,
      "learning_rate": 3.434675222162809e-05,
      "loss": 1.7365,
      "step": 245900
    },
    {
      "epoch": 18.79153616988771,
      "grad_norm": 5.770178318023682,
      "learning_rate": 3.4340386525093575e-05,
      "loss": 1.7881,
      "step": 246000
    },
    {
      "epoch": 18.799175005729126,
      "grad_norm": 5.016200542449951,
      "learning_rate": 3.433402082855906e-05,
      "loss": 1.6803,
      "step": 246100
    },
    {
      "epoch": 18.806813841570545,
      "grad_norm": 5.204094409942627,
      "learning_rate": 3.432765513202455e-05,
      "loss": 1.6369,
      "step": 246200
    },
    {
      "epoch": 18.814452677411964,
      "grad_norm": 4.806265830993652,
      "learning_rate": 3.432128943549003e-05,
      "loss": 1.796,
      "step": 246300
    },
    {
      "epoch": 18.82209151325338,
      "grad_norm": 6.115759372711182,
      "learning_rate": 3.4314923738955516e-05,
      "loss": 1.7777,
      "step": 246400
    },
    {
      "epoch": 18.8297303490948,
      "grad_norm": 4.644920349121094,
      "learning_rate": 3.4308558042421e-05,
      "loss": 1.7348,
      "step": 246500
    },
    {
      "epoch": 18.837369184936215,
      "grad_norm": 5.370680332183838,
      "learning_rate": 3.430219234588648e-05,
      "loss": 1.7671,
      "step": 246600
    },
    {
      "epoch": 18.845008020777634,
      "grad_norm": 6.038580417633057,
      "learning_rate": 3.4295826649351973e-05,
      "loss": 1.7124,
      "step": 246700
    },
    {
      "epoch": 18.852646856619053,
      "grad_norm": 5.071678638458252,
      "learning_rate": 3.428946095281746e-05,
      "loss": 1.7998,
      "step": 246800
    },
    {
      "epoch": 18.86028569246047,
      "grad_norm": 4.941584587097168,
      "learning_rate": 3.428309525628294e-05,
      "loss": 1.7821,
      "step": 246900
    },
    {
      "epoch": 18.867924528301888,
      "grad_norm": 5.248208999633789,
      "learning_rate": 3.4276729559748424e-05,
      "loss": 1.8398,
      "step": 247000
    },
    {
      "epoch": 18.875563364143304,
      "grad_norm": 5.573829174041748,
      "learning_rate": 3.4270363863213914e-05,
      "loss": 1.7499,
      "step": 247100
    },
    {
      "epoch": 18.883202199984723,
      "grad_norm": 3.0808258056640625,
      "learning_rate": 3.42639981666794e-05,
      "loss": 1.7607,
      "step": 247200
    },
    {
      "epoch": 18.89084103582614,
      "grad_norm": 5.403395652770996,
      "learning_rate": 3.425763247014488e-05,
      "loss": 1.8322,
      "step": 247300
    },
    {
      "epoch": 18.898479871667558,
      "grad_norm": 5.819021701812744,
      "learning_rate": 3.425126677361037e-05,
      "loss": 1.791,
      "step": 247400
    },
    {
      "epoch": 18.906118707508977,
      "grad_norm": 4.117214679718018,
      "learning_rate": 3.4244901077075855e-05,
      "loss": 1.7992,
      "step": 247500
    },
    {
      "epoch": 18.913757543350393,
      "grad_norm": 4.400209426879883,
      "learning_rate": 3.4238535380541346e-05,
      "loss": 1.7964,
      "step": 247600
    },
    {
      "epoch": 18.921396379191812,
      "grad_norm": 4.200484275817871,
      "learning_rate": 3.423216968400683e-05,
      "loss": 1.814,
      "step": 247700
    },
    {
      "epoch": 18.929035215033228,
      "grad_norm": 4.9154229164123535,
      "learning_rate": 3.422580398747231e-05,
      "loss": 1.8047,
      "step": 247800
    },
    {
      "epoch": 18.936674050874647,
      "grad_norm": 3.854813814163208,
      "learning_rate": 3.4219438290937796e-05,
      "loss": 1.7796,
      "step": 247900
    },
    {
      "epoch": 18.944312886716066,
      "grad_norm": 3.8880839347839355,
      "learning_rate": 3.4213072594403287e-05,
      "loss": 1.6968,
      "step": 248000
    },
    {
      "epoch": 18.95195172255748,
      "grad_norm": 4.354147911071777,
      "learning_rate": 3.420670689786877e-05,
      "loss": 1.8712,
      "step": 248100
    },
    {
      "epoch": 18.9595905583989,
      "grad_norm": 5.339828014373779,
      "learning_rate": 3.4200341201334254e-05,
      "loss": 1.8843,
      "step": 248200
    },
    {
      "epoch": 18.967229394240317,
      "grad_norm": 6.7035908699035645,
      "learning_rate": 3.419397550479974e-05,
      "loss": 1.8308,
      "step": 248300
    },
    {
      "epoch": 18.974868230081736,
      "grad_norm": 5.571484088897705,
      "learning_rate": 3.418760980826522e-05,
      "loss": 1.8976,
      "step": 248400
    },
    {
      "epoch": 18.982507065923155,
      "grad_norm": 4.348418235778809,
      "learning_rate": 3.418124411173071e-05,
      "loss": 1.7766,
      "step": 248500
    },
    {
      "epoch": 18.99014590176457,
      "grad_norm": 5.120179653167725,
      "learning_rate": 3.4174878415196195e-05,
      "loss": 1.7596,
      "step": 248600
    },
    {
      "epoch": 18.99778473760599,
      "grad_norm": 5.135763645172119,
      "learning_rate": 3.416851271866168e-05,
      "loss": 1.7399,
      "step": 248700
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.810680627822876,
      "eval_runtime": 3.2213,
      "eval_samples_per_second": 214.199,
      "eval_steps_per_second": 214.199,
      "step": 248729
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.549200415611267,
      "eval_runtime": 58.3664,
      "eval_samples_per_second": 224.29,
      "eval_steps_per_second": 224.29,
      "step": 248729
    },
    {
      "epoch": 19.005423573447406,
      "grad_norm": 4.605083465576172,
      "learning_rate": 3.416214702212716e-05,
      "loss": 1.724,
      "step": 248800
    },
    {
      "epoch": 19.013062409288825,
      "grad_norm": 5.146409034729004,
      "learning_rate": 3.415578132559265e-05,
      "loss": 1.8088,
      "step": 248900
    },
    {
      "epoch": 19.02070124513024,
      "grad_norm": 6.9593610763549805,
      "learning_rate": 3.4149415629058135e-05,
      "loss": 1.6867,
      "step": 249000
    },
    {
      "epoch": 19.02834008097166,
      "grad_norm": 5.877488136291504,
      "learning_rate": 3.414304993252362e-05,
      "loss": 1.7288,
      "step": 249100
    },
    {
      "epoch": 19.03597891681308,
      "grad_norm": 5.274502277374268,
      "learning_rate": 3.41366842359891e-05,
      "loss": 1.8071,
      "step": 249200
    },
    {
      "epoch": 19.043617752654495,
      "grad_norm": 6.464929103851318,
      "learning_rate": 3.4130318539454586e-05,
      "loss": 1.8465,
      "step": 249300
    },
    {
      "epoch": 19.051256588495914,
      "grad_norm": 5.044271469116211,
      "learning_rate": 3.4123952842920076e-05,
      "loss": 1.7708,
      "step": 249400
    },
    {
      "epoch": 19.05889542433733,
      "grad_norm": 4.123907089233398,
      "learning_rate": 3.411758714638556e-05,
      "loss": 1.7447,
      "step": 249500
    },
    {
      "epoch": 19.06653426017875,
      "grad_norm": 5.928465843200684,
      "learning_rate": 3.4111221449851043e-05,
      "loss": 1.7838,
      "step": 249600
    },
    {
      "epoch": 19.074173096020168,
      "grad_norm": 4.831732749938965,
      "learning_rate": 3.410485575331653e-05,
      "loss": 1.7742,
      "step": 249700
    },
    {
      "epoch": 19.081811931861584,
      "grad_norm": 5.690650939941406,
      "learning_rate": 3.409849005678201e-05,
      "loss": 1.7868,
      "step": 249800
    },
    {
      "epoch": 19.089450767703003,
      "grad_norm": 6.166746616363525,
      "learning_rate": 3.40921243602475e-05,
      "loss": 1.8356,
      "step": 249900
    },
    {
      "epoch": 19.09708960354442,
      "grad_norm": 6.500434398651123,
      "learning_rate": 3.4085758663712984e-05,
      "loss": 1.7736,
      "step": 250000
    },
    {
      "epoch": 19.104728439385838,
      "grad_norm": 5.674914360046387,
      "learning_rate": 3.407939296717847e-05,
      "loss": 1.681,
      "step": 250100
    },
    {
      "epoch": 19.112367275227257,
      "grad_norm": 5.087159156799316,
      "learning_rate": 3.407302727064395e-05,
      "loss": 1.8073,
      "step": 250200
    },
    {
      "epoch": 19.120006111068673,
      "grad_norm": 4.644565582275391,
      "learning_rate": 3.406666157410944e-05,
      "loss": 1.7682,
      "step": 250300
    },
    {
      "epoch": 19.12764494691009,
      "grad_norm": 5.269781112670898,
      "learning_rate": 3.4060295877574925e-05,
      "loss": 1.7631,
      "step": 250400
    },
    {
      "epoch": 19.135283782751507,
      "grad_norm": 5.777450084686279,
      "learning_rate": 3.405393018104041e-05,
      "loss": 1.8122,
      "step": 250500
    },
    {
      "epoch": 19.142922618592927,
      "grad_norm": 6.899005889892578,
      "learning_rate": 3.404756448450589e-05,
      "loss": 1.7506,
      "step": 250600
    },
    {
      "epoch": 19.150561454434346,
      "grad_norm": 6.545494079589844,
      "learning_rate": 3.4041198787971376e-05,
      "loss": 1.7146,
      "step": 250700
    },
    {
      "epoch": 19.15820029027576,
      "grad_norm": 4.965969085693359,
      "learning_rate": 3.4034833091436866e-05,
      "loss": 1.7319,
      "step": 250800
    },
    {
      "epoch": 19.16583912611718,
      "grad_norm": 4.72564697265625,
      "learning_rate": 3.402846739490235e-05,
      "loss": 1.7668,
      "step": 250900
    },
    {
      "epoch": 19.173477961958596,
      "grad_norm": 5.316677570343018,
      "learning_rate": 3.402210169836783e-05,
      "loss": 1.7141,
      "step": 251000
    },
    {
      "epoch": 19.181116797800016,
      "grad_norm": 5.009809970855713,
      "learning_rate": 3.4015736001833324e-05,
      "loss": 1.8529,
      "step": 251100
    },
    {
      "epoch": 19.18875563364143,
      "grad_norm": 5.962344169616699,
      "learning_rate": 3.400937030529881e-05,
      "loss": 1.8051,
      "step": 251200
    },
    {
      "epoch": 19.19639446948285,
      "grad_norm": 5.4674763679504395,
      "learning_rate": 3.400300460876429e-05,
      "loss": 1.7812,
      "step": 251300
    },
    {
      "epoch": 19.20403330532427,
      "grad_norm": 7.6226043701171875,
      "learning_rate": 3.399663891222978e-05,
      "loss": 1.7924,
      "step": 251400
    },
    {
      "epoch": 19.211672141165685,
      "grad_norm": 5.350492477416992,
      "learning_rate": 3.3990273215695265e-05,
      "loss": 1.8084,
      "step": 251500
    },
    {
      "epoch": 19.219310977007105,
      "grad_norm": 4.180199146270752,
      "learning_rate": 3.398390751916075e-05,
      "loss": 1.7242,
      "step": 251600
    },
    {
      "epoch": 19.22694981284852,
      "grad_norm": 4.970120906829834,
      "learning_rate": 3.397754182262624e-05,
      "loss": 1.765,
      "step": 251700
    },
    {
      "epoch": 19.23458864868994,
      "grad_norm": 7.260569095611572,
      "learning_rate": 3.397117612609172e-05,
      "loss": 1.7872,
      "step": 251800
    },
    {
      "epoch": 19.24222748453136,
      "grad_norm": 5.438204288482666,
      "learning_rate": 3.3964810429557205e-05,
      "loss": 1.7996,
      "step": 251900
    },
    {
      "epoch": 19.249866320372774,
      "grad_norm": 4.896341800689697,
      "learning_rate": 3.395844473302269e-05,
      "loss": 1.7852,
      "step": 252000
    },
    {
      "epoch": 19.257505156214194,
      "grad_norm": 5.815859317779541,
      "learning_rate": 3.395207903648818e-05,
      "loss": 1.8887,
      "step": 252100
    },
    {
      "epoch": 19.26514399205561,
      "grad_norm": 5.843307018280029,
      "learning_rate": 3.394571333995366e-05,
      "loss": 1.746,
      "step": 252200
    },
    {
      "epoch": 19.27278282789703,
      "grad_norm": 7.7862677574157715,
      "learning_rate": 3.3939347643419146e-05,
      "loss": 1.7623,
      "step": 252300
    },
    {
      "epoch": 19.280421663738448,
      "grad_norm": 6.111161231994629,
      "learning_rate": 3.393298194688463e-05,
      "loss": 1.8702,
      "step": 252400
    },
    {
      "epoch": 19.288060499579863,
      "grad_norm": 4.103032112121582,
      "learning_rate": 3.3926616250350113e-05,
      "loss": 1.8213,
      "step": 252500
    },
    {
      "epoch": 19.295699335421283,
      "grad_norm": 5.8998260498046875,
      "learning_rate": 3.3920250553815604e-05,
      "loss": 1.7372,
      "step": 252600
    },
    {
      "epoch": 19.3033381712627,
      "grad_norm": 7.282650470733643,
      "learning_rate": 3.391388485728109e-05,
      "loss": 1.7711,
      "step": 252700
    },
    {
      "epoch": 19.310977007104118,
      "grad_norm": 7.786950588226318,
      "learning_rate": 3.390751916074657e-05,
      "loss": 1.7863,
      "step": 252800
    },
    {
      "epoch": 19.318615842945537,
      "grad_norm": 5.524378299713135,
      "learning_rate": 3.3901153464212054e-05,
      "loss": 1.7749,
      "step": 252900
    },
    {
      "epoch": 19.326254678786952,
      "grad_norm": 6.54180908203125,
      "learning_rate": 3.389478776767754e-05,
      "loss": 1.7229,
      "step": 253000
    },
    {
      "epoch": 19.33389351462837,
      "grad_norm": 4.413781642913818,
      "learning_rate": 3.388842207114303e-05,
      "loss": 1.6958,
      "step": 253100
    },
    {
      "epoch": 19.341532350469787,
      "grad_norm": 5.224818706512451,
      "learning_rate": 3.388205637460851e-05,
      "loss": 1.6754,
      "step": 253200
    },
    {
      "epoch": 19.349171186311207,
      "grad_norm": 4.840777397155762,
      "learning_rate": 3.3875690678073995e-05,
      "loss": 1.7867,
      "step": 253300
    },
    {
      "epoch": 19.356810022152622,
      "grad_norm": 5.614297866821289,
      "learning_rate": 3.386932498153948e-05,
      "loss": 1.7349,
      "step": 253400
    },
    {
      "epoch": 19.36444885799404,
      "grad_norm": 4.150506496429443,
      "learning_rate": 3.386295928500497e-05,
      "loss": 1.693,
      "step": 253500
    },
    {
      "epoch": 19.37208769383546,
      "grad_norm": 5.518985748291016,
      "learning_rate": 3.385659358847045e-05,
      "loss": 1.7713,
      "step": 253600
    },
    {
      "epoch": 19.379726529676876,
      "grad_norm": 5.964200496673584,
      "learning_rate": 3.3850227891935936e-05,
      "loss": 1.8788,
      "step": 253700
    },
    {
      "epoch": 19.387365365518296,
      "grad_norm": 4.760849475860596,
      "learning_rate": 3.384386219540142e-05,
      "loss": 1.7935,
      "step": 253800
    },
    {
      "epoch": 19.39500420135971,
      "grad_norm": 5.5806050300598145,
      "learning_rate": 3.38374964988669e-05,
      "loss": 1.8405,
      "step": 253900
    },
    {
      "epoch": 19.40264303720113,
      "grad_norm": 5.881447792053223,
      "learning_rate": 3.3831130802332394e-05,
      "loss": 1.8795,
      "step": 254000
    },
    {
      "epoch": 19.41028187304255,
      "grad_norm": 5.622643947601318,
      "learning_rate": 3.382476510579788e-05,
      "loss": 1.7603,
      "step": 254100
    },
    {
      "epoch": 19.417920708883965,
      "grad_norm": 9.146767616271973,
      "learning_rate": 3.381839940926336e-05,
      "loss": 1.9145,
      "step": 254200
    },
    {
      "epoch": 19.425559544725385,
      "grad_norm": 6.304701328277588,
      "learning_rate": 3.3812033712728844e-05,
      "loss": 1.754,
      "step": 254300
    },
    {
      "epoch": 19.4331983805668,
      "grad_norm": 5.4059953689575195,
      "learning_rate": 3.3805668016194335e-05,
      "loss": 1.6941,
      "step": 254400
    },
    {
      "epoch": 19.44083721640822,
      "grad_norm": 6.315765380859375,
      "learning_rate": 3.379930231965982e-05,
      "loss": 1.7904,
      "step": 254500
    },
    {
      "epoch": 19.44847605224964,
      "grad_norm": 4.641218662261963,
      "learning_rate": 3.37929366231253e-05,
      "loss": 1.7691,
      "step": 254600
    },
    {
      "epoch": 19.456114888091054,
      "grad_norm": 5.819750785827637,
      "learning_rate": 3.3786570926590785e-05,
      "loss": 1.7656,
      "step": 254700
    },
    {
      "epoch": 19.463753723932474,
      "grad_norm": 5.187779426574707,
      "learning_rate": 3.3780205230056275e-05,
      "loss": 1.8233,
      "step": 254800
    },
    {
      "epoch": 19.47139255977389,
      "grad_norm": 5.329618453979492,
      "learning_rate": 3.377383953352176e-05,
      "loss": 1.8148,
      "step": 254900
    },
    {
      "epoch": 19.47903139561531,
      "grad_norm": 4.705997467041016,
      "learning_rate": 3.376747383698724e-05,
      "loss": 1.784,
      "step": 255000
    },
    {
      "epoch": 19.486670231456728,
      "grad_norm": 3.0841031074523926,
      "learning_rate": 3.376110814045273e-05,
      "loss": 1.6952,
      "step": 255100
    },
    {
      "epoch": 19.494309067298143,
      "grad_norm": 6.1761579513549805,
      "learning_rate": 3.3754742443918216e-05,
      "loss": 1.8544,
      "step": 255200
    },
    {
      "epoch": 19.501947903139563,
      "grad_norm": 3.5272955894470215,
      "learning_rate": 3.37483767473837e-05,
      "loss": 1.8053,
      "step": 255300
    },
    {
      "epoch": 19.50958673898098,
      "grad_norm": 4.7712507247924805,
      "learning_rate": 3.374201105084919e-05,
      "loss": 1.7227,
      "step": 255400
    },
    {
      "epoch": 19.517225574822398,
      "grad_norm": 7.239047527313232,
      "learning_rate": 3.3735645354314674e-05,
      "loss": 1.6424,
      "step": 255500
    },
    {
      "epoch": 19.524864410663813,
      "grad_norm": 6.331160545349121,
      "learning_rate": 3.372927965778016e-05,
      "loss": 1.8203,
      "step": 255600
    },
    {
      "epoch": 19.532503246505232,
      "grad_norm": 5.077178955078125,
      "learning_rate": 3.372291396124564e-05,
      "loss": 1.8119,
      "step": 255700
    },
    {
      "epoch": 19.54014208234665,
      "grad_norm": 4.546092987060547,
      "learning_rate": 3.371654826471113e-05,
      "loss": 1.7415,
      "step": 255800
    },
    {
      "epoch": 19.547780918188067,
      "grad_norm": 5.649351119995117,
      "learning_rate": 3.3710182568176615e-05,
      "loss": 1.7852,
      "step": 255900
    },
    {
      "epoch": 19.555419754029487,
      "grad_norm": 4.2389984130859375,
      "learning_rate": 3.37038168716421e-05,
      "loss": 1.7609,
      "step": 256000
    },
    {
      "epoch": 19.563058589870902,
      "grad_norm": 5.728728771209717,
      "learning_rate": 3.369745117510758e-05,
      "loss": 1.8239,
      "step": 256100
    },
    {
      "epoch": 19.57069742571232,
      "grad_norm": 6.5577216148376465,
      "learning_rate": 3.3691085478573065e-05,
      "loss": 1.8275,
      "step": 256200
    },
    {
      "epoch": 19.57833626155374,
      "grad_norm": 5.256232738494873,
      "learning_rate": 3.3684719782038556e-05,
      "loss": 1.7017,
      "step": 256300
    },
    {
      "epoch": 19.585975097395156,
      "grad_norm": 3.521902561187744,
      "learning_rate": 3.367835408550404e-05,
      "loss": 1.7287,
      "step": 256400
    },
    {
      "epoch": 19.593613933236576,
      "grad_norm": 5.552823543548584,
      "learning_rate": 3.367198838896952e-05,
      "loss": 1.7848,
      "step": 256500
    },
    {
      "epoch": 19.60125276907799,
      "grad_norm": 6.746807098388672,
      "learning_rate": 3.3665622692435006e-05,
      "loss": 1.7871,
      "step": 256600
    },
    {
      "epoch": 19.60889160491941,
      "grad_norm": 6.05059289932251,
      "learning_rate": 3.3659256995900497e-05,
      "loss": 1.6855,
      "step": 256700
    },
    {
      "epoch": 19.61653044076083,
      "grad_norm": 4.579965591430664,
      "learning_rate": 3.365289129936598e-05,
      "loss": 1.7632,
      "step": 256800
    },
    {
      "epoch": 19.624169276602245,
      "grad_norm": 6.012331008911133,
      "learning_rate": 3.3646525602831464e-05,
      "loss": 1.8259,
      "step": 256900
    },
    {
      "epoch": 19.631808112443665,
      "grad_norm": 5.742774963378906,
      "learning_rate": 3.364015990629695e-05,
      "loss": 1.8259,
      "step": 257000
    },
    {
      "epoch": 19.63944694828508,
      "grad_norm": 4.960878372192383,
      "learning_rate": 3.363379420976243e-05,
      "loss": 1.8389,
      "step": 257100
    },
    {
      "epoch": 19.6470857841265,
      "grad_norm": 5.117522239685059,
      "learning_rate": 3.362742851322792e-05,
      "loss": 1.7895,
      "step": 257200
    },
    {
      "epoch": 19.654724619967915,
      "grad_norm": 6.196835041046143,
      "learning_rate": 3.3621062816693405e-05,
      "loss": 1.7289,
      "step": 257300
    },
    {
      "epoch": 19.662363455809334,
      "grad_norm": 5.241010665893555,
      "learning_rate": 3.361469712015889e-05,
      "loss": 1.7761,
      "step": 257400
    },
    {
      "epoch": 19.670002291650754,
      "grad_norm": 5.091373443603516,
      "learning_rate": 3.360833142362437e-05,
      "loss": 1.8408,
      "step": 257500
    },
    {
      "epoch": 19.67764112749217,
      "grad_norm": 5.09747838973999,
      "learning_rate": 3.360196572708986e-05,
      "loss": 1.7693,
      "step": 257600
    },
    {
      "epoch": 19.68527996333359,
      "grad_norm": 5.741650104522705,
      "learning_rate": 3.3595600030555345e-05,
      "loss": 1.7548,
      "step": 257700
    },
    {
      "epoch": 19.692918799175004,
      "grad_norm": 4.555041790008545,
      "learning_rate": 3.358923433402083e-05,
      "loss": 1.798,
      "step": 257800
    },
    {
      "epoch": 19.700557635016423,
      "grad_norm": 6.545891284942627,
      "learning_rate": 3.358286863748631e-05,
      "loss": 1.8059,
      "step": 257900
    },
    {
      "epoch": 19.708196470857843,
      "grad_norm": 6.413641452789307,
      "learning_rate": 3.3576502940951796e-05,
      "loss": 1.8364,
      "step": 258000
    },
    {
      "epoch": 19.715835306699258,
      "grad_norm": 5.2427167892456055,
      "learning_rate": 3.3570137244417286e-05,
      "loss": 1.798,
      "step": 258100
    },
    {
      "epoch": 19.723474142540677,
      "grad_norm": 6.0797247886657715,
      "learning_rate": 3.356377154788277e-05,
      "loss": 1.64,
      "step": 258200
    },
    {
      "epoch": 19.731112978382093,
      "grad_norm": 5.6316680908203125,
      "learning_rate": 3.3557405851348253e-05,
      "loss": 1.8087,
      "step": 258300
    },
    {
      "epoch": 19.738751814223512,
      "grad_norm": 5.430662631988525,
      "learning_rate": 3.355104015481374e-05,
      "loss": 1.6738,
      "step": 258400
    },
    {
      "epoch": 19.74639065006493,
      "grad_norm": 5.467822551727295,
      "learning_rate": 3.354467445827922e-05,
      "loss": 1.7974,
      "step": 258500
    },
    {
      "epoch": 19.754029485906347,
      "grad_norm": 4.971360683441162,
      "learning_rate": 3.353830876174471e-05,
      "loss": 1.8054,
      "step": 258600
    },
    {
      "epoch": 19.761668321747766,
      "grad_norm": 4.4963884353637695,
      "learning_rate": 3.3531943065210194e-05,
      "loss": 1.6929,
      "step": 258700
    },
    {
      "epoch": 19.769307157589182,
      "grad_norm": 5.227025985717773,
      "learning_rate": 3.3525577368675685e-05,
      "loss": 1.8721,
      "step": 258800
    },
    {
      "epoch": 19.7769459934306,
      "grad_norm": 4.485941410064697,
      "learning_rate": 3.351921167214117e-05,
      "loss": 1.7587,
      "step": 258900
    },
    {
      "epoch": 19.78458482927202,
      "grad_norm": 6.149507522583008,
      "learning_rate": 3.351284597560665e-05,
      "loss": 1.7898,
      "step": 259000
    },
    {
      "epoch": 19.792223665113436,
      "grad_norm": 8.357484817504883,
      "learning_rate": 3.350648027907214e-05,
      "loss": 1.7418,
      "step": 259100
    },
    {
      "epoch": 19.799862500954855,
      "grad_norm": 4.947106838226318,
      "learning_rate": 3.3500114582537626e-05,
      "loss": 1.7233,
      "step": 259200
    },
    {
      "epoch": 19.80750133679627,
      "grad_norm": 4.805685043334961,
      "learning_rate": 3.349374888600311e-05,
      "loss": 1.8138,
      "step": 259300
    },
    {
      "epoch": 19.81514017263769,
      "grad_norm": 5.6383538246154785,
      "learning_rate": 3.348738318946859e-05,
      "loss": 1.8204,
      "step": 259400
    },
    {
      "epoch": 19.82277900847911,
      "grad_norm": 6.837986469268799,
      "learning_rate": 3.348101749293408e-05,
      "loss": 1.768,
      "step": 259500
    },
    {
      "epoch": 19.830417844320525,
      "grad_norm": 5.58306884765625,
      "learning_rate": 3.3474651796399567e-05,
      "loss": 1.8334,
      "step": 259600
    },
    {
      "epoch": 19.838056680161944,
      "grad_norm": 5.360318183898926,
      "learning_rate": 3.346828609986505e-05,
      "loss": 1.6609,
      "step": 259700
    },
    {
      "epoch": 19.84569551600336,
      "grad_norm": 6.654257774353027,
      "learning_rate": 3.3461920403330534e-05,
      "loss": 1.7739,
      "step": 259800
    },
    {
      "epoch": 19.85333435184478,
      "grad_norm": 4.675461769104004,
      "learning_rate": 3.3455554706796024e-05,
      "loss": 1.8773,
      "step": 259900
    },
    {
      "epoch": 19.860973187686195,
      "grad_norm": 5.061629295349121,
      "learning_rate": 3.344918901026151e-05,
      "loss": 1.7406,
      "step": 260000
    },
    {
      "epoch": 19.868612023527614,
      "grad_norm": 5.141446590423584,
      "learning_rate": 3.344282331372699e-05,
      "loss": 1.757,
      "step": 260100
    },
    {
      "epoch": 19.876250859369033,
      "grad_norm": 4.418941497802734,
      "learning_rate": 3.3436457617192475e-05,
      "loss": 1.7516,
      "step": 260200
    },
    {
      "epoch": 19.88388969521045,
      "grad_norm": 4.702230453491211,
      "learning_rate": 3.343009192065796e-05,
      "loss": 1.7514,
      "step": 260300
    },
    {
      "epoch": 19.89152853105187,
      "grad_norm": 5.7258195877075195,
      "learning_rate": 3.342372622412345e-05,
      "loss": 1.7504,
      "step": 260400
    },
    {
      "epoch": 19.899167366893284,
      "grad_norm": 3.423041820526123,
      "learning_rate": 3.341736052758893e-05,
      "loss": 1.7329,
      "step": 260500
    },
    {
      "epoch": 19.906806202734703,
      "grad_norm": 4.800998210906982,
      "learning_rate": 3.3410994831054415e-05,
      "loss": 1.8497,
      "step": 260600
    },
    {
      "epoch": 19.914445038576122,
      "grad_norm": 6.212441444396973,
      "learning_rate": 3.34046291345199e-05,
      "loss": 1.81,
      "step": 260700
    },
    {
      "epoch": 19.922083874417538,
      "grad_norm": 5.082820892333984,
      "learning_rate": 3.339826343798539e-05,
      "loss": 1.8075,
      "step": 260800
    },
    {
      "epoch": 19.929722710258957,
      "grad_norm": 3.642843246459961,
      "learning_rate": 3.339189774145087e-05,
      "loss": 1.771,
      "step": 260900
    },
    {
      "epoch": 19.937361546100373,
      "grad_norm": 5.383366107940674,
      "learning_rate": 3.3385532044916356e-05,
      "loss": 1.7959,
      "step": 261000
    },
    {
      "epoch": 19.945000381941792,
      "grad_norm": 6.819878578186035,
      "learning_rate": 3.337916634838184e-05,
      "loss": 1.8516,
      "step": 261100
    },
    {
      "epoch": 19.95263921778321,
      "grad_norm": 4.655232906341553,
      "learning_rate": 3.3372800651847323e-05,
      "loss": 1.8188,
      "step": 261200
    },
    {
      "epoch": 19.960278053624627,
      "grad_norm": 4.660582542419434,
      "learning_rate": 3.3366434955312814e-05,
      "loss": 1.8025,
      "step": 261300
    },
    {
      "epoch": 19.967916889466046,
      "grad_norm": 4.220623016357422,
      "learning_rate": 3.33600692587783e-05,
      "loss": 1.8121,
      "step": 261400
    },
    {
      "epoch": 19.975555725307462,
      "grad_norm": 5.18765926361084,
      "learning_rate": 3.335370356224378e-05,
      "loss": 1.7718,
      "step": 261500
    },
    {
      "epoch": 19.98319456114888,
      "grad_norm": 6.10144567489624,
      "learning_rate": 3.3347337865709264e-05,
      "loss": 1.6979,
      "step": 261600
    },
    {
      "epoch": 19.990833396990297,
      "grad_norm": 4.819129467010498,
      "learning_rate": 3.334097216917475e-05,
      "loss": 1.81,
      "step": 261700
    },
    {
      "epoch": 19.998472232831716,
      "grad_norm": 5.370359897613525,
      "learning_rate": 3.333460647264024e-05,
      "loss": 1.8248,
      "step": 261800
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.8036726713180542,
      "eval_runtime": 3.06,
      "eval_samples_per_second": 225.493,
      "eval_steps_per_second": 225.493,
      "step": 261820
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.5431064367294312,
      "eval_runtime": 58.4278,
      "eval_samples_per_second": 224.054,
      "eval_steps_per_second": 224.054,
      "step": 261820
    },
    {
      "epoch": 20.006111068673135,
      "grad_norm": 4.871217250823975,
      "learning_rate": 3.332824077610572e-05,
      "loss": 1.8112,
      "step": 261900
    },
    {
      "epoch": 20.01374990451455,
      "grad_norm": 4.8026275634765625,
      "learning_rate": 3.3321875079571205e-05,
      "loss": 1.7748,
      "step": 262000
    },
    {
      "epoch": 20.02138874035597,
      "grad_norm": 4.588955879211426,
      "learning_rate": 3.331550938303669e-05,
      "loss": 1.7492,
      "step": 262100
    },
    {
      "epoch": 20.029027576197386,
      "grad_norm": 5.9498209953308105,
      "learning_rate": 3.330914368650218e-05,
      "loss": 1.7608,
      "step": 262200
    },
    {
      "epoch": 20.036666412038805,
      "grad_norm": 5.480939865112305,
      "learning_rate": 3.330277798996766e-05,
      "loss": 1.807,
      "step": 262300
    },
    {
      "epoch": 20.044305247880224,
      "grad_norm": 7.156487464904785,
      "learning_rate": 3.3296412293433146e-05,
      "loss": 1.7703,
      "step": 262400
    },
    {
      "epoch": 20.05194408372164,
      "grad_norm": 4.877126693725586,
      "learning_rate": 3.329004659689863e-05,
      "loss": 1.7437,
      "step": 262500
    },
    {
      "epoch": 20.05958291956306,
      "grad_norm": 6.548354148864746,
      "learning_rate": 3.328368090036412e-05,
      "loss": 1.7931,
      "step": 262600
    },
    {
      "epoch": 20.067221755404475,
      "grad_norm": 4.952149391174316,
      "learning_rate": 3.3277315203829604e-05,
      "loss": 1.8055,
      "step": 262700
    },
    {
      "epoch": 20.074860591245894,
      "grad_norm": 4.710919380187988,
      "learning_rate": 3.327094950729509e-05,
      "loss": 1.8385,
      "step": 262800
    },
    {
      "epoch": 20.082499427087313,
      "grad_norm": 4.917740345001221,
      "learning_rate": 3.326458381076058e-05,
      "loss": 1.7453,
      "step": 262900
    },
    {
      "epoch": 20.09013826292873,
      "grad_norm": 5.354560852050781,
      "learning_rate": 3.325821811422606e-05,
      "loss": 1.8085,
      "step": 263000
    },
    {
      "epoch": 20.09777709877015,
      "grad_norm": 6.451955795288086,
      "learning_rate": 3.325185241769155e-05,
      "loss": 1.7337,
      "step": 263100
    },
    {
      "epoch": 20.105415934611564,
      "grad_norm": 4.839236736297607,
      "learning_rate": 3.3245486721157035e-05,
      "loss": 1.8591,
      "step": 263200
    },
    {
      "epoch": 20.113054770452983,
      "grad_norm": 5.411108493804932,
      "learning_rate": 3.323912102462252e-05,
      "loss": 1.7046,
      "step": 263300
    },
    {
      "epoch": 20.120693606294402,
      "grad_norm": 6.97467041015625,
      "learning_rate": 3.3232755328088e-05,
      "loss": 1.7661,
      "step": 263400
    },
    {
      "epoch": 20.128332442135818,
      "grad_norm": 5.1541619300842285,
      "learning_rate": 3.3226389631553485e-05,
      "loss": 1.823,
      "step": 263500
    },
    {
      "epoch": 20.135971277977237,
      "grad_norm": 3.6693763732910156,
      "learning_rate": 3.3220023935018976e-05,
      "loss": 1.7657,
      "step": 263600
    },
    {
      "epoch": 20.143610113818653,
      "grad_norm": 5.005786895751953,
      "learning_rate": 3.321365823848446e-05,
      "loss": 1.7908,
      "step": 263700
    },
    {
      "epoch": 20.151248949660072,
      "grad_norm": 5.067907333374023,
      "learning_rate": 3.320729254194994e-05,
      "loss": 1.7762,
      "step": 263800
    },
    {
      "epoch": 20.158887785501488,
      "grad_norm": 3.1361122131347656,
      "learning_rate": 3.3200926845415426e-05,
      "loss": 1.7662,
      "step": 263900
    },
    {
      "epoch": 20.166526621342907,
      "grad_norm": 5.623093605041504,
      "learning_rate": 3.319456114888091e-05,
      "loss": 1.7582,
      "step": 264000
    },
    {
      "epoch": 20.174165457184326,
      "grad_norm": 4.763171195983887,
      "learning_rate": 3.31881954523464e-05,
      "loss": 1.7548,
      "step": 264100
    },
    {
      "epoch": 20.181804293025742,
      "grad_norm": 3.061147928237915,
      "learning_rate": 3.3181829755811884e-05,
      "loss": 1.7872,
      "step": 264200
    },
    {
      "epoch": 20.18944312886716,
      "grad_norm": 6.229980945587158,
      "learning_rate": 3.317546405927737e-05,
      "loss": 1.73,
      "step": 264300
    },
    {
      "epoch": 20.197081964708577,
      "grad_norm": 6.4005351066589355,
      "learning_rate": 3.316909836274285e-05,
      "loss": 1.699,
      "step": 264400
    },
    {
      "epoch": 20.204720800549996,
      "grad_norm": 5.112342357635498,
      "learning_rate": 3.316273266620834e-05,
      "loss": 1.7946,
      "step": 264500
    },
    {
      "epoch": 20.212359636391415,
      "grad_norm": 4.2951531410217285,
      "learning_rate": 3.3156366969673825e-05,
      "loss": 1.6725,
      "step": 264600
    },
    {
      "epoch": 20.21999847223283,
      "grad_norm": 6.238949775695801,
      "learning_rate": 3.315000127313931e-05,
      "loss": 1.8007,
      "step": 264700
    },
    {
      "epoch": 20.22763730807425,
      "grad_norm": 5.239902019500732,
      "learning_rate": 3.314363557660479e-05,
      "loss": 1.7527,
      "step": 264800
    },
    {
      "epoch": 20.235276143915666,
      "grad_norm": 5.331529140472412,
      "learning_rate": 3.3137269880070275e-05,
      "loss": 1.8263,
      "step": 264900
    },
    {
      "epoch": 20.242914979757085,
      "grad_norm": 6.448617458343506,
      "learning_rate": 3.3130904183535766e-05,
      "loss": 1.7228,
      "step": 265000
    },
    {
      "epoch": 20.250553815598504,
      "grad_norm": 4.7546610832214355,
      "learning_rate": 3.312453848700125e-05,
      "loss": 1.6599,
      "step": 265100
    },
    {
      "epoch": 20.25819265143992,
      "grad_norm": 6.4247846603393555,
      "learning_rate": 3.311817279046673e-05,
      "loss": 1.817,
      "step": 265200
    },
    {
      "epoch": 20.26583148728134,
      "grad_norm": 7.4676594734191895,
      "learning_rate": 3.3111807093932216e-05,
      "loss": 1.8108,
      "step": 265300
    },
    {
      "epoch": 20.273470323122755,
      "grad_norm": 5.240146160125732,
      "learning_rate": 3.3105441397397707e-05,
      "loss": 1.8119,
      "step": 265400
    },
    {
      "epoch": 20.281109158964174,
      "grad_norm": 4.6419806480407715,
      "learning_rate": 3.309907570086319e-05,
      "loss": 1.7762,
      "step": 265500
    },
    {
      "epoch": 20.288747994805593,
      "grad_norm": 4.9170966148376465,
      "learning_rate": 3.3092710004328674e-05,
      "loss": 1.7462,
      "step": 265600
    },
    {
      "epoch": 20.29638683064701,
      "grad_norm": 5.3996663093566895,
      "learning_rate": 3.308634430779416e-05,
      "loss": 1.7541,
      "step": 265700
    },
    {
      "epoch": 20.304025666488428,
      "grad_norm": 5.223659515380859,
      "learning_rate": 3.307997861125964e-05,
      "loss": 1.7618,
      "step": 265800
    },
    {
      "epoch": 20.311664502329844,
      "grad_norm": 5.382320880889893,
      "learning_rate": 3.307361291472513e-05,
      "loss": 1.8058,
      "step": 265900
    },
    {
      "epoch": 20.319303338171263,
      "grad_norm": 4.10582160949707,
      "learning_rate": 3.3067247218190615e-05,
      "loss": 1.7833,
      "step": 266000
    },
    {
      "epoch": 20.32694217401268,
      "grad_norm": 5.648346424102783,
      "learning_rate": 3.30608815216561e-05,
      "loss": 1.7585,
      "step": 266100
    },
    {
      "epoch": 20.334581009854098,
      "grad_norm": 5.023955345153809,
      "learning_rate": 3.305451582512158e-05,
      "loss": 1.8553,
      "step": 266200
    },
    {
      "epoch": 20.342219845695517,
      "grad_norm": 5.639865875244141,
      "learning_rate": 3.304815012858707e-05,
      "loss": 1.6611,
      "step": 266300
    },
    {
      "epoch": 20.349858681536933,
      "grad_norm": 6.540017604827881,
      "learning_rate": 3.3041784432052555e-05,
      "loss": 1.8099,
      "step": 266400
    },
    {
      "epoch": 20.357497517378352,
      "grad_norm": 5.258211612701416,
      "learning_rate": 3.303541873551804e-05,
      "loss": 1.7165,
      "step": 266500
    },
    {
      "epoch": 20.365136353219768,
      "grad_norm": 5.948254108428955,
      "learning_rate": 3.302905303898353e-05,
      "loss": 1.7621,
      "step": 266600
    },
    {
      "epoch": 20.372775189061187,
      "grad_norm": 5.448042869567871,
      "learning_rate": 3.302268734244901e-05,
      "loss": 1.7597,
      "step": 266700
    },
    {
      "epoch": 20.380414024902606,
      "grad_norm": 5.230390548706055,
      "learning_rate": 3.3016321645914496e-05,
      "loss": 1.8445,
      "step": 266800
    },
    {
      "epoch": 20.388052860744022,
      "grad_norm": 7.358399868011475,
      "learning_rate": 3.300995594937999e-05,
      "loss": 1.8214,
      "step": 266900
    },
    {
      "epoch": 20.39569169658544,
      "grad_norm": 6.276939392089844,
      "learning_rate": 3.300359025284547e-05,
      "loss": 1.8168,
      "step": 267000
    },
    {
      "epoch": 20.403330532426857,
      "grad_norm": 4.33156681060791,
      "learning_rate": 3.2997224556310954e-05,
      "loss": 1.9056,
      "step": 267100
    },
    {
      "epoch": 20.410969368268276,
      "grad_norm": 6.626154899597168,
      "learning_rate": 3.299085885977644e-05,
      "loss": 1.7388,
      "step": 267200
    },
    {
      "epoch": 20.418608204109695,
      "grad_norm": 5.443278789520264,
      "learning_rate": 3.298449316324193e-05,
      "loss": 1.8785,
      "step": 267300
    },
    {
      "epoch": 20.42624703995111,
      "grad_norm": 5.01084566116333,
      "learning_rate": 3.297812746670741e-05,
      "loss": 1.7096,
      "step": 267400
    },
    {
      "epoch": 20.43388587579253,
      "grad_norm": 6.653237342834473,
      "learning_rate": 3.2971761770172895e-05,
      "loss": 1.6331,
      "step": 267500
    },
    {
      "epoch": 20.441524711633946,
      "grad_norm": 6.306039810180664,
      "learning_rate": 3.296539607363838e-05,
      "loss": 1.7229,
      "step": 267600
    },
    {
      "epoch": 20.449163547475365,
      "grad_norm": 5.604292869567871,
      "learning_rate": 3.295903037710387e-05,
      "loss": 1.8226,
      "step": 267700
    },
    {
      "epoch": 20.456802383316784,
      "grad_norm": 7.751125812530518,
      "learning_rate": 3.295266468056935e-05,
      "loss": 1.7282,
      "step": 267800
    },
    {
      "epoch": 20.4644412191582,
      "grad_norm": 6.541319370269775,
      "learning_rate": 3.2946298984034836e-05,
      "loss": 1.782,
      "step": 267900
    },
    {
      "epoch": 20.47208005499962,
      "grad_norm": 6.7958478927612305,
      "learning_rate": 3.293993328750032e-05,
      "loss": 1.7499,
      "step": 268000
    },
    {
      "epoch": 20.479718890841035,
      "grad_norm": 6.233141899108887,
      "learning_rate": 3.29335675909658e-05,
      "loss": 1.7894,
      "step": 268100
    },
    {
      "epoch": 20.487357726682454,
      "grad_norm": 4.869743824005127,
      "learning_rate": 3.292720189443129e-05,
      "loss": 1.6746,
      "step": 268200
    },
    {
      "epoch": 20.49499656252387,
      "grad_norm": 4.9609375,
      "learning_rate": 3.2920836197896777e-05,
      "loss": 1.8558,
      "step": 268300
    },
    {
      "epoch": 20.50263539836529,
      "grad_norm": 10.865286827087402,
      "learning_rate": 3.291447050136226e-05,
      "loss": 1.7228,
      "step": 268400
    },
    {
      "epoch": 20.510274234206708,
      "grad_norm": 4.936910629272461,
      "learning_rate": 3.2908104804827744e-05,
      "loss": 1.8612,
      "step": 268500
    },
    {
      "epoch": 20.517913070048124,
      "grad_norm": 4.849244594573975,
      "learning_rate": 3.2901739108293234e-05,
      "loss": 1.8161,
      "step": 268600
    },
    {
      "epoch": 20.525551905889543,
      "grad_norm": 5.543489456176758,
      "learning_rate": 3.289537341175872e-05,
      "loss": 1.7673,
      "step": 268700
    },
    {
      "epoch": 20.53319074173096,
      "grad_norm": 8.448060035705566,
      "learning_rate": 3.28890077152242e-05,
      "loss": 1.6921,
      "step": 268800
    },
    {
      "epoch": 20.540829577572378,
      "grad_norm": 5.694599628448486,
      "learning_rate": 3.2882642018689685e-05,
      "loss": 1.76,
      "step": 268900
    },
    {
      "epoch": 20.548468413413797,
      "grad_norm": 5.742857933044434,
      "learning_rate": 3.287627632215517e-05,
      "loss": 1.7487,
      "step": 269000
    },
    {
      "epoch": 20.556107249255213,
      "grad_norm": 5.453084945678711,
      "learning_rate": 3.286991062562066e-05,
      "loss": 1.82,
      "step": 269100
    },
    {
      "epoch": 20.563746085096632,
      "grad_norm": 3.3046915531158447,
      "learning_rate": 3.286354492908614e-05,
      "loss": 1.7036,
      "step": 269200
    },
    {
      "epoch": 20.571384920938048,
      "grad_norm": 5.015454292297363,
      "learning_rate": 3.2857179232551625e-05,
      "loss": 1.878,
      "step": 269300
    },
    {
      "epoch": 20.579023756779467,
      "grad_norm": 6.092305660247803,
      "learning_rate": 3.285081353601711e-05,
      "loss": 1.6945,
      "step": 269400
    },
    {
      "epoch": 20.586662592620886,
      "grad_norm": 4.453991413116455,
      "learning_rate": 3.28444478394826e-05,
      "loss": 1.8331,
      "step": 269500
    },
    {
      "epoch": 20.594301428462302,
      "grad_norm": 4.398707389831543,
      "learning_rate": 3.283808214294808e-05,
      "loss": 1.7144,
      "step": 269600
    },
    {
      "epoch": 20.60194026430372,
      "grad_norm": 5.7713141441345215,
      "learning_rate": 3.2831716446413566e-05,
      "loss": 1.6848,
      "step": 269700
    },
    {
      "epoch": 20.609579100145137,
      "grad_norm": 4.265415668487549,
      "learning_rate": 3.282535074987905e-05,
      "loss": 1.7444,
      "step": 269800
    },
    {
      "epoch": 20.617217935986556,
      "grad_norm": 6.2271504402160645,
      "learning_rate": 3.2818985053344533e-05,
      "loss": 1.6673,
      "step": 269900
    },
    {
      "epoch": 20.62485677182797,
      "grad_norm": 5.261368751525879,
      "learning_rate": 3.2812619356810024e-05,
      "loss": 1.7345,
      "step": 270000
    },
    {
      "epoch": 20.63249560766939,
      "grad_norm": 4.337031841278076,
      "learning_rate": 3.280625366027551e-05,
      "loss": 1.7781,
      "step": 270100
    },
    {
      "epoch": 20.64013444351081,
      "grad_norm": 6.329127311706543,
      "learning_rate": 3.279988796374099e-05,
      "loss": 1.7653,
      "step": 270200
    },
    {
      "epoch": 20.647773279352226,
      "grad_norm": 5.421004772186279,
      "learning_rate": 3.279352226720648e-05,
      "loss": 1.7951,
      "step": 270300
    },
    {
      "epoch": 20.655412115193645,
      "grad_norm": 7.860649585723877,
      "learning_rate": 3.2787156570671965e-05,
      "loss": 1.8284,
      "step": 270400
    },
    {
      "epoch": 20.66305095103506,
      "grad_norm": 5.044131278991699,
      "learning_rate": 3.278079087413745e-05,
      "loss": 1.7952,
      "step": 270500
    },
    {
      "epoch": 20.67068978687648,
      "grad_norm": 6.041208744049072,
      "learning_rate": 3.277442517760294e-05,
      "loss": 1.7642,
      "step": 270600
    },
    {
      "epoch": 20.6783286227179,
      "grad_norm": 5.415584087371826,
      "learning_rate": 3.276805948106842e-05,
      "loss": 1.7463,
      "step": 270700
    },
    {
      "epoch": 20.685967458559315,
      "grad_norm": 4.829152584075928,
      "learning_rate": 3.2761693784533906e-05,
      "loss": 1.724,
      "step": 270800
    },
    {
      "epoch": 20.693606294400734,
      "grad_norm": 5.927853584289551,
      "learning_rate": 3.2755328087999396e-05,
      "loss": 1.7045,
      "step": 270900
    },
    {
      "epoch": 20.70124513024215,
      "grad_norm": 4.7103190422058105,
      "learning_rate": 3.274896239146488e-05,
      "loss": 1.7673,
      "step": 271000
    },
    {
      "epoch": 20.70888396608357,
      "grad_norm": 4.998058319091797,
      "learning_rate": 3.274259669493036e-05,
      "loss": 1.7954,
      "step": 271100
    },
    {
      "epoch": 20.716522801924988,
      "grad_norm": 5.733242034912109,
      "learning_rate": 3.2736230998395847e-05,
      "loss": 1.7446,
      "step": 271200
    },
    {
      "epoch": 20.724161637766404,
      "grad_norm": 4.356980323791504,
      "learning_rate": 3.272986530186133e-05,
      "loss": 1.7333,
      "step": 271300
    },
    {
      "epoch": 20.731800473607823,
      "grad_norm": 4.57454776763916,
      "learning_rate": 3.272349960532682e-05,
      "loss": 1.8105,
      "step": 271400
    },
    {
      "epoch": 20.73943930944924,
      "grad_norm": 4.631016731262207,
      "learning_rate": 3.2717133908792304e-05,
      "loss": 1.7033,
      "step": 271500
    },
    {
      "epoch": 20.747078145290658,
      "grad_norm": 4.409245014190674,
      "learning_rate": 3.271076821225779e-05,
      "loss": 1.7703,
      "step": 271600
    },
    {
      "epoch": 20.754716981132077,
      "grad_norm": 5.708498954772949,
      "learning_rate": 3.270440251572327e-05,
      "loss": 1.7629,
      "step": 271700
    },
    {
      "epoch": 20.762355816973493,
      "grad_norm": 5.686657905578613,
      "learning_rate": 3.269803681918876e-05,
      "loss": 1.7055,
      "step": 271800
    },
    {
      "epoch": 20.769994652814912,
      "grad_norm": 4.72293758392334,
      "learning_rate": 3.2691671122654245e-05,
      "loss": 1.7332,
      "step": 271900
    },
    {
      "epoch": 20.777633488656328,
      "grad_norm": 5.2426252365112305,
      "learning_rate": 3.268530542611973e-05,
      "loss": 1.7449,
      "step": 272000
    },
    {
      "epoch": 20.785272324497747,
      "grad_norm": 5.528879165649414,
      "learning_rate": 3.267893972958521e-05,
      "loss": 1.8085,
      "step": 272100
    },
    {
      "epoch": 20.792911160339166,
      "grad_norm": 5.334548473358154,
      "learning_rate": 3.2672574033050695e-05,
      "loss": 1.7749,
      "step": 272200
    },
    {
      "epoch": 20.80054999618058,
      "grad_norm": 4.891336441040039,
      "learning_rate": 3.2666208336516186e-05,
      "loss": 1.7456,
      "step": 272300
    },
    {
      "epoch": 20.808188832022,
      "grad_norm": 5.560766696929932,
      "learning_rate": 3.265984263998167e-05,
      "loss": 1.8003,
      "step": 272400
    },
    {
      "epoch": 20.815827667863417,
      "grad_norm": 4.777658939361572,
      "learning_rate": 3.265347694344715e-05,
      "loss": 1.7903,
      "step": 272500
    },
    {
      "epoch": 20.823466503704836,
      "grad_norm": 5.152381420135498,
      "learning_rate": 3.2647111246912636e-05,
      "loss": 1.8493,
      "step": 272600
    },
    {
      "epoch": 20.83110533954625,
      "grad_norm": 7.40731954574585,
      "learning_rate": 3.264074555037812e-05,
      "loss": 1.7985,
      "step": 272700
    },
    {
      "epoch": 20.83874417538767,
      "grad_norm": 5.68884801864624,
      "learning_rate": 3.263437985384361e-05,
      "loss": 1.7231,
      "step": 272800
    },
    {
      "epoch": 20.84638301122909,
      "grad_norm": 4.511798858642578,
      "learning_rate": 3.2628014157309094e-05,
      "loss": 1.7224,
      "step": 272900
    },
    {
      "epoch": 20.854021847070506,
      "grad_norm": 4.857561111450195,
      "learning_rate": 3.262164846077458e-05,
      "loss": 1.7499,
      "step": 273000
    },
    {
      "epoch": 20.861660682911925,
      "grad_norm": 5.988554954528809,
      "learning_rate": 3.261528276424006e-05,
      "loss": 1.728,
      "step": 273100
    },
    {
      "epoch": 20.86929951875334,
      "grad_norm": 5.376276969909668,
      "learning_rate": 3.260891706770555e-05,
      "loss": 1.8145,
      "step": 273200
    },
    {
      "epoch": 20.87693835459476,
      "grad_norm": 5.202948093414307,
      "learning_rate": 3.2602551371171035e-05,
      "loss": 1.8405,
      "step": 273300
    },
    {
      "epoch": 20.88457719043618,
      "grad_norm": 5.764257907867432,
      "learning_rate": 3.259618567463652e-05,
      "loss": 1.8624,
      "step": 273400
    },
    {
      "epoch": 20.892216026277595,
      "grad_norm": 4.73615837097168,
      "learning_rate": 3.2589819978102e-05,
      "loss": 1.7602,
      "step": 273500
    },
    {
      "epoch": 20.899854862119014,
      "grad_norm": 5.083000659942627,
      "learning_rate": 3.2583454281567485e-05,
      "loss": 1.7368,
      "step": 273600
    },
    {
      "epoch": 20.90749369796043,
      "grad_norm": 8.47059154510498,
      "learning_rate": 3.2577088585032976e-05,
      "loss": 1.7173,
      "step": 273700
    },
    {
      "epoch": 20.91513253380185,
      "grad_norm": 4.453982830047607,
      "learning_rate": 3.257072288849846e-05,
      "loss": 1.8049,
      "step": 273800
    },
    {
      "epoch": 20.922771369643268,
      "grad_norm": 5.019822120666504,
      "learning_rate": 3.256435719196394e-05,
      "loss": 1.8069,
      "step": 273900
    },
    {
      "epoch": 20.930410205484684,
      "grad_norm": 5.136969566345215,
      "learning_rate": 3.255799149542943e-05,
      "loss": 1.7707,
      "step": 274000
    },
    {
      "epoch": 20.938049041326103,
      "grad_norm": 3.81313157081604,
      "learning_rate": 3.2551625798894917e-05,
      "loss": 1.691,
      "step": 274100
    },
    {
      "epoch": 20.94568787716752,
      "grad_norm": 4.898757457733154,
      "learning_rate": 3.25452601023604e-05,
      "loss": 1.8765,
      "step": 274200
    },
    {
      "epoch": 20.953326713008938,
      "grad_norm": 4.948798179626465,
      "learning_rate": 3.253889440582589e-05,
      "loss": 1.7918,
      "step": 274300
    },
    {
      "epoch": 20.960965548850353,
      "grad_norm": 5.52703857421875,
      "learning_rate": 3.2532528709291374e-05,
      "loss": 1.8556,
      "step": 274400
    },
    {
      "epoch": 20.968604384691773,
      "grad_norm": 5.298275947570801,
      "learning_rate": 3.252616301275686e-05,
      "loss": 1.826,
      "step": 274500
    },
    {
      "epoch": 20.976243220533192,
      "grad_norm": 9.166399955749512,
      "learning_rate": 3.251979731622235e-05,
      "loss": 1.688,
      "step": 274600
    },
    {
      "epoch": 20.983882056374608,
      "grad_norm": 5.295637607574463,
      "learning_rate": 3.251343161968783e-05,
      "loss": 1.8257,
      "step": 274700
    },
    {
      "epoch": 20.991520892216027,
      "grad_norm": 4.921550750732422,
      "learning_rate": 3.2507065923153315e-05,
      "loss": 1.7371,
      "step": 274800
    },
    {
      "epoch": 20.999159728057442,
      "grad_norm": 5.946765422821045,
      "learning_rate": 3.25007002266188e-05,
      "loss": 1.8282,
      "step": 274900
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.797850489616394,
      "eval_runtime": 3.0624,
      "eval_samples_per_second": 225.311,
      "eval_steps_per_second": 225.311,
      "step": 274911
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.5328086614608765,
      "eval_runtime": 57.9486,
      "eval_samples_per_second": 225.907,
      "eval_steps_per_second": 225.907,
      "step": 274911
    },
    {
      "epoch": 21.00679856389886,
      "grad_norm": 4.983119010925293,
      "learning_rate": 3.249433453008429e-05,
      "loss": 1.7812,
      "step": 275000
    },
    {
      "epoch": 21.01443739974028,
      "grad_norm": 4.314858913421631,
      "learning_rate": 3.248796883354977e-05,
      "loss": 1.7335,
      "step": 275100
    },
    {
      "epoch": 21.022076235581697,
      "grad_norm": 5.45161771774292,
      "learning_rate": 3.2481603137015256e-05,
      "loss": 1.7074,
      "step": 275200
    },
    {
      "epoch": 21.029715071423116,
      "grad_norm": 5.1525163650512695,
      "learning_rate": 3.247523744048074e-05,
      "loss": 1.7402,
      "step": 275300
    },
    {
      "epoch": 21.03735390726453,
      "grad_norm": 7.294266223907471,
      "learning_rate": 3.246887174394622e-05,
      "loss": 1.773,
      "step": 275400
    },
    {
      "epoch": 21.04499274310595,
      "grad_norm": 4.820675373077393,
      "learning_rate": 3.246250604741171e-05,
      "loss": 1.7778,
      "step": 275500
    },
    {
      "epoch": 21.05263157894737,
      "grad_norm": 6.057847499847412,
      "learning_rate": 3.24561403508772e-05,
      "loss": 1.747,
      "step": 275600
    },
    {
      "epoch": 21.060270414788786,
      "grad_norm": 4.427558422088623,
      "learning_rate": 3.244977465434268e-05,
      "loss": 1.6504,
      "step": 275700
    },
    {
      "epoch": 21.067909250630205,
      "grad_norm": 5.108290195465088,
      "learning_rate": 3.2443408957808164e-05,
      "loss": 1.8003,
      "step": 275800
    },
    {
      "epoch": 21.07554808647162,
      "grad_norm": 7.501963138580322,
      "learning_rate": 3.243704326127365e-05,
      "loss": 1.6788,
      "step": 275900
    },
    {
      "epoch": 21.08318692231304,
      "grad_norm": 5.540159702301025,
      "learning_rate": 3.243067756473914e-05,
      "loss": 1.8607,
      "step": 276000
    },
    {
      "epoch": 21.09082575815446,
      "grad_norm": 6.374531269073486,
      "learning_rate": 3.242431186820462e-05,
      "loss": 1.7164,
      "step": 276100
    },
    {
      "epoch": 21.098464593995875,
      "grad_norm": 6.779896259307861,
      "learning_rate": 3.2417946171670105e-05,
      "loss": 1.6777,
      "step": 276200
    },
    {
      "epoch": 21.106103429837294,
      "grad_norm": 6.005769729614258,
      "learning_rate": 3.241158047513559e-05,
      "loss": 1.6729,
      "step": 276300
    },
    {
      "epoch": 21.11374226567871,
      "grad_norm": 5.484744548797607,
      "learning_rate": 3.240521477860108e-05,
      "loss": 1.8118,
      "step": 276400
    },
    {
      "epoch": 21.12138110152013,
      "grad_norm": 4.714601993560791,
      "learning_rate": 3.239884908206656e-05,
      "loss": 1.8546,
      "step": 276500
    },
    {
      "epoch": 21.129019937361544,
      "grad_norm": 5.7391438484191895,
      "learning_rate": 3.2392483385532046e-05,
      "loss": 1.738,
      "step": 276600
    },
    {
      "epoch": 21.136658773202964,
      "grad_norm": 5.063401699066162,
      "learning_rate": 3.238611768899753e-05,
      "loss": 1.72,
      "step": 276700
    },
    {
      "epoch": 21.144297609044383,
      "grad_norm": 7.325076580047607,
      "learning_rate": 3.237975199246301e-05,
      "loss": 1.7403,
      "step": 276800
    },
    {
      "epoch": 21.1519364448858,
      "grad_norm": 4.258529186248779,
      "learning_rate": 3.23733862959285e-05,
      "loss": 1.535,
      "step": 276900
    },
    {
      "epoch": 21.159575280727218,
      "grad_norm": 5.593767166137695,
      "learning_rate": 3.2367020599393987e-05,
      "loss": 1.7282,
      "step": 277000
    },
    {
      "epoch": 21.167214116568633,
      "grad_norm": 4.30566930770874,
      "learning_rate": 3.236065490285947e-05,
      "loss": 1.771,
      "step": 277100
    },
    {
      "epoch": 21.174852952410053,
      "grad_norm": 5.043085098266602,
      "learning_rate": 3.2354289206324954e-05,
      "loss": 1.717,
      "step": 277200
    },
    {
      "epoch": 21.182491788251472,
      "grad_norm": 6.142752170562744,
      "learning_rate": 3.2347923509790444e-05,
      "loss": 1.8309,
      "step": 277300
    },
    {
      "epoch": 21.190130624092888,
      "grad_norm": 4.417633056640625,
      "learning_rate": 3.234155781325593e-05,
      "loss": 1.8844,
      "step": 277400
    },
    {
      "epoch": 21.197769459934307,
      "grad_norm": 4.149073600769043,
      "learning_rate": 3.233519211672141e-05,
      "loss": 1.7087,
      "step": 277500
    },
    {
      "epoch": 21.205408295775722,
      "grad_norm": 4.750640869140625,
      "learning_rate": 3.2328826420186895e-05,
      "loss": 1.7772,
      "step": 277600
    },
    {
      "epoch": 21.21304713161714,
      "grad_norm": 4.330711364746094,
      "learning_rate": 3.232246072365238e-05,
      "loss": 1.8093,
      "step": 277700
    },
    {
      "epoch": 21.22068596745856,
      "grad_norm": 5.185001373291016,
      "learning_rate": 3.231609502711787e-05,
      "loss": 1.815,
      "step": 277800
    },
    {
      "epoch": 21.228324803299977,
      "grad_norm": 5.8160810470581055,
      "learning_rate": 3.230972933058335e-05,
      "loss": 1.7636,
      "step": 277900
    },
    {
      "epoch": 21.235963639141396,
      "grad_norm": 6.914150238037109,
      "learning_rate": 3.2303363634048835e-05,
      "loss": 1.7936,
      "step": 278000
    },
    {
      "epoch": 21.24360247498281,
      "grad_norm": 5.828738689422607,
      "learning_rate": 3.2296997937514326e-05,
      "loss": 1.6128,
      "step": 278100
    },
    {
      "epoch": 21.25124131082423,
      "grad_norm": 4.478687763214111,
      "learning_rate": 3.229063224097981e-05,
      "loss": 1.8221,
      "step": 278200
    },
    {
      "epoch": 21.25888014666565,
      "grad_norm": 5.2806806564331055,
      "learning_rate": 3.22842665444453e-05,
      "loss": 1.7207,
      "step": 278300
    },
    {
      "epoch": 21.266518982507066,
      "grad_norm": 6.053858757019043,
      "learning_rate": 3.227790084791078e-05,
      "loss": 1.8278,
      "step": 278400
    },
    {
      "epoch": 21.274157818348485,
      "grad_norm": 5.2775654792785645,
      "learning_rate": 3.227153515137627e-05,
      "loss": 1.7503,
      "step": 278500
    },
    {
      "epoch": 21.2817966541899,
      "grad_norm": 6.417111396789551,
      "learning_rate": 3.226516945484175e-05,
      "loss": 1.8216,
      "step": 278600
    },
    {
      "epoch": 21.28943549003132,
      "grad_norm": 5.7843499183654785,
      "learning_rate": 3.225880375830724e-05,
      "loss": 1.6861,
      "step": 278700
    },
    {
      "epoch": 21.297074325872735,
      "grad_norm": 4.675653457641602,
      "learning_rate": 3.2252438061772724e-05,
      "loss": 1.6476,
      "step": 278800
    },
    {
      "epoch": 21.304713161714155,
      "grad_norm": 7.166767597198486,
      "learning_rate": 3.224607236523821e-05,
      "loss": 1.7225,
      "step": 278900
    },
    {
      "epoch": 21.312351997555574,
      "grad_norm": 5.955686092376709,
      "learning_rate": 3.223970666870369e-05,
      "loss": 1.7101,
      "step": 279000
    },
    {
      "epoch": 21.31999083339699,
      "grad_norm": 4.617900371551514,
      "learning_rate": 3.2233340972169175e-05,
      "loss": 1.7657,
      "step": 279100
    },
    {
      "epoch": 21.32762966923841,
      "grad_norm": 4.992736339569092,
      "learning_rate": 3.2226975275634665e-05,
      "loss": 1.8194,
      "step": 279200
    },
    {
      "epoch": 21.335268505079824,
      "grad_norm": 5.166881084442139,
      "learning_rate": 3.222060957910015e-05,
      "loss": 1.7545,
      "step": 279300
    },
    {
      "epoch": 21.342907340921244,
      "grad_norm": 5.192469596862793,
      "learning_rate": 3.221424388256563e-05,
      "loss": 1.7992,
      "step": 279400
    },
    {
      "epoch": 21.350546176762663,
      "grad_norm": 6.164193630218506,
      "learning_rate": 3.2207878186031116e-05,
      "loss": 1.8355,
      "step": 279500
    },
    {
      "epoch": 21.35818501260408,
      "grad_norm": 5.16742467880249,
      "learning_rate": 3.2201512489496606e-05,
      "loss": 1.697,
      "step": 279600
    },
    {
      "epoch": 21.365823848445498,
      "grad_norm": 3.786900758743286,
      "learning_rate": 3.219514679296209e-05,
      "loss": 1.8539,
      "step": 279700
    },
    {
      "epoch": 21.373462684286913,
      "grad_norm": 5.231466293334961,
      "learning_rate": 3.218878109642757e-05,
      "loss": 1.7574,
      "step": 279800
    },
    {
      "epoch": 21.381101520128333,
      "grad_norm": 7.704646110534668,
      "learning_rate": 3.2182415399893057e-05,
      "loss": 1.797,
      "step": 279900
    },
    {
      "epoch": 21.38874035596975,
      "grad_norm": 5.53226375579834,
      "learning_rate": 3.217604970335854e-05,
      "loss": 1.7902,
      "step": 280000
    },
    {
      "epoch": 21.396379191811167,
      "grad_norm": 4.464946269989014,
      "learning_rate": 3.216968400682403e-05,
      "loss": 1.8098,
      "step": 280100
    },
    {
      "epoch": 21.404018027652587,
      "grad_norm": 7.656271457672119,
      "learning_rate": 3.2163318310289514e-05,
      "loss": 1.7848,
      "step": 280200
    },
    {
      "epoch": 21.411656863494002,
      "grad_norm": 5.396100997924805,
      "learning_rate": 3.2156952613755e-05,
      "loss": 1.7872,
      "step": 280300
    },
    {
      "epoch": 21.41929569933542,
      "grad_norm": 6.3879475593566895,
      "learning_rate": 3.215058691722048e-05,
      "loss": 1.8639,
      "step": 280400
    },
    {
      "epoch": 21.42693453517684,
      "grad_norm": 5.160478115081787,
      "learning_rate": 3.214422122068597e-05,
      "loss": 1.7847,
      "step": 280500
    },
    {
      "epoch": 21.434573371018256,
      "grad_norm": 5.752984523773193,
      "learning_rate": 3.2137855524151455e-05,
      "loss": 1.8023,
      "step": 280600
    },
    {
      "epoch": 21.442212206859676,
      "grad_norm": 4.17737340927124,
      "learning_rate": 3.213148982761694e-05,
      "loss": 1.782,
      "step": 280700
    },
    {
      "epoch": 21.44985104270109,
      "grad_norm": 9.811624526977539,
      "learning_rate": 3.212512413108242e-05,
      "loss": 1.8561,
      "step": 280800
    },
    {
      "epoch": 21.45748987854251,
      "grad_norm": 6.103677272796631,
      "learning_rate": 3.2118758434547905e-05,
      "loss": 1.7649,
      "step": 280900
    },
    {
      "epoch": 21.465128714383926,
      "grad_norm": 4.800711154937744,
      "learning_rate": 3.2112392738013396e-05,
      "loss": 1.789,
      "step": 281000
    },
    {
      "epoch": 21.472767550225345,
      "grad_norm": 5.429269790649414,
      "learning_rate": 3.210602704147888e-05,
      "loss": 1.7225,
      "step": 281100
    },
    {
      "epoch": 21.480406386066765,
      "grad_norm": 4.189426898956299,
      "learning_rate": 3.209966134494436e-05,
      "loss": 1.7612,
      "step": 281200
    },
    {
      "epoch": 21.48804522190818,
      "grad_norm": 4.040805339813232,
      "learning_rate": 3.2093295648409846e-05,
      "loss": 1.7154,
      "step": 281300
    },
    {
      "epoch": 21.4956840577496,
      "grad_norm": 5.989403247833252,
      "learning_rate": 3.208692995187533e-05,
      "loss": 1.7666,
      "step": 281400
    },
    {
      "epoch": 21.503322893591015,
      "grad_norm": 4.888879776000977,
      "learning_rate": 3.208056425534082e-05,
      "loss": 1.762,
      "step": 281500
    },
    {
      "epoch": 21.510961729432434,
      "grad_norm": 5.949973106384277,
      "learning_rate": 3.2074198558806304e-05,
      "loss": 1.778,
      "step": 281600
    },
    {
      "epoch": 21.518600565273854,
      "grad_norm": 3.811854124069214,
      "learning_rate": 3.206783286227179e-05,
      "loss": 1.7552,
      "step": 281700
    },
    {
      "epoch": 21.52623940111527,
      "grad_norm": 5.1805548667907715,
      "learning_rate": 3.206146716573728e-05,
      "loss": 1.7425,
      "step": 281800
    },
    {
      "epoch": 21.53387823695669,
      "grad_norm": 5.105372428894043,
      "learning_rate": 3.205510146920276e-05,
      "loss": 1.7317,
      "step": 281900
    },
    {
      "epoch": 21.541517072798104,
      "grad_norm": 4.558998107910156,
      "learning_rate": 3.2048735772668245e-05,
      "loss": 1.774,
      "step": 282000
    },
    {
      "epoch": 21.549155908639523,
      "grad_norm": 5.60522985458374,
      "learning_rate": 3.2042370076133735e-05,
      "loss": 1.7289,
      "step": 282100
    },
    {
      "epoch": 21.556794744480943,
      "grad_norm": 4.998294353485107,
      "learning_rate": 3.203600437959922e-05,
      "loss": 1.7768,
      "step": 282200
    },
    {
      "epoch": 21.56443358032236,
      "grad_norm": 7.276837348937988,
      "learning_rate": 3.20296386830647e-05,
      "loss": 1.7895,
      "step": 282300
    },
    {
      "epoch": 21.572072416163778,
      "grad_norm": 5.734870910644531,
      "learning_rate": 3.202327298653019e-05,
      "loss": 1.7608,
      "step": 282400
    },
    {
      "epoch": 21.579711252005193,
      "grad_norm": 5.994805335998535,
      "learning_rate": 3.2016907289995676e-05,
      "loss": 1.7442,
      "step": 282500
    },
    {
      "epoch": 21.587350087846612,
      "grad_norm": 5.366870880126953,
      "learning_rate": 3.201054159346116e-05,
      "loss": 1.7195,
      "step": 282600
    },
    {
      "epoch": 21.594988923688028,
      "grad_norm": 3.850057601928711,
      "learning_rate": 3.200417589692664e-05,
      "loss": 1.7574,
      "step": 282700
    },
    {
      "epoch": 21.602627759529447,
      "grad_norm": 4.640442848205566,
      "learning_rate": 3.199781020039213e-05,
      "loss": 1.8334,
      "step": 282800
    },
    {
      "epoch": 21.610266595370867,
      "grad_norm": 5.807615756988525,
      "learning_rate": 3.199144450385762e-05,
      "loss": 1.7949,
      "step": 282900
    },
    {
      "epoch": 21.617905431212282,
      "grad_norm": 6.549623489379883,
      "learning_rate": 3.19850788073231e-05,
      "loss": 1.624,
      "step": 283000
    },
    {
      "epoch": 21.6255442670537,
      "grad_norm": 4.323608875274658,
      "learning_rate": 3.1978713110788584e-05,
      "loss": 1.775,
      "step": 283100
    },
    {
      "epoch": 21.633183102895117,
      "grad_norm": 5.449113845825195,
      "learning_rate": 3.197234741425407e-05,
      "loss": 1.8212,
      "step": 283200
    },
    {
      "epoch": 21.640821938736536,
      "grad_norm": 6.04995584487915,
      "learning_rate": 3.196598171771956e-05,
      "loss": 1.79,
      "step": 283300
    },
    {
      "epoch": 21.648460774577956,
      "grad_norm": 5.115094184875488,
      "learning_rate": 3.195961602118504e-05,
      "loss": 1.814,
      "step": 283400
    },
    {
      "epoch": 21.65609961041937,
      "grad_norm": 4.962934970855713,
      "learning_rate": 3.1953250324650525e-05,
      "loss": 1.6488,
      "step": 283500
    },
    {
      "epoch": 21.66373844626079,
      "grad_norm": 4.8924126625061035,
      "learning_rate": 3.194688462811601e-05,
      "loss": 1.7756,
      "step": 283600
    },
    {
      "epoch": 21.671377282102206,
      "grad_norm": 6.036793231964111,
      "learning_rate": 3.19405189315815e-05,
      "loss": 1.772,
      "step": 283700
    },
    {
      "epoch": 21.679016117943625,
      "grad_norm": 5.605578899383545,
      "learning_rate": 3.193415323504698e-05,
      "loss": 1.7605,
      "step": 283800
    },
    {
      "epoch": 21.686654953785045,
      "grad_norm": 5.119637966156006,
      "learning_rate": 3.1927787538512466e-05,
      "loss": 1.862,
      "step": 283900
    },
    {
      "epoch": 21.69429378962646,
      "grad_norm": 5.3116374015808105,
      "learning_rate": 3.192142184197795e-05,
      "loss": 1.7441,
      "step": 284000
    },
    {
      "epoch": 21.70193262546788,
      "grad_norm": 6.159831523895264,
      "learning_rate": 3.191505614544343e-05,
      "loss": 1.7026,
      "step": 284100
    },
    {
      "epoch": 21.709571461309295,
      "grad_norm": 6.890139579772949,
      "learning_rate": 3.190869044890892e-05,
      "loss": 1.7735,
      "step": 284200
    },
    {
      "epoch": 21.717210297150714,
      "grad_norm": 5.73054838180542,
      "learning_rate": 3.190232475237441e-05,
      "loss": 1.7829,
      "step": 284300
    },
    {
      "epoch": 21.724849132992134,
      "grad_norm": 4.965189456939697,
      "learning_rate": 3.189595905583989e-05,
      "loss": 1.8219,
      "step": 284400
    },
    {
      "epoch": 21.73248796883355,
      "grad_norm": 4.941059112548828,
      "learning_rate": 3.1889593359305374e-05,
      "loss": 1.7562,
      "step": 284500
    },
    {
      "epoch": 21.74012680467497,
      "grad_norm": 4.990627765655518,
      "learning_rate": 3.188322766277086e-05,
      "loss": 1.7491,
      "step": 284600
    },
    {
      "epoch": 21.747765640516384,
      "grad_norm": 4.6953206062316895,
      "learning_rate": 3.187686196623635e-05,
      "loss": 1.8365,
      "step": 284700
    },
    {
      "epoch": 21.755404476357803,
      "grad_norm": 6.24359655380249,
      "learning_rate": 3.187049626970183e-05,
      "loss": 1.7937,
      "step": 284800
    },
    {
      "epoch": 21.763043312199223,
      "grad_norm": 4.709331035614014,
      "learning_rate": 3.1864130573167315e-05,
      "loss": 1.7899,
      "step": 284900
    },
    {
      "epoch": 21.77068214804064,
      "grad_norm": 5.20723819732666,
      "learning_rate": 3.18577648766328e-05,
      "loss": 1.7988,
      "step": 285000
    },
    {
      "epoch": 21.778320983882058,
      "grad_norm": 6.325667381286621,
      "learning_rate": 3.185139918009829e-05,
      "loss": 1.7456,
      "step": 285100
    },
    {
      "epoch": 21.785959819723473,
      "grad_norm": 4.4387407302856445,
      "learning_rate": 3.184503348356377e-05,
      "loss": 1.7633,
      "step": 285200
    },
    {
      "epoch": 21.793598655564892,
      "grad_norm": 4.046383380889893,
      "learning_rate": 3.1838667787029256e-05,
      "loss": 1.8278,
      "step": 285300
    },
    {
      "epoch": 21.801237491406308,
      "grad_norm": 5.294003963470459,
      "learning_rate": 3.183230209049474e-05,
      "loss": 1.786,
      "step": 285400
    },
    {
      "epoch": 21.808876327247727,
      "grad_norm": 4.476635932922363,
      "learning_rate": 3.182593639396023e-05,
      "loss": 1.7763,
      "step": 285500
    },
    {
      "epoch": 21.816515163089147,
      "grad_norm": 5.410116672515869,
      "learning_rate": 3.181957069742571e-05,
      "loss": 1.7659,
      "step": 285600
    },
    {
      "epoch": 21.824153998930562,
      "grad_norm": 5.9722137451171875,
      "learning_rate": 3.1813205000891197e-05,
      "loss": 1.8558,
      "step": 285700
    },
    {
      "epoch": 21.83179283477198,
      "grad_norm": 5.615874767303467,
      "learning_rate": 3.180683930435669e-05,
      "loss": 1.7068,
      "step": 285800
    },
    {
      "epoch": 21.839431670613397,
      "grad_norm": 5.2556610107421875,
      "learning_rate": 3.180047360782217e-05,
      "loss": 1.7918,
      "step": 285900
    },
    {
      "epoch": 21.847070506454816,
      "grad_norm": 5.640233039855957,
      "learning_rate": 3.1794107911287654e-05,
      "loss": 1.7299,
      "step": 286000
    },
    {
      "epoch": 21.854709342296236,
      "grad_norm": 4.837912082672119,
      "learning_rate": 3.1787742214753144e-05,
      "loss": 1.7983,
      "step": 286100
    },
    {
      "epoch": 21.86234817813765,
      "grad_norm": 5.970785617828369,
      "learning_rate": 3.178137651821863e-05,
      "loss": 1.6876,
      "step": 286200
    },
    {
      "epoch": 21.86998701397907,
      "grad_norm": 4.531253814697266,
      "learning_rate": 3.177501082168411e-05,
      "loss": 1.7484,
      "step": 286300
    },
    {
      "epoch": 21.877625849820486,
      "grad_norm": 5.557708740234375,
      "learning_rate": 3.1768645125149595e-05,
      "loss": 1.662,
      "step": 286400
    },
    {
      "epoch": 21.885264685661905,
      "grad_norm": 6.2595977783203125,
      "learning_rate": 3.1762279428615085e-05,
      "loss": 1.7196,
      "step": 286500
    },
    {
      "epoch": 21.892903521503325,
      "grad_norm": 4.407717704772949,
      "learning_rate": 3.175591373208057e-05,
      "loss": 1.7772,
      "step": 286600
    },
    {
      "epoch": 21.90054235734474,
      "grad_norm": 6.296108722686768,
      "learning_rate": 3.174954803554605e-05,
      "loss": 1.8289,
      "step": 286700
    },
    {
      "epoch": 21.90818119318616,
      "grad_norm": 4.556016445159912,
      "learning_rate": 3.1743182339011536e-05,
      "loss": 1.7928,
      "step": 286800
    },
    {
      "epoch": 21.915820029027575,
      "grad_norm": 6.551656723022461,
      "learning_rate": 3.1736816642477026e-05,
      "loss": 1.7351,
      "step": 286900
    },
    {
      "epoch": 21.923458864868994,
      "grad_norm": 5.99371862411499,
      "learning_rate": 3.173045094594251e-05,
      "loss": 1.7935,
      "step": 287000
    },
    {
      "epoch": 21.93109770071041,
      "grad_norm": 4.797683238983154,
      "learning_rate": 3.172408524940799e-05,
      "loss": 1.8331,
      "step": 287100
    },
    {
      "epoch": 21.93873653655183,
      "grad_norm": 5.388484477996826,
      "learning_rate": 3.171771955287348e-05,
      "loss": 1.7159,
      "step": 287200
    },
    {
      "epoch": 21.94637537239325,
      "grad_norm": 6.172691345214844,
      "learning_rate": 3.171135385633896e-05,
      "loss": 1.7605,
      "step": 287300
    },
    {
      "epoch": 21.954014208234664,
      "grad_norm": 6.0105719566345215,
      "learning_rate": 3.170498815980445e-05,
      "loss": 1.7247,
      "step": 287400
    },
    {
      "epoch": 21.961653044076083,
      "grad_norm": 4.860771179199219,
      "learning_rate": 3.1698622463269934e-05,
      "loss": 1.762,
      "step": 287500
    },
    {
      "epoch": 21.9692918799175,
      "grad_norm": 3.9648854732513428,
      "learning_rate": 3.169225676673542e-05,
      "loss": 1.7231,
      "step": 287600
    },
    {
      "epoch": 21.976930715758918,
      "grad_norm": 5.974628448486328,
      "learning_rate": 3.16858910702009e-05,
      "loss": 1.777,
      "step": 287700
    },
    {
      "epoch": 21.984569551600337,
      "grad_norm": 4.975127220153809,
      "learning_rate": 3.1679525373666385e-05,
      "loss": 1.7492,
      "step": 287800
    },
    {
      "epoch": 21.992208387441753,
      "grad_norm": 7.584469318389893,
      "learning_rate": 3.1673159677131875e-05,
      "loss": 1.7909,
      "step": 287900
    },
    {
      "epoch": 21.999847223283172,
      "grad_norm": 7.697685241699219,
      "learning_rate": 3.166679398059736e-05,
      "loss": 1.7348,
      "step": 288000
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.8030052185058594,
      "eval_runtime": 3.007,
      "eval_samples_per_second": 229.466,
      "eval_steps_per_second": 229.466,
      "step": 288002
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.529585838317871,
      "eval_runtime": 57.9952,
      "eval_samples_per_second": 225.726,
      "eval_steps_per_second": 225.726,
      "step": 288002
    },
    {
      "epoch": 22.007486059124588,
      "grad_norm": 5.762602806091309,
      "learning_rate": 3.166042828406284e-05,
      "loss": 1.7076,
      "step": 288100
    },
    {
      "epoch": 22.015124894966007,
      "grad_norm": 7.311099529266357,
      "learning_rate": 3.1654062587528326e-05,
      "loss": 1.5954,
      "step": 288200
    },
    {
      "epoch": 22.022763730807426,
      "grad_norm": 6.104642868041992,
      "learning_rate": 3.1647696890993816e-05,
      "loss": 1.7917,
      "step": 288300
    },
    {
      "epoch": 22.030402566648842,
      "grad_norm": 4.039697170257568,
      "learning_rate": 3.16413311944593e-05,
      "loss": 1.8059,
      "step": 288400
    },
    {
      "epoch": 22.03804140249026,
      "grad_norm": 6.551675796508789,
      "learning_rate": 3.163496549792478e-05,
      "loss": 1.7329,
      "step": 288500
    },
    {
      "epoch": 22.045680238331677,
      "grad_norm": 5.841903209686279,
      "learning_rate": 3.1628599801390267e-05,
      "loss": 1.7594,
      "step": 288600
    },
    {
      "epoch": 22.053319074173096,
      "grad_norm": 5.395068645477295,
      "learning_rate": 3.162223410485575e-05,
      "loss": 1.7003,
      "step": 288700
    },
    {
      "epoch": 22.060957910014515,
      "grad_norm": 5.841897010803223,
      "learning_rate": 3.161586840832124e-05,
      "loss": 1.8382,
      "step": 288800
    },
    {
      "epoch": 22.06859674585593,
      "grad_norm": 5.42845344543457,
      "learning_rate": 3.1609502711786724e-05,
      "loss": 1.7178,
      "step": 288900
    },
    {
      "epoch": 22.07623558169735,
      "grad_norm": 7.270487308502197,
      "learning_rate": 3.160313701525221e-05,
      "loss": 1.7946,
      "step": 289000
    },
    {
      "epoch": 22.083874417538766,
      "grad_norm": 4.704692363739014,
      "learning_rate": 3.159677131871769e-05,
      "loss": 1.6491,
      "step": 289100
    },
    {
      "epoch": 22.091513253380185,
      "grad_norm": 5.679625034332275,
      "learning_rate": 3.159040562218318e-05,
      "loss": 1.7335,
      "step": 289200
    },
    {
      "epoch": 22.0991520892216,
      "grad_norm": 6.1383585929870605,
      "learning_rate": 3.1584039925648665e-05,
      "loss": 1.6955,
      "step": 289300
    },
    {
      "epoch": 22.10679092506302,
      "grad_norm": 4.859498023986816,
      "learning_rate": 3.157767422911415e-05,
      "loss": 1.5849,
      "step": 289400
    },
    {
      "epoch": 22.11442976090444,
      "grad_norm": 4.185042381286621,
      "learning_rate": 3.157130853257964e-05,
      "loss": 1.6999,
      "step": 289500
    },
    {
      "epoch": 22.122068596745855,
      "grad_norm": 6.190993309020996,
      "learning_rate": 3.156494283604512e-05,
      "loss": 1.766,
      "step": 289600
    },
    {
      "epoch": 22.129707432587274,
      "grad_norm": 4.333526134490967,
      "learning_rate": 3.1558577139510606e-05,
      "loss": 1.7716,
      "step": 289700
    },
    {
      "epoch": 22.13734626842869,
      "grad_norm": 5.7018280029296875,
      "learning_rate": 3.1552211442976096e-05,
      "loss": 1.7596,
      "step": 289800
    },
    {
      "epoch": 22.14498510427011,
      "grad_norm": 4.537242412567139,
      "learning_rate": 3.154584574644158e-05,
      "loss": 1.655,
      "step": 289900
    },
    {
      "epoch": 22.15262394011153,
      "grad_norm": 5.431728839874268,
      "learning_rate": 3.153948004990706e-05,
      "loss": 1.7294,
      "step": 290000
    },
    {
      "epoch": 22.160262775952944,
      "grad_norm": 5.41933012008667,
      "learning_rate": 3.153311435337255e-05,
      "loss": 1.6833,
      "step": 290100
    },
    {
      "epoch": 22.167901611794363,
      "grad_norm": 5.494882106781006,
      "learning_rate": 3.152674865683804e-05,
      "loss": 1.8643,
      "step": 290200
    },
    {
      "epoch": 22.17554044763578,
      "grad_norm": 5.3323140144348145,
      "learning_rate": 3.152038296030352e-05,
      "loss": 1.7454,
      "step": 290300
    },
    {
      "epoch": 22.183179283477198,
      "grad_norm": 5.640623092651367,
      "learning_rate": 3.1514017263769004e-05,
      "loss": 1.7664,
      "step": 290400
    },
    {
      "epoch": 22.190818119318617,
      "grad_norm": 5.860628604888916,
      "learning_rate": 3.150765156723449e-05,
      "loss": 1.8484,
      "step": 290500
    },
    {
      "epoch": 22.198456955160033,
      "grad_norm": 6.670901775360107,
      "learning_rate": 3.150128587069998e-05,
      "loss": 1.661,
      "step": 290600
    },
    {
      "epoch": 22.206095791001452,
      "grad_norm": 5.945833206176758,
      "learning_rate": 3.149492017416546e-05,
      "loss": 1.8199,
      "step": 290700
    },
    {
      "epoch": 22.213734626842868,
      "grad_norm": 4.716411590576172,
      "learning_rate": 3.1488554477630945e-05,
      "loss": 1.755,
      "step": 290800
    },
    {
      "epoch": 22.221373462684287,
      "grad_norm": 5.237460136413574,
      "learning_rate": 3.148218878109643e-05,
      "loss": 1.7782,
      "step": 290900
    },
    {
      "epoch": 22.229012298525706,
      "grad_norm": 4.843746662139893,
      "learning_rate": 3.147582308456191e-05,
      "loss": 1.7089,
      "step": 291000
    },
    {
      "epoch": 22.236651134367122,
      "grad_norm": 5.530828475952148,
      "learning_rate": 3.14694573880274e-05,
      "loss": 1.7311,
      "step": 291100
    },
    {
      "epoch": 22.24428997020854,
      "grad_norm": 6.6647186279296875,
      "learning_rate": 3.1463091691492886e-05,
      "loss": 1.7497,
      "step": 291200
    },
    {
      "epoch": 22.251928806049957,
      "grad_norm": 4.444736003875732,
      "learning_rate": 3.145672599495837e-05,
      "loss": 1.7803,
      "step": 291300
    },
    {
      "epoch": 22.259567641891376,
      "grad_norm": 7.949181079864502,
      "learning_rate": 3.145036029842385e-05,
      "loss": 1.7282,
      "step": 291400
    },
    {
      "epoch": 22.267206477732792,
      "grad_norm": 6.283308029174805,
      "learning_rate": 3.144399460188934e-05,
      "loss": 1.7029,
      "step": 291500
    },
    {
      "epoch": 22.27484531357421,
      "grad_norm": 5.889889717102051,
      "learning_rate": 3.143762890535483e-05,
      "loss": 1.805,
      "step": 291600
    },
    {
      "epoch": 22.28248414941563,
      "grad_norm": 4.3384833335876465,
      "learning_rate": 3.143126320882031e-05,
      "loss": 1.6631,
      "step": 291700
    },
    {
      "epoch": 22.290122985257046,
      "grad_norm": 5.542716026306152,
      "learning_rate": 3.1424897512285794e-05,
      "loss": 1.7039,
      "step": 291800
    },
    {
      "epoch": 22.297761821098465,
      "grad_norm": 4.433662414550781,
      "learning_rate": 3.141853181575128e-05,
      "loss": 1.8004,
      "step": 291900
    },
    {
      "epoch": 22.30540065693988,
      "grad_norm": 5.775059223175049,
      "learning_rate": 3.141216611921677e-05,
      "loss": 1.7276,
      "step": 292000
    },
    {
      "epoch": 22.3130394927813,
      "grad_norm": 5.180105209350586,
      "learning_rate": 3.140580042268225e-05,
      "loss": 1.7303,
      "step": 292100
    },
    {
      "epoch": 22.32067832862272,
      "grad_norm": 5.900928020477295,
      "learning_rate": 3.1399434726147735e-05,
      "loss": 1.784,
      "step": 292200
    },
    {
      "epoch": 22.328317164464135,
      "grad_norm": 5.2331976890563965,
      "learning_rate": 3.139306902961322e-05,
      "loss": 1.8217,
      "step": 292300
    },
    {
      "epoch": 22.335956000305554,
      "grad_norm": 5.805271625518799,
      "learning_rate": 3.138670333307871e-05,
      "loss": 1.79,
      "step": 292400
    },
    {
      "epoch": 22.34359483614697,
      "grad_norm": 5.972928524017334,
      "learning_rate": 3.138033763654419e-05,
      "loss": 1.7871,
      "step": 292500
    },
    {
      "epoch": 22.35123367198839,
      "grad_norm": 5.50914192199707,
      "learning_rate": 3.1373971940009676e-05,
      "loss": 1.7327,
      "step": 292600
    },
    {
      "epoch": 22.35887250782981,
      "grad_norm": 5.589864730834961,
      "learning_rate": 3.136760624347516e-05,
      "loss": 1.8543,
      "step": 292700
    },
    {
      "epoch": 22.366511343671224,
      "grad_norm": 3.9190833568573,
      "learning_rate": 3.136124054694064e-05,
      "loss": 1.7176,
      "step": 292800
    },
    {
      "epoch": 22.374150179512643,
      "grad_norm": 6.5657758712768555,
      "learning_rate": 3.135487485040613e-05,
      "loss": 1.7535,
      "step": 292900
    },
    {
      "epoch": 22.38178901535406,
      "grad_norm": 5.0649213790893555,
      "learning_rate": 3.134850915387162e-05,
      "loss": 1.7624,
      "step": 293000
    },
    {
      "epoch": 22.389427851195478,
      "grad_norm": 4.261684417724609,
      "learning_rate": 3.13421434573371e-05,
      "loss": 1.6906,
      "step": 293100
    },
    {
      "epoch": 22.397066687036897,
      "grad_norm": 6.841546058654785,
      "learning_rate": 3.1335777760802584e-05,
      "loss": 1.7385,
      "step": 293200
    },
    {
      "epoch": 22.404705522878313,
      "grad_norm": 4.2780303955078125,
      "learning_rate": 3.1329412064268074e-05,
      "loss": 1.7797,
      "step": 293300
    },
    {
      "epoch": 22.412344358719732,
      "grad_norm": 4.9272847175598145,
      "learning_rate": 3.132304636773356e-05,
      "loss": 1.7821,
      "step": 293400
    },
    {
      "epoch": 22.419983194561148,
      "grad_norm": 5.635711669921875,
      "learning_rate": 3.131668067119905e-05,
      "loss": 1.7409,
      "step": 293500
    },
    {
      "epoch": 22.427622030402567,
      "grad_norm": 5.839315414428711,
      "learning_rate": 3.131031497466453e-05,
      "loss": 1.7763,
      "step": 293600
    },
    {
      "epoch": 22.435260866243983,
      "grad_norm": 4.4414496421813965,
      "learning_rate": 3.1303949278130015e-05,
      "loss": 1.8013,
      "step": 293700
    },
    {
      "epoch": 22.442899702085402,
      "grad_norm": 6.1293625831604,
      "learning_rate": 3.1297583581595505e-05,
      "loss": 1.6975,
      "step": 293800
    },
    {
      "epoch": 22.45053853792682,
      "grad_norm": 4.976979732513428,
      "learning_rate": 3.129121788506099e-05,
      "loss": 1.7879,
      "step": 293900
    },
    {
      "epoch": 22.458177373768237,
      "grad_norm": 5.827627182006836,
      "learning_rate": 3.128485218852647e-05,
      "loss": 1.8273,
      "step": 294000
    },
    {
      "epoch": 22.465816209609656,
      "grad_norm": 5.311654567718506,
      "learning_rate": 3.1278486491991956e-05,
      "loss": 1.8604,
      "step": 294100
    },
    {
      "epoch": 22.47345504545107,
      "grad_norm": 6.682403564453125,
      "learning_rate": 3.127212079545744e-05,
      "loss": 1.7895,
      "step": 294200
    },
    {
      "epoch": 22.48109388129249,
      "grad_norm": 5.302765369415283,
      "learning_rate": 3.126575509892293e-05,
      "loss": 1.7875,
      "step": 294300
    },
    {
      "epoch": 22.48873271713391,
      "grad_norm": 4.829726696014404,
      "learning_rate": 3.125938940238841e-05,
      "loss": 1.8564,
      "step": 294400
    },
    {
      "epoch": 22.496371552975326,
      "grad_norm": 5.270428657531738,
      "learning_rate": 3.12530237058539e-05,
      "loss": 1.6541,
      "step": 294500
    },
    {
      "epoch": 22.504010388816745,
      "grad_norm": 4.290690898895264,
      "learning_rate": 3.124665800931938e-05,
      "loss": 1.7394,
      "step": 294600
    },
    {
      "epoch": 22.51164922465816,
      "grad_norm": 4.250433444976807,
      "learning_rate": 3.124029231278487e-05,
      "loss": 1.7321,
      "step": 294700
    },
    {
      "epoch": 22.51928806049958,
      "grad_norm": 4.669078350067139,
      "learning_rate": 3.1233926616250354e-05,
      "loss": 1.7224,
      "step": 294800
    },
    {
      "epoch": 22.526926896341,
      "grad_norm": 4.783209323883057,
      "learning_rate": 3.122756091971584e-05,
      "loss": 1.7385,
      "step": 294900
    },
    {
      "epoch": 22.534565732182415,
      "grad_norm": 6.152286052703857,
      "learning_rate": 3.122119522318132e-05,
      "loss": 1.793,
      "step": 295000
    },
    {
      "epoch": 22.542204568023834,
      "grad_norm": 6.964349269866943,
      "learning_rate": 3.1214829526646805e-05,
      "loss": 1.7832,
      "step": 295100
    },
    {
      "epoch": 22.54984340386525,
      "grad_norm": 5.416075706481934,
      "learning_rate": 3.1208463830112295e-05,
      "loss": 1.7773,
      "step": 295200
    },
    {
      "epoch": 22.55748223970667,
      "grad_norm": 6.195380687713623,
      "learning_rate": 3.120209813357778e-05,
      "loss": 1.7582,
      "step": 295300
    },
    {
      "epoch": 22.565121075548085,
      "grad_norm": 7.525324821472168,
      "learning_rate": 3.119573243704326e-05,
      "loss": 1.6823,
      "step": 295400
    },
    {
      "epoch": 22.572759911389504,
      "grad_norm": 4.328110218048096,
      "learning_rate": 3.1189366740508746e-05,
      "loss": 1.7674,
      "step": 295500
    },
    {
      "epoch": 22.580398747230923,
      "grad_norm": 5.47364616394043,
      "learning_rate": 3.118300104397423e-05,
      "loss": 1.7689,
      "step": 295600
    },
    {
      "epoch": 22.58803758307234,
      "grad_norm": 4.689483642578125,
      "learning_rate": 3.117663534743972e-05,
      "loss": 1.7399,
      "step": 295700
    },
    {
      "epoch": 22.595676418913758,
      "grad_norm": 6.6810760498046875,
      "learning_rate": 3.11702696509052e-05,
      "loss": 1.7699,
      "step": 295800
    },
    {
      "epoch": 22.603315254755174,
      "grad_norm": 4.9244561195373535,
      "learning_rate": 3.116390395437069e-05,
      "loss": 1.7379,
      "step": 295900
    },
    {
      "epoch": 22.610954090596593,
      "grad_norm": 7.180735111236572,
      "learning_rate": 3.115753825783617e-05,
      "loss": 1.8631,
      "step": 296000
    },
    {
      "epoch": 22.618592926438012,
      "grad_norm": 6.558082580566406,
      "learning_rate": 3.115117256130166e-05,
      "loss": 1.7943,
      "step": 296100
    },
    {
      "epoch": 22.626231762279428,
      "grad_norm": 5.0458292961120605,
      "learning_rate": 3.1144806864767144e-05,
      "loss": 1.7929,
      "step": 296200
    },
    {
      "epoch": 22.633870598120847,
      "grad_norm": 4.6186370849609375,
      "learning_rate": 3.113844116823263e-05,
      "loss": 1.7142,
      "step": 296300
    },
    {
      "epoch": 22.641509433962263,
      "grad_norm": 5.213414669036865,
      "learning_rate": 3.113207547169811e-05,
      "loss": 1.7707,
      "step": 296400
    },
    {
      "epoch": 22.649148269803682,
      "grad_norm": 5.716042518615723,
      "learning_rate": 3.1125709775163595e-05,
      "loss": 1.7914,
      "step": 296500
    },
    {
      "epoch": 22.6567871056451,
      "grad_norm": 5.551144599914551,
      "learning_rate": 3.1119344078629085e-05,
      "loss": 1.8397,
      "step": 296600
    },
    {
      "epoch": 22.664425941486517,
      "grad_norm": 4.1238789558410645,
      "learning_rate": 3.111297838209457e-05,
      "loss": 1.6892,
      "step": 296700
    },
    {
      "epoch": 22.672064777327936,
      "grad_norm": 6.765837669372559,
      "learning_rate": 3.110661268556005e-05,
      "loss": 1.8107,
      "step": 296800
    },
    {
      "epoch": 22.67970361316935,
      "grad_norm": 3.75042724609375,
      "learning_rate": 3.1100246989025536e-05,
      "loss": 1.8181,
      "step": 296900
    },
    {
      "epoch": 22.68734244901077,
      "grad_norm": 6.599621295928955,
      "learning_rate": 3.1093881292491026e-05,
      "loss": 1.7701,
      "step": 297000
    },
    {
      "epoch": 22.69498128485219,
      "grad_norm": 4.662594795227051,
      "learning_rate": 3.108751559595651e-05,
      "loss": 1.6852,
      "step": 297100
    },
    {
      "epoch": 22.702620120693606,
      "grad_norm": 4.491422653198242,
      "learning_rate": 3.108114989942199e-05,
      "loss": 1.84,
      "step": 297200
    },
    {
      "epoch": 22.710258956535025,
      "grad_norm": 5.838564395904541,
      "learning_rate": 3.107478420288748e-05,
      "loss": 1.7456,
      "step": 297300
    },
    {
      "epoch": 22.71789779237644,
      "grad_norm": 8.244117736816406,
      "learning_rate": 3.106841850635297e-05,
      "loss": 1.806,
      "step": 297400
    },
    {
      "epoch": 22.72553662821786,
      "grad_norm": 4.826456069946289,
      "learning_rate": 3.106205280981846e-05,
      "loss": 1.7322,
      "step": 297500
    },
    {
      "epoch": 22.73317546405928,
      "grad_norm": 7.272989273071289,
      "learning_rate": 3.105568711328394e-05,
      "loss": 1.7828,
      "step": 297600
    },
    {
      "epoch": 22.740814299900695,
      "grad_norm": 4.202924728393555,
      "learning_rate": 3.1049321416749424e-05,
      "loss": 1.7398,
      "step": 297700
    },
    {
      "epoch": 22.748453135742114,
      "grad_norm": 5.889104843139648,
      "learning_rate": 3.104295572021491e-05,
      "loss": 1.7348,
      "step": 297800
    },
    {
      "epoch": 22.75609197158353,
      "grad_norm": 6.0698065757751465,
      "learning_rate": 3.10365900236804e-05,
      "loss": 1.7481,
      "step": 297900
    },
    {
      "epoch": 22.76373080742495,
      "grad_norm": 6.1588053703308105,
      "learning_rate": 3.103022432714588e-05,
      "loss": 1.8183,
      "step": 298000
    },
    {
      "epoch": 22.771369643266365,
      "grad_norm": 6.1319732666015625,
      "learning_rate": 3.1023858630611365e-05,
      "loss": 1.7189,
      "step": 298100
    },
    {
      "epoch": 22.779008479107784,
      "grad_norm": 5.532901763916016,
      "learning_rate": 3.101749293407685e-05,
      "loss": 1.767,
      "step": 298200
    },
    {
      "epoch": 22.786647314949203,
      "grad_norm": 5.704442501068115,
      "learning_rate": 3.101112723754233e-05,
      "loss": 1.7723,
      "step": 298300
    },
    {
      "epoch": 22.79428615079062,
      "grad_norm": 4.668169975280762,
      "learning_rate": 3.100476154100782e-05,
      "loss": 1.691,
      "step": 298400
    },
    {
      "epoch": 22.801924986632038,
      "grad_norm": 4.673677444458008,
      "learning_rate": 3.0998395844473306e-05,
      "loss": 1.7632,
      "step": 298500
    },
    {
      "epoch": 22.809563822473454,
      "grad_norm": 4.881502628326416,
      "learning_rate": 3.099203014793879e-05,
      "loss": 1.7156,
      "step": 298600
    },
    {
      "epoch": 22.817202658314873,
      "grad_norm": 6.631074905395508,
      "learning_rate": 3.098566445140427e-05,
      "loss": 1.6836,
      "step": 298700
    },
    {
      "epoch": 22.824841494156292,
      "grad_norm": 3.9050445556640625,
      "learning_rate": 3.097929875486976e-05,
      "loss": 1.8212,
      "step": 298800
    },
    {
      "epoch": 22.832480329997708,
      "grad_norm": 4.230005264282227,
      "learning_rate": 3.097293305833525e-05,
      "loss": 1.7857,
      "step": 298900
    },
    {
      "epoch": 22.840119165839127,
      "grad_norm": 3.9125733375549316,
      "learning_rate": 3.096656736180073e-05,
      "loss": 1.7428,
      "step": 299000
    },
    {
      "epoch": 22.847758001680543,
      "grad_norm": 4.3132710456848145,
      "learning_rate": 3.0960201665266214e-05,
      "loss": 1.7752,
      "step": 299100
    },
    {
      "epoch": 22.855396837521962,
      "grad_norm": 5.434032917022705,
      "learning_rate": 3.09538359687317e-05,
      "loss": 1.7088,
      "step": 299200
    },
    {
      "epoch": 22.86303567336338,
      "grad_norm": 6.066683769226074,
      "learning_rate": 3.094747027219719e-05,
      "loss": 1.7467,
      "step": 299300
    },
    {
      "epoch": 22.870674509204797,
      "grad_norm": 7.643670558929443,
      "learning_rate": 3.094110457566267e-05,
      "loss": 1.7409,
      "step": 299400
    },
    {
      "epoch": 22.878313345046216,
      "grad_norm": 6.635303020477295,
      "learning_rate": 3.0934738879128155e-05,
      "loss": 1.7806,
      "step": 299500
    },
    {
      "epoch": 22.88595218088763,
      "grad_norm": 4.571168422698975,
      "learning_rate": 3.092837318259364e-05,
      "loss": 1.7459,
      "step": 299600
    },
    {
      "epoch": 22.89359101672905,
      "grad_norm": 4.170203685760498,
      "learning_rate": 3.092200748605912e-05,
      "loss": 1.716,
      "step": 299700
    },
    {
      "epoch": 22.901229852570467,
      "grad_norm": 6.192083835601807,
      "learning_rate": 3.091564178952461e-05,
      "loss": 1.7776,
      "step": 299800
    },
    {
      "epoch": 22.908868688411886,
      "grad_norm": 5.937430381774902,
      "learning_rate": 3.0909276092990096e-05,
      "loss": 1.6598,
      "step": 299900
    },
    {
      "epoch": 22.916507524253305,
      "grad_norm": 6.935107231140137,
      "learning_rate": 3.090291039645558e-05,
      "loss": 1.8248,
      "step": 300000
    },
    {
      "epoch": 22.92414636009472,
      "grad_norm": 5.93627405166626,
      "learning_rate": 3.089654469992106e-05,
      "loss": 1.8288,
      "step": 300100
    },
    {
      "epoch": 22.93178519593614,
      "grad_norm": 5.573097229003906,
      "learning_rate": 3.089017900338655e-05,
      "loss": 1.7846,
      "step": 300200
    },
    {
      "epoch": 22.939424031777556,
      "grad_norm": 4.591455936431885,
      "learning_rate": 3.088381330685204e-05,
      "loss": 1.8463,
      "step": 300300
    },
    {
      "epoch": 22.947062867618975,
      "grad_norm": 5.831401824951172,
      "learning_rate": 3.087744761031752e-05,
      "loss": 1.7024,
      "step": 300400
    },
    {
      "epoch": 22.954701703460394,
      "grad_norm": 6.6441426277160645,
      "learning_rate": 3.0871081913783004e-05,
      "loss": 1.778,
      "step": 300500
    },
    {
      "epoch": 22.96234053930181,
      "grad_norm": 5.1696882247924805,
      "learning_rate": 3.086471621724849e-05,
      "loss": 1.8343,
      "step": 300600
    },
    {
      "epoch": 22.96997937514323,
      "grad_norm": 6.7770891189575195,
      "learning_rate": 3.085835052071398e-05,
      "loss": 1.8419,
      "step": 300700
    },
    {
      "epoch": 22.977618210984645,
      "grad_norm": 6.237677097320557,
      "learning_rate": 3.085198482417946e-05,
      "loss": 1.7781,
      "step": 300800
    },
    {
      "epoch": 22.985257046826064,
      "grad_norm": 5.05625581741333,
      "learning_rate": 3.0845619127644945e-05,
      "loss": 1.8669,
      "step": 300900
    },
    {
      "epoch": 22.992895882667483,
      "grad_norm": 6.354054927825928,
      "learning_rate": 3.0839253431110435e-05,
      "loss": 1.7725,
      "step": 301000
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.7959980964660645,
      "eval_runtime": 3.032,
      "eval_samples_per_second": 227.576,
      "eval_steps_per_second": 227.576,
      "step": 301093
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.5210022926330566,
      "eval_runtime": 57.7271,
      "eval_samples_per_second": 226.774,
      "eval_steps_per_second": 226.774,
      "step": 301093
    },
    {
      "epoch": 23.0005347185089,
      "grad_norm": 4.945085048675537,
      "learning_rate": 3.083288773457592e-05,
      "loss": 1.7709,
      "step": 301100
    },
    {
      "epoch": 23.008173554350318,
      "grad_norm": 4.670012950897217,
      "learning_rate": 3.08265220380414e-05,
      "loss": 1.6944,
      "step": 301200
    },
    {
      "epoch": 23.015812390191734,
      "grad_norm": 5.140767574310303,
      "learning_rate": 3.082015634150689e-05,
      "loss": 1.8218,
      "step": 301300
    },
    {
      "epoch": 23.023451226033153,
      "grad_norm": 6.089683532714844,
      "learning_rate": 3.0813790644972376e-05,
      "loss": 1.7008,
      "step": 301400
    },
    {
      "epoch": 23.031090061874572,
      "grad_norm": 5.396432399749756,
      "learning_rate": 3.080742494843786e-05,
      "loss": 1.7065,
      "step": 301500
    },
    {
      "epoch": 23.038728897715988,
      "grad_norm": 6.063399314880371,
      "learning_rate": 3.080105925190335e-05,
      "loss": 1.7758,
      "step": 301600
    },
    {
      "epoch": 23.046367733557407,
      "grad_norm": 5.046395778656006,
      "learning_rate": 3.0794693555368833e-05,
      "loss": 1.8498,
      "step": 301700
    },
    {
      "epoch": 23.054006569398823,
      "grad_norm": 6.049987316131592,
      "learning_rate": 3.078832785883432e-05,
      "loss": 1.7001,
      "step": 301800
    },
    {
      "epoch": 23.06164540524024,
      "grad_norm": 4.987829208374023,
      "learning_rate": 3.07819621622998e-05,
      "loss": 1.7051,
      "step": 301900
    },
    {
      "epoch": 23.069284241081657,
      "grad_norm": 3.531400680541992,
      "learning_rate": 3.0775596465765284e-05,
      "loss": 1.7669,
      "step": 302000
    },
    {
      "epoch": 23.076923076923077,
      "grad_norm": 6.150291919708252,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 1.7726,
      "step": 302100
    },
    {
      "epoch": 23.084561912764496,
      "grad_norm": 3.9348385334014893,
      "learning_rate": 3.076286507269626e-05,
      "loss": 1.6301,
      "step": 302200
    },
    {
      "epoch": 23.09220074860591,
      "grad_norm": 5.2207489013671875,
      "learning_rate": 3.075649937616174e-05,
      "loss": 1.8283,
      "step": 302300
    },
    {
      "epoch": 23.09983958444733,
      "grad_norm": 9.77763557434082,
      "learning_rate": 3.0750133679627225e-05,
      "loss": 1.7992,
      "step": 302400
    },
    {
      "epoch": 23.107478420288746,
      "grad_norm": 4.071603775024414,
      "learning_rate": 3.0743767983092715e-05,
      "loss": 1.7237,
      "step": 302500
    },
    {
      "epoch": 23.115117256130166,
      "grad_norm": 5.499142646789551,
      "learning_rate": 3.07374022865582e-05,
      "loss": 1.856,
      "step": 302600
    },
    {
      "epoch": 23.122756091971585,
      "grad_norm": 5.5284624099731445,
      "learning_rate": 3.073103659002368e-05,
      "loss": 1.7574,
      "step": 302700
    },
    {
      "epoch": 23.130394927813,
      "grad_norm": 5.049781799316406,
      "learning_rate": 3.0724670893489166e-05,
      "loss": 1.7841,
      "step": 302800
    },
    {
      "epoch": 23.13803376365442,
      "grad_norm": 5.844510555267334,
      "learning_rate": 3.071830519695465e-05,
      "loss": 1.7868,
      "step": 302900
    },
    {
      "epoch": 23.145672599495835,
      "grad_norm": 4.441913604736328,
      "learning_rate": 3.071193950042014e-05,
      "loss": 1.6262,
      "step": 303000
    },
    {
      "epoch": 23.153311435337255,
      "grad_norm": 5.677403450012207,
      "learning_rate": 3.070557380388562e-05,
      "loss": 1.7319,
      "step": 303100
    },
    {
      "epoch": 23.160950271178674,
      "grad_norm": 5.471381664276123,
      "learning_rate": 3.069920810735111e-05,
      "loss": 1.7521,
      "step": 303200
    },
    {
      "epoch": 23.16858910702009,
      "grad_norm": 5.449193954467773,
      "learning_rate": 3.069284241081659e-05,
      "loss": 1.7104,
      "step": 303300
    },
    {
      "epoch": 23.17622794286151,
      "grad_norm": 5.150532245635986,
      "learning_rate": 3.068647671428208e-05,
      "loss": 1.7863,
      "step": 303400
    },
    {
      "epoch": 23.183866778702924,
      "grad_norm": 6.986101150512695,
      "learning_rate": 3.0680111017747564e-05,
      "loss": 1.801,
      "step": 303500
    },
    {
      "epoch": 23.191505614544344,
      "grad_norm": 4.707923412322998,
      "learning_rate": 3.067374532121305e-05,
      "loss": 1.6645,
      "step": 303600
    },
    {
      "epoch": 23.199144450385763,
      "grad_norm": 4.56549072265625,
      "learning_rate": 3.066737962467853e-05,
      "loss": 1.719,
      "step": 303700
    },
    {
      "epoch": 23.20678328622718,
      "grad_norm": 5.04283332824707,
      "learning_rate": 3.0661013928144015e-05,
      "loss": 1.7512,
      "step": 303800
    },
    {
      "epoch": 23.214422122068598,
      "grad_norm": 4.992753982543945,
      "learning_rate": 3.0654648231609505e-05,
      "loss": 1.7361,
      "step": 303900
    },
    {
      "epoch": 23.222060957910013,
      "grad_norm": 5.272355079650879,
      "learning_rate": 3.064828253507499e-05,
      "loss": 1.7611,
      "step": 304000
    },
    {
      "epoch": 23.229699793751433,
      "grad_norm": 4.176757335662842,
      "learning_rate": 3.064191683854047e-05,
      "loss": 1.6904,
      "step": 304100
    },
    {
      "epoch": 23.23733862959285,
      "grad_norm": 5.895267486572266,
      "learning_rate": 3.0635551142005956e-05,
      "loss": 1.647,
      "step": 304200
    },
    {
      "epoch": 23.244977465434268,
      "grad_norm": 4.443841934204102,
      "learning_rate": 3.062918544547144e-05,
      "loss": 1.83,
      "step": 304300
    },
    {
      "epoch": 23.252616301275687,
      "grad_norm": 4.301595211029053,
      "learning_rate": 3.062281974893693e-05,
      "loss": 1.7406,
      "step": 304400
    },
    {
      "epoch": 23.260255137117102,
      "grad_norm": 5.749722480773926,
      "learning_rate": 3.061645405240241e-05,
      "loss": 1.7801,
      "step": 304500
    },
    {
      "epoch": 23.26789397295852,
      "grad_norm": 6.5456085205078125,
      "learning_rate": 3.06100883558679e-05,
      "loss": 1.8047,
      "step": 304600
    },
    {
      "epoch": 23.275532808799937,
      "grad_norm": 4.942381858825684,
      "learning_rate": 3.060372265933339e-05,
      "loss": 1.7227,
      "step": 304700
    },
    {
      "epoch": 23.283171644641357,
      "grad_norm": 4.382545471191406,
      "learning_rate": 3.059735696279887e-05,
      "loss": 1.7165,
      "step": 304800
    },
    {
      "epoch": 23.290810480482776,
      "grad_norm": 6.367372035980225,
      "learning_rate": 3.0590991266264354e-05,
      "loss": 1.6349,
      "step": 304900
    },
    {
      "epoch": 23.29844931632419,
      "grad_norm": 4.645062446594238,
      "learning_rate": 3.0584625569729844e-05,
      "loss": 1.7885,
      "step": 305000
    },
    {
      "epoch": 23.30608815216561,
      "grad_norm": 7.262628078460693,
      "learning_rate": 3.057825987319533e-05,
      "loss": 1.7282,
      "step": 305100
    },
    {
      "epoch": 23.313726988007026,
      "grad_norm": 5.573502063751221,
      "learning_rate": 3.057189417666081e-05,
      "loss": 1.7528,
      "step": 305200
    },
    {
      "epoch": 23.321365823848446,
      "grad_norm": 4.755427837371826,
      "learning_rate": 3.05655284801263e-05,
      "loss": 1.726,
      "step": 305300
    },
    {
      "epoch": 23.329004659689865,
      "grad_norm": 4.765389442443848,
      "learning_rate": 3.0559162783591785e-05,
      "loss": 1.83,
      "step": 305400
    },
    {
      "epoch": 23.33664349553128,
      "grad_norm": 5.235108852386475,
      "learning_rate": 3.055279708705727e-05,
      "loss": 1.7828,
      "step": 305500
    },
    {
      "epoch": 23.3442823313727,
      "grad_norm": 5.211676120758057,
      "learning_rate": 3.054643139052275e-05,
      "loss": 1.6972,
      "step": 305600
    },
    {
      "epoch": 23.351921167214115,
      "grad_norm": 7.691896438598633,
      "learning_rate": 3.054006569398824e-05,
      "loss": 1.7551,
      "step": 305700
    },
    {
      "epoch": 23.359560003055535,
      "grad_norm": 6.206045627593994,
      "learning_rate": 3.0533699997453726e-05,
      "loss": 1.6731,
      "step": 305800
    },
    {
      "epoch": 23.367198838896954,
      "grad_norm": 5.602575302124023,
      "learning_rate": 3.052733430091921e-05,
      "loss": 1.7794,
      "step": 305900
    },
    {
      "epoch": 23.37483767473837,
      "grad_norm": 4.75175666809082,
      "learning_rate": 3.052096860438469e-05,
      "loss": 1.7033,
      "step": 306000
    },
    {
      "epoch": 23.38247651057979,
      "grad_norm": 5.988614559173584,
      "learning_rate": 3.0514602907850177e-05,
      "loss": 1.8203,
      "step": 306100
    },
    {
      "epoch": 23.390115346421204,
      "grad_norm": 4.32045841217041,
      "learning_rate": 3.0508237211315667e-05,
      "loss": 1.7023,
      "step": 306200
    },
    {
      "epoch": 23.397754182262624,
      "grad_norm": 8.912705421447754,
      "learning_rate": 3.050187151478115e-05,
      "loss": 1.7977,
      "step": 306300
    },
    {
      "epoch": 23.40539301810404,
      "grad_norm": 5.761802673339844,
      "learning_rate": 3.0495505818246634e-05,
      "loss": 1.7937,
      "step": 306400
    },
    {
      "epoch": 23.41303185394546,
      "grad_norm": 5.6674485206604,
      "learning_rate": 3.0489140121712118e-05,
      "loss": 1.8026,
      "step": 306500
    },
    {
      "epoch": 23.420670689786878,
      "grad_norm": 5.887609481811523,
      "learning_rate": 3.0482774425177608e-05,
      "loss": 1.8697,
      "step": 306600
    },
    {
      "epoch": 23.428309525628293,
      "grad_norm": 7.1447834968566895,
      "learning_rate": 3.047640872864309e-05,
      "loss": 1.814,
      "step": 306700
    },
    {
      "epoch": 23.435948361469713,
      "grad_norm": 5.532761096954346,
      "learning_rate": 3.0470043032108575e-05,
      "loss": 1.7745,
      "step": 306800
    },
    {
      "epoch": 23.44358719731113,
      "grad_norm": 6.281663417816162,
      "learning_rate": 3.046367733557406e-05,
      "loss": 1.6319,
      "step": 306900
    },
    {
      "epoch": 23.451226033152548,
      "grad_norm": 4.928821086883545,
      "learning_rate": 3.0457311639039542e-05,
      "loss": 1.7022,
      "step": 307000
    },
    {
      "epoch": 23.458864868993967,
      "grad_norm": 4.2029008865356445,
      "learning_rate": 3.0450945942505033e-05,
      "loss": 1.6613,
      "step": 307100
    },
    {
      "epoch": 23.466503704835382,
      "grad_norm": 5.905494689941406,
      "learning_rate": 3.0444580245970516e-05,
      "loss": 1.7056,
      "step": 307200
    },
    {
      "epoch": 23.4741425406768,
      "grad_norm": 6.875413417816162,
      "learning_rate": 3.0438214549436e-05,
      "loss": 1.7262,
      "step": 307300
    },
    {
      "epoch": 23.481781376518217,
      "grad_norm": 5.918224334716797,
      "learning_rate": 3.0431848852901483e-05,
      "loss": 1.9027,
      "step": 307400
    },
    {
      "epoch": 23.489420212359637,
      "grad_norm": 6.145145893096924,
      "learning_rate": 3.042548315636697e-05,
      "loss": 1.8184,
      "step": 307500
    },
    {
      "epoch": 23.497059048201056,
      "grad_norm": 6.352155685424805,
      "learning_rate": 3.0419117459832457e-05,
      "loss": 1.8151,
      "step": 307600
    },
    {
      "epoch": 23.50469788404247,
      "grad_norm": 6.891397953033447,
      "learning_rate": 3.041275176329794e-05,
      "loss": 1.7706,
      "step": 307700
    },
    {
      "epoch": 23.51233671988389,
      "grad_norm": 5.262948989868164,
      "learning_rate": 3.0406386066763427e-05,
      "loss": 1.6747,
      "step": 307800
    },
    {
      "epoch": 23.519975555725306,
      "grad_norm": 5.377205848693848,
      "learning_rate": 3.040002037022891e-05,
      "loss": 1.7368,
      "step": 307900
    },
    {
      "epoch": 23.527614391566726,
      "grad_norm": 4.829653263092041,
      "learning_rate": 3.0393654673694398e-05,
      "loss": 1.7487,
      "step": 308000
    },
    {
      "epoch": 23.53525322740814,
      "grad_norm": 5.545378684997559,
      "learning_rate": 3.0387288977159885e-05,
      "loss": 1.8575,
      "step": 308100
    },
    {
      "epoch": 23.54289206324956,
      "grad_norm": 5.729093074798584,
      "learning_rate": 3.038092328062537e-05,
      "loss": 1.6668,
      "step": 308200
    },
    {
      "epoch": 23.55053089909098,
      "grad_norm": 4.185577869415283,
      "learning_rate": 3.0374557584090852e-05,
      "loss": 1.7426,
      "step": 308300
    },
    {
      "epoch": 23.558169734932395,
      "grad_norm": 5.552777290344238,
      "learning_rate": 3.0368191887556335e-05,
      "loss": 1.7722,
      "step": 308400
    },
    {
      "epoch": 23.565808570773815,
      "grad_norm": 4.253104209899902,
      "learning_rate": 3.0361826191021826e-05,
      "loss": 1.755,
      "step": 308500
    },
    {
      "epoch": 23.57344740661523,
      "grad_norm": 5.6331562995910645,
      "learning_rate": 3.035546049448731e-05,
      "loss": 1.7491,
      "step": 308600
    },
    {
      "epoch": 23.58108624245665,
      "grad_norm": 4.981155872344971,
      "learning_rate": 3.0349094797952793e-05,
      "loss": 1.8088,
      "step": 308700
    },
    {
      "epoch": 23.58872507829807,
      "grad_norm": 4.1587910652160645,
      "learning_rate": 3.0342729101418276e-05,
      "loss": 1.7775,
      "step": 308800
    },
    {
      "epoch": 23.596363914139484,
      "grad_norm": 5.4190778732299805,
      "learning_rate": 3.0336363404883767e-05,
      "loss": 1.7415,
      "step": 308900
    },
    {
      "epoch": 23.604002749980904,
      "grad_norm": 4.598272323608398,
      "learning_rate": 3.032999770834925e-05,
      "loss": 1.8149,
      "step": 309000
    },
    {
      "epoch": 23.61164158582232,
      "grad_norm": 4.7748703956604,
      "learning_rate": 3.0323632011814734e-05,
      "loss": 1.765,
      "step": 309100
    },
    {
      "epoch": 23.61928042166374,
      "grad_norm": 5.2638325691223145,
      "learning_rate": 3.0317266315280217e-05,
      "loss": 1.7545,
      "step": 309200
    },
    {
      "epoch": 23.626919257505158,
      "grad_norm": 7.014258861541748,
      "learning_rate": 3.03109006187457e-05,
      "loss": 1.813,
      "step": 309300
    },
    {
      "epoch": 23.634558093346573,
      "grad_norm": 4.823853969573975,
      "learning_rate": 3.030453492221119e-05,
      "loss": 1.7561,
      "step": 309400
    },
    {
      "epoch": 23.642196929187993,
      "grad_norm": 5.753990650177002,
      "learning_rate": 3.0298169225676675e-05,
      "loss": 1.7313,
      "step": 309500
    },
    {
      "epoch": 23.649835765029408,
      "grad_norm": 6.053552150726318,
      "learning_rate": 3.0291803529142158e-05,
      "loss": 1.7084,
      "step": 309600
    },
    {
      "epoch": 23.657474600870827,
      "grad_norm": 6.666933536529541,
      "learning_rate": 3.0285437832607645e-05,
      "loss": 1.7379,
      "step": 309700
    },
    {
      "epoch": 23.665113436712247,
      "grad_norm": 5.118027687072754,
      "learning_rate": 3.0279072136073132e-05,
      "loss": 1.7723,
      "step": 309800
    },
    {
      "epoch": 23.672752272553662,
      "grad_norm": 5.079283714294434,
      "learning_rate": 3.027270643953862e-05,
      "loss": 1.8014,
      "step": 309900
    },
    {
      "epoch": 23.68039110839508,
      "grad_norm": 4.746374607086182,
      "learning_rate": 3.0266340743004103e-05,
      "loss": 1.7173,
      "step": 310000
    },
    {
      "epoch": 23.688029944236497,
      "grad_norm": 5.3923115730285645,
      "learning_rate": 3.0259975046469586e-05,
      "loss": 1.7395,
      "step": 310100
    },
    {
      "epoch": 23.695668780077916,
      "grad_norm": 6.156134605407715,
      "learning_rate": 3.025360934993507e-05,
      "loss": 1.7073,
      "step": 310200
    },
    {
      "epoch": 23.703307615919336,
      "grad_norm": 6.779321193695068,
      "learning_rate": 3.024724365340056e-05,
      "loss": 1.7282,
      "step": 310300
    },
    {
      "epoch": 23.71094645176075,
      "grad_norm": 5.989250659942627,
      "learning_rate": 3.0240877956866043e-05,
      "loss": 1.8475,
      "step": 310400
    },
    {
      "epoch": 23.71858528760217,
      "grad_norm": 7.025508880615234,
      "learning_rate": 3.0234512260331527e-05,
      "loss": 1.7606,
      "step": 310500
    },
    {
      "epoch": 23.726224123443586,
      "grad_norm": 6.310924530029297,
      "learning_rate": 3.022814656379701e-05,
      "loss": 1.7422,
      "step": 310600
    },
    {
      "epoch": 23.733862959285005,
      "grad_norm": 6.442338943481445,
      "learning_rate": 3.0221780867262494e-05,
      "loss": 1.7814,
      "step": 310700
    },
    {
      "epoch": 23.74150179512642,
      "grad_norm": 6.841838836669922,
      "learning_rate": 3.0215415170727984e-05,
      "loss": 1.8696,
      "step": 310800
    },
    {
      "epoch": 23.74914063096784,
      "grad_norm": 5.344551086425781,
      "learning_rate": 3.0209049474193468e-05,
      "loss": 1.8033,
      "step": 310900
    },
    {
      "epoch": 23.75677946680926,
      "grad_norm": 4.287937641143799,
      "learning_rate": 3.020268377765895e-05,
      "loss": 1.7009,
      "step": 311000
    },
    {
      "epoch": 23.764418302650675,
      "grad_norm": 7.620766639709473,
      "learning_rate": 3.0196318081124435e-05,
      "loss": 1.7701,
      "step": 311100
    },
    {
      "epoch": 23.772057138492094,
      "grad_norm": 5.583759784698486,
      "learning_rate": 3.0189952384589925e-05,
      "loss": 1.7404,
      "step": 311200
    },
    {
      "epoch": 23.77969597433351,
      "grad_norm": 5.092541217803955,
      "learning_rate": 3.018358668805541e-05,
      "loss": 1.769,
      "step": 311300
    },
    {
      "epoch": 23.78733481017493,
      "grad_norm": 4.3887434005737305,
      "learning_rate": 3.0177220991520892e-05,
      "loss": 1.7101,
      "step": 311400
    },
    {
      "epoch": 23.79497364601635,
      "grad_norm": 5.02505350112915,
      "learning_rate": 3.017085529498638e-05,
      "loss": 1.6734,
      "step": 311500
    },
    {
      "epoch": 23.802612481857764,
      "grad_norm": 5.978891372680664,
      "learning_rate": 3.0164489598451863e-05,
      "loss": 1.816,
      "step": 311600
    },
    {
      "epoch": 23.810251317699183,
      "grad_norm": 5.283752918243408,
      "learning_rate": 3.015812390191735e-05,
      "loss": 1.7063,
      "step": 311700
    },
    {
      "epoch": 23.8178901535406,
      "grad_norm": 6.503390312194824,
      "learning_rate": 3.0151758205382837e-05,
      "loss": 1.7144,
      "step": 311800
    },
    {
      "epoch": 23.82552898938202,
      "grad_norm": 4.680459022521973,
      "learning_rate": 3.014539250884832e-05,
      "loss": 1.731,
      "step": 311900
    },
    {
      "epoch": 23.833167825223438,
      "grad_norm": 7.80552864074707,
      "learning_rate": 3.0139026812313804e-05,
      "loss": 1.8551,
      "step": 312000
    },
    {
      "epoch": 23.840806661064853,
      "grad_norm": 6.295780658721924,
      "learning_rate": 3.0132661115779294e-05,
      "loss": 1.781,
      "step": 312100
    },
    {
      "epoch": 23.848445496906272,
      "grad_norm": 5.253237724304199,
      "learning_rate": 3.0126295419244778e-05,
      "loss": 1.7455,
      "step": 312200
    },
    {
      "epoch": 23.856084332747688,
      "grad_norm": 5.201444625854492,
      "learning_rate": 3.011992972271026e-05,
      "loss": 1.6898,
      "step": 312300
    },
    {
      "epoch": 23.863723168589107,
      "grad_norm": 5.377193450927734,
      "learning_rate": 3.0113564026175745e-05,
      "loss": 1.7094,
      "step": 312400
    },
    {
      "epoch": 23.871362004430523,
      "grad_norm": 6.056978225708008,
      "learning_rate": 3.0107198329641228e-05,
      "loss": 1.6596,
      "step": 312500
    },
    {
      "epoch": 23.879000840271942,
      "grad_norm": 4.015249729156494,
      "learning_rate": 3.010083263310672e-05,
      "loss": 1.7883,
      "step": 312600
    },
    {
      "epoch": 23.88663967611336,
      "grad_norm": 6.7789716720581055,
      "learning_rate": 3.0094466936572202e-05,
      "loss": 1.7473,
      "step": 312700
    },
    {
      "epoch": 23.894278511954777,
      "grad_norm": 5.35807991027832,
      "learning_rate": 3.0088101240037686e-05,
      "loss": 1.6873,
      "step": 312800
    },
    {
      "epoch": 23.901917347796196,
      "grad_norm": 4.6374616622924805,
      "learning_rate": 3.008173554350317e-05,
      "loss": 1.7671,
      "step": 312900
    },
    {
      "epoch": 23.909556183637612,
      "grad_norm": 5.796525478363037,
      "learning_rate": 3.0075369846968653e-05,
      "loss": 1.7178,
      "step": 313000
    },
    {
      "epoch": 23.91719501947903,
      "grad_norm": 5.1785759925842285,
      "learning_rate": 3.0069004150434143e-05,
      "loss": 1.6615,
      "step": 313100
    },
    {
      "epoch": 23.92483385532045,
      "grad_norm": 3.8846399784088135,
      "learning_rate": 3.0062638453899626e-05,
      "loss": 1.8475,
      "step": 313200
    },
    {
      "epoch": 23.932472691161866,
      "grad_norm": 6.695642471313477,
      "learning_rate": 3.005627275736511e-05,
      "loss": 1.7504,
      "step": 313300
    },
    {
      "epoch": 23.940111527003285,
      "grad_norm": 4.925021171569824,
      "learning_rate": 3.0049907060830597e-05,
      "loss": 1.7307,
      "step": 313400
    },
    {
      "epoch": 23.9477503628447,
      "grad_norm": 5.917128562927246,
      "learning_rate": 3.0043541364296084e-05,
      "loss": 1.7426,
      "step": 313500
    },
    {
      "epoch": 23.95538919868612,
      "grad_norm": 5.54699182510376,
      "learning_rate": 3.0037175667761567e-05,
      "loss": 1.7505,
      "step": 313600
    },
    {
      "epoch": 23.96302803452754,
      "grad_norm": 4.932650566101074,
      "learning_rate": 3.0030809971227054e-05,
      "loss": 1.7595,
      "step": 313700
    },
    {
      "epoch": 23.970666870368955,
      "grad_norm": 5.301932334899902,
      "learning_rate": 3.0024444274692538e-05,
      "loss": 1.8031,
      "step": 313800
    },
    {
      "epoch": 23.978305706210374,
      "grad_norm": 6.628185749053955,
      "learning_rate": 3.001807857815802e-05,
      "loss": 1.7778,
      "step": 313900
    },
    {
      "epoch": 23.98594454205179,
      "grad_norm": 4.805454254150391,
      "learning_rate": 3.0011712881623512e-05,
      "loss": 1.7227,
      "step": 314000
    },
    {
      "epoch": 23.99358337789321,
      "grad_norm": 5.076239585876465,
      "learning_rate": 3.0005347185088995e-05,
      "loss": 1.7829,
      "step": 314100
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.7963998317718506,
      "eval_runtime": 3.0078,
      "eval_samples_per_second": 229.407,
      "eval_steps_per_second": 229.407,
      "step": 314184
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.5179097652435303,
      "eval_runtime": 58.2151,
      "eval_samples_per_second": 224.873,
      "eval_steps_per_second": 224.873,
      "step": 314184
    },
    {
      "epoch": 24.00122221373463,
      "grad_norm": 5.723301410675049,
      "learning_rate": 2.999898148855448e-05,
      "loss": 1.766,
      "step": 314200
    },
    {
      "epoch": 24.008861049576044,
      "grad_norm": 6.646219730377197,
      "learning_rate": 2.9992615792019962e-05,
      "loss": 1.6913,
      "step": 314300
    },
    {
      "epoch": 24.016499885417463,
      "grad_norm": 5.809302806854248,
      "learning_rate": 2.9986250095485453e-05,
      "loss": 1.7576,
      "step": 314400
    },
    {
      "epoch": 24.02413872125888,
      "grad_norm": 4.704820156097412,
      "learning_rate": 2.9979884398950936e-05,
      "loss": 1.7604,
      "step": 314500
    },
    {
      "epoch": 24.0317775571003,
      "grad_norm": 6.203147888183594,
      "learning_rate": 2.997351870241642e-05,
      "loss": 1.7473,
      "step": 314600
    },
    {
      "epoch": 24.039416392941714,
      "grad_norm": 6.1783013343811035,
      "learning_rate": 2.9967153005881903e-05,
      "loss": 1.7329,
      "step": 314700
    },
    {
      "epoch": 24.047055228783133,
      "grad_norm": 6.2152099609375,
      "learning_rate": 2.9960787309347387e-05,
      "loss": 1.7207,
      "step": 314800
    },
    {
      "epoch": 24.054694064624552,
      "grad_norm": 5.840861797332764,
      "learning_rate": 2.9954421612812877e-05,
      "loss": 1.7008,
      "step": 314900
    },
    {
      "epoch": 24.062332900465968,
      "grad_norm": 4.8773064613342285,
      "learning_rate": 2.994805591627836e-05,
      "loss": 1.6356,
      "step": 315000
    },
    {
      "epoch": 24.069971736307387,
      "grad_norm": 4.968263626098633,
      "learning_rate": 2.9941690219743844e-05,
      "loss": 1.8346,
      "step": 315100
    },
    {
      "epoch": 24.077610572148803,
      "grad_norm": 4.805960655212402,
      "learning_rate": 2.9935324523209328e-05,
      "loss": 1.7166,
      "step": 315200
    },
    {
      "epoch": 24.085249407990222,
      "grad_norm": 4.417869567871094,
      "learning_rate": 2.9928958826674818e-05,
      "loss": 1.6062,
      "step": 315300
    },
    {
      "epoch": 24.09288824383164,
      "grad_norm": 5.844922065734863,
      "learning_rate": 2.99225931301403e-05,
      "loss": 1.7417,
      "step": 315400
    },
    {
      "epoch": 24.100527079673057,
      "grad_norm": 4.948807716369629,
      "learning_rate": 2.991622743360579e-05,
      "loss": 1.8086,
      "step": 315500
    },
    {
      "epoch": 24.108165915514476,
      "grad_norm": 5.836705207824707,
      "learning_rate": 2.9909861737071272e-05,
      "loss": 1.6683,
      "step": 315600
    },
    {
      "epoch": 24.115804751355892,
      "grad_norm": 6.662283897399902,
      "learning_rate": 2.9903496040536756e-05,
      "loss": 1.6966,
      "step": 315700
    },
    {
      "epoch": 24.12344358719731,
      "grad_norm": 5.79251766204834,
      "learning_rate": 2.9897130344002246e-05,
      "loss": 1.7226,
      "step": 315800
    },
    {
      "epoch": 24.13108242303873,
      "grad_norm": 6.178858280181885,
      "learning_rate": 2.989076464746773e-05,
      "loss": 1.8647,
      "step": 315900
    },
    {
      "epoch": 24.138721258880146,
      "grad_norm": 5.97304630279541,
      "learning_rate": 2.9884398950933213e-05,
      "loss": 1.7181,
      "step": 316000
    },
    {
      "epoch": 24.146360094721565,
      "grad_norm": 5.816867828369141,
      "learning_rate": 2.9878033254398696e-05,
      "loss": 1.8269,
      "step": 316100
    },
    {
      "epoch": 24.15399893056298,
      "grad_norm": 3.646364688873291,
      "learning_rate": 2.987166755786418e-05,
      "loss": 1.7464,
      "step": 316200
    },
    {
      "epoch": 24.1616377664044,
      "grad_norm": 5.13368034362793,
      "learning_rate": 2.986530186132967e-05,
      "loss": 1.7657,
      "step": 316300
    },
    {
      "epoch": 24.16927660224582,
      "grad_norm": 4.749722480773926,
      "learning_rate": 2.9858936164795154e-05,
      "loss": 1.8311,
      "step": 316400
    },
    {
      "epoch": 24.176915438087235,
      "grad_norm": 5.314711570739746,
      "learning_rate": 2.9852570468260637e-05,
      "loss": 1.7028,
      "step": 316500
    },
    {
      "epoch": 24.184554273928654,
      "grad_norm": 6.855350971221924,
      "learning_rate": 2.984620477172612e-05,
      "loss": 1.7534,
      "step": 316600
    },
    {
      "epoch": 24.19219310977007,
      "grad_norm": 4.834742069244385,
      "learning_rate": 2.983983907519161e-05,
      "loss": 1.6896,
      "step": 316700
    },
    {
      "epoch": 24.19983194561149,
      "grad_norm": 6.519028186798096,
      "learning_rate": 2.9833473378657095e-05,
      "loss": 1.784,
      "step": 316800
    },
    {
      "epoch": 24.207470781452905,
      "grad_norm": 5.18242073059082,
      "learning_rate": 2.982710768212258e-05,
      "loss": 1.7079,
      "step": 316900
    },
    {
      "epoch": 24.215109617294324,
      "grad_norm": 7.042185306549072,
      "learning_rate": 2.9820741985588062e-05,
      "loss": 1.7763,
      "step": 317000
    },
    {
      "epoch": 24.222748453135743,
      "grad_norm": 5.756460189819336,
      "learning_rate": 2.981437628905355e-05,
      "loss": 1.6665,
      "step": 317100
    },
    {
      "epoch": 24.23038728897716,
      "grad_norm": 7.26433801651001,
      "learning_rate": 2.9808010592519036e-05,
      "loss": 1.7698,
      "step": 317200
    },
    {
      "epoch": 24.238026124818578,
      "grad_norm": 4.666209697723389,
      "learning_rate": 2.980164489598452e-05,
      "loss": 1.6923,
      "step": 317300
    },
    {
      "epoch": 24.245664960659994,
      "grad_norm": 5.028308868408203,
      "learning_rate": 2.9795279199450006e-05,
      "loss": 1.6791,
      "step": 317400
    },
    {
      "epoch": 24.253303796501413,
      "grad_norm": 6.150525093078613,
      "learning_rate": 2.978891350291549e-05,
      "loss": 1.7549,
      "step": 317500
    },
    {
      "epoch": 24.260942632342832,
      "grad_norm": 5.292896747589111,
      "learning_rate": 2.9782547806380977e-05,
      "loss": 1.8053,
      "step": 317600
    },
    {
      "epoch": 24.268581468184248,
      "grad_norm": 4.774625301361084,
      "learning_rate": 2.9776182109846464e-05,
      "loss": 1.6391,
      "step": 317700
    },
    {
      "epoch": 24.276220304025667,
      "grad_norm": 7.488475799560547,
      "learning_rate": 2.9769816413311947e-05,
      "loss": 1.7459,
      "step": 317800
    },
    {
      "epoch": 24.283859139867083,
      "grad_norm": 5.226603984832764,
      "learning_rate": 2.976345071677743e-05,
      "loss": 1.78,
      "step": 317900
    },
    {
      "epoch": 24.291497975708502,
      "grad_norm": 6.277462482452393,
      "learning_rate": 2.9757085020242914e-05,
      "loss": 1.7343,
      "step": 318000
    },
    {
      "epoch": 24.29913681154992,
      "grad_norm": 4.963732719421387,
      "learning_rate": 2.9750719323708404e-05,
      "loss": 1.7618,
      "step": 318100
    },
    {
      "epoch": 24.306775647391337,
      "grad_norm": 5.23192024230957,
      "learning_rate": 2.9744353627173888e-05,
      "loss": 1.751,
      "step": 318200
    },
    {
      "epoch": 24.314414483232756,
      "grad_norm": 6.953038215637207,
      "learning_rate": 2.973798793063937e-05,
      "loss": 1.7376,
      "step": 318300
    },
    {
      "epoch": 24.322053319074172,
      "grad_norm": 4.886464595794678,
      "learning_rate": 2.9731622234104855e-05,
      "loss": 1.8416,
      "step": 318400
    },
    {
      "epoch": 24.32969215491559,
      "grad_norm": 4.974604606628418,
      "learning_rate": 2.9725256537570345e-05,
      "loss": 1.7314,
      "step": 318500
    },
    {
      "epoch": 24.33733099075701,
      "grad_norm": 5.188365936279297,
      "learning_rate": 2.971889084103583e-05,
      "loss": 1.7113,
      "step": 318600
    },
    {
      "epoch": 24.344969826598426,
      "grad_norm": 5.863614559173584,
      "learning_rate": 2.9712525144501312e-05,
      "loss": 1.8047,
      "step": 318700
    },
    {
      "epoch": 24.352608662439845,
      "grad_norm": 5.597905158996582,
      "learning_rate": 2.9706159447966796e-05,
      "loss": 1.7272,
      "step": 318800
    },
    {
      "epoch": 24.36024749828126,
      "grad_norm": 7.222692012786865,
      "learning_rate": 2.969979375143228e-05,
      "loss": 1.7061,
      "step": 318900
    },
    {
      "epoch": 24.36788633412268,
      "grad_norm": 7.724486827850342,
      "learning_rate": 2.969342805489777e-05,
      "loss": 1.6961,
      "step": 319000
    },
    {
      "epoch": 24.375525169964096,
      "grad_norm": 3.885791540145874,
      "learning_rate": 2.9687062358363253e-05,
      "loss": 1.6903,
      "step": 319100
    },
    {
      "epoch": 24.383164005805515,
      "grad_norm": 5.718804836273193,
      "learning_rate": 2.9680696661828737e-05,
      "loss": 1.7571,
      "step": 319200
    },
    {
      "epoch": 24.390802841646934,
      "grad_norm": 5.7466535568237305,
      "learning_rate": 2.9674330965294224e-05,
      "loss": 1.7342,
      "step": 319300
    },
    {
      "epoch": 24.39844167748835,
      "grad_norm": 6.1328959465026855,
      "learning_rate": 2.9667965268759707e-05,
      "loss": 1.7662,
      "step": 319400
    },
    {
      "epoch": 24.40608051332977,
      "grad_norm": 5.852446556091309,
      "learning_rate": 2.9661599572225194e-05,
      "loss": 1.8395,
      "step": 319500
    },
    {
      "epoch": 24.413719349171185,
      "grad_norm": 6.604166507720947,
      "learning_rate": 2.965523387569068e-05,
      "loss": 1.6192,
      "step": 319600
    },
    {
      "epoch": 24.421358185012604,
      "grad_norm": 4.8228840827941895,
      "learning_rate": 2.9648868179156165e-05,
      "loss": 1.7879,
      "step": 319700
    },
    {
      "epoch": 24.428997020854023,
      "grad_norm": 5.605000972747803,
      "learning_rate": 2.964250248262165e-05,
      "loss": 1.7575,
      "step": 319800
    },
    {
      "epoch": 24.43663585669544,
      "grad_norm": 5.245743274688721,
      "learning_rate": 2.963613678608714e-05,
      "loss": 1.7944,
      "step": 319900
    },
    {
      "epoch": 24.444274692536858,
      "grad_norm": 4.93669319152832,
      "learning_rate": 2.9629771089552622e-05,
      "loss": 1.6442,
      "step": 320000
    },
    {
      "epoch": 24.451913528378274,
      "grad_norm": 4.932295322418213,
      "learning_rate": 2.9623405393018106e-05,
      "loss": 1.7306,
      "step": 320100
    },
    {
      "epoch": 24.459552364219693,
      "grad_norm": 5.889383316040039,
      "learning_rate": 2.961703969648359e-05,
      "loss": 1.7798,
      "step": 320200
    },
    {
      "epoch": 24.467191200061112,
      "grad_norm": 6.166011810302734,
      "learning_rate": 2.9610673999949073e-05,
      "loss": 1.7515,
      "step": 320300
    },
    {
      "epoch": 24.474830035902528,
      "grad_norm": 4.806390285491943,
      "learning_rate": 2.9604308303414563e-05,
      "loss": 1.7945,
      "step": 320400
    },
    {
      "epoch": 24.482468871743947,
      "grad_norm": 6.945050239562988,
      "learning_rate": 2.9597942606880047e-05,
      "loss": 1.7773,
      "step": 320500
    },
    {
      "epoch": 24.490107707585363,
      "grad_norm": 7.0805511474609375,
      "learning_rate": 2.959157691034553e-05,
      "loss": 1.7227,
      "step": 320600
    },
    {
      "epoch": 24.497746543426782,
      "grad_norm": 6.831483364105225,
      "learning_rate": 2.9585211213811014e-05,
      "loss": 1.8044,
      "step": 320700
    },
    {
      "epoch": 24.505385379268198,
      "grad_norm": 6.733030796051025,
      "learning_rate": 2.9578845517276504e-05,
      "loss": 1.7668,
      "step": 320800
    },
    {
      "epoch": 24.513024215109617,
      "grad_norm": 8.496949195861816,
      "learning_rate": 2.9572479820741988e-05,
      "loss": 1.7483,
      "step": 320900
    },
    {
      "epoch": 24.520663050951036,
      "grad_norm": 6.364340782165527,
      "learning_rate": 2.956611412420747e-05,
      "loss": 1.6886,
      "step": 321000
    },
    {
      "epoch": 24.528301886792452,
      "grad_norm": 6.148329257965088,
      "learning_rate": 2.9559748427672958e-05,
      "loss": 1.7473,
      "step": 321100
    },
    {
      "epoch": 24.53594072263387,
      "grad_norm": 5.33975076675415,
      "learning_rate": 2.955338273113844e-05,
      "loss": 1.773,
      "step": 321200
    },
    {
      "epoch": 24.543579558475287,
      "grad_norm": 4.801389217376709,
      "learning_rate": 2.954701703460393e-05,
      "loss": 1.6764,
      "step": 321300
    },
    {
      "epoch": 24.551218394316706,
      "grad_norm": 6.407203197479248,
      "learning_rate": 2.9540651338069415e-05,
      "loss": 1.7934,
      "step": 321400
    },
    {
      "epoch": 24.558857230158125,
      "grad_norm": 4.532105445861816,
      "learning_rate": 2.95342856415349e-05,
      "loss": 1.8016,
      "step": 321500
    },
    {
      "epoch": 24.56649606599954,
      "grad_norm": 5.070849418640137,
      "learning_rate": 2.9527919945000382e-05,
      "loss": 1.79,
      "step": 321600
    },
    {
      "epoch": 24.57413490184096,
      "grad_norm": 6.038986682891846,
      "learning_rate": 2.9521554248465866e-05,
      "loss": 1.7227,
      "step": 321700
    },
    {
      "epoch": 24.581773737682376,
      "grad_norm": 5.629619121551514,
      "learning_rate": 2.9515188551931356e-05,
      "loss": 1.7682,
      "step": 321800
    },
    {
      "epoch": 24.589412573523795,
      "grad_norm": 5.873937606811523,
      "learning_rate": 2.950882285539684e-05,
      "loss": 1.82,
      "step": 321900
    },
    {
      "epoch": 24.597051409365214,
      "grad_norm": 3.5869171619415283,
      "learning_rate": 2.9502457158862323e-05,
      "loss": 1.7647,
      "step": 322000
    },
    {
      "epoch": 24.60469024520663,
      "grad_norm": 5.34085750579834,
      "learning_rate": 2.9496091462327807e-05,
      "loss": 1.775,
      "step": 322100
    },
    {
      "epoch": 24.61232908104805,
      "grad_norm": 6.273331165313721,
      "learning_rate": 2.9489725765793297e-05,
      "loss": 1.8234,
      "step": 322200
    },
    {
      "epoch": 24.619967916889465,
      "grad_norm": 4.589003086090088,
      "learning_rate": 2.948336006925878e-05,
      "loss": 1.6515,
      "step": 322300
    },
    {
      "epoch": 24.627606752730884,
      "grad_norm": 10.100128173828125,
      "learning_rate": 2.9476994372724264e-05,
      "loss": 1.8402,
      "step": 322400
    },
    {
      "epoch": 24.635245588572303,
      "grad_norm": 4.918803691864014,
      "learning_rate": 2.9470628676189748e-05,
      "loss": 1.679,
      "step": 322500
    },
    {
      "epoch": 24.64288442441372,
      "grad_norm": 7.125776767730713,
      "learning_rate": 2.946426297965523e-05,
      "loss": 1.8753,
      "step": 322600
    },
    {
      "epoch": 24.650523260255138,
      "grad_norm": 5.733813285827637,
      "learning_rate": 2.9457897283120722e-05,
      "loss": 1.7436,
      "step": 322700
    },
    {
      "epoch": 24.658162096096554,
      "grad_norm": 5.425826549530029,
      "learning_rate": 2.9451531586586205e-05,
      "loss": 1.7386,
      "step": 322800
    },
    {
      "epoch": 24.665800931937973,
      "grad_norm": 7.350246906280518,
      "learning_rate": 2.944516589005169e-05,
      "loss": 1.8256,
      "step": 322900
    },
    {
      "epoch": 24.673439767779392,
      "grad_norm": 4.998598575592041,
      "learning_rate": 2.9438800193517176e-05,
      "loss": 1.8233,
      "step": 323000
    },
    {
      "epoch": 24.681078603620808,
      "grad_norm": 7.399407863616943,
      "learning_rate": 2.9432434496982663e-05,
      "loss": 1.6355,
      "step": 323100
    },
    {
      "epoch": 24.688717439462227,
      "grad_norm": 5.012080669403076,
      "learning_rate": 2.9426068800448146e-05,
      "loss": 1.683,
      "step": 323200
    },
    {
      "epoch": 24.696356275303643,
      "grad_norm": 5.259026527404785,
      "learning_rate": 2.9419703103913633e-05,
      "loss": 1.8093,
      "step": 323300
    },
    {
      "epoch": 24.703995111145062,
      "grad_norm": 6.951790809631348,
      "learning_rate": 2.9413337407379117e-05,
      "loss": 1.835,
      "step": 323400
    },
    {
      "epoch": 24.711633946986478,
      "grad_norm": 6.406591415405273,
      "learning_rate": 2.94069717108446e-05,
      "loss": 1.6295,
      "step": 323500
    },
    {
      "epoch": 24.719272782827897,
      "grad_norm": 5.0980916023254395,
      "learning_rate": 2.940060601431009e-05,
      "loss": 1.7571,
      "step": 323600
    },
    {
      "epoch": 24.726911618669316,
      "grad_norm": 4.698384761810303,
      "learning_rate": 2.9394240317775574e-05,
      "loss": 1.7692,
      "step": 323700
    },
    {
      "epoch": 24.73455045451073,
      "grad_norm": 4.860545635223389,
      "learning_rate": 2.9387874621241058e-05,
      "loss": 1.8016,
      "step": 323800
    },
    {
      "epoch": 24.74218929035215,
      "grad_norm": 4.758181571960449,
      "learning_rate": 2.938150892470654e-05,
      "loss": 1.7587,
      "step": 323900
    },
    {
      "epoch": 24.749828126193567,
      "grad_norm": 5.321476459503174,
      "learning_rate": 2.937514322817203e-05,
      "loss": 1.7871,
      "step": 324000
    },
    {
      "epoch": 24.757466962034986,
      "grad_norm": 11.479843139648438,
      "learning_rate": 2.9368777531637515e-05,
      "loss": 1.7595,
      "step": 324100
    },
    {
      "epoch": 24.765105797876405,
      "grad_norm": 8.3128662109375,
      "learning_rate": 2.9362411835103e-05,
      "loss": 1.7465,
      "step": 324200
    },
    {
      "epoch": 24.77274463371782,
      "grad_norm": 4.534977436065674,
      "learning_rate": 2.9356046138568482e-05,
      "loss": 1.7413,
      "step": 324300
    },
    {
      "epoch": 24.78038346955924,
      "grad_norm": 5.35781717300415,
      "learning_rate": 2.9349680442033966e-05,
      "loss": 1.7983,
      "step": 324400
    },
    {
      "epoch": 24.788022305400656,
      "grad_norm": 5.510727405548096,
      "learning_rate": 2.9343314745499456e-05,
      "loss": 1.7627,
      "step": 324500
    },
    {
      "epoch": 24.795661141242075,
      "grad_norm": 5.345678329467773,
      "learning_rate": 2.933694904896494e-05,
      "loss": 1.8013,
      "step": 324600
    },
    {
      "epoch": 24.803299977083494,
      "grad_norm": 5.90162992477417,
      "learning_rate": 2.9330583352430423e-05,
      "loss": 1.7172,
      "step": 324700
    },
    {
      "epoch": 24.81093881292491,
      "grad_norm": 5.050446510314941,
      "learning_rate": 2.9324217655895906e-05,
      "loss": 1.8071,
      "step": 324800
    },
    {
      "epoch": 24.81857764876633,
      "grad_norm": 6.30964469909668,
      "learning_rate": 2.9317851959361393e-05,
      "loss": 1.7933,
      "step": 324900
    },
    {
      "epoch": 24.826216484607745,
      "grad_norm": 5.269135475158691,
      "learning_rate": 2.931148626282688e-05,
      "loss": 1.7272,
      "step": 325000
    },
    {
      "epoch": 24.833855320449164,
      "grad_norm": 4.521081447601318,
      "learning_rate": 2.9305120566292367e-05,
      "loss": 1.7796,
      "step": 325100
    },
    {
      "epoch": 24.84149415629058,
      "grad_norm": 6.6546430587768555,
      "learning_rate": 2.929875486975785e-05,
      "loss": 1.8186,
      "step": 325200
    },
    {
      "epoch": 24.849132992132,
      "grad_norm": 5.4969940185546875,
      "learning_rate": 2.9292389173223334e-05,
      "loss": 1.7053,
      "step": 325300
    },
    {
      "epoch": 24.856771827973418,
      "grad_norm": 6.8240556716918945,
      "learning_rate": 2.9286023476688825e-05,
      "loss": 1.7496,
      "step": 325400
    },
    {
      "epoch": 24.864410663814834,
      "grad_norm": 4.540884017944336,
      "learning_rate": 2.9279657780154308e-05,
      "loss": 1.7446,
      "step": 325500
    },
    {
      "epoch": 24.872049499656253,
      "grad_norm": 5.0067901611328125,
      "learning_rate": 2.9273292083619792e-05,
      "loss": 1.8184,
      "step": 325600
    },
    {
      "epoch": 24.87968833549767,
      "grad_norm": 5.488085746765137,
      "learning_rate": 2.9266926387085275e-05,
      "loss": 1.7351,
      "step": 325700
    },
    {
      "epoch": 24.887327171339088,
      "grad_norm": 8.037139892578125,
      "learning_rate": 2.926056069055076e-05,
      "loss": 1.7726,
      "step": 325800
    },
    {
      "epoch": 24.894966007180507,
      "grad_norm": 7.927584171295166,
      "learning_rate": 2.925419499401625e-05,
      "loss": 1.7261,
      "step": 325900
    },
    {
      "epoch": 24.902604843021923,
      "grad_norm": 5.312366485595703,
      "learning_rate": 2.9247829297481733e-05,
      "loss": 1.7277,
      "step": 326000
    },
    {
      "epoch": 24.910243678863342,
      "grad_norm": 4.256097316741943,
      "learning_rate": 2.9241463600947216e-05,
      "loss": 1.7216,
      "step": 326100
    },
    {
      "epoch": 24.917882514704758,
      "grad_norm": 5.8695878982543945,
      "learning_rate": 2.92350979044127e-05,
      "loss": 1.6366,
      "step": 326200
    },
    {
      "epoch": 24.925521350546177,
      "grad_norm": 5.1452460289001465,
      "learning_rate": 2.922873220787819e-05,
      "loss": 1.7936,
      "step": 326300
    },
    {
      "epoch": 24.933160186387596,
      "grad_norm": 7.782437801361084,
      "learning_rate": 2.9222366511343674e-05,
      "loss": 1.6907,
      "step": 326400
    },
    {
      "epoch": 24.94079902222901,
      "grad_norm": 8.131149291992188,
      "learning_rate": 2.9216000814809157e-05,
      "loss": 1.7742,
      "step": 326500
    },
    {
      "epoch": 24.94843785807043,
      "grad_norm": 7.476831912994385,
      "learning_rate": 2.920963511827464e-05,
      "loss": 1.6909,
      "step": 326600
    },
    {
      "epoch": 24.956076693911847,
      "grad_norm": 7.226861476898193,
      "learning_rate": 2.9203269421740128e-05,
      "loss": 1.7102,
      "step": 326700
    },
    {
      "epoch": 24.963715529753266,
      "grad_norm": 7.583714485168457,
      "learning_rate": 2.9196903725205614e-05,
      "loss": 1.7407,
      "step": 326800
    },
    {
      "epoch": 24.971354365594685,
      "grad_norm": 5.586243629455566,
      "learning_rate": 2.9190538028671098e-05,
      "loss": 1.6338,
      "step": 326900
    },
    {
      "epoch": 24.9789932014361,
      "grad_norm": 4.5743088722229,
      "learning_rate": 2.9184172332136585e-05,
      "loss": 1.8141,
      "step": 327000
    },
    {
      "epoch": 24.98663203727752,
      "grad_norm": 4.975037097930908,
      "learning_rate": 2.917780663560207e-05,
      "loss": 1.7257,
      "step": 327100
    },
    {
      "epoch": 24.994270873118936,
      "grad_norm": 6.650301933288574,
      "learning_rate": 2.9171440939067555e-05,
      "loss": 1.7747,
      "step": 327200
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.7964928150177002,
      "eval_runtime": 1.6914,
      "eval_samples_per_second": 407.944,
      "eval_steps_per_second": 407.944,
      "step": 327275
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.511340856552124,
      "eval_runtime": 32.1175,
      "eval_samples_per_second": 407.597,
      "eval_steps_per_second": 407.597,
      "step": 327275
    },
    {
      "epoch": 25.001909708960355,
      "grad_norm": 4.454517364501953,
      "learning_rate": 2.9165075242533042e-05,
      "loss": 1.6916,
      "step": 327300
    },
    {
      "epoch": 25.00954854480177,
      "grad_norm": 5.70767879486084,
      "learning_rate": 2.9158709545998526e-05,
      "loss": 1.7003,
      "step": 327400
    },
    {
      "epoch": 25.01718738064319,
      "grad_norm": 5.799129486083984,
      "learning_rate": 2.915234384946401e-05,
      "loss": 1.73,
      "step": 327500
    },
    {
      "epoch": 25.02482621648461,
      "grad_norm": 5.815304279327393,
      "learning_rate": 2.9145978152929493e-05,
      "loss": 1.6706,
      "step": 327600
    },
    {
      "epoch": 25.032465052326025,
      "grad_norm": 6.909124851226807,
      "learning_rate": 2.9139612456394983e-05,
      "loss": 1.7232,
      "step": 327700
    },
    {
      "epoch": 25.040103888167444,
      "grad_norm": 4.491621017456055,
      "learning_rate": 2.9133246759860467e-05,
      "loss": 1.7485,
      "step": 327800
    },
    {
      "epoch": 25.04774272400886,
      "grad_norm": 4.119067192077637,
      "learning_rate": 2.912688106332595e-05,
      "loss": 1.6917,
      "step": 327900
    },
    {
      "epoch": 25.05538155985028,
      "grad_norm": 6.173881530761719,
      "learning_rate": 2.9120515366791434e-05,
      "loss": 1.8121,
      "step": 328000
    },
    {
      "epoch": 25.063020395691698,
      "grad_norm": 5.790942192077637,
      "learning_rate": 2.9114149670256917e-05,
      "loss": 1.785,
      "step": 328100
    },
    {
      "epoch": 25.070659231533114,
      "grad_norm": 9.613550186157227,
      "learning_rate": 2.9107783973722408e-05,
      "loss": 1.7125,
      "step": 328200
    },
    {
      "epoch": 25.078298067374533,
      "grad_norm": 5.428351402282715,
      "learning_rate": 2.910141827718789e-05,
      "loss": 1.7122,
      "step": 328300
    },
    {
      "epoch": 25.08593690321595,
      "grad_norm": 5.880377292633057,
      "learning_rate": 2.9095052580653375e-05,
      "loss": 1.7178,
      "step": 328400
    },
    {
      "epoch": 25.093575739057368,
      "grad_norm": 6.054392337799072,
      "learning_rate": 2.908868688411886e-05,
      "loss": 1.6941,
      "step": 328500
    },
    {
      "epoch": 25.101214574898787,
      "grad_norm": 4.875428676605225,
      "learning_rate": 2.908232118758435e-05,
      "loss": 1.6583,
      "step": 328600
    },
    {
      "epoch": 25.108853410740203,
      "grad_norm": 6.942430019378662,
      "learning_rate": 2.9075955491049832e-05,
      "loss": 1.7529,
      "step": 328700
    },
    {
      "epoch": 25.116492246581622,
      "grad_norm": 5.834548473358154,
      "learning_rate": 2.9069589794515316e-05,
      "loss": 1.7196,
      "step": 328800
    },
    {
      "epoch": 25.124131082423037,
      "grad_norm": 5.799129962921143,
      "learning_rate": 2.9063224097980803e-05,
      "loss": 1.7162,
      "step": 328900
    },
    {
      "epoch": 25.131769918264457,
      "grad_norm": 5.390501976013184,
      "learning_rate": 2.9056858401446286e-05,
      "loss": 1.8405,
      "step": 329000
    },
    {
      "epoch": 25.139408754105876,
      "grad_norm": 5.9351677894592285,
      "learning_rate": 2.9050492704911773e-05,
      "loss": 1.7157,
      "step": 329100
    },
    {
      "epoch": 25.14704758994729,
      "grad_norm": 5.026716232299805,
      "learning_rate": 2.904412700837726e-05,
      "loss": 1.7126,
      "step": 329200
    },
    {
      "epoch": 25.15468642578871,
      "grad_norm": 4.931205749511719,
      "learning_rate": 2.9037761311842744e-05,
      "loss": 1.6354,
      "step": 329300
    },
    {
      "epoch": 25.162325261630127,
      "grad_norm": 5.494887828826904,
      "learning_rate": 2.9031395615308227e-05,
      "loss": 1.6794,
      "step": 329400
    },
    {
      "epoch": 25.169964097471546,
      "grad_norm": 5.541001796722412,
      "learning_rate": 2.9025029918773717e-05,
      "loss": 1.6624,
      "step": 329500
    },
    {
      "epoch": 25.17760293331296,
      "grad_norm": 5.143683910369873,
      "learning_rate": 2.90186642222392e-05,
      "loss": 1.8235,
      "step": 329600
    },
    {
      "epoch": 25.18524176915438,
      "grad_norm": 7.222500324249268,
      "learning_rate": 2.9012298525704684e-05,
      "loss": 1.7403,
      "step": 329700
    },
    {
      "epoch": 25.1928806049958,
      "grad_norm": 4.907817840576172,
      "learning_rate": 2.9005932829170168e-05,
      "loss": 1.7285,
      "step": 329800
    },
    {
      "epoch": 25.200519440837216,
      "grad_norm": 3.846121072769165,
      "learning_rate": 2.899956713263565e-05,
      "loss": 1.8314,
      "step": 329900
    },
    {
      "epoch": 25.208158276678635,
      "grad_norm": 4.951231956481934,
      "learning_rate": 2.8993201436101142e-05,
      "loss": 1.6973,
      "step": 330000
    },
    {
      "epoch": 25.21579711252005,
      "grad_norm": 4.659604549407959,
      "learning_rate": 2.8986835739566625e-05,
      "loss": 1.7786,
      "step": 330100
    },
    {
      "epoch": 25.22343594836147,
      "grad_norm": 6.008034706115723,
      "learning_rate": 2.898047004303211e-05,
      "loss": 1.8491,
      "step": 330200
    },
    {
      "epoch": 25.23107478420289,
      "grad_norm": 6.1850409507751465,
      "learning_rate": 2.8974104346497592e-05,
      "loss": 1.779,
      "step": 330300
    },
    {
      "epoch": 25.238713620044305,
      "grad_norm": 5.6005473136901855,
      "learning_rate": 2.8967738649963076e-05,
      "loss": 1.7682,
      "step": 330400
    },
    {
      "epoch": 25.246352455885724,
      "grad_norm": 7.147811412811279,
      "learning_rate": 2.8961372953428566e-05,
      "loss": 1.6361,
      "step": 330500
    },
    {
      "epoch": 25.25399129172714,
      "grad_norm": 5.984086990356445,
      "learning_rate": 2.895500725689405e-05,
      "loss": 1.7992,
      "step": 330600
    },
    {
      "epoch": 25.26163012756856,
      "grad_norm": 7.849973678588867,
      "learning_rate": 2.8948641560359537e-05,
      "loss": 1.7589,
      "step": 330700
    },
    {
      "epoch": 25.269268963409978,
      "grad_norm": 6.762118816375732,
      "learning_rate": 2.894227586382502e-05,
      "loss": 1.6977,
      "step": 330800
    },
    {
      "epoch": 25.276907799251394,
      "grad_norm": 5.1550092697143555,
      "learning_rate": 2.8935910167290507e-05,
      "loss": 1.6891,
      "step": 330900
    },
    {
      "epoch": 25.284546635092813,
      "grad_norm": 5.0974860191345215,
      "learning_rate": 2.8929544470755994e-05,
      "loss": 1.8199,
      "step": 331000
    },
    {
      "epoch": 25.29218547093423,
      "grad_norm": 5.312957763671875,
      "learning_rate": 2.8923178774221478e-05,
      "loss": 1.7542,
      "step": 331100
    },
    {
      "epoch": 25.299824306775648,
      "grad_norm": 6.817986488342285,
      "learning_rate": 2.891681307768696e-05,
      "loss": 1.8213,
      "step": 331200
    },
    {
      "epoch": 25.307463142617067,
      "grad_norm": 5.246842861175537,
      "learning_rate": 2.8910447381152445e-05,
      "loss": 1.7092,
      "step": 331300
    },
    {
      "epoch": 25.315101978458483,
      "grad_norm": 6.948000431060791,
      "learning_rate": 2.8904081684617935e-05,
      "loss": 1.7782,
      "step": 331400
    },
    {
      "epoch": 25.3227408142999,
      "grad_norm": 5.176092147827148,
      "learning_rate": 2.889771598808342e-05,
      "loss": 1.8428,
      "step": 331500
    },
    {
      "epoch": 25.330379650141317,
      "grad_norm": 3.3020668029785156,
      "learning_rate": 2.8891350291548902e-05,
      "loss": 1.7546,
      "step": 331600
    },
    {
      "epoch": 25.338018485982737,
      "grad_norm": 5.026732444763184,
      "learning_rate": 2.8884984595014386e-05,
      "loss": 1.72,
      "step": 331700
    },
    {
      "epoch": 25.345657321824152,
      "grad_norm": 8.21894359588623,
      "learning_rate": 2.8878618898479876e-05,
      "loss": 1.7613,
      "step": 331800
    },
    {
      "epoch": 25.35329615766557,
      "grad_norm": 5.845413684844971,
      "learning_rate": 2.887225320194536e-05,
      "loss": 1.7042,
      "step": 331900
    },
    {
      "epoch": 25.36093499350699,
      "grad_norm": 4.323905944824219,
      "learning_rate": 2.8865887505410843e-05,
      "loss": 1.8233,
      "step": 332000
    },
    {
      "epoch": 25.368573829348406,
      "grad_norm": 6.661251068115234,
      "learning_rate": 2.8859521808876327e-05,
      "loss": 1.7326,
      "step": 332100
    },
    {
      "epoch": 25.376212665189826,
      "grad_norm": 5.444075107574463,
      "learning_rate": 2.885315611234181e-05,
      "loss": 1.7393,
      "step": 332200
    },
    {
      "epoch": 25.38385150103124,
      "grad_norm": 5.142563819885254,
      "learning_rate": 2.88467904158073e-05,
      "loss": 1.7348,
      "step": 332300
    },
    {
      "epoch": 25.39149033687266,
      "grad_norm": 4.747624397277832,
      "learning_rate": 2.8840424719272784e-05,
      "loss": 1.7117,
      "step": 332400
    },
    {
      "epoch": 25.39912917271408,
      "grad_norm": 6.08740234375,
      "learning_rate": 2.8834059022738268e-05,
      "loss": 1.7311,
      "step": 332500
    },
    {
      "epoch": 25.406768008555495,
      "grad_norm": 5.559725284576416,
      "learning_rate": 2.8827693326203754e-05,
      "loss": 1.8192,
      "step": 332600
    },
    {
      "epoch": 25.414406844396915,
      "grad_norm": 5.064250469207764,
      "learning_rate": 2.882132762966924e-05,
      "loss": 1.7507,
      "step": 332700
    },
    {
      "epoch": 25.42204568023833,
      "grad_norm": 5.332756042480469,
      "learning_rate": 2.8814961933134725e-05,
      "loss": 1.733,
      "step": 332800
    },
    {
      "epoch": 25.42968451607975,
      "grad_norm": 4.912295818328857,
      "learning_rate": 2.8808596236600212e-05,
      "loss": 1.6829,
      "step": 332900
    },
    {
      "epoch": 25.43732335192117,
      "grad_norm": 5.061941623687744,
      "learning_rate": 2.8802230540065695e-05,
      "loss": 1.7557,
      "step": 333000
    },
    {
      "epoch": 25.444962187762584,
      "grad_norm": 5.701029300689697,
      "learning_rate": 2.879586484353118e-05,
      "loss": 1.7868,
      "step": 333100
    },
    {
      "epoch": 25.452601023604004,
      "grad_norm": 3.5312135219573975,
      "learning_rate": 2.878949914699667e-05,
      "loss": 1.7307,
      "step": 333200
    },
    {
      "epoch": 25.46023985944542,
      "grad_norm": 5.789217472076416,
      "learning_rate": 2.8783133450462153e-05,
      "loss": 1.7331,
      "step": 333300
    },
    {
      "epoch": 25.46787869528684,
      "grad_norm": 4.553678512573242,
      "learning_rate": 2.8776767753927636e-05,
      "loss": 1.668,
      "step": 333400
    },
    {
      "epoch": 25.475517531128258,
      "grad_norm": 6.107542514801025,
      "learning_rate": 2.877040205739312e-05,
      "loss": 1.6896,
      "step": 333500
    },
    {
      "epoch": 25.483156366969673,
      "grad_norm": 5.958364009857178,
      "learning_rate": 2.8764036360858603e-05,
      "loss": 1.7055,
      "step": 333600
    },
    {
      "epoch": 25.490795202811093,
      "grad_norm": 6.671004295349121,
      "learning_rate": 2.8757670664324094e-05,
      "loss": 1.627,
      "step": 333700
    },
    {
      "epoch": 25.49843403865251,
      "grad_norm": 5.240405082702637,
      "learning_rate": 2.8751304967789577e-05,
      "loss": 1.7251,
      "step": 333800
    },
    {
      "epoch": 25.506072874493928,
      "grad_norm": 5.17565393447876,
      "learning_rate": 2.874493927125506e-05,
      "loss": 1.7089,
      "step": 333900
    },
    {
      "epoch": 25.513711710335343,
      "grad_norm": 5.160388946533203,
      "learning_rate": 2.8738573574720544e-05,
      "loss": 1.7334,
      "step": 334000
    },
    {
      "epoch": 25.521350546176762,
      "grad_norm": 5.8628058433532715,
      "learning_rate": 2.8732207878186035e-05,
      "loss": 1.6841,
      "step": 334100
    },
    {
      "epoch": 25.52898938201818,
      "grad_norm": 43.39572525024414,
      "learning_rate": 2.8725842181651518e-05,
      "loss": 1.8124,
      "step": 334200
    },
    {
      "epoch": 25.536628217859597,
      "grad_norm": 5.693486213684082,
      "learning_rate": 2.8719476485117002e-05,
      "loss": 1.8078,
      "step": 334300
    },
    {
      "epoch": 25.544267053701017,
      "grad_norm": 5.936595439910889,
      "learning_rate": 2.8713110788582485e-05,
      "loss": 1.7326,
      "step": 334400
    },
    {
      "epoch": 25.551905889542432,
      "grad_norm": 6.048452854156494,
      "learning_rate": 2.8706745092047972e-05,
      "loss": 1.8184,
      "step": 334500
    },
    {
      "epoch": 25.55954472538385,
      "grad_norm": 5.820732593536377,
      "learning_rate": 2.870037939551346e-05,
      "loss": 1.7101,
      "step": 334600
    },
    {
      "epoch": 25.56718356122527,
      "grad_norm": 3.7663984298706055,
      "learning_rate": 2.8694013698978943e-05,
      "loss": 1.6555,
      "step": 334700
    },
    {
      "epoch": 25.574822397066686,
      "grad_norm": 5.423797130584717,
      "learning_rate": 2.868764800244443e-05,
      "loss": 1.6195,
      "step": 334800
    },
    {
      "epoch": 25.582461232908106,
      "grad_norm": 8.097554206848145,
      "learning_rate": 2.8681282305909913e-05,
      "loss": 1.7665,
      "step": 334900
    },
    {
      "epoch": 25.59010006874952,
      "grad_norm": 8.531304359436035,
      "learning_rate": 2.8674916609375403e-05,
      "loss": 1.7407,
      "step": 335000
    },
    {
      "epoch": 25.59773890459094,
      "grad_norm": 5.967470169067383,
      "learning_rate": 2.8668550912840887e-05,
      "loss": 1.746,
      "step": 335100
    },
    {
      "epoch": 25.60537774043236,
      "grad_norm": 6.296582221984863,
      "learning_rate": 2.866218521630637e-05,
      "loss": 1.7729,
      "step": 335200
    },
    {
      "epoch": 25.613016576273775,
      "grad_norm": 4.93475866317749,
      "learning_rate": 2.8655819519771854e-05,
      "loss": 1.7067,
      "step": 335300
    },
    {
      "epoch": 25.620655412115195,
      "grad_norm": 5.674041748046875,
      "learning_rate": 2.8649453823237338e-05,
      "loss": 1.7642,
      "step": 335400
    },
    {
      "epoch": 25.62829424795661,
      "grad_norm": 5.484790802001953,
      "learning_rate": 2.8643088126702828e-05,
      "loss": 1.7416,
      "step": 335500
    },
    {
      "epoch": 25.63593308379803,
      "grad_norm": 5.790212154388428,
      "learning_rate": 2.863672243016831e-05,
      "loss": 1.719,
      "step": 335600
    },
    {
      "epoch": 25.64357191963945,
      "grad_norm": 6.193973064422607,
      "learning_rate": 2.8630356733633795e-05,
      "loss": 1.7504,
      "step": 335700
    },
    {
      "epoch": 25.651210755480864,
      "grad_norm": 6.550705432891846,
      "learning_rate": 2.862399103709928e-05,
      "loss": 1.6278,
      "step": 335800
    },
    {
      "epoch": 25.658849591322284,
      "grad_norm": 5.52938985824585,
      "learning_rate": 2.861762534056477e-05,
      "loss": 1.7808,
      "step": 335900
    },
    {
      "epoch": 25.6664884271637,
      "grad_norm": 5.976080417633057,
      "learning_rate": 2.8611259644030252e-05,
      "loss": 1.8007,
      "step": 336000
    },
    {
      "epoch": 25.67412726300512,
      "grad_norm": 5.0252156257629395,
      "learning_rate": 2.8604893947495736e-05,
      "loss": 1.7143,
      "step": 336100
    },
    {
      "epoch": 25.681766098846534,
      "grad_norm": 6.3926920890808105,
      "learning_rate": 2.859852825096122e-05,
      "loss": 1.7662,
      "step": 336200
    },
    {
      "epoch": 25.689404934687953,
      "grad_norm": 6.290194034576416,
      "learning_rate": 2.8592162554426706e-05,
      "loss": 1.7978,
      "step": 336300
    },
    {
      "epoch": 25.697043770529373,
      "grad_norm": 6.101322650909424,
      "learning_rate": 2.8585796857892193e-05,
      "loss": 1.8182,
      "step": 336400
    },
    {
      "epoch": 25.70468260637079,
      "grad_norm": 7.238098621368408,
      "learning_rate": 2.8579431161357677e-05,
      "loss": 1.691,
      "step": 336500
    },
    {
      "epoch": 25.712321442212208,
      "grad_norm": 4.165693759918213,
      "learning_rate": 2.8573065464823164e-05,
      "loss": 1.7233,
      "step": 336600
    },
    {
      "epoch": 25.719960278053623,
      "grad_norm": 4.9353837966918945,
      "learning_rate": 2.8566699768288647e-05,
      "loss": 1.7987,
      "step": 336700
    },
    {
      "epoch": 25.727599113895042,
      "grad_norm": 5.163836479187012,
      "learning_rate": 2.856033407175413e-05,
      "loss": 1.7082,
      "step": 336800
    },
    {
      "epoch": 25.73523794973646,
      "grad_norm": 5.736542701721191,
      "learning_rate": 2.855396837521962e-05,
      "loss": 1.7588,
      "step": 336900
    },
    {
      "epoch": 25.742876785577877,
      "grad_norm": 5.784781455993652,
      "learning_rate": 2.8547602678685105e-05,
      "loss": 1.798,
      "step": 337000
    },
    {
      "epoch": 25.750515621419297,
      "grad_norm": 5.483381271362305,
      "learning_rate": 2.8541236982150588e-05,
      "loss": 1.6979,
      "step": 337100
    },
    {
      "epoch": 25.758154457260712,
      "grad_norm": 5.0342912673950195,
      "learning_rate": 2.8534871285616072e-05,
      "loss": 1.8037,
      "step": 337200
    },
    {
      "epoch": 25.76579329310213,
      "grad_norm": 5.466119289398193,
      "learning_rate": 2.8528505589081562e-05,
      "loss": 1.7904,
      "step": 337300
    },
    {
      "epoch": 25.77343212894355,
      "grad_norm": 6.93731689453125,
      "learning_rate": 2.8522139892547046e-05,
      "loss": 1.7292,
      "step": 337400
    },
    {
      "epoch": 25.781070964784966,
      "grad_norm": 5.383206844329834,
      "learning_rate": 2.851577419601253e-05,
      "loss": 1.7298,
      "step": 337500
    },
    {
      "epoch": 25.788709800626386,
      "grad_norm": 4.858067512512207,
      "learning_rate": 2.8509408499478013e-05,
      "loss": 1.7093,
      "step": 337600
    },
    {
      "epoch": 25.7963486364678,
      "grad_norm": 5.475985527038574,
      "learning_rate": 2.8503042802943496e-05,
      "loss": 1.7823,
      "step": 337700
    },
    {
      "epoch": 25.80398747230922,
      "grad_norm": 6.635644912719727,
      "learning_rate": 2.8496677106408986e-05,
      "loss": 1.7978,
      "step": 337800
    },
    {
      "epoch": 25.811626308150636,
      "grad_norm": 5.235746383666992,
      "learning_rate": 2.849031140987447e-05,
      "loss": 1.7503,
      "step": 337900
    },
    {
      "epoch": 25.819265143992055,
      "grad_norm": 4.800553798675537,
      "learning_rate": 2.8483945713339954e-05,
      "loss": 1.8206,
      "step": 338000
    },
    {
      "epoch": 25.826903979833475,
      "grad_norm": 6.361867904663086,
      "learning_rate": 2.8477580016805437e-05,
      "loss": 1.714,
      "step": 338100
    },
    {
      "epoch": 25.83454281567489,
      "grad_norm": 6.73337984085083,
      "learning_rate": 2.8471214320270927e-05,
      "loss": 1.7078,
      "step": 338200
    },
    {
      "epoch": 25.84218165151631,
      "grad_norm": 5.068084716796875,
      "learning_rate": 2.846484862373641e-05,
      "loss": 1.7861,
      "step": 338300
    },
    {
      "epoch": 25.849820487357725,
      "grad_norm": 5.201709270477295,
      "learning_rate": 2.8458482927201894e-05,
      "loss": 1.7883,
      "step": 338400
    },
    {
      "epoch": 25.857459323199144,
      "grad_norm": 4.95694637298584,
      "learning_rate": 2.845211723066738e-05,
      "loss": 1.8075,
      "step": 338500
    },
    {
      "epoch": 25.865098159040564,
      "grad_norm": 5.786435604095459,
      "learning_rate": 2.8445751534132865e-05,
      "loss": 1.7135,
      "step": 338600
    },
    {
      "epoch": 25.87273699488198,
      "grad_norm": 5.175882816314697,
      "learning_rate": 2.8439385837598352e-05,
      "loss": 1.7704,
      "step": 338700
    },
    {
      "epoch": 25.8803758307234,
      "grad_norm": 6.97836446762085,
      "learning_rate": 2.843302014106384e-05,
      "loss": 1.7273,
      "step": 338800
    },
    {
      "epoch": 25.888014666564814,
      "grad_norm": 4.707234859466553,
      "learning_rate": 2.8426654444529322e-05,
      "loss": 1.7491,
      "step": 338900
    },
    {
      "epoch": 25.895653502406233,
      "grad_norm": 5.4721174240112305,
      "learning_rate": 2.8420288747994806e-05,
      "loss": 1.699,
      "step": 339000
    },
    {
      "epoch": 25.903292338247653,
      "grad_norm": 6.054315567016602,
      "learning_rate": 2.841392305146029e-05,
      "loss": 1.6932,
      "step": 339100
    },
    {
      "epoch": 25.910931174089068,
      "grad_norm": 5.946444034576416,
      "learning_rate": 2.840755735492578e-05,
      "loss": 1.712,
      "step": 339200
    },
    {
      "epoch": 25.918570009930487,
      "grad_norm": 6.4169182777404785,
      "learning_rate": 2.8401191658391263e-05,
      "loss": 1.7179,
      "step": 339300
    },
    {
      "epoch": 25.926208845771903,
      "grad_norm": 6.817497253417969,
      "learning_rate": 2.8394825961856747e-05,
      "loss": 1.8066,
      "step": 339400
    },
    {
      "epoch": 25.933847681613322,
      "grad_norm": 5.91174840927124,
      "learning_rate": 2.838846026532223e-05,
      "loss": 1.7618,
      "step": 339500
    },
    {
      "epoch": 25.94148651745474,
      "grad_norm": 3.7619824409484863,
      "learning_rate": 2.838209456878772e-05,
      "loss": 1.7267,
      "step": 339600
    },
    {
      "epoch": 25.949125353296157,
      "grad_norm": 6.51605224609375,
      "learning_rate": 2.8375728872253204e-05,
      "loss": 1.7542,
      "step": 339700
    },
    {
      "epoch": 25.956764189137576,
      "grad_norm": 10.337993621826172,
      "learning_rate": 2.8369363175718688e-05,
      "loss": 1.7781,
      "step": 339800
    },
    {
      "epoch": 25.964403024978992,
      "grad_norm": 5.482574939727783,
      "learning_rate": 2.836299747918417e-05,
      "loss": 1.753,
      "step": 339900
    },
    {
      "epoch": 25.97204186082041,
      "grad_norm": 6.102374076843262,
      "learning_rate": 2.8356631782649655e-05,
      "loss": 1.7751,
      "step": 340000
    },
    {
      "epoch": 25.97968069666183,
      "grad_norm": 5.623290538787842,
      "learning_rate": 2.8350266086115145e-05,
      "loss": 1.7049,
      "step": 340100
    },
    {
      "epoch": 25.987319532503246,
      "grad_norm": 3.5810532569885254,
      "learning_rate": 2.834390038958063e-05,
      "loss": 1.7093,
      "step": 340200
    },
    {
      "epoch": 25.994958368344665,
      "grad_norm": 5.0134687423706055,
      "learning_rate": 2.8337534693046112e-05,
      "loss": 1.7751,
      "step": 340300
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.786224126815796,
      "eval_runtime": 1.6623,
      "eval_samples_per_second": 415.076,
      "eval_steps_per_second": 415.076,
      "step": 340366
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.5031814575195312,
      "eval_runtime": 31.966,
      "eval_samples_per_second": 409.529,
      "eval_steps_per_second": 409.529,
      "step": 340366
    },
    {
      "epoch": 26.00259720418608,
      "grad_norm": 5.361832618713379,
      "learning_rate": 2.83311689965116e-05,
      "loss": 1.7623,
      "step": 340400
    },
    {
      "epoch": 26.0102360400275,
      "grad_norm": 4.805333137512207,
      "learning_rate": 2.8324803299977086e-05,
      "loss": 1.7181,
      "step": 340500
    },
    {
      "epoch": 26.017874875868916,
      "grad_norm": 15.763779640197754,
      "learning_rate": 2.8318437603442573e-05,
      "loss": 1.7752,
      "step": 340600
    },
    {
      "epoch": 26.025513711710335,
      "grad_norm": 6.0350847244262695,
      "learning_rate": 2.8312071906908056e-05,
      "loss": 1.7422,
      "step": 340700
    },
    {
      "epoch": 26.033152547551754,
      "grad_norm": 5.1628313064575195,
      "learning_rate": 2.830570621037354e-05,
      "loss": 1.7404,
      "step": 340800
    },
    {
      "epoch": 26.04079138339317,
      "grad_norm": 5.8652777671813965,
      "learning_rate": 2.8299340513839024e-05,
      "loss": 1.8134,
      "step": 340900
    },
    {
      "epoch": 26.04843021923459,
      "grad_norm": 5.186185359954834,
      "learning_rate": 2.8292974817304514e-05,
      "loss": 1.6654,
      "step": 341000
    },
    {
      "epoch": 26.056069055076005,
      "grad_norm": 5.485393524169922,
      "learning_rate": 2.8286609120769997e-05,
      "loss": 1.7803,
      "step": 341100
    },
    {
      "epoch": 26.063707890917424,
      "grad_norm": 6.432659149169922,
      "learning_rate": 2.828024342423548e-05,
      "loss": 1.8119,
      "step": 341200
    },
    {
      "epoch": 26.071346726758843,
      "grad_norm": 6.643533706665039,
      "learning_rate": 2.8273877727700964e-05,
      "loss": 1.6311,
      "step": 341300
    },
    {
      "epoch": 26.07898556260026,
      "grad_norm": 4.451286315917969,
      "learning_rate": 2.8267512031166455e-05,
      "loss": 1.7082,
      "step": 341400
    },
    {
      "epoch": 26.08662439844168,
      "grad_norm": 4.3068389892578125,
      "learning_rate": 2.826114633463194e-05,
      "loss": 1.6872,
      "step": 341500
    },
    {
      "epoch": 26.094263234283094,
      "grad_norm": 7.210463047027588,
      "learning_rate": 2.8254780638097422e-05,
      "loss": 1.7879,
      "step": 341600
    },
    {
      "epoch": 26.101902070124513,
      "grad_norm": 5.92836856842041,
      "learning_rate": 2.8248414941562905e-05,
      "loss": 1.763,
      "step": 341700
    },
    {
      "epoch": 26.109540905965932,
      "grad_norm": 5.878829002380371,
      "learning_rate": 2.824204924502839e-05,
      "loss": 1.7182,
      "step": 341800
    },
    {
      "epoch": 26.117179741807348,
      "grad_norm": 5.69448709487915,
      "learning_rate": 2.823568354849388e-05,
      "loss": 1.7798,
      "step": 341900
    },
    {
      "epoch": 26.124818577648767,
      "grad_norm": 4.863220691680908,
      "learning_rate": 2.8229317851959363e-05,
      "loss": 1.7723,
      "step": 342000
    },
    {
      "epoch": 26.132457413490183,
      "grad_norm": 7.247811317443848,
      "learning_rate": 2.8222952155424846e-05,
      "loss": 1.7597,
      "step": 342100
    },
    {
      "epoch": 26.140096249331602,
      "grad_norm": 3.509280204772949,
      "learning_rate": 2.8216586458890333e-05,
      "loss": 1.6876,
      "step": 342200
    },
    {
      "epoch": 26.147735085173018,
      "grad_norm": 4.728260040283203,
      "learning_rate": 2.8210220762355817e-05,
      "loss": 1.7194,
      "step": 342300
    },
    {
      "epoch": 26.155373921014437,
      "grad_norm": 5.619220733642578,
      "learning_rate": 2.8203855065821304e-05,
      "loss": 1.6884,
      "step": 342400
    },
    {
      "epoch": 26.163012756855856,
      "grad_norm": 6.562044620513916,
      "learning_rate": 2.819748936928679e-05,
      "loss": 1.7371,
      "step": 342500
    },
    {
      "epoch": 26.170651592697272,
      "grad_norm": 3.7006237506866455,
      "learning_rate": 2.8191123672752274e-05,
      "loss": 1.6879,
      "step": 342600
    },
    {
      "epoch": 26.17829042853869,
      "grad_norm": 6.9135332107543945,
      "learning_rate": 2.8184757976217758e-05,
      "loss": 1.7521,
      "step": 342700
    },
    {
      "epoch": 26.185929264380107,
      "grad_norm": 3.945664882659912,
      "learning_rate": 2.8178392279683248e-05,
      "loss": 1.6616,
      "step": 342800
    },
    {
      "epoch": 26.193568100221526,
      "grad_norm": 4.751379013061523,
      "learning_rate": 2.817202658314873e-05,
      "loss": 1.8632,
      "step": 342900
    },
    {
      "epoch": 26.201206936062945,
      "grad_norm": 7.934079170227051,
      "learning_rate": 2.8165660886614215e-05,
      "loss": 1.75,
      "step": 343000
    },
    {
      "epoch": 26.20884577190436,
      "grad_norm": 7.315325736999512,
      "learning_rate": 2.81592951900797e-05,
      "loss": 1.7109,
      "step": 343100
    },
    {
      "epoch": 26.21648460774578,
      "grad_norm": 4.203252792358398,
      "learning_rate": 2.8152929493545182e-05,
      "loss": 1.8537,
      "step": 343200
    },
    {
      "epoch": 26.224123443587196,
      "grad_norm": 5.308610439300537,
      "learning_rate": 2.8146563797010672e-05,
      "loss": 1.7405,
      "step": 343300
    },
    {
      "epoch": 26.231762279428615,
      "grad_norm": 4.896254539489746,
      "learning_rate": 2.8140198100476156e-05,
      "loss": 1.6803,
      "step": 343400
    },
    {
      "epoch": 26.239401115270034,
      "grad_norm": 6.384270191192627,
      "learning_rate": 2.813383240394164e-05,
      "loss": 1.7554,
      "step": 343500
    },
    {
      "epoch": 26.24703995111145,
      "grad_norm": 6.21829891204834,
      "learning_rate": 2.8127466707407123e-05,
      "loss": 1.7738,
      "step": 343600
    },
    {
      "epoch": 26.25467878695287,
      "grad_norm": 5.946749687194824,
      "learning_rate": 2.8121101010872613e-05,
      "loss": 1.7212,
      "step": 343700
    },
    {
      "epoch": 26.262317622794285,
      "grad_norm": 5.6242780685424805,
      "learning_rate": 2.8114735314338097e-05,
      "loss": 1.7302,
      "step": 343800
    },
    {
      "epoch": 26.269956458635704,
      "grad_norm": 4.022266864776611,
      "learning_rate": 2.810836961780358e-05,
      "loss": 1.744,
      "step": 343900
    },
    {
      "epoch": 26.277595294477123,
      "grad_norm": 4.313119411468506,
      "learning_rate": 2.8102003921269064e-05,
      "loss": 1.8191,
      "step": 344000
    },
    {
      "epoch": 26.28523413031854,
      "grad_norm": 5.200544357299805,
      "learning_rate": 2.809563822473455e-05,
      "loss": 1.6953,
      "step": 344100
    },
    {
      "epoch": 26.29287296615996,
      "grad_norm": 4.428543567657471,
      "learning_rate": 2.8089272528200038e-05,
      "loss": 1.6198,
      "step": 344200
    },
    {
      "epoch": 26.300511802001374,
      "grad_norm": 5.941013813018799,
      "learning_rate": 2.808290683166552e-05,
      "loss": 1.7176,
      "step": 344300
    },
    {
      "epoch": 26.308150637842793,
      "grad_norm": 5.303244590759277,
      "learning_rate": 2.807654113513101e-05,
      "loss": 1.7222,
      "step": 344400
    },
    {
      "epoch": 26.31578947368421,
      "grad_norm": 5.481066703796387,
      "learning_rate": 2.8070175438596492e-05,
      "loss": 1.67,
      "step": 344500
    },
    {
      "epoch": 26.323428309525628,
      "grad_norm": 4.270764350891113,
      "learning_rate": 2.8063809742061975e-05,
      "loss": 1.7149,
      "step": 344600
    },
    {
      "epoch": 26.331067145367047,
      "grad_norm": 4.917791843414307,
      "learning_rate": 2.8057444045527466e-05,
      "loss": 1.8511,
      "step": 344700
    },
    {
      "epoch": 26.338705981208463,
      "grad_norm": 6.115751266479492,
      "learning_rate": 2.805107834899295e-05,
      "loss": 1.6528,
      "step": 344800
    },
    {
      "epoch": 26.346344817049882,
      "grad_norm": 4.650022506713867,
      "learning_rate": 2.8044712652458433e-05,
      "loss": 1.7583,
      "step": 344900
    },
    {
      "epoch": 26.353983652891298,
      "grad_norm": 5.746829509735107,
      "learning_rate": 2.8038346955923916e-05,
      "loss": 1.6678,
      "step": 345000
    },
    {
      "epoch": 26.361622488732717,
      "grad_norm": 4.584747791290283,
      "learning_rate": 2.8031981259389407e-05,
      "loss": 1.6896,
      "step": 345100
    },
    {
      "epoch": 26.369261324574136,
      "grad_norm": 7.941413402557373,
      "learning_rate": 2.802561556285489e-05,
      "loss": 1.7619,
      "step": 345200
    },
    {
      "epoch": 26.376900160415552,
      "grad_norm": 5.423081874847412,
      "learning_rate": 2.8019249866320374e-05,
      "loss": 1.7434,
      "step": 345300
    },
    {
      "epoch": 26.38453899625697,
      "grad_norm": 5.205498218536377,
      "learning_rate": 2.8012884169785857e-05,
      "loss": 1.6922,
      "step": 345400
    },
    {
      "epoch": 26.392177832098387,
      "grad_norm": 3.993029832839966,
      "learning_rate": 2.800651847325134e-05,
      "loss": 1.6828,
      "step": 345500
    },
    {
      "epoch": 26.399816667939806,
      "grad_norm": 4.880222320556641,
      "learning_rate": 2.800015277671683e-05,
      "loss": 1.7823,
      "step": 345600
    },
    {
      "epoch": 26.407455503781225,
      "grad_norm": 5.796927452087402,
      "learning_rate": 2.7993787080182315e-05,
      "loss": 1.7026,
      "step": 345700
    },
    {
      "epoch": 26.41509433962264,
      "grad_norm": 5.11574125289917,
      "learning_rate": 2.7987421383647798e-05,
      "loss": 1.8351,
      "step": 345800
    },
    {
      "epoch": 26.42273317546406,
      "grad_norm": 4.603543758392334,
      "learning_rate": 2.7981055687113285e-05,
      "loss": 1.6959,
      "step": 345900
    },
    {
      "epoch": 26.430372011305476,
      "grad_norm": 8.41715145111084,
      "learning_rate": 2.7974689990578772e-05,
      "loss": 1.7871,
      "step": 346000
    },
    {
      "epoch": 26.438010847146895,
      "grad_norm": 4.864820957183838,
      "learning_rate": 2.7968324294044256e-05,
      "loss": 1.7282,
      "step": 346100
    },
    {
      "epoch": 26.445649682988314,
      "grad_norm": 4.848111152648926,
      "learning_rate": 2.7961958597509742e-05,
      "loss": 1.7647,
      "step": 346200
    },
    {
      "epoch": 26.45328851882973,
      "grad_norm": 5.462027072906494,
      "learning_rate": 2.7955592900975226e-05,
      "loss": 1.717,
      "step": 346300
    },
    {
      "epoch": 26.46092735467115,
      "grad_norm": 5.658600807189941,
      "learning_rate": 2.794922720444071e-05,
      "loss": 1.7196,
      "step": 346400
    },
    {
      "epoch": 26.468566190512565,
      "grad_norm": 6.058638572692871,
      "learning_rate": 2.79428615079062e-05,
      "loss": 1.7416,
      "step": 346500
    },
    {
      "epoch": 26.476205026353984,
      "grad_norm": 4.163687705993652,
      "learning_rate": 2.7936495811371683e-05,
      "loss": 1.7695,
      "step": 346600
    },
    {
      "epoch": 26.4838438621954,
      "grad_norm": 7.069519519805908,
      "learning_rate": 2.7930130114837167e-05,
      "loss": 1.6946,
      "step": 346700
    },
    {
      "epoch": 26.49148269803682,
      "grad_norm": 5.668646335601807,
      "learning_rate": 2.792376441830265e-05,
      "loss": 1.6452,
      "step": 346800
    },
    {
      "epoch": 26.499121533878238,
      "grad_norm": 5.241106986999512,
      "learning_rate": 2.791739872176814e-05,
      "loss": 1.7272,
      "step": 346900
    },
    {
      "epoch": 26.506760369719654,
      "grad_norm": 6.377179145812988,
      "learning_rate": 2.7911033025233624e-05,
      "loss": 1.6969,
      "step": 347000
    },
    {
      "epoch": 26.514399205561073,
      "grad_norm": 5.1174821853637695,
      "learning_rate": 2.7904667328699108e-05,
      "loss": 1.8115,
      "step": 347100
    },
    {
      "epoch": 26.52203804140249,
      "grad_norm": 5.129570484161377,
      "learning_rate": 2.789830163216459e-05,
      "loss": 1.6726,
      "step": 347200
    },
    {
      "epoch": 26.529676877243908,
      "grad_norm": 6.335714340209961,
      "learning_rate": 2.7891935935630075e-05,
      "loss": 1.844,
      "step": 347300
    },
    {
      "epoch": 26.537315713085327,
      "grad_norm": 4.976821422576904,
      "learning_rate": 2.7885570239095565e-05,
      "loss": 1.6513,
      "step": 347400
    },
    {
      "epoch": 26.544954548926743,
      "grad_norm": 6.349978923797607,
      "learning_rate": 2.787920454256105e-05,
      "loss": 1.7151,
      "step": 347500
    },
    {
      "epoch": 26.552593384768162,
      "grad_norm": 4.769114971160889,
      "learning_rate": 2.7872838846026532e-05,
      "loss": 1.7795,
      "step": 347600
    },
    {
      "epoch": 26.560232220609578,
      "grad_norm": 6.444522380828857,
      "learning_rate": 2.7866473149492016e-05,
      "loss": 1.7181,
      "step": 347700
    },
    {
      "epoch": 26.567871056450997,
      "grad_norm": 4.45473575592041,
      "learning_rate": 2.7860107452957503e-05,
      "loss": 1.7071,
      "step": 347800
    },
    {
      "epoch": 26.575509892292416,
      "grad_norm": 5.178365230560303,
      "learning_rate": 2.785374175642299e-05,
      "loss": 1.675,
      "step": 347900
    },
    {
      "epoch": 26.583148728133832,
      "grad_norm": 5.343338966369629,
      "learning_rate": 2.7847376059888473e-05,
      "loss": 1.7577,
      "step": 348000
    },
    {
      "epoch": 26.59078756397525,
      "grad_norm": 6.815938949584961,
      "learning_rate": 2.784101036335396e-05,
      "loss": 1.7126,
      "step": 348100
    },
    {
      "epoch": 26.598426399816667,
      "grad_norm": 5.176433086395264,
      "learning_rate": 2.7834644666819444e-05,
      "loss": 1.7763,
      "step": 348200
    },
    {
      "epoch": 26.606065235658086,
      "grad_norm": 5.606815338134766,
      "learning_rate": 2.782827897028493e-05,
      "loss": 1.6357,
      "step": 348300
    },
    {
      "epoch": 26.613704071499505,
      "grad_norm": 4.621034622192383,
      "learning_rate": 2.7821913273750418e-05,
      "loss": 1.7055,
      "step": 348400
    },
    {
      "epoch": 26.62134290734092,
      "grad_norm": 6.448976993560791,
      "learning_rate": 2.78155475772159e-05,
      "loss": 1.7127,
      "step": 348500
    },
    {
      "epoch": 26.62898174318234,
      "grad_norm": 5.745697975158691,
      "learning_rate": 2.7809181880681385e-05,
      "loss": 1.7688,
      "step": 348600
    },
    {
      "epoch": 26.636620579023756,
      "grad_norm": 5.256552696228027,
      "learning_rate": 2.7802816184146868e-05,
      "loss": 1.6432,
      "step": 348700
    },
    {
      "epoch": 26.644259414865175,
      "grad_norm": 4.952173709869385,
      "learning_rate": 2.779645048761236e-05,
      "loss": 1.8203,
      "step": 348800
    },
    {
      "epoch": 26.65189825070659,
      "grad_norm": 4.2082037925720215,
      "learning_rate": 2.7790084791077842e-05,
      "loss": 1.6673,
      "step": 348900
    },
    {
      "epoch": 26.65953708654801,
      "grad_norm": 5.091677665710449,
      "learning_rate": 2.7783719094543326e-05,
      "loss": 1.7729,
      "step": 349000
    },
    {
      "epoch": 26.66717592238943,
      "grad_norm": 4.474245548248291,
      "learning_rate": 2.777735339800881e-05,
      "loss": 1.7629,
      "step": 349100
    },
    {
      "epoch": 26.674814758230845,
      "grad_norm": 5.438687801361084,
      "learning_rate": 2.77709877014743e-05,
      "loss": 1.7428,
      "step": 349200
    },
    {
      "epoch": 26.682453594072264,
      "grad_norm": 5.4106221199035645,
      "learning_rate": 2.7764622004939783e-05,
      "loss": 1.7876,
      "step": 349300
    },
    {
      "epoch": 26.69009242991368,
      "grad_norm": 5.93723201751709,
      "learning_rate": 2.7758256308405266e-05,
      "loss": 1.6355,
      "step": 349400
    },
    {
      "epoch": 26.6977312657551,
      "grad_norm": 4.726729869842529,
      "learning_rate": 2.775189061187075e-05,
      "loss": 1.8556,
      "step": 349500
    },
    {
      "epoch": 26.705370101596518,
      "grad_norm": 5.647520065307617,
      "learning_rate": 2.7745524915336234e-05,
      "loss": 1.6509,
      "step": 349600
    },
    {
      "epoch": 26.713008937437934,
      "grad_norm": 5.263008117675781,
      "learning_rate": 2.7739159218801724e-05,
      "loss": 1.7683,
      "step": 349700
    },
    {
      "epoch": 26.720647773279353,
      "grad_norm": 4.3388447761535645,
      "learning_rate": 2.7732793522267207e-05,
      "loss": 1.748,
      "step": 349800
    },
    {
      "epoch": 26.72828660912077,
      "grad_norm": 5.379178047180176,
      "learning_rate": 2.772642782573269e-05,
      "loss": 1.7614,
      "step": 349900
    },
    {
      "epoch": 26.735925444962188,
      "grad_norm": 4.614146709442139,
      "learning_rate": 2.7720062129198178e-05,
      "loss": 1.7542,
      "step": 350000
    },
    {
      "epoch": 26.743564280803607,
      "grad_norm": 7.902753829956055,
      "learning_rate": 2.7713696432663665e-05,
      "loss": 1.7325,
      "step": 350100
    },
    {
      "epoch": 26.751203116645023,
      "grad_norm": 4.538783073425293,
      "learning_rate": 2.7707330736129152e-05,
      "loss": 1.706,
      "step": 350200
    },
    {
      "epoch": 26.758841952486442,
      "grad_norm": 4.68821907043457,
      "learning_rate": 2.7700965039594635e-05,
      "loss": 1.7744,
      "step": 350300
    },
    {
      "epoch": 26.766480788327858,
      "grad_norm": 4.548982620239258,
      "learning_rate": 2.769459934306012e-05,
      "loss": 1.9482,
      "step": 350400
    },
    {
      "epoch": 26.774119624169277,
      "grad_norm": 5.187947750091553,
      "learning_rate": 2.7688233646525602e-05,
      "loss": 1.7804,
      "step": 350500
    },
    {
      "epoch": 26.781758460010693,
      "grad_norm": 5.390008449554443,
      "learning_rate": 2.7681867949991093e-05,
      "loss": 1.6712,
      "step": 350600
    },
    {
      "epoch": 26.789397295852112,
      "grad_norm": 5.376811981201172,
      "learning_rate": 2.7675502253456576e-05,
      "loss": 1.7637,
      "step": 350700
    },
    {
      "epoch": 26.79703613169353,
      "grad_norm": 7.46491003036499,
      "learning_rate": 2.766913655692206e-05,
      "loss": 1.7466,
      "step": 350800
    },
    {
      "epoch": 26.804674967534947,
      "grad_norm": 6.159297943115234,
      "learning_rate": 2.7662770860387543e-05,
      "loss": 1.7164,
      "step": 350900
    },
    {
      "epoch": 26.812313803376366,
      "grad_norm": 6.176617622375488,
      "learning_rate": 2.7656405163853027e-05,
      "loss": 1.7179,
      "step": 351000
    },
    {
      "epoch": 26.81995263921778,
      "grad_norm": 7.047853469848633,
      "learning_rate": 2.7650039467318517e-05,
      "loss": 1.8098,
      "step": 351100
    },
    {
      "epoch": 26.8275914750592,
      "grad_norm": 5.093535423278809,
      "learning_rate": 2.7643673770784e-05,
      "loss": 1.7848,
      "step": 351200
    },
    {
      "epoch": 26.83523031090062,
      "grad_norm": 4.632694721221924,
      "learning_rate": 2.7637308074249484e-05,
      "loss": 1.747,
      "step": 351300
    },
    {
      "epoch": 26.842869146742036,
      "grad_norm": 6.076164245605469,
      "learning_rate": 2.7630942377714968e-05,
      "loss": 1.7315,
      "step": 351400
    },
    {
      "epoch": 26.850507982583455,
      "grad_norm": 6.108221530914307,
      "learning_rate": 2.7624576681180458e-05,
      "loss": 1.6503,
      "step": 351500
    },
    {
      "epoch": 26.85814681842487,
      "grad_norm": 5.7330756187438965,
      "learning_rate": 2.761821098464594e-05,
      "loss": 1.7832,
      "step": 351600
    },
    {
      "epoch": 26.86578565426629,
      "grad_norm": 7.267268180847168,
      "learning_rate": 2.7611845288111425e-05,
      "loss": 1.6926,
      "step": 351700
    },
    {
      "epoch": 26.87342449010771,
      "grad_norm": 6.165748596191406,
      "learning_rate": 2.7605479591576912e-05,
      "loss": 1.6662,
      "step": 351800
    },
    {
      "epoch": 26.881063325949125,
      "grad_norm": 5.69649600982666,
      "learning_rate": 2.7599113895042396e-05,
      "loss": 1.7543,
      "step": 351900
    },
    {
      "epoch": 26.888702161790544,
      "grad_norm": 5.330083847045898,
      "learning_rate": 2.7592748198507882e-05,
      "loss": 1.7809,
      "step": 352000
    },
    {
      "epoch": 26.89634099763196,
      "grad_norm": 8.034161567687988,
      "learning_rate": 2.758638250197337e-05,
      "loss": 1.7728,
      "step": 352100
    },
    {
      "epoch": 26.90397983347338,
      "grad_norm": 5.996140003204346,
      "learning_rate": 2.7580016805438853e-05,
      "loss": 1.7871,
      "step": 352200
    },
    {
      "epoch": 26.911618669314798,
      "grad_norm": 5.97252893447876,
      "learning_rate": 2.7573651108904336e-05,
      "loss": 1.7161,
      "step": 352300
    },
    {
      "epoch": 26.919257505156214,
      "grad_norm": 6.038491249084473,
      "learning_rate": 2.7567285412369827e-05,
      "loss": 1.7422,
      "step": 352400
    },
    {
      "epoch": 26.926896340997633,
      "grad_norm": 4.478087902069092,
      "learning_rate": 2.756091971583531e-05,
      "loss": 1.703,
      "step": 352500
    },
    {
      "epoch": 26.93453517683905,
      "grad_norm": 5.776589393615723,
      "learning_rate": 2.7554554019300794e-05,
      "loss": 1.703,
      "step": 352600
    },
    {
      "epoch": 26.942174012680468,
      "grad_norm": 5.340043067932129,
      "learning_rate": 2.7548188322766277e-05,
      "loss": 1.7554,
      "step": 352700
    },
    {
      "epoch": 26.949812848521887,
      "grad_norm": 7.591472625732422,
      "learning_rate": 2.754182262623176e-05,
      "loss": 1.8086,
      "step": 352800
    },
    {
      "epoch": 26.957451684363303,
      "grad_norm": 5.451143264770508,
      "learning_rate": 2.753545692969725e-05,
      "loss": 1.742,
      "step": 352900
    },
    {
      "epoch": 26.965090520204722,
      "grad_norm": 5.6960768699646,
      "learning_rate": 2.7529091233162735e-05,
      "loss": 1.7247,
      "step": 353000
    },
    {
      "epoch": 26.972729356046138,
      "grad_norm": 4.845108985900879,
      "learning_rate": 2.752272553662822e-05,
      "loss": 1.7045,
      "step": 353100
    },
    {
      "epoch": 26.980368191887557,
      "grad_norm": 3.6878035068511963,
      "learning_rate": 2.7516359840093702e-05,
      "loss": 1.782,
      "step": 353200
    },
    {
      "epoch": 26.988007027728973,
      "grad_norm": 4.554958820343018,
      "learning_rate": 2.7509994143559185e-05,
      "loss": 1.753,
      "step": 353300
    },
    {
      "epoch": 26.99564586357039,
      "grad_norm": 6.208313941955566,
      "learning_rate": 2.7503628447024676e-05,
      "loss": 1.7337,
      "step": 353400
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.7944233417510986,
      "eval_runtime": 3.0187,
      "eval_samples_per_second": 228.572,
      "eval_steps_per_second": 228.572,
      "step": 353457
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.502411961555481,
      "eval_runtime": 57.5733,
      "eval_samples_per_second": 227.38,
      "eval_steps_per_second": 227.38,
      "step": 353457
    },
    {
      "epoch": 27.00328469941181,
      "grad_norm": 4.803799152374268,
      "learning_rate": 2.749726275049016e-05,
      "loss": 1.7852,
      "step": 353500
    },
    {
      "epoch": 27.010923535253227,
      "grad_norm": 5.969759941101074,
      "learning_rate": 2.7490897053955643e-05,
      "loss": 1.6201,
      "step": 353600
    },
    {
      "epoch": 27.018562371094646,
      "grad_norm": 4.9624786376953125,
      "learning_rate": 2.748453135742113e-05,
      "loss": 1.6836,
      "step": 353700
    },
    {
      "epoch": 27.02620120693606,
      "grad_norm": 4.926640510559082,
      "learning_rate": 2.7478165660886617e-05,
      "loss": 1.7014,
      "step": 353800
    },
    {
      "epoch": 27.03384004277748,
      "grad_norm": 6.61851167678833,
      "learning_rate": 2.74717999643521e-05,
      "loss": 1.7862,
      "step": 353900
    },
    {
      "epoch": 27.0414788786189,
      "grad_norm": 5.645806789398193,
      "learning_rate": 2.7465434267817587e-05,
      "loss": 1.788,
      "step": 354000
    },
    {
      "epoch": 27.049117714460316,
      "grad_norm": 4.126518726348877,
      "learning_rate": 2.745906857128307e-05,
      "loss": 1.787,
      "step": 354100
    },
    {
      "epoch": 27.056756550301735,
      "grad_norm": 3.398709297180176,
      "learning_rate": 2.7452702874748554e-05,
      "loss": 1.7135,
      "step": 354200
    },
    {
      "epoch": 27.06439538614315,
      "grad_norm": 4.490538120269775,
      "learning_rate": 2.7446337178214044e-05,
      "loss": 1.7098,
      "step": 354300
    },
    {
      "epoch": 27.07203422198457,
      "grad_norm": 5.833497047424316,
      "learning_rate": 2.7439971481679528e-05,
      "loss": 1.8147,
      "step": 354400
    },
    {
      "epoch": 27.07967305782599,
      "grad_norm": 3.92034912109375,
      "learning_rate": 2.743360578514501e-05,
      "loss": 1.6354,
      "step": 354500
    },
    {
      "epoch": 27.087311893667405,
      "grad_norm": 6.040980815887451,
      "learning_rate": 2.7427240088610495e-05,
      "loss": 1.7605,
      "step": 354600
    },
    {
      "epoch": 27.094950729508824,
      "grad_norm": 5.987432479858398,
      "learning_rate": 2.7420874392075985e-05,
      "loss": 1.689,
      "step": 354700
    },
    {
      "epoch": 27.10258956535024,
      "grad_norm": 5.9015350341796875,
      "learning_rate": 2.741450869554147e-05,
      "loss": 1.8252,
      "step": 354800
    },
    {
      "epoch": 27.11022840119166,
      "grad_norm": 5.4673752784729,
      "learning_rate": 2.7408142999006952e-05,
      "loss": 1.7991,
      "step": 354900
    },
    {
      "epoch": 27.117867237033074,
      "grad_norm": 4.7845048904418945,
      "learning_rate": 2.7401777302472436e-05,
      "loss": 1.7077,
      "step": 355000
    },
    {
      "epoch": 27.125506072874494,
      "grad_norm": 5.8776140213012695,
      "learning_rate": 2.739541160593792e-05,
      "loss": 1.661,
      "step": 355100
    },
    {
      "epoch": 27.133144908715913,
      "grad_norm": 8.244344711303711,
      "learning_rate": 2.738904590940341e-05,
      "loss": 1.7435,
      "step": 355200
    },
    {
      "epoch": 27.14078374455733,
      "grad_norm": 5.8442792892456055,
      "learning_rate": 2.7382680212868893e-05,
      "loss": 1.7026,
      "step": 355300
    },
    {
      "epoch": 27.148422580398748,
      "grad_norm": 5.579628944396973,
      "learning_rate": 2.7376314516334377e-05,
      "loss": 1.7837,
      "step": 355400
    },
    {
      "epoch": 27.156061416240163,
      "grad_norm": 5.179530620574951,
      "learning_rate": 2.736994881979986e-05,
      "loss": 1.6782,
      "step": 355500
    },
    {
      "epoch": 27.163700252081583,
      "grad_norm": 5.725595951080322,
      "learning_rate": 2.736358312326535e-05,
      "loss": 1.7793,
      "step": 355600
    },
    {
      "epoch": 27.171339087923002,
      "grad_norm": 3.592095136642456,
      "learning_rate": 2.7357217426730834e-05,
      "loss": 1.6835,
      "step": 355700
    },
    {
      "epoch": 27.178977923764418,
      "grad_norm": 6.493999004364014,
      "learning_rate": 2.735085173019632e-05,
      "loss": 1.7412,
      "step": 355800
    },
    {
      "epoch": 27.186616759605837,
      "grad_norm": 5.770066261291504,
      "learning_rate": 2.7344486033661805e-05,
      "loss": 1.744,
      "step": 355900
    },
    {
      "epoch": 27.194255595447252,
      "grad_norm": 4.509318828582764,
      "learning_rate": 2.733812033712729e-05,
      "loss": 1.7202,
      "step": 356000
    },
    {
      "epoch": 27.20189443128867,
      "grad_norm": 10.843029022216797,
      "learning_rate": 2.733175464059278e-05,
      "loss": 1.8211,
      "step": 356100
    },
    {
      "epoch": 27.20953326713009,
      "grad_norm": 4.396698951721191,
      "learning_rate": 2.7325388944058262e-05,
      "loss": 1.7535,
      "step": 356200
    },
    {
      "epoch": 27.217172102971507,
      "grad_norm": 3.5282163619995117,
      "learning_rate": 2.7319023247523746e-05,
      "loss": 1.7303,
      "step": 356300
    },
    {
      "epoch": 27.224810938812926,
      "grad_norm": 4.523567199707031,
      "learning_rate": 2.731265755098923e-05,
      "loss": 1.7957,
      "step": 356400
    },
    {
      "epoch": 27.23244977465434,
      "grad_norm": 4.868929862976074,
      "learning_rate": 2.7306291854454713e-05,
      "loss": 1.672,
      "step": 356500
    },
    {
      "epoch": 27.24008861049576,
      "grad_norm": 4.606978416442871,
      "learning_rate": 2.7299926157920203e-05,
      "loss": 1.7936,
      "step": 356600
    },
    {
      "epoch": 27.24772744633718,
      "grad_norm": 3.603423595428467,
      "learning_rate": 2.7293560461385687e-05,
      "loss": 1.7333,
      "step": 356700
    },
    {
      "epoch": 27.255366282178596,
      "grad_norm": 6.36139440536499,
      "learning_rate": 2.728719476485117e-05,
      "loss": 1.7258,
      "step": 356800
    },
    {
      "epoch": 27.263005118020015,
      "grad_norm": 5.469269275665283,
      "learning_rate": 2.7280829068316654e-05,
      "loss": 1.6715,
      "step": 356900
    },
    {
      "epoch": 27.27064395386143,
      "grad_norm": 6.18309211730957,
      "learning_rate": 2.7274463371782144e-05,
      "loss": 1.6666,
      "step": 357000
    },
    {
      "epoch": 27.27828278970285,
      "grad_norm": 3.9391894340515137,
      "learning_rate": 2.7268097675247628e-05,
      "loss": 1.6854,
      "step": 357100
    },
    {
      "epoch": 27.285921625544265,
      "grad_norm": 5.911134243011475,
      "learning_rate": 2.726173197871311e-05,
      "loss": 1.6415,
      "step": 357200
    },
    {
      "epoch": 27.293560461385685,
      "grad_norm": 7.198776721954346,
      "learning_rate": 2.7255366282178595e-05,
      "loss": 1.7056,
      "step": 357300
    },
    {
      "epoch": 27.301199297227104,
      "grad_norm": 5.305093288421631,
      "learning_rate": 2.724900058564408e-05,
      "loss": 1.798,
      "step": 357400
    },
    {
      "epoch": 27.30883813306852,
      "grad_norm": 5.498727321624756,
      "learning_rate": 2.724263488910957e-05,
      "loss": 1.7431,
      "step": 357500
    },
    {
      "epoch": 27.31647696890994,
      "grad_norm": 6.864176273345947,
      "learning_rate": 2.7236269192575052e-05,
      "loss": 1.7063,
      "step": 357600
    },
    {
      "epoch": 27.324115804751354,
      "grad_norm": 6.83540153503418,
      "learning_rate": 2.722990349604054e-05,
      "loss": 1.7363,
      "step": 357700
    },
    {
      "epoch": 27.331754640592774,
      "grad_norm": 4.986609935760498,
      "learning_rate": 2.7223537799506022e-05,
      "loss": 1.709,
      "step": 357800
    },
    {
      "epoch": 27.339393476434193,
      "grad_norm": 7.802868843078613,
      "learning_rate": 2.721717210297151e-05,
      "loss": 1.6084,
      "step": 357900
    },
    {
      "epoch": 27.34703231227561,
      "grad_norm": 4.431925296783447,
      "learning_rate": 2.7210806406436996e-05,
      "loss": 1.7881,
      "step": 358000
    },
    {
      "epoch": 27.354671148117028,
      "grad_norm": 5.042713165283203,
      "learning_rate": 2.720444070990248e-05,
      "loss": 1.7467,
      "step": 358100
    },
    {
      "epoch": 27.362309983958443,
      "grad_norm": 4.623302459716797,
      "learning_rate": 2.7198075013367963e-05,
      "loss": 1.7212,
      "step": 358200
    },
    {
      "epoch": 27.369948819799863,
      "grad_norm": 4.098763942718506,
      "learning_rate": 2.7191709316833447e-05,
      "loss": 1.68,
      "step": 358300
    },
    {
      "epoch": 27.377587655641282,
      "grad_norm": 4.797390937805176,
      "learning_rate": 2.7185343620298937e-05,
      "loss": 1.7615,
      "step": 358400
    },
    {
      "epoch": 27.385226491482697,
      "grad_norm": 5.922180652618408,
      "learning_rate": 2.717897792376442e-05,
      "loss": 1.7879,
      "step": 358500
    },
    {
      "epoch": 27.392865327324117,
      "grad_norm": 4.738697528839111,
      "learning_rate": 2.7172612227229904e-05,
      "loss": 1.6846,
      "step": 358600
    },
    {
      "epoch": 27.400504163165532,
      "grad_norm": 5.761393070220947,
      "learning_rate": 2.7166246530695388e-05,
      "loss": 1.6849,
      "step": 358700
    },
    {
      "epoch": 27.40814299900695,
      "grad_norm": 4.943535327911377,
      "learning_rate": 2.7159880834160878e-05,
      "loss": 1.7555,
      "step": 358800
    },
    {
      "epoch": 27.41578183484837,
      "grad_norm": 6.0659098625183105,
      "learning_rate": 2.7153515137626362e-05,
      "loss": 1.8093,
      "step": 358900
    },
    {
      "epoch": 27.423420670689787,
      "grad_norm": 4.642782688140869,
      "learning_rate": 2.7147149441091845e-05,
      "loss": 1.6655,
      "step": 359000
    },
    {
      "epoch": 27.431059506531206,
      "grad_norm": 6.035356521606445,
      "learning_rate": 2.714078374455733e-05,
      "loss": 1.8373,
      "step": 359100
    },
    {
      "epoch": 27.43869834237262,
      "grad_norm": 6.754858016967773,
      "learning_rate": 2.7134418048022812e-05,
      "loss": 1.835,
      "step": 359200
    },
    {
      "epoch": 27.44633717821404,
      "grad_norm": 6.692112922668457,
      "learning_rate": 2.7128052351488303e-05,
      "loss": 1.8112,
      "step": 359300
    },
    {
      "epoch": 27.453976014055456,
      "grad_norm": 6.221403121948242,
      "learning_rate": 2.7121686654953786e-05,
      "loss": 1.7046,
      "step": 359400
    },
    {
      "epoch": 27.461614849896876,
      "grad_norm": 9.103565216064453,
      "learning_rate": 2.711532095841927e-05,
      "loss": 1.7756,
      "step": 359500
    },
    {
      "epoch": 27.469253685738295,
      "grad_norm": 5.143050670623779,
      "learning_rate": 2.7108955261884757e-05,
      "loss": 1.7001,
      "step": 359600
    },
    {
      "epoch": 27.47689252157971,
      "grad_norm": 5.353907108306885,
      "learning_rate": 2.710258956535024e-05,
      "loss": 1.8128,
      "step": 359700
    },
    {
      "epoch": 27.48453135742113,
      "grad_norm": 5.592231750488281,
      "learning_rate": 2.709622386881573e-05,
      "loss": 1.6273,
      "step": 359800
    },
    {
      "epoch": 27.492170193262545,
      "grad_norm": 6.092338562011719,
      "learning_rate": 2.7089858172281214e-05,
      "loss": 1.7055,
      "step": 359900
    },
    {
      "epoch": 27.499809029103965,
      "grad_norm": 4.643557548522949,
      "learning_rate": 2.7083492475746698e-05,
      "loss": 1.6514,
      "step": 360000
    },
    {
      "epoch": 27.507447864945384,
      "grad_norm": 5.957435607910156,
      "learning_rate": 2.707712677921218e-05,
      "loss": 1.7444,
      "step": 360100
    },
    {
      "epoch": 27.5150867007868,
      "grad_norm": 4.49980354309082,
      "learning_rate": 2.707076108267767e-05,
      "loss": 1.7496,
      "step": 360200
    },
    {
      "epoch": 27.52272553662822,
      "grad_norm": 4.760321617126465,
      "learning_rate": 2.7064395386143155e-05,
      "loss": 1.7938,
      "step": 360300
    },
    {
      "epoch": 27.530364372469634,
      "grad_norm": 4.812627792358398,
      "learning_rate": 2.705802968960864e-05,
      "loss": 1.7999,
      "step": 360400
    },
    {
      "epoch": 27.538003208311054,
      "grad_norm": 7.7141804695129395,
      "learning_rate": 2.7051663993074122e-05,
      "loss": 1.6786,
      "step": 360500
    },
    {
      "epoch": 27.545642044152473,
      "grad_norm": 4.558579444885254,
      "learning_rate": 2.7045298296539606e-05,
      "loss": 1.7766,
      "step": 360600
    },
    {
      "epoch": 27.55328087999389,
      "grad_norm": 7.340663909912109,
      "learning_rate": 2.7038932600005096e-05,
      "loss": 1.6874,
      "step": 360700
    },
    {
      "epoch": 27.560919715835308,
      "grad_norm": 4.128868579864502,
      "learning_rate": 2.703256690347058e-05,
      "loss": 1.6911,
      "step": 360800
    },
    {
      "epoch": 27.568558551676723,
      "grad_norm": 6.661353588104248,
      "learning_rate": 2.7026201206936063e-05,
      "loss": 1.6879,
      "step": 360900
    },
    {
      "epoch": 27.576197387518143,
      "grad_norm": 5.2484211921691895,
      "learning_rate": 2.7019835510401546e-05,
      "loss": 1.7987,
      "step": 361000
    },
    {
      "epoch": 27.58383622335956,
      "grad_norm": 5.637572765350342,
      "learning_rate": 2.7013469813867037e-05,
      "loss": 1.7243,
      "step": 361100
    },
    {
      "epoch": 27.591475059200977,
      "grad_norm": 4.467207908630371,
      "learning_rate": 2.700710411733252e-05,
      "loss": 1.8244,
      "step": 361200
    },
    {
      "epoch": 27.599113895042397,
      "grad_norm": 6.1240434646606445,
      "learning_rate": 2.7000738420798004e-05,
      "loss": 1.6184,
      "step": 361300
    },
    {
      "epoch": 27.606752730883812,
      "grad_norm": 6.55259895324707,
      "learning_rate": 2.699437272426349e-05,
      "loss": 1.7583,
      "step": 361400
    },
    {
      "epoch": 27.61439156672523,
      "grad_norm": 4.800669193267822,
      "learning_rate": 2.6988007027728974e-05,
      "loss": 1.7409,
      "step": 361500
    },
    {
      "epoch": 27.622030402566647,
      "grad_norm": 5.124151229858398,
      "learning_rate": 2.698164133119446e-05,
      "loss": 1.6442,
      "step": 361600
    },
    {
      "epoch": 27.629669238408066,
      "grad_norm": 5.421281814575195,
      "learning_rate": 2.6975275634659948e-05,
      "loss": 1.7696,
      "step": 361700
    },
    {
      "epoch": 27.637308074249486,
      "grad_norm": 6.237902641296387,
      "learning_rate": 2.6968909938125432e-05,
      "loss": 1.7579,
      "step": 361800
    },
    {
      "epoch": 27.6449469100909,
      "grad_norm": 4.374264240264893,
      "learning_rate": 2.6962544241590915e-05,
      "loss": 1.591,
      "step": 361900
    },
    {
      "epoch": 27.65258574593232,
      "grad_norm": 5.999856948852539,
      "learning_rate": 2.69561785450564e-05,
      "loss": 1.6726,
      "step": 362000
    },
    {
      "epoch": 27.660224581773736,
      "grad_norm": 5.281294822692871,
      "learning_rate": 2.694981284852189e-05,
      "loss": 1.6211,
      "step": 362100
    },
    {
      "epoch": 27.667863417615155,
      "grad_norm": 5.079476833343506,
      "learning_rate": 2.6943447151987373e-05,
      "loss": 1.8854,
      "step": 362200
    },
    {
      "epoch": 27.675502253456575,
      "grad_norm": 5.333648204803467,
      "learning_rate": 2.6937081455452856e-05,
      "loss": 1.7499,
      "step": 362300
    },
    {
      "epoch": 27.68314108929799,
      "grad_norm": 5.814057350158691,
      "learning_rate": 2.693071575891834e-05,
      "loss": 1.7353,
      "step": 362400
    },
    {
      "epoch": 27.69077992513941,
      "grad_norm": 5.743632793426514,
      "learning_rate": 2.692435006238383e-05,
      "loss": 1.6721,
      "step": 362500
    },
    {
      "epoch": 27.698418760980825,
      "grad_norm": 4.305292129516602,
      "learning_rate": 2.6917984365849314e-05,
      "loss": 1.6664,
      "step": 362600
    },
    {
      "epoch": 27.706057596822244,
      "grad_norm": 5.483376502990723,
      "learning_rate": 2.6911618669314797e-05,
      "loss": 1.7308,
      "step": 362700
    },
    {
      "epoch": 27.713696432663664,
      "grad_norm": 5.844059944152832,
      "learning_rate": 2.690525297278028e-05,
      "loss": 1.6522,
      "step": 362800
    },
    {
      "epoch": 27.72133526850508,
      "grad_norm": 5.849111080169678,
      "learning_rate": 2.6898887276245764e-05,
      "loss": 1.7769,
      "step": 362900
    },
    {
      "epoch": 27.7289741043465,
      "grad_norm": 3.7758469581604004,
      "learning_rate": 2.6892521579711254e-05,
      "loss": 1.7585,
      "step": 363000
    },
    {
      "epoch": 27.736612940187914,
      "grad_norm": 6.965542316436768,
      "learning_rate": 2.6886155883176738e-05,
      "loss": 1.7134,
      "step": 363100
    },
    {
      "epoch": 27.744251776029333,
      "grad_norm": 4.207866668701172,
      "learning_rate": 2.687979018664222e-05,
      "loss": 1.7401,
      "step": 363200
    },
    {
      "epoch": 27.75189061187075,
      "grad_norm": 5.908998012542725,
      "learning_rate": 2.687342449010771e-05,
      "loss": 1.763,
      "step": 363300
    },
    {
      "epoch": 27.75952944771217,
      "grad_norm": 5.840761661529541,
      "learning_rate": 2.6867058793573195e-05,
      "loss": 1.7598,
      "step": 363400
    },
    {
      "epoch": 27.767168283553588,
      "grad_norm": 5.7100982666015625,
      "learning_rate": 2.686069309703868e-05,
      "loss": 1.8017,
      "step": 363500
    },
    {
      "epoch": 27.774807119395003,
      "grad_norm": 2.426603317260742,
      "learning_rate": 2.6854327400504166e-05,
      "loss": 1.7042,
      "step": 363600
    },
    {
      "epoch": 27.782445955236422,
      "grad_norm": 6.995980262756348,
      "learning_rate": 2.684796170396965e-05,
      "loss": 1.7183,
      "step": 363700
    },
    {
      "epoch": 27.790084791077838,
      "grad_norm": 8.025906562805176,
      "learning_rate": 2.6841596007435133e-05,
      "loss": 1.8187,
      "step": 363800
    },
    {
      "epoch": 27.797723626919257,
      "grad_norm": 5.174130439758301,
      "learning_rate": 2.6835230310900623e-05,
      "loss": 1.7642,
      "step": 363900
    },
    {
      "epoch": 27.805362462760677,
      "grad_norm": 5.284183025360107,
      "learning_rate": 2.6828864614366107e-05,
      "loss": 1.7394,
      "step": 364000
    },
    {
      "epoch": 27.813001298602092,
      "grad_norm": 6.600344181060791,
      "learning_rate": 2.682249891783159e-05,
      "loss": 1.7648,
      "step": 364100
    },
    {
      "epoch": 27.82064013444351,
      "grad_norm": 6.036309719085693,
      "learning_rate": 2.6816133221297074e-05,
      "loss": 1.7324,
      "step": 364200
    },
    {
      "epoch": 27.828278970284927,
      "grad_norm": 4.941290378570557,
      "learning_rate": 2.6809767524762564e-05,
      "loss": 1.7426,
      "step": 364300
    },
    {
      "epoch": 27.835917806126346,
      "grad_norm": 5.086702346801758,
      "learning_rate": 2.6803401828228048e-05,
      "loss": 1.7533,
      "step": 364400
    },
    {
      "epoch": 27.843556641967766,
      "grad_norm": 6.510773658752441,
      "learning_rate": 2.679703613169353e-05,
      "loss": 1.6579,
      "step": 364500
    },
    {
      "epoch": 27.85119547780918,
      "grad_norm": 6.870227813720703,
      "learning_rate": 2.6790670435159015e-05,
      "loss": 1.7321,
      "step": 364600
    },
    {
      "epoch": 27.8588343136506,
      "grad_norm": 5.028481960296631,
      "learning_rate": 2.6784304738624498e-05,
      "loss": 1.8031,
      "step": 364700
    },
    {
      "epoch": 27.866473149492016,
      "grad_norm": 2.770446300506592,
      "learning_rate": 2.677793904208999e-05,
      "loss": 1.7649,
      "step": 364800
    },
    {
      "epoch": 27.874111985333435,
      "grad_norm": 4.208221912384033,
      "learning_rate": 2.6771573345555472e-05,
      "loss": 1.6538,
      "step": 364900
    },
    {
      "epoch": 27.881750821174855,
      "grad_norm": 6.105583667755127,
      "learning_rate": 2.6765207649020956e-05,
      "loss": 1.7871,
      "step": 365000
    },
    {
      "epoch": 27.88938965701627,
      "grad_norm": 5.362480163574219,
      "learning_rate": 2.675884195248644e-05,
      "loss": 1.6742,
      "step": 365100
    },
    {
      "epoch": 27.89702849285769,
      "grad_norm": 5.937629222869873,
      "learning_rate": 2.6752476255951926e-05,
      "loss": 1.7539,
      "step": 365200
    },
    {
      "epoch": 27.904667328699105,
      "grad_norm": 5.520604133605957,
      "learning_rate": 2.6746110559417413e-05,
      "loss": 1.6572,
      "step": 365300
    },
    {
      "epoch": 27.912306164540524,
      "grad_norm": 6.406582832336426,
      "learning_rate": 2.67397448628829e-05,
      "loss": 1.8531,
      "step": 365400
    },
    {
      "epoch": 27.919945000381944,
      "grad_norm": 6.901878833770752,
      "learning_rate": 2.6733379166348384e-05,
      "loss": 1.8206,
      "step": 365500
    },
    {
      "epoch": 27.92758383622336,
      "grad_norm": 5.983466148376465,
      "learning_rate": 2.6727013469813867e-05,
      "loss": 1.6706,
      "step": 365600
    },
    {
      "epoch": 27.93522267206478,
      "grad_norm": 4.495162010192871,
      "learning_rate": 2.6720647773279357e-05,
      "loss": 1.7923,
      "step": 365700
    },
    {
      "epoch": 27.942861507906194,
      "grad_norm": 5.442444324493408,
      "learning_rate": 2.671428207674484e-05,
      "loss": 1.6966,
      "step": 365800
    },
    {
      "epoch": 27.950500343747613,
      "grad_norm": 7.358706951141357,
      "learning_rate": 2.6707916380210324e-05,
      "loss": 1.6792,
      "step": 365900
    },
    {
      "epoch": 27.95813917958903,
      "grad_norm": 4.343113422393799,
      "learning_rate": 2.6701550683675808e-05,
      "loss": 1.7214,
      "step": 366000
    },
    {
      "epoch": 27.96577801543045,
      "grad_norm": 5.9090118408203125,
      "learning_rate": 2.669518498714129e-05,
      "loss": 1.8349,
      "step": 366100
    },
    {
      "epoch": 27.973416851271868,
      "grad_norm": 5.6865739822387695,
      "learning_rate": 2.6688819290606782e-05,
      "loss": 1.7221,
      "step": 366200
    },
    {
      "epoch": 27.981055687113283,
      "grad_norm": 5.869864463806152,
      "learning_rate": 2.6682453594072265e-05,
      "loss": 1.8189,
      "step": 366300
    },
    {
      "epoch": 27.988694522954702,
      "grad_norm": 4.995884418487549,
      "learning_rate": 2.667608789753775e-05,
      "loss": 1.8242,
      "step": 366400
    },
    {
      "epoch": 27.996333358796118,
      "grad_norm": 5.301004409790039,
      "learning_rate": 2.6669722201003232e-05,
      "loss": 1.7708,
      "step": 366500
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.7847404479980469,
      "eval_runtime": 1.5594,
      "eval_samples_per_second": 442.485,
      "eval_steps_per_second": 442.485,
      "step": 366548
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.4960373640060425,
      "eval_runtime": 38.3165,
      "eval_samples_per_second": 341.654,
      "eval_steps_per_second": 341.654,
      "step": 366548
    },
    {
      "epoch": 28.003972194637537,
      "grad_norm": 4.698646545410156,
      "learning_rate": 2.6663356504468723e-05,
      "loss": 1.7694,
      "step": 366600
    },
    {
      "epoch": 28.011611030478957,
      "grad_norm": 6.530004978179932,
      "learning_rate": 2.6656990807934206e-05,
      "loss": 1.768,
      "step": 366700
    },
    {
      "epoch": 28.019249866320372,
      "grad_norm": 4.434330940246582,
      "learning_rate": 2.665062511139969e-05,
      "loss": 1.6948,
      "step": 366800
    },
    {
      "epoch": 28.02688870216179,
      "grad_norm": 6.67392635345459,
      "learning_rate": 2.6644259414865173e-05,
      "loss": 1.7773,
      "step": 366900
    },
    {
      "epoch": 28.034527538003207,
      "grad_norm": 4.879701614379883,
      "learning_rate": 2.663789371833066e-05,
      "loss": 1.6825,
      "step": 367000
    },
    {
      "epoch": 28.042166373844626,
      "grad_norm": 6.206464767456055,
      "learning_rate": 2.6631528021796147e-05,
      "loss": 1.7064,
      "step": 367100
    },
    {
      "epoch": 28.049805209686046,
      "grad_norm": 4.932743549346924,
      "learning_rate": 2.662516232526163e-05,
      "loss": 1.7634,
      "step": 367200
    },
    {
      "epoch": 28.05744404552746,
      "grad_norm": 5.585020065307617,
      "learning_rate": 2.6618796628727118e-05,
      "loss": 1.6396,
      "step": 367300
    },
    {
      "epoch": 28.06508288136888,
      "grad_norm": 4.107214450836182,
      "learning_rate": 2.66124309321926e-05,
      "loss": 1.5996,
      "step": 367400
    },
    {
      "epoch": 28.072721717210296,
      "grad_norm": 5.064294338226318,
      "learning_rate": 2.6606065235658088e-05,
      "loss": 1.7468,
      "step": 367500
    },
    {
      "epoch": 28.080360553051715,
      "grad_norm": 9.249030113220215,
      "learning_rate": 2.6599699539123575e-05,
      "loss": 1.6961,
      "step": 367600
    },
    {
      "epoch": 28.08799938889313,
      "grad_norm": 4.427839756011963,
      "learning_rate": 2.659333384258906e-05,
      "loss": 1.7416,
      "step": 367700
    },
    {
      "epoch": 28.09563822473455,
      "grad_norm": 6.040685176849365,
      "learning_rate": 2.6586968146054542e-05,
      "loss": 1.7371,
      "step": 367800
    },
    {
      "epoch": 28.10327706057597,
      "grad_norm": 6.519083023071289,
      "learning_rate": 2.6580602449520026e-05,
      "loss": 1.6773,
      "step": 367900
    },
    {
      "epoch": 28.110915896417385,
      "grad_norm": 5.143161296844482,
      "learning_rate": 2.6574236752985516e-05,
      "loss": 1.7692,
      "step": 368000
    },
    {
      "epoch": 28.118554732258804,
      "grad_norm": 6.501771450042725,
      "learning_rate": 2.6567871056451e-05,
      "loss": 1.8384,
      "step": 368100
    },
    {
      "epoch": 28.12619356810022,
      "grad_norm": 4.971611499786377,
      "learning_rate": 2.6561505359916483e-05,
      "loss": 1.72,
      "step": 368200
    },
    {
      "epoch": 28.13383240394164,
      "grad_norm": 4.549580097198486,
      "learning_rate": 2.6555139663381967e-05,
      "loss": 1.8175,
      "step": 368300
    },
    {
      "epoch": 28.14147123978306,
      "grad_norm": 4.445704460144043,
      "learning_rate": 2.654877396684745e-05,
      "loss": 1.8157,
      "step": 368400
    },
    {
      "epoch": 28.149110075624474,
      "grad_norm": 5.90997314453125,
      "learning_rate": 2.654240827031294e-05,
      "loss": 1.7756,
      "step": 368500
    },
    {
      "epoch": 28.156748911465893,
      "grad_norm": 4.856388092041016,
      "learning_rate": 2.6536042573778424e-05,
      "loss": 1.7154,
      "step": 368600
    },
    {
      "epoch": 28.16438774730731,
      "grad_norm": 5.104438304901123,
      "learning_rate": 2.6529676877243908e-05,
      "loss": 1.7558,
      "step": 368700
    },
    {
      "epoch": 28.172026583148728,
      "grad_norm": 4.948068141937256,
      "learning_rate": 2.652331118070939e-05,
      "loss": 1.7516,
      "step": 368800
    },
    {
      "epoch": 28.179665418990147,
      "grad_norm": 4.33783483505249,
      "learning_rate": 2.651694548417488e-05,
      "loss": 1.7259,
      "step": 368900
    },
    {
      "epoch": 28.187304254831563,
      "grad_norm": 5.78506326675415,
      "learning_rate": 2.6510579787640365e-05,
      "loss": 1.7388,
      "step": 369000
    },
    {
      "epoch": 28.194943090672982,
      "grad_norm": 4.030710220336914,
      "learning_rate": 2.650421409110585e-05,
      "loss": 1.6621,
      "step": 369100
    },
    {
      "epoch": 28.202581926514398,
      "grad_norm": 5.466397285461426,
      "learning_rate": 2.6497848394571335e-05,
      "loss": 1.7221,
      "step": 369200
    },
    {
      "epoch": 28.210220762355817,
      "grad_norm": 5.993225574493408,
      "learning_rate": 2.649148269803682e-05,
      "loss": 1.7856,
      "step": 369300
    },
    {
      "epoch": 28.217859598197236,
      "grad_norm": 6.572902202606201,
      "learning_rate": 2.648511700150231e-05,
      "loss": 1.8668,
      "step": 369400
    },
    {
      "epoch": 28.225498434038652,
      "grad_norm": 5.336798191070557,
      "learning_rate": 2.6478751304967793e-05,
      "loss": 1.675,
      "step": 369500
    },
    {
      "epoch": 28.23313726988007,
      "grad_norm": 5.092998027801514,
      "learning_rate": 2.6472385608433276e-05,
      "loss": 1.7041,
      "step": 369600
    },
    {
      "epoch": 28.240776105721487,
      "grad_norm": 6.823355197906494,
      "learning_rate": 2.646601991189876e-05,
      "loss": 1.7376,
      "step": 369700
    },
    {
      "epoch": 28.248414941562906,
      "grad_norm": 4.986593246459961,
      "learning_rate": 2.645965421536425e-05,
      "loss": 1.759,
      "step": 369800
    },
    {
      "epoch": 28.256053777404322,
      "grad_norm": 6.573587417602539,
      "learning_rate": 2.6453288518829734e-05,
      "loss": 1.66,
      "step": 369900
    },
    {
      "epoch": 28.26369261324574,
      "grad_norm": 8.05463695526123,
      "learning_rate": 2.6446922822295217e-05,
      "loss": 1.6742,
      "step": 370000
    },
    {
      "epoch": 28.27133144908716,
      "grad_norm": 5.876303195953369,
      "learning_rate": 2.64405571257607e-05,
      "loss": 1.6278,
      "step": 370100
    },
    {
      "epoch": 28.278970284928576,
      "grad_norm": 4.852136135101318,
      "learning_rate": 2.6434191429226184e-05,
      "loss": 1.6846,
      "step": 370200
    },
    {
      "epoch": 28.286609120769995,
      "grad_norm": 5.23798131942749,
      "learning_rate": 2.6427825732691675e-05,
      "loss": 1.6536,
      "step": 370300
    },
    {
      "epoch": 28.29424795661141,
      "grad_norm": 4.407510280609131,
      "learning_rate": 2.6421460036157158e-05,
      "loss": 1.7961,
      "step": 370400
    },
    {
      "epoch": 28.30188679245283,
      "grad_norm": 6.692582130432129,
      "learning_rate": 2.641509433962264e-05,
      "loss": 1.725,
      "step": 370500
    },
    {
      "epoch": 28.30952562829425,
      "grad_norm": 6.214886665344238,
      "learning_rate": 2.6408728643088125e-05,
      "loss": 1.7391,
      "step": 370600
    },
    {
      "epoch": 28.317164464135665,
      "grad_norm": 5.330878734588623,
      "learning_rate": 2.640236294655361e-05,
      "loss": 1.776,
      "step": 370700
    },
    {
      "epoch": 28.324803299977084,
      "grad_norm": 6.904229164123535,
      "learning_rate": 2.63959972500191e-05,
      "loss": 1.8365,
      "step": 370800
    },
    {
      "epoch": 28.3324421358185,
      "grad_norm": 4.356451511383057,
      "learning_rate": 2.6389631553484583e-05,
      "loss": 1.7102,
      "step": 370900
    },
    {
      "epoch": 28.34008097165992,
      "grad_norm": 4.879714488983154,
      "learning_rate": 2.638326585695007e-05,
      "loss": 1.7306,
      "step": 371000
    },
    {
      "epoch": 28.34771980750134,
      "grad_norm": 5.434067249298096,
      "learning_rate": 2.6376900160415553e-05,
      "loss": 1.6653,
      "step": 371100
    },
    {
      "epoch": 28.355358643342754,
      "grad_norm": 5.2747039794921875,
      "learning_rate": 2.637053446388104e-05,
      "loss": 1.76,
      "step": 371200
    },
    {
      "epoch": 28.362997479184173,
      "grad_norm": 5.024360179901123,
      "learning_rate": 2.6364168767346527e-05,
      "loss": 1.704,
      "step": 371300
    },
    {
      "epoch": 28.37063631502559,
      "grad_norm": 5.549657344818115,
      "learning_rate": 2.635780307081201e-05,
      "loss": 1.6941,
      "step": 371400
    },
    {
      "epoch": 28.378275150867008,
      "grad_norm": 6.638161659240723,
      "learning_rate": 2.6351437374277494e-05,
      "loss": 1.7498,
      "step": 371500
    },
    {
      "epoch": 28.385913986708427,
      "grad_norm": 5.605391025543213,
      "learning_rate": 2.6345071677742978e-05,
      "loss": 1.7698,
      "step": 371600
    },
    {
      "epoch": 28.393552822549843,
      "grad_norm": 4.892289638519287,
      "learning_rate": 2.6338705981208468e-05,
      "loss": 1.7526,
      "step": 371700
    },
    {
      "epoch": 28.401191658391262,
      "grad_norm": 6.637102127075195,
      "learning_rate": 2.633234028467395e-05,
      "loss": 1.6453,
      "step": 371800
    },
    {
      "epoch": 28.408830494232678,
      "grad_norm": 6.141404151916504,
      "learning_rate": 2.6325974588139435e-05,
      "loss": 1.7188,
      "step": 371900
    },
    {
      "epoch": 28.416469330074097,
      "grad_norm": 5.247582912445068,
      "learning_rate": 2.631960889160492e-05,
      "loss": 1.8898,
      "step": 372000
    },
    {
      "epoch": 28.424108165915513,
      "grad_norm": 4.444952487945557,
      "learning_rate": 2.631324319507041e-05,
      "loss": 1.798,
      "step": 372100
    },
    {
      "epoch": 28.431747001756932,
      "grad_norm": 8.68967342376709,
      "learning_rate": 2.6306877498535892e-05,
      "loss": 1.7439,
      "step": 372200
    },
    {
      "epoch": 28.43938583759835,
      "grad_norm": 6.795529365539551,
      "learning_rate": 2.6300511802001376e-05,
      "loss": 1.5903,
      "step": 372300
    },
    {
      "epoch": 28.447024673439767,
      "grad_norm": 4.947595596313477,
      "learning_rate": 2.629414610546686e-05,
      "loss": 1.6772,
      "step": 372400
    },
    {
      "epoch": 28.454663509281186,
      "grad_norm": 6.655381202697754,
      "learning_rate": 2.6287780408932343e-05,
      "loss": 1.7648,
      "step": 372500
    },
    {
      "epoch": 28.462302345122602,
      "grad_norm": 4.595761775970459,
      "learning_rate": 2.6281414712397833e-05,
      "loss": 1.7695,
      "step": 372600
    },
    {
      "epoch": 28.46994118096402,
      "grad_norm": 4.511509418487549,
      "learning_rate": 2.6275049015863317e-05,
      "loss": 1.7176,
      "step": 372700
    },
    {
      "epoch": 28.47758001680544,
      "grad_norm": 6.777787208557129,
      "learning_rate": 2.62686833193288e-05,
      "loss": 1.6492,
      "step": 372800
    },
    {
      "epoch": 28.485218852646856,
      "grad_norm": 6.251708984375,
      "learning_rate": 2.6262317622794287e-05,
      "loss": 1.6261,
      "step": 372900
    },
    {
      "epoch": 28.492857688488275,
      "grad_norm": 6.483322620391846,
      "learning_rate": 2.6255951926259774e-05,
      "loss": 1.738,
      "step": 373000
    },
    {
      "epoch": 28.50049652432969,
      "grad_norm": 5.943048477172852,
      "learning_rate": 2.6249586229725258e-05,
      "loss": 1.7384,
      "step": 373100
    },
    {
      "epoch": 28.50813536017111,
      "grad_norm": 5.91445255279541,
      "learning_rate": 2.6243220533190745e-05,
      "loss": 1.7107,
      "step": 373200
    },
    {
      "epoch": 28.51577419601253,
      "grad_norm": 5.129137992858887,
      "learning_rate": 2.6236854836656228e-05,
      "loss": 1.6813,
      "step": 373300
    },
    {
      "epoch": 28.523413031853945,
      "grad_norm": 6.3960113525390625,
      "learning_rate": 2.623048914012171e-05,
      "loss": 1.6691,
      "step": 373400
    },
    {
      "epoch": 28.531051867695364,
      "grad_norm": 6.658360958099365,
      "learning_rate": 2.6224123443587202e-05,
      "loss": 1.7665,
      "step": 373500
    },
    {
      "epoch": 28.53869070353678,
      "grad_norm": 6.455083847045898,
      "learning_rate": 2.6217757747052686e-05,
      "loss": 1.676,
      "step": 373600
    },
    {
      "epoch": 28.5463295393782,
      "grad_norm": 4.740650653839111,
      "learning_rate": 2.621139205051817e-05,
      "loss": 1.6421,
      "step": 373700
    },
    {
      "epoch": 28.55396837521962,
      "grad_norm": 6.200715065002441,
      "learning_rate": 2.6205026353983653e-05,
      "loss": 1.7536,
      "step": 373800
    },
    {
      "epoch": 28.561607211061034,
      "grad_norm": 6.717466354370117,
      "learning_rate": 2.6198660657449136e-05,
      "loss": 1.7039,
      "step": 373900
    },
    {
      "epoch": 28.569246046902453,
      "grad_norm": 6.9489922523498535,
      "learning_rate": 2.6192294960914626e-05,
      "loss": 1.7568,
      "step": 374000
    },
    {
      "epoch": 28.57688488274387,
      "grad_norm": 7.5249857902526855,
      "learning_rate": 2.618592926438011e-05,
      "loss": 1.6577,
      "step": 374100
    },
    {
      "epoch": 28.584523718585288,
      "grad_norm": 4.568755626678467,
      "learning_rate": 2.6179563567845594e-05,
      "loss": 1.789,
      "step": 374200
    },
    {
      "epoch": 28.592162554426704,
      "grad_norm": 5.782225608825684,
      "learning_rate": 2.6173197871311077e-05,
      "loss": 1.7182,
      "step": 374300
    },
    {
      "epoch": 28.599801390268123,
      "grad_norm": 4.3847551345825195,
      "learning_rate": 2.6166832174776567e-05,
      "loss": 1.697,
      "step": 374400
    },
    {
      "epoch": 28.607440226109542,
      "grad_norm": 6.31898307800293,
      "learning_rate": 2.616046647824205e-05,
      "loss": 1.87,
      "step": 374500
    },
    {
      "epoch": 28.615079061950958,
      "grad_norm": 5.31646728515625,
      "learning_rate": 2.6154100781707534e-05,
      "loss": 1.6976,
      "step": 374600
    },
    {
      "epoch": 28.622717897792377,
      "grad_norm": 5.790056228637695,
      "learning_rate": 2.6147735085173018e-05,
      "loss": 1.8109,
      "step": 374700
    },
    {
      "epoch": 28.630356733633793,
      "grad_norm": 8.070232391357422,
      "learning_rate": 2.6141369388638505e-05,
      "loss": 1.7686,
      "step": 374800
    },
    {
      "epoch": 28.637995569475212,
      "grad_norm": 5.782084941864014,
      "learning_rate": 2.6135003692103992e-05,
      "loss": 1.6718,
      "step": 374900
    },
    {
      "epoch": 28.64563440531663,
      "grad_norm": 5.064765930175781,
      "learning_rate": 2.612863799556948e-05,
      "loss": 1.7729,
      "step": 375000
    },
    {
      "epoch": 28.653273241158047,
      "grad_norm": 5.1283369064331055,
      "learning_rate": 2.6122272299034962e-05,
      "loss": 1.7536,
      "step": 375100
    },
    {
      "epoch": 28.660912076999466,
      "grad_norm": 4.906350612640381,
      "learning_rate": 2.6115906602500446e-05,
      "loss": 1.7266,
      "step": 375200
    },
    {
      "epoch": 28.66855091284088,
      "grad_norm": 6.665738105773926,
      "learning_rate": 2.6109540905965936e-05,
      "loss": 1.7253,
      "step": 375300
    },
    {
      "epoch": 28.6761897486823,
      "grad_norm": 7.377460479736328,
      "learning_rate": 2.610317520943142e-05,
      "loss": 1.801,
      "step": 375400
    },
    {
      "epoch": 28.68382858452372,
      "grad_norm": 5.798818111419678,
      "learning_rate": 2.6096809512896903e-05,
      "loss": 1.722,
      "step": 375500
    },
    {
      "epoch": 28.691467420365136,
      "grad_norm": 4.144550800323486,
      "learning_rate": 2.6090443816362387e-05,
      "loss": 1.7457,
      "step": 375600
    },
    {
      "epoch": 28.699106256206555,
      "grad_norm": 5.146682262420654,
      "learning_rate": 2.608407811982787e-05,
      "loss": 1.7165,
      "step": 375700
    },
    {
      "epoch": 28.70674509204797,
      "grad_norm": 5.241578102111816,
      "learning_rate": 2.607771242329336e-05,
      "loss": 1.7618,
      "step": 375800
    },
    {
      "epoch": 28.71438392788939,
      "grad_norm": 5.400286674499512,
      "learning_rate": 2.6071346726758844e-05,
      "loss": 1.6201,
      "step": 375900
    },
    {
      "epoch": 28.722022763730806,
      "grad_norm": 4.471656799316406,
      "learning_rate": 2.6064981030224328e-05,
      "loss": 1.7877,
      "step": 376000
    },
    {
      "epoch": 28.729661599572225,
      "grad_norm": 4.9845733642578125,
      "learning_rate": 2.605861533368981e-05,
      "loss": 1.8054,
      "step": 376100
    },
    {
      "epoch": 28.737300435413644,
      "grad_norm": 4.663106918334961,
      "learning_rate": 2.60522496371553e-05,
      "loss": 1.7077,
      "step": 376200
    },
    {
      "epoch": 28.74493927125506,
      "grad_norm": 5.270319938659668,
      "learning_rate": 2.6045883940620785e-05,
      "loss": 1.7359,
      "step": 376300
    },
    {
      "epoch": 28.75257810709648,
      "grad_norm": 4.8023247718811035,
      "learning_rate": 2.603951824408627e-05,
      "loss": 1.687,
      "step": 376400
    },
    {
      "epoch": 28.760216942937895,
      "grad_norm": 4.688981056213379,
      "learning_rate": 2.6033152547551752e-05,
      "loss": 1.7887,
      "step": 376500
    },
    {
      "epoch": 28.767855778779314,
      "grad_norm": 6.063352108001709,
      "learning_rate": 2.602678685101724e-05,
      "loss": 1.759,
      "step": 376600
    },
    {
      "epoch": 28.775494614620733,
      "grad_norm": 6.651668071746826,
      "learning_rate": 2.6020421154482726e-05,
      "loss": 1.7255,
      "step": 376700
    },
    {
      "epoch": 28.78313345046215,
      "grad_norm": 5.1544365882873535,
      "learning_rate": 2.601405545794821e-05,
      "loss": 1.7303,
      "step": 376800
    },
    {
      "epoch": 28.790772286303568,
      "grad_norm": 5.188780307769775,
      "learning_rate": 2.6007689761413696e-05,
      "loss": 1.7397,
      "step": 376900
    },
    {
      "epoch": 28.798411122144984,
      "grad_norm": 5.726927757263184,
      "learning_rate": 2.600132406487918e-05,
      "loss": 1.7612,
      "step": 377000
    },
    {
      "epoch": 28.806049957986403,
      "grad_norm": 5.499741077423096,
      "learning_rate": 2.5994958368344664e-05,
      "loss": 1.8238,
      "step": 377100
    },
    {
      "epoch": 28.813688793827822,
      "grad_norm": 4.778005123138428,
      "learning_rate": 2.5988592671810154e-05,
      "loss": 1.7127,
      "step": 377200
    },
    {
      "epoch": 28.821327629669238,
      "grad_norm": 4.901310443878174,
      "learning_rate": 2.5982226975275637e-05,
      "loss": 1.7441,
      "step": 377300
    },
    {
      "epoch": 28.828966465510657,
      "grad_norm": 5.121355056762695,
      "learning_rate": 2.597586127874112e-05,
      "loss": 1.822,
      "step": 377400
    },
    {
      "epoch": 28.836605301352073,
      "grad_norm": 6.001650333404541,
      "learning_rate": 2.5969495582206604e-05,
      "loss": 1.753,
      "step": 377500
    },
    {
      "epoch": 28.844244137193492,
      "grad_norm": 9.334463119506836,
      "learning_rate": 2.5963129885672095e-05,
      "loss": 1.7076,
      "step": 377600
    },
    {
      "epoch": 28.85188297303491,
      "grad_norm": 9.22406005859375,
      "learning_rate": 2.5956764189137578e-05,
      "loss": 1.7658,
      "step": 377700
    },
    {
      "epoch": 28.859521808876327,
      "grad_norm": 5.131890773773193,
      "learning_rate": 2.5950398492603062e-05,
      "loss": 1.6891,
      "step": 377800
    },
    {
      "epoch": 28.867160644717746,
      "grad_norm": 3.1273140907287598,
      "learning_rate": 2.5944032796068545e-05,
      "loss": 1.6532,
      "step": 377900
    },
    {
      "epoch": 28.87479948055916,
      "grad_norm": 6.615315914154053,
      "learning_rate": 2.593766709953403e-05,
      "loss": 1.7564,
      "step": 378000
    },
    {
      "epoch": 28.88243831640058,
      "grad_norm": 4.672173500061035,
      "learning_rate": 2.593130140299952e-05,
      "loss": 1.7514,
      "step": 378100
    },
    {
      "epoch": 28.890077152242,
      "grad_norm": 3.989088296890259,
      "learning_rate": 2.5924935706465003e-05,
      "loss": 1.7997,
      "step": 378200
    },
    {
      "epoch": 28.897715988083416,
      "grad_norm": 4.816130638122559,
      "learning_rate": 2.5918570009930486e-05,
      "loss": 1.7347,
      "step": 378300
    },
    {
      "epoch": 28.905354823924835,
      "grad_norm": 4.521337985992432,
      "learning_rate": 2.591220431339597e-05,
      "loss": 1.7751,
      "step": 378400
    },
    {
      "epoch": 28.91299365976625,
      "grad_norm": 4.887562274932861,
      "learning_rate": 2.590583861686146e-05,
      "loss": 1.6567,
      "step": 378500
    },
    {
      "epoch": 28.92063249560767,
      "grad_norm": 3.7191617488861084,
      "learning_rate": 2.5899472920326944e-05,
      "loss": 1.7448,
      "step": 378600
    },
    {
      "epoch": 28.928271331449086,
      "grad_norm": 5.462791442871094,
      "learning_rate": 2.5893107223792427e-05,
      "loss": 1.64,
      "step": 378700
    },
    {
      "epoch": 28.935910167290505,
      "grad_norm": 4.547937393188477,
      "learning_rate": 2.5886741527257914e-05,
      "loss": 1.7023,
      "step": 378800
    },
    {
      "epoch": 28.943549003131924,
      "grad_norm": 5.004700660705566,
      "learning_rate": 2.5880375830723398e-05,
      "loss": 1.7188,
      "step": 378900
    },
    {
      "epoch": 28.95118783897334,
      "grad_norm": 5.700007438659668,
      "learning_rate": 2.5874010134188888e-05,
      "loss": 1.7006,
      "step": 379000
    },
    {
      "epoch": 28.95882667481476,
      "grad_norm": 5.795906066894531,
      "learning_rate": 2.586764443765437e-05,
      "loss": 1.6748,
      "step": 379100
    },
    {
      "epoch": 28.966465510656175,
      "grad_norm": 4.68004035949707,
      "learning_rate": 2.5861278741119855e-05,
      "loss": 1.6978,
      "step": 379200
    },
    {
      "epoch": 28.974104346497594,
      "grad_norm": 5.053482532501221,
      "learning_rate": 2.585491304458534e-05,
      "loss": 1.6639,
      "step": 379300
    },
    {
      "epoch": 28.981743182339013,
      "grad_norm": 6.615296840667725,
      "learning_rate": 2.5848547348050822e-05,
      "loss": 1.7311,
      "step": 379400
    },
    {
      "epoch": 28.98938201818043,
      "grad_norm": 5.694295883178711,
      "learning_rate": 2.5842181651516312e-05,
      "loss": 1.7135,
      "step": 379500
    },
    {
      "epoch": 28.997020854021848,
      "grad_norm": 4.4560418128967285,
      "learning_rate": 2.5835815954981796e-05,
      "loss": 1.7273,
      "step": 379600
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.7828192710876465,
      "eval_runtime": 1.4634,
      "eval_samples_per_second": 471.516,
      "eval_steps_per_second": 471.516,
      "step": 379639
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.49100661277771,
      "eval_runtime": 27.5611,
      "eval_samples_per_second": 474.981,
      "eval_steps_per_second": 474.981,
      "step": 379639
    },
    {
      "epoch": 29.004659689863264,
      "grad_norm": 5.7447943687438965,
      "learning_rate": 2.582945025844728e-05,
      "loss": 1.8094,
      "step": 379700
    },
    {
      "epoch": 29.012298525704683,
      "grad_norm": 5.822081089019775,
      "learning_rate": 2.5823084561912763e-05,
      "loss": 1.7239,
      "step": 379800
    },
    {
      "epoch": 29.019937361546102,
      "grad_norm": 7.872248649597168,
      "learning_rate": 2.5816718865378253e-05,
      "loss": 1.5909,
      "step": 379900
    },
    {
      "epoch": 29.027576197387518,
      "grad_norm": 4.386070728302002,
      "learning_rate": 2.5810353168843737e-05,
      "loss": 1.7059,
      "step": 380000
    },
    {
      "epoch": 29.035215033228937,
      "grad_norm": 5.7223310470581055,
      "learning_rate": 2.580398747230922e-05,
      "loss": 1.734,
      "step": 380100
    },
    {
      "epoch": 29.042853869070353,
      "grad_norm": 7.01611852645874,
      "learning_rate": 2.5797621775774704e-05,
      "loss": 1.7533,
      "step": 380200
    },
    {
      "epoch": 29.050492704911772,
      "grad_norm": 6.420439720153809,
      "learning_rate": 2.5791256079240188e-05,
      "loss": 1.6318,
      "step": 380300
    },
    {
      "epoch": 29.058131540753187,
      "grad_norm": 4.181617736816406,
      "learning_rate": 2.5784890382705678e-05,
      "loss": 1.6838,
      "step": 380400
    },
    {
      "epoch": 29.065770376594607,
      "grad_norm": 5.75125789642334,
      "learning_rate": 2.577852468617116e-05,
      "loss": 1.7057,
      "step": 380500
    },
    {
      "epoch": 29.073409212436026,
      "grad_norm": 4.302958965301514,
      "learning_rate": 2.5772158989636648e-05,
      "loss": 1.7139,
      "step": 380600
    },
    {
      "epoch": 29.08104804827744,
      "grad_norm": 5.691173076629639,
      "learning_rate": 2.5765793293102132e-05,
      "loss": 1.8003,
      "step": 380700
    },
    {
      "epoch": 29.08868688411886,
      "grad_norm": 5.336520195007324,
      "learning_rate": 2.575942759656762e-05,
      "loss": 1.789,
      "step": 380800
    },
    {
      "epoch": 29.096325719960277,
      "grad_norm": 8.089066505432129,
      "learning_rate": 2.5753061900033106e-05,
      "loss": 1.7205,
      "step": 380900
    },
    {
      "epoch": 29.103964555801696,
      "grad_norm": 5.844686508178711,
      "learning_rate": 2.574669620349859e-05,
      "loss": 1.7716,
      "step": 381000
    },
    {
      "epoch": 29.111603391643115,
      "grad_norm": 6.2672295570373535,
      "learning_rate": 2.5740330506964073e-05,
      "loss": 1.7961,
      "step": 381100
    },
    {
      "epoch": 29.11924222748453,
      "grad_norm": 4.716187953948975,
      "learning_rate": 2.5733964810429556e-05,
      "loss": 1.8097,
      "step": 381200
    },
    {
      "epoch": 29.12688106332595,
      "grad_norm": 5.0730133056640625,
      "learning_rate": 2.5727599113895047e-05,
      "loss": 1.602,
      "step": 381300
    },
    {
      "epoch": 29.134519899167366,
      "grad_norm": 6.446019649505615,
      "learning_rate": 2.572123341736053e-05,
      "loss": 1.8279,
      "step": 381400
    },
    {
      "epoch": 29.142158735008785,
      "grad_norm": 3.9507970809936523,
      "learning_rate": 2.5714867720826014e-05,
      "loss": 1.6746,
      "step": 381500
    },
    {
      "epoch": 29.149797570850204,
      "grad_norm": 4.32431697845459,
      "learning_rate": 2.5708502024291497e-05,
      "loss": 1.7048,
      "step": 381600
    },
    {
      "epoch": 29.15743640669162,
      "grad_norm": 6.060612678527832,
      "learning_rate": 2.5702136327756988e-05,
      "loss": 1.709,
      "step": 381700
    },
    {
      "epoch": 29.16507524253304,
      "grad_norm": 5.4313273429870605,
      "learning_rate": 2.569577063122247e-05,
      "loss": 1.7833,
      "step": 381800
    },
    {
      "epoch": 29.172714078374455,
      "grad_norm": 6.673884868621826,
      "learning_rate": 2.5689404934687955e-05,
      "loss": 1.7833,
      "step": 381900
    },
    {
      "epoch": 29.180352914215874,
      "grad_norm": 6.602471828460693,
      "learning_rate": 2.5683039238153438e-05,
      "loss": 1.7147,
      "step": 382000
    },
    {
      "epoch": 29.187991750057293,
      "grad_norm": 3.882303237915039,
      "learning_rate": 2.567667354161892e-05,
      "loss": 1.6624,
      "step": 382100
    },
    {
      "epoch": 29.19563058589871,
      "grad_norm": 5.733218669891357,
      "learning_rate": 2.5670307845084412e-05,
      "loss": 1.6981,
      "step": 382200
    },
    {
      "epoch": 29.203269421740128,
      "grad_norm": 5.467670440673828,
      "learning_rate": 2.5663942148549896e-05,
      "loss": 1.7342,
      "step": 382300
    },
    {
      "epoch": 29.210908257581544,
      "grad_norm": 6.1128716468811035,
      "learning_rate": 2.565757645201538e-05,
      "loss": 1.708,
      "step": 382400
    },
    {
      "epoch": 29.218547093422963,
      "grad_norm": 5.767496585845947,
      "learning_rate": 2.5651210755480866e-05,
      "loss": 1.7576,
      "step": 382500
    },
    {
      "epoch": 29.22618592926438,
      "grad_norm": 4.421591281890869,
      "learning_rate": 2.564484505894635e-05,
      "loss": 1.7114,
      "step": 382600
    },
    {
      "epoch": 29.233824765105798,
      "grad_norm": 4.232973575592041,
      "learning_rate": 2.5638479362411836e-05,
      "loss": 1.768,
      "step": 382700
    },
    {
      "epoch": 29.241463600947217,
      "grad_norm": 4.722524166107178,
      "learning_rate": 2.5632113665877323e-05,
      "loss": 1.7675,
      "step": 382800
    },
    {
      "epoch": 29.249102436788633,
      "grad_norm": 5.373094081878662,
      "learning_rate": 2.5625747969342807e-05,
      "loss": 1.6377,
      "step": 382900
    },
    {
      "epoch": 29.25674127263005,
      "grad_norm": 6.0813188552856445,
      "learning_rate": 2.561938227280829e-05,
      "loss": 1.7456,
      "step": 383000
    },
    {
      "epoch": 29.264380108471467,
      "grad_norm": 5.459483623504639,
      "learning_rate": 2.561301657627378e-05,
      "loss": 1.6608,
      "step": 383100
    },
    {
      "epoch": 29.272018944312887,
      "grad_norm": 5.759819030761719,
      "learning_rate": 2.5606650879739264e-05,
      "loss": 1.7048,
      "step": 383200
    },
    {
      "epoch": 29.279657780154306,
      "grad_norm": 6.391115188598633,
      "learning_rate": 2.5600285183204748e-05,
      "loss": 1.7433,
      "step": 383300
    },
    {
      "epoch": 29.28729661599572,
      "grad_norm": 6.564849376678467,
      "learning_rate": 2.559391948667023e-05,
      "loss": 1.6766,
      "step": 383400
    },
    {
      "epoch": 29.29493545183714,
      "grad_norm": 7.5942206382751465,
      "learning_rate": 2.5587553790135715e-05,
      "loss": 1.7525,
      "step": 383500
    },
    {
      "epoch": 29.302574287678556,
      "grad_norm": 6.042600631713867,
      "learning_rate": 2.5581188093601205e-05,
      "loss": 1.6525,
      "step": 383600
    },
    {
      "epoch": 29.310213123519976,
      "grad_norm": 3.9137117862701416,
      "learning_rate": 2.557482239706669e-05,
      "loss": 1.7176,
      "step": 383700
    },
    {
      "epoch": 29.317851959361395,
      "grad_norm": 5.687123775482178,
      "learning_rate": 2.5568456700532172e-05,
      "loss": 1.7536,
      "step": 383800
    },
    {
      "epoch": 29.32549079520281,
      "grad_norm": 5.11006498336792,
      "learning_rate": 2.5562091003997656e-05,
      "loss": 1.7755,
      "step": 383900
    },
    {
      "epoch": 29.33312963104423,
      "grad_norm": 9.321486473083496,
      "learning_rate": 2.5555725307463146e-05,
      "loss": 1.6999,
      "step": 384000
    },
    {
      "epoch": 29.340768466885645,
      "grad_norm": 4.6686248779296875,
      "learning_rate": 2.554935961092863e-05,
      "loss": 1.6874,
      "step": 384100
    },
    {
      "epoch": 29.348407302727065,
      "grad_norm": 4.875765800476074,
      "learning_rate": 2.5542993914394113e-05,
      "loss": 1.731,
      "step": 384200
    },
    {
      "epoch": 29.356046138568484,
      "grad_norm": 4.860480785369873,
      "learning_rate": 2.5536628217859597e-05,
      "loss": 1.7652,
      "step": 384300
    },
    {
      "epoch": 29.3636849744099,
      "grad_norm": 7.3804216384887695,
      "learning_rate": 2.5530262521325084e-05,
      "loss": 1.6457,
      "step": 384400
    },
    {
      "epoch": 29.37132381025132,
      "grad_norm": 6.871114253997803,
      "learning_rate": 2.552389682479057e-05,
      "loss": 1.7217,
      "step": 384500
    },
    {
      "epoch": 29.378962646092734,
      "grad_norm": 5.358286380767822,
      "learning_rate": 2.5517531128256058e-05,
      "loss": 1.7521,
      "step": 384600
    },
    {
      "epoch": 29.386601481934154,
      "grad_norm": 4.7337212562561035,
      "learning_rate": 2.551116543172154e-05,
      "loss": 1.7363,
      "step": 384700
    },
    {
      "epoch": 29.39424031777557,
      "grad_norm": 5.120908737182617,
      "learning_rate": 2.5504799735187025e-05,
      "loss": 1.7539,
      "step": 384800
    },
    {
      "epoch": 29.40187915361699,
      "grad_norm": 5.395084381103516,
      "learning_rate": 2.5498434038652515e-05,
      "loss": 1.6776,
      "step": 384900
    },
    {
      "epoch": 29.409517989458408,
      "grad_norm": 4.874395370483398,
      "learning_rate": 2.5492068342118e-05,
      "loss": 1.7857,
      "step": 385000
    },
    {
      "epoch": 29.417156825299823,
      "grad_norm": 7.148366928100586,
      "learning_rate": 2.5485702645583482e-05,
      "loss": 1.6131,
      "step": 385100
    },
    {
      "epoch": 29.424795661141243,
      "grad_norm": 4.98681640625,
      "learning_rate": 2.5479336949048966e-05,
      "loss": 1.7204,
      "step": 385200
    },
    {
      "epoch": 29.43243449698266,
      "grad_norm": 7.249533176422119,
      "learning_rate": 2.547297125251445e-05,
      "loss": 1.714,
      "step": 385300
    },
    {
      "epoch": 29.440073332824078,
      "grad_norm": 6.658695220947266,
      "learning_rate": 2.546660555597994e-05,
      "loss": 1.7216,
      "step": 385400
    },
    {
      "epoch": 29.447712168665497,
      "grad_norm": 5.439887523651123,
      "learning_rate": 2.5460239859445423e-05,
      "loss": 1.7304,
      "step": 385500
    },
    {
      "epoch": 29.455351004506912,
      "grad_norm": 4.903374671936035,
      "learning_rate": 2.5453874162910906e-05,
      "loss": 1.7938,
      "step": 385600
    },
    {
      "epoch": 29.46298984034833,
      "grad_norm": 5.838513374328613,
      "learning_rate": 2.544750846637639e-05,
      "loss": 1.7184,
      "step": 385700
    },
    {
      "epoch": 29.470628676189747,
      "grad_norm": 5.741500377655029,
      "learning_rate": 2.5441142769841874e-05,
      "loss": 1.7622,
      "step": 385800
    },
    {
      "epoch": 29.478267512031167,
      "grad_norm": 5.193605422973633,
      "learning_rate": 2.5434777073307364e-05,
      "loss": 1.6912,
      "step": 385900
    },
    {
      "epoch": 29.485906347872586,
      "grad_norm": 6.818456649780273,
      "learning_rate": 2.5428411376772847e-05,
      "loss": 1.8341,
      "step": 386000
    },
    {
      "epoch": 29.493545183714,
      "grad_norm": 5.584736347198486,
      "learning_rate": 2.542204568023833e-05,
      "loss": 1.7799,
      "step": 386100
    },
    {
      "epoch": 29.50118401955542,
      "grad_norm": 4.583426475524902,
      "learning_rate": 2.5415679983703818e-05,
      "loss": 1.7071,
      "step": 386200
    },
    {
      "epoch": 29.508822855396836,
      "grad_norm": 4.479147434234619,
      "learning_rate": 2.5409314287169305e-05,
      "loss": 1.6823,
      "step": 386300
    },
    {
      "epoch": 29.516461691238256,
      "grad_norm": 6.1677165031433105,
      "learning_rate": 2.5402948590634788e-05,
      "loss": 1.7028,
      "step": 386400
    },
    {
      "epoch": 29.524100527079675,
      "grad_norm": 7.330926418304443,
      "learning_rate": 2.5396582894100275e-05,
      "loss": 1.6391,
      "step": 386500
    },
    {
      "epoch": 29.53173936292109,
      "grad_norm": 3.435302257537842,
      "learning_rate": 2.539021719756576e-05,
      "loss": 1.7267,
      "step": 386600
    },
    {
      "epoch": 29.53937819876251,
      "grad_norm": 4.514364242553711,
      "learning_rate": 2.5383851501031242e-05,
      "loss": 1.6991,
      "step": 386700
    },
    {
      "epoch": 29.547017034603925,
      "grad_norm": 5.160058498382568,
      "learning_rate": 2.5377485804496733e-05,
      "loss": 1.7696,
      "step": 386800
    },
    {
      "epoch": 29.554655870445345,
      "grad_norm": 6.004591941833496,
      "learning_rate": 2.5371120107962216e-05,
      "loss": 1.6841,
      "step": 386900
    },
    {
      "epoch": 29.56229470628676,
      "grad_norm": 6.150455474853516,
      "learning_rate": 2.53647544114277e-05,
      "loss": 1.7342,
      "step": 387000
    },
    {
      "epoch": 29.56993354212818,
      "grad_norm": 5.744346618652344,
      "learning_rate": 2.5358388714893183e-05,
      "loss": 1.7955,
      "step": 387100
    },
    {
      "epoch": 29.5775723779696,
      "grad_norm": 5.667072772979736,
      "learning_rate": 2.5352023018358674e-05,
      "loss": 1.7044,
      "step": 387200
    },
    {
      "epoch": 29.585211213811014,
      "grad_norm": 4.696323871612549,
      "learning_rate": 2.5345657321824157e-05,
      "loss": 1.6871,
      "step": 387300
    },
    {
      "epoch": 29.592850049652434,
      "grad_norm": 6.6092047691345215,
      "learning_rate": 2.533929162528964e-05,
      "loss": 1.6801,
      "step": 387400
    },
    {
      "epoch": 29.60048888549385,
      "grad_norm": 6.147196292877197,
      "learning_rate": 2.5332925928755124e-05,
      "loss": 1.6983,
      "step": 387500
    },
    {
      "epoch": 29.60812772133527,
      "grad_norm": 3.4998106956481934,
      "learning_rate": 2.5326560232220608e-05,
      "loss": 1.6929,
      "step": 387600
    },
    {
      "epoch": 29.615766557176688,
      "grad_norm": 4.661947727203369,
      "learning_rate": 2.5320194535686098e-05,
      "loss": 1.7338,
      "step": 387700
    },
    {
      "epoch": 29.623405393018103,
      "grad_norm": 5.324975490570068,
      "learning_rate": 2.531382883915158e-05,
      "loss": 1.7406,
      "step": 387800
    },
    {
      "epoch": 29.631044228859523,
      "grad_norm": 7.76993989944458,
      "learning_rate": 2.5307463142617065e-05,
      "loss": 1.8471,
      "step": 387900
    },
    {
      "epoch": 29.63868306470094,
      "grad_norm": 5.659323215484619,
      "learning_rate": 2.530109744608255e-05,
      "loss": 1.7786,
      "step": 388000
    },
    {
      "epoch": 29.646321900542357,
      "grad_norm": 5.961826801300049,
      "learning_rate": 2.5294731749548036e-05,
      "loss": 1.7344,
      "step": 388100
    },
    {
      "epoch": 29.653960736383777,
      "grad_norm": 6.640830993652344,
      "learning_rate": 2.5288366053013522e-05,
      "loss": 1.7785,
      "step": 388200
    },
    {
      "epoch": 29.661599572225192,
      "grad_norm": 4.451566696166992,
      "learning_rate": 2.5282000356479006e-05,
      "loss": 1.7275,
      "step": 388300
    },
    {
      "epoch": 29.66923840806661,
      "grad_norm": 9.542363166809082,
      "learning_rate": 2.5275634659944493e-05,
      "loss": 1.7984,
      "step": 388400
    },
    {
      "epoch": 29.676877243908027,
      "grad_norm": 5.64582633972168,
      "learning_rate": 2.5269268963409976e-05,
      "loss": 1.6761,
      "step": 388500
    },
    {
      "epoch": 29.684516079749447,
      "grad_norm": 6.269953727722168,
      "learning_rate": 2.5262903266875463e-05,
      "loss": 1.759,
      "step": 388600
    },
    {
      "epoch": 29.692154915590862,
      "grad_norm": 4.81160306930542,
      "learning_rate": 2.525653757034095e-05,
      "loss": 1.7089,
      "step": 388700
    },
    {
      "epoch": 29.69979375143228,
      "grad_norm": 4.433628082275391,
      "learning_rate": 2.5250171873806434e-05,
      "loss": 1.823,
      "step": 388800
    },
    {
      "epoch": 29.7074325872737,
      "grad_norm": 5.050408840179443,
      "learning_rate": 2.5243806177271917e-05,
      "loss": 1.721,
      "step": 388900
    },
    {
      "epoch": 29.715071423115116,
      "grad_norm": 4.460754871368408,
      "learning_rate": 2.52374404807374e-05,
      "loss": 1.789,
      "step": 389000
    },
    {
      "epoch": 29.722710258956536,
      "grad_norm": 4.09617805480957,
      "learning_rate": 2.523107478420289e-05,
      "loss": 1.7041,
      "step": 389100
    },
    {
      "epoch": 29.73034909479795,
      "grad_norm": 6.810123920440674,
      "learning_rate": 2.5224709087668375e-05,
      "loss": 1.8139,
      "step": 389200
    },
    {
      "epoch": 29.73798793063937,
      "grad_norm": 6.404267311096191,
      "learning_rate": 2.5218343391133858e-05,
      "loss": 1.7175,
      "step": 389300
    },
    {
      "epoch": 29.74562676648079,
      "grad_norm": 5.285488605499268,
      "learning_rate": 2.5211977694599342e-05,
      "loss": 1.6908,
      "step": 389400
    },
    {
      "epoch": 29.753265602322205,
      "grad_norm": 5.367547988891602,
      "learning_rate": 2.5205611998064832e-05,
      "loss": 1.7379,
      "step": 389500
    },
    {
      "epoch": 29.760904438163625,
      "grad_norm": 6.257955074310303,
      "learning_rate": 2.5199246301530316e-05,
      "loss": 1.6855,
      "step": 389600
    },
    {
      "epoch": 29.76854327400504,
      "grad_norm": 4.240238189697266,
      "learning_rate": 2.51928806049958e-05,
      "loss": 1.7869,
      "step": 389700
    },
    {
      "epoch": 29.77618210984646,
      "grad_norm": 9.593388557434082,
      "learning_rate": 2.5186514908461283e-05,
      "loss": 1.7733,
      "step": 389800
    },
    {
      "epoch": 29.78382094568788,
      "grad_norm": 5.644293785095215,
      "learning_rate": 2.5180149211926766e-05,
      "loss": 1.7545,
      "step": 389900
    },
    {
      "epoch": 29.791459781529294,
      "grad_norm": 5.230128288269043,
      "learning_rate": 2.5173783515392257e-05,
      "loss": 1.7081,
      "step": 390000
    },
    {
      "epoch": 29.799098617370714,
      "grad_norm": 6.156044960021973,
      "learning_rate": 2.516741781885774e-05,
      "loss": 1.7188,
      "step": 390100
    },
    {
      "epoch": 29.80673745321213,
      "grad_norm": 5.232336044311523,
      "learning_rate": 2.5161052122323227e-05,
      "loss": 1.813,
      "step": 390200
    },
    {
      "epoch": 29.81437628905355,
      "grad_norm": 5.329723358154297,
      "learning_rate": 2.515468642578871e-05,
      "loss": 1.6943,
      "step": 390300
    },
    {
      "epoch": 29.822015124894968,
      "grad_norm": 4.380548000335693,
      "learning_rate": 2.5148320729254198e-05,
      "loss": 1.7558,
      "step": 390400
    },
    {
      "epoch": 29.829653960736383,
      "grad_norm": 4.433810710906982,
      "learning_rate": 2.5141955032719684e-05,
      "loss": 1.705,
      "step": 390500
    },
    {
      "epoch": 29.837292796577803,
      "grad_norm": 5.118480205535889,
      "learning_rate": 2.5135589336185168e-05,
      "loss": 1.5963,
      "step": 390600
    },
    {
      "epoch": 29.844931632419218,
      "grad_norm": 5.737285137176514,
      "learning_rate": 2.512922363965065e-05,
      "loss": 1.7593,
      "step": 390700
    },
    {
      "epoch": 29.852570468260637,
      "grad_norm": 6.352425575256348,
      "learning_rate": 2.5122857943116135e-05,
      "loss": 1.7095,
      "step": 390800
    },
    {
      "epoch": 29.860209304102057,
      "grad_norm": 5.76517391204834,
      "learning_rate": 2.5116492246581625e-05,
      "loss": 1.8058,
      "step": 390900
    },
    {
      "epoch": 29.867848139943472,
      "grad_norm": 6.018085956573486,
      "learning_rate": 2.511012655004711e-05,
      "loss": 1.7208,
      "step": 391000
    },
    {
      "epoch": 29.87548697578489,
      "grad_norm": 5.057094573974609,
      "learning_rate": 2.5103760853512592e-05,
      "loss": 1.6688,
      "step": 391100
    },
    {
      "epoch": 29.883125811626307,
      "grad_norm": 5.967267990112305,
      "learning_rate": 2.5097395156978076e-05,
      "loss": 1.7449,
      "step": 391200
    },
    {
      "epoch": 29.890764647467726,
      "grad_norm": 7.586346626281738,
      "learning_rate": 2.509102946044356e-05,
      "loss": 1.7461,
      "step": 391300
    },
    {
      "epoch": 29.898403483309142,
      "grad_norm": 3.6338510513305664,
      "learning_rate": 2.508466376390905e-05,
      "loss": 1.6201,
      "step": 391400
    },
    {
      "epoch": 29.90604231915056,
      "grad_norm": 4.886025428771973,
      "learning_rate": 2.5078298067374533e-05,
      "loss": 1.6624,
      "step": 391500
    },
    {
      "epoch": 29.91368115499198,
      "grad_norm": 4.98253870010376,
      "learning_rate": 2.5071932370840017e-05,
      "loss": 1.7271,
      "step": 391600
    },
    {
      "epoch": 29.921319990833396,
      "grad_norm": 6.2970805168151855,
      "learning_rate": 2.50655666743055e-05,
      "loss": 1.6869,
      "step": 391700
    },
    {
      "epoch": 29.928958826674815,
      "grad_norm": 5.6278157234191895,
      "learning_rate": 2.505920097777099e-05,
      "loss": 1.7511,
      "step": 391800
    },
    {
      "epoch": 29.93659766251623,
      "grad_norm": 5.798032283782959,
      "learning_rate": 2.5052835281236474e-05,
      "loss": 1.7359,
      "step": 391900
    },
    {
      "epoch": 29.94423649835765,
      "grad_norm": 4.550423622131348,
      "learning_rate": 2.5046469584701958e-05,
      "loss": 1.7017,
      "step": 392000
    },
    {
      "epoch": 29.95187533419907,
      "grad_norm": 4.237597465515137,
      "learning_rate": 2.5040103888167445e-05,
      "loss": 1.6517,
      "step": 392100
    },
    {
      "epoch": 29.959514170040485,
      "grad_norm": 5.746285438537598,
      "learning_rate": 2.5033738191632928e-05,
      "loss": 1.6919,
      "step": 392200
    },
    {
      "epoch": 29.967153005881904,
      "grad_norm": 6.329461097717285,
      "learning_rate": 2.5027372495098415e-05,
      "loss": 1.6764,
      "step": 392300
    },
    {
      "epoch": 29.97479184172332,
      "grad_norm": 6.028476715087891,
      "learning_rate": 2.5021006798563902e-05,
      "loss": 1.7294,
      "step": 392400
    },
    {
      "epoch": 29.98243067756474,
      "grad_norm": 6.464465141296387,
      "learning_rate": 2.5014641102029386e-05,
      "loss": 1.6544,
      "step": 392500
    },
    {
      "epoch": 29.99006951340616,
      "grad_norm": 6.637927055358887,
      "learning_rate": 2.500827540549487e-05,
      "loss": 1.6808,
      "step": 392600
    },
    {
      "epoch": 29.997708349247574,
      "grad_norm": 5.204420566558838,
      "learning_rate": 2.500190970896036e-05,
      "loss": 1.756,
      "step": 392700
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.7831776142120361,
      "eval_runtime": 1.4646,
      "eval_samples_per_second": 471.118,
      "eval_steps_per_second": 471.118,
      "step": 392730
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.4847456216812134,
      "eval_runtime": 27.4775,
      "eval_samples_per_second": 476.426,
      "eval_steps_per_second": 476.426,
      "step": 392730
    },
    {
      "epoch": 30.005347185088993,
      "grad_norm": 4.566812992095947,
      "learning_rate": 2.4995544012425843e-05,
      "loss": 1.6999,
      "step": 392800
    },
    {
      "epoch": 30.01298602093041,
      "grad_norm": 7.553965091705322,
      "learning_rate": 2.4989178315891327e-05,
      "loss": 1.7138,
      "step": 392900
    },
    {
      "epoch": 30.02062485677183,
      "grad_norm": 6.410830974578857,
      "learning_rate": 2.498281261935681e-05,
      "loss": 1.6559,
      "step": 393000
    },
    {
      "epoch": 30.028263692613244,
      "grad_norm": 5.118245601654053,
      "learning_rate": 2.4976446922822297e-05,
      "loss": 1.7517,
      "step": 393100
    },
    {
      "epoch": 30.035902528454663,
      "grad_norm": 5.532745361328125,
      "learning_rate": 2.497008122628778e-05,
      "loss": 1.6543,
      "step": 393200
    },
    {
      "epoch": 30.043541364296082,
      "grad_norm": 5.345926761627197,
      "learning_rate": 2.4963715529753268e-05,
      "loss": 1.6831,
      "step": 393300
    },
    {
      "epoch": 30.051180200137498,
      "grad_norm": 5.431240081787109,
      "learning_rate": 2.495734983321875e-05,
      "loss": 1.6405,
      "step": 393400
    },
    {
      "epoch": 30.058819035978917,
      "grad_norm": 6.057519435882568,
      "learning_rate": 2.4950984136684238e-05,
      "loss": 1.6795,
      "step": 393500
    },
    {
      "epoch": 30.066457871820333,
      "grad_norm": 5.5069499015808105,
      "learning_rate": 2.494461844014972e-05,
      "loss": 1.7966,
      "step": 393600
    },
    {
      "epoch": 30.074096707661752,
      "grad_norm": 4.6380791664123535,
      "learning_rate": 2.4938252743615205e-05,
      "loss": 1.7587,
      "step": 393700
    },
    {
      "epoch": 30.08173554350317,
      "grad_norm": 4.47410774230957,
      "learning_rate": 2.4931887047080692e-05,
      "loss": 1.729,
      "step": 393800
    },
    {
      "epoch": 30.089374379344587,
      "grad_norm": 6.842835903167725,
      "learning_rate": 2.4925521350546176e-05,
      "loss": 1.7312,
      "step": 393900
    },
    {
      "epoch": 30.097013215186006,
      "grad_norm": 4.776220321655273,
      "learning_rate": 2.4919155654011662e-05,
      "loss": 1.7053,
      "step": 394000
    },
    {
      "epoch": 30.104652051027422,
      "grad_norm": 6.225131988525391,
      "learning_rate": 2.491278995747715e-05,
      "loss": 1.7217,
      "step": 394100
    },
    {
      "epoch": 30.11229088686884,
      "grad_norm": 4.985837936401367,
      "learning_rate": 2.4906424260942633e-05,
      "loss": 1.7811,
      "step": 394200
    },
    {
      "epoch": 30.11992972271026,
      "grad_norm": 5.5224504470825195,
      "learning_rate": 2.490005856440812e-05,
      "loss": 1.7232,
      "step": 394300
    },
    {
      "epoch": 30.127568558551676,
      "grad_norm": 10.514443397521973,
      "learning_rate": 2.4893692867873603e-05,
      "loss": 1.7306,
      "step": 394400
    },
    {
      "epoch": 30.135207394393095,
      "grad_norm": 6.079345703125,
      "learning_rate": 2.488732717133909e-05,
      "loss": 1.7557,
      "step": 394500
    },
    {
      "epoch": 30.14284623023451,
      "grad_norm": 5.896199703216553,
      "learning_rate": 2.4880961474804574e-05,
      "loss": 1.7094,
      "step": 394600
    },
    {
      "epoch": 30.15048506607593,
      "grad_norm": 5.355297088623047,
      "learning_rate": 2.487459577827006e-05,
      "loss": 1.6602,
      "step": 394700
    },
    {
      "epoch": 30.15812390191735,
      "grad_norm": 6.918508529663086,
      "learning_rate": 2.4868230081735544e-05,
      "loss": 1.6488,
      "step": 394800
    },
    {
      "epoch": 30.165762737758765,
      "grad_norm": 4.584921360015869,
      "learning_rate": 2.486186438520103e-05,
      "loss": 1.7134,
      "step": 394900
    },
    {
      "epoch": 30.173401573600184,
      "grad_norm": 3.9659509658813477,
      "learning_rate": 2.4855498688666515e-05,
      "loss": 1.6635,
      "step": 395000
    },
    {
      "epoch": 30.1810404094416,
      "grad_norm": 6.241058826446533,
      "learning_rate": 2.4849132992132e-05,
      "loss": 1.651,
      "step": 395100
    },
    {
      "epoch": 30.18867924528302,
      "grad_norm": 5.512385845184326,
      "learning_rate": 2.4842767295597485e-05,
      "loss": 1.784,
      "step": 395200
    },
    {
      "epoch": 30.196318081124435,
      "grad_norm": 5.654117584228516,
      "learning_rate": 2.483640159906297e-05,
      "loss": 1.706,
      "step": 395300
    },
    {
      "epoch": 30.203956916965854,
      "grad_norm": 5.730324745178223,
      "learning_rate": 2.4830035902528456e-05,
      "loss": 1.7192,
      "step": 395400
    },
    {
      "epoch": 30.211595752807273,
      "grad_norm": 5.884134769439697,
      "learning_rate": 2.482367020599394e-05,
      "loss": 1.6986,
      "step": 395500
    },
    {
      "epoch": 30.21923458864869,
      "grad_norm": 4.605091571807861,
      "learning_rate": 2.4817304509459426e-05,
      "loss": 1.748,
      "step": 395600
    },
    {
      "epoch": 30.22687342449011,
      "grad_norm": 5.475800514221191,
      "learning_rate": 2.481093881292491e-05,
      "loss": 1.7769,
      "step": 395700
    },
    {
      "epoch": 30.234512260331524,
      "grad_norm": 5.736154079437256,
      "learning_rate": 2.4804573116390397e-05,
      "loss": 1.7277,
      "step": 395800
    },
    {
      "epoch": 30.242151096172943,
      "grad_norm": 6.143010139465332,
      "learning_rate": 2.479820741985588e-05,
      "loss": 1.6114,
      "step": 395900
    },
    {
      "epoch": 30.249789932014362,
      "grad_norm": 4.700416564941406,
      "learning_rate": 2.4791841723321367e-05,
      "loss": 1.7749,
      "step": 396000
    },
    {
      "epoch": 30.257428767855778,
      "grad_norm": 5.294533729553223,
      "learning_rate": 2.4785476026786854e-05,
      "loss": 1.7649,
      "step": 396100
    },
    {
      "epoch": 30.265067603697197,
      "grad_norm": 5.424044609069824,
      "learning_rate": 2.4779110330252338e-05,
      "loss": 1.6708,
      "step": 396200
    },
    {
      "epoch": 30.272706439538613,
      "grad_norm": 3.978642702102661,
      "learning_rate": 2.4772744633717824e-05,
      "loss": 1.7384,
      "step": 396300
    },
    {
      "epoch": 30.280345275380032,
      "grad_norm": 6.976500511169434,
      "learning_rate": 2.4766378937183308e-05,
      "loss": 1.6767,
      "step": 396400
    },
    {
      "epoch": 30.28798411122145,
      "grad_norm": 5.760197639465332,
      "learning_rate": 2.4760013240648795e-05,
      "loss": 1.698,
      "step": 396500
    },
    {
      "epoch": 30.295622947062867,
      "grad_norm": 5.195185661315918,
      "learning_rate": 2.475364754411428e-05,
      "loss": 1.7145,
      "step": 396600
    },
    {
      "epoch": 30.303261782904286,
      "grad_norm": 5.112947463989258,
      "learning_rate": 2.4747281847579765e-05,
      "loss": 1.7008,
      "step": 396700
    },
    {
      "epoch": 30.310900618745702,
      "grad_norm": 6.139257431030273,
      "learning_rate": 2.474091615104525e-05,
      "loss": 1.7549,
      "step": 396800
    },
    {
      "epoch": 30.31853945458712,
      "grad_norm": 6.315303325653076,
      "learning_rate": 2.4734550454510732e-05,
      "loss": 1.7512,
      "step": 396900
    },
    {
      "epoch": 30.32617829042854,
      "grad_norm": 7.489443302154541,
      "learning_rate": 2.472818475797622e-05,
      "loss": 1.7304,
      "step": 397000
    },
    {
      "epoch": 30.333817126269956,
      "grad_norm": 4.915894985198975,
      "learning_rate": 2.4721819061441703e-05,
      "loss": 1.6946,
      "step": 397100
    },
    {
      "epoch": 30.341455962111375,
      "grad_norm": 5.735318660736084,
      "learning_rate": 2.471545336490719e-05,
      "loss": 1.7574,
      "step": 397200
    },
    {
      "epoch": 30.34909479795279,
      "grad_norm": 5.269827842712402,
      "learning_rate": 2.4709087668372673e-05,
      "loss": 1.7836,
      "step": 397300
    },
    {
      "epoch": 30.35673363379421,
      "grad_norm": 6.069008827209473,
      "learning_rate": 2.470272197183816e-05,
      "loss": 1.7881,
      "step": 397400
    },
    {
      "epoch": 30.364372469635626,
      "grad_norm": 5.525094509124756,
      "learning_rate": 2.4696356275303644e-05,
      "loss": 1.6523,
      "step": 397500
    },
    {
      "epoch": 30.372011305477045,
      "grad_norm": 5.460230350494385,
      "learning_rate": 2.4689990578769127e-05,
      "loss": 1.7237,
      "step": 397600
    },
    {
      "epoch": 30.379650141318464,
      "grad_norm": 7.036474227905273,
      "learning_rate": 2.4683624882234614e-05,
      "loss": 1.7164,
      "step": 397700
    },
    {
      "epoch": 30.38728897715988,
      "grad_norm": 6.021576404571533,
      "learning_rate": 2.46772591857001e-05,
      "loss": 1.7549,
      "step": 397800
    },
    {
      "epoch": 30.3949278130013,
      "grad_norm": 5.8263044357299805,
      "learning_rate": 2.4670893489165585e-05,
      "loss": 1.7579,
      "step": 397900
    },
    {
      "epoch": 30.402566648842715,
      "grad_norm": 7.666676998138428,
      "learning_rate": 2.466452779263107e-05,
      "loss": 1.6963,
      "step": 398000
    },
    {
      "epoch": 30.410205484684134,
      "grad_norm": 5.69838285446167,
      "learning_rate": 2.465816209609656e-05,
      "loss": 1.7554,
      "step": 398100
    },
    {
      "epoch": 30.417844320525553,
      "grad_norm": 6.967535972595215,
      "learning_rate": 2.4651796399562042e-05,
      "loss": 1.7412,
      "step": 398200
    },
    {
      "epoch": 30.42548315636697,
      "grad_norm": 5.628523826599121,
      "learning_rate": 2.464543070302753e-05,
      "loss": 1.7935,
      "step": 398300
    },
    {
      "epoch": 30.433121992208388,
      "grad_norm": 5.846782684326172,
      "learning_rate": 2.4639065006493013e-05,
      "loss": 1.7425,
      "step": 398400
    },
    {
      "epoch": 30.440760828049804,
      "grad_norm": 5.495542526245117,
      "learning_rate": 2.4632699309958496e-05,
      "loss": 1.7188,
      "step": 398500
    },
    {
      "epoch": 30.448399663891223,
      "grad_norm": 3.7532637119293213,
      "learning_rate": 2.4626333613423983e-05,
      "loss": 1.5953,
      "step": 398600
    },
    {
      "epoch": 30.456038499732642,
      "grad_norm": 5.159230709075928,
      "learning_rate": 2.4619967916889467e-05,
      "loss": 1.688,
      "step": 398700
    },
    {
      "epoch": 30.463677335574058,
      "grad_norm": 6.078054904937744,
      "learning_rate": 2.4613602220354954e-05,
      "loss": 1.7337,
      "step": 398800
    },
    {
      "epoch": 30.471316171415477,
      "grad_norm": 5.582396984100342,
      "learning_rate": 2.4607236523820437e-05,
      "loss": 1.7607,
      "step": 398900
    },
    {
      "epoch": 30.478955007256893,
      "grad_norm": 7.0744171142578125,
      "learning_rate": 2.4600870827285924e-05,
      "loss": 1.6978,
      "step": 399000
    },
    {
      "epoch": 30.486593843098312,
      "grad_norm": 4.663106918334961,
      "learning_rate": 2.4594505130751408e-05,
      "loss": 1.7237,
      "step": 399100
    },
    {
      "epoch": 30.49423267893973,
      "grad_norm": 5.6266560554504395,
      "learning_rate": 2.458813943421689e-05,
      "loss": 1.698,
      "step": 399200
    },
    {
      "epoch": 30.501871514781147,
      "grad_norm": 4.344931602478027,
      "learning_rate": 2.4581773737682378e-05,
      "loss": 1.7034,
      "step": 399300
    },
    {
      "epoch": 30.509510350622566,
      "grad_norm": 5.840676307678223,
      "learning_rate": 2.457540804114786e-05,
      "loss": 1.6469,
      "step": 399400
    },
    {
      "epoch": 30.517149186463982,
      "grad_norm": 5.684196949005127,
      "learning_rate": 2.456904234461335e-05,
      "loss": 1.6894,
      "step": 399500
    },
    {
      "epoch": 30.5247880223054,
      "grad_norm": 4.6919050216674805,
      "learning_rate": 2.4562676648078832e-05,
      "loss": 1.6822,
      "step": 399600
    },
    {
      "epoch": 30.532426858146817,
      "grad_norm": 7.066061973571777,
      "learning_rate": 2.455631095154432e-05,
      "loss": 1.7061,
      "step": 399700
    },
    {
      "epoch": 30.540065693988236,
      "grad_norm": 6.402394771575928,
      "learning_rate": 2.4549945255009806e-05,
      "loss": 1.7596,
      "step": 399800
    },
    {
      "epoch": 30.547704529829655,
      "grad_norm": 5.078896999359131,
      "learning_rate": 2.454357955847529e-05,
      "loss": 1.6429,
      "step": 399900
    },
    {
      "epoch": 30.55534336567107,
      "grad_norm": 7.146615028381348,
      "learning_rate": 2.4537213861940776e-05,
      "loss": 1.8047,
      "step": 400000
    },
    {
      "epoch": 30.56298220151249,
      "grad_norm": 5.7795023918151855,
      "learning_rate": 2.453084816540626e-05,
      "loss": 1.7231,
      "step": 400100
    },
    {
      "epoch": 30.570621037353906,
      "grad_norm": 6.536462783813477,
      "learning_rate": 2.4524482468871747e-05,
      "loss": 1.6064,
      "step": 400200
    },
    {
      "epoch": 30.578259873195325,
      "grad_norm": 4.581770420074463,
      "learning_rate": 2.451811677233723e-05,
      "loss": 1.7945,
      "step": 400300
    },
    {
      "epoch": 30.585898709036744,
      "grad_norm": 5.066290855407715,
      "learning_rate": 2.4511751075802717e-05,
      "loss": 1.7791,
      "step": 400400
    },
    {
      "epoch": 30.59353754487816,
      "grad_norm": 5.406435012817383,
      "learning_rate": 2.45053853792682e-05,
      "loss": 1.635,
      "step": 400500
    },
    {
      "epoch": 30.60117638071958,
      "grad_norm": 5.035554885864258,
      "learning_rate": 2.4499019682733688e-05,
      "loss": 1.6781,
      "step": 400600
    },
    {
      "epoch": 30.608815216560995,
      "grad_norm": 7.5192484855651855,
      "learning_rate": 2.449265398619917e-05,
      "loss": 1.7265,
      "step": 400700
    },
    {
      "epoch": 30.616454052402414,
      "grad_norm": 5.855836391448975,
      "learning_rate": 2.4486288289664655e-05,
      "loss": 1.6768,
      "step": 400800
    },
    {
      "epoch": 30.624092888243833,
      "grad_norm": 4.893009662628174,
      "learning_rate": 2.447992259313014e-05,
      "loss": 1.7554,
      "step": 400900
    },
    {
      "epoch": 30.63173172408525,
      "grad_norm": 7.61356782913208,
      "learning_rate": 2.4473556896595625e-05,
      "loss": 1.8313,
      "step": 401000
    },
    {
      "epoch": 30.639370559926668,
      "grad_norm": 5.980066299438477,
      "learning_rate": 2.4467191200061112e-05,
      "loss": 1.6984,
      "step": 401100
    },
    {
      "epoch": 30.647009395768084,
      "grad_norm": 8.201598167419434,
      "learning_rate": 2.4460825503526596e-05,
      "loss": 1.7168,
      "step": 401200
    },
    {
      "epoch": 30.654648231609503,
      "grad_norm": 4.8167805671691895,
      "learning_rate": 2.4454459806992083e-05,
      "loss": 1.7588,
      "step": 401300
    },
    {
      "epoch": 30.66228706745092,
      "grad_norm": 5.528451442718506,
      "learning_rate": 2.4448094110457566e-05,
      "loss": 1.6825,
      "step": 401400
    },
    {
      "epoch": 30.669925903292338,
      "grad_norm": 5.669682025909424,
      "learning_rate": 2.4441728413923053e-05,
      "loss": 1.7353,
      "step": 401500
    },
    {
      "epoch": 30.677564739133757,
      "grad_norm": 7.427054405212402,
      "learning_rate": 2.4435362717388537e-05,
      "loss": 1.6917,
      "step": 401600
    },
    {
      "epoch": 30.685203574975173,
      "grad_norm": 5.048207759857178,
      "learning_rate": 2.4428997020854024e-05,
      "loss": 1.6767,
      "step": 401700
    },
    {
      "epoch": 30.692842410816592,
      "grad_norm": 5.454507827758789,
      "learning_rate": 2.4422631324319507e-05,
      "loss": 1.7257,
      "step": 401800
    },
    {
      "epoch": 30.700481246658008,
      "grad_norm": 5.591255187988281,
      "learning_rate": 2.4416265627784994e-05,
      "loss": 1.7504,
      "step": 401900
    },
    {
      "epoch": 30.708120082499427,
      "grad_norm": 4.9366044998168945,
      "learning_rate": 2.440989993125048e-05,
      "loss": 1.7013,
      "step": 402000
    },
    {
      "epoch": 30.715758918340846,
      "grad_norm": 6.507220268249512,
      "learning_rate": 2.4403534234715964e-05,
      "loss": 1.6978,
      "step": 402100
    },
    {
      "epoch": 30.723397754182262,
      "grad_norm": 5.874775409698486,
      "learning_rate": 2.439716853818145e-05,
      "loss": 1.7376,
      "step": 402200
    },
    {
      "epoch": 30.73103659002368,
      "grad_norm": 5.7001495361328125,
      "learning_rate": 2.4390802841646935e-05,
      "loss": 1.6775,
      "step": 402300
    },
    {
      "epoch": 30.738675425865097,
      "grad_norm": 6.351173400878906,
      "learning_rate": 2.438443714511242e-05,
      "loss": 1.7535,
      "step": 402400
    },
    {
      "epoch": 30.746314261706516,
      "grad_norm": 4.504819869995117,
      "learning_rate": 2.4378071448577905e-05,
      "loss": 1.7652,
      "step": 402500
    },
    {
      "epoch": 30.753953097547935,
      "grad_norm": 4.718486309051514,
      "learning_rate": 2.437170575204339e-05,
      "loss": 1.784,
      "step": 402600
    },
    {
      "epoch": 30.76159193338935,
      "grad_norm": 6.15299654006958,
      "learning_rate": 2.4365340055508876e-05,
      "loss": 1.667,
      "step": 402700
    },
    {
      "epoch": 30.76923076923077,
      "grad_norm": 5.220668792724609,
      "learning_rate": 2.435897435897436e-05,
      "loss": 1.7882,
      "step": 402800
    },
    {
      "epoch": 30.776869605072186,
      "grad_norm": 5.151165008544922,
      "learning_rate": 2.4352608662439846e-05,
      "loss": 1.6711,
      "step": 402900
    },
    {
      "epoch": 30.784508440913605,
      "grad_norm": 5.505407810211182,
      "learning_rate": 2.434624296590533e-05,
      "loss": 1.7463,
      "step": 403000
    },
    {
      "epoch": 30.792147276755024,
      "grad_norm": 8.028800010681152,
      "learning_rate": 2.4339877269370813e-05,
      "loss": 1.7667,
      "step": 403100
    },
    {
      "epoch": 30.79978611259644,
      "grad_norm": 6.5708537101745605,
      "learning_rate": 2.43335115728363e-05,
      "loss": 1.7107,
      "step": 403200
    },
    {
      "epoch": 30.80742494843786,
      "grad_norm": 5.718262672424316,
      "learning_rate": 2.4327145876301784e-05,
      "loss": 1.7239,
      "step": 403300
    },
    {
      "epoch": 30.815063784279275,
      "grad_norm": 5.826657772064209,
      "learning_rate": 2.432078017976727e-05,
      "loss": 1.8024,
      "step": 403400
    },
    {
      "epoch": 30.822702620120694,
      "grad_norm": 3.3246490955352783,
      "learning_rate": 2.4314414483232754e-05,
      "loss": 1.6773,
      "step": 403500
    },
    {
      "epoch": 30.830341455962113,
      "grad_norm": 5.638228893280029,
      "learning_rate": 2.430804878669824e-05,
      "loss": 1.7339,
      "step": 403600
    },
    {
      "epoch": 30.83798029180353,
      "grad_norm": 4.964591979980469,
      "learning_rate": 2.4301683090163728e-05,
      "loss": 1.7468,
      "step": 403700
    },
    {
      "epoch": 30.845619127644948,
      "grad_norm": 4.583801746368408,
      "learning_rate": 2.429531739362921e-05,
      "loss": 1.682,
      "step": 403800
    },
    {
      "epoch": 30.853257963486364,
      "grad_norm": 5.509840488433838,
      "learning_rate": 2.42889516970947e-05,
      "loss": 1.7302,
      "step": 403900
    },
    {
      "epoch": 30.860896799327783,
      "grad_norm": 6.542698860168457,
      "learning_rate": 2.4282586000560182e-05,
      "loss": 1.6956,
      "step": 404000
    },
    {
      "epoch": 30.8685356351692,
      "grad_norm": 5.789373874664307,
      "learning_rate": 2.427622030402567e-05,
      "loss": 1.6804,
      "step": 404100
    },
    {
      "epoch": 30.876174471010618,
      "grad_norm": 4.018040657043457,
      "learning_rate": 2.4269854607491153e-05,
      "loss": 1.7146,
      "step": 404200
    },
    {
      "epoch": 30.883813306852037,
      "grad_norm": 6.119897842407227,
      "learning_rate": 2.426348891095664e-05,
      "loss": 1.74,
      "step": 404300
    },
    {
      "epoch": 30.891452142693453,
      "grad_norm": 5.01554012298584,
      "learning_rate": 2.4257123214422123e-05,
      "loss": 1.6611,
      "step": 404400
    },
    {
      "epoch": 30.899090978534872,
      "grad_norm": 6.380453586578369,
      "learning_rate": 2.425075751788761e-05,
      "loss": 1.8055,
      "step": 404500
    },
    {
      "epoch": 30.906729814376288,
      "grad_norm": 5.058679103851318,
      "learning_rate": 2.4244391821353094e-05,
      "loss": 1.761,
      "step": 404600
    },
    {
      "epoch": 30.914368650217707,
      "grad_norm": 3.9937853813171387,
      "learning_rate": 2.4238026124818577e-05,
      "loss": 1.7521,
      "step": 404700
    },
    {
      "epoch": 30.922007486059126,
      "grad_norm": 5.106536865234375,
      "learning_rate": 2.4231660428284064e-05,
      "loss": 1.7351,
      "step": 404800
    },
    {
      "epoch": 30.92964632190054,
      "grad_norm": 5.234288215637207,
      "learning_rate": 2.4225294731749548e-05,
      "loss": 1.7655,
      "step": 404900
    },
    {
      "epoch": 30.93728515774196,
      "grad_norm": 5.908797740936279,
      "learning_rate": 2.4218929035215034e-05,
      "loss": 1.7038,
      "step": 405000
    },
    {
      "epoch": 30.944923993583377,
      "grad_norm": 6.098484039306641,
      "learning_rate": 2.4212563338680518e-05,
      "loss": 1.6829,
      "step": 405100
    },
    {
      "epoch": 30.952562829424796,
      "grad_norm": 6.87929105758667,
      "learning_rate": 2.4206197642146005e-05,
      "loss": 1.7155,
      "step": 405200
    },
    {
      "epoch": 30.960201665266215,
      "grad_norm": 3.901723861694336,
      "learning_rate": 2.419983194561149e-05,
      "loss": 1.7562,
      "step": 405300
    },
    {
      "epoch": 30.96784050110763,
      "grad_norm": 4.784280300140381,
      "learning_rate": 2.4193466249076975e-05,
      "loss": 1.6808,
      "step": 405400
    },
    {
      "epoch": 30.97547933694905,
      "grad_norm": 4.840707778930664,
      "learning_rate": 2.418710055254246e-05,
      "loss": 1.7786,
      "step": 405500
    },
    {
      "epoch": 30.983118172790466,
      "grad_norm": 4.884090423583984,
      "learning_rate": 2.4180734856007946e-05,
      "loss": 1.7684,
      "step": 405600
    },
    {
      "epoch": 30.990757008631885,
      "grad_norm": 5.458867073059082,
      "learning_rate": 2.4174369159473433e-05,
      "loss": 1.6383,
      "step": 405700
    },
    {
      "epoch": 30.9983958444733,
      "grad_norm": 5.862200736999512,
      "learning_rate": 2.4168003462938916e-05,
      "loss": 1.7576,
      "step": 405800
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.7841304540634155,
      "eval_runtime": 1.4424,
      "eval_samples_per_second": 478.354,
      "eval_steps_per_second": 478.354,
      "step": 405821
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.4799919128417969,
      "eval_runtime": 27.5433,
      "eval_samples_per_second": 475.288,
      "eval_steps_per_second": 475.288,
      "step": 405821
    },
    {
      "epoch": 31.00603468031472,
      "grad_norm": 5.7297749519348145,
      "learning_rate": 2.4161637766404403e-05,
      "loss": 1.6931,
      "step": 405900
    },
    {
      "epoch": 31.01367351615614,
      "grad_norm": 5.844600200653076,
      "learning_rate": 2.4155272069869887e-05,
      "loss": 1.8252,
      "step": 406000
    },
    {
      "epoch": 31.021312351997555,
      "grad_norm": 6.948217868804932,
      "learning_rate": 2.4148906373335374e-05,
      "loss": 1.6681,
      "step": 406100
    },
    {
      "epoch": 31.028951187838974,
      "grad_norm": 5.627684593200684,
      "learning_rate": 2.4142540676800857e-05,
      "loss": 1.7682,
      "step": 406200
    },
    {
      "epoch": 31.03659002368039,
      "grad_norm": 9.118670463562012,
      "learning_rate": 2.413617498026634e-05,
      "loss": 1.7014,
      "step": 406300
    },
    {
      "epoch": 31.04422885952181,
      "grad_norm": 5.693665981292725,
      "learning_rate": 2.4129809283731828e-05,
      "loss": 1.7436,
      "step": 406400
    },
    {
      "epoch": 31.051867695363228,
      "grad_norm": 5.068138122558594,
      "learning_rate": 2.412344358719731e-05,
      "loss": 1.7424,
      "step": 406500
    },
    {
      "epoch": 31.059506531204644,
      "grad_norm": 5.156418323516846,
      "learning_rate": 2.4117077890662798e-05,
      "loss": 1.6078,
      "step": 406600
    },
    {
      "epoch": 31.067145367046063,
      "grad_norm": 4.901696681976318,
      "learning_rate": 2.411071219412828e-05,
      "loss": 1.6727,
      "step": 406700
    },
    {
      "epoch": 31.07478420288748,
      "grad_norm": 5.478796005249023,
      "learning_rate": 2.410434649759377e-05,
      "loss": 1.6799,
      "step": 406800
    },
    {
      "epoch": 31.082423038728898,
      "grad_norm": 4.330188274383545,
      "learning_rate": 2.4097980801059252e-05,
      "loss": 1.6046,
      "step": 406900
    },
    {
      "epoch": 31.090061874570317,
      "grad_norm": 5.349184036254883,
      "learning_rate": 2.409161510452474e-05,
      "loss": 1.8117,
      "step": 407000
    },
    {
      "epoch": 31.097700710411733,
      "grad_norm": 5.782310485839844,
      "learning_rate": 2.4085249407990223e-05,
      "loss": 1.6628,
      "step": 407100
    },
    {
      "epoch": 31.105339546253152,
      "grad_norm": 6.777194023132324,
      "learning_rate": 2.4078883711455706e-05,
      "loss": 1.7351,
      "step": 407200
    },
    {
      "epoch": 31.112978382094568,
      "grad_norm": 4.093118667602539,
      "learning_rate": 2.4072518014921193e-05,
      "loss": 1.6473,
      "step": 407300
    },
    {
      "epoch": 31.120617217935987,
      "grad_norm": 5.064237117767334,
      "learning_rate": 2.4066152318386677e-05,
      "loss": 1.7516,
      "step": 407400
    },
    {
      "epoch": 31.128256053777406,
      "grad_norm": 7.66307258605957,
      "learning_rate": 2.4059786621852164e-05,
      "loss": 1.6955,
      "step": 407500
    },
    {
      "epoch": 31.13589488961882,
      "grad_norm": 7.11785364151001,
      "learning_rate": 2.405342092531765e-05,
      "loss": 1.7619,
      "step": 407600
    },
    {
      "epoch": 31.14353372546024,
      "grad_norm": 4.030415058135986,
      "learning_rate": 2.4047055228783137e-05,
      "loss": 1.7234,
      "step": 407700
    },
    {
      "epoch": 31.151172561301657,
      "grad_norm": 4.761451721191406,
      "learning_rate": 2.404068953224862e-05,
      "loss": 1.7034,
      "step": 407800
    },
    {
      "epoch": 31.158811397143076,
      "grad_norm": 6.234499931335449,
      "learning_rate": 2.4034323835714104e-05,
      "loss": 1.7434,
      "step": 407900
    },
    {
      "epoch": 31.16645023298449,
      "grad_norm": 9.039888381958008,
      "learning_rate": 2.402795813917959e-05,
      "loss": 1.7053,
      "step": 408000
    },
    {
      "epoch": 31.17408906882591,
      "grad_norm": 4.442431449890137,
      "learning_rate": 2.4021592442645075e-05,
      "loss": 1.6944,
      "step": 408100
    },
    {
      "epoch": 31.18172790466733,
      "grad_norm": 5.390038013458252,
      "learning_rate": 2.4015226746110562e-05,
      "loss": 1.6919,
      "step": 408200
    },
    {
      "epoch": 31.189366740508746,
      "grad_norm": 8.085429191589355,
      "learning_rate": 2.4008861049576045e-05,
      "loss": 1.7696,
      "step": 408300
    },
    {
      "epoch": 31.197005576350165,
      "grad_norm": 5.53214168548584,
      "learning_rate": 2.4002495353041532e-05,
      "loss": 1.7467,
      "step": 408400
    },
    {
      "epoch": 31.20464441219158,
      "grad_norm": 3.5631327629089355,
      "learning_rate": 2.3996129656507016e-05,
      "loss": 1.7685,
      "step": 408500
    },
    {
      "epoch": 31.212283248033,
      "grad_norm": 5.00616455078125,
      "learning_rate": 2.3989763959972503e-05,
      "loss": 1.7917,
      "step": 408600
    },
    {
      "epoch": 31.21992208387442,
      "grad_norm": 3.752146005630493,
      "learning_rate": 2.3983398263437986e-05,
      "loss": 1.67,
      "step": 408700
    },
    {
      "epoch": 31.227560919715835,
      "grad_norm": 6.10659646987915,
      "learning_rate": 2.397703256690347e-05,
      "loss": 1.634,
      "step": 408800
    },
    {
      "epoch": 31.235199755557254,
      "grad_norm": 7.198127746582031,
      "learning_rate": 2.3970666870368957e-05,
      "loss": 1.728,
      "step": 408900
    },
    {
      "epoch": 31.24283859139867,
      "grad_norm": 5.7580766677856445,
      "learning_rate": 2.396430117383444e-05,
      "loss": 1.7215,
      "step": 409000
    },
    {
      "epoch": 31.25047742724009,
      "grad_norm": 6.878831386566162,
      "learning_rate": 2.3957935477299927e-05,
      "loss": 1.71,
      "step": 409100
    },
    {
      "epoch": 31.258116263081508,
      "grad_norm": 4.4904890060424805,
      "learning_rate": 2.395156978076541e-05,
      "loss": 1.611,
      "step": 409200
    },
    {
      "epoch": 31.265755098922924,
      "grad_norm": 4.999725818634033,
      "learning_rate": 2.3945204084230898e-05,
      "loss": 1.7039,
      "step": 409300
    },
    {
      "epoch": 31.273393934764343,
      "grad_norm": 5.72172737121582,
      "learning_rate": 2.393883838769638e-05,
      "loss": 1.7509,
      "step": 409400
    },
    {
      "epoch": 31.28103277060576,
      "grad_norm": 5.799651145935059,
      "learning_rate": 2.3932472691161868e-05,
      "loss": 1.6392,
      "step": 409500
    },
    {
      "epoch": 31.288671606447178,
      "grad_norm": 6.272284030914307,
      "learning_rate": 2.3926106994627355e-05,
      "loss": 1.717,
      "step": 409600
    },
    {
      "epoch": 31.296310442288597,
      "grad_norm": 5.297605991363525,
      "learning_rate": 2.391974129809284e-05,
      "loss": 1.705,
      "step": 409700
    },
    {
      "epoch": 31.303949278130013,
      "grad_norm": 3.825549602508545,
      "learning_rate": 2.3913375601558326e-05,
      "loss": 1.7484,
      "step": 409800
    },
    {
      "epoch": 31.311588113971432,
      "grad_norm": 4.87096643447876,
      "learning_rate": 2.390700990502381e-05,
      "loss": 1.6745,
      "step": 409900
    },
    {
      "epoch": 31.319226949812847,
      "grad_norm": 5.590961933135986,
      "learning_rate": 2.3900644208489296e-05,
      "loss": 1.7504,
      "step": 410000
    },
    {
      "epoch": 31.326865785654267,
      "grad_norm": 4.14864444732666,
      "learning_rate": 2.389427851195478e-05,
      "loss": 1.6792,
      "step": 410100
    },
    {
      "epoch": 31.334504621495682,
      "grad_norm": 3.9124467372894287,
      "learning_rate": 2.3887912815420263e-05,
      "loss": 1.7237,
      "step": 410200
    },
    {
      "epoch": 31.3421434573371,
      "grad_norm": 7.0688676834106445,
      "learning_rate": 2.388154711888575e-05,
      "loss": 1.7042,
      "step": 410300
    },
    {
      "epoch": 31.34978229317852,
      "grad_norm": 5.5733642578125,
      "learning_rate": 2.3875181422351234e-05,
      "loss": 1.7144,
      "step": 410400
    },
    {
      "epoch": 31.357421129019937,
      "grad_norm": 5.894885540008545,
      "learning_rate": 2.386881572581672e-05,
      "loss": 1.836,
      "step": 410500
    },
    {
      "epoch": 31.365059964861356,
      "grad_norm": 4.766432285308838,
      "learning_rate": 2.3862450029282204e-05,
      "loss": 1.6593,
      "step": 410600
    },
    {
      "epoch": 31.37269880070277,
      "grad_norm": 4.163215160369873,
      "learning_rate": 2.385608433274769e-05,
      "loss": 1.7806,
      "step": 410700
    },
    {
      "epoch": 31.38033763654419,
      "grad_norm": 5.0308122634887695,
      "learning_rate": 2.3849718636213174e-05,
      "loss": 1.765,
      "step": 410800
    },
    {
      "epoch": 31.38797647238561,
      "grad_norm": 5.820759296417236,
      "learning_rate": 2.384335293967866e-05,
      "loss": 1.713,
      "step": 410900
    },
    {
      "epoch": 31.395615308227026,
      "grad_norm": 3.940995931625366,
      "learning_rate": 2.3836987243144145e-05,
      "loss": 1.8359,
      "step": 411000
    },
    {
      "epoch": 31.403254144068445,
      "grad_norm": 6.155082702636719,
      "learning_rate": 2.383062154660963e-05,
      "loss": 1.7237,
      "step": 411100
    },
    {
      "epoch": 31.41089297990986,
      "grad_norm": 7.335752964019775,
      "learning_rate": 2.3824255850075115e-05,
      "loss": 1.7114,
      "step": 411200
    },
    {
      "epoch": 31.41853181575128,
      "grad_norm": 5.408636093139648,
      "learning_rate": 2.3817890153540602e-05,
      "loss": 1.6908,
      "step": 411300
    },
    {
      "epoch": 31.4261706515927,
      "grad_norm": 4.838520526885986,
      "learning_rate": 2.3811524457006086e-05,
      "loss": 1.6818,
      "step": 411400
    },
    {
      "epoch": 31.433809487434115,
      "grad_norm": 9.397852897644043,
      "learning_rate": 2.3805158760471573e-05,
      "loss": 1.7164,
      "step": 411500
    },
    {
      "epoch": 31.441448323275534,
      "grad_norm": 4.995196342468262,
      "learning_rate": 2.379879306393706e-05,
      "loss": 1.6152,
      "step": 411600
    },
    {
      "epoch": 31.44908715911695,
      "grad_norm": 5.299865245819092,
      "learning_rate": 2.3792427367402543e-05,
      "loss": 1.7678,
      "step": 411700
    },
    {
      "epoch": 31.45672599495837,
      "grad_norm": 4.728789806365967,
      "learning_rate": 2.3786061670868027e-05,
      "loss": 1.752,
      "step": 411800
    },
    {
      "epoch": 31.464364830799788,
      "grad_norm": 7.300608158111572,
      "learning_rate": 2.3779695974333514e-05,
      "loss": 1.7574,
      "step": 411900
    },
    {
      "epoch": 31.472003666641204,
      "grad_norm": 5.259843349456787,
      "learning_rate": 2.3773330277798997e-05,
      "loss": 1.7391,
      "step": 412000
    },
    {
      "epoch": 31.479642502482623,
      "grad_norm": 5.975590705871582,
      "learning_rate": 2.3766964581264484e-05,
      "loss": 1.7632,
      "step": 412100
    },
    {
      "epoch": 31.48728133832404,
      "grad_norm": 4.496400356292725,
      "learning_rate": 2.3760598884729968e-05,
      "loss": 1.7387,
      "step": 412200
    },
    {
      "epoch": 31.494920174165458,
      "grad_norm": 4.88356876373291,
      "learning_rate": 2.3754233188195455e-05,
      "loss": 1.6783,
      "step": 412300
    },
    {
      "epoch": 31.502559010006873,
      "grad_norm": 4.98804235458374,
      "learning_rate": 2.3747867491660938e-05,
      "loss": 1.8039,
      "step": 412400
    },
    {
      "epoch": 31.510197845848293,
      "grad_norm": 4.85331916809082,
      "learning_rate": 2.3741501795126425e-05,
      "loss": 1.7017,
      "step": 412500
    },
    {
      "epoch": 31.51783668168971,
      "grad_norm": 6.015202522277832,
      "learning_rate": 2.373513609859191e-05,
      "loss": 1.8502,
      "step": 412600
    },
    {
      "epoch": 31.525475517531127,
      "grad_norm": 6.112006187438965,
      "learning_rate": 2.3728770402057392e-05,
      "loss": 1.7276,
      "step": 412700
    },
    {
      "epoch": 31.533114353372547,
      "grad_norm": 3.642909288406372,
      "learning_rate": 2.372240470552288e-05,
      "loss": 1.7972,
      "step": 412800
    },
    {
      "epoch": 31.540753189213962,
      "grad_norm": 4.494691848754883,
      "learning_rate": 2.3716039008988363e-05,
      "loss": 1.7131,
      "step": 412900
    },
    {
      "epoch": 31.54839202505538,
      "grad_norm": 6.666355609893799,
      "learning_rate": 2.370967331245385e-05,
      "loss": 1.7464,
      "step": 413000
    },
    {
      "epoch": 31.5560308608968,
      "grad_norm": 5.035190105438232,
      "learning_rate": 2.3703307615919333e-05,
      "loss": 1.7329,
      "step": 413100
    },
    {
      "epoch": 31.563669696738216,
      "grad_norm": 4.145860195159912,
      "learning_rate": 2.369694191938482e-05,
      "loss": 1.6708,
      "step": 413200
    },
    {
      "epoch": 31.571308532579636,
      "grad_norm": 5.429652214050293,
      "learning_rate": 2.3690576222850307e-05,
      "loss": 1.7753,
      "step": 413300
    },
    {
      "epoch": 31.57894736842105,
      "grad_norm": 5.748322486877441,
      "learning_rate": 2.368421052631579e-05,
      "loss": 1.626,
      "step": 413400
    },
    {
      "epoch": 31.58658620426247,
      "grad_norm": 4.854522228240967,
      "learning_rate": 2.3677844829781277e-05,
      "loss": 1.7653,
      "step": 413500
    },
    {
      "epoch": 31.59422504010389,
      "grad_norm": 5.5783796310424805,
      "learning_rate": 2.367147913324676e-05,
      "loss": 1.7084,
      "step": 413600
    },
    {
      "epoch": 31.601863875945305,
      "grad_norm": 6.806907653808594,
      "learning_rate": 2.3665113436712248e-05,
      "loss": 1.6913,
      "step": 413700
    },
    {
      "epoch": 31.609502711786725,
      "grad_norm": 5.028125286102295,
      "learning_rate": 2.365874774017773e-05,
      "loss": 1.6907,
      "step": 413800
    },
    {
      "epoch": 31.61714154762814,
      "grad_norm": 6.090077877044678,
      "learning_rate": 2.3652382043643218e-05,
      "loss": 1.7809,
      "step": 413900
    },
    {
      "epoch": 31.62478038346956,
      "grad_norm": 5.934323310852051,
      "learning_rate": 2.3646016347108702e-05,
      "loss": 1.6761,
      "step": 414000
    },
    {
      "epoch": 31.632419219310975,
      "grad_norm": 6.868094444274902,
      "learning_rate": 2.363965065057419e-05,
      "loss": 1.6543,
      "step": 414100
    },
    {
      "epoch": 31.640058055152394,
      "grad_norm": 6.364013671875,
      "learning_rate": 2.3633284954039672e-05,
      "loss": 1.7978,
      "step": 414200
    },
    {
      "epoch": 31.647696890993814,
      "grad_norm": 5.869101524353027,
      "learning_rate": 2.3626919257505156e-05,
      "loss": 1.6491,
      "step": 414300
    },
    {
      "epoch": 31.65533572683523,
      "grad_norm": 4.907527446746826,
      "learning_rate": 2.3620553560970643e-05,
      "loss": 1.7449,
      "step": 414400
    },
    {
      "epoch": 31.66297456267665,
      "grad_norm": 5.936131477355957,
      "learning_rate": 2.3614187864436126e-05,
      "loss": 1.675,
      "step": 414500
    },
    {
      "epoch": 31.670613398518064,
      "grad_norm": 4.936668872833252,
      "learning_rate": 2.3607822167901613e-05,
      "loss": 1.6752,
      "step": 414600
    },
    {
      "epoch": 31.678252234359483,
      "grad_norm": 6.000504970550537,
      "learning_rate": 2.3601456471367097e-05,
      "loss": 1.6839,
      "step": 414700
    },
    {
      "epoch": 31.685891070200903,
      "grad_norm": 3.923549175262451,
      "learning_rate": 2.3595090774832584e-05,
      "loss": 1.716,
      "step": 414800
    },
    {
      "epoch": 31.69352990604232,
      "grad_norm": 6.450545787811279,
      "learning_rate": 2.3588725078298067e-05,
      "loss": 1.7622,
      "step": 414900
    },
    {
      "epoch": 31.701168741883738,
      "grad_norm": 4.782499313354492,
      "learning_rate": 2.358235938176355e-05,
      "loss": 1.7287,
      "step": 415000
    },
    {
      "epoch": 31.708807577725153,
      "grad_norm": 6.311882495880127,
      "learning_rate": 2.3575993685229038e-05,
      "loss": 1.6298,
      "step": 415100
    },
    {
      "epoch": 31.716446413566572,
      "grad_norm": 6.852009296417236,
      "learning_rate": 2.3569627988694525e-05,
      "loss": 1.7551,
      "step": 415200
    },
    {
      "epoch": 31.72408524940799,
      "grad_norm": 5.81201696395874,
      "learning_rate": 2.356326229216001e-05,
      "loss": 1.8344,
      "step": 415300
    },
    {
      "epoch": 31.731724085249407,
      "grad_norm": 5.287972927093506,
      "learning_rate": 2.3556896595625495e-05,
      "loss": 1.6451,
      "step": 415400
    },
    {
      "epoch": 31.739362921090827,
      "grad_norm": 5.072876930236816,
      "learning_rate": 2.3550530899090982e-05,
      "loss": 1.6297,
      "step": 415500
    },
    {
      "epoch": 31.747001756932242,
      "grad_norm": 6.673508644104004,
      "learning_rate": 2.3544165202556466e-05,
      "loss": 1.6322,
      "step": 415600
    },
    {
      "epoch": 31.75464059277366,
      "grad_norm": 6.707568168640137,
      "learning_rate": 2.3537799506021952e-05,
      "loss": 1.7627,
      "step": 415700
    },
    {
      "epoch": 31.76227942861508,
      "grad_norm": 5.342321395874023,
      "learning_rate": 2.3531433809487436e-05,
      "loss": 1.6718,
      "step": 415800
    },
    {
      "epoch": 31.769918264456496,
      "grad_norm": 4.991833209991455,
      "learning_rate": 2.352506811295292e-05,
      "loss": 1.6833,
      "step": 415900
    },
    {
      "epoch": 31.777557100297916,
      "grad_norm": 5.548736572265625,
      "learning_rate": 2.3518702416418406e-05,
      "loss": 1.7179,
      "step": 416000
    },
    {
      "epoch": 31.78519593613933,
      "grad_norm": 5.880224704742432,
      "learning_rate": 2.351233671988389e-05,
      "loss": 1.769,
      "step": 416100
    },
    {
      "epoch": 31.79283477198075,
      "grad_norm": 6.0578131675720215,
      "learning_rate": 2.3505971023349377e-05,
      "loss": 1.7514,
      "step": 416200
    },
    {
      "epoch": 31.80047360782217,
      "grad_norm": 4.399991989135742,
      "learning_rate": 2.349960532681486e-05,
      "loss": 1.7867,
      "step": 416300
    },
    {
      "epoch": 31.808112443663585,
      "grad_norm": 7.196582317352295,
      "learning_rate": 2.3493239630280347e-05,
      "loss": 1.6525,
      "step": 416400
    },
    {
      "epoch": 31.815751279505005,
      "grad_norm": 5.0911545753479,
      "learning_rate": 2.348687393374583e-05,
      "loss": 1.7287,
      "step": 416500
    },
    {
      "epoch": 31.82339011534642,
      "grad_norm": 6.3215718269348145,
      "learning_rate": 2.3480508237211314e-05,
      "loss": 1.6325,
      "step": 416600
    },
    {
      "epoch": 31.83102895118784,
      "grad_norm": 7.0489935874938965,
      "learning_rate": 2.34741425406768e-05,
      "loss": 1.7537,
      "step": 416700
    },
    {
      "epoch": 31.838667787029255,
      "grad_norm": 6.404745101928711,
      "learning_rate": 2.3467776844142285e-05,
      "loss": 1.7904,
      "step": 416800
    },
    {
      "epoch": 31.846306622870674,
      "grad_norm": 6.02178955078125,
      "learning_rate": 2.3461411147607772e-05,
      "loss": 1.7595,
      "step": 416900
    },
    {
      "epoch": 31.853945458712094,
      "grad_norm": 4.353187561035156,
      "learning_rate": 2.3455045451073255e-05,
      "loss": 1.714,
      "step": 417000
    },
    {
      "epoch": 31.86158429455351,
      "grad_norm": 5.29369592666626,
      "learning_rate": 2.3448679754538742e-05,
      "loss": 1.652,
      "step": 417100
    },
    {
      "epoch": 31.86922313039493,
      "grad_norm": 5.008917331695557,
      "learning_rate": 2.344231405800423e-05,
      "loss": 1.6749,
      "step": 417200
    },
    {
      "epoch": 31.876861966236344,
      "grad_norm": 5.695868492126465,
      "learning_rate": 2.3435948361469716e-05,
      "loss": 1.8206,
      "step": 417300
    },
    {
      "epoch": 31.884500802077763,
      "grad_norm": 6.3884358406066895,
      "learning_rate": 2.34295826649352e-05,
      "loss": 1.7051,
      "step": 417400
    },
    {
      "epoch": 31.892139637919183,
      "grad_norm": 5.909282684326172,
      "learning_rate": 2.3423216968400683e-05,
      "loss": 1.6676,
      "step": 417500
    },
    {
      "epoch": 31.8997784737606,
      "grad_norm": 8.381195068359375,
      "learning_rate": 2.341685127186617e-05,
      "loss": 1.7084,
      "step": 417600
    },
    {
      "epoch": 31.907417309602017,
      "grad_norm": 7.221092700958252,
      "learning_rate": 2.3410485575331654e-05,
      "loss": 1.7017,
      "step": 417700
    },
    {
      "epoch": 31.915056145443433,
      "grad_norm": 8.919231414794922,
      "learning_rate": 2.340411987879714e-05,
      "loss": 1.7399,
      "step": 417800
    },
    {
      "epoch": 31.922694981284852,
      "grad_norm": 4.231845378875732,
      "learning_rate": 2.3397754182262624e-05,
      "loss": 1.7376,
      "step": 417900
    },
    {
      "epoch": 31.93033381712627,
      "grad_norm": 4.184365749359131,
      "learning_rate": 2.339138848572811e-05,
      "loss": 1.7125,
      "step": 418000
    },
    {
      "epoch": 31.937972652967687,
      "grad_norm": 5.782565116882324,
      "learning_rate": 2.3385022789193595e-05,
      "loss": 1.7007,
      "step": 418100
    },
    {
      "epoch": 31.945611488809107,
      "grad_norm": 5.249534606933594,
      "learning_rate": 2.3378657092659078e-05,
      "loss": 1.7229,
      "step": 418200
    },
    {
      "epoch": 31.953250324650522,
      "grad_norm": 5.985377788543701,
      "learning_rate": 2.3372291396124565e-05,
      "loss": 1.65,
      "step": 418300
    },
    {
      "epoch": 31.96088916049194,
      "grad_norm": 7.56099796295166,
      "learning_rate": 2.336592569959005e-05,
      "loss": 1.8152,
      "step": 418400
    },
    {
      "epoch": 31.968527996333357,
      "grad_norm": 5.501145362854004,
      "learning_rate": 2.3359560003055536e-05,
      "loss": 1.71,
      "step": 418500
    },
    {
      "epoch": 31.976166832174776,
      "grad_norm": 6.379695892333984,
      "learning_rate": 2.335319430652102e-05,
      "loss": 1.7023,
      "step": 418600
    },
    {
      "epoch": 31.983805668016196,
      "grad_norm": 6.458169937133789,
      "learning_rate": 2.3346828609986506e-05,
      "loss": 1.6002,
      "step": 418700
    },
    {
      "epoch": 31.99144450385761,
      "grad_norm": 6.6071929931640625,
      "learning_rate": 2.334046291345199e-05,
      "loss": 1.7567,
      "step": 418800
    },
    {
      "epoch": 31.99908333969903,
      "grad_norm": 5.761111736297607,
      "learning_rate": 2.3334097216917476e-05,
      "loss": 1.8219,
      "step": 418900
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.7810331583023071,
      "eval_runtime": 1.4425,
      "eval_samples_per_second": 478.344,
      "eval_steps_per_second": 478.344,
      "step": 418912
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.4793407917022705,
      "eval_runtime": 27.5589,
      "eval_samples_per_second": 475.018,
      "eval_steps_per_second": 475.018,
      "step": 418912
    },
    {
      "epoch": 32.006722175540446,
      "grad_norm": 5.482280731201172,
      "learning_rate": 2.332773152038296e-05,
      "loss": 1.7539,
      "step": 419000
    },
    {
      "epoch": 32.014361011381865,
      "grad_norm": 7.505959987640381,
      "learning_rate": 2.3321365823848447e-05,
      "loss": 1.7346,
      "step": 419100
    },
    {
      "epoch": 32.021999847223285,
      "grad_norm": 6.974273681640625,
      "learning_rate": 2.3315000127313934e-05,
      "loss": 1.6416,
      "step": 419200
    },
    {
      "epoch": 32.029638683064704,
      "grad_norm": 6.3807854652404785,
      "learning_rate": 2.3308634430779417e-05,
      "loss": 1.6351,
      "step": 419300
    },
    {
      "epoch": 32.037277518906116,
      "grad_norm": 5.170651435852051,
      "learning_rate": 2.3302268734244904e-05,
      "loss": 1.6868,
      "step": 419400
    },
    {
      "epoch": 32.044916354747535,
      "grad_norm": 6.109926223754883,
      "learning_rate": 2.3295903037710388e-05,
      "loss": 1.6832,
      "step": 419500
    },
    {
      "epoch": 32.052555190588954,
      "grad_norm": 6.536916732788086,
      "learning_rate": 2.3289537341175875e-05,
      "loss": 1.7857,
      "step": 419600
    },
    {
      "epoch": 32.06019402643037,
      "grad_norm": 4.5685319900512695,
      "learning_rate": 2.3283171644641358e-05,
      "loss": 1.7188,
      "step": 419700
    },
    {
      "epoch": 32.06783286227179,
      "grad_norm": 6.379975318908691,
      "learning_rate": 2.3276805948106842e-05,
      "loss": 1.6184,
      "step": 419800
    },
    {
      "epoch": 32.075471698113205,
      "grad_norm": 5.670656204223633,
      "learning_rate": 2.327044025157233e-05,
      "loss": 1.7466,
      "step": 419900
    },
    {
      "epoch": 32.083110533954624,
      "grad_norm": 5.621361255645752,
      "learning_rate": 2.3264074555037812e-05,
      "loss": 1.7279,
      "step": 420000
    },
    {
      "epoch": 32.09074936979604,
      "grad_norm": 6.129852771759033,
      "learning_rate": 2.32577088585033e-05,
      "loss": 1.7109,
      "step": 420100
    },
    {
      "epoch": 32.09838820563746,
      "grad_norm": 5.737197399139404,
      "learning_rate": 2.3251343161968783e-05,
      "loss": 1.6659,
      "step": 420200
    },
    {
      "epoch": 32.10602704147888,
      "grad_norm": 5.033868312835693,
      "learning_rate": 2.324497746543427e-05,
      "loss": 1.7109,
      "step": 420300
    },
    {
      "epoch": 32.113665877320294,
      "grad_norm": 6.195271968841553,
      "learning_rate": 2.3238611768899753e-05,
      "loss": 1.742,
      "step": 420400
    },
    {
      "epoch": 32.12130471316171,
      "grad_norm": 4.947885513305664,
      "learning_rate": 2.3232246072365237e-05,
      "loss": 1.6832,
      "step": 420500
    },
    {
      "epoch": 32.12894354900313,
      "grad_norm": 6.968204975128174,
      "learning_rate": 2.3225880375830724e-05,
      "loss": 1.7275,
      "step": 420600
    },
    {
      "epoch": 32.13658238484455,
      "grad_norm": 6.795361518859863,
      "learning_rate": 2.3219514679296207e-05,
      "loss": 1.8033,
      "step": 420700
    },
    {
      "epoch": 32.14422122068597,
      "grad_norm": 5.3870344161987305,
      "learning_rate": 2.3213148982761694e-05,
      "loss": 1.643,
      "step": 420800
    },
    {
      "epoch": 32.15186005652738,
      "grad_norm": 6.487068176269531,
      "learning_rate": 2.320678328622718e-05,
      "loss": 1.6609,
      "step": 420900
    },
    {
      "epoch": 32.1594988923688,
      "grad_norm": 7.961830139160156,
      "learning_rate": 2.3200417589692665e-05,
      "loss": 1.7968,
      "step": 421000
    },
    {
      "epoch": 32.16713772821022,
      "grad_norm": 9.217037200927734,
      "learning_rate": 2.319405189315815e-05,
      "loss": 1.6279,
      "step": 421100
    },
    {
      "epoch": 32.17477656405164,
      "grad_norm": 5.31409215927124,
      "learning_rate": 2.318768619662364e-05,
      "loss": 1.6854,
      "step": 421200
    },
    {
      "epoch": 32.18241539989306,
      "grad_norm": 7.282495021820068,
      "learning_rate": 2.3181320500089122e-05,
      "loss": 1.6624,
      "step": 421300
    },
    {
      "epoch": 32.19005423573447,
      "grad_norm": 4.601370811462402,
      "learning_rate": 2.3174954803554606e-05,
      "loss": 1.7508,
      "step": 421400
    },
    {
      "epoch": 32.19769307157589,
      "grad_norm": 5.9625372886657715,
      "learning_rate": 2.3168589107020092e-05,
      "loss": 1.6173,
      "step": 421500
    },
    {
      "epoch": 32.20533190741731,
      "grad_norm": 4.829470157623291,
      "learning_rate": 2.3162223410485576e-05,
      "loss": 1.8017,
      "step": 421600
    },
    {
      "epoch": 32.21297074325873,
      "grad_norm": 6.267845630645752,
      "learning_rate": 2.3155857713951063e-05,
      "loss": 1.6859,
      "step": 421700
    },
    {
      "epoch": 32.22060957910014,
      "grad_norm": 4.593508720397949,
      "learning_rate": 2.3149492017416546e-05,
      "loss": 1.7255,
      "step": 421800
    },
    {
      "epoch": 32.22824841494156,
      "grad_norm": 4.386449337005615,
      "learning_rate": 2.3143126320882033e-05,
      "loss": 1.6716,
      "step": 421900
    },
    {
      "epoch": 32.23588725078298,
      "grad_norm": 5.459444522857666,
      "learning_rate": 2.3136760624347517e-05,
      "loss": 1.657,
      "step": 422000
    },
    {
      "epoch": 32.2435260866244,
      "grad_norm": 6.204333782196045,
      "learning_rate": 2.3130394927813e-05,
      "loss": 1.6976,
      "step": 422100
    },
    {
      "epoch": 32.25116492246582,
      "grad_norm": 8.688410758972168,
      "learning_rate": 2.3124029231278487e-05,
      "loss": 1.7165,
      "step": 422200
    },
    {
      "epoch": 32.25880375830723,
      "grad_norm": 5.945576190948486,
      "learning_rate": 2.311766353474397e-05,
      "loss": 1.719,
      "step": 422300
    },
    {
      "epoch": 32.26644259414865,
      "grad_norm": 6.635311603546143,
      "learning_rate": 2.3111297838209458e-05,
      "loss": 1.7495,
      "step": 422400
    },
    {
      "epoch": 32.27408142999007,
      "grad_norm": 6.3187947273254395,
      "learning_rate": 2.310493214167494e-05,
      "loss": 1.7935,
      "step": 422500
    },
    {
      "epoch": 32.28172026583149,
      "grad_norm": 4.918920516967773,
      "learning_rate": 2.3098566445140428e-05,
      "loss": 1.7422,
      "step": 422600
    },
    {
      "epoch": 32.28935910167291,
      "grad_norm": 4.517783164978027,
      "learning_rate": 2.3092200748605912e-05,
      "loss": 1.7888,
      "step": 422700
    },
    {
      "epoch": 32.29699793751432,
      "grad_norm": 3.8753161430358887,
      "learning_rate": 2.30858350520714e-05,
      "loss": 1.7678,
      "step": 422800
    },
    {
      "epoch": 32.30463677335574,
      "grad_norm": 5.769226551055908,
      "learning_rate": 2.3079469355536886e-05,
      "loss": 1.599,
      "step": 422900
    },
    {
      "epoch": 32.31227560919716,
      "grad_norm": 5.524023532867432,
      "learning_rate": 2.307310365900237e-05,
      "loss": 1.7262,
      "step": 423000
    },
    {
      "epoch": 32.31991444503858,
      "grad_norm": 5.360753536224365,
      "learning_rate": 2.3066737962467856e-05,
      "loss": 1.6978,
      "step": 423100
    },
    {
      "epoch": 32.32755328088,
      "grad_norm": 4.478679180145264,
      "learning_rate": 2.306037226593334e-05,
      "loss": 1.7041,
      "step": 423200
    },
    {
      "epoch": 32.33519211672141,
      "grad_norm": 5.727509021759033,
      "learning_rate": 2.3054006569398827e-05,
      "loss": 1.7537,
      "step": 423300
    },
    {
      "epoch": 32.34283095256283,
      "grad_norm": 4.602668285369873,
      "learning_rate": 2.304764087286431e-05,
      "loss": 1.6044,
      "step": 423400
    },
    {
      "epoch": 32.35046978840425,
      "grad_norm": 5.3501362800598145,
      "learning_rate": 2.3041275176329797e-05,
      "loss": 1.8346,
      "step": 423500
    },
    {
      "epoch": 32.358108624245666,
      "grad_norm": 5.325484275817871,
      "learning_rate": 2.303490947979528e-05,
      "loss": 1.7193,
      "step": 423600
    },
    {
      "epoch": 32.365747460087086,
      "grad_norm": 6.430894374847412,
      "learning_rate": 2.3028543783260764e-05,
      "loss": 1.8454,
      "step": 423700
    },
    {
      "epoch": 32.3733862959285,
      "grad_norm": 5.638967990875244,
      "learning_rate": 2.302217808672625e-05,
      "loss": 1.6919,
      "step": 423800
    },
    {
      "epoch": 32.38102513176992,
      "grad_norm": 4.799129009246826,
      "learning_rate": 2.3015812390191735e-05,
      "loss": 1.6834,
      "step": 423900
    },
    {
      "epoch": 32.388663967611336,
      "grad_norm": 6.363901138305664,
      "learning_rate": 2.300944669365722e-05,
      "loss": 1.7246,
      "step": 424000
    },
    {
      "epoch": 32.396302803452755,
      "grad_norm": 4.629639625549316,
      "learning_rate": 2.3003080997122705e-05,
      "loss": 1.8038,
      "step": 424100
    },
    {
      "epoch": 32.403941639294175,
      "grad_norm": 5.848518371582031,
      "learning_rate": 2.2996715300588192e-05,
      "loss": 1.6562,
      "step": 424200
    },
    {
      "epoch": 32.41158047513559,
      "grad_norm": 5.690576076507568,
      "learning_rate": 2.2990349604053675e-05,
      "loss": 1.7196,
      "step": 424300
    },
    {
      "epoch": 32.419219310977006,
      "grad_norm": 4.748055934906006,
      "learning_rate": 2.2983983907519162e-05,
      "loss": 1.7092,
      "step": 424400
    },
    {
      "epoch": 32.426858146818425,
      "grad_norm": 5.966780662536621,
      "learning_rate": 2.2977618210984646e-05,
      "loss": 1.7357,
      "step": 424500
    },
    {
      "epoch": 32.434496982659844,
      "grad_norm": 6.442547798156738,
      "learning_rate": 2.297125251445013e-05,
      "loss": 1.7055,
      "step": 424600
    },
    {
      "epoch": 32.442135818501264,
      "grad_norm": 5.988017559051514,
      "learning_rate": 2.2964886817915616e-05,
      "loss": 1.7683,
      "step": 424700
    },
    {
      "epoch": 32.449774654342676,
      "grad_norm": 4.5576653480529785,
      "learning_rate": 2.2958521121381103e-05,
      "loss": 1.7494,
      "step": 424800
    },
    {
      "epoch": 32.457413490184095,
      "grad_norm": 5.679787635803223,
      "learning_rate": 2.295215542484659e-05,
      "loss": 1.685,
      "step": 424900
    },
    {
      "epoch": 32.465052326025514,
      "grad_norm": 6.142622470855713,
      "learning_rate": 2.2945789728312074e-05,
      "loss": 1.8008,
      "step": 425000
    },
    {
      "epoch": 32.47269116186693,
      "grad_norm": 4.8211517333984375,
      "learning_rate": 2.293942403177756e-05,
      "loss": 1.7365,
      "step": 425100
    },
    {
      "epoch": 32.48032999770835,
      "grad_norm": 6.83621883392334,
      "learning_rate": 2.2933058335243044e-05,
      "loss": 1.6748,
      "step": 425200
    },
    {
      "epoch": 32.487968833549765,
      "grad_norm": 5.322911739349365,
      "learning_rate": 2.2926692638708528e-05,
      "loss": 1.7281,
      "step": 425300
    },
    {
      "epoch": 32.495607669391184,
      "grad_norm": 7.508693218231201,
      "learning_rate": 2.2920326942174015e-05,
      "loss": 1.7264,
      "step": 425400
    },
    {
      "epoch": 32.5032465052326,
      "grad_norm": 5.738246917724609,
      "learning_rate": 2.2913961245639498e-05,
      "loss": 1.7445,
      "step": 425500
    },
    {
      "epoch": 32.51088534107402,
      "grad_norm": 4.615869522094727,
      "learning_rate": 2.2907595549104985e-05,
      "loss": 1.664,
      "step": 425600
    },
    {
      "epoch": 32.518524176915435,
      "grad_norm": 7.403569221496582,
      "learning_rate": 2.290122985257047e-05,
      "loss": 1.716,
      "step": 425700
    },
    {
      "epoch": 32.526163012756854,
      "grad_norm": 4.430939197540283,
      "learning_rate": 2.2894864156035956e-05,
      "loss": 1.6265,
      "step": 425800
    },
    {
      "epoch": 32.53380184859827,
      "grad_norm": 5.980095386505127,
      "learning_rate": 2.288849845950144e-05,
      "loss": 1.7869,
      "step": 425900
    },
    {
      "epoch": 32.54144068443969,
      "grad_norm": 7.332479953765869,
      "learning_rate": 2.2882132762966926e-05,
      "loss": 1.7892,
      "step": 426000
    },
    {
      "epoch": 32.54907952028111,
      "grad_norm": 5.190194606781006,
      "learning_rate": 2.287576706643241e-05,
      "loss": 1.6804,
      "step": 426100
    },
    {
      "epoch": 32.55671835612252,
      "grad_norm": 5.651073932647705,
      "learning_rate": 2.2869401369897893e-05,
      "loss": 1.7817,
      "step": 426200
    },
    {
      "epoch": 32.56435719196394,
      "grad_norm": 4.607849597930908,
      "learning_rate": 2.286303567336338e-05,
      "loss": 1.7005,
      "step": 426300
    },
    {
      "epoch": 32.57199602780536,
      "grad_norm": 7.679828643798828,
      "learning_rate": 2.2856669976828864e-05,
      "loss": 1.6417,
      "step": 426400
    },
    {
      "epoch": 32.57963486364678,
      "grad_norm": 6.511417865753174,
      "learning_rate": 2.285030428029435e-05,
      "loss": 1.6206,
      "step": 426500
    },
    {
      "epoch": 32.5872736994882,
      "grad_norm": 5.2038702964782715,
      "learning_rate": 2.2843938583759834e-05,
      "loss": 1.7262,
      "step": 426600
    },
    {
      "epoch": 32.59491253532961,
      "grad_norm": 3.7754018306732178,
      "learning_rate": 2.283757288722532e-05,
      "loss": 1.6533,
      "step": 426700
    },
    {
      "epoch": 32.60255137117103,
      "grad_norm": 5.839900016784668,
      "learning_rate": 2.2831207190690808e-05,
      "loss": 1.7278,
      "step": 426800
    },
    {
      "epoch": 32.61019020701245,
      "grad_norm": 6.3116888999938965,
      "learning_rate": 2.282484149415629e-05,
      "loss": 1.7572,
      "step": 426900
    },
    {
      "epoch": 32.61782904285387,
      "grad_norm": 4.807529926300049,
      "learning_rate": 2.281847579762178e-05,
      "loss": 1.7631,
      "step": 427000
    },
    {
      "epoch": 32.62546787869529,
      "grad_norm": 5.839535236358643,
      "learning_rate": 2.2812110101087262e-05,
      "loss": 1.7231,
      "step": 427100
    },
    {
      "epoch": 32.6331067145367,
      "grad_norm": 7.484116077423096,
      "learning_rate": 2.280574440455275e-05,
      "loss": 1.8159,
      "step": 427200
    },
    {
      "epoch": 32.64074555037812,
      "grad_norm": 5.699751377105713,
      "learning_rate": 2.2799378708018232e-05,
      "loss": 1.7347,
      "step": 427300
    },
    {
      "epoch": 32.64838438621954,
      "grad_norm": 5.329753875732422,
      "learning_rate": 2.279301301148372e-05,
      "loss": 1.6221,
      "step": 427400
    },
    {
      "epoch": 32.65602322206096,
      "grad_norm": 5.55589485168457,
      "learning_rate": 2.2786647314949203e-05,
      "loss": 1.6577,
      "step": 427500
    },
    {
      "epoch": 32.66366205790238,
      "grad_norm": 4.861120223999023,
      "learning_rate": 2.2780281618414686e-05,
      "loss": 1.7893,
      "step": 427600
    },
    {
      "epoch": 32.67130089374379,
      "grad_norm": 4.935731887817383,
      "learning_rate": 2.2773915921880173e-05,
      "loss": 1.7214,
      "step": 427700
    },
    {
      "epoch": 32.67893972958521,
      "grad_norm": 4.285508155822754,
      "learning_rate": 2.2767550225345657e-05,
      "loss": 1.7148,
      "step": 427800
    },
    {
      "epoch": 32.68657856542663,
      "grad_norm": 4.202752590179443,
      "learning_rate": 2.2761184528811144e-05,
      "loss": 1.5688,
      "step": 427900
    },
    {
      "epoch": 32.69421740126805,
      "grad_norm": 5.197017192840576,
      "learning_rate": 2.2754818832276627e-05,
      "loss": 1.699,
      "step": 428000
    },
    {
      "epoch": 32.70185623710947,
      "grad_norm": 5.689459800720215,
      "learning_rate": 2.2748453135742114e-05,
      "loss": 1.7103,
      "step": 428100
    },
    {
      "epoch": 32.70949507295088,
      "grad_norm": 5.255361557006836,
      "learning_rate": 2.2742087439207598e-05,
      "loss": 1.7358,
      "step": 428200
    },
    {
      "epoch": 32.7171339087923,
      "grad_norm": 5.90843391418457,
      "learning_rate": 2.2735721742673085e-05,
      "loss": 1.7705,
      "step": 428300
    },
    {
      "epoch": 32.72477274463372,
      "grad_norm": 7.590860843658447,
      "learning_rate": 2.2729356046138568e-05,
      "loss": 1.8043,
      "step": 428400
    },
    {
      "epoch": 32.73241158047514,
      "grad_norm": 4.871232032775879,
      "learning_rate": 2.2722990349604055e-05,
      "loss": 1.6832,
      "step": 428500
    },
    {
      "epoch": 32.740050416316556,
      "grad_norm": 6.265678405761719,
      "learning_rate": 2.271662465306954e-05,
      "loss": 1.608,
      "step": 428600
    },
    {
      "epoch": 32.74768925215797,
      "grad_norm": 4.587138652801514,
      "learning_rate": 2.2710258956535026e-05,
      "loss": 1.6737,
      "step": 428700
    },
    {
      "epoch": 32.75532808799939,
      "grad_norm": 6.002613544464111,
      "learning_rate": 2.2703893260000513e-05,
      "loss": 1.774,
      "step": 428800
    },
    {
      "epoch": 32.76296692384081,
      "grad_norm": 5.606353759765625,
      "learning_rate": 2.2697527563465996e-05,
      "loss": 1.6775,
      "step": 428900
    },
    {
      "epoch": 32.770605759682226,
      "grad_norm": 7.294227123260498,
      "learning_rate": 2.2691161866931483e-05,
      "loss": 1.7056,
      "step": 429000
    },
    {
      "epoch": 32.778244595523645,
      "grad_norm": 5.8536376953125,
      "learning_rate": 2.2684796170396967e-05,
      "loss": 1.6034,
      "step": 429100
    },
    {
      "epoch": 32.78588343136506,
      "grad_norm": 5.111408233642578,
      "learning_rate": 2.267843047386245e-05,
      "loss": 1.6843,
      "step": 429200
    },
    {
      "epoch": 32.79352226720648,
      "grad_norm": 5.483267784118652,
      "learning_rate": 2.2672064777327937e-05,
      "loss": 1.6999,
      "step": 429300
    },
    {
      "epoch": 32.801161103047896,
      "grad_norm": 6.835545063018799,
      "learning_rate": 2.266569908079342e-05,
      "loss": 1.8298,
      "step": 429400
    },
    {
      "epoch": 32.808799938889315,
      "grad_norm": 4.969388484954834,
      "learning_rate": 2.2659333384258907e-05,
      "loss": 1.6655,
      "step": 429500
    },
    {
      "epoch": 32.816438774730734,
      "grad_norm": 6.397458553314209,
      "learning_rate": 2.265296768772439e-05,
      "loss": 1.7103,
      "step": 429600
    },
    {
      "epoch": 32.82407761057215,
      "grad_norm": 6.706971168518066,
      "learning_rate": 2.2646601991189878e-05,
      "loss": 1.6447,
      "step": 429700
    },
    {
      "epoch": 32.831716446413566,
      "grad_norm": 7.302886962890625,
      "learning_rate": 2.264023629465536e-05,
      "loss": 1.7619,
      "step": 429800
    },
    {
      "epoch": 32.839355282254985,
      "grad_norm": 6.117109775543213,
      "learning_rate": 2.263387059812085e-05,
      "loss": 1.7218,
      "step": 429900
    },
    {
      "epoch": 32.846994118096404,
      "grad_norm": 4.554698467254639,
      "learning_rate": 2.2627504901586332e-05,
      "loss": 1.6874,
      "step": 430000
    },
    {
      "epoch": 32.85463295393782,
      "grad_norm": 5.089025020599365,
      "learning_rate": 2.2621139205051815e-05,
      "loss": 1.7329,
      "step": 430100
    },
    {
      "epoch": 32.862271789779236,
      "grad_norm": 4.794121742248535,
      "learning_rate": 2.2614773508517302e-05,
      "loss": 1.804,
      "step": 430200
    },
    {
      "epoch": 32.869910625620655,
      "grad_norm": 3.687213182449341,
      "learning_rate": 2.2608407811982786e-05,
      "loss": 1.6272,
      "step": 430300
    },
    {
      "epoch": 32.877549461462074,
      "grad_norm": 6.4482221603393555,
      "learning_rate": 2.2602042115448273e-05,
      "loss": 1.7111,
      "step": 430400
    },
    {
      "epoch": 32.88518829730349,
      "grad_norm": 6.123465061187744,
      "learning_rate": 2.259567641891376e-05,
      "loss": 1.7218,
      "step": 430500
    },
    {
      "epoch": 32.892827133144905,
      "grad_norm": 5.520358562469482,
      "learning_rate": 2.2589310722379243e-05,
      "loss": 1.7616,
      "step": 430600
    },
    {
      "epoch": 32.900465968986325,
      "grad_norm": 6.991438865661621,
      "learning_rate": 2.258294502584473e-05,
      "loss": 1.6933,
      "step": 430700
    },
    {
      "epoch": 32.908104804827744,
      "grad_norm": 4.825100898742676,
      "learning_rate": 2.2576579329310214e-05,
      "loss": 1.763,
      "step": 430800
    },
    {
      "epoch": 32.91574364066916,
      "grad_norm": 5.563538074493408,
      "learning_rate": 2.25702136327757e-05,
      "loss": 1.6492,
      "step": 430900
    },
    {
      "epoch": 32.92338247651058,
      "grad_norm": 7.947152614593506,
      "learning_rate": 2.2563847936241184e-05,
      "loss": 1.7092,
      "step": 431000
    },
    {
      "epoch": 32.931021312351994,
      "grad_norm": 6.116511344909668,
      "learning_rate": 2.255748223970667e-05,
      "loss": 1.6156,
      "step": 431100
    },
    {
      "epoch": 32.938660148193414,
      "grad_norm": 5.274980068206787,
      "learning_rate": 2.2551116543172155e-05,
      "loss": 1.7276,
      "step": 431200
    },
    {
      "epoch": 32.94629898403483,
      "grad_norm": 5.972294807434082,
      "learning_rate": 2.254475084663764e-05,
      "loss": 1.6733,
      "step": 431300
    },
    {
      "epoch": 32.95393781987625,
      "grad_norm": 5.377496242523193,
      "learning_rate": 2.2538385150103125e-05,
      "loss": 1.6741,
      "step": 431400
    },
    {
      "epoch": 32.96157665571767,
      "grad_norm": 6.0101799964904785,
      "learning_rate": 2.2532019453568612e-05,
      "loss": 1.806,
      "step": 431500
    },
    {
      "epoch": 32.96921549155908,
      "grad_norm": 5.643598556518555,
      "learning_rate": 2.2525653757034096e-05,
      "loss": 1.778,
      "step": 431600
    },
    {
      "epoch": 32.9768543274005,
      "grad_norm": 7.219012260437012,
      "learning_rate": 2.251928806049958e-05,
      "loss": 1.6781,
      "step": 431700
    },
    {
      "epoch": 32.98449316324192,
      "grad_norm": 3.926300525665283,
      "learning_rate": 2.2512922363965066e-05,
      "loss": 1.6746,
      "step": 431800
    },
    {
      "epoch": 32.99213199908334,
      "grad_norm": 4.7388505935668945,
      "learning_rate": 2.250655666743055e-05,
      "loss": 1.7102,
      "step": 431900
    },
    {
      "epoch": 32.99977083492476,
      "grad_norm": 6.6096625328063965,
      "learning_rate": 2.2500190970896037e-05,
      "loss": 1.8321,
      "step": 432000
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.783823013305664,
      "eval_runtime": 1.4621,
      "eval_samples_per_second": 471.933,
      "eval_steps_per_second": 471.933,
      "step": 432003
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.4772506952285767,
      "eval_runtime": 27.6277,
      "eval_samples_per_second": 473.836,
      "eval_steps_per_second": 473.836,
      "step": 432003
    },
    {
      "epoch": 33.00740967076617,
      "grad_norm": 5.902515888214111,
      "learning_rate": 2.249382527436152e-05,
      "loss": 1.6623,
      "step": 432100
    },
    {
      "epoch": 33.01504850660759,
      "grad_norm": 5.8597517013549805,
      "learning_rate": 2.2487459577827007e-05,
      "loss": 1.7395,
      "step": 432200
    },
    {
      "epoch": 33.02268734244901,
      "grad_norm": 5.890353202819824,
      "learning_rate": 2.248109388129249e-05,
      "loss": 1.7937,
      "step": 432300
    },
    {
      "epoch": 33.03032617829043,
      "grad_norm": 4.553028583526611,
      "learning_rate": 2.2474728184757977e-05,
      "loss": 1.7274,
      "step": 432400
    },
    {
      "epoch": 33.03796501413185,
      "grad_norm": 5.635434150695801,
      "learning_rate": 2.2468362488223464e-05,
      "loss": 1.6913,
      "step": 432500
    },
    {
      "epoch": 33.04560384997326,
      "grad_norm": 5.7869744300842285,
      "learning_rate": 2.2461996791688948e-05,
      "loss": 1.7004,
      "step": 432600
    },
    {
      "epoch": 33.05324268581468,
      "grad_norm": 6.573660373687744,
      "learning_rate": 2.2455631095154435e-05,
      "loss": 1.6126,
      "step": 432700
    },
    {
      "epoch": 33.0608815216561,
      "grad_norm": 6.853799819946289,
      "learning_rate": 2.244926539861992e-05,
      "loss": 1.6907,
      "step": 432800
    },
    {
      "epoch": 33.06852035749752,
      "grad_norm": 5.360218048095703,
      "learning_rate": 2.2442899702085405e-05,
      "loss": 1.7123,
      "step": 432900
    },
    {
      "epoch": 33.07615919333894,
      "grad_norm": 6.236590385437012,
      "learning_rate": 2.243653400555089e-05,
      "loss": 1.7605,
      "step": 433000
    },
    {
      "epoch": 33.08379802918035,
      "grad_norm": 5.949195861816406,
      "learning_rate": 2.2430168309016376e-05,
      "loss": 1.7397,
      "step": 433100
    },
    {
      "epoch": 33.09143686502177,
      "grad_norm": 8.218396186828613,
      "learning_rate": 2.242380261248186e-05,
      "loss": 1.7457,
      "step": 433200
    },
    {
      "epoch": 33.09907570086319,
      "grad_norm": 4.2366414070129395,
      "learning_rate": 2.2417436915947343e-05,
      "loss": 1.65,
      "step": 433300
    },
    {
      "epoch": 33.10671453670461,
      "grad_norm": 5.7882866859436035,
      "learning_rate": 2.241107121941283e-05,
      "loss": 1.7619,
      "step": 433400
    },
    {
      "epoch": 33.11435337254603,
      "grad_norm": 5.838213920593262,
      "learning_rate": 2.2404705522878313e-05,
      "loss": 1.7643,
      "step": 433500
    },
    {
      "epoch": 33.12199220838744,
      "grad_norm": 5.514691352844238,
      "learning_rate": 2.23983398263438e-05,
      "loss": 1.6272,
      "step": 433600
    },
    {
      "epoch": 33.12963104422886,
      "grad_norm": 5.219743728637695,
      "learning_rate": 2.2391974129809284e-05,
      "loss": 1.7105,
      "step": 433700
    },
    {
      "epoch": 33.13726988007028,
      "grad_norm": 8.218018531799316,
      "learning_rate": 2.238560843327477e-05,
      "loss": 1.7347,
      "step": 433800
    },
    {
      "epoch": 33.1449087159117,
      "grad_norm": 4.9065093994140625,
      "learning_rate": 2.2379242736740254e-05,
      "loss": 1.7636,
      "step": 433900
    },
    {
      "epoch": 33.152547551753116,
      "grad_norm": 5.063995838165283,
      "learning_rate": 2.2372877040205738e-05,
      "loss": 1.6476,
      "step": 434000
    },
    {
      "epoch": 33.16018638759453,
      "grad_norm": 5.494420528411865,
      "learning_rate": 2.2366511343671225e-05,
      "loss": 1.6945,
      "step": 434100
    },
    {
      "epoch": 33.16782522343595,
      "grad_norm": 7.2837958335876465,
      "learning_rate": 2.2360145647136708e-05,
      "loss": 1.6823,
      "step": 434200
    },
    {
      "epoch": 33.17546405927737,
      "grad_norm": 6.243747711181641,
      "learning_rate": 2.2353779950602195e-05,
      "loss": 1.5779,
      "step": 434300
    },
    {
      "epoch": 33.183102895118786,
      "grad_norm": 5.19547700881958,
      "learning_rate": 2.2347414254067682e-05,
      "loss": 1.7408,
      "step": 434400
    },
    {
      "epoch": 33.1907417309602,
      "grad_norm": 4.953257083892822,
      "learning_rate": 2.234104855753317e-05,
      "loss": 1.7367,
      "step": 434500
    },
    {
      "epoch": 33.19838056680162,
      "grad_norm": 5.701154708862305,
      "learning_rate": 2.2334682860998653e-05,
      "loss": 1.6798,
      "step": 434600
    },
    {
      "epoch": 33.20601940264304,
      "grad_norm": 4.101428508758545,
      "learning_rate": 2.2328317164464136e-05,
      "loss": 1.691,
      "step": 434700
    },
    {
      "epoch": 33.213658238484456,
      "grad_norm": 6.867006778717041,
      "learning_rate": 2.2321951467929623e-05,
      "loss": 1.7735,
      "step": 434800
    },
    {
      "epoch": 33.221297074325875,
      "grad_norm": 5.058597564697266,
      "learning_rate": 2.2315585771395107e-05,
      "loss": 1.6954,
      "step": 434900
    },
    {
      "epoch": 33.22893591016729,
      "grad_norm": 4.966192245483398,
      "learning_rate": 2.2309220074860593e-05,
      "loss": 1.6964,
      "step": 435000
    },
    {
      "epoch": 33.236574746008706,
      "grad_norm": 5.831509113311768,
      "learning_rate": 2.2302854378326077e-05,
      "loss": 1.7662,
      "step": 435100
    },
    {
      "epoch": 33.244213581850126,
      "grad_norm": 4.732599258422852,
      "learning_rate": 2.2296488681791564e-05,
      "loss": 1.7266,
      "step": 435200
    },
    {
      "epoch": 33.251852417691545,
      "grad_norm": 5.806576728820801,
      "learning_rate": 2.2290122985257047e-05,
      "loss": 1.719,
      "step": 435300
    },
    {
      "epoch": 33.259491253532964,
      "grad_norm": 5.707846641540527,
      "learning_rate": 2.2283757288722534e-05,
      "loss": 1.66,
      "step": 435400
    },
    {
      "epoch": 33.267130089374376,
      "grad_norm": 7.239462375640869,
      "learning_rate": 2.2277391592188018e-05,
      "loss": 1.7537,
      "step": 435500
    },
    {
      "epoch": 33.274768925215795,
      "grad_norm": 6.266228199005127,
      "learning_rate": 2.22710258956535e-05,
      "loss": 1.7175,
      "step": 435600
    },
    {
      "epoch": 33.282407761057215,
      "grad_norm": 5.225872993469238,
      "learning_rate": 2.226466019911899e-05,
      "loss": 1.7493,
      "step": 435700
    },
    {
      "epoch": 33.290046596898634,
      "grad_norm": 8.656644821166992,
      "learning_rate": 2.2258294502584472e-05,
      "loss": 1.6607,
      "step": 435800
    },
    {
      "epoch": 33.29768543274005,
      "grad_norm": 6.019259452819824,
      "learning_rate": 2.225192880604996e-05,
      "loss": 1.8039,
      "step": 435900
    },
    {
      "epoch": 33.305324268581465,
      "grad_norm": 5.865582466125488,
      "learning_rate": 2.2245563109515442e-05,
      "loss": 1.8017,
      "step": 436000
    },
    {
      "epoch": 33.312963104422884,
      "grad_norm": 5.5920844078063965,
      "learning_rate": 2.223919741298093e-05,
      "loss": 1.7249,
      "step": 436100
    },
    {
      "epoch": 33.320601940264304,
      "grad_norm": 4.980959892272949,
      "learning_rate": 2.2232831716446413e-05,
      "loss": 1.6883,
      "step": 436200
    },
    {
      "epoch": 33.32824077610572,
      "grad_norm": 6.274118900299072,
      "learning_rate": 2.22264660199119e-05,
      "loss": 1.6801,
      "step": 436300
    },
    {
      "epoch": 33.33587961194714,
      "grad_norm": 4.073568344116211,
      "learning_rate": 2.2220100323377387e-05,
      "loss": 1.6221,
      "step": 436400
    },
    {
      "epoch": 33.343518447788554,
      "grad_norm": 5.694753646850586,
      "learning_rate": 2.221373462684287e-05,
      "loss": 1.6212,
      "step": 436500
    },
    {
      "epoch": 33.35115728362997,
      "grad_norm": 6.317846298217773,
      "learning_rate": 2.2207368930308357e-05,
      "loss": 1.7074,
      "step": 436600
    },
    {
      "epoch": 33.35879611947139,
      "grad_norm": 4.667160511016846,
      "learning_rate": 2.220100323377384e-05,
      "loss": 1.7551,
      "step": 436700
    },
    {
      "epoch": 33.36643495531281,
      "grad_norm": 5.650017738342285,
      "learning_rate": 2.2194637537239328e-05,
      "loss": 1.7093,
      "step": 436800
    },
    {
      "epoch": 33.37407379115423,
      "grad_norm": 5.2831830978393555,
      "learning_rate": 2.218827184070481e-05,
      "loss": 1.737,
      "step": 436900
    },
    {
      "epoch": 33.38171262699564,
      "grad_norm": 6.838406562805176,
      "learning_rate": 2.2181906144170298e-05,
      "loss": 1.6988,
      "step": 437000
    },
    {
      "epoch": 33.38935146283706,
      "grad_norm": 5.479259490966797,
      "learning_rate": 2.217554044763578e-05,
      "loss": 1.7135,
      "step": 437100
    },
    {
      "epoch": 33.39699029867848,
      "grad_norm": 4.811848163604736,
      "learning_rate": 2.2169174751101265e-05,
      "loss": 1.6221,
      "step": 437200
    },
    {
      "epoch": 33.4046291345199,
      "grad_norm": 6.711441516876221,
      "learning_rate": 2.2162809054566752e-05,
      "loss": 1.7918,
      "step": 437300
    },
    {
      "epoch": 33.41226797036132,
      "grad_norm": 4.827728271484375,
      "learning_rate": 2.2156443358032236e-05,
      "loss": 1.7394,
      "step": 437400
    },
    {
      "epoch": 33.41990680620273,
      "grad_norm": 6.67534875869751,
      "learning_rate": 2.2150077661497723e-05,
      "loss": 1.7619,
      "step": 437500
    },
    {
      "epoch": 33.42754564204415,
      "grad_norm": 5.049725532531738,
      "learning_rate": 2.2143711964963206e-05,
      "loss": 1.7293,
      "step": 437600
    },
    {
      "epoch": 33.43518447788557,
      "grad_norm": 5.57042121887207,
      "learning_rate": 2.2137346268428693e-05,
      "loss": 1.6255,
      "step": 437700
    },
    {
      "epoch": 33.44282331372699,
      "grad_norm": 4.745423793792725,
      "learning_rate": 2.2130980571894177e-05,
      "loss": 1.6784,
      "step": 437800
    },
    {
      "epoch": 33.45046214956841,
      "grad_norm": 5.297149658203125,
      "learning_rate": 2.212461487535966e-05,
      "loss": 1.7522,
      "step": 437900
    },
    {
      "epoch": 33.45810098540982,
      "grad_norm": 5.976206302642822,
      "learning_rate": 2.2118249178825147e-05,
      "loss": 1.7293,
      "step": 438000
    },
    {
      "epoch": 33.46573982125124,
      "grad_norm": 5.889464855194092,
      "learning_rate": 2.2111883482290634e-05,
      "loss": 1.6948,
      "step": 438100
    },
    {
      "epoch": 33.47337865709266,
      "grad_norm": 5.208151340484619,
      "learning_rate": 2.2105517785756117e-05,
      "loss": 1.6907,
      "step": 438200
    },
    {
      "epoch": 33.48101749293408,
      "grad_norm": 5.488498210906982,
      "learning_rate": 2.2099152089221604e-05,
      "loss": 1.5767,
      "step": 438300
    },
    {
      "epoch": 33.4886563287755,
      "grad_norm": 4.3902268409729,
      "learning_rate": 2.209278639268709e-05,
      "loss": 1.6757,
      "step": 438400
    },
    {
      "epoch": 33.49629516461691,
      "grad_norm": 4.980752944946289,
      "learning_rate": 2.2086420696152575e-05,
      "loss": 1.6657,
      "step": 438500
    },
    {
      "epoch": 33.50393400045833,
      "grad_norm": 5.074211597442627,
      "learning_rate": 2.2080054999618062e-05,
      "loss": 1.7862,
      "step": 438600
    },
    {
      "epoch": 33.51157283629975,
      "grad_norm": 4.9617018699646,
      "learning_rate": 2.2073689303083545e-05,
      "loss": 1.6899,
      "step": 438700
    },
    {
      "epoch": 33.51921167214117,
      "grad_norm": 7.468912601470947,
      "learning_rate": 2.206732360654903e-05,
      "loss": 1.7231,
      "step": 438800
    },
    {
      "epoch": 33.52685050798258,
      "grad_norm": 5.63866662979126,
      "learning_rate": 2.2060957910014516e-05,
      "loss": 1.7463,
      "step": 438900
    },
    {
      "epoch": 33.534489343824,
      "grad_norm": 5.0715556144714355,
      "learning_rate": 2.205459221348e-05,
      "loss": 1.7291,
      "step": 439000
    },
    {
      "epoch": 33.54212817966542,
      "grad_norm": 10.02480411529541,
      "learning_rate": 2.2048226516945486e-05,
      "loss": 1.6812,
      "step": 439100
    },
    {
      "epoch": 33.54976701550684,
      "grad_norm": 5.117819786071777,
      "learning_rate": 2.204186082041097e-05,
      "loss": 1.6334,
      "step": 439200
    },
    {
      "epoch": 33.55740585134826,
      "grad_norm": 6.23259162902832,
      "learning_rate": 2.2035495123876457e-05,
      "loss": 1.6705,
      "step": 439300
    },
    {
      "epoch": 33.56504468718967,
      "grad_norm": 5.193808078765869,
      "learning_rate": 2.202912942734194e-05,
      "loss": 1.7243,
      "step": 439400
    },
    {
      "epoch": 33.57268352303109,
      "grad_norm": 5.508538722991943,
      "learning_rate": 2.2022763730807424e-05,
      "loss": 1.7318,
      "step": 439500
    },
    {
      "epoch": 33.58032235887251,
      "grad_norm": 5.6516499519348145,
      "learning_rate": 2.201639803427291e-05,
      "loss": 1.7646,
      "step": 439600
    },
    {
      "epoch": 33.58796119471393,
      "grad_norm": 7.8683552742004395,
      "learning_rate": 2.2010032337738394e-05,
      "loss": 1.8318,
      "step": 439700
    },
    {
      "epoch": 33.595600030555346,
      "grad_norm": 6.731184959411621,
      "learning_rate": 2.200366664120388e-05,
      "loss": 1.6181,
      "step": 439800
    },
    {
      "epoch": 33.60323886639676,
      "grad_norm": 6.475037574768066,
      "learning_rate": 2.1997300944669365e-05,
      "loss": 1.7008,
      "step": 439900
    },
    {
      "epoch": 33.61087770223818,
      "grad_norm": 4.816226482391357,
      "learning_rate": 2.199093524813485e-05,
      "loss": 1.6744,
      "step": 440000
    },
    {
      "epoch": 33.6185165380796,
      "grad_norm": 4.286260604858398,
      "learning_rate": 2.198456955160034e-05,
      "loss": 1.6578,
      "step": 440100
    },
    {
      "epoch": 33.626155373921016,
      "grad_norm": 4.288950443267822,
      "learning_rate": 2.1978203855065822e-05,
      "loss": 1.7127,
      "step": 440200
    },
    {
      "epoch": 33.633794209762435,
      "grad_norm": 5.870294570922852,
      "learning_rate": 2.197183815853131e-05,
      "loss": 1.7462,
      "step": 440300
    },
    {
      "epoch": 33.64143304560385,
      "grad_norm": 5.793991565704346,
      "learning_rate": 2.1965472461996793e-05,
      "loss": 1.6773,
      "step": 440400
    },
    {
      "epoch": 33.649071881445266,
      "grad_norm": 5.120676040649414,
      "learning_rate": 2.195910676546228e-05,
      "loss": 1.6834,
      "step": 440500
    },
    {
      "epoch": 33.656710717286686,
      "grad_norm": 6.821471691131592,
      "learning_rate": 2.1952741068927763e-05,
      "loss": 1.7364,
      "step": 440600
    },
    {
      "epoch": 33.664349553128105,
      "grad_norm": 7.493962287902832,
      "learning_rate": 2.194637537239325e-05,
      "loss": 1.7843,
      "step": 440700
    },
    {
      "epoch": 33.671988388969524,
      "grad_norm": 6.936748027801514,
      "learning_rate": 2.1940009675858733e-05,
      "loss": 1.6917,
      "step": 440800
    },
    {
      "epoch": 33.679627224810936,
      "grad_norm": 5.05023717880249,
      "learning_rate": 2.193364397932422e-05,
      "loss": 1.7757,
      "step": 440900
    },
    {
      "epoch": 33.687266060652355,
      "grad_norm": 6.723036766052246,
      "learning_rate": 2.1927278282789704e-05,
      "loss": 1.7189,
      "step": 441000
    },
    {
      "epoch": 33.694904896493775,
      "grad_norm": 5.862705230712891,
      "learning_rate": 2.1920912586255187e-05,
      "loss": 1.662,
      "step": 441100
    },
    {
      "epoch": 33.702543732335194,
      "grad_norm": 6.801120758056641,
      "learning_rate": 2.1914546889720674e-05,
      "loss": 1.6649,
      "step": 441200
    },
    {
      "epoch": 33.71018256817661,
      "grad_norm": 4.759026527404785,
      "learning_rate": 2.1908181193186158e-05,
      "loss": 1.7061,
      "step": 441300
    },
    {
      "epoch": 33.717821404018025,
      "grad_norm": 6.949041366577148,
      "learning_rate": 2.1901815496651645e-05,
      "loss": 1.6384,
      "step": 441400
    },
    {
      "epoch": 33.725460239859444,
      "grad_norm": 6.371451377868652,
      "learning_rate": 2.189544980011713e-05,
      "loss": 1.6392,
      "step": 441500
    },
    {
      "epoch": 33.73309907570086,
      "grad_norm": 5.072495937347412,
      "learning_rate": 2.1889084103582615e-05,
      "loss": 1.7363,
      "step": 441600
    },
    {
      "epoch": 33.74073791154228,
      "grad_norm": 4.578860759735107,
      "learning_rate": 2.18827184070481e-05,
      "loss": 1.7291,
      "step": 441700
    },
    {
      "epoch": 33.7483767473837,
      "grad_norm": 6.618049144744873,
      "learning_rate": 2.1876352710513586e-05,
      "loss": 1.6774,
      "step": 441800
    },
    {
      "epoch": 33.756015583225114,
      "grad_norm": 5.755649089813232,
      "learning_rate": 2.186998701397907e-05,
      "loss": 1.7096,
      "step": 441900
    },
    {
      "epoch": 33.76365441906653,
      "grad_norm": 6.070591926574707,
      "learning_rate": 2.1863621317444556e-05,
      "loss": 1.7564,
      "step": 442000
    },
    {
      "epoch": 33.77129325490795,
      "grad_norm": 6.070008754730225,
      "learning_rate": 2.1857255620910043e-05,
      "loss": 1.7115,
      "step": 442100
    },
    {
      "epoch": 33.77893209074937,
      "grad_norm": 5.978919506072998,
      "learning_rate": 2.1850889924375527e-05,
      "loss": 1.8067,
      "step": 442200
    },
    {
      "epoch": 33.78657092659079,
      "grad_norm": 4.416228294372559,
      "learning_rate": 2.1844524227841014e-05,
      "loss": 1.7449,
      "step": 442300
    },
    {
      "epoch": 33.7942097624322,
      "grad_norm": 4.775999069213867,
      "learning_rate": 2.1838158531306497e-05,
      "loss": 1.7254,
      "step": 442400
    },
    {
      "epoch": 33.80184859827362,
      "grad_norm": 4.041687488555908,
      "learning_rate": 2.1831792834771984e-05,
      "loss": 1.6795,
      "step": 442500
    },
    {
      "epoch": 33.80948743411504,
      "grad_norm": 5.376269817352295,
      "learning_rate": 2.1825427138237468e-05,
      "loss": 1.6835,
      "step": 442600
    },
    {
      "epoch": 33.81712626995646,
      "grad_norm": 4.6792988777160645,
      "learning_rate": 2.181906144170295e-05,
      "loss": 1.776,
      "step": 442700
    },
    {
      "epoch": 33.82476510579788,
      "grad_norm": 4.03969144821167,
      "learning_rate": 2.1812695745168438e-05,
      "loss": 1.7251,
      "step": 442800
    },
    {
      "epoch": 33.83240394163929,
      "grad_norm": 5.375283718109131,
      "learning_rate": 2.180633004863392e-05,
      "loss": 1.7605,
      "step": 442900
    },
    {
      "epoch": 33.84004277748071,
      "grad_norm": 6.24618673324585,
      "learning_rate": 2.179996435209941e-05,
      "loss": 1.7862,
      "step": 443000
    },
    {
      "epoch": 33.84768161332213,
      "grad_norm": 4.720929145812988,
      "learning_rate": 2.1793598655564892e-05,
      "loss": 1.7852,
      "step": 443100
    },
    {
      "epoch": 33.85532044916355,
      "grad_norm": 5.620171546936035,
      "learning_rate": 2.178723295903038e-05,
      "loss": 1.7385,
      "step": 443200
    },
    {
      "epoch": 33.86295928500496,
      "grad_norm": 2.898169994354248,
      "learning_rate": 2.1780867262495863e-05,
      "loss": 1.6766,
      "step": 443300
    },
    {
      "epoch": 33.87059812084638,
      "grad_norm": 5.388227462768555,
      "learning_rate": 2.1774501565961346e-05,
      "loss": 1.702,
      "step": 443400
    },
    {
      "epoch": 33.8782369566878,
      "grad_norm": 4.528345108032227,
      "learning_rate": 2.1768135869426833e-05,
      "loss": 1.649,
      "step": 443500
    },
    {
      "epoch": 33.88587579252922,
      "grad_norm": 5.314126491546631,
      "learning_rate": 2.1761770172892317e-05,
      "loss": 1.7251,
      "step": 443600
    },
    {
      "epoch": 33.89351462837064,
      "grad_norm": 6.269608020782471,
      "learning_rate": 2.1755404476357803e-05,
      "loss": 1.7407,
      "step": 443700
    },
    {
      "epoch": 33.90115346421205,
      "grad_norm": 7.233375072479248,
      "learning_rate": 2.1749038779823287e-05,
      "loss": 1.6872,
      "step": 443800
    },
    {
      "epoch": 33.90879230005347,
      "grad_norm": 6.148917198181152,
      "learning_rate": 2.1742673083288774e-05,
      "loss": 1.7068,
      "step": 443900
    },
    {
      "epoch": 33.91643113589489,
      "grad_norm": 4.889853000640869,
      "learning_rate": 2.173630738675426e-05,
      "loss": 1.7404,
      "step": 444000
    },
    {
      "epoch": 33.92406997173631,
      "grad_norm": 6.439995765686035,
      "learning_rate": 2.1729941690219748e-05,
      "loss": 1.7793,
      "step": 444100
    },
    {
      "epoch": 33.93170880757773,
      "grad_norm": 6.4653401374816895,
      "learning_rate": 2.172357599368523e-05,
      "loss": 1.6257,
      "step": 444200
    },
    {
      "epoch": 33.93934764341914,
      "grad_norm": 4.7545647621154785,
      "learning_rate": 2.1717210297150715e-05,
      "loss": 1.7292,
      "step": 444300
    },
    {
      "epoch": 33.94698647926056,
      "grad_norm": 5.661289215087891,
      "learning_rate": 2.1710844600616202e-05,
      "loss": 1.7889,
      "step": 444400
    },
    {
      "epoch": 33.95462531510198,
      "grad_norm": 6.210344314575195,
      "learning_rate": 2.1704478904081685e-05,
      "loss": 1.725,
      "step": 444500
    },
    {
      "epoch": 33.9622641509434,
      "grad_norm": 5.0507402420043945,
      "learning_rate": 2.1698113207547172e-05,
      "loss": 1.7486,
      "step": 444600
    },
    {
      "epoch": 33.96990298678482,
      "grad_norm": 5.9450531005859375,
      "learning_rate": 2.1691747511012656e-05,
      "loss": 1.7358,
      "step": 444700
    },
    {
      "epoch": 33.97754182262623,
      "grad_norm": 5.839919090270996,
      "learning_rate": 2.1685381814478143e-05,
      "loss": 1.6063,
      "step": 444800
    },
    {
      "epoch": 33.98518065846765,
      "grad_norm": 6.8747453689575195,
      "learning_rate": 2.1679016117943626e-05,
      "loss": 1.7019,
      "step": 444900
    },
    {
      "epoch": 33.99281949430907,
      "grad_norm": 7.228786468505859,
      "learning_rate": 2.167265042140911e-05,
      "loss": 1.6235,
      "step": 445000
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.782986044883728,
      "eval_runtime": 3.3347,
      "eval_samples_per_second": 206.916,
      "eval_steps_per_second": 206.916,
      "step": 445094
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.4709192514419556,
      "eval_runtime": 60.7419,
      "eval_samples_per_second": 215.518,
      "eval_steps_per_second": 215.518,
      "step": 445094
    },
    {
      "epoch": 34.00045833015049,
      "grad_norm": 4.185883522033691,
      "learning_rate": 2.1666284724874597e-05,
      "loss": 1.6965,
      "step": 445100
    },
    {
      "epoch": 34.008097165991906,
      "grad_norm": 4.981090068817139,
      "learning_rate": 2.165991902834008e-05,
      "loss": 1.7118,
      "step": 445200
    },
    {
      "epoch": 34.01573600183332,
      "grad_norm": 4.993686199188232,
      "learning_rate": 2.1653553331805567e-05,
      "loss": 1.6951,
      "step": 445300
    },
    {
      "epoch": 34.02337483767474,
      "grad_norm": 6.007361888885498,
      "learning_rate": 2.164718763527105e-05,
      "loss": 1.6304,
      "step": 445400
    },
    {
      "epoch": 34.031013673516156,
      "grad_norm": 8.37541675567627,
      "learning_rate": 2.1640821938736538e-05,
      "loss": 1.7078,
      "step": 445500
    },
    {
      "epoch": 34.038652509357576,
      "grad_norm": 5.564915180206299,
      "learning_rate": 2.163445624220202e-05,
      "loss": 1.6098,
      "step": 445600
    },
    {
      "epoch": 34.046291345198995,
      "grad_norm": 5.4014811515808105,
      "learning_rate": 2.1628090545667508e-05,
      "loss": 1.7389,
      "step": 445700
    },
    {
      "epoch": 34.05393018104041,
      "grad_norm": 5.254875183105469,
      "learning_rate": 2.162172484913299e-05,
      "loss": 1.7743,
      "step": 445800
    },
    {
      "epoch": 34.061569016881826,
      "grad_norm": 5.674080848693848,
      "learning_rate": 2.161535915259848e-05,
      "loss": 1.6851,
      "step": 445900
    },
    {
      "epoch": 34.069207852723245,
      "grad_norm": 4.36374044418335,
      "learning_rate": 2.1608993456063965e-05,
      "loss": 1.6722,
      "step": 446000
    },
    {
      "epoch": 34.076846688564665,
      "grad_norm": 5.536933898925781,
      "learning_rate": 2.160262775952945e-05,
      "loss": 1.6414,
      "step": 446100
    },
    {
      "epoch": 34.084485524406084,
      "grad_norm": 5.137818336486816,
      "learning_rate": 2.1596262062994936e-05,
      "loss": 1.6331,
      "step": 446200
    },
    {
      "epoch": 34.092124360247496,
      "grad_norm": 5.3998308181762695,
      "learning_rate": 2.158989636646042e-05,
      "loss": 1.5917,
      "step": 446300
    },
    {
      "epoch": 34.099763196088915,
      "grad_norm": 5.867542266845703,
      "learning_rate": 2.1583530669925906e-05,
      "loss": 1.756,
      "step": 446400
    },
    {
      "epoch": 34.107402031930334,
      "grad_norm": 4.6315202713012695,
      "learning_rate": 2.157716497339139e-05,
      "loss": 1.6662,
      "step": 446500
    },
    {
      "epoch": 34.115040867771754,
      "grad_norm": 6.121766090393066,
      "learning_rate": 2.1570799276856873e-05,
      "loss": 1.6913,
      "step": 446600
    },
    {
      "epoch": 34.12267970361317,
      "grad_norm": 5.02741813659668,
      "learning_rate": 2.156443358032236e-05,
      "loss": 1.7613,
      "step": 446700
    },
    {
      "epoch": 34.130318539454585,
      "grad_norm": 6.002103328704834,
      "learning_rate": 2.1558067883787844e-05,
      "loss": 1.7419,
      "step": 446800
    },
    {
      "epoch": 34.137957375296004,
      "grad_norm": 4.645228385925293,
      "learning_rate": 2.155170218725333e-05,
      "loss": 1.6923,
      "step": 446900
    },
    {
      "epoch": 34.14559621113742,
      "grad_norm": 6.039500713348389,
      "learning_rate": 2.1545336490718814e-05,
      "loss": 1.7379,
      "step": 447000
    },
    {
      "epoch": 34.15323504697884,
      "grad_norm": 5.065227031707764,
      "learning_rate": 2.15389707941843e-05,
      "loss": 1.6624,
      "step": 447100
    },
    {
      "epoch": 34.160873882820255,
      "grad_norm": 5.496744632720947,
      "learning_rate": 2.1532605097649785e-05,
      "loss": 1.6554,
      "step": 447200
    },
    {
      "epoch": 34.168512718661674,
      "grad_norm": 6.428843975067139,
      "learning_rate": 2.1526239401115272e-05,
      "loss": 1.7369,
      "step": 447300
    },
    {
      "epoch": 34.17615155450309,
      "grad_norm": 5.013542175292969,
      "learning_rate": 2.1519873704580755e-05,
      "loss": 1.7427,
      "step": 447400
    },
    {
      "epoch": 34.18379039034451,
      "grad_norm": 5.989386558532715,
      "learning_rate": 2.151350800804624e-05,
      "loss": 1.7082,
      "step": 447500
    },
    {
      "epoch": 34.19142922618593,
      "grad_norm": 4.584922790527344,
      "learning_rate": 2.1507142311511726e-05,
      "loss": 1.7087,
      "step": 447600
    },
    {
      "epoch": 34.199068062027344,
      "grad_norm": 5.711907863616943,
      "learning_rate": 2.1500776614977213e-05,
      "loss": 1.6044,
      "step": 447700
    },
    {
      "epoch": 34.20670689786876,
      "grad_norm": 5.7585320472717285,
      "learning_rate": 2.1494410918442696e-05,
      "loss": 1.6374,
      "step": 447800
    },
    {
      "epoch": 34.21434573371018,
      "grad_norm": 5.076930999755859,
      "learning_rate": 2.1488045221908183e-05,
      "loss": 1.8183,
      "step": 447900
    },
    {
      "epoch": 34.2219845695516,
      "grad_norm": 6.1023268699646,
      "learning_rate": 2.148167952537367e-05,
      "loss": 1.6428,
      "step": 448000
    },
    {
      "epoch": 34.22962340539302,
      "grad_norm": 8.948603630065918,
      "learning_rate": 2.1475313828839154e-05,
      "loss": 1.67,
      "step": 448100
    },
    {
      "epoch": 34.23726224123443,
      "grad_norm": 6.41288948059082,
      "learning_rate": 2.1468948132304637e-05,
      "loss": 1.7185,
      "step": 448200
    },
    {
      "epoch": 34.24490107707585,
      "grad_norm": 4.557564735412598,
      "learning_rate": 2.1462582435770124e-05,
      "loss": 1.6177,
      "step": 448300
    },
    {
      "epoch": 34.25253991291727,
      "grad_norm": 6.270735263824463,
      "learning_rate": 2.1456216739235608e-05,
      "loss": 1.6827,
      "step": 448400
    },
    {
      "epoch": 34.26017874875869,
      "grad_norm": 6.4255452156066895,
      "learning_rate": 2.1449851042701095e-05,
      "loss": 1.8705,
      "step": 448500
    },
    {
      "epoch": 34.26781758460011,
      "grad_norm": 5.487451553344727,
      "learning_rate": 2.1443485346166578e-05,
      "loss": 1.7513,
      "step": 448600
    },
    {
      "epoch": 34.27545642044152,
      "grad_norm": 5.075536251068115,
      "learning_rate": 2.1437119649632065e-05,
      "loss": 1.7367,
      "step": 448700
    },
    {
      "epoch": 34.28309525628294,
      "grad_norm": 5.4220123291015625,
      "learning_rate": 2.143075395309755e-05,
      "loss": 1.6917,
      "step": 448800
    },
    {
      "epoch": 34.29073409212436,
      "grad_norm": 5.579606533050537,
      "learning_rate": 2.1424388256563035e-05,
      "loss": 1.6971,
      "step": 448900
    },
    {
      "epoch": 34.29837292796578,
      "grad_norm": 6.27368688583374,
      "learning_rate": 2.141802256002852e-05,
      "loss": 1.5841,
      "step": 449000
    },
    {
      "epoch": 34.3060117638072,
      "grad_norm": 4.631824970245361,
      "learning_rate": 2.1411656863494003e-05,
      "loss": 1.7589,
      "step": 449100
    },
    {
      "epoch": 34.31365059964861,
      "grad_norm": 5.2375311851501465,
      "learning_rate": 2.140529116695949e-05,
      "loss": 1.7366,
      "step": 449200
    },
    {
      "epoch": 34.32128943549003,
      "grad_norm": 6.884017467498779,
      "learning_rate": 2.1398925470424973e-05,
      "loss": 1.7275,
      "step": 449300
    },
    {
      "epoch": 34.32892827133145,
      "grad_norm": 4.408163547515869,
      "learning_rate": 2.139255977389046e-05,
      "loss": 1.8286,
      "step": 449400
    },
    {
      "epoch": 34.33656710717287,
      "grad_norm": 5.403750896453857,
      "learning_rate": 2.1386194077355943e-05,
      "loss": 1.7063,
      "step": 449500
    },
    {
      "epoch": 34.34420594301429,
      "grad_norm": 5.55841064453125,
      "learning_rate": 2.137982838082143e-05,
      "loss": 1.7014,
      "step": 449600
    },
    {
      "epoch": 34.3518447788557,
      "grad_norm": 6.934462547302246,
      "learning_rate": 2.1373462684286917e-05,
      "loss": 1.6334,
      "step": 449700
    },
    {
      "epoch": 34.35948361469712,
      "grad_norm": 5.757354259490967,
      "learning_rate": 2.13670969877524e-05,
      "loss": 1.7106,
      "step": 449800
    },
    {
      "epoch": 34.36712245053854,
      "grad_norm": 6.206175327301025,
      "learning_rate": 2.1360731291217888e-05,
      "loss": 1.6519,
      "step": 449900
    },
    {
      "epoch": 34.37476128637996,
      "grad_norm": 5.678430557250977,
      "learning_rate": 2.135436559468337e-05,
      "loss": 1.6841,
      "step": 450000
    },
    {
      "epoch": 34.38240012222138,
      "grad_norm": 6.125701427459717,
      "learning_rate": 2.1347999898148858e-05,
      "loss": 1.7747,
      "step": 450100
    },
    {
      "epoch": 34.39003895806279,
      "grad_norm": 5.4386305809021,
      "learning_rate": 2.1341634201614342e-05,
      "loss": 1.7142,
      "step": 450200
    },
    {
      "epoch": 34.39767779390421,
      "grad_norm": 6.51067590713501,
      "learning_rate": 2.133526850507983e-05,
      "loss": 1.7313,
      "step": 450300
    },
    {
      "epoch": 34.40531662974563,
      "grad_norm": 4.520613193511963,
      "learning_rate": 2.1328902808545312e-05,
      "loss": 1.7073,
      "step": 450400
    },
    {
      "epoch": 34.412955465587046,
      "grad_norm": 7.282374858856201,
      "learning_rate": 2.13225371120108e-05,
      "loss": 1.6789,
      "step": 450500
    },
    {
      "epoch": 34.420594301428466,
      "grad_norm": 6.713383674621582,
      "learning_rate": 2.1316171415476283e-05,
      "loss": 1.6767,
      "step": 450600
    },
    {
      "epoch": 34.42823313726988,
      "grad_norm": 6.119466781616211,
      "learning_rate": 2.1309805718941766e-05,
      "loss": 1.6501,
      "step": 450700
    },
    {
      "epoch": 34.4358719731113,
      "grad_norm": 6.3024582862854,
      "learning_rate": 2.1303440022407253e-05,
      "loss": 1.7084,
      "step": 450800
    },
    {
      "epoch": 34.443510808952716,
      "grad_norm": 6.058742523193359,
      "learning_rate": 2.1297074325872737e-05,
      "loss": 1.6772,
      "step": 450900
    },
    {
      "epoch": 34.451149644794135,
      "grad_norm": 5.422056198120117,
      "learning_rate": 2.1290708629338224e-05,
      "loss": 1.7371,
      "step": 451000
    },
    {
      "epoch": 34.458788480635555,
      "grad_norm": 4.802272319793701,
      "learning_rate": 2.1284342932803707e-05,
      "loss": 1.7224,
      "step": 451100
    },
    {
      "epoch": 34.46642731647697,
      "grad_norm": 5.2156500816345215,
      "learning_rate": 2.1277977236269194e-05,
      "loss": 1.6532,
      "step": 451200
    },
    {
      "epoch": 34.474066152318386,
      "grad_norm": 4.118031024932861,
      "learning_rate": 2.1271611539734678e-05,
      "loss": 1.7239,
      "step": 451300
    },
    {
      "epoch": 34.481704988159805,
      "grad_norm": 4.950110912322998,
      "learning_rate": 2.126524584320016e-05,
      "loss": 1.6543,
      "step": 451400
    },
    {
      "epoch": 34.489343824001224,
      "grad_norm": 5.1002302169799805,
      "learning_rate": 2.1258880146665648e-05,
      "loss": 1.7276,
      "step": 451500
    },
    {
      "epoch": 34.49698265984264,
      "grad_norm": 4.7229533195495605,
      "learning_rate": 2.1252514450131135e-05,
      "loss": 1.65,
      "step": 451600
    },
    {
      "epoch": 34.504621495684056,
      "grad_norm": 4.156162738800049,
      "learning_rate": 2.1246148753596622e-05,
      "loss": 1.6243,
      "step": 451700
    },
    {
      "epoch": 34.512260331525475,
      "grad_norm": 5.243303298950195,
      "learning_rate": 2.1239783057062105e-05,
      "loss": 1.7345,
      "step": 451800
    },
    {
      "epoch": 34.519899167366894,
      "grad_norm": 4.917239189147949,
      "learning_rate": 2.1233417360527592e-05,
      "loss": 1.7901,
      "step": 451900
    },
    {
      "epoch": 34.52753800320831,
      "grad_norm": 5.307596206665039,
      "learning_rate": 2.1227051663993076e-05,
      "loss": 1.7163,
      "step": 452000
    },
    {
      "epoch": 34.535176839049726,
      "grad_norm": 4.202643871307373,
      "learning_rate": 2.122068596745856e-05,
      "loss": 1.6848,
      "step": 452100
    },
    {
      "epoch": 34.542815674891145,
      "grad_norm": 5.317468166351318,
      "learning_rate": 2.1214320270924046e-05,
      "loss": 1.6913,
      "step": 452200
    },
    {
      "epoch": 34.550454510732564,
      "grad_norm": 5.643336296081543,
      "learning_rate": 2.120795457438953e-05,
      "loss": 1.6691,
      "step": 452300
    },
    {
      "epoch": 34.55809334657398,
      "grad_norm": 5.878880500793457,
      "learning_rate": 2.1201588877855017e-05,
      "loss": 1.7586,
      "step": 452400
    },
    {
      "epoch": 34.5657321824154,
      "grad_norm": 6.285255432128906,
      "learning_rate": 2.11952231813205e-05,
      "loss": 1.7353,
      "step": 452500
    },
    {
      "epoch": 34.573371018256815,
      "grad_norm": 5.5130228996276855,
      "learning_rate": 2.1188857484785987e-05,
      "loss": 1.6819,
      "step": 452600
    },
    {
      "epoch": 34.581009854098234,
      "grad_norm": 4.297138690948486,
      "learning_rate": 2.118249178825147e-05,
      "loss": 1.741,
      "step": 452700
    },
    {
      "epoch": 34.58864868993965,
      "grad_norm": 6.184596061706543,
      "learning_rate": 2.1176126091716958e-05,
      "loss": 1.7049,
      "step": 452800
    },
    {
      "epoch": 34.59628752578107,
      "grad_norm": 5.129471778869629,
      "learning_rate": 2.116976039518244e-05,
      "loss": 1.6981,
      "step": 452900
    },
    {
      "epoch": 34.60392636162249,
      "grad_norm": 6.424797058105469,
      "learning_rate": 2.1163394698647925e-05,
      "loss": 1.6871,
      "step": 453000
    },
    {
      "epoch": 34.611565197463904,
      "grad_norm": 5.1550469398498535,
      "learning_rate": 2.1157029002113412e-05,
      "loss": 1.7578,
      "step": 453100
    },
    {
      "epoch": 34.61920403330532,
      "grad_norm": 4.104244232177734,
      "learning_rate": 2.1150663305578895e-05,
      "loss": 1.6894,
      "step": 453200
    },
    {
      "epoch": 34.62684286914674,
      "grad_norm": 4.941099166870117,
      "learning_rate": 2.1144297609044382e-05,
      "loss": 1.6553,
      "step": 453300
    },
    {
      "epoch": 34.63448170498816,
      "grad_norm": 7.096569538116455,
      "learning_rate": 2.1137931912509866e-05,
      "loss": 1.7005,
      "step": 453400
    },
    {
      "epoch": 34.64212054082958,
      "grad_norm": 4.943208694458008,
      "learning_rate": 2.1131566215975353e-05,
      "loss": 1.7154,
      "step": 453500
    },
    {
      "epoch": 34.64975937667099,
      "grad_norm": 7.132774353027344,
      "learning_rate": 2.112520051944084e-05,
      "loss": 1.7048,
      "step": 453600
    },
    {
      "epoch": 34.65739821251241,
      "grad_norm": 6.5024237632751465,
      "learning_rate": 2.1118834822906323e-05,
      "loss": 1.7131,
      "step": 453700
    },
    {
      "epoch": 34.66503704835383,
      "grad_norm": 6.309192180633545,
      "learning_rate": 2.111246912637181e-05,
      "loss": 1.7217,
      "step": 453800
    },
    {
      "epoch": 34.67267588419525,
      "grad_norm": 4.172898292541504,
      "learning_rate": 2.1106103429837294e-05,
      "loss": 1.7394,
      "step": 453900
    },
    {
      "epoch": 34.68031472003667,
      "grad_norm": 4.416336536407471,
      "learning_rate": 2.109973773330278e-05,
      "loss": 1.6941,
      "step": 454000
    },
    {
      "epoch": 34.68795355587808,
      "grad_norm": 6.240994453430176,
      "learning_rate": 2.1093372036768264e-05,
      "loss": 1.7192,
      "step": 454100
    },
    {
      "epoch": 34.6955923917195,
      "grad_norm": 3.6525237560272217,
      "learning_rate": 2.108700634023375e-05,
      "loss": 1.7405,
      "step": 454200
    },
    {
      "epoch": 34.70323122756092,
      "grad_norm": 4.7653703689575195,
      "learning_rate": 2.1080640643699235e-05,
      "loss": 1.6839,
      "step": 454300
    },
    {
      "epoch": 34.71087006340234,
      "grad_norm": 6.481566429138184,
      "learning_rate": 2.107427494716472e-05,
      "loss": 1.811,
      "step": 454400
    },
    {
      "epoch": 34.71850889924376,
      "grad_norm": 7.890856742858887,
      "learning_rate": 2.1067909250630205e-05,
      "loss": 1.7423,
      "step": 454500
    },
    {
      "epoch": 34.72614773508517,
      "grad_norm": 6.166484355926514,
      "learning_rate": 2.106154355409569e-05,
      "loss": 1.7903,
      "step": 454600
    },
    {
      "epoch": 34.73378657092659,
      "grad_norm": 5.253898620605469,
      "learning_rate": 2.1055177857561175e-05,
      "loss": 1.6721,
      "step": 454700
    },
    {
      "epoch": 34.74142540676801,
      "grad_norm": 6.652660369873047,
      "learning_rate": 2.104881216102666e-05,
      "loss": 1.747,
      "step": 454800
    },
    {
      "epoch": 34.74906424260943,
      "grad_norm": 4.747810363769531,
      "learning_rate": 2.1042446464492146e-05,
      "loss": 1.7485,
      "step": 454900
    },
    {
      "epoch": 34.75670307845085,
      "grad_norm": 6.0704851150512695,
      "learning_rate": 2.103608076795763e-05,
      "loss": 1.5906,
      "step": 455000
    },
    {
      "epoch": 34.76434191429226,
      "grad_norm": 4.971625328063965,
      "learning_rate": 2.1029715071423116e-05,
      "loss": 1.7198,
      "step": 455100
    },
    {
      "epoch": 34.77198075013368,
      "grad_norm": 5.51571798324585,
      "learning_rate": 2.10233493748886e-05,
      "loss": 1.7,
      "step": 455200
    },
    {
      "epoch": 34.7796195859751,
      "grad_norm": 6.3391242027282715,
      "learning_rate": 2.1016983678354087e-05,
      "loss": 1.8553,
      "step": 455300
    },
    {
      "epoch": 34.78725842181652,
      "grad_norm": 5.154613494873047,
      "learning_rate": 2.101061798181957e-05,
      "loss": 1.787,
      "step": 455400
    },
    {
      "epoch": 34.79489725765794,
      "grad_norm": 6.129431247711182,
      "learning_rate": 2.1004252285285057e-05,
      "loss": 1.6564,
      "step": 455500
    },
    {
      "epoch": 34.80253609349935,
      "grad_norm": 4.891382694244385,
      "learning_rate": 2.0997886588750544e-05,
      "loss": 1.635,
      "step": 455600
    },
    {
      "epoch": 34.81017492934077,
      "grad_norm": 6.246921062469482,
      "learning_rate": 2.0991520892216028e-05,
      "loss": 1.7217,
      "step": 455700
    },
    {
      "epoch": 34.81781376518219,
      "grad_norm": 4.92236852645874,
      "learning_rate": 2.0985155195681515e-05,
      "loss": 1.718,
      "step": 455800
    },
    {
      "epoch": 34.825452601023606,
      "grad_norm": 5.844257354736328,
      "learning_rate": 2.0978789499146998e-05,
      "loss": 1.7746,
      "step": 455900
    },
    {
      "epoch": 34.83309143686502,
      "grad_norm": 4.346563339233398,
      "learning_rate": 2.0972423802612485e-05,
      "loss": 1.6972,
      "step": 456000
    },
    {
      "epoch": 34.84073027270644,
      "grad_norm": 5.810451984405518,
      "learning_rate": 2.096605810607797e-05,
      "loss": 1.6827,
      "step": 456100
    },
    {
      "epoch": 34.84836910854786,
      "grad_norm": 6.043671607971191,
      "learning_rate": 2.0959692409543452e-05,
      "loss": 1.8125,
      "step": 456200
    },
    {
      "epoch": 34.856007944389276,
      "grad_norm": 5.056591510772705,
      "learning_rate": 2.095332671300894e-05,
      "loss": 1.767,
      "step": 456300
    },
    {
      "epoch": 34.863646780230695,
      "grad_norm": 4.796302318572998,
      "learning_rate": 2.0946961016474423e-05,
      "loss": 1.7043,
      "step": 456400
    },
    {
      "epoch": 34.87128561607211,
      "grad_norm": 5.8267436027526855,
      "learning_rate": 2.094059531993991e-05,
      "loss": 1.7515,
      "step": 456500
    },
    {
      "epoch": 34.87892445191353,
      "grad_norm": 5.408376216888428,
      "learning_rate": 2.0934229623405393e-05,
      "loss": 1.6781,
      "step": 456600
    },
    {
      "epoch": 34.886563287754946,
      "grad_norm": 9.279913902282715,
      "learning_rate": 2.092786392687088e-05,
      "loss": 1.7084,
      "step": 456700
    },
    {
      "epoch": 34.894202123596365,
      "grad_norm": 3.252957820892334,
      "learning_rate": 2.0921498230336364e-05,
      "loss": 1.5875,
      "step": 456800
    },
    {
      "epoch": 34.901840959437784,
      "grad_norm": 5.486346244812012,
      "learning_rate": 2.0915132533801847e-05,
      "loss": 1.6186,
      "step": 456900
    },
    {
      "epoch": 34.909479795279196,
      "grad_norm": 5.906168460845947,
      "learning_rate": 2.0908766837267334e-05,
      "loss": 1.7534,
      "step": 457000
    },
    {
      "epoch": 34.917118631120616,
      "grad_norm": 5.670705795288086,
      "learning_rate": 2.0902401140732818e-05,
      "loss": 1.6775,
      "step": 457100
    },
    {
      "epoch": 34.924757466962035,
      "grad_norm": 5.825434684753418,
      "learning_rate": 2.0896035444198305e-05,
      "loss": 1.7132,
      "step": 457200
    },
    {
      "epoch": 34.932396302803454,
      "grad_norm": 5.53770112991333,
      "learning_rate": 2.088966974766379e-05,
      "loss": 1.7172,
      "step": 457300
    },
    {
      "epoch": 34.94003513864487,
      "grad_norm": 3.934295654296875,
      "learning_rate": 2.0883304051129275e-05,
      "loss": 1.6184,
      "step": 457400
    },
    {
      "epoch": 34.947673974486285,
      "grad_norm": 5.988593101501465,
      "learning_rate": 2.0876938354594762e-05,
      "loss": 1.804,
      "step": 457500
    },
    {
      "epoch": 34.955312810327705,
      "grad_norm": 7.746533393859863,
      "learning_rate": 2.087057265806025e-05,
      "loss": 1.6933,
      "step": 457600
    },
    {
      "epoch": 34.962951646169124,
      "grad_norm": 5.992278575897217,
      "learning_rate": 2.0864206961525732e-05,
      "loss": 1.7751,
      "step": 457700
    },
    {
      "epoch": 34.97059048201054,
      "grad_norm": 5.674112319946289,
      "learning_rate": 2.0857841264991216e-05,
      "loss": 1.7302,
      "step": 457800
    },
    {
      "epoch": 34.97822931785196,
      "grad_norm": 4.648573398590088,
      "learning_rate": 2.0851475568456703e-05,
      "loss": 1.7301,
      "step": 457900
    },
    {
      "epoch": 34.985868153693374,
      "grad_norm": 5.52748966217041,
      "learning_rate": 2.0845109871922186e-05,
      "loss": 1.6792,
      "step": 458000
    },
    {
      "epoch": 34.993506989534794,
      "grad_norm": 4.670000076293945,
      "learning_rate": 2.0838744175387673e-05,
      "loss": 1.7311,
      "step": 458100
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.7758913040161133,
      "eval_runtime": 2.9929,
      "eval_samples_per_second": 230.544,
      "eval_steps_per_second": 230.544,
      "step": 458185
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.4643934965133667,
      "eval_runtime": 57.0343,
      "eval_samples_per_second": 229.528,
      "eval_steps_per_second": 229.528,
      "step": 458185
    },
    {
      "epoch": 35.00114582537621,
      "grad_norm": 6.222235679626465,
      "learning_rate": 2.0832378478853157e-05,
      "loss": 1.72,
      "step": 458200
    },
    {
      "epoch": 35.00878466121763,
      "grad_norm": 5.846090316772461,
      "learning_rate": 2.0826012782318644e-05,
      "loss": 1.6954,
      "step": 458300
    },
    {
      "epoch": 35.01642349705905,
      "grad_norm": 4.713428497314453,
      "learning_rate": 2.0819647085784127e-05,
      "loss": 1.6351,
      "step": 458400
    },
    {
      "epoch": 35.02406233290046,
      "grad_norm": 5.540900707244873,
      "learning_rate": 2.081328138924961e-05,
      "loss": 1.6962,
      "step": 458500
    },
    {
      "epoch": 35.03170116874188,
      "grad_norm": 5.779457092285156,
      "learning_rate": 2.0806915692715098e-05,
      "loss": 1.6754,
      "step": 458600
    },
    {
      "epoch": 35.0393400045833,
      "grad_norm": 4.693145751953125,
      "learning_rate": 2.080054999618058e-05,
      "loss": 1.7332,
      "step": 458700
    },
    {
      "epoch": 35.04697884042472,
      "grad_norm": 5.224885940551758,
      "learning_rate": 2.0794184299646068e-05,
      "loss": 1.6752,
      "step": 458800
    },
    {
      "epoch": 35.05461767626614,
      "grad_norm": 6.081293106079102,
      "learning_rate": 2.0787818603111552e-05,
      "loss": 1.7731,
      "step": 458900
    },
    {
      "epoch": 35.06225651210755,
      "grad_norm": 4.486306190490723,
      "learning_rate": 2.078145290657704e-05,
      "loss": 1.8054,
      "step": 459000
    },
    {
      "epoch": 35.06989534794897,
      "grad_norm": 5.48904275894165,
      "learning_rate": 2.0775087210042522e-05,
      "loss": 1.6312,
      "step": 459100
    },
    {
      "epoch": 35.07753418379039,
      "grad_norm": 5.407763481140137,
      "learning_rate": 2.076872151350801e-05,
      "loss": 1.6336,
      "step": 459200
    },
    {
      "epoch": 35.08517301963181,
      "grad_norm": 7.331713676452637,
      "learning_rate": 2.0762355816973496e-05,
      "loss": 1.665,
      "step": 459300
    },
    {
      "epoch": 35.09281185547323,
      "grad_norm": 3.832163095474243,
      "learning_rate": 2.075599012043898e-05,
      "loss": 1.7057,
      "step": 459400
    },
    {
      "epoch": 35.10045069131464,
      "grad_norm": 6.125871658325195,
      "learning_rate": 2.0749624423904467e-05,
      "loss": 1.7145,
      "step": 459500
    },
    {
      "epoch": 35.10808952715606,
      "grad_norm": 5.8933796882629395,
      "learning_rate": 2.074325872736995e-05,
      "loss": 1.6361,
      "step": 459600
    },
    {
      "epoch": 35.11572836299748,
      "grad_norm": 4.755420207977295,
      "learning_rate": 2.0736893030835437e-05,
      "loss": 1.7485,
      "step": 459700
    },
    {
      "epoch": 35.1233671988389,
      "grad_norm": 5.085766792297363,
      "learning_rate": 2.073052733430092e-05,
      "loss": 1.6952,
      "step": 459800
    },
    {
      "epoch": 35.13100603468031,
      "grad_norm": 4.885275363922119,
      "learning_rate": 2.0724161637766407e-05,
      "loss": 1.732,
      "step": 459900
    },
    {
      "epoch": 35.13864487052173,
      "grad_norm": 6.04453706741333,
      "learning_rate": 2.071779594123189e-05,
      "loss": 1.6977,
      "step": 460000
    },
    {
      "epoch": 35.14628370636315,
      "grad_norm": 5.001723766326904,
      "learning_rate": 2.0711430244697375e-05,
      "loss": 1.6585,
      "step": 460100
    },
    {
      "epoch": 35.15392254220457,
      "grad_norm": 5.751364707946777,
      "learning_rate": 2.070506454816286e-05,
      "loss": 1.7184,
      "step": 460200
    },
    {
      "epoch": 35.16156137804599,
      "grad_norm": 6.058569431304932,
      "learning_rate": 2.0698698851628345e-05,
      "loss": 1.6545,
      "step": 460300
    },
    {
      "epoch": 35.1692002138874,
      "grad_norm": 5.865267753601074,
      "learning_rate": 2.0692333155093832e-05,
      "loss": 1.7026,
      "step": 460400
    },
    {
      "epoch": 35.17683904972882,
      "grad_norm": 5.901152610778809,
      "learning_rate": 2.0685967458559315e-05,
      "loss": 1.8091,
      "step": 460500
    },
    {
      "epoch": 35.18447788557024,
      "grad_norm": 4.593698024749756,
      "learning_rate": 2.0679601762024802e-05,
      "loss": 1.7897,
      "step": 460600
    },
    {
      "epoch": 35.19211672141166,
      "grad_norm": 6.689107418060303,
      "learning_rate": 2.0673236065490286e-05,
      "loss": 1.66,
      "step": 460700
    },
    {
      "epoch": 35.19975555725308,
      "grad_norm": 4.456325531005859,
      "learning_rate": 2.066687036895577e-05,
      "loss": 1.6067,
      "step": 460800
    },
    {
      "epoch": 35.20739439309449,
      "grad_norm": 3.9055073261260986,
      "learning_rate": 2.0660504672421256e-05,
      "loss": 1.6639,
      "step": 460900
    },
    {
      "epoch": 35.21503322893591,
      "grad_norm": 6.6533203125,
      "learning_rate": 2.065413897588674e-05,
      "loss": 1.7218,
      "step": 461000
    },
    {
      "epoch": 35.22267206477733,
      "grad_norm": 8.09080696105957,
      "learning_rate": 2.0647773279352227e-05,
      "loss": 1.7269,
      "step": 461100
    },
    {
      "epoch": 35.23031090061875,
      "grad_norm": 7.041169166564941,
      "learning_rate": 2.0641407582817714e-05,
      "loss": 1.6939,
      "step": 461200
    },
    {
      "epoch": 35.237949736460166,
      "grad_norm": 4.8997344970703125,
      "learning_rate": 2.0635041886283197e-05,
      "loss": 1.6912,
      "step": 461300
    },
    {
      "epoch": 35.24558857230158,
      "grad_norm": 3.334779739379883,
      "learning_rate": 2.0628676189748684e-05,
      "loss": 1.7589,
      "step": 461400
    },
    {
      "epoch": 35.253227408143,
      "grad_norm": 6.06494665145874,
      "learning_rate": 2.062231049321417e-05,
      "loss": 1.7452,
      "step": 461500
    },
    {
      "epoch": 35.26086624398442,
      "grad_norm": 4.76857328414917,
      "learning_rate": 2.0615944796679655e-05,
      "loss": 1.669,
      "step": 461600
    },
    {
      "epoch": 35.268505079825836,
      "grad_norm": 5.62275505065918,
      "learning_rate": 2.0609579100145138e-05,
      "loss": 1.6873,
      "step": 461700
    },
    {
      "epoch": 35.276143915667255,
      "grad_norm": 5.868768215179443,
      "learning_rate": 2.0603213403610625e-05,
      "loss": 1.7155,
      "step": 461800
    },
    {
      "epoch": 35.28378275150867,
      "grad_norm": 4.737376689910889,
      "learning_rate": 2.059684770707611e-05,
      "loss": 1.6074,
      "step": 461900
    },
    {
      "epoch": 35.29142158735009,
      "grad_norm": 6.430348873138428,
      "learning_rate": 2.0590482010541596e-05,
      "loss": 1.7126,
      "step": 462000
    },
    {
      "epoch": 35.299060423191506,
      "grad_norm": 4.979262828826904,
      "learning_rate": 2.058411631400708e-05,
      "loss": 1.724,
      "step": 462100
    },
    {
      "epoch": 35.306699259032925,
      "grad_norm": 5.018101215362549,
      "learning_rate": 2.0577750617472566e-05,
      "loss": 1.703,
      "step": 462200
    },
    {
      "epoch": 35.314338094874344,
      "grad_norm": 4.416450023651123,
      "learning_rate": 2.057138492093805e-05,
      "loss": 1.7108,
      "step": 462300
    },
    {
      "epoch": 35.321976930715756,
      "grad_norm": 4.543964862823486,
      "learning_rate": 2.0565019224403533e-05,
      "loss": 1.7074,
      "step": 462400
    },
    {
      "epoch": 35.329615766557176,
      "grad_norm": 7.370569229125977,
      "learning_rate": 2.055865352786902e-05,
      "loss": 1.769,
      "step": 462500
    },
    {
      "epoch": 35.337254602398595,
      "grad_norm": 4.6470489501953125,
      "learning_rate": 2.0552287831334504e-05,
      "loss": 1.7499,
      "step": 462600
    },
    {
      "epoch": 35.344893438240014,
      "grad_norm": 7.390648365020752,
      "learning_rate": 2.054592213479999e-05,
      "loss": 1.6903,
      "step": 462700
    },
    {
      "epoch": 35.35253227408143,
      "grad_norm": 5.840189456939697,
      "learning_rate": 2.0539556438265474e-05,
      "loss": 1.6641,
      "step": 462800
    },
    {
      "epoch": 35.360171109922845,
      "grad_norm": 5.074398517608643,
      "learning_rate": 2.053319074173096e-05,
      "loss": 1.7627,
      "step": 462900
    },
    {
      "epoch": 35.367809945764265,
      "grad_norm": 4.258008003234863,
      "learning_rate": 2.0526825045196445e-05,
      "loss": 1.7149,
      "step": 463000
    },
    {
      "epoch": 35.375448781605684,
      "grad_norm": 5.530575275421143,
      "learning_rate": 2.052045934866193e-05,
      "loss": 1.6587,
      "step": 463100
    },
    {
      "epoch": 35.3830876174471,
      "grad_norm": 4.6811442375183105,
      "learning_rate": 2.051409365212742e-05,
      "loss": 1.7007,
      "step": 463200
    },
    {
      "epoch": 35.39072645328852,
      "grad_norm": 5.757706642150879,
      "learning_rate": 2.0507727955592902e-05,
      "loss": 1.6782,
      "step": 463300
    },
    {
      "epoch": 35.398365289129934,
      "grad_norm": 6.068052768707275,
      "learning_rate": 2.050136225905839e-05,
      "loss": 1.6373,
      "step": 463400
    },
    {
      "epoch": 35.40600412497135,
      "grad_norm": 5.84447717666626,
      "learning_rate": 2.0494996562523872e-05,
      "loss": 1.6572,
      "step": 463500
    },
    {
      "epoch": 35.41364296081277,
      "grad_norm": 5.789104461669922,
      "learning_rate": 2.048863086598936e-05,
      "loss": 1.6751,
      "step": 463600
    },
    {
      "epoch": 35.42128179665419,
      "grad_norm": 6.476278781890869,
      "learning_rate": 2.0482265169454843e-05,
      "loss": 1.6987,
      "step": 463700
    },
    {
      "epoch": 35.42892063249561,
      "grad_norm": 6.408113956451416,
      "learning_rate": 2.047589947292033e-05,
      "loss": 1.6927,
      "step": 463800
    },
    {
      "epoch": 35.43655946833702,
      "grad_norm": 5.085468292236328,
      "learning_rate": 2.0469533776385813e-05,
      "loss": 1.649,
      "step": 463900
    },
    {
      "epoch": 35.44419830417844,
      "grad_norm": 5.083526134490967,
      "learning_rate": 2.0463168079851297e-05,
      "loss": 1.7153,
      "step": 464000
    },
    {
      "epoch": 35.45183714001986,
      "grad_norm": 5.481196403503418,
      "learning_rate": 2.0456802383316784e-05,
      "loss": 1.7449,
      "step": 464100
    },
    {
      "epoch": 35.45947597586128,
      "grad_norm": 5.413240909576416,
      "learning_rate": 2.0450436686782267e-05,
      "loss": 1.6985,
      "step": 464200
    },
    {
      "epoch": 35.46711481170269,
      "grad_norm": 5.65920877456665,
      "learning_rate": 2.0444070990247754e-05,
      "loss": 1.8299,
      "step": 464300
    },
    {
      "epoch": 35.47475364754411,
      "grad_norm": 5.535093307495117,
      "learning_rate": 2.0437705293713238e-05,
      "loss": 1.6556,
      "step": 464400
    },
    {
      "epoch": 35.48239248338553,
      "grad_norm": 4.5503668785095215,
      "learning_rate": 2.0431339597178725e-05,
      "loss": 1.7389,
      "step": 464500
    },
    {
      "epoch": 35.49003131922695,
      "grad_norm": 7.66887092590332,
      "learning_rate": 2.0424973900644208e-05,
      "loss": 1.6373,
      "step": 464600
    },
    {
      "epoch": 35.49767015506837,
      "grad_norm": 5.270827293395996,
      "learning_rate": 2.0418608204109695e-05,
      "loss": 1.6575,
      "step": 464700
    },
    {
      "epoch": 35.50530899090978,
      "grad_norm": 6.012408256530762,
      "learning_rate": 2.041224250757518e-05,
      "loss": 1.6728,
      "step": 464800
    },
    {
      "epoch": 35.5129478267512,
      "grad_norm": 6.3730244636535645,
      "learning_rate": 2.0405876811040666e-05,
      "loss": 1.6599,
      "step": 464900
    },
    {
      "epoch": 35.52058666259262,
      "grad_norm": 3.221466302871704,
      "learning_rate": 2.039951111450615e-05,
      "loss": 1.5813,
      "step": 465000
    },
    {
      "epoch": 35.52822549843404,
      "grad_norm": 6.97092866897583,
      "learning_rate": 2.0393145417971636e-05,
      "loss": 1.6943,
      "step": 465100
    },
    {
      "epoch": 35.53586433427546,
      "grad_norm": 9.07701587677002,
      "learning_rate": 2.0386779721437123e-05,
      "loss": 1.7497,
      "step": 465200
    },
    {
      "epoch": 35.54350317011687,
      "grad_norm": 4.121545314788818,
      "learning_rate": 2.0380414024902607e-05,
      "loss": 1.6909,
      "step": 465300
    },
    {
      "epoch": 35.55114200595829,
      "grad_norm": 6.984888076782227,
      "learning_rate": 2.0374048328368093e-05,
      "loss": 1.811,
      "step": 465400
    },
    {
      "epoch": 35.55878084179971,
      "grad_norm": 5.218887805938721,
      "learning_rate": 2.0367682631833577e-05,
      "loss": 1.7113,
      "step": 465500
    },
    {
      "epoch": 35.56641967764113,
      "grad_norm": 4.871539115905762,
      "learning_rate": 2.036131693529906e-05,
      "loss": 1.7255,
      "step": 465600
    },
    {
      "epoch": 35.57405851348255,
      "grad_norm": 5.6788201332092285,
      "learning_rate": 2.0354951238764547e-05,
      "loss": 1.6688,
      "step": 465700
    },
    {
      "epoch": 35.58169734932396,
      "grad_norm": 7.4369378089904785,
      "learning_rate": 2.034858554223003e-05,
      "loss": 1.7337,
      "step": 465800
    },
    {
      "epoch": 35.58933618516538,
      "grad_norm": 4.593442916870117,
      "learning_rate": 2.0342219845695518e-05,
      "loss": 1.5877,
      "step": 465900
    },
    {
      "epoch": 35.5969750210068,
      "grad_norm": 4.735991477966309,
      "learning_rate": 2.0335854149161e-05,
      "loss": 1.6727,
      "step": 466000
    },
    {
      "epoch": 35.60461385684822,
      "grad_norm": 5.013026714324951,
      "learning_rate": 2.032948845262649e-05,
      "loss": 1.7527,
      "step": 466100
    },
    {
      "epoch": 35.61225269268964,
      "grad_norm": 6.180505275726318,
      "learning_rate": 2.0323122756091972e-05,
      "loss": 1.7036,
      "step": 466200
    },
    {
      "epoch": 35.61989152853105,
      "grad_norm": 5.922394752502441,
      "learning_rate": 2.031675705955746e-05,
      "loss": 1.7556,
      "step": 466300
    },
    {
      "epoch": 35.62753036437247,
      "grad_norm": 7.233767509460449,
      "learning_rate": 2.0310391363022942e-05,
      "loss": 1.7611,
      "step": 466400
    },
    {
      "epoch": 35.63516920021389,
      "grad_norm": 4.942646503448486,
      "learning_rate": 2.0304025666488426e-05,
      "loss": 1.6692,
      "step": 466500
    },
    {
      "epoch": 35.64280803605531,
      "grad_norm": 6.366608619689941,
      "learning_rate": 2.0297659969953913e-05,
      "loss": 1.668,
      "step": 466600
    },
    {
      "epoch": 35.650446871896726,
      "grad_norm": 5.8911638259887695,
      "learning_rate": 2.0291294273419396e-05,
      "loss": 1.7413,
      "step": 466700
    },
    {
      "epoch": 35.65808570773814,
      "grad_norm": 6.076925754547119,
      "learning_rate": 2.0284928576884883e-05,
      "loss": 1.7315,
      "step": 466800
    },
    {
      "epoch": 35.66572454357956,
      "grad_norm": 8.247076988220215,
      "learning_rate": 2.027856288035037e-05,
      "loss": 1.7513,
      "step": 466900
    },
    {
      "epoch": 35.67336337942098,
      "grad_norm": 5.810815811157227,
      "learning_rate": 2.0272197183815854e-05,
      "loss": 1.7785,
      "step": 467000
    },
    {
      "epoch": 35.681002215262396,
      "grad_norm": 4.973836421966553,
      "learning_rate": 2.026583148728134e-05,
      "loss": 1.6253,
      "step": 467100
    },
    {
      "epoch": 35.688641051103815,
      "grad_norm": 3.488304615020752,
      "learning_rate": 2.0259465790746824e-05,
      "loss": 1.6842,
      "step": 467200
    },
    {
      "epoch": 35.69627988694523,
      "grad_norm": 6.6547064781188965,
      "learning_rate": 2.025310009421231e-05,
      "loss": 1.7366,
      "step": 467300
    },
    {
      "epoch": 35.703918722786646,
      "grad_norm": 5.711550712585449,
      "learning_rate": 2.0246734397677795e-05,
      "loss": 1.6714,
      "step": 467400
    },
    {
      "epoch": 35.711557558628066,
      "grad_norm": 5.7437334060668945,
      "learning_rate": 2.024036870114328e-05,
      "loss": 1.6646,
      "step": 467500
    },
    {
      "epoch": 35.719196394469485,
      "grad_norm": 6.953187942504883,
      "learning_rate": 2.0234003004608765e-05,
      "loss": 1.739,
      "step": 467600
    },
    {
      "epoch": 35.726835230310904,
      "grad_norm": 5.571831226348877,
      "learning_rate": 2.0227637308074252e-05,
      "loss": 1.6169,
      "step": 467700
    },
    {
      "epoch": 35.734474066152316,
      "grad_norm": 5.626943111419678,
      "learning_rate": 2.0221271611539736e-05,
      "loss": 1.798,
      "step": 467800
    },
    {
      "epoch": 35.742112901993735,
      "grad_norm": 5.637975215911865,
      "learning_rate": 2.021490591500522e-05,
      "loss": 1.6416,
      "step": 467900
    },
    {
      "epoch": 35.749751737835155,
      "grad_norm": 4.254384517669678,
      "learning_rate": 2.0208540218470706e-05,
      "loss": 1.7184,
      "step": 468000
    },
    {
      "epoch": 35.757390573676574,
      "grad_norm": 6.007549285888672,
      "learning_rate": 2.020217452193619e-05,
      "loss": 1.6504,
      "step": 468100
    },
    {
      "epoch": 35.76502940951799,
      "grad_norm": 5.254918575286865,
      "learning_rate": 2.0195808825401677e-05,
      "loss": 1.7682,
      "step": 468200
    },
    {
      "epoch": 35.772668245359405,
      "grad_norm": 4.9322357177734375,
      "learning_rate": 2.018944312886716e-05,
      "loss": 1.6878,
      "step": 468300
    },
    {
      "epoch": 35.780307081200824,
      "grad_norm": 7.929524898529053,
      "learning_rate": 2.0183077432332647e-05,
      "loss": 1.661,
      "step": 468400
    },
    {
      "epoch": 35.787945917042244,
      "grad_norm": 13.14462947845459,
      "learning_rate": 2.017671173579813e-05,
      "loss": 1.8192,
      "step": 468500
    },
    {
      "epoch": 35.79558475288366,
      "grad_norm": 4.804004669189453,
      "learning_rate": 2.0170346039263617e-05,
      "loss": 1.6422,
      "step": 468600
    },
    {
      "epoch": 35.803223588725075,
      "grad_norm": 3.9270219802856445,
      "learning_rate": 2.01639803427291e-05,
      "loss": 1.7285,
      "step": 468700
    },
    {
      "epoch": 35.810862424566494,
      "grad_norm": 4.701144695281982,
      "learning_rate": 2.0157614646194588e-05,
      "loss": 1.7047,
      "step": 468800
    },
    {
      "epoch": 35.81850126040791,
      "grad_norm": 6.824648380279541,
      "learning_rate": 2.015124894966007e-05,
      "loss": 1.693,
      "step": 468900
    },
    {
      "epoch": 35.82614009624933,
      "grad_norm": 4.561769008636475,
      "learning_rate": 2.014488325312556e-05,
      "loss": 1.7171,
      "step": 469000
    },
    {
      "epoch": 35.83377893209075,
      "grad_norm": 4.958868503570557,
      "learning_rate": 2.0138517556591045e-05,
      "loss": 1.7261,
      "step": 469100
    },
    {
      "epoch": 35.841417767932164,
      "grad_norm": 4.999289035797119,
      "learning_rate": 2.013215186005653e-05,
      "loss": 1.7013,
      "step": 469200
    },
    {
      "epoch": 35.84905660377358,
      "grad_norm": 5.885797023773193,
      "learning_rate": 2.0125786163522016e-05,
      "loss": 1.6269,
      "step": 469300
    },
    {
      "epoch": 35.856695439615,
      "grad_norm": 4.028043746948242,
      "learning_rate": 2.01194204669875e-05,
      "loss": 1.7022,
      "step": 469400
    },
    {
      "epoch": 35.86433427545642,
      "grad_norm": 4.007762432098389,
      "learning_rate": 2.0113054770452983e-05,
      "loss": 1.6684,
      "step": 469500
    },
    {
      "epoch": 35.87197311129784,
      "grad_norm": 6.720741271972656,
      "learning_rate": 2.010668907391847e-05,
      "loss": 1.7222,
      "step": 469600
    },
    {
      "epoch": 35.87961194713925,
      "grad_norm": 4.65997838973999,
      "learning_rate": 2.0100323377383953e-05,
      "loss": 1.6517,
      "step": 469700
    },
    {
      "epoch": 35.88725078298067,
      "grad_norm": 2.880066394805908,
      "learning_rate": 2.009395768084944e-05,
      "loss": 1.6314,
      "step": 469800
    },
    {
      "epoch": 35.89488961882209,
      "grad_norm": 5.356178283691406,
      "learning_rate": 2.0087591984314924e-05,
      "loss": 1.7604,
      "step": 469900
    },
    {
      "epoch": 35.90252845466351,
      "grad_norm": 6.244957447052002,
      "learning_rate": 2.008122628778041e-05,
      "loss": 1.7846,
      "step": 470000
    },
    {
      "epoch": 35.91016729050493,
      "grad_norm": 6.4066877365112305,
      "learning_rate": 2.0074860591245894e-05,
      "loss": 1.7815,
      "step": 470100
    },
    {
      "epoch": 35.91780612634634,
      "grad_norm": 4.49078893661499,
      "learning_rate": 2.006849489471138e-05,
      "loss": 1.7316,
      "step": 470200
    },
    {
      "epoch": 35.92544496218776,
      "grad_norm": 5.046594142913818,
      "learning_rate": 2.0062129198176865e-05,
      "loss": 1.8162,
      "step": 470300
    },
    {
      "epoch": 35.93308379802918,
      "grad_norm": 4.540280342102051,
      "learning_rate": 2.0055763501642348e-05,
      "loss": 1.7479,
      "step": 470400
    },
    {
      "epoch": 35.9407226338706,
      "grad_norm": 4.503177642822266,
      "learning_rate": 2.0049397805107835e-05,
      "loss": 1.7815,
      "step": 470500
    },
    {
      "epoch": 35.94836146971202,
      "grad_norm": 6.349766254425049,
      "learning_rate": 2.004303210857332e-05,
      "loss": 1.6747,
      "step": 470600
    },
    {
      "epoch": 35.95600030555343,
      "grad_norm": 6.423491477966309,
      "learning_rate": 2.0036666412038806e-05,
      "loss": 1.7262,
      "step": 470700
    },
    {
      "epoch": 35.96363914139485,
      "grad_norm": 5.344613552093506,
      "learning_rate": 2.0030300715504293e-05,
      "loss": 1.7069,
      "step": 470800
    },
    {
      "epoch": 35.97127797723627,
      "grad_norm": 4.627375602722168,
      "learning_rate": 2.0023935018969776e-05,
      "loss": 1.7603,
      "step": 470900
    },
    {
      "epoch": 35.97891681307769,
      "grad_norm": 6.167588233947754,
      "learning_rate": 2.0017569322435263e-05,
      "loss": 1.8019,
      "step": 471000
    },
    {
      "epoch": 35.98655564891911,
      "grad_norm": 4.8430047035217285,
      "learning_rate": 2.0011203625900747e-05,
      "loss": 1.6751,
      "step": 471100
    },
    {
      "epoch": 35.99419448476052,
      "grad_norm": 5.001241207122803,
      "learning_rate": 2.0004837929366233e-05,
      "loss": 1.6876,
      "step": 471200
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.7759758234024048,
      "eval_runtime": 2.9733,
      "eval_samples_per_second": 232.064,
      "eval_steps_per_second": 232.064,
      "step": 471276
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.4617048501968384,
      "eval_runtime": 57.5234,
      "eval_samples_per_second": 227.577,
      "eval_steps_per_second": 227.577,
      "step": 471276
    },
    {
      "epoch": 36.00183332060194,
      "grad_norm": 6.2826619148254395,
      "learning_rate": 1.9998472232831717e-05,
      "loss": 1.7108,
      "step": 471300
    },
    {
      "epoch": 36.00947215644336,
      "grad_norm": 6.0023722648620605,
      "learning_rate": 1.9992106536297204e-05,
      "loss": 1.6799,
      "step": 471400
    },
    {
      "epoch": 36.01711099228478,
      "grad_norm": 5.77388858795166,
      "learning_rate": 1.9985740839762687e-05,
      "loss": 1.6716,
      "step": 471500
    },
    {
      "epoch": 36.0247498281262,
      "grad_norm": 7.605691432952881,
      "learning_rate": 1.9979375143228174e-05,
      "loss": 1.6892,
      "step": 471600
    },
    {
      "epoch": 36.03238866396761,
      "grad_norm": 4.712551593780518,
      "learning_rate": 1.9973009446693658e-05,
      "loss": 1.6606,
      "step": 471700
    },
    {
      "epoch": 36.04002749980903,
      "grad_norm": 6.19886589050293,
      "learning_rate": 1.9966643750159145e-05,
      "loss": 1.699,
      "step": 471800
    },
    {
      "epoch": 36.04766633565045,
      "grad_norm": 5.201526641845703,
      "learning_rate": 1.996027805362463e-05,
      "loss": 1.7381,
      "step": 471900
    },
    {
      "epoch": 36.05530517149187,
      "grad_norm": 5.522347450256348,
      "learning_rate": 1.9953912357090112e-05,
      "loss": 1.6529,
      "step": 472000
    },
    {
      "epoch": 36.062944007333286,
      "grad_norm": 7.2187042236328125,
      "learning_rate": 1.99475466605556e-05,
      "loss": 1.6968,
      "step": 472100
    },
    {
      "epoch": 36.0705828431747,
      "grad_norm": 5.59382963180542,
      "learning_rate": 1.9941180964021082e-05,
      "loss": 1.6665,
      "step": 472200
    },
    {
      "epoch": 36.07822167901612,
      "grad_norm": 5.449637413024902,
      "learning_rate": 1.993481526748657e-05,
      "loss": 1.7407,
      "step": 472300
    },
    {
      "epoch": 36.085860514857536,
      "grad_norm": 4.901132583618164,
      "learning_rate": 1.9928449570952053e-05,
      "loss": 1.66,
      "step": 472400
    },
    {
      "epoch": 36.093499350698956,
      "grad_norm": 3.589771032333374,
      "learning_rate": 1.992208387441754e-05,
      "loss": 1.6369,
      "step": 472500
    },
    {
      "epoch": 36.10113818654037,
      "grad_norm": 6.425072193145752,
      "learning_rate": 1.9915718177883023e-05,
      "loss": 1.7317,
      "step": 472600
    },
    {
      "epoch": 36.10877702238179,
      "grad_norm": 5.651327133178711,
      "learning_rate": 1.990935248134851e-05,
      "loss": 1.681,
      "step": 472700
    },
    {
      "epoch": 36.116415858223206,
      "grad_norm": 7.202240467071533,
      "learning_rate": 1.9902986784813997e-05,
      "loss": 1.6098,
      "step": 472800
    },
    {
      "epoch": 36.124054694064625,
      "grad_norm": 6.917398452758789,
      "learning_rate": 1.989662108827948e-05,
      "loss": 1.6238,
      "step": 472900
    },
    {
      "epoch": 36.131693529906045,
      "grad_norm": 4.0460052490234375,
      "learning_rate": 1.9890255391744968e-05,
      "loss": 1.6604,
      "step": 473000
    },
    {
      "epoch": 36.13933236574746,
      "grad_norm": 6.178135871887207,
      "learning_rate": 1.988388969521045e-05,
      "loss": 1.6848,
      "step": 473100
    },
    {
      "epoch": 36.146971201588876,
      "grad_norm": 5.109909534454346,
      "learning_rate": 1.9877523998675938e-05,
      "loss": 1.7338,
      "step": 473200
    },
    {
      "epoch": 36.154610037430295,
      "grad_norm": 5.32128381729126,
      "learning_rate": 1.987115830214142e-05,
      "loss": 1.752,
      "step": 473300
    },
    {
      "epoch": 36.162248873271714,
      "grad_norm": 6.128170967102051,
      "learning_rate": 1.986479260560691e-05,
      "loss": 1.7868,
      "step": 473400
    },
    {
      "epoch": 36.169887709113134,
      "grad_norm": 5.376378059387207,
      "learning_rate": 1.9858426909072392e-05,
      "loss": 1.6548,
      "step": 473500
    },
    {
      "epoch": 36.177526544954546,
      "grad_norm": 6.591424465179443,
      "learning_rate": 1.9852061212537876e-05,
      "loss": 1.7464,
      "step": 473600
    },
    {
      "epoch": 36.185165380795965,
      "grad_norm": 5.483874797821045,
      "learning_rate": 1.9845695516003363e-05,
      "loss": 1.722,
      "step": 473700
    },
    {
      "epoch": 36.192804216637384,
      "grad_norm": 4.245369911193848,
      "learning_rate": 1.9839329819468846e-05,
      "loss": 1.6836,
      "step": 473800
    },
    {
      "epoch": 36.2004430524788,
      "grad_norm": 4.659516334533691,
      "learning_rate": 1.9832964122934333e-05,
      "loss": 1.6723,
      "step": 473900
    },
    {
      "epoch": 36.20808188832022,
      "grad_norm": 5.219612121582031,
      "learning_rate": 1.9826598426399817e-05,
      "loss": 1.6975,
      "step": 474000
    },
    {
      "epoch": 36.215720724161635,
      "grad_norm": 6.928557872772217,
      "learning_rate": 1.9820232729865303e-05,
      "loss": 1.6258,
      "step": 474100
    },
    {
      "epoch": 36.223359560003054,
      "grad_norm": 6.471262454986572,
      "learning_rate": 1.9813867033330787e-05,
      "loss": 1.6237,
      "step": 474200
    },
    {
      "epoch": 36.23099839584447,
      "grad_norm": 6.418649196624756,
      "learning_rate": 1.980750133679627e-05,
      "loss": 1.717,
      "step": 474300
    },
    {
      "epoch": 36.23863723168589,
      "grad_norm": 6.426332950592041,
      "learning_rate": 1.9801135640261757e-05,
      "loss": 1.6793,
      "step": 474400
    },
    {
      "epoch": 36.24627606752731,
      "grad_norm": 4.373970985412598,
      "learning_rate": 1.979476994372724e-05,
      "loss": 1.745,
      "step": 474500
    },
    {
      "epoch": 36.253914903368724,
      "grad_norm": 7.172796249389648,
      "learning_rate": 1.9788404247192728e-05,
      "loss": 1.7504,
      "step": 474600
    },
    {
      "epoch": 36.26155373921014,
      "grad_norm": 6.807942867279053,
      "learning_rate": 1.9782038550658215e-05,
      "loss": 1.7006,
      "step": 474700
    },
    {
      "epoch": 36.26919257505156,
      "grad_norm": 7.631201267242432,
      "learning_rate": 1.9775672854123702e-05,
      "loss": 1.709,
      "step": 474800
    },
    {
      "epoch": 36.27683141089298,
      "grad_norm": 4.695339679718018,
      "learning_rate": 1.9769307157589185e-05,
      "loss": 1.6888,
      "step": 474900
    },
    {
      "epoch": 36.2844702467344,
      "grad_norm": 4.994908332824707,
      "learning_rate": 1.9762941461054672e-05,
      "loss": 1.7828,
      "step": 475000
    },
    {
      "epoch": 36.29210908257581,
      "grad_norm": 7.948445796966553,
      "learning_rate": 1.9756575764520156e-05,
      "loss": 1.7592,
      "step": 475100
    },
    {
      "epoch": 36.29974791841723,
      "grad_norm": 5.109744548797607,
      "learning_rate": 1.975021006798564e-05,
      "loss": 1.6947,
      "step": 475200
    },
    {
      "epoch": 36.30738675425865,
      "grad_norm": 8.448732376098633,
      "learning_rate": 1.9743844371451126e-05,
      "loss": 1.7272,
      "step": 475300
    },
    {
      "epoch": 36.31502559010007,
      "grad_norm": 5.712568283081055,
      "learning_rate": 1.973747867491661e-05,
      "loss": 1.6759,
      "step": 475400
    },
    {
      "epoch": 36.32266442594149,
      "grad_norm": 5.241894721984863,
      "learning_rate": 1.9731112978382097e-05,
      "loss": 1.6326,
      "step": 475500
    },
    {
      "epoch": 36.3303032617829,
      "grad_norm": 6.213022708892822,
      "learning_rate": 1.972474728184758e-05,
      "loss": 1.661,
      "step": 475600
    },
    {
      "epoch": 36.33794209762432,
      "grad_norm": 3.8203022480010986,
      "learning_rate": 1.9718381585313067e-05,
      "loss": 1.6928,
      "step": 475700
    },
    {
      "epoch": 36.34558093346574,
      "grad_norm": 6.095459461212158,
      "learning_rate": 1.971201588877855e-05,
      "loss": 1.6849,
      "step": 475800
    },
    {
      "epoch": 36.35321976930716,
      "grad_norm": 6.006993293762207,
      "learning_rate": 1.9705650192244034e-05,
      "loss": 1.7128,
      "step": 475900
    },
    {
      "epoch": 36.36085860514858,
      "grad_norm": 6.465453624725342,
      "learning_rate": 1.969928449570952e-05,
      "loss": 1.7867,
      "step": 476000
    },
    {
      "epoch": 36.36849744098999,
      "grad_norm": 6.037656307220459,
      "learning_rate": 1.9692918799175005e-05,
      "loss": 1.7836,
      "step": 476100
    },
    {
      "epoch": 36.37613627683141,
      "grad_norm": 3.543069362640381,
      "learning_rate": 1.968655310264049e-05,
      "loss": 1.6485,
      "step": 476200
    },
    {
      "epoch": 36.38377511267283,
      "grad_norm": 6.095911026000977,
      "learning_rate": 1.9680187406105975e-05,
      "loss": 1.6554,
      "step": 476300
    },
    {
      "epoch": 36.39141394851425,
      "grad_norm": 5.04749059677124,
      "learning_rate": 1.9673821709571462e-05,
      "loss": 1.6037,
      "step": 476400
    },
    {
      "epoch": 36.39905278435567,
      "grad_norm": 8.301931381225586,
      "learning_rate": 1.9667456013036946e-05,
      "loss": 1.6105,
      "step": 476500
    },
    {
      "epoch": 36.40669162019708,
      "grad_norm": 6.918807029724121,
      "learning_rate": 1.9661090316502433e-05,
      "loss": 1.7099,
      "step": 476600
    },
    {
      "epoch": 36.4143304560385,
      "grad_norm": 5.254270076751709,
      "learning_rate": 1.965472461996792e-05,
      "loss": 1.6194,
      "step": 476700
    },
    {
      "epoch": 36.42196929187992,
      "grad_norm": 6.36359167098999,
      "learning_rate": 1.9648358923433403e-05,
      "loss": 1.6841,
      "step": 476800
    },
    {
      "epoch": 36.42960812772134,
      "grad_norm": 6.079885959625244,
      "learning_rate": 1.964199322689889e-05,
      "loss": 1.7988,
      "step": 476900
    },
    {
      "epoch": 36.43724696356275,
      "grad_norm": 4.870455265045166,
      "learning_rate": 1.9635627530364373e-05,
      "loss": 1.6596,
      "step": 477000
    },
    {
      "epoch": 36.44488579940417,
      "grad_norm": 5.7474164962768555,
      "learning_rate": 1.962926183382986e-05,
      "loss": 1.6494,
      "step": 477100
    },
    {
      "epoch": 36.45252463524559,
      "grad_norm": 5.503138065338135,
      "learning_rate": 1.9622896137295344e-05,
      "loss": 1.7143,
      "step": 477200
    },
    {
      "epoch": 36.46016347108701,
      "grad_norm": 6.0539631843566895,
      "learning_rate": 1.961653044076083e-05,
      "loss": 1.6827,
      "step": 477300
    },
    {
      "epoch": 36.46780230692843,
      "grad_norm": 5.705803871154785,
      "learning_rate": 1.9610164744226314e-05,
      "loss": 1.6881,
      "step": 477400
    },
    {
      "epoch": 36.47544114276984,
      "grad_norm": 5.141717433929443,
      "learning_rate": 1.9603799047691798e-05,
      "loss": 1.6176,
      "step": 477500
    },
    {
      "epoch": 36.48307997861126,
      "grad_norm": 6.275976181030273,
      "learning_rate": 1.9597433351157285e-05,
      "loss": 1.7237,
      "step": 477600
    },
    {
      "epoch": 36.49071881445268,
      "grad_norm": 5.712996959686279,
      "learning_rate": 1.959106765462277e-05,
      "loss": 1.61,
      "step": 477700
    },
    {
      "epoch": 36.498357650294096,
      "grad_norm": 5.363781929016113,
      "learning_rate": 1.9584701958088255e-05,
      "loss": 1.6556,
      "step": 477800
    },
    {
      "epoch": 36.505996486135516,
      "grad_norm": 5.083850860595703,
      "learning_rate": 1.957833626155374e-05,
      "loss": 1.7185,
      "step": 477900
    },
    {
      "epoch": 36.51363532197693,
      "grad_norm": 5.387607097625732,
      "learning_rate": 1.9571970565019226e-05,
      "loss": 1.7821,
      "step": 478000
    },
    {
      "epoch": 36.52127415781835,
      "grad_norm": 7.485880374908447,
      "learning_rate": 1.956560486848471e-05,
      "loss": 1.6346,
      "step": 478100
    },
    {
      "epoch": 36.528912993659766,
      "grad_norm": 6.097114562988281,
      "learning_rate": 1.9559239171950193e-05,
      "loss": 1.6511,
      "step": 478200
    },
    {
      "epoch": 36.536551829501185,
      "grad_norm": 5.068452835083008,
      "learning_rate": 1.955287347541568e-05,
      "loss": 1.7806,
      "step": 478300
    },
    {
      "epoch": 36.544190665342605,
      "grad_norm": 10.048529624938965,
      "learning_rate": 1.9546507778881167e-05,
      "loss": 1.7309,
      "step": 478400
    },
    {
      "epoch": 36.55182950118402,
      "grad_norm": 4.025397777557373,
      "learning_rate": 1.954014208234665e-05,
      "loss": 1.7202,
      "step": 478500
    },
    {
      "epoch": 36.559468337025436,
      "grad_norm": 6.17783784866333,
      "learning_rate": 1.9533776385812137e-05,
      "loss": 1.7017,
      "step": 478600
    },
    {
      "epoch": 36.567107172866855,
      "grad_norm": 4.37643575668335,
      "learning_rate": 1.9527410689277624e-05,
      "loss": 1.6667,
      "step": 478700
    },
    {
      "epoch": 36.574746008708274,
      "grad_norm": 4.964786529541016,
      "learning_rate": 1.9521044992743108e-05,
      "loss": 1.7073,
      "step": 478800
    },
    {
      "epoch": 36.58238484454969,
      "grad_norm": 4.104438304901123,
      "learning_rate": 1.9514679296208595e-05,
      "loss": 1.7297,
      "step": 478900
    },
    {
      "epoch": 36.590023680391106,
      "grad_norm": 4.750208854675293,
      "learning_rate": 1.9508313599674078e-05,
      "loss": 1.7033,
      "step": 479000
    },
    {
      "epoch": 36.597662516232525,
      "grad_norm": 6.0886735916137695,
      "learning_rate": 1.950194790313956e-05,
      "loss": 1.645,
      "step": 479100
    },
    {
      "epoch": 36.605301352073944,
      "grad_norm": 5.767865180969238,
      "learning_rate": 1.949558220660505e-05,
      "loss": 1.6856,
      "step": 479200
    },
    {
      "epoch": 36.61294018791536,
      "grad_norm": 5.134254455566406,
      "learning_rate": 1.9489216510070532e-05,
      "loss": 1.6741,
      "step": 479300
    },
    {
      "epoch": 36.62057902375678,
      "grad_norm": 5.8097243309021,
      "learning_rate": 1.948285081353602e-05,
      "loss": 1.7194,
      "step": 479400
    },
    {
      "epoch": 36.628217859598195,
      "grad_norm": 5.376064300537109,
      "learning_rate": 1.9476485117001503e-05,
      "loss": 1.78,
      "step": 479500
    },
    {
      "epoch": 36.635856695439614,
      "grad_norm": 5.105269908905029,
      "learning_rate": 1.947011942046699e-05,
      "loss": 1.7248,
      "step": 479600
    },
    {
      "epoch": 36.64349553128103,
      "grad_norm": 7.053208827972412,
      "learning_rate": 1.9463753723932473e-05,
      "loss": 1.7725,
      "step": 479700
    },
    {
      "epoch": 36.65113436712245,
      "grad_norm": 7.749186992645264,
      "learning_rate": 1.9457388027397957e-05,
      "loss": 1.8431,
      "step": 479800
    },
    {
      "epoch": 36.65877320296387,
      "grad_norm": 6.303428649902344,
      "learning_rate": 1.9451022330863443e-05,
      "loss": 1.7136,
      "step": 479900
    },
    {
      "epoch": 36.666412038805284,
      "grad_norm": 5.742936134338379,
      "learning_rate": 1.9444656634328927e-05,
      "loss": 1.6519,
      "step": 480000
    },
    {
      "epoch": 36.6740508746467,
      "grad_norm": 5.543557167053223,
      "learning_rate": 1.9438290937794414e-05,
      "loss": 1.6842,
      "step": 480100
    },
    {
      "epoch": 36.68168971048812,
      "grad_norm": 4.291703224182129,
      "learning_rate": 1.9431925241259897e-05,
      "loss": 1.7338,
      "step": 480200
    },
    {
      "epoch": 36.68932854632954,
      "grad_norm": 4.705350875854492,
      "learning_rate": 1.9425559544725384e-05,
      "loss": 1.7466,
      "step": 480300
    },
    {
      "epoch": 36.69696738217096,
      "grad_norm": 6.0400776863098145,
      "learning_rate": 1.941919384819087e-05,
      "loss": 1.7238,
      "step": 480400
    },
    {
      "epoch": 36.70460621801237,
      "grad_norm": 7.291630744934082,
      "learning_rate": 1.9412828151656355e-05,
      "loss": 1.7577,
      "step": 480500
    },
    {
      "epoch": 36.71224505385379,
      "grad_norm": 5.265016078948975,
      "learning_rate": 1.9406462455121842e-05,
      "loss": 1.6915,
      "step": 480600
    },
    {
      "epoch": 36.71988388969521,
      "grad_norm": 6.145546913146973,
      "learning_rate": 1.9400096758587325e-05,
      "loss": 1.7001,
      "step": 480700
    },
    {
      "epoch": 36.72752272553663,
      "grad_norm": 7.279637813568115,
      "learning_rate": 1.9393731062052812e-05,
      "loss": 1.8012,
      "step": 480800
    },
    {
      "epoch": 36.73516156137805,
      "grad_norm": 6.411364555358887,
      "learning_rate": 1.9387365365518296e-05,
      "loss": 1.6478,
      "step": 480900
    },
    {
      "epoch": 36.74280039721946,
      "grad_norm": 4.903784275054932,
      "learning_rate": 1.9380999668983783e-05,
      "loss": 1.7775,
      "step": 481000
    },
    {
      "epoch": 36.75043923306088,
      "grad_norm": 3.8805315494537354,
      "learning_rate": 1.9374633972449266e-05,
      "loss": 1.6513,
      "step": 481100
    },
    {
      "epoch": 36.7580780689023,
      "grad_norm": 5.137089729309082,
      "learning_rate": 1.9368268275914753e-05,
      "loss": 1.655,
      "step": 481200
    },
    {
      "epoch": 36.76571690474372,
      "grad_norm": 5.7751078605651855,
      "learning_rate": 1.9361902579380237e-05,
      "loss": 1.7299,
      "step": 481300
    },
    {
      "epoch": 36.77335574058513,
      "grad_norm": 4.426506996154785,
      "learning_rate": 1.935553688284572e-05,
      "loss": 1.7065,
      "step": 481400
    },
    {
      "epoch": 36.78099457642655,
      "grad_norm": 4.912946701049805,
      "learning_rate": 1.9349171186311207e-05,
      "loss": 1.6592,
      "step": 481500
    },
    {
      "epoch": 36.78863341226797,
      "grad_norm": 4.773153305053711,
      "learning_rate": 1.934280548977669e-05,
      "loss": 1.6962,
      "step": 481600
    },
    {
      "epoch": 36.79627224810939,
      "grad_norm": 6.0512776374816895,
      "learning_rate": 1.9336439793242178e-05,
      "loss": 1.7681,
      "step": 481700
    },
    {
      "epoch": 36.80391108395081,
      "grad_norm": 6.487823009490967,
      "learning_rate": 1.933007409670766e-05,
      "loss": 1.7331,
      "step": 481800
    },
    {
      "epoch": 36.81154991979222,
      "grad_norm": 6.232152462005615,
      "learning_rate": 1.9323708400173148e-05,
      "loss": 1.6711,
      "step": 481900
    },
    {
      "epoch": 36.81918875563364,
      "grad_norm": 6.397688865661621,
      "learning_rate": 1.931734270363863e-05,
      "loss": 1.6944,
      "step": 482000
    },
    {
      "epoch": 36.82682759147506,
      "grad_norm": 4.74492883682251,
      "learning_rate": 1.931097700710412e-05,
      "loss": 1.6655,
      "step": 482100
    },
    {
      "epoch": 36.83446642731648,
      "grad_norm": 8.190716743469238,
      "learning_rate": 1.9304611310569602e-05,
      "loss": 1.6387,
      "step": 482200
    },
    {
      "epoch": 36.8421052631579,
      "grad_norm": 4.662082195281982,
      "learning_rate": 1.929824561403509e-05,
      "loss": 1.6844,
      "step": 482300
    },
    {
      "epoch": 36.84974409899931,
      "grad_norm": 5.705284118652344,
      "learning_rate": 1.9291879917500576e-05,
      "loss": 1.6186,
      "step": 482400
    },
    {
      "epoch": 36.85738293484073,
      "grad_norm": 4.329461574554443,
      "learning_rate": 1.928551422096606e-05,
      "loss": 1.7772,
      "step": 482500
    },
    {
      "epoch": 36.86502177068215,
      "grad_norm": 4.831543922424316,
      "learning_rate": 1.9279148524431546e-05,
      "loss": 1.6689,
      "step": 482600
    },
    {
      "epoch": 36.87266060652357,
      "grad_norm": 7.2449750900268555,
      "learning_rate": 1.927278282789703e-05,
      "loss": 1.7046,
      "step": 482700
    },
    {
      "epoch": 36.880299442364986,
      "grad_norm": 4.690394878387451,
      "learning_rate": 1.9266417131362517e-05,
      "loss": 1.6968,
      "step": 482800
    },
    {
      "epoch": 36.8879382782064,
      "grad_norm": 4.909374237060547,
      "learning_rate": 1.9260051434828e-05,
      "loss": 1.6664,
      "step": 482900
    },
    {
      "epoch": 36.89557711404782,
      "grad_norm": 6.5541205406188965,
      "learning_rate": 1.9253685738293484e-05,
      "loss": 1.6385,
      "step": 483000
    },
    {
      "epoch": 36.90321594988924,
      "grad_norm": 4.882598876953125,
      "learning_rate": 1.924732004175897e-05,
      "loss": 1.7676,
      "step": 483100
    },
    {
      "epoch": 36.910854785730656,
      "grad_norm": 6.628563404083252,
      "learning_rate": 1.9240954345224454e-05,
      "loss": 1.6749,
      "step": 483200
    },
    {
      "epoch": 36.918493621572075,
      "grad_norm": 4.221138954162598,
      "learning_rate": 1.923458864868994e-05,
      "loss": 1.6329,
      "step": 483300
    },
    {
      "epoch": 36.92613245741349,
      "grad_norm": 6.281845569610596,
      "learning_rate": 1.9228222952155425e-05,
      "loss": 1.783,
      "step": 483400
    },
    {
      "epoch": 36.93377129325491,
      "grad_norm": 7.476413726806641,
      "learning_rate": 1.9221857255620912e-05,
      "loss": 1.7182,
      "step": 483500
    },
    {
      "epoch": 36.941410129096326,
      "grad_norm": 6.4658708572387695,
      "learning_rate": 1.9215491559086395e-05,
      "loss": 1.6874,
      "step": 483600
    },
    {
      "epoch": 36.949048964937745,
      "grad_norm": 7.329553604125977,
      "learning_rate": 1.9209125862551882e-05,
      "loss": 1.7599,
      "step": 483700
    },
    {
      "epoch": 36.956687800779164,
      "grad_norm": 5.346794605255127,
      "learning_rate": 1.9202760166017366e-05,
      "loss": 1.6858,
      "step": 483800
    },
    {
      "epoch": 36.96432663662058,
      "grad_norm": 6.809916019439697,
      "learning_rate": 1.919639446948285e-05,
      "loss": 1.7789,
      "step": 483900
    },
    {
      "epoch": 36.971965472461996,
      "grad_norm": 3.1734695434570312,
      "learning_rate": 1.9190028772948336e-05,
      "loss": 1.7132,
      "step": 484000
    },
    {
      "epoch": 36.979604308303415,
      "grad_norm": 5.1899800300598145,
      "learning_rate": 1.918366307641382e-05,
      "loss": 1.7096,
      "step": 484100
    },
    {
      "epoch": 36.987243144144834,
      "grad_norm": 7.416684150695801,
      "learning_rate": 1.9177297379879307e-05,
      "loss": 1.6214,
      "step": 484200
    },
    {
      "epoch": 36.99488197998625,
      "grad_norm": 5.663403034210205,
      "learning_rate": 1.9170931683344794e-05,
      "loss": 1.8197,
      "step": 484300
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.7727912664413452,
      "eval_runtime": 2.9952,
      "eval_samples_per_second": 230.372,
      "eval_steps_per_second": 230.372,
      "step": 484367
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.4576208591461182,
      "eval_runtime": 57.2496,
      "eval_samples_per_second": 228.665,
      "eval_steps_per_second": 228.665,
      "step": 484367
    },
    {
      "epoch": 37.002520815827666,
      "grad_norm": 6.231780529022217,
      "learning_rate": 1.916456598681028e-05,
      "loss": 1.7556,
      "step": 484400
    },
    {
      "epoch": 37.010159651669085,
      "grad_norm": 5.262040615081787,
      "learning_rate": 1.9158200290275764e-05,
      "loss": 1.7117,
      "step": 484500
    },
    {
      "epoch": 37.017798487510504,
      "grad_norm": 5.780648231506348,
      "learning_rate": 1.9151834593741248e-05,
      "loss": 1.7539,
      "step": 484600
    },
    {
      "epoch": 37.02543732335192,
      "grad_norm": 5.301315784454346,
      "learning_rate": 1.9145468897206735e-05,
      "loss": 1.7119,
      "step": 484700
    },
    {
      "epoch": 37.03307615919334,
      "grad_norm": 5.623338222503662,
      "learning_rate": 1.9139103200672218e-05,
      "loss": 1.6898,
      "step": 484800
    },
    {
      "epoch": 37.040714995034755,
      "grad_norm": 5.4913554191589355,
      "learning_rate": 1.9132737504137705e-05,
      "loss": 1.7781,
      "step": 484900
    },
    {
      "epoch": 37.048353830876174,
      "grad_norm": 3.6072521209716797,
      "learning_rate": 1.912637180760319e-05,
      "loss": 1.6624,
      "step": 485000
    },
    {
      "epoch": 37.05599266671759,
      "grad_norm": 4.708780765533447,
      "learning_rate": 1.9120006111068675e-05,
      "loss": 1.6946,
      "step": 485100
    },
    {
      "epoch": 37.06363150255901,
      "grad_norm": 6.102852821350098,
      "learning_rate": 1.911364041453416e-05,
      "loss": 1.7596,
      "step": 485200
    },
    {
      "epoch": 37.071270338400424,
      "grad_norm": 5.930429935455322,
      "learning_rate": 1.9107274717999643e-05,
      "loss": 1.7169,
      "step": 485300
    },
    {
      "epoch": 37.07890917424184,
      "grad_norm": 5.176983833312988,
      "learning_rate": 1.910090902146513e-05,
      "loss": 1.6636,
      "step": 485400
    },
    {
      "epoch": 37.08654801008326,
      "grad_norm": 4.924861431121826,
      "learning_rate": 1.9094543324930613e-05,
      "loss": 1.7432,
      "step": 485500
    },
    {
      "epoch": 37.09418684592468,
      "grad_norm": 6.004842758178711,
      "learning_rate": 1.90881776283961e-05,
      "loss": 1.6818,
      "step": 485600
    },
    {
      "epoch": 37.1018256817661,
      "grad_norm": 5.540991306304932,
      "learning_rate": 1.9081811931861583e-05,
      "loss": 1.737,
      "step": 485700
    },
    {
      "epoch": 37.10946451760751,
      "grad_norm": 5.306698799133301,
      "learning_rate": 1.907544623532707e-05,
      "loss": 1.7432,
      "step": 485800
    },
    {
      "epoch": 37.11710335344893,
      "grad_norm": 4.5385518074035645,
      "learning_rate": 1.9069080538792554e-05,
      "loss": 1.6072,
      "step": 485900
    },
    {
      "epoch": 37.12474218929035,
      "grad_norm": 5.610350131988525,
      "learning_rate": 1.906271484225804e-05,
      "loss": 1.7202,
      "step": 486000
    },
    {
      "epoch": 37.13238102513177,
      "grad_norm": 5.134124279022217,
      "learning_rate": 1.9056349145723524e-05,
      "loss": 1.7379,
      "step": 486100
    },
    {
      "epoch": 37.14001986097319,
      "grad_norm": 5.634005069732666,
      "learning_rate": 1.904998344918901e-05,
      "loss": 1.7293,
      "step": 486200
    },
    {
      "epoch": 37.1476586968146,
      "grad_norm": 4.445362567901611,
      "learning_rate": 1.9043617752654498e-05,
      "loss": 1.7056,
      "step": 486300
    },
    {
      "epoch": 37.15529753265602,
      "grad_norm": 4.422042369842529,
      "learning_rate": 1.9037252056119982e-05,
      "loss": 1.6178,
      "step": 486400
    },
    {
      "epoch": 37.16293636849744,
      "grad_norm": 2.3882036209106445,
      "learning_rate": 1.903088635958547e-05,
      "loss": 1.6695,
      "step": 486500
    },
    {
      "epoch": 37.17057520433886,
      "grad_norm": 4.678660869598389,
      "learning_rate": 1.9024520663050952e-05,
      "loss": 1.7088,
      "step": 486600
    },
    {
      "epoch": 37.17821404018028,
      "grad_norm": 4.728643417358398,
      "learning_rate": 1.901815496651644e-05,
      "loss": 1.728,
      "step": 486700
    },
    {
      "epoch": 37.18585287602169,
      "grad_norm": 4.913186073303223,
      "learning_rate": 1.9011789269981923e-05,
      "loss": 1.6623,
      "step": 486800
    },
    {
      "epoch": 37.19349171186311,
      "grad_norm": 4.588411808013916,
      "learning_rate": 1.9005423573447406e-05,
      "loss": 1.6713,
      "step": 486900
    },
    {
      "epoch": 37.20113054770453,
      "grad_norm": 5.0411882400512695,
      "learning_rate": 1.8999057876912893e-05,
      "loss": 1.6141,
      "step": 487000
    },
    {
      "epoch": 37.20876938354595,
      "grad_norm": 5.840670108795166,
      "learning_rate": 1.8992692180378377e-05,
      "loss": 1.6165,
      "step": 487100
    },
    {
      "epoch": 37.21640821938737,
      "grad_norm": 6.028225898742676,
      "learning_rate": 1.8986326483843864e-05,
      "loss": 1.6194,
      "step": 487200
    },
    {
      "epoch": 37.22404705522878,
      "grad_norm": 5.424212455749512,
      "learning_rate": 1.8979960787309347e-05,
      "loss": 1.6959,
      "step": 487300
    },
    {
      "epoch": 37.2316858910702,
      "grad_norm": 5.003288745880127,
      "learning_rate": 1.8973595090774834e-05,
      "loss": 1.6604,
      "step": 487400
    },
    {
      "epoch": 37.23932472691162,
      "grad_norm": 5.271368980407715,
      "learning_rate": 1.8967229394240318e-05,
      "loss": 1.6664,
      "step": 487500
    },
    {
      "epoch": 37.24696356275304,
      "grad_norm": 5.807906627655029,
      "learning_rate": 1.8960863697705805e-05,
      "loss": 1.7059,
      "step": 487600
    },
    {
      "epoch": 37.25460239859446,
      "grad_norm": 7.242680072784424,
      "learning_rate": 1.8954498001171288e-05,
      "loss": 1.7202,
      "step": 487700
    },
    {
      "epoch": 37.26224123443587,
      "grad_norm": 6.234005928039551,
      "learning_rate": 1.894813230463677e-05,
      "loss": 1.7135,
      "step": 487800
    },
    {
      "epoch": 37.26988007027729,
      "grad_norm": 5.421964168548584,
      "learning_rate": 1.894176660810226e-05,
      "loss": 1.7062,
      "step": 487900
    },
    {
      "epoch": 37.27751890611871,
      "grad_norm": 4.345611095428467,
      "learning_rate": 1.8935400911567745e-05,
      "loss": 1.6369,
      "step": 488000
    },
    {
      "epoch": 37.28515774196013,
      "grad_norm": 5.3006110191345215,
      "learning_rate": 1.892903521503323e-05,
      "loss": 1.7206,
      "step": 488100
    },
    {
      "epoch": 37.292796577801546,
      "grad_norm": 6.996433258056641,
      "learning_rate": 1.8922669518498716e-05,
      "loss": 1.6307,
      "step": 488200
    },
    {
      "epoch": 37.30043541364296,
      "grad_norm": 4.456561088562012,
      "learning_rate": 1.8916303821964203e-05,
      "loss": 1.6273,
      "step": 488300
    },
    {
      "epoch": 37.30807424948438,
      "grad_norm": 6.8512349128723145,
      "learning_rate": 1.8909938125429686e-05,
      "loss": 1.7447,
      "step": 488400
    },
    {
      "epoch": 37.3157130853258,
      "grad_norm": 5.982110500335693,
      "learning_rate": 1.890357242889517e-05,
      "loss": 1.7239,
      "step": 488500
    },
    {
      "epoch": 37.323351921167216,
      "grad_norm": 4.59840726852417,
      "learning_rate": 1.8897206732360657e-05,
      "loss": 1.7164,
      "step": 488600
    },
    {
      "epoch": 37.330990757008635,
      "grad_norm": 5.476855278015137,
      "learning_rate": 1.889084103582614e-05,
      "loss": 1.6815,
      "step": 488700
    },
    {
      "epoch": 37.33862959285005,
      "grad_norm": 5.518133163452148,
      "learning_rate": 1.8884475339291627e-05,
      "loss": 1.7665,
      "step": 488800
    },
    {
      "epoch": 37.34626842869147,
      "grad_norm": 4.785820960998535,
      "learning_rate": 1.887810964275711e-05,
      "loss": 1.7461,
      "step": 488900
    },
    {
      "epoch": 37.353907264532886,
      "grad_norm": 7.850465774536133,
      "learning_rate": 1.8871743946222598e-05,
      "loss": 1.6835,
      "step": 489000
    },
    {
      "epoch": 37.361546100374305,
      "grad_norm": 6.94636869430542,
      "learning_rate": 1.886537824968808e-05,
      "loss": 1.732,
      "step": 489100
    },
    {
      "epoch": 37.369184936215724,
      "grad_norm": 7.360908508300781,
      "learning_rate": 1.8859012553153568e-05,
      "loss": 1.6515,
      "step": 489200
    },
    {
      "epoch": 37.376823772057136,
      "grad_norm": 4.794207572937012,
      "learning_rate": 1.8852646856619052e-05,
      "loss": 1.6756,
      "step": 489300
    },
    {
      "epoch": 37.384462607898556,
      "grad_norm": 5.267786502838135,
      "learning_rate": 1.8846281160084535e-05,
      "loss": 1.7569,
      "step": 489400
    },
    {
      "epoch": 37.392101443739975,
      "grad_norm": 2.6157567501068115,
      "learning_rate": 1.8839915463550022e-05,
      "loss": 1.6642,
      "step": 489500
    },
    {
      "epoch": 37.399740279581394,
      "grad_norm": 8.101724624633789,
      "learning_rate": 1.8833549767015506e-05,
      "loss": 1.6123,
      "step": 489600
    },
    {
      "epoch": 37.407379115422806,
      "grad_norm": 4.608006477355957,
      "learning_rate": 1.8827184070480993e-05,
      "loss": 1.7904,
      "step": 489700
    },
    {
      "epoch": 37.415017951264225,
      "grad_norm": 5.872188568115234,
      "learning_rate": 1.8820818373946476e-05,
      "loss": 1.7133,
      "step": 489800
    },
    {
      "epoch": 37.422656787105645,
      "grad_norm": 5.002398490905762,
      "learning_rate": 1.8814452677411963e-05,
      "loss": 1.6541,
      "step": 489900
    },
    {
      "epoch": 37.430295622947064,
      "grad_norm": 6.576997756958008,
      "learning_rate": 1.880808698087745e-05,
      "loss": 1.6967,
      "step": 490000
    },
    {
      "epoch": 37.43793445878848,
      "grad_norm": 4.856992244720459,
      "learning_rate": 1.8801721284342934e-05,
      "loss": 1.7638,
      "step": 490100
    },
    {
      "epoch": 37.445573294629895,
      "grad_norm": 5.864701747894287,
      "learning_rate": 1.879535558780842e-05,
      "loss": 1.6746,
      "step": 490200
    },
    {
      "epoch": 37.453212130471314,
      "grad_norm": 6.658441066741943,
      "learning_rate": 1.8788989891273904e-05,
      "loss": 1.6553,
      "step": 490300
    },
    {
      "epoch": 37.460850966312734,
      "grad_norm": 5.062366008758545,
      "learning_rate": 1.878262419473939e-05,
      "loss": 1.7562,
      "step": 490400
    },
    {
      "epoch": 37.46848980215415,
      "grad_norm": 6.078460216522217,
      "learning_rate": 1.8776258498204875e-05,
      "loss": 1.7209,
      "step": 490500
    },
    {
      "epoch": 37.47612863799557,
      "grad_norm": 5.745965003967285,
      "learning_rate": 1.876989280167036e-05,
      "loss": 1.7537,
      "step": 490600
    },
    {
      "epoch": 37.483767473836984,
      "grad_norm": 5.496408939361572,
      "learning_rate": 1.8763527105135845e-05,
      "loss": 1.7153,
      "step": 490700
    },
    {
      "epoch": 37.4914063096784,
      "grad_norm": 6.178145408630371,
      "learning_rate": 1.8757161408601332e-05,
      "loss": 1.6666,
      "step": 490800
    },
    {
      "epoch": 37.49904514551982,
      "grad_norm": 6.51919412612915,
      "learning_rate": 1.8750795712066815e-05,
      "loss": 1.6502,
      "step": 490900
    },
    {
      "epoch": 37.50668398136124,
      "grad_norm": 5.614480972290039,
      "learning_rate": 1.87444300155323e-05,
      "loss": 1.7137,
      "step": 491000
    },
    {
      "epoch": 37.51432281720266,
      "grad_norm": 5.906186580657959,
      "learning_rate": 1.8738064318997786e-05,
      "loss": 1.7451,
      "step": 491100
    },
    {
      "epoch": 37.52196165304407,
      "grad_norm": 4.829225540161133,
      "learning_rate": 1.873169862246327e-05,
      "loss": 1.6534,
      "step": 491200
    },
    {
      "epoch": 37.52960048888549,
      "grad_norm": 5.902710437774658,
      "learning_rate": 1.8725332925928756e-05,
      "loss": 1.6388,
      "step": 491300
    },
    {
      "epoch": 37.53723932472691,
      "grad_norm": 5.03184175491333,
      "learning_rate": 1.871896722939424e-05,
      "loss": 1.6734,
      "step": 491400
    },
    {
      "epoch": 37.54487816056833,
      "grad_norm": 6.355733871459961,
      "learning_rate": 1.8712601532859727e-05,
      "loss": 1.7128,
      "step": 491500
    },
    {
      "epoch": 37.55251699640975,
      "grad_norm": 7.770182132720947,
      "learning_rate": 1.870623583632521e-05,
      "loss": 1.718,
      "step": 491600
    },
    {
      "epoch": 37.56015583225116,
      "grad_norm": 7.542567729949951,
      "learning_rate": 1.8699870139790694e-05,
      "loss": 1.6459,
      "step": 491700
    },
    {
      "epoch": 37.56779466809258,
      "grad_norm": 6.226337432861328,
      "learning_rate": 1.869350444325618e-05,
      "loss": 1.7142,
      "step": 491800
    },
    {
      "epoch": 37.575433503934,
      "grad_norm": 4.365689277648926,
      "learning_rate": 1.8687138746721668e-05,
      "loss": 1.6882,
      "step": 491900
    },
    {
      "epoch": 37.58307233977542,
      "grad_norm": 4.992551803588867,
      "learning_rate": 1.8680773050187155e-05,
      "loss": 1.769,
      "step": 492000
    },
    {
      "epoch": 37.59071117561684,
      "grad_norm": 5.777671813964844,
      "learning_rate": 1.8674407353652638e-05,
      "loss": 1.7373,
      "step": 492100
    },
    {
      "epoch": 37.59835001145825,
      "grad_norm": 6.456189155578613,
      "learning_rate": 1.8668041657118125e-05,
      "loss": 1.7267,
      "step": 492200
    },
    {
      "epoch": 37.60598884729967,
      "grad_norm": 6.641139030456543,
      "learning_rate": 1.866167596058361e-05,
      "loss": 1.6517,
      "step": 492300
    },
    {
      "epoch": 37.61362768314109,
      "grad_norm": 6.389616012573242,
      "learning_rate": 1.8655310264049092e-05,
      "loss": 1.6328,
      "step": 492400
    },
    {
      "epoch": 37.62126651898251,
      "grad_norm": 4.429844856262207,
      "learning_rate": 1.864894456751458e-05,
      "loss": 1.6422,
      "step": 492500
    },
    {
      "epoch": 37.62890535482393,
      "grad_norm": 9.264373779296875,
      "learning_rate": 1.8642578870980063e-05,
      "loss": 1.6337,
      "step": 492600
    },
    {
      "epoch": 37.63654419066534,
      "grad_norm": 5.594592094421387,
      "learning_rate": 1.863621317444555e-05,
      "loss": 1.628,
      "step": 492700
    },
    {
      "epoch": 37.64418302650676,
      "grad_norm": 5.375170707702637,
      "learning_rate": 1.8629847477911033e-05,
      "loss": 1.692,
      "step": 492800
    },
    {
      "epoch": 37.65182186234818,
      "grad_norm": 7.399355888366699,
      "learning_rate": 1.862348178137652e-05,
      "loss": 1.6367,
      "step": 492900
    },
    {
      "epoch": 37.6594606981896,
      "grad_norm": 6.854835033416748,
      "learning_rate": 1.8617116084842004e-05,
      "loss": 1.7657,
      "step": 493000
    },
    {
      "epoch": 37.66709953403102,
      "grad_norm": 6.355406761169434,
      "learning_rate": 1.861075038830749e-05,
      "loss": 1.8075,
      "step": 493100
    },
    {
      "epoch": 37.67473836987243,
      "grad_norm": 4.031575679779053,
      "learning_rate": 1.8604384691772974e-05,
      "loss": 1.6513,
      "step": 493200
    },
    {
      "epoch": 37.68237720571385,
      "grad_norm": 4.717452526092529,
      "learning_rate": 1.8598018995238458e-05,
      "loss": 1.7888,
      "step": 493300
    },
    {
      "epoch": 37.69001604155527,
      "grad_norm": 4.832627773284912,
      "learning_rate": 1.8591653298703945e-05,
      "loss": 1.7939,
      "step": 493400
    },
    {
      "epoch": 37.69765487739669,
      "grad_norm": 5.563406944274902,
      "learning_rate": 1.8585287602169428e-05,
      "loss": 1.8215,
      "step": 493500
    },
    {
      "epoch": 37.705293713238106,
      "grad_norm": 6.169562816619873,
      "learning_rate": 1.8578921905634915e-05,
      "loss": 1.7068,
      "step": 493600
    },
    {
      "epoch": 37.71293254907952,
      "grad_norm": 5.290639400482178,
      "learning_rate": 1.85725562091004e-05,
      "loss": 1.682,
      "step": 493700
    },
    {
      "epoch": 37.72057138492094,
      "grad_norm": 5.476622104644775,
      "learning_rate": 1.8566190512565885e-05,
      "loss": 1.6808,
      "step": 493800
    },
    {
      "epoch": 37.72821022076236,
      "grad_norm": 4.846256732940674,
      "learning_rate": 1.8559824816031372e-05,
      "loss": 1.5938,
      "step": 493900
    },
    {
      "epoch": 37.735849056603776,
      "grad_norm": 6.390502452850342,
      "learning_rate": 1.8553459119496856e-05,
      "loss": 1.6668,
      "step": 494000
    },
    {
      "epoch": 37.74348789244519,
      "grad_norm": 4.527165412902832,
      "learning_rate": 1.8547093422962343e-05,
      "loss": 1.6975,
      "step": 494100
    },
    {
      "epoch": 37.75112672828661,
      "grad_norm": 5.353705883026123,
      "learning_rate": 1.8540727726427826e-05,
      "loss": 1.6807,
      "step": 494200
    },
    {
      "epoch": 37.758765564128026,
      "grad_norm": 5.874845504760742,
      "learning_rate": 1.8534362029893313e-05,
      "loss": 1.821,
      "step": 494300
    },
    {
      "epoch": 37.766404399969446,
      "grad_norm": 5.586630821228027,
      "learning_rate": 1.8527996333358797e-05,
      "loss": 1.7409,
      "step": 494400
    },
    {
      "epoch": 37.774043235810865,
      "grad_norm": 5.554355144500732,
      "learning_rate": 1.8521630636824284e-05,
      "loss": 1.6213,
      "step": 494500
    },
    {
      "epoch": 37.78168207165228,
      "grad_norm": 7.0714592933654785,
      "learning_rate": 1.8515264940289767e-05,
      "loss": 1.727,
      "step": 494600
    },
    {
      "epoch": 37.789320907493696,
      "grad_norm": 5.652618408203125,
      "learning_rate": 1.8508899243755254e-05,
      "loss": 1.6303,
      "step": 494700
    },
    {
      "epoch": 37.796959743335115,
      "grad_norm": 5.433409690856934,
      "learning_rate": 1.8502533547220738e-05,
      "loss": 1.7003,
      "step": 494800
    },
    {
      "epoch": 37.804598579176535,
      "grad_norm": 6.361233234405518,
      "learning_rate": 1.849616785068622e-05,
      "loss": 1.7082,
      "step": 494900
    },
    {
      "epoch": 37.812237415017954,
      "grad_norm": 5.824007511138916,
      "learning_rate": 1.8489802154151708e-05,
      "loss": 1.7214,
      "step": 495000
    },
    {
      "epoch": 37.819876250859366,
      "grad_norm": 5.857821941375732,
      "learning_rate": 1.8483436457617192e-05,
      "loss": 1.6585,
      "step": 495100
    },
    {
      "epoch": 37.827515086700785,
      "grad_norm": 6.457461357116699,
      "learning_rate": 1.847707076108268e-05,
      "loss": 1.7043,
      "step": 495200
    },
    {
      "epoch": 37.835153922542204,
      "grad_norm": 5.5646138191223145,
      "learning_rate": 1.8470705064548162e-05,
      "loss": 1.7237,
      "step": 495300
    },
    {
      "epoch": 37.842792758383624,
      "grad_norm": 6.206080436706543,
      "learning_rate": 1.846433936801365e-05,
      "loss": 1.5477,
      "step": 495400
    },
    {
      "epoch": 37.85043159422504,
      "grad_norm": 4.270040035247803,
      "learning_rate": 1.8457973671479133e-05,
      "loss": 1.6335,
      "step": 495500
    },
    {
      "epoch": 37.858070430066455,
      "grad_norm": 4.566975116729736,
      "learning_rate": 1.845160797494462e-05,
      "loss": 1.7146,
      "step": 495600
    },
    {
      "epoch": 37.865709265907874,
      "grad_norm": 5.6925249099731445,
      "learning_rate": 1.8445242278410103e-05,
      "loss": 1.722,
      "step": 495700
    },
    {
      "epoch": 37.87334810174929,
      "grad_norm": 5.455094814300537,
      "learning_rate": 1.843887658187559e-05,
      "loss": 1.6382,
      "step": 495800
    },
    {
      "epoch": 37.88098693759071,
      "grad_norm": 6.165139198303223,
      "learning_rate": 1.8432510885341077e-05,
      "loss": 1.7182,
      "step": 495900
    },
    {
      "epoch": 37.88862577343213,
      "grad_norm": 4.818182945251465,
      "learning_rate": 1.842614518880656e-05,
      "loss": 1.705,
      "step": 496000
    },
    {
      "epoch": 37.896264609273544,
      "grad_norm": 5.770341396331787,
      "learning_rate": 1.8419779492272047e-05,
      "loss": 1.6954,
      "step": 496100
    },
    {
      "epoch": 37.90390344511496,
      "grad_norm": 3.1905736923217773,
      "learning_rate": 1.841341379573753e-05,
      "loss": 1.5883,
      "step": 496200
    },
    {
      "epoch": 37.91154228095638,
      "grad_norm": 3.367729663848877,
      "learning_rate": 1.8407048099203018e-05,
      "loss": 1.7405,
      "step": 496300
    },
    {
      "epoch": 37.9191811167978,
      "grad_norm": 5.8077192306518555,
      "learning_rate": 1.84006824026685e-05,
      "loss": 1.8498,
      "step": 496400
    },
    {
      "epoch": 37.92681995263922,
      "grad_norm": 6.976704120635986,
      "learning_rate": 1.8394316706133985e-05,
      "loss": 1.7417,
      "step": 496500
    },
    {
      "epoch": 37.93445878848063,
      "grad_norm": 4.919018745422363,
      "learning_rate": 1.8387951009599472e-05,
      "loss": 1.6649,
      "step": 496600
    },
    {
      "epoch": 37.94209762432205,
      "grad_norm": 4.820483207702637,
      "learning_rate": 1.8381585313064955e-05,
      "loss": 1.7156,
      "step": 496700
    },
    {
      "epoch": 37.94973646016347,
      "grad_norm": 5.742657661437988,
      "learning_rate": 1.8375219616530442e-05,
      "loss": 1.6159,
      "step": 496800
    },
    {
      "epoch": 37.95737529600489,
      "grad_norm": 4.433561325073242,
      "learning_rate": 1.8368853919995926e-05,
      "loss": 1.6748,
      "step": 496900
    },
    {
      "epoch": 37.96501413184631,
      "grad_norm": 4.398402214050293,
      "learning_rate": 1.8362488223461413e-05,
      "loss": 1.6914,
      "step": 497000
    },
    {
      "epoch": 37.97265296768772,
      "grad_norm": 6.1669816970825195,
      "learning_rate": 1.8356122526926896e-05,
      "loss": 1.7093,
      "step": 497100
    },
    {
      "epoch": 37.98029180352914,
      "grad_norm": 6.435131072998047,
      "learning_rate": 1.834975683039238e-05,
      "loss": 1.7718,
      "step": 497200
    },
    {
      "epoch": 37.98793063937056,
      "grad_norm": 4.484100818634033,
      "learning_rate": 1.8343391133857867e-05,
      "loss": 1.5895,
      "step": 497300
    },
    {
      "epoch": 37.99556947521198,
      "grad_norm": 6.529546737670898,
      "learning_rate": 1.833702543732335e-05,
      "loss": 1.7441,
      "step": 497400
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.7789340019226074,
      "eval_runtime": 3.0359,
      "eval_samples_per_second": 227.279,
      "eval_steps_per_second": 227.279,
      "step": 497458
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.4581665992736816,
      "eval_runtime": 57.8331,
      "eval_samples_per_second": 226.358,
      "eval_steps_per_second": 226.358,
      "step": 497458
    },
    {
      "epoch": 38.0032083110534,
      "grad_norm": 4.358875274658203,
      "learning_rate": 1.8330659740788837e-05,
      "loss": 1.6927,
      "step": 497500
    },
    {
      "epoch": 38.01084714689481,
      "grad_norm": 6.806156635284424,
      "learning_rate": 1.8324294044254324e-05,
      "loss": 1.7059,
      "step": 497600
    },
    {
      "epoch": 38.01848598273623,
      "grad_norm": 5.231612205505371,
      "learning_rate": 1.8317928347719808e-05,
      "loss": 1.7145,
      "step": 497700
    },
    {
      "epoch": 38.02612481857765,
      "grad_norm": 5.309061527252197,
      "learning_rate": 1.8311562651185295e-05,
      "loss": 1.6059,
      "step": 497800
    },
    {
      "epoch": 38.03376365441907,
      "grad_norm": 7.001138687133789,
      "learning_rate": 1.830519695465078e-05,
      "loss": 1.6461,
      "step": 497900
    },
    {
      "epoch": 38.04140249026048,
      "grad_norm": 9.762895584106445,
      "learning_rate": 1.8298831258116265e-05,
      "loss": 1.755,
      "step": 498000
    },
    {
      "epoch": 38.0490413261019,
      "grad_norm": 6.201384544372559,
      "learning_rate": 1.829246556158175e-05,
      "loss": 1.679,
      "step": 498100
    },
    {
      "epoch": 38.05668016194332,
      "grad_norm": 5.037818431854248,
      "learning_rate": 1.8286099865047236e-05,
      "loss": 1.7301,
      "step": 498200
    },
    {
      "epoch": 38.06431899778474,
      "grad_norm": 5.807804107666016,
      "learning_rate": 1.827973416851272e-05,
      "loss": 1.6111,
      "step": 498300
    },
    {
      "epoch": 38.07195783362616,
      "grad_norm": 6.196157932281494,
      "learning_rate": 1.8273368471978206e-05,
      "loss": 1.6971,
      "step": 498400
    },
    {
      "epoch": 38.07959666946757,
      "grad_norm": 4.724363803863525,
      "learning_rate": 1.826700277544369e-05,
      "loss": 1.7345,
      "step": 498500
    },
    {
      "epoch": 38.08723550530899,
      "grad_norm": 5.517103672027588,
      "learning_rate": 1.8260637078909177e-05,
      "loss": 1.647,
      "step": 498600
    },
    {
      "epoch": 38.09487434115041,
      "grad_norm": 5.827165603637695,
      "learning_rate": 1.825427138237466e-05,
      "loss": 1.7226,
      "step": 498700
    },
    {
      "epoch": 38.10251317699183,
      "grad_norm": 6.316381454467773,
      "learning_rate": 1.8247905685840144e-05,
      "loss": 1.6915,
      "step": 498800
    },
    {
      "epoch": 38.11015201283325,
      "grad_norm": 4.849983215332031,
      "learning_rate": 1.824153998930563e-05,
      "loss": 1.6845,
      "step": 498900
    },
    {
      "epoch": 38.11779084867466,
      "grad_norm": 7.120841026306152,
      "learning_rate": 1.8235174292771114e-05,
      "loss": 1.6957,
      "step": 499000
    },
    {
      "epoch": 38.12542968451608,
      "grad_norm": 5.970295429229736,
      "learning_rate": 1.82288085962366e-05,
      "loss": 1.7931,
      "step": 499100
    },
    {
      "epoch": 38.1330685203575,
      "grad_norm": 6.238229274749756,
      "learning_rate": 1.8222442899702085e-05,
      "loss": 1.6587,
      "step": 499200
    },
    {
      "epoch": 38.14070735619892,
      "grad_norm": 5.076948642730713,
      "learning_rate": 1.821607720316757e-05,
      "loss": 1.6334,
      "step": 499300
    },
    {
      "epoch": 38.148346192040336,
      "grad_norm": 3.992016553878784,
      "learning_rate": 1.8209711506633055e-05,
      "loss": 1.6219,
      "step": 499400
    },
    {
      "epoch": 38.15598502788175,
      "grad_norm": 6.494619846343994,
      "learning_rate": 1.8203345810098542e-05,
      "loss": 1.5923,
      "step": 499500
    },
    {
      "epoch": 38.16362386372317,
      "grad_norm": 5.228952407836914,
      "learning_rate": 1.819698011356403e-05,
      "loss": 1.6316,
      "step": 499600
    },
    {
      "epoch": 38.171262699564586,
      "grad_norm": 7.025961399078369,
      "learning_rate": 1.8190614417029512e-05,
      "loss": 1.5924,
      "step": 499700
    },
    {
      "epoch": 38.178901535406006,
      "grad_norm": 5.357893466949463,
      "learning_rate": 1.8184248720495e-05,
      "loss": 1.7705,
      "step": 499800
    },
    {
      "epoch": 38.186540371247425,
      "grad_norm": 6.628675937652588,
      "learning_rate": 1.8177883023960483e-05,
      "loss": 1.6663,
      "step": 499900
    },
    {
      "epoch": 38.19417920708884,
      "grad_norm": 7.097280502319336,
      "learning_rate": 1.817151732742597e-05,
      "loss": 1.6681,
      "step": 500000
    },
    {
      "epoch": 38.201818042930256,
      "grad_norm": 5.683618068695068,
      "learning_rate": 1.8165151630891453e-05,
      "loss": 1.8017,
      "step": 500100
    },
    {
      "epoch": 38.209456878771675,
      "grad_norm": 5.008846282958984,
      "learning_rate": 1.815878593435694e-05,
      "loss": 1.7748,
      "step": 500200
    },
    {
      "epoch": 38.217095714613095,
      "grad_norm": 5.586978435516357,
      "learning_rate": 1.8152420237822424e-05,
      "loss": 1.7931,
      "step": 500300
    },
    {
      "epoch": 38.224734550454514,
      "grad_norm": 7.466530799865723,
      "learning_rate": 1.8146054541287907e-05,
      "loss": 1.7282,
      "step": 500400
    },
    {
      "epoch": 38.232373386295926,
      "grad_norm": 5.837275981903076,
      "learning_rate": 1.8139688844753394e-05,
      "loss": 1.6754,
      "step": 500500
    },
    {
      "epoch": 38.240012222137345,
      "grad_norm": 6.525106430053711,
      "learning_rate": 1.8133323148218878e-05,
      "loss": 1.6273,
      "step": 500600
    },
    {
      "epoch": 38.247651057978764,
      "grad_norm": 6.047097206115723,
      "learning_rate": 1.8126957451684365e-05,
      "loss": 1.6447,
      "step": 500700
    },
    {
      "epoch": 38.25528989382018,
      "grad_norm": 5.1156392097473145,
      "learning_rate": 1.8120591755149848e-05,
      "loss": 1.7939,
      "step": 500800
    },
    {
      "epoch": 38.2629287296616,
      "grad_norm": 4.777286052703857,
      "learning_rate": 1.8114226058615335e-05,
      "loss": 1.7015,
      "step": 500900
    },
    {
      "epoch": 38.270567565503015,
      "grad_norm": 6.097798824310303,
      "learning_rate": 1.810786036208082e-05,
      "loss": 1.7696,
      "step": 501000
    },
    {
      "epoch": 38.278206401344434,
      "grad_norm": 4.032526016235352,
      "learning_rate": 1.8101494665546302e-05,
      "loss": 1.6683,
      "step": 501100
    },
    {
      "epoch": 38.28584523718585,
      "grad_norm": 5.838403701782227,
      "learning_rate": 1.809512896901179e-05,
      "loss": 1.6546,
      "step": 501200
    },
    {
      "epoch": 38.29348407302727,
      "grad_norm": 4.629443645477295,
      "learning_rate": 1.8088763272477273e-05,
      "loss": 1.6606,
      "step": 501300
    },
    {
      "epoch": 38.30112290886869,
      "grad_norm": 7.8902764320373535,
      "learning_rate": 1.808239757594276e-05,
      "loss": 1.7579,
      "step": 501400
    },
    {
      "epoch": 38.308761744710104,
      "grad_norm": 5.54238748550415,
      "learning_rate": 1.8076031879408247e-05,
      "loss": 1.7227,
      "step": 501500
    },
    {
      "epoch": 38.31640058055152,
      "grad_norm": 5.446608543395996,
      "learning_rate": 1.8069666182873733e-05,
      "loss": 1.6689,
      "step": 501600
    },
    {
      "epoch": 38.32403941639294,
      "grad_norm": 6.52936315536499,
      "learning_rate": 1.8063300486339217e-05,
      "loss": 1.5318,
      "step": 501700
    },
    {
      "epoch": 38.33167825223436,
      "grad_norm": 4.322125434875488,
      "learning_rate": 1.8056934789804704e-05,
      "loss": 1.6587,
      "step": 501800
    },
    {
      "epoch": 38.33931708807578,
      "grad_norm": 4.1503167152404785,
      "learning_rate": 1.8050569093270187e-05,
      "loss": 1.7353,
      "step": 501900
    },
    {
      "epoch": 38.34695592391719,
      "grad_norm": 5.313251495361328,
      "learning_rate": 1.804420339673567e-05,
      "loss": 1.7216,
      "step": 502000
    },
    {
      "epoch": 38.35459475975861,
      "grad_norm": 4.426581382751465,
      "learning_rate": 1.8037837700201158e-05,
      "loss": 1.7017,
      "step": 502100
    },
    {
      "epoch": 38.36223359560003,
      "grad_norm": 5.2813496589660645,
      "learning_rate": 1.803147200366664e-05,
      "loss": 1.7703,
      "step": 502200
    },
    {
      "epoch": 38.36987243144145,
      "grad_norm": 5.44925594329834,
      "learning_rate": 1.802510630713213e-05,
      "loss": 1.7199,
      "step": 502300
    },
    {
      "epoch": 38.37751126728286,
      "grad_norm": 6.9710493087768555,
      "learning_rate": 1.8018740610597612e-05,
      "loss": 1.7138,
      "step": 502400
    },
    {
      "epoch": 38.38515010312428,
      "grad_norm": 6.698244571685791,
      "learning_rate": 1.80123749140631e-05,
      "loss": 1.6669,
      "step": 502500
    },
    {
      "epoch": 38.3927889389657,
      "grad_norm": 5.475009441375732,
      "learning_rate": 1.8006009217528582e-05,
      "loss": 1.7499,
      "step": 502600
    },
    {
      "epoch": 38.40042777480712,
      "grad_norm": 5.223718643188477,
      "learning_rate": 1.7999643520994066e-05,
      "loss": 1.6326,
      "step": 502700
    },
    {
      "epoch": 38.40806661064854,
      "grad_norm": 4.954367160797119,
      "learning_rate": 1.7993277824459553e-05,
      "loss": 1.6701,
      "step": 502800
    },
    {
      "epoch": 38.41570544648995,
      "grad_norm": 7.021584510803223,
      "learning_rate": 1.7986912127925036e-05,
      "loss": 1.6423,
      "step": 502900
    },
    {
      "epoch": 38.42334428233137,
      "grad_norm": 4.77261209487915,
      "learning_rate": 1.7980546431390523e-05,
      "loss": 1.6903,
      "step": 503000
    },
    {
      "epoch": 38.43098311817279,
      "grad_norm": 5.736084461212158,
      "learning_rate": 1.7974180734856007e-05,
      "loss": 1.6589,
      "step": 503100
    },
    {
      "epoch": 38.43862195401421,
      "grad_norm": 6.677990436553955,
      "learning_rate": 1.7967815038321494e-05,
      "loss": 1.7287,
      "step": 503200
    },
    {
      "epoch": 38.44626078985563,
      "grad_norm": 4.364688396453857,
      "learning_rate": 1.7961449341786977e-05,
      "loss": 1.6628,
      "step": 503300
    },
    {
      "epoch": 38.45389962569704,
      "grad_norm": 5.336720943450928,
      "learning_rate": 1.7955083645252464e-05,
      "loss": 1.6598,
      "step": 503400
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 5.011138916015625,
      "learning_rate": 1.794871794871795e-05,
      "loss": 1.6043,
      "step": 503500
    },
    {
      "epoch": 38.46917729737988,
      "grad_norm": 4.596194267272949,
      "learning_rate": 1.7942352252183435e-05,
      "loss": 1.6918,
      "step": 503600
    },
    {
      "epoch": 38.4768161332213,
      "grad_norm": 5.584061145782471,
      "learning_rate": 1.793598655564892e-05,
      "loss": 1.6627,
      "step": 503700
    },
    {
      "epoch": 38.48445496906272,
      "grad_norm": 4.440799713134766,
      "learning_rate": 1.7929620859114405e-05,
      "loss": 1.7793,
      "step": 503800
    },
    {
      "epoch": 38.49209380490413,
      "grad_norm": 4.32509708404541,
      "learning_rate": 1.7923255162579892e-05,
      "loss": 1.694,
      "step": 503900
    },
    {
      "epoch": 38.49973264074555,
      "grad_norm": 6.557054042816162,
      "learning_rate": 1.7916889466045376e-05,
      "loss": 1.7567,
      "step": 504000
    },
    {
      "epoch": 38.50737147658697,
      "grad_norm": 7.808916091918945,
      "learning_rate": 1.7910523769510863e-05,
      "loss": 1.6799,
      "step": 504100
    },
    {
      "epoch": 38.51501031242839,
      "grad_norm": 6.590592861175537,
      "learning_rate": 1.7904158072976346e-05,
      "loss": 1.6239,
      "step": 504200
    },
    {
      "epoch": 38.52264914826981,
      "grad_norm": 5.07921838760376,
      "learning_rate": 1.789779237644183e-05,
      "loss": 1.6482,
      "step": 504300
    },
    {
      "epoch": 38.53028798411122,
      "grad_norm": 6.762571334838867,
      "learning_rate": 1.7891426679907317e-05,
      "loss": 1.6205,
      "step": 504400
    },
    {
      "epoch": 38.53792681995264,
      "grad_norm": 6.211395740509033,
      "learning_rate": 1.78850609833728e-05,
      "loss": 1.7425,
      "step": 504500
    },
    {
      "epoch": 38.54556565579406,
      "grad_norm": 5.40811824798584,
      "learning_rate": 1.7878695286838287e-05,
      "loss": 1.6719,
      "step": 504600
    },
    {
      "epoch": 38.553204491635476,
      "grad_norm": 5.503217697143555,
      "learning_rate": 1.787232959030377e-05,
      "loss": 1.6986,
      "step": 504700
    },
    {
      "epoch": 38.560843327476896,
      "grad_norm": 4.832221984863281,
      "learning_rate": 1.7865963893769257e-05,
      "loss": 1.605,
      "step": 504800
    },
    {
      "epoch": 38.56848216331831,
      "grad_norm": 4.365626335144043,
      "learning_rate": 1.785959819723474e-05,
      "loss": 1.6626,
      "step": 504900
    },
    {
      "epoch": 38.57612099915973,
      "grad_norm": 6.709005832672119,
      "learning_rate": 1.7853232500700228e-05,
      "loss": 1.6892,
      "step": 505000
    },
    {
      "epoch": 38.583759835001146,
      "grad_norm": 5.912522315979004,
      "learning_rate": 1.784686680416571e-05,
      "loss": 1.6879,
      "step": 505100
    },
    {
      "epoch": 38.591398670842565,
      "grad_norm": 3.822798013687134,
      "learning_rate": 1.78405011076312e-05,
      "loss": 1.7048,
      "step": 505200
    },
    {
      "epoch": 38.599037506683985,
      "grad_norm": 5.098725318908691,
      "learning_rate": 1.7834135411096682e-05,
      "loss": 1.734,
      "step": 505300
    },
    {
      "epoch": 38.6066763425254,
      "grad_norm": 5.308437824249268,
      "learning_rate": 1.782776971456217e-05,
      "loss": 1.6792,
      "step": 505400
    },
    {
      "epoch": 38.614315178366816,
      "grad_norm": 6.400661945343018,
      "learning_rate": 1.7821404018027656e-05,
      "loss": 1.7219,
      "step": 505500
    },
    {
      "epoch": 38.621954014208235,
      "grad_norm": 4.285553932189941,
      "learning_rate": 1.781503832149314e-05,
      "loss": 1.7512,
      "step": 505600
    },
    {
      "epoch": 38.629592850049654,
      "grad_norm": 7.401483058929443,
      "learning_rate": 1.7808672624958626e-05,
      "loss": 1.7339,
      "step": 505700
    },
    {
      "epoch": 38.637231685891074,
      "grad_norm": 5.399475574493408,
      "learning_rate": 1.780230692842411e-05,
      "loss": 1.7321,
      "step": 505800
    },
    {
      "epoch": 38.644870521732486,
      "grad_norm": 4.84979248046875,
      "learning_rate": 1.7795941231889593e-05,
      "loss": 1.6458,
      "step": 505900
    },
    {
      "epoch": 38.652509357573905,
      "grad_norm": 6.275557994842529,
      "learning_rate": 1.778957553535508e-05,
      "loss": 1.7566,
      "step": 506000
    },
    {
      "epoch": 38.660148193415324,
      "grad_norm": 7.330981731414795,
      "learning_rate": 1.7783209838820564e-05,
      "loss": 1.7704,
      "step": 506100
    },
    {
      "epoch": 38.66778702925674,
      "grad_norm": 4.992046356201172,
      "learning_rate": 1.777684414228605e-05,
      "loss": 1.7266,
      "step": 506200
    },
    {
      "epoch": 38.67542586509816,
      "grad_norm": 5.695833683013916,
      "learning_rate": 1.7770478445751534e-05,
      "loss": 1.6935,
      "step": 506300
    },
    {
      "epoch": 38.683064700939575,
      "grad_norm": 5.958909034729004,
      "learning_rate": 1.776411274921702e-05,
      "loss": 1.6584,
      "step": 506400
    },
    {
      "epoch": 38.690703536780994,
      "grad_norm": 3.886146306991577,
      "learning_rate": 1.7757747052682505e-05,
      "loss": 1.7214,
      "step": 506500
    },
    {
      "epoch": 38.69834237262241,
      "grad_norm": 4.299613952636719,
      "learning_rate": 1.775138135614799e-05,
      "loss": 1.6694,
      "step": 506600
    },
    {
      "epoch": 38.70598120846383,
      "grad_norm": 4.294862270355225,
      "learning_rate": 1.7745015659613475e-05,
      "loss": 1.6672,
      "step": 506700
    },
    {
      "epoch": 38.713620044305245,
      "grad_norm": 5.664459228515625,
      "learning_rate": 1.773864996307896e-05,
      "loss": 1.7212,
      "step": 506800
    },
    {
      "epoch": 38.721258880146664,
      "grad_norm": 6.631159782409668,
      "learning_rate": 1.7732284266544446e-05,
      "loss": 1.7418,
      "step": 506900
    },
    {
      "epoch": 38.72889771598808,
      "grad_norm": 6.900363922119141,
      "learning_rate": 1.772591857000993e-05,
      "loss": 1.6708,
      "step": 507000
    },
    {
      "epoch": 38.7365365518295,
      "grad_norm": 6.507351398468018,
      "learning_rate": 1.7719552873475416e-05,
      "loss": 1.6995,
      "step": 507100
    },
    {
      "epoch": 38.74417538767092,
      "grad_norm": 4.279799938201904,
      "learning_rate": 1.7713187176940903e-05,
      "loss": 1.7432,
      "step": 507200
    },
    {
      "epoch": 38.75181422351233,
      "grad_norm": 5.807358264923096,
      "learning_rate": 1.7706821480406387e-05,
      "loss": 1.7304,
      "step": 507300
    },
    {
      "epoch": 38.75945305935375,
      "grad_norm": 6.960065841674805,
      "learning_rate": 1.7700455783871873e-05,
      "loss": 1.6763,
      "step": 507400
    },
    {
      "epoch": 38.76709189519517,
      "grad_norm": 5.713072776794434,
      "learning_rate": 1.7694090087337357e-05,
      "loss": 1.7874,
      "step": 507500
    },
    {
      "epoch": 38.77473073103659,
      "grad_norm": 5.267526149749756,
      "learning_rate": 1.7687724390802844e-05,
      "loss": 1.7834,
      "step": 507600
    },
    {
      "epoch": 38.78236956687801,
      "grad_norm": 5.816675662994385,
      "learning_rate": 1.7681358694268327e-05,
      "loss": 1.6759,
      "step": 507700
    },
    {
      "epoch": 38.79000840271942,
      "grad_norm": 5.436675071716309,
      "learning_rate": 1.7674992997733814e-05,
      "loss": 1.8125,
      "step": 507800
    },
    {
      "epoch": 38.79764723856084,
      "grad_norm": 3.2738821506500244,
      "learning_rate": 1.7668627301199298e-05,
      "loss": 1.7664,
      "step": 507900
    },
    {
      "epoch": 38.80528607440226,
      "grad_norm": 6.255436897277832,
      "learning_rate": 1.7662261604664785e-05,
      "loss": 1.6716,
      "step": 508000
    },
    {
      "epoch": 38.81292491024368,
      "grad_norm": 4.468188762664795,
      "learning_rate": 1.765589590813027e-05,
      "loss": 1.6559,
      "step": 508100
    },
    {
      "epoch": 38.8205637460851,
      "grad_norm": 5.8984551429748535,
      "learning_rate": 1.7649530211595755e-05,
      "loss": 1.6884,
      "step": 508200
    },
    {
      "epoch": 38.82820258192651,
      "grad_norm": 4.0664143562316895,
      "learning_rate": 1.764316451506124e-05,
      "loss": 1.7066,
      "step": 508300
    },
    {
      "epoch": 38.83584141776793,
      "grad_norm": 5.634781837463379,
      "learning_rate": 1.7636798818526722e-05,
      "loss": 1.6606,
      "step": 508400
    },
    {
      "epoch": 38.84348025360935,
      "grad_norm": 6.568236351013184,
      "learning_rate": 1.763043312199221e-05,
      "loss": 1.7353,
      "step": 508500
    },
    {
      "epoch": 38.85111908945077,
      "grad_norm": 5.733943462371826,
      "learning_rate": 1.7624067425457693e-05,
      "loss": 1.7641,
      "step": 508600
    },
    {
      "epoch": 38.85875792529219,
      "grad_norm": 7.795054912567139,
      "learning_rate": 1.761770172892318e-05,
      "loss": 1.7161,
      "step": 508700
    },
    {
      "epoch": 38.8663967611336,
      "grad_norm": 6.594485759735107,
      "learning_rate": 1.7611336032388663e-05,
      "loss": 1.6935,
      "step": 508800
    },
    {
      "epoch": 38.87403559697502,
      "grad_norm": 5.538825035095215,
      "learning_rate": 1.760497033585415e-05,
      "loss": 1.6732,
      "step": 508900
    },
    {
      "epoch": 38.88167443281644,
      "grad_norm": 4.944626331329346,
      "learning_rate": 1.7598604639319634e-05,
      "loss": 1.6424,
      "step": 509000
    },
    {
      "epoch": 38.88931326865786,
      "grad_norm": 5.960599422454834,
      "learning_rate": 1.759223894278512e-05,
      "loss": 1.7093,
      "step": 509100
    },
    {
      "epoch": 38.89695210449928,
      "grad_norm": 6.282641410827637,
      "learning_rate": 1.7585873246250608e-05,
      "loss": 1.7636,
      "step": 509200
    },
    {
      "epoch": 38.90459094034069,
      "grad_norm": 6.154107093811035,
      "learning_rate": 1.757950754971609e-05,
      "loss": 1.6182,
      "step": 509300
    },
    {
      "epoch": 38.91222977618211,
      "grad_norm": 8.195128440856934,
      "learning_rate": 1.7573141853181578e-05,
      "loss": 1.6907,
      "step": 509400
    },
    {
      "epoch": 38.91986861202353,
      "grad_norm": 7.046324729919434,
      "learning_rate": 1.756677615664706e-05,
      "loss": 1.761,
      "step": 509500
    },
    {
      "epoch": 38.92750744786495,
      "grad_norm": 5.631155014038086,
      "learning_rate": 1.756041046011255e-05,
      "loss": 1.6475,
      "step": 509600
    },
    {
      "epoch": 38.935146283706366,
      "grad_norm": 6.365618705749512,
      "learning_rate": 1.7554044763578032e-05,
      "loss": 1.6891,
      "step": 509700
    },
    {
      "epoch": 38.94278511954778,
      "grad_norm": 4.880665302276611,
      "learning_rate": 1.7547679067043516e-05,
      "loss": 1.6383,
      "step": 509800
    },
    {
      "epoch": 38.9504239553892,
      "grad_norm": 5.696164608001709,
      "learning_rate": 1.7541313370509003e-05,
      "loss": 1.6126,
      "step": 509900
    },
    {
      "epoch": 38.95806279123062,
      "grad_norm": 4.819591045379639,
      "learning_rate": 1.7534947673974486e-05,
      "loss": 1.6763,
      "step": 510000
    },
    {
      "epoch": 38.965701627072036,
      "grad_norm": 4.773796558380127,
      "learning_rate": 1.7528581977439973e-05,
      "loss": 1.7429,
      "step": 510100
    },
    {
      "epoch": 38.973340462913455,
      "grad_norm": 7.291500568389893,
      "learning_rate": 1.7522216280905457e-05,
      "loss": 1.761,
      "step": 510200
    },
    {
      "epoch": 38.98097929875487,
      "grad_norm": 4.773255825042725,
      "learning_rate": 1.7515850584370943e-05,
      "loss": 1.6159,
      "step": 510300
    },
    {
      "epoch": 38.98861813459629,
      "grad_norm": 6.04173469543457,
      "learning_rate": 1.7509484887836427e-05,
      "loss": 1.6485,
      "step": 510400
    },
    {
      "epoch": 38.996256970437706,
      "grad_norm": 5.402402400970459,
      "learning_rate": 1.7503119191301914e-05,
      "loss": 1.7331,
      "step": 510500
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.7705403566360474,
      "eval_runtime": 2.9968,
      "eval_samples_per_second": 230.242,
      "eval_steps_per_second": 230.242,
      "step": 510549
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.4516795873641968,
      "eval_runtime": 57.8368,
      "eval_samples_per_second": 226.344,
      "eval_steps_per_second": 226.344,
      "step": 510549
    },
    {
      "epoch": 39.003895806279125,
      "grad_norm": 5.609951019287109,
      "learning_rate": 1.7496753494767397e-05,
      "loss": 1.6806,
      "step": 510600
    },
    {
      "epoch": 39.01153464212054,
      "grad_norm": 5.833983898162842,
      "learning_rate": 1.749038779823288e-05,
      "loss": 1.6718,
      "step": 510700
    },
    {
      "epoch": 39.01917347796196,
      "grad_norm": 6.252645969390869,
      "learning_rate": 1.7484022101698368e-05,
      "loss": 1.695,
      "step": 510800
    },
    {
      "epoch": 39.026812313803376,
      "grad_norm": 5.961729526519775,
      "learning_rate": 1.747765640516385e-05,
      "loss": 1.6855,
      "step": 510900
    },
    {
      "epoch": 39.034451149644795,
      "grad_norm": 5.187579154968262,
      "learning_rate": 1.747129070862934e-05,
      "loss": 1.5763,
      "step": 511000
    },
    {
      "epoch": 39.042089985486214,
      "grad_norm": 7.17023229598999,
      "learning_rate": 1.7464925012094825e-05,
      "loss": 1.6941,
      "step": 511100
    },
    {
      "epoch": 39.049728821327626,
      "grad_norm": 4.87139892578125,
      "learning_rate": 1.7458559315560312e-05,
      "loss": 1.5906,
      "step": 511200
    },
    {
      "epoch": 39.057367657169046,
      "grad_norm": 5.696836948394775,
      "learning_rate": 1.7452193619025796e-05,
      "loss": 1.6484,
      "step": 511300
    },
    {
      "epoch": 39.065006493010465,
      "grad_norm": 5.312474727630615,
      "learning_rate": 1.744582792249128e-05,
      "loss": 1.7515,
      "step": 511400
    },
    {
      "epoch": 39.072645328851884,
      "grad_norm": 7.66087007522583,
      "learning_rate": 1.7439462225956766e-05,
      "loss": 1.7492,
      "step": 511500
    },
    {
      "epoch": 39.0802841646933,
      "grad_norm": 5.212421894073486,
      "learning_rate": 1.743309652942225e-05,
      "loss": 1.6659,
      "step": 511600
    },
    {
      "epoch": 39.087923000534715,
      "grad_norm": 4.789114952087402,
      "learning_rate": 1.7426730832887737e-05,
      "loss": 1.6779,
      "step": 511700
    },
    {
      "epoch": 39.095561836376135,
      "grad_norm": 8.608386039733887,
      "learning_rate": 1.742036513635322e-05,
      "loss": 1.7032,
      "step": 511800
    },
    {
      "epoch": 39.103200672217554,
      "grad_norm": 4.284245014190674,
      "learning_rate": 1.7413999439818707e-05,
      "loss": 1.7688,
      "step": 511900
    },
    {
      "epoch": 39.11083950805897,
      "grad_norm": 4.4499430656433105,
      "learning_rate": 1.740763374328419e-05,
      "loss": 1.5602,
      "step": 512000
    },
    {
      "epoch": 39.11847834390039,
      "grad_norm": 6.921934127807617,
      "learning_rate": 1.7401268046749678e-05,
      "loss": 1.7326,
      "step": 512100
    },
    {
      "epoch": 39.126117179741804,
      "grad_norm": 4.786986351013184,
      "learning_rate": 1.739490235021516e-05,
      "loss": 1.7897,
      "step": 512200
    },
    {
      "epoch": 39.133756015583224,
      "grad_norm": 9.176809310913086,
      "learning_rate": 1.7388536653680645e-05,
      "loss": 1.7051,
      "step": 512300
    },
    {
      "epoch": 39.14139485142464,
      "grad_norm": 7.747707366943359,
      "learning_rate": 1.738217095714613e-05,
      "loss": 1.7361,
      "step": 512400
    },
    {
      "epoch": 39.14903368726606,
      "grad_norm": 7.238940715789795,
      "learning_rate": 1.7375805260611615e-05,
      "loss": 1.7986,
      "step": 512500
    },
    {
      "epoch": 39.15667252310748,
      "grad_norm": 5.972296237945557,
      "learning_rate": 1.7369439564077102e-05,
      "loss": 1.7192,
      "step": 512600
    },
    {
      "epoch": 39.16431135894889,
      "grad_norm": 5.264621257781982,
      "learning_rate": 1.7363073867542586e-05,
      "loss": 1.5878,
      "step": 512700
    },
    {
      "epoch": 39.17195019479031,
      "grad_norm": 4.417184352874756,
      "learning_rate": 1.7356708171008073e-05,
      "loss": 1.7196,
      "step": 512800
    },
    {
      "epoch": 39.17958903063173,
      "grad_norm": 6.044865608215332,
      "learning_rate": 1.7350342474473556e-05,
      "loss": 1.6942,
      "step": 512900
    },
    {
      "epoch": 39.18722786647315,
      "grad_norm": 4.906814098358154,
      "learning_rate": 1.7343976777939043e-05,
      "loss": 1.6365,
      "step": 513000
    },
    {
      "epoch": 39.19486670231457,
      "grad_norm": 5.496377468109131,
      "learning_rate": 1.733761108140453e-05,
      "loss": 1.7192,
      "step": 513100
    },
    {
      "epoch": 39.20250553815598,
      "grad_norm": 4.573817253112793,
      "learning_rate": 1.7331245384870013e-05,
      "loss": 1.6382,
      "step": 513200
    },
    {
      "epoch": 39.2101443739974,
      "grad_norm": 5.31236457824707,
      "learning_rate": 1.73248796883355e-05,
      "loss": 1.7137,
      "step": 513300
    },
    {
      "epoch": 39.21778320983882,
      "grad_norm": 4.44277811050415,
      "learning_rate": 1.7318513991800984e-05,
      "loss": 1.6642,
      "step": 513400
    },
    {
      "epoch": 39.22542204568024,
      "grad_norm": 7.280062675476074,
      "learning_rate": 1.731214829526647e-05,
      "loss": 1.6736,
      "step": 513500
    },
    {
      "epoch": 39.23306088152166,
      "grad_norm": 7.002180576324463,
      "learning_rate": 1.7305782598731954e-05,
      "loss": 1.6928,
      "step": 513600
    },
    {
      "epoch": 39.24069971736307,
      "grad_norm": 6.677453517913818,
      "learning_rate": 1.729941690219744e-05,
      "loss": 1.698,
      "step": 513700
    },
    {
      "epoch": 39.24833855320449,
      "grad_norm": 5.7327880859375,
      "learning_rate": 1.7293051205662925e-05,
      "loss": 1.674,
      "step": 513800
    },
    {
      "epoch": 39.25597738904591,
      "grad_norm": 6.708446502685547,
      "learning_rate": 1.728668550912841e-05,
      "loss": 1.6563,
      "step": 513900
    },
    {
      "epoch": 39.26361622488733,
      "grad_norm": 4.833281517028809,
      "learning_rate": 1.7280319812593895e-05,
      "loss": 1.74,
      "step": 514000
    },
    {
      "epoch": 39.27125506072875,
      "grad_norm": 7.945830821990967,
      "learning_rate": 1.727395411605938e-05,
      "loss": 1.7097,
      "step": 514100
    },
    {
      "epoch": 39.27889389657016,
      "grad_norm": 5.4684247970581055,
      "learning_rate": 1.7267588419524866e-05,
      "loss": 1.7104,
      "step": 514200
    },
    {
      "epoch": 39.28653273241158,
      "grad_norm": 6.659455299377441,
      "learning_rate": 1.726122272299035e-05,
      "loss": 1.6093,
      "step": 514300
    },
    {
      "epoch": 39.294171568253,
      "grad_norm": 6.054856777191162,
      "learning_rate": 1.7254857026455836e-05,
      "loss": 1.7941,
      "step": 514400
    },
    {
      "epoch": 39.30181040409442,
      "grad_norm": 5.426356792449951,
      "learning_rate": 1.724849132992132e-05,
      "loss": 1.6826,
      "step": 514500
    },
    {
      "epoch": 39.30944923993584,
      "grad_norm": 6.18011999130249,
      "learning_rate": 1.7242125633386803e-05,
      "loss": 1.7445,
      "step": 514600
    },
    {
      "epoch": 39.31708807577725,
      "grad_norm": 5.298040866851807,
      "learning_rate": 1.723575993685229e-05,
      "loss": 1.704,
      "step": 514700
    },
    {
      "epoch": 39.32472691161867,
      "grad_norm": 5.782391548156738,
      "learning_rate": 1.7229394240317777e-05,
      "loss": 1.7344,
      "step": 514800
    },
    {
      "epoch": 39.33236574746009,
      "grad_norm": 6.061119079589844,
      "learning_rate": 1.722302854378326e-05,
      "loss": 1.7241,
      "step": 514900
    },
    {
      "epoch": 39.34000458330151,
      "grad_norm": 5.385603427886963,
      "learning_rate": 1.7216662847248748e-05,
      "loss": 1.7218,
      "step": 515000
    },
    {
      "epoch": 39.34764341914292,
      "grad_norm": 5.599390983581543,
      "learning_rate": 1.7210297150714235e-05,
      "loss": 1.6575,
      "step": 515100
    },
    {
      "epoch": 39.35528225498434,
      "grad_norm": 5.252690315246582,
      "learning_rate": 1.7203931454179718e-05,
      "loss": 1.6958,
      "step": 515200
    },
    {
      "epoch": 39.36292109082576,
      "grad_norm": 5.0807719230651855,
      "learning_rate": 1.7197565757645205e-05,
      "loss": 1.6157,
      "step": 515300
    },
    {
      "epoch": 39.37055992666718,
      "grad_norm": 5.762627601623535,
      "learning_rate": 1.719120006111069e-05,
      "loss": 1.7426,
      "step": 515400
    },
    {
      "epoch": 39.378198762508596,
      "grad_norm": 4.54915714263916,
      "learning_rate": 1.7184834364576172e-05,
      "loss": 1.6772,
      "step": 515500
    },
    {
      "epoch": 39.38583759835001,
      "grad_norm": 5.544641494750977,
      "learning_rate": 1.717846866804166e-05,
      "loss": 1.7289,
      "step": 515600
    },
    {
      "epoch": 39.39347643419143,
      "grad_norm": 5.626723766326904,
      "learning_rate": 1.7172102971507143e-05,
      "loss": 1.6929,
      "step": 515700
    },
    {
      "epoch": 39.40111527003285,
      "grad_norm": 5.426801681518555,
      "learning_rate": 1.716573727497263e-05,
      "loss": 1.6786,
      "step": 515800
    },
    {
      "epoch": 39.408754105874266,
      "grad_norm": 6.946484565734863,
      "learning_rate": 1.7159371578438113e-05,
      "loss": 1.6415,
      "step": 515900
    },
    {
      "epoch": 39.416392941715685,
      "grad_norm": 5.100653171539307,
      "learning_rate": 1.71530058819036e-05,
      "loss": 1.6488,
      "step": 516000
    },
    {
      "epoch": 39.4240317775571,
      "grad_norm": 5.63976526260376,
      "learning_rate": 1.7146640185369083e-05,
      "loss": 1.694,
      "step": 516100
    },
    {
      "epoch": 39.431670613398516,
      "grad_norm": 6.276580810546875,
      "learning_rate": 1.7140274488834567e-05,
      "loss": 1.6568,
      "step": 516200
    },
    {
      "epoch": 39.439309449239936,
      "grad_norm": 5.848723411560059,
      "learning_rate": 1.7133908792300054e-05,
      "loss": 1.6593,
      "step": 516300
    },
    {
      "epoch": 39.446948285081355,
      "grad_norm": 7.3442511558532715,
      "learning_rate": 1.7127543095765537e-05,
      "loss": 1.7095,
      "step": 516400
    },
    {
      "epoch": 39.454587120922774,
      "grad_norm": 7.947855472564697,
      "learning_rate": 1.7121177399231024e-05,
      "loss": 1.6557,
      "step": 516500
    },
    {
      "epoch": 39.462225956764186,
      "grad_norm": 5.960643291473389,
      "learning_rate": 1.7114811702696508e-05,
      "loss": 1.6469,
      "step": 516600
    },
    {
      "epoch": 39.469864792605605,
      "grad_norm": 4.874569892883301,
      "learning_rate": 1.7108446006161995e-05,
      "loss": 1.6719,
      "step": 516700
    },
    {
      "epoch": 39.477503628447025,
      "grad_norm": 6.47582483291626,
      "learning_rate": 1.7102080309627482e-05,
      "loss": 1.7057,
      "step": 516800
    },
    {
      "epoch": 39.485142464288444,
      "grad_norm": 7.298696041107178,
      "learning_rate": 1.7095714613092965e-05,
      "loss": 1.6242,
      "step": 516900
    },
    {
      "epoch": 39.49278130012986,
      "grad_norm": 5.141847610473633,
      "learning_rate": 1.7089348916558452e-05,
      "loss": 1.5696,
      "step": 517000
    },
    {
      "epoch": 39.500420135971275,
      "grad_norm": 6.389241695404053,
      "learning_rate": 1.7082983220023936e-05,
      "loss": 1.7397,
      "step": 517100
    },
    {
      "epoch": 39.508058971812694,
      "grad_norm": 6.718910217285156,
      "learning_rate": 1.7076617523489423e-05,
      "loss": 1.7502,
      "step": 517200
    },
    {
      "epoch": 39.515697807654114,
      "grad_norm": 6.814177989959717,
      "learning_rate": 1.7070251826954906e-05,
      "loss": 1.7245,
      "step": 517300
    },
    {
      "epoch": 39.52333664349553,
      "grad_norm": 7.211148738861084,
      "learning_rate": 1.7063886130420393e-05,
      "loss": 1.7471,
      "step": 517400
    },
    {
      "epoch": 39.53097547933695,
      "grad_norm": 5.831488132476807,
      "learning_rate": 1.7057520433885877e-05,
      "loss": 1.6547,
      "step": 517500
    },
    {
      "epoch": 39.538614315178364,
      "grad_norm": 6.777359485626221,
      "learning_rate": 1.7051154737351364e-05,
      "loss": 1.6746,
      "step": 517600
    },
    {
      "epoch": 39.54625315101978,
      "grad_norm": 5.255627155303955,
      "learning_rate": 1.7044789040816847e-05,
      "loss": 1.6815,
      "step": 517700
    },
    {
      "epoch": 39.5538919868612,
      "grad_norm": 5.8053083419799805,
      "learning_rate": 1.703842334428233e-05,
      "loss": 1.7789,
      "step": 517800
    },
    {
      "epoch": 39.56153082270262,
      "grad_norm": 5.574501991271973,
      "learning_rate": 1.7032057647747818e-05,
      "loss": 1.6492,
      "step": 517900
    },
    {
      "epoch": 39.56916965854404,
      "grad_norm": 6.792634963989258,
      "learning_rate": 1.70256919512133e-05,
      "loss": 1.6714,
      "step": 518000
    },
    {
      "epoch": 39.57680849438545,
      "grad_norm": 3.1919822692871094,
      "learning_rate": 1.7019326254678788e-05,
      "loss": 1.5889,
      "step": 518100
    },
    {
      "epoch": 39.58444733022687,
      "grad_norm": 5.209650993347168,
      "learning_rate": 1.701296055814427e-05,
      "loss": 1.6188,
      "step": 518200
    },
    {
      "epoch": 39.59208616606829,
      "grad_norm": 5.062025547027588,
      "learning_rate": 1.700659486160976e-05,
      "loss": 1.7315,
      "step": 518300
    },
    {
      "epoch": 39.59972500190971,
      "grad_norm": 4.790211200714111,
      "learning_rate": 1.7000229165075242e-05,
      "loss": 1.7194,
      "step": 518400
    },
    {
      "epoch": 39.60736383775113,
      "grad_norm": 4.078691482543945,
      "learning_rate": 1.6993863468540726e-05,
      "loss": 1.6955,
      "step": 518500
    },
    {
      "epoch": 39.61500267359254,
      "grad_norm": 7.698022365570068,
      "learning_rate": 1.6987497772006213e-05,
      "loss": 1.7118,
      "step": 518600
    },
    {
      "epoch": 39.62264150943396,
      "grad_norm": 5.431014537811279,
      "learning_rate": 1.69811320754717e-05,
      "loss": 1.7866,
      "step": 518700
    },
    {
      "epoch": 39.63028034527538,
      "grad_norm": 7.285569190979004,
      "learning_rate": 1.6974766378937186e-05,
      "loss": 1.7496,
      "step": 518800
    },
    {
      "epoch": 39.6379191811168,
      "grad_norm": 5.278969764709473,
      "learning_rate": 1.696840068240267e-05,
      "loss": 1.6812,
      "step": 518900
    },
    {
      "epoch": 39.64555801695822,
      "grad_norm": 4.878967761993408,
      "learning_rate": 1.6962034985868157e-05,
      "loss": 1.702,
      "step": 519000
    },
    {
      "epoch": 39.65319685279963,
      "grad_norm": 4.875666618347168,
      "learning_rate": 1.695566928933364e-05,
      "loss": 1.6584,
      "step": 519100
    },
    {
      "epoch": 39.66083568864105,
      "grad_norm": 6.155455112457275,
      "learning_rate": 1.6949303592799127e-05,
      "loss": 1.5951,
      "step": 519200
    },
    {
      "epoch": 39.66847452448247,
      "grad_norm": 5.7794976234436035,
      "learning_rate": 1.694293789626461e-05,
      "loss": 1.7546,
      "step": 519300
    },
    {
      "epoch": 39.67611336032389,
      "grad_norm": 5.196534156799316,
      "learning_rate": 1.6936572199730094e-05,
      "loss": 1.6127,
      "step": 519400
    },
    {
      "epoch": 39.6837521961653,
      "grad_norm": 5.409002780914307,
      "learning_rate": 1.693020650319558e-05,
      "loss": 1.7242,
      "step": 519500
    },
    {
      "epoch": 39.69139103200672,
      "grad_norm": 5.587547779083252,
      "learning_rate": 1.6923840806661065e-05,
      "loss": 1.7702,
      "step": 519600
    },
    {
      "epoch": 39.69902986784814,
      "grad_norm": 5.13830041885376,
      "learning_rate": 1.6917475110126552e-05,
      "loss": 1.7474,
      "step": 519700
    },
    {
      "epoch": 39.70666870368956,
      "grad_norm": 5.895874500274658,
      "learning_rate": 1.6911109413592035e-05,
      "loss": 1.7069,
      "step": 519800
    },
    {
      "epoch": 39.71430753953098,
      "grad_norm": 6.086788177490234,
      "learning_rate": 1.6904743717057522e-05,
      "loss": 1.5263,
      "step": 519900
    },
    {
      "epoch": 39.72194637537239,
      "grad_norm": 6.831451416015625,
      "learning_rate": 1.6898378020523006e-05,
      "loss": 1.6945,
      "step": 520000
    },
    {
      "epoch": 39.72958521121381,
      "grad_norm": 5.536256313323975,
      "learning_rate": 1.689201232398849e-05,
      "loss": 1.6831,
      "step": 520100
    },
    {
      "epoch": 39.73722404705523,
      "grad_norm": 6.111766815185547,
      "learning_rate": 1.6885646627453976e-05,
      "loss": 1.6287,
      "step": 520200
    },
    {
      "epoch": 39.74486288289665,
      "grad_norm": 7.444522380828857,
      "learning_rate": 1.687928093091946e-05,
      "loss": 1.7084,
      "step": 520300
    },
    {
      "epoch": 39.75250171873807,
      "grad_norm": 7.032271385192871,
      "learning_rate": 1.6872915234384947e-05,
      "loss": 1.7634,
      "step": 520400
    },
    {
      "epoch": 39.76014055457948,
      "grad_norm": 4.141106128692627,
      "learning_rate": 1.686654953785043e-05,
      "loss": 1.6284,
      "step": 520500
    },
    {
      "epoch": 39.7677793904209,
      "grad_norm": 4.8601908683776855,
      "learning_rate": 1.6860183841315917e-05,
      "loss": 1.6869,
      "step": 520600
    },
    {
      "epoch": 39.77541822626232,
      "grad_norm": 7.369577407836914,
      "learning_rate": 1.6853818144781404e-05,
      "loss": 1.6478,
      "step": 520700
    },
    {
      "epoch": 39.78305706210374,
      "grad_norm": 4.8093485832214355,
      "learning_rate": 1.684745244824689e-05,
      "loss": 1.688,
      "step": 520800
    },
    {
      "epoch": 39.790695897945156,
      "grad_norm": 5.195518970489502,
      "learning_rate": 1.6841086751712375e-05,
      "loss": 1.766,
      "step": 520900
    },
    {
      "epoch": 39.79833473378657,
      "grad_norm": 4.8614397048950195,
      "learning_rate": 1.6834721055177858e-05,
      "loss": 1.6556,
      "step": 521000
    },
    {
      "epoch": 39.80597356962799,
      "grad_norm": 5.890698432922363,
      "learning_rate": 1.6828355358643345e-05,
      "loss": 1.6677,
      "step": 521100
    },
    {
      "epoch": 39.81361240546941,
      "grad_norm": 5.699529647827148,
      "learning_rate": 1.682198966210883e-05,
      "loss": 1.6865,
      "step": 521200
    },
    {
      "epoch": 39.821251241310826,
      "grad_norm": 7.532620906829834,
      "learning_rate": 1.6815623965574315e-05,
      "loss": 1.6865,
      "step": 521300
    },
    {
      "epoch": 39.828890077152245,
      "grad_norm": 4.635317802429199,
      "learning_rate": 1.68092582690398e-05,
      "loss": 1.6879,
      "step": 521400
    },
    {
      "epoch": 39.83652891299366,
      "grad_norm": 7.906293869018555,
      "learning_rate": 1.6802892572505286e-05,
      "loss": 1.6868,
      "step": 521500
    },
    {
      "epoch": 39.844167748835076,
      "grad_norm": 5.462059497833252,
      "learning_rate": 1.679652687597077e-05,
      "loss": 1.8284,
      "step": 521600
    },
    {
      "epoch": 39.851806584676496,
      "grad_norm": 4.055054664611816,
      "learning_rate": 1.6790161179436253e-05,
      "loss": 1.6387,
      "step": 521700
    },
    {
      "epoch": 39.859445420517915,
      "grad_norm": 4.922047138214111,
      "learning_rate": 1.678379548290174e-05,
      "loss": 1.7218,
      "step": 521800
    },
    {
      "epoch": 39.867084256359334,
      "grad_norm": 6.373239517211914,
      "learning_rate": 1.6777429786367223e-05,
      "loss": 1.7016,
      "step": 521900
    },
    {
      "epoch": 39.874723092200746,
      "grad_norm": 5.1938581466674805,
      "learning_rate": 1.677106408983271e-05,
      "loss": 1.7106,
      "step": 522000
    },
    {
      "epoch": 39.882361928042165,
      "grad_norm": 4.590864181518555,
      "learning_rate": 1.6764698393298194e-05,
      "loss": 1.7913,
      "step": 522100
    },
    {
      "epoch": 39.890000763883585,
      "grad_norm": 4.462146282196045,
      "learning_rate": 1.675833269676368e-05,
      "loss": 1.7272,
      "step": 522200
    },
    {
      "epoch": 39.897639599725004,
      "grad_norm": 5.299127578735352,
      "learning_rate": 1.6751967000229164e-05,
      "loss": 1.7656,
      "step": 522300
    },
    {
      "epoch": 39.90527843556642,
      "grad_norm": 5.212259292602539,
      "learning_rate": 1.674560130369465e-05,
      "loss": 1.6623,
      "step": 522400
    },
    {
      "epoch": 39.912917271407835,
      "grad_norm": 6.113844871520996,
      "learning_rate": 1.6739235607160135e-05,
      "loss": 1.7006,
      "step": 522500
    },
    {
      "epoch": 39.920556107249254,
      "grad_norm": 5.669926166534424,
      "learning_rate": 1.6732869910625622e-05,
      "loss": 1.6828,
      "step": 522600
    },
    {
      "epoch": 39.92819494309067,
      "grad_norm": 5.511893272399902,
      "learning_rate": 1.672650421409111e-05,
      "loss": 1.7026,
      "step": 522700
    },
    {
      "epoch": 39.93583377893209,
      "grad_norm": 5.90743350982666,
      "learning_rate": 1.6720138517556592e-05,
      "loss": 1.7238,
      "step": 522800
    },
    {
      "epoch": 39.94347261477351,
      "grad_norm": 6.279882907867432,
      "learning_rate": 1.671377282102208e-05,
      "loss": 1.6966,
      "step": 522900
    },
    {
      "epoch": 39.951111450614924,
      "grad_norm": 6.312755107879639,
      "learning_rate": 1.6707407124487563e-05,
      "loss": 1.6145,
      "step": 523000
    },
    {
      "epoch": 39.95875028645634,
      "grad_norm": 4.214784145355225,
      "learning_rate": 1.670104142795305e-05,
      "loss": 1.6508,
      "step": 523100
    },
    {
      "epoch": 39.96638912229776,
      "grad_norm": 6.602724075317383,
      "learning_rate": 1.6694675731418533e-05,
      "loss": 1.6367,
      "step": 523200
    },
    {
      "epoch": 39.97402795813918,
      "grad_norm": 5.738419055938721,
      "learning_rate": 1.6688310034884017e-05,
      "loss": 1.7187,
      "step": 523300
    },
    {
      "epoch": 39.981666793980594,
      "grad_norm": 5.566035747528076,
      "learning_rate": 1.6681944338349504e-05,
      "loss": 1.676,
      "step": 523400
    },
    {
      "epoch": 39.98930562982201,
      "grad_norm": 5.6516432762146,
      "learning_rate": 1.6675578641814987e-05,
      "loss": 1.6693,
      "step": 523500
    },
    {
      "epoch": 39.99694446566343,
      "grad_norm": 3.926992416381836,
      "learning_rate": 1.6669212945280474e-05,
      "loss": 1.7879,
      "step": 523600
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.7711259126663208,
      "eval_runtime": 4.4697,
      "eval_samples_per_second": 154.374,
      "eval_steps_per_second": 154.374,
      "step": 523640
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.4483447074890137,
      "eval_runtime": 67.0897,
      "eval_samples_per_second": 195.127,
      "eval_steps_per_second": 195.127,
      "step": 523640
    },
    {
      "epoch": 40.00458330150485,
      "grad_norm": 5.884591102600098,
      "learning_rate": 1.6662847248745958e-05,
      "loss": 1.6726,
      "step": 523700
    },
    {
      "epoch": 40.01222213734627,
      "grad_norm": 4.397016525268555,
      "learning_rate": 1.6656481552211445e-05,
      "loss": 1.6586,
      "step": 523800
    },
    {
      "epoch": 40.01986097318768,
      "grad_norm": 5.067828178405762,
      "learning_rate": 1.6650115855676928e-05,
      "loss": 1.5884,
      "step": 523900
    },
    {
      "epoch": 40.0274998090291,
      "grad_norm": 6.06924295425415,
      "learning_rate": 1.6643750159142415e-05,
      "loss": 1.6835,
      "step": 524000
    },
    {
      "epoch": 40.03513864487052,
      "grad_norm": 6.450778007507324,
      "learning_rate": 1.66373844626079e-05,
      "loss": 1.6498,
      "step": 524100
    },
    {
      "epoch": 40.04277748071194,
      "grad_norm": 5.690211772918701,
      "learning_rate": 1.6631018766073382e-05,
      "loss": 1.6564,
      "step": 524200
    },
    {
      "epoch": 40.05041631655336,
      "grad_norm": 5.608642101287842,
      "learning_rate": 1.662465306953887e-05,
      "loss": 1.6229,
      "step": 524300
    },
    {
      "epoch": 40.05805515239477,
      "grad_norm": 6.899318218231201,
      "learning_rate": 1.6618287373004356e-05,
      "loss": 1.6623,
      "step": 524400
    },
    {
      "epoch": 40.06569398823619,
      "grad_norm": 7.1381659507751465,
      "learning_rate": 1.661192167646984e-05,
      "loss": 1.6685,
      "step": 524500
    },
    {
      "epoch": 40.07333282407761,
      "grad_norm": 4.886730194091797,
      "learning_rate": 1.6605555979935326e-05,
      "loss": 1.7266,
      "step": 524600
    },
    {
      "epoch": 40.08097165991903,
      "grad_norm": 6.030370235443115,
      "learning_rate": 1.6599190283400813e-05,
      "loss": 1.6853,
      "step": 524700
    },
    {
      "epoch": 40.08861049576045,
      "grad_norm": 5.529521942138672,
      "learning_rate": 1.6592824586866297e-05,
      "loss": 1.7085,
      "step": 524800
    },
    {
      "epoch": 40.09624933160186,
      "grad_norm": 6.206518173217773,
      "learning_rate": 1.658645889033178e-05,
      "loss": 1.674,
      "step": 524900
    },
    {
      "epoch": 40.10388816744328,
      "grad_norm": 5.30925989151001,
      "learning_rate": 1.6580093193797267e-05,
      "loss": 1.7088,
      "step": 525000
    },
    {
      "epoch": 40.1115270032847,
      "grad_norm": 6.670733451843262,
      "learning_rate": 1.657372749726275e-05,
      "loss": 1.7666,
      "step": 525100
    },
    {
      "epoch": 40.11916583912612,
      "grad_norm": 5.040129661560059,
      "learning_rate": 1.6567361800728238e-05,
      "loss": 1.7008,
      "step": 525200
    },
    {
      "epoch": 40.12680467496754,
      "grad_norm": 4.565901756286621,
      "learning_rate": 1.656099610419372e-05,
      "loss": 1.6767,
      "step": 525300
    },
    {
      "epoch": 40.13444351080895,
      "grad_norm": 4.94880485534668,
      "learning_rate": 1.6554630407659208e-05,
      "loss": 1.7114,
      "step": 525400
    },
    {
      "epoch": 40.14208234665037,
      "grad_norm": 5.969627380371094,
      "learning_rate": 1.6548264711124692e-05,
      "loss": 1.6304,
      "step": 525500
    },
    {
      "epoch": 40.14972118249179,
      "grad_norm": 5.619089126586914,
      "learning_rate": 1.6541899014590175e-05,
      "loss": 1.6466,
      "step": 525600
    },
    {
      "epoch": 40.15736001833321,
      "grad_norm": 7.311174392700195,
      "learning_rate": 1.6535533318055662e-05,
      "loss": 1.628,
      "step": 525700
    },
    {
      "epoch": 40.16499885417463,
      "grad_norm": 5.375792503356934,
      "learning_rate": 1.6529167621521146e-05,
      "loss": 1.7084,
      "step": 525800
    },
    {
      "epoch": 40.17263769001604,
      "grad_norm": 4.8012213706970215,
      "learning_rate": 1.6522801924986633e-05,
      "loss": 1.7681,
      "step": 525900
    },
    {
      "epoch": 40.18027652585746,
      "grad_norm": 4.757791519165039,
      "learning_rate": 1.6516436228452116e-05,
      "loss": 1.5717,
      "step": 526000
    },
    {
      "epoch": 40.18791536169888,
      "grad_norm": 3.741405725479126,
      "learning_rate": 1.6510070531917603e-05,
      "loss": 1.7001,
      "step": 526100
    },
    {
      "epoch": 40.1955541975403,
      "grad_norm": 5.635575294494629,
      "learning_rate": 1.6503704835383087e-05,
      "loss": 1.7858,
      "step": 526200
    },
    {
      "epoch": 40.203193033381716,
      "grad_norm": 5.6515679359436035,
      "learning_rate": 1.6497339138848574e-05,
      "loss": 1.7053,
      "step": 526300
    },
    {
      "epoch": 40.21083186922313,
      "grad_norm": 5.5840044021606445,
      "learning_rate": 1.649097344231406e-05,
      "loss": 1.6886,
      "step": 526400
    },
    {
      "epoch": 40.21847070506455,
      "grad_norm": 3.750199794769287,
      "learning_rate": 1.6484607745779544e-05,
      "loss": 1.7453,
      "step": 526500
    },
    {
      "epoch": 40.226109540905966,
      "grad_norm": 5.585111141204834,
      "learning_rate": 1.647824204924503e-05,
      "loss": 1.7291,
      "step": 526600
    },
    {
      "epoch": 40.233748376747386,
      "grad_norm": 6.742563724517822,
      "learning_rate": 1.6471876352710515e-05,
      "loss": 1.6421,
      "step": 526700
    },
    {
      "epoch": 40.241387212588805,
      "grad_norm": 6.579160690307617,
      "learning_rate": 1.6465510656176e-05,
      "loss": 1.7275,
      "step": 526800
    },
    {
      "epoch": 40.24902604843022,
      "grad_norm": 6.886378765106201,
      "learning_rate": 1.6459144959641485e-05,
      "loss": 1.7397,
      "step": 526900
    },
    {
      "epoch": 40.256664884271636,
      "grad_norm": 4.176815986633301,
      "learning_rate": 1.6452779263106972e-05,
      "loss": 1.6424,
      "step": 527000
    },
    {
      "epoch": 40.264303720113055,
      "grad_norm": 5.68301248550415,
      "learning_rate": 1.6446413566572455e-05,
      "loss": 1.7217,
      "step": 527100
    },
    {
      "epoch": 40.271942555954475,
      "grad_norm": 6.230128765106201,
      "learning_rate": 1.644004787003794e-05,
      "loss": 1.6262,
      "step": 527200
    },
    {
      "epoch": 40.279581391795894,
      "grad_norm": 6.018547058105469,
      "learning_rate": 1.6433682173503426e-05,
      "loss": 1.635,
      "step": 527300
    },
    {
      "epoch": 40.287220227637306,
      "grad_norm": 4.761445045471191,
      "learning_rate": 1.642731647696891e-05,
      "loss": 1.6325,
      "step": 527400
    },
    {
      "epoch": 40.294859063478725,
      "grad_norm": 6.324136257171631,
      "learning_rate": 1.6420950780434396e-05,
      "loss": 1.6626,
      "step": 527500
    },
    {
      "epoch": 40.302497899320144,
      "grad_norm": 5.196450233459473,
      "learning_rate": 1.641458508389988e-05,
      "loss": 1.6583,
      "step": 527600
    },
    {
      "epoch": 40.310136735161564,
      "grad_norm": 6.264503002166748,
      "learning_rate": 1.6408219387365367e-05,
      "loss": 1.7594,
      "step": 527700
    },
    {
      "epoch": 40.317775571002976,
      "grad_norm": 5.559698104858398,
      "learning_rate": 1.640185369083085e-05,
      "loss": 1.7214,
      "step": 527800
    },
    {
      "epoch": 40.325414406844395,
      "grad_norm": 6.944470405578613,
      "learning_rate": 1.6395487994296337e-05,
      "loss": 1.7115,
      "step": 527900
    },
    {
      "epoch": 40.333053242685814,
      "grad_norm": 6.7530131340026855,
      "learning_rate": 1.638912229776182e-05,
      "loss": 1.6667,
      "step": 528000
    },
    {
      "epoch": 40.34069207852723,
      "grad_norm": 6.049326419830322,
      "learning_rate": 1.6382756601227304e-05,
      "loss": 1.6392,
      "step": 528100
    },
    {
      "epoch": 40.34833091436865,
      "grad_norm": 5.154923439025879,
      "learning_rate": 1.637639090469279e-05,
      "loss": 1.6276,
      "step": 528200
    },
    {
      "epoch": 40.355969750210065,
      "grad_norm": 5.4355926513671875,
      "learning_rate": 1.6370025208158278e-05,
      "loss": 1.7343,
      "step": 528300
    },
    {
      "epoch": 40.363608586051484,
      "grad_norm": 4.817526340484619,
      "learning_rate": 1.6363659511623762e-05,
      "loss": 1.6958,
      "step": 528400
    },
    {
      "epoch": 40.3712474218929,
      "grad_norm": 6.591560363769531,
      "learning_rate": 1.635729381508925e-05,
      "loss": 1.7602,
      "step": 528500
    },
    {
      "epoch": 40.37888625773432,
      "grad_norm": 6.10250186920166,
      "learning_rate": 1.6350928118554736e-05,
      "loss": 1.7273,
      "step": 528600
    },
    {
      "epoch": 40.38652509357574,
      "grad_norm": 8.301921844482422,
      "learning_rate": 1.634456242202022e-05,
      "loss": 1.7386,
      "step": 528700
    },
    {
      "epoch": 40.394163929417154,
      "grad_norm": 7.151231288909912,
      "learning_rate": 1.6338196725485703e-05,
      "loss": 1.6658,
      "step": 528800
    },
    {
      "epoch": 40.40180276525857,
      "grad_norm": 6.278758525848389,
      "learning_rate": 1.633183102895119e-05,
      "loss": 1.6878,
      "step": 528900
    },
    {
      "epoch": 40.40944160109999,
      "grad_norm": 5.298990249633789,
      "learning_rate": 1.6325465332416673e-05,
      "loss": 1.6413,
      "step": 529000
    },
    {
      "epoch": 40.41708043694141,
      "grad_norm": 4.9093546867370605,
      "learning_rate": 1.631909963588216e-05,
      "loss": 1.8177,
      "step": 529100
    },
    {
      "epoch": 40.42471927278283,
      "grad_norm": 6.866766452789307,
      "learning_rate": 1.6312733939347644e-05,
      "loss": 1.7642,
      "step": 529200
    },
    {
      "epoch": 40.43235810862424,
      "grad_norm": 5.6200666427612305,
      "learning_rate": 1.630636824281313e-05,
      "loss": 1.7352,
      "step": 529300
    },
    {
      "epoch": 40.43999694446566,
      "grad_norm": 5.136236667633057,
      "learning_rate": 1.6300002546278614e-05,
      "loss": 1.7091,
      "step": 529400
    },
    {
      "epoch": 40.44763578030708,
      "grad_norm": 4.304256916046143,
      "learning_rate": 1.62936368497441e-05,
      "loss": 1.7187,
      "step": 529500
    },
    {
      "epoch": 40.4552746161485,
      "grad_norm": 5.905710220336914,
      "learning_rate": 1.6287271153209585e-05,
      "loss": 1.6749,
      "step": 529600
    },
    {
      "epoch": 40.46291345198992,
      "grad_norm": 6.598405361175537,
      "learning_rate": 1.6280905456675068e-05,
      "loss": 1.6434,
      "step": 529700
    },
    {
      "epoch": 40.47055228783133,
      "grad_norm": 6.37156867980957,
      "learning_rate": 1.6274539760140555e-05,
      "loss": 1.6998,
      "step": 529800
    },
    {
      "epoch": 40.47819112367275,
      "grad_norm": 5.229824066162109,
      "learning_rate": 1.626817406360604e-05,
      "loss": 1.6391,
      "step": 529900
    },
    {
      "epoch": 40.48582995951417,
      "grad_norm": 5.916285037994385,
      "learning_rate": 1.6261808367071525e-05,
      "loss": 1.6888,
      "step": 530000
    },
    {
      "epoch": 40.49346879535559,
      "grad_norm": 5.404869079589844,
      "learning_rate": 1.625544267053701e-05,
      "loss": 1.8533,
      "step": 530100
    },
    {
      "epoch": 40.50110763119701,
      "grad_norm": 5.683801651000977,
      "learning_rate": 1.6249076974002496e-05,
      "loss": 1.5498,
      "step": 530200
    },
    {
      "epoch": 40.50874646703842,
      "grad_norm": 6.202042102813721,
      "learning_rate": 1.6242711277467983e-05,
      "loss": 1.7045,
      "step": 530300
    },
    {
      "epoch": 40.51638530287984,
      "grad_norm": 9.572297096252441,
      "learning_rate": 1.6236345580933466e-05,
      "loss": 1.7165,
      "step": 530400
    },
    {
      "epoch": 40.52402413872126,
      "grad_norm": 6.180325984954834,
      "learning_rate": 1.6229979884398953e-05,
      "loss": 1.6675,
      "step": 530500
    },
    {
      "epoch": 40.53166297456268,
      "grad_norm": 6.119084358215332,
      "learning_rate": 1.6223614187864437e-05,
      "loss": 1.7333,
      "step": 530600
    },
    {
      "epoch": 40.5393018104041,
      "grad_norm": 5.9381585121154785,
      "learning_rate": 1.6217248491329924e-05,
      "loss": 1.6651,
      "step": 530700
    },
    {
      "epoch": 40.54694064624551,
      "grad_norm": 8.057579040527344,
      "learning_rate": 1.6210882794795407e-05,
      "loss": 1.6562,
      "step": 530800
    },
    {
      "epoch": 40.55457948208693,
      "grad_norm": 7.033856391906738,
      "learning_rate": 1.6204517098260894e-05,
      "loss": 1.6694,
      "step": 530900
    },
    {
      "epoch": 40.56221831792835,
      "grad_norm": 5.213130474090576,
      "learning_rate": 1.6198151401726378e-05,
      "loss": 1.7172,
      "step": 531000
    },
    {
      "epoch": 40.56985715376977,
      "grad_norm": 5.169839382171631,
      "learning_rate": 1.6191785705191865e-05,
      "loss": 1.6432,
      "step": 531100
    },
    {
      "epoch": 40.57749598961119,
      "grad_norm": 6.5178542137146,
      "learning_rate": 1.6185420008657348e-05,
      "loss": 1.6737,
      "step": 531200
    },
    {
      "epoch": 40.5851348254526,
      "grad_norm": 6.977729320526123,
      "learning_rate": 1.6179054312122832e-05,
      "loss": 1.6778,
      "step": 531300
    },
    {
      "epoch": 40.59277366129402,
      "grad_norm": 5.9031548500061035,
      "learning_rate": 1.617268861558832e-05,
      "loss": 1.6696,
      "step": 531400
    },
    {
      "epoch": 40.60041249713544,
      "grad_norm": 6.0080647468566895,
      "learning_rate": 1.6166322919053802e-05,
      "loss": 1.6923,
      "step": 531500
    },
    {
      "epoch": 40.608051332976856,
      "grad_norm": 4.593063831329346,
      "learning_rate": 1.615995722251929e-05,
      "loss": 1.7257,
      "step": 531600
    },
    {
      "epoch": 40.615690168818276,
      "grad_norm": 4.573798179626465,
      "learning_rate": 1.6153591525984773e-05,
      "loss": 1.6896,
      "step": 531700
    },
    {
      "epoch": 40.62332900465969,
      "grad_norm": 4.572826385498047,
      "learning_rate": 1.614722582945026e-05,
      "loss": 1.738,
      "step": 531800
    },
    {
      "epoch": 40.63096784050111,
      "grad_norm": 6.625574111938477,
      "learning_rate": 1.6140860132915743e-05,
      "loss": 1.6402,
      "step": 531900
    },
    {
      "epoch": 40.638606676342526,
      "grad_norm": 4.864926815032959,
      "learning_rate": 1.613449443638123e-05,
      "loss": 1.7064,
      "step": 532000
    },
    {
      "epoch": 40.646245512183945,
      "grad_norm": 5.633861064910889,
      "learning_rate": 1.6128128739846714e-05,
      "loss": 1.6404,
      "step": 532100
    },
    {
      "epoch": 40.65388434802536,
      "grad_norm": 5.315274715423584,
      "learning_rate": 1.61217630433122e-05,
      "loss": 1.7296,
      "step": 532200
    },
    {
      "epoch": 40.66152318386678,
      "grad_norm": 7.253521919250488,
      "learning_rate": 1.6115397346777687e-05,
      "loss": 1.6599,
      "step": 532300
    },
    {
      "epoch": 40.669162019708196,
      "grad_norm": 5.169803142547607,
      "learning_rate": 1.610903165024317e-05,
      "loss": 1.5751,
      "step": 532400
    },
    {
      "epoch": 40.676800855549615,
      "grad_norm": 7.787304401397705,
      "learning_rate": 1.6102665953708658e-05,
      "loss": 1.748,
      "step": 532500
    },
    {
      "epoch": 40.684439691391034,
      "grad_norm": 4.23175048828125,
      "learning_rate": 1.609630025717414e-05,
      "loss": 1.5826,
      "step": 532600
    },
    {
      "epoch": 40.69207852723245,
      "grad_norm": 4.396202087402344,
      "learning_rate": 1.608993456063963e-05,
      "loss": 1.6432,
      "step": 532700
    },
    {
      "epoch": 40.699717363073866,
      "grad_norm": 5.865601539611816,
      "learning_rate": 1.6083568864105112e-05,
      "loss": 1.8778,
      "step": 532800
    },
    {
      "epoch": 40.707356198915285,
      "grad_norm": 4.673372745513916,
      "learning_rate": 1.6077203167570595e-05,
      "loss": 1.6248,
      "step": 532900
    },
    {
      "epoch": 40.714995034756704,
      "grad_norm": 6.611476898193359,
      "learning_rate": 1.6070837471036082e-05,
      "loss": 1.7074,
      "step": 533000
    },
    {
      "epoch": 40.72263387059812,
      "grad_norm": 4.971940994262695,
      "learning_rate": 1.6064471774501566e-05,
      "loss": 1.7071,
      "step": 533100
    },
    {
      "epoch": 40.730272706439536,
      "grad_norm": 6.428013324737549,
      "learning_rate": 1.6058106077967053e-05,
      "loss": 1.7301,
      "step": 533200
    },
    {
      "epoch": 40.737911542280955,
      "grad_norm": 5.1114420890808105,
      "learning_rate": 1.6051740381432536e-05,
      "loss": 1.7046,
      "step": 533300
    },
    {
      "epoch": 40.745550378122374,
      "grad_norm": 5.331705570220947,
      "learning_rate": 1.6045374684898023e-05,
      "loss": 1.6683,
      "step": 533400
    },
    {
      "epoch": 40.75318921396379,
      "grad_norm": 4.666200637817383,
      "learning_rate": 1.6039008988363507e-05,
      "loss": 1.6889,
      "step": 533500
    },
    {
      "epoch": 40.76082804980521,
      "grad_norm": 4.8365888595581055,
      "learning_rate": 1.603264329182899e-05,
      "loss": 1.603,
      "step": 533600
    },
    {
      "epoch": 40.768466885646625,
      "grad_norm": 4.788589000701904,
      "learning_rate": 1.6026277595294477e-05,
      "loss": 1.6727,
      "step": 533700
    },
    {
      "epoch": 40.776105721488044,
      "grad_norm": 5.756982326507568,
      "learning_rate": 1.601991189875996e-05,
      "loss": 1.741,
      "step": 533800
    },
    {
      "epoch": 40.78374455732946,
      "grad_norm": 5.060156345367432,
      "learning_rate": 1.6013546202225448e-05,
      "loss": 1.8189,
      "step": 533900
    },
    {
      "epoch": 40.79138339317088,
      "grad_norm": 5.117401599884033,
      "learning_rate": 1.6007180505690935e-05,
      "loss": 1.6399,
      "step": 534000
    },
    {
      "epoch": 40.7990222290123,
      "grad_norm": 5.373307228088379,
      "learning_rate": 1.6000814809156418e-05,
      "loss": 1.7283,
      "step": 534100
    },
    {
      "epoch": 40.806661064853714,
      "grad_norm": 5.353148937225342,
      "learning_rate": 1.5994449112621905e-05,
      "loss": 1.7229,
      "step": 534200
    },
    {
      "epoch": 40.81429990069513,
      "grad_norm": 4.734427452087402,
      "learning_rate": 1.598808341608739e-05,
      "loss": 1.7002,
      "step": 534300
    },
    {
      "epoch": 40.82193873653655,
      "grad_norm": 5.601449489593506,
      "learning_rate": 1.5981717719552876e-05,
      "loss": 1.6128,
      "step": 534400
    },
    {
      "epoch": 40.82957757237797,
      "grad_norm": 5.313391208648682,
      "learning_rate": 1.597535202301836e-05,
      "loss": 1.7486,
      "step": 534500
    },
    {
      "epoch": 40.83721640821939,
      "grad_norm": 4.950139999389648,
      "learning_rate": 1.5968986326483846e-05,
      "loss": 1.6331,
      "step": 534600
    },
    {
      "epoch": 40.8448552440608,
      "grad_norm": 6.2116522789001465,
      "learning_rate": 1.596262062994933e-05,
      "loss": 1.6818,
      "step": 534700
    },
    {
      "epoch": 40.85249407990222,
      "grad_norm": 3.8380846977233887,
      "learning_rate": 1.5956254933414817e-05,
      "loss": 1.6501,
      "step": 534800
    },
    {
      "epoch": 40.86013291574364,
      "grad_norm": 5.657947063446045,
      "learning_rate": 1.59498892368803e-05,
      "loss": 1.693,
      "step": 534900
    },
    {
      "epoch": 40.86777175158506,
      "grad_norm": 6.390317916870117,
      "learning_rate": 1.5943523540345787e-05,
      "loss": 1.6802,
      "step": 535000
    },
    {
      "epoch": 40.87541058742648,
      "grad_norm": 5.915256500244141,
      "learning_rate": 1.593715784381127e-05,
      "loss": 1.7535,
      "step": 535100
    },
    {
      "epoch": 40.88304942326789,
      "grad_norm": 7.038327217102051,
      "learning_rate": 1.5930792147276754e-05,
      "loss": 1.6324,
      "step": 535200
    },
    {
      "epoch": 40.89068825910931,
      "grad_norm": 6.222355365753174,
      "learning_rate": 1.592442645074224e-05,
      "loss": 1.6496,
      "step": 535300
    },
    {
      "epoch": 40.89832709495073,
      "grad_norm": 4.905177116394043,
      "learning_rate": 1.5918060754207725e-05,
      "loss": 1.7545,
      "step": 535400
    },
    {
      "epoch": 40.90596593079215,
      "grad_norm": 7.195612907409668,
      "learning_rate": 1.591169505767321e-05,
      "loss": 1.592,
      "step": 535500
    },
    {
      "epoch": 40.91360476663357,
      "grad_norm": 14.664935111999512,
      "learning_rate": 1.5905329361138695e-05,
      "loss": 1.6902,
      "step": 535600
    },
    {
      "epoch": 40.92124360247498,
      "grad_norm": 4.441144943237305,
      "learning_rate": 1.5898963664604182e-05,
      "loss": 1.7354,
      "step": 535700
    },
    {
      "epoch": 40.9288824383164,
      "grad_norm": 5.949670314788818,
      "learning_rate": 1.5892597968069665e-05,
      "loss": 1.6633,
      "step": 535800
    },
    {
      "epoch": 40.93652127415782,
      "grad_norm": 5.765960216522217,
      "learning_rate": 1.5886232271535152e-05,
      "loss": 1.6832,
      "step": 535900
    },
    {
      "epoch": 40.94416010999924,
      "grad_norm": 7.051271915435791,
      "learning_rate": 1.5879866575000636e-05,
      "loss": 1.783,
      "step": 536000
    },
    {
      "epoch": 40.95179894584065,
      "grad_norm": 8.47215747833252,
      "learning_rate": 1.5873500878466123e-05,
      "loss": 1.706,
      "step": 536100
    },
    {
      "epoch": 40.95943778168207,
      "grad_norm": 4.137965679168701,
      "learning_rate": 1.586713518193161e-05,
      "loss": 1.7174,
      "step": 536200
    },
    {
      "epoch": 40.96707661752349,
      "grad_norm": 5.8989362716674805,
      "learning_rate": 1.5860769485397093e-05,
      "loss": 1.752,
      "step": 536300
    },
    {
      "epoch": 40.97471545336491,
      "grad_norm": 4.629089832305908,
      "learning_rate": 1.585440378886258e-05,
      "loss": 1.6527,
      "step": 536400
    },
    {
      "epoch": 40.98235428920633,
      "grad_norm": 5.900635242462158,
      "learning_rate": 1.5848038092328064e-05,
      "loss": 1.6904,
      "step": 536500
    },
    {
      "epoch": 40.98999312504774,
      "grad_norm": 6.917508125305176,
      "learning_rate": 1.584167239579355e-05,
      "loss": 1.6371,
      "step": 536600
    },
    {
      "epoch": 40.99763196088916,
      "grad_norm": 6.876604080200195,
      "learning_rate": 1.5835306699259034e-05,
      "loss": 1.6578,
      "step": 536700
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.7771880626678467,
      "eval_runtime": 4.2989,
      "eval_samples_per_second": 160.506,
      "eval_steps_per_second": 160.506,
      "step": 536731
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.4492918252944946,
      "eval_runtime": 70.6944,
      "eval_samples_per_second": 185.177,
      "eval_steps_per_second": 185.177,
      "step": 536731
    },
    {
      "epoch": 41.00527079673058,
      "grad_norm": 5.814488887786865,
      "learning_rate": 1.5828941002724518e-05,
      "loss": 1.7034,
      "step": 536800
    },
    {
      "epoch": 41.012909632572,
      "grad_norm": 6.106342792510986,
      "learning_rate": 1.5822575306190005e-05,
      "loss": 1.6819,
      "step": 536900
    },
    {
      "epoch": 41.020548468413416,
      "grad_norm": 4.867318153381348,
      "learning_rate": 1.5816209609655488e-05,
      "loss": 1.6823,
      "step": 537000
    },
    {
      "epoch": 41.02818730425483,
      "grad_norm": 5.225308418273926,
      "learning_rate": 1.5809843913120975e-05,
      "loss": 1.6383,
      "step": 537100
    },
    {
      "epoch": 41.03582614009625,
      "grad_norm": 6.168756484985352,
      "learning_rate": 1.580347821658646e-05,
      "loss": 1.6067,
      "step": 537200
    },
    {
      "epoch": 41.04346497593767,
      "grad_norm": 6.121972560882568,
      "learning_rate": 1.5797112520051946e-05,
      "loss": 1.5685,
      "step": 537300
    },
    {
      "epoch": 41.051103811779086,
      "grad_norm": 7.131007671356201,
      "learning_rate": 1.579074682351743e-05,
      "loss": 1.6173,
      "step": 537400
    },
    {
      "epoch": 41.058742647620505,
      "grad_norm": 5.398599147796631,
      "learning_rate": 1.5784381126982913e-05,
      "loss": 1.7467,
      "step": 537500
    },
    {
      "epoch": 41.06638148346192,
      "grad_norm": 6.2779974937438965,
      "learning_rate": 1.57780154304484e-05,
      "loss": 1.6993,
      "step": 537600
    },
    {
      "epoch": 41.07402031930334,
      "grad_norm": 4.398294925689697,
      "learning_rate": 1.5771649733913883e-05,
      "loss": 1.7048,
      "step": 537700
    },
    {
      "epoch": 41.081659155144756,
      "grad_norm": 5.776651382446289,
      "learning_rate": 1.576528403737937e-05,
      "loss": 1.6583,
      "step": 537800
    },
    {
      "epoch": 41.089297990986175,
      "grad_norm": 8.915870666503906,
      "learning_rate": 1.5758918340844857e-05,
      "loss": 1.6587,
      "step": 537900
    },
    {
      "epoch": 41.096936826827594,
      "grad_norm": 6.34816837310791,
      "learning_rate": 1.575255264431034e-05,
      "loss": 1.6419,
      "step": 538000
    },
    {
      "epoch": 41.104575662669006,
      "grad_norm": 5.40307092666626,
      "learning_rate": 1.5746186947775827e-05,
      "loss": 1.6604,
      "step": 538100
    },
    {
      "epoch": 41.112214498510426,
      "grad_norm": 4.809548377990723,
      "learning_rate": 1.5739821251241314e-05,
      "loss": 1.6863,
      "step": 538200
    },
    {
      "epoch": 41.119853334351845,
      "grad_norm": 9.601129531860352,
      "learning_rate": 1.5733455554706798e-05,
      "loss": 1.6491,
      "step": 538300
    },
    {
      "epoch": 41.127492170193264,
      "grad_norm": 6.177089214324951,
      "learning_rate": 1.572708985817228e-05,
      "loss": 1.6477,
      "step": 538400
    },
    {
      "epoch": 41.13513100603468,
      "grad_norm": 8.397010803222656,
      "learning_rate": 1.572072416163777e-05,
      "loss": 1.7234,
      "step": 538500
    },
    {
      "epoch": 41.142769841876095,
      "grad_norm": 6.2227935791015625,
      "learning_rate": 1.5714358465103252e-05,
      "loss": 1.6679,
      "step": 538600
    },
    {
      "epoch": 41.150408677717515,
      "grad_norm": 5.575841426849365,
      "learning_rate": 1.570799276856874e-05,
      "loss": 1.6817,
      "step": 538700
    },
    {
      "epoch": 41.158047513558934,
      "grad_norm": 5.361269474029541,
      "learning_rate": 1.5701627072034222e-05,
      "loss": 1.7509,
      "step": 538800
    },
    {
      "epoch": 41.16568634940035,
      "grad_norm": 4.665386199951172,
      "learning_rate": 1.569526137549971e-05,
      "loss": 1.5569,
      "step": 538900
    },
    {
      "epoch": 41.17332518524177,
      "grad_norm": 4.098054885864258,
      "learning_rate": 1.5688895678965193e-05,
      "loss": 1.7589,
      "step": 539000
    },
    {
      "epoch": 41.180964021083184,
      "grad_norm": 6.44225549697876,
      "learning_rate": 1.5682529982430676e-05,
      "loss": 1.7739,
      "step": 539100
    },
    {
      "epoch": 41.188602856924604,
      "grad_norm": 5.7512335777282715,
      "learning_rate": 1.5676164285896163e-05,
      "loss": 1.6299,
      "step": 539200
    },
    {
      "epoch": 41.19624169276602,
      "grad_norm": 5.283498287200928,
      "learning_rate": 1.5669798589361647e-05,
      "loss": 1.668,
      "step": 539300
    },
    {
      "epoch": 41.20388052860744,
      "grad_norm": 5.207051753997803,
      "learning_rate": 1.5663432892827134e-05,
      "loss": 1.7388,
      "step": 539400
    },
    {
      "epoch": 41.21151936444886,
      "grad_norm": 4.910832405090332,
      "learning_rate": 1.5657067196292617e-05,
      "loss": 1.7258,
      "step": 539500
    },
    {
      "epoch": 41.21915820029027,
      "grad_norm": 5.887693405151367,
      "learning_rate": 1.5650701499758104e-05,
      "loss": 1.716,
      "step": 539600
    },
    {
      "epoch": 41.22679703613169,
      "grad_norm": 4.62813138961792,
      "learning_rate": 1.5644335803223588e-05,
      "loss": 1.6053,
      "step": 539700
    },
    {
      "epoch": 41.23443587197311,
      "grad_norm": 2.3315067291259766,
      "learning_rate": 1.5637970106689075e-05,
      "loss": 1.6857,
      "step": 539800
    },
    {
      "epoch": 41.24207470781453,
      "grad_norm": 5.295918941497803,
      "learning_rate": 1.563160441015456e-05,
      "loss": 1.7088,
      "step": 539900
    },
    {
      "epoch": 41.24971354365595,
      "grad_norm": 5.632903099060059,
      "learning_rate": 1.5625238713620045e-05,
      "loss": 1.808,
      "step": 540000
    },
    {
      "epoch": 41.25735237949736,
      "grad_norm": 5.904195785522461,
      "learning_rate": 1.5618873017085532e-05,
      "loss": 1.7713,
      "step": 540100
    },
    {
      "epoch": 41.26499121533878,
      "grad_norm": 5.399456024169922,
      "learning_rate": 1.5612507320551016e-05,
      "loss": 1.783,
      "step": 540200
    },
    {
      "epoch": 41.2726300511802,
      "grad_norm": 5.183536052703857,
      "learning_rate": 1.5606141624016503e-05,
      "loss": 1.7174,
      "step": 540300
    },
    {
      "epoch": 41.28026888702162,
      "grad_norm": 5.178740501403809,
      "learning_rate": 1.5599775927481986e-05,
      "loss": 1.6313,
      "step": 540400
    },
    {
      "epoch": 41.28790772286303,
      "grad_norm": 6.158990859985352,
      "learning_rate": 1.5593410230947473e-05,
      "loss": 1.6921,
      "step": 540500
    },
    {
      "epoch": 41.29554655870445,
      "grad_norm": 4.416592121124268,
      "learning_rate": 1.5587044534412957e-05,
      "loss": 1.5986,
      "step": 540600
    },
    {
      "epoch": 41.30318539454587,
      "grad_norm": 5.663374423980713,
      "learning_rate": 1.558067883787844e-05,
      "loss": 1.714,
      "step": 540700
    },
    {
      "epoch": 41.31082423038729,
      "grad_norm": 6.085068225860596,
      "learning_rate": 1.5574313141343927e-05,
      "loss": 1.6713,
      "step": 540800
    },
    {
      "epoch": 41.31846306622871,
      "grad_norm": 4.715085029602051,
      "learning_rate": 1.556794744480941e-05,
      "loss": 1.6704,
      "step": 540900
    },
    {
      "epoch": 41.32610190207012,
      "grad_norm": 5.73787260055542,
      "learning_rate": 1.5561581748274897e-05,
      "loss": 1.7998,
      "step": 541000
    },
    {
      "epoch": 41.33374073791154,
      "grad_norm": 4.699492454528809,
      "learning_rate": 1.555521605174038e-05,
      "loss": 1.6684,
      "step": 541100
    },
    {
      "epoch": 41.34137957375296,
      "grad_norm": 4.5812883377075195,
      "learning_rate": 1.5548850355205868e-05,
      "loss": 1.6438,
      "step": 541200
    },
    {
      "epoch": 41.34901840959438,
      "grad_norm": 4.075199127197266,
      "learning_rate": 1.554248465867135e-05,
      "loss": 1.762,
      "step": 541300
    },
    {
      "epoch": 41.3566572454358,
      "grad_norm": 3.9623217582702637,
      "learning_rate": 1.5536118962136835e-05,
      "loss": 1.7068,
      "step": 541400
    },
    {
      "epoch": 41.36429608127721,
      "grad_norm": 5.2234039306640625,
      "learning_rate": 1.5529753265602322e-05,
      "loss": 1.6938,
      "step": 541500
    },
    {
      "epoch": 41.37193491711863,
      "grad_norm": 5.345855236053467,
      "learning_rate": 1.5523387569067805e-05,
      "loss": 1.7947,
      "step": 541600
    },
    {
      "epoch": 41.37957375296005,
      "grad_norm": 7.3783369064331055,
      "learning_rate": 1.5517021872533292e-05,
      "loss": 1.6452,
      "step": 541700
    },
    {
      "epoch": 41.38721258880147,
      "grad_norm": 5.588844299316406,
      "learning_rate": 1.551065617599878e-05,
      "loss": 1.6941,
      "step": 541800
    },
    {
      "epoch": 41.39485142464289,
      "grad_norm": 5.72573184967041,
      "learning_rate": 1.5504290479464266e-05,
      "loss": 1.6963,
      "step": 541900
    },
    {
      "epoch": 41.4024902604843,
      "grad_norm": 4.805891036987305,
      "learning_rate": 1.549792478292975e-05,
      "loss": 1.6148,
      "step": 542000
    },
    {
      "epoch": 41.41012909632572,
      "grad_norm": 6.75877571105957,
      "learning_rate": 1.5491559086395237e-05,
      "loss": 1.6827,
      "step": 542100
    },
    {
      "epoch": 41.41776793216714,
      "grad_norm": 5.127388000488281,
      "learning_rate": 1.548519338986072e-05,
      "loss": 1.5864,
      "step": 542200
    },
    {
      "epoch": 41.42540676800856,
      "grad_norm": 6.195955753326416,
      "learning_rate": 1.5478827693326204e-05,
      "loss": 1.6779,
      "step": 542300
    },
    {
      "epoch": 41.433045603849976,
      "grad_norm": 6.038455009460449,
      "learning_rate": 1.547246199679169e-05,
      "loss": 1.6925,
      "step": 542400
    },
    {
      "epoch": 41.44068443969139,
      "grad_norm": 5.485677242279053,
      "learning_rate": 1.5466096300257174e-05,
      "loss": 1.6046,
      "step": 542500
    },
    {
      "epoch": 41.44832327553281,
      "grad_norm": 6.84433126449585,
      "learning_rate": 1.545973060372266e-05,
      "loss": 1.7264,
      "step": 542600
    },
    {
      "epoch": 41.45596211137423,
      "grad_norm": 6.284018039703369,
      "learning_rate": 1.5453364907188145e-05,
      "loss": 1.7256,
      "step": 542700
    },
    {
      "epoch": 41.463600947215646,
      "grad_norm": 5.372848033905029,
      "learning_rate": 1.544699921065363e-05,
      "loss": 1.6405,
      "step": 542800
    },
    {
      "epoch": 41.471239783057065,
      "grad_norm": 5.480966567993164,
      "learning_rate": 1.5440633514119115e-05,
      "loss": 1.7574,
      "step": 542900
    },
    {
      "epoch": 41.47887861889848,
      "grad_norm": 3.8313310146331787,
      "learning_rate": 1.54342678175846e-05,
      "loss": 1.77,
      "step": 543000
    },
    {
      "epoch": 41.4865174547399,
      "grad_norm": 4.8236236572265625,
      "learning_rate": 1.5427902121050086e-05,
      "loss": 1.7041,
      "step": 543100
    },
    {
      "epoch": 41.494156290581316,
      "grad_norm": 7.085505962371826,
      "learning_rate": 1.542153642451557e-05,
      "loss": 1.7369,
      "step": 543200
    },
    {
      "epoch": 41.501795126422735,
      "grad_norm": 5.530239105224609,
      "learning_rate": 1.5415170727981056e-05,
      "loss": 1.6361,
      "step": 543300
    },
    {
      "epoch": 41.509433962264154,
      "grad_norm": 6.941317558288574,
      "learning_rate": 1.540880503144654e-05,
      "loss": 1.6496,
      "step": 543400
    },
    {
      "epoch": 41.517072798105566,
      "grad_norm": 5.759117603302002,
      "learning_rate": 1.5402439334912026e-05,
      "loss": 1.6811,
      "step": 543500
    },
    {
      "epoch": 41.524711633946986,
      "grad_norm": 4.284043788909912,
      "learning_rate": 1.539607363837751e-05,
      "loss": 1.6808,
      "step": 543600
    },
    {
      "epoch": 41.532350469788405,
      "grad_norm": 5.106747150421143,
      "learning_rate": 1.5389707941842997e-05,
      "loss": 1.6473,
      "step": 543700
    },
    {
      "epoch": 41.539989305629824,
      "grad_norm": 6.295834541320801,
      "learning_rate": 1.5383342245308484e-05,
      "loss": 1.6627,
      "step": 543800
    },
    {
      "epoch": 41.54762814147124,
      "grad_norm": 4.587374210357666,
      "learning_rate": 1.5376976548773967e-05,
      "loss": 1.7654,
      "step": 543900
    },
    {
      "epoch": 41.555266977312655,
      "grad_norm": 5.5616374015808105,
      "learning_rate": 1.5370610852239454e-05,
      "loss": 1.6047,
      "step": 544000
    },
    {
      "epoch": 41.562905813154075,
      "grad_norm": 4.373710632324219,
      "learning_rate": 1.5364245155704938e-05,
      "loss": 1.7392,
      "step": 544100
    },
    {
      "epoch": 41.570544648995494,
      "grad_norm": 5.677994251251221,
      "learning_rate": 1.5357879459170425e-05,
      "loss": 1.6688,
      "step": 544200
    },
    {
      "epoch": 41.57818348483691,
      "grad_norm": 5.150476932525635,
      "learning_rate": 1.535151376263591e-05,
      "loss": 1.758,
      "step": 544300
    },
    {
      "epoch": 41.58582232067833,
      "grad_norm": 4.522743225097656,
      "learning_rate": 1.5345148066101395e-05,
      "loss": 1.6689,
      "step": 544400
    },
    {
      "epoch": 41.593461156519744,
      "grad_norm": 7.101710319519043,
      "learning_rate": 1.533878236956688e-05,
      "loss": 1.6898,
      "step": 544500
    },
    {
      "epoch": 41.60109999236116,
      "grad_norm": 5.974049091339111,
      "learning_rate": 1.5332416673032362e-05,
      "loss": 1.684,
      "step": 544600
    },
    {
      "epoch": 41.60873882820258,
      "grad_norm": 6.7687273025512695,
      "learning_rate": 1.532605097649785e-05,
      "loss": 1.6681,
      "step": 544700
    },
    {
      "epoch": 41.616377664044,
      "grad_norm": 4.99547815322876,
      "learning_rate": 1.5319685279963333e-05,
      "loss": 1.663,
      "step": 544800
    },
    {
      "epoch": 41.624016499885414,
      "grad_norm": 5.497094631195068,
      "learning_rate": 1.531331958342882e-05,
      "loss": 1.7322,
      "step": 544900
    },
    {
      "epoch": 41.63165533572683,
      "grad_norm": 6.13677978515625,
      "learning_rate": 1.5306953886894303e-05,
      "loss": 1.6838,
      "step": 545000
    },
    {
      "epoch": 41.63929417156825,
      "grad_norm": 5.986048698425293,
      "learning_rate": 1.530058819035979e-05,
      "loss": 1.7138,
      "step": 545100
    },
    {
      "epoch": 41.64693300740967,
      "grad_norm": 5.522103309631348,
      "learning_rate": 1.5294222493825274e-05,
      "loss": 1.6928,
      "step": 545200
    },
    {
      "epoch": 41.65457184325109,
      "grad_norm": 5.473409652709961,
      "learning_rate": 1.528785679729076e-05,
      "loss": 1.6974,
      "step": 545300
    },
    {
      "epoch": 41.6622106790925,
      "grad_norm": 3.8721983432769775,
      "learning_rate": 1.5281491100756244e-05,
      "loss": 1.6491,
      "step": 545400
    },
    {
      "epoch": 41.66984951493392,
      "grad_norm": 9.70792293548584,
      "learning_rate": 1.527512540422173e-05,
      "loss": 1.6357,
      "step": 545500
    },
    {
      "epoch": 41.67748835077534,
      "grad_norm": 7.0119452476501465,
      "learning_rate": 1.5268759707687215e-05,
      "loss": 1.6897,
      "step": 545600
    },
    {
      "epoch": 41.68512718661676,
      "grad_norm": 6.740281581878662,
      "learning_rate": 1.52623940111527e-05,
      "loss": 1.7381,
      "step": 545700
    },
    {
      "epoch": 41.69276602245818,
      "grad_norm": 5.378667831420898,
      "learning_rate": 1.5256028314618187e-05,
      "loss": 1.581,
      "step": 545800
    },
    {
      "epoch": 41.70040485829959,
      "grad_norm": 7.426781177520752,
      "learning_rate": 1.524966261808367e-05,
      "loss": 1.7213,
      "step": 545900
    },
    {
      "epoch": 41.70804369414101,
      "grad_norm": 5.272555828094482,
      "learning_rate": 1.5243296921549157e-05,
      "loss": 1.5881,
      "step": 546000
    },
    {
      "epoch": 41.71568252998243,
      "grad_norm": 8.847904205322266,
      "learning_rate": 1.5236931225014642e-05,
      "loss": 1.7574,
      "step": 546100
    },
    {
      "epoch": 41.72332136582385,
      "grad_norm": 6.25084114074707,
      "learning_rate": 1.5230565528480126e-05,
      "loss": 1.7737,
      "step": 546200
    },
    {
      "epoch": 41.73096020166527,
      "grad_norm": 7.362093925476074,
      "learning_rate": 1.5224199831945613e-05,
      "loss": 1.6653,
      "step": 546300
    },
    {
      "epoch": 41.73859903750668,
      "grad_norm": 5.077321529388428,
      "learning_rate": 1.5217834135411096e-05,
      "loss": 1.6385,
      "step": 546400
    },
    {
      "epoch": 41.7462378733481,
      "grad_norm": 6.0059051513671875,
      "learning_rate": 1.5211468438876583e-05,
      "loss": 1.7224,
      "step": 546500
    },
    {
      "epoch": 41.75387670918952,
      "grad_norm": 5.1323323249816895,
      "learning_rate": 1.5205102742342067e-05,
      "loss": 1.6884,
      "step": 546600
    },
    {
      "epoch": 41.76151554503094,
      "grad_norm": 5.575589656829834,
      "learning_rate": 1.5198737045807554e-05,
      "loss": 1.6787,
      "step": 546700
    },
    {
      "epoch": 41.76915438087236,
      "grad_norm": 4.771968841552734,
      "learning_rate": 1.5192371349273037e-05,
      "loss": 1.63,
      "step": 546800
    },
    {
      "epoch": 41.77679321671377,
      "grad_norm": 5.16580867767334,
      "learning_rate": 1.5186005652738524e-05,
      "loss": 1.6804,
      "step": 546900
    },
    {
      "epoch": 41.78443205255519,
      "grad_norm": 4.754772186279297,
      "learning_rate": 1.5179639956204008e-05,
      "loss": 1.6576,
      "step": 547000
    },
    {
      "epoch": 41.79207088839661,
      "grad_norm": 6.428387641906738,
      "learning_rate": 1.5173274259669493e-05,
      "loss": 1.6801,
      "step": 547100
    },
    {
      "epoch": 41.79970972423803,
      "grad_norm": 4.313982963562012,
      "learning_rate": 1.516690856313498e-05,
      "loss": 1.7019,
      "step": 547200
    },
    {
      "epoch": 41.80734856007945,
      "grad_norm": 6.5096893310546875,
      "learning_rate": 1.5160542866600464e-05,
      "loss": 1.6823,
      "step": 547300
    },
    {
      "epoch": 41.81498739592086,
      "grad_norm": 5.490476608276367,
      "learning_rate": 1.515417717006595e-05,
      "loss": 1.7346,
      "step": 547400
    },
    {
      "epoch": 41.82262623176228,
      "grad_norm": 7.107046604156494,
      "learning_rate": 1.5147811473531434e-05,
      "loss": 1.7323,
      "step": 547500
    },
    {
      "epoch": 41.8302650676037,
      "grad_norm": 6.721187114715576,
      "learning_rate": 1.5141445776996921e-05,
      "loss": 1.7599,
      "step": 547600
    },
    {
      "epoch": 41.83790390344512,
      "grad_norm": 4.6415696144104,
      "learning_rate": 1.5135080080462404e-05,
      "loss": 1.7823,
      "step": 547700
    },
    {
      "epoch": 41.845542739286536,
      "grad_norm": 5.237356662750244,
      "learning_rate": 1.5128714383927888e-05,
      "loss": 1.6795,
      "step": 547800
    },
    {
      "epoch": 41.85318157512795,
      "grad_norm": 6.825457572937012,
      "learning_rate": 1.5122348687393375e-05,
      "loss": 1.6676,
      "step": 547900
    },
    {
      "epoch": 41.86082041096937,
      "grad_norm": 9.018540382385254,
      "learning_rate": 1.511598299085886e-05,
      "loss": 1.659,
      "step": 548000
    },
    {
      "epoch": 41.86845924681079,
      "grad_norm": 5.7142767906188965,
      "learning_rate": 1.5109617294324347e-05,
      "loss": 1.6873,
      "step": 548100
    },
    {
      "epoch": 41.876098082652206,
      "grad_norm": 4.41254186630249,
      "learning_rate": 1.510325159778983e-05,
      "loss": 1.6914,
      "step": 548200
    },
    {
      "epoch": 41.883736918493625,
      "grad_norm": 6.591005325317383,
      "learning_rate": 1.5096885901255318e-05,
      "loss": 1.6443,
      "step": 548300
    },
    {
      "epoch": 41.89137575433504,
      "grad_norm": 5.721472263336182,
      "learning_rate": 1.5090520204720801e-05,
      "loss": 1.7264,
      "step": 548400
    },
    {
      "epoch": 41.899014590176456,
      "grad_norm": 5.413400173187256,
      "learning_rate": 1.5084154508186288e-05,
      "loss": 1.6414,
      "step": 548500
    },
    {
      "epoch": 41.906653426017876,
      "grad_norm": 5.609478950500488,
      "learning_rate": 1.5077788811651772e-05,
      "loss": 1.6242,
      "step": 548600
    },
    {
      "epoch": 41.914292261859295,
      "grad_norm": 5.509446620941162,
      "learning_rate": 1.5071423115117255e-05,
      "loss": 1.6858,
      "step": 548700
    },
    {
      "epoch": 41.92193109770071,
      "grad_norm": 4.812030792236328,
      "learning_rate": 1.5065057418582742e-05,
      "loss": 1.6787,
      "step": 548800
    },
    {
      "epoch": 41.929569933542126,
      "grad_norm": 6.124260902404785,
      "learning_rate": 1.5058691722048227e-05,
      "loss": 1.6692,
      "step": 548900
    },
    {
      "epoch": 41.937208769383545,
      "grad_norm": 5.853743553161621,
      "learning_rate": 1.5052326025513712e-05,
      "loss": 1.7397,
      "step": 549000
    },
    {
      "epoch": 41.944847605224965,
      "grad_norm": 5.4547224044799805,
      "learning_rate": 1.5045960328979198e-05,
      "loss": 1.6891,
      "step": 549100
    },
    {
      "epoch": 41.952486441066384,
      "grad_norm": 6.51530122756958,
      "learning_rate": 1.5039594632444685e-05,
      "loss": 1.6743,
      "step": 549200
    },
    {
      "epoch": 41.960125276907796,
      "grad_norm": 5.796194076538086,
      "learning_rate": 1.5033228935910168e-05,
      "loss": 1.6062,
      "step": 549300
    },
    {
      "epoch": 41.967764112749215,
      "grad_norm": 6.212568759918213,
      "learning_rate": 1.5026863239375652e-05,
      "loss": 1.7082,
      "step": 549400
    },
    {
      "epoch": 41.975402948590634,
      "grad_norm": 5.751699924468994,
      "learning_rate": 1.5020497542841139e-05,
      "loss": 1.672,
      "step": 549500
    },
    {
      "epoch": 41.983041784432054,
      "grad_norm": 4.796169281005859,
      "learning_rate": 1.5014131846306622e-05,
      "loss": 1.6175,
      "step": 549600
    },
    {
      "epoch": 41.99068062027347,
      "grad_norm": 4.828354835510254,
      "learning_rate": 1.5007766149772109e-05,
      "loss": 1.7299,
      "step": 549700
    },
    {
      "epoch": 41.998319456114885,
      "grad_norm": 5.823118209838867,
      "learning_rate": 1.5001400453237593e-05,
      "loss": 1.671,
      "step": 549800
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.7760710716247559,
      "eval_runtime": 4.0651,
      "eval_samples_per_second": 169.737,
      "eval_steps_per_second": 169.737,
      "step": 549822
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.446377158164978,
      "eval_runtime": 68.8816,
      "eval_samples_per_second": 190.051,
      "eval_steps_per_second": 190.051,
      "step": 549822
    },
    {
      "epoch": 42.005958291956304,
      "grad_norm": 4.603597640991211,
      "learning_rate": 1.499503475670308e-05,
      "loss": 1.6434,
      "step": 549900
    },
    {
      "epoch": 42.01359712779772,
      "grad_norm": 4.319990634918213,
      "learning_rate": 1.4988669060168565e-05,
      "loss": 1.7445,
      "step": 550000
    },
    {
      "epoch": 42.02123596363914,
      "grad_norm": 5.817945957183838,
      "learning_rate": 1.4982303363634048e-05,
      "loss": 1.6667,
      "step": 550100
    },
    {
      "epoch": 42.02887479948056,
      "grad_norm": 6.346928596496582,
      "learning_rate": 1.4975937667099535e-05,
      "loss": 1.7045,
      "step": 550200
    },
    {
      "epoch": 42.036513635321974,
      "grad_norm": 6.5910234451293945,
      "learning_rate": 1.4969571970565019e-05,
      "loss": 1.6193,
      "step": 550300
    },
    {
      "epoch": 42.04415247116339,
      "grad_norm": 7.392049789428711,
      "learning_rate": 1.4963206274030506e-05,
      "loss": 1.7225,
      "step": 550400
    },
    {
      "epoch": 42.05179130700481,
      "grad_norm": 4.519199371337891,
      "learning_rate": 1.495684057749599e-05,
      "loss": 1.6449,
      "step": 550500
    },
    {
      "epoch": 42.05943014284623,
      "grad_norm": 5.495881080627441,
      "learning_rate": 1.4950474880961476e-05,
      "loss": 1.6428,
      "step": 550600
    },
    {
      "epoch": 42.06706897868765,
      "grad_norm": 8.253615379333496,
      "learning_rate": 1.494410918442696e-05,
      "loss": 1.5866,
      "step": 550700
    },
    {
      "epoch": 42.07470781452906,
      "grad_norm": 5.570407390594482,
      "learning_rate": 1.4937743487892447e-05,
      "loss": 1.716,
      "step": 550800
    },
    {
      "epoch": 42.08234665037048,
      "grad_norm": 4.6044158935546875,
      "learning_rate": 1.4931377791357932e-05,
      "loss": 1.734,
      "step": 550900
    },
    {
      "epoch": 42.0899854862119,
      "grad_norm": 4.762157917022705,
      "learning_rate": 1.4925012094823415e-05,
      "loss": 1.7066,
      "step": 551000
    },
    {
      "epoch": 42.09762432205332,
      "grad_norm": 6.795443058013916,
      "learning_rate": 1.4918646398288902e-05,
      "loss": 1.6107,
      "step": 551100
    },
    {
      "epoch": 42.10526315789474,
      "grad_norm": 5.955743312835693,
      "learning_rate": 1.4912280701754386e-05,
      "loss": 1.6659,
      "step": 551200
    },
    {
      "epoch": 42.11290199373615,
      "grad_norm": 5.630720138549805,
      "learning_rate": 1.4905915005219873e-05,
      "loss": 1.6513,
      "step": 551300
    },
    {
      "epoch": 42.12054082957757,
      "grad_norm": 4.672736644744873,
      "learning_rate": 1.4899549308685356e-05,
      "loss": 1.6797,
      "step": 551400
    },
    {
      "epoch": 42.12817966541899,
      "grad_norm": 6.047415733337402,
      "learning_rate": 1.4893183612150843e-05,
      "loss": 1.7707,
      "step": 551500
    },
    {
      "epoch": 42.13581850126041,
      "grad_norm": 7.3538899421691895,
      "learning_rate": 1.4886817915616327e-05,
      "loss": 1.69,
      "step": 551600
    },
    {
      "epoch": 42.14345733710183,
      "grad_norm": 5.068541049957275,
      "learning_rate": 1.4880452219081812e-05,
      "loss": 1.6184,
      "step": 551700
    },
    {
      "epoch": 42.15109617294324,
      "grad_norm": 5.7288408279418945,
      "learning_rate": 1.4874086522547297e-05,
      "loss": 1.6477,
      "step": 551800
    },
    {
      "epoch": 42.15873500878466,
      "grad_norm": 4.066075801849365,
      "learning_rate": 1.4867720826012782e-05,
      "loss": 1.7056,
      "step": 551900
    },
    {
      "epoch": 42.16637384462608,
      "grad_norm": 5.485313415527344,
      "learning_rate": 1.486135512947827e-05,
      "loss": 1.7393,
      "step": 552000
    },
    {
      "epoch": 42.1740126804675,
      "grad_norm": 9.177181243896484,
      "learning_rate": 1.4854989432943753e-05,
      "loss": 1.7013,
      "step": 552100
    },
    {
      "epoch": 42.18165151630892,
      "grad_norm": 5.237245559692383,
      "learning_rate": 1.484862373640924e-05,
      "loss": 1.6284,
      "step": 552200
    },
    {
      "epoch": 42.18929035215033,
      "grad_norm": 6.118577003479004,
      "learning_rate": 1.4842258039874723e-05,
      "loss": 1.608,
      "step": 552300
    },
    {
      "epoch": 42.19692918799175,
      "grad_norm": 6.312287330627441,
      "learning_rate": 1.483589234334021e-05,
      "loss": 1.7263,
      "step": 552400
    },
    {
      "epoch": 42.20456802383317,
      "grad_norm": 7.4816060066223145,
      "learning_rate": 1.4829526646805694e-05,
      "loss": 1.7712,
      "step": 552500
    },
    {
      "epoch": 42.21220685967459,
      "grad_norm": 6.157604217529297,
      "learning_rate": 1.4823160950271177e-05,
      "loss": 1.6603,
      "step": 552600
    },
    {
      "epoch": 42.21984569551601,
      "grad_norm": 6.365086078643799,
      "learning_rate": 1.4816795253736664e-05,
      "loss": 1.6586,
      "step": 552700
    },
    {
      "epoch": 42.22748453135742,
      "grad_norm": 4.517831802368164,
      "learning_rate": 1.481042955720215e-05,
      "loss": 1.6298,
      "step": 552800
    },
    {
      "epoch": 42.23512336719884,
      "grad_norm": 6.807694435119629,
      "learning_rate": 1.4804063860667636e-05,
      "loss": 1.6226,
      "step": 552900
    },
    {
      "epoch": 42.24276220304026,
      "grad_norm": 6.230762958526611,
      "learning_rate": 1.479769816413312e-05,
      "loss": 1.6619,
      "step": 553000
    },
    {
      "epoch": 42.25040103888168,
      "grad_norm": 5.370612621307373,
      "learning_rate": 1.4791332467598607e-05,
      "loss": 1.6776,
      "step": 553100
    },
    {
      "epoch": 42.25803987472309,
      "grad_norm": 6.39830207824707,
      "learning_rate": 1.478496677106409e-05,
      "loss": 1.6956,
      "step": 553200
    },
    {
      "epoch": 42.26567871056451,
      "grad_norm": 7.014081954956055,
      "learning_rate": 1.4778601074529574e-05,
      "loss": 1.6105,
      "step": 553300
    },
    {
      "epoch": 42.27331754640593,
      "grad_norm": 4.406668186187744,
      "learning_rate": 1.4772235377995061e-05,
      "loss": 1.6671,
      "step": 553400
    },
    {
      "epoch": 42.280956382247346,
      "grad_norm": 4.170187950134277,
      "learning_rate": 1.4765869681460544e-05,
      "loss": 1.693,
      "step": 553500
    },
    {
      "epoch": 42.288595218088766,
      "grad_norm": 5.9035420417785645,
      "learning_rate": 1.4759503984926031e-05,
      "loss": 1.6845,
      "step": 553600
    },
    {
      "epoch": 42.29623405393018,
      "grad_norm": 6.392446041107178,
      "learning_rate": 1.4753138288391517e-05,
      "loss": 1.6636,
      "step": 553700
    },
    {
      "epoch": 42.3038728897716,
      "grad_norm": 6.616151332855225,
      "learning_rate": 1.4746772591857002e-05,
      "loss": 1.6812,
      "step": 553800
    },
    {
      "epoch": 42.311511725613016,
      "grad_norm": 6.786551475524902,
      "learning_rate": 1.4740406895322487e-05,
      "loss": 1.713,
      "step": 553900
    },
    {
      "epoch": 42.319150561454435,
      "grad_norm": 6.236376762390137,
      "learning_rate": 1.4734041198787974e-05,
      "loss": 1.6828,
      "step": 554000
    },
    {
      "epoch": 42.326789397295855,
      "grad_norm": 7.654984474182129,
      "learning_rate": 1.4727675502253458e-05,
      "loss": 1.6156,
      "step": 554100
    },
    {
      "epoch": 42.33442823313727,
      "grad_norm": 5.593729019165039,
      "learning_rate": 1.4721309805718941e-05,
      "loss": 1.6818,
      "step": 554200
    },
    {
      "epoch": 42.342067068978686,
      "grad_norm": 4.6861748695373535,
      "learning_rate": 1.4714944109184428e-05,
      "loss": 1.7011,
      "step": 554300
    },
    {
      "epoch": 42.349705904820105,
      "grad_norm": 4.906874656677246,
      "learning_rate": 1.4708578412649912e-05,
      "loss": 1.6474,
      "step": 554400
    },
    {
      "epoch": 42.357344740661524,
      "grad_norm": 5.2358479499816895,
      "learning_rate": 1.4702212716115398e-05,
      "loss": 1.5852,
      "step": 554500
    },
    {
      "epoch": 42.364983576502944,
      "grad_norm": 4.221368789672852,
      "learning_rate": 1.4695847019580882e-05,
      "loss": 1.6859,
      "step": 554600
    },
    {
      "epoch": 42.372622412344356,
      "grad_norm": 5.653530120849609,
      "learning_rate": 1.4689481323046369e-05,
      "loss": 1.665,
      "step": 554700
    },
    {
      "epoch": 42.380261248185775,
      "grad_norm": 7.428507328033447,
      "learning_rate": 1.4683115626511854e-05,
      "loss": 1.6652,
      "step": 554800
    },
    {
      "epoch": 42.387900084027194,
      "grad_norm": 7.332929611206055,
      "learning_rate": 1.4676749929977338e-05,
      "loss": 1.7016,
      "step": 554900
    },
    {
      "epoch": 42.39553891986861,
      "grad_norm": 6.214793682098389,
      "learning_rate": 1.4670384233442825e-05,
      "loss": 1.7483,
      "step": 555000
    },
    {
      "epoch": 42.40317775571003,
      "grad_norm": 5.272063732147217,
      "learning_rate": 1.4664018536908308e-05,
      "loss": 1.7018,
      "step": 555100
    },
    {
      "epoch": 42.410816591551445,
      "grad_norm": 5.818746566772461,
      "learning_rate": 1.4657652840373795e-05,
      "loss": 1.7047,
      "step": 555200
    },
    {
      "epoch": 42.418455427392864,
      "grad_norm": 5.409379959106445,
      "learning_rate": 1.4651287143839279e-05,
      "loss": 1.6858,
      "step": 555300
    },
    {
      "epoch": 42.42609426323428,
      "grad_norm": 4.739928722381592,
      "learning_rate": 1.4644921447304766e-05,
      "loss": 1.8498,
      "step": 555400
    },
    {
      "epoch": 42.4337330990757,
      "grad_norm": 5.70469331741333,
      "learning_rate": 1.4638555750770249e-05,
      "loss": 1.7484,
      "step": 555500
    },
    {
      "epoch": 42.44137193491712,
      "grad_norm": 5.0535712242126465,
      "learning_rate": 1.4632190054235736e-05,
      "loss": 1.7136,
      "step": 555600
    },
    {
      "epoch": 42.449010770758534,
      "grad_norm": 5.468496322631836,
      "learning_rate": 1.4625824357701221e-05,
      "loss": 1.7736,
      "step": 555700
    },
    {
      "epoch": 42.45664960659995,
      "grad_norm": 5.609159469604492,
      "learning_rate": 1.4619458661166705e-05,
      "loss": 1.6248,
      "step": 555800
    },
    {
      "epoch": 42.46428844244137,
      "grad_norm": 4.089473247528076,
      "learning_rate": 1.4613092964632192e-05,
      "loss": 1.7161,
      "step": 555900
    },
    {
      "epoch": 42.47192727828279,
      "grad_norm": 5.704797267913818,
      "learning_rate": 1.4606727268097675e-05,
      "loss": 1.7309,
      "step": 556000
    },
    {
      "epoch": 42.47956611412421,
      "grad_norm": 4.901934623718262,
      "learning_rate": 1.4600361571563162e-05,
      "loss": 1.6241,
      "step": 556100
    },
    {
      "epoch": 42.48720494996562,
      "grad_norm": 4.690131664276123,
      "learning_rate": 1.4593995875028646e-05,
      "loss": 1.6571,
      "step": 556200
    },
    {
      "epoch": 42.49484378580704,
      "grad_norm": 4.716534614562988,
      "learning_rate": 1.4587630178494133e-05,
      "loss": 1.7658,
      "step": 556300
    },
    {
      "epoch": 42.50248262164846,
      "grad_norm": 3.049886465072632,
      "learning_rate": 1.4581264481959616e-05,
      "loss": 1.7251,
      "step": 556400
    },
    {
      "epoch": 42.51012145748988,
      "grad_norm": 4.383362293243408,
      "learning_rate": 1.4574898785425101e-05,
      "loss": 1.606,
      "step": 556500
    },
    {
      "epoch": 42.5177602933313,
      "grad_norm": 4.373417377471924,
      "learning_rate": 1.4568533088890587e-05,
      "loss": 1.7707,
      "step": 556600
    },
    {
      "epoch": 42.52539912917271,
      "grad_norm": 5.908456325531006,
      "learning_rate": 1.4562167392356072e-05,
      "loss": 1.8588,
      "step": 556700
    },
    {
      "epoch": 42.53303796501413,
      "grad_norm": 5.168757915496826,
      "learning_rate": 1.4555801695821559e-05,
      "loss": 1.6332,
      "step": 556800
    },
    {
      "epoch": 42.54067680085555,
      "grad_norm": 5.370179653167725,
      "learning_rate": 1.4549435999287042e-05,
      "loss": 1.6476,
      "step": 556900
    },
    {
      "epoch": 42.54831563669697,
      "grad_norm": 5.5188374519348145,
      "learning_rate": 1.454307030275253e-05,
      "loss": 1.7383,
      "step": 557000
    },
    {
      "epoch": 42.55595447253839,
      "grad_norm": 4.900411605834961,
      "learning_rate": 1.4536704606218013e-05,
      "loss": 1.6604,
      "step": 557100
    },
    {
      "epoch": 42.5635933083798,
      "grad_norm": 5.280425548553467,
      "learning_rate": 1.45303389096835e-05,
      "loss": 1.7128,
      "step": 557200
    },
    {
      "epoch": 42.57123214422122,
      "grad_norm": 5.963978290557861,
      "learning_rate": 1.4523973213148983e-05,
      "loss": 1.6426,
      "step": 557300
    },
    {
      "epoch": 42.57887098006264,
      "grad_norm": 7.022675514221191,
      "learning_rate": 1.4517607516614467e-05,
      "loss": 1.6657,
      "step": 557400
    },
    {
      "epoch": 42.58650981590406,
      "grad_norm": 5.397556781768799,
      "learning_rate": 1.4511241820079954e-05,
      "loss": 1.6491,
      "step": 557500
    },
    {
      "epoch": 42.59414865174547,
      "grad_norm": 5.307620525360107,
      "learning_rate": 1.4504876123545439e-05,
      "loss": 1.667,
      "step": 557600
    },
    {
      "epoch": 42.60178748758689,
      "grad_norm": 7.237502574920654,
      "learning_rate": 1.4498510427010926e-05,
      "loss": 1.7411,
      "step": 557700
    },
    {
      "epoch": 42.60942632342831,
      "grad_norm": 5.922873497009277,
      "learning_rate": 1.449214473047641e-05,
      "loss": 1.7012,
      "step": 557800
    },
    {
      "epoch": 42.61706515926973,
      "grad_norm": 6.760730743408203,
      "learning_rate": 1.4485779033941896e-05,
      "loss": 1.6726,
      "step": 557900
    },
    {
      "epoch": 42.62470399511115,
      "grad_norm": 5.794750213623047,
      "learning_rate": 1.447941333740738e-05,
      "loss": 1.5718,
      "step": 558000
    },
    {
      "epoch": 42.63234283095256,
      "grad_norm": 4.651841163635254,
      "learning_rate": 1.4473047640872863e-05,
      "loss": 1.6544,
      "step": 558100
    },
    {
      "epoch": 42.63998166679398,
      "grad_norm": 5.620187759399414,
      "learning_rate": 1.446668194433835e-05,
      "loss": 1.6595,
      "step": 558200
    },
    {
      "epoch": 42.6476205026354,
      "grad_norm": 5.85942268371582,
      "learning_rate": 1.4460316247803834e-05,
      "loss": 1.6845,
      "step": 558300
    },
    {
      "epoch": 42.65525933847682,
      "grad_norm": 5.2324042320251465,
      "learning_rate": 1.445395055126932e-05,
      "loss": 1.7468,
      "step": 558400
    },
    {
      "epoch": 42.66289817431824,
      "grad_norm": 5.03507661819458,
      "learning_rate": 1.4447584854734806e-05,
      "loss": 1.7032,
      "step": 558500
    },
    {
      "epoch": 42.67053701015965,
      "grad_norm": 5.763736248016357,
      "learning_rate": 1.4441219158200291e-05,
      "loss": 1.5867,
      "step": 558600
    },
    {
      "epoch": 42.67817584600107,
      "grad_norm": 6.254916667938232,
      "learning_rate": 1.4434853461665776e-05,
      "loss": 1.6636,
      "step": 558700
    },
    {
      "epoch": 42.68581468184249,
      "grad_norm": 5.378329753875732,
      "learning_rate": 1.442848776513126e-05,
      "loss": 1.6182,
      "step": 558800
    },
    {
      "epoch": 42.693453517683906,
      "grad_norm": 4.9410200119018555,
      "learning_rate": 1.4422122068596747e-05,
      "loss": 1.8047,
      "step": 558900
    },
    {
      "epoch": 42.701092353525326,
      "grad_norm": 5.209792137145996,
      "learning_rate": 1.441575637206223e-05,
      "loss": 1.6299,
      "step": 559000
    },
    {
      "epoch": 42.70873118936674,
      "grad_norm": 5.6226725578308105,
      "learning_rate": 1.4409390675527717e-05,
      "loss": 1.6937,
      "step": 559100
    },
    {
      "epoch": 42.71637002520816,
      "grad_norm": 4.9967217445373535,
      "learning_rate": 1.4403024978993201e-05,
      "loss": 1.7663,
      "step": 559200
    },
    {
      "epoch": 42.724008861049576,
      "grad_norm": 6.848029613494873,
      "learning_rate": 1.4396659282458688e-05,
      "loss": 1.6187,
      "step": 559300
    },
    {
      "epoch": 42.731647696890995,
      "grad_norm": 4.0138044357299805,
      "learning_rate": 1.4390293585924171e-05,
      "loss": 1.701,
      "step": 559400
    },
    {
      "epoch": 42.739286532732415,
      "grad_norm": 6.5442094802856445,
      "learning_rate": 1.4383927889389658e-05,
      "loss": 1.6185,
      "step": 559500
    },
    {
      "epoch": 42.74692536857383,
      "grad_norm": 9.183457374572754,
      "learning_rate": 1.4377562192855144e-05,
      "loss": 1.8106,
      "step": 559600
    },
    {
      "epoch": 42.754564204415246,
      "grad_norm": 7.886317253112793,
      "learning_rate": 1.4371196496320627e-05,
      "loss": 1.6107,
      "step": 559700
    },
    {
      "epoch": 42.762203040256665,
      "grad_norm": 4.694963455200195,
      "learning_rate": 1.4364830799786114e-05,
      "loss": 1.624,
      "step": 559800
    },
    {
      "epoch": 42.769841876098084,
      "grad_norm": 5.892913341522217,
      "learning_rate": 1.4358465103251598e-05,
      "loss": 1.6136,
      "step": 559900
    },
    {
      "epoch": 42.7774807119395,
      "grad_norm": 5.184670925140381,
      "learning_rate": 1.4352099406717084e-05,
      "loss": 1.7121,
      "step": 560000
    },
    {
      "epoch": 42.785119547780916,
      "grad_norm": 4.717937469482422,
      "learning_rate": 1.4345733710182568e-05,
      "loss": 1.7017,
      "step": 560100
    },
    {
      "epoch": 42.792758383622335,
      "grad_norm": 5.624083518981934,
      "learning_rate": 1.4339368013648055e-05,
      "loss": 1.6432,
      "step": 560200
    },
    {
      "epoch": 42.800397219463754,
      "grad_norm": 4.985570430755615,
      "learning_rate": 1.4333002317113538e-05,
      "loss": 1.7581,
      "step": 560300
    },
    {
      "epoch": 42.80803605530517,
      "grad_norm": 6.220646858215332,
      "learning_rate": 1.4326636620579024e-05,
      "loss": 1.5871,
      "step": 560400
    },
    {
      "epoch": 42.81567489114659,
      "grad_norm": 5.752943992614746,
      "learning_rate": 1.432027092404451e-05,
      "loss": 1.669,
      "step": 560500
    },
    {
      "epoch": 42.823313726988005,
      "grad_norm": 5.303401947021484,
      "learning_rate": 1.4313905227509994e-05,
      "loss": 1.7162,
      "step": 560600
    },
    {
      "epoch": 42.830952562829424,
      "grad_norm": 7.244537830352783,
      "learning_rate": 1.4307539530975481e-05,
      "loss": 1.677,
      "step": 560700
    },
    {
      "epoch": 42.83859139867084,
      "grad_norm": 5.838322639465332,
      "learning_rate": 1.4301173834440965e-05,
      "loss": 1.7428,
      "step": 560800
    },
    {
      "epoch": 42.84623023451226,
      "grad_norm": 5.839036464691162,
      "learning_rate": 1.4294808137906452e-05,
      "loss": 1.76,
      "step": 560900
    },
    {
      "epoch": 42.85386907035368,
      "grad_norm": 5.717906475067139,
      "learning_rate": 1.4288442441371935e-05,
      "loss": 1.6488,
      "step": 561000
    },
    {
      "epoch": 42.861507906195094,
      "grad_norm": 4.678764820098877,
      "learning_rate": 1.4282076744837422e-05,
      "loss": 1.7952,
      "step": 561100
    },
    {
      "epoch": 42.86914674203651,
      "grad_norm": 6.2101640701293945,
      "learning_rate": 1.4275711048302906e-05,
      "loss": 1.6837,
      "step": 561200
    },
    {
      "epoch": 42.87678557787793,
      "grad_norm": 8.516865730285645,
      "learning_rate": 1.426934535176839e-05,
      "loss": 1.6061,
      "step": 561300
    },
    {
      "epoch": 42.88442441371935,
      "grad_norm": 6.696669578552246,
      "learning_rate": 1.4262979655233876e-05,
      "loss": 1.7315,
      "step": 561400
    },
    {
      "epoch": 42.89206324956076,
      "grad_norm": 6.230597496032715,
      "learning_rate": 1.4256613958699361e-05,
      "loss": 1.7353,
      "step": 561500
    },
    {
      "epoch": 42.89970208540218,
      "grad_norm": 4.685408592224121,
      "learning_rate": 1.4250248262164848e-05,
      "loss": 1.6599,
      "step": 561600
    },
    {
      "epoch": 42.9073409212436,
      "grad_norm": 6.2881035804748535,
      "learning_rate": 1.4243882565630332e-05,
      "loss": 1.7311,
      "step": 561700
    },
    {
      "epoch": 42.91497975708502,
      "grad_norm": 5.2588348388671875,
      "learning_rate": 1.4237516869095819e-05,
      "loss": 1.5906,
      "step": 561800
    },
    {
      "epoch": 42.92261859292644,
      "grad_norm": 5.501895427703857,
      "learning_rate": 1.4231151172561302e-05,
      "loss": 1.7702,
      "step": 561900
    },
    {
      "epoch": 42.93025742876785,
      "grad_norm": 4.719282150268555,
      "learning_rate": 1.4224785476026786e-05,
      "loss": 1.7298,
      "step": 562000
    },
    {
      "epoch": 42.93789626460927,
      "grad_norm": 4.6435980796813965,
      "learning_rate": 1.4218419779492273e-05,
      "loss": 1.7135,
      "step": 562100
    },
    {
      "epoch": 42.94553510045069,
      "grad_norm": 4.412672519683838,
      "learning_rate": 1.4212054082957756e-05,
      "loss": 1.6513,
      "step": 562200
    },
    {
      "epoch": 42.95317393629211,
      "grad_norm": 4.570460796356201,
      "learning_rate": 1.4205688386423243e-05,
      "loss": 1.5675,
      "step": 562300
    },
    {
      "epoch": 42.96081277213353,
      "grad_norm": 5.435142517089844,
      "learning_rate": 1.4199322689888728e-05,
      "loss": 1.7118,
      "step": 562400
    },
    {
      "epoch": 42.96845160797494,
      "grad_norm": 5.203220844268799,
      "learning_rate": 1.4192956993354215e-05,
      "loss": 1.627,
      "step": 562500
    },
    {
      "epoch": 42.97609044381636,
      "grad_norm": 7.191726207733154,
      "learning_rate": 1.4186591296819699e-05,
      "loss": 1.6466,
      "step": 562600
    },
    {
      "epoch": 42.98372927965778,
      "grad_norm": 6.006887435913086,
      "learning_rate": 1.4180225600285186e-05,
      "loss": 1.7167,
      "step": 562700
    },
    {
      "epoch": 42.9913681154992,
      "grad_norm": 7.078794956207275,
      "learning_rate": 1.417385990375067e-05,
      "loss": 1.7143,
      "step": 562800
    },
    {
      "epoch": 42.99900695134062,
      "grad_norm": 6.235076427459717,
      "learning_rate": 1.4167494207216153e-05,
      "loss": 1.6727,
      "step": 562900
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.7738361358642578,
      "eval_runtime": 3.5557,
      "eval_samples_per_second": 194.052,
      "eval_steps_per_second": 194.052,
      "step": 562913
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.4431347846984863,
      "eval_runtime": 66.238,
      "eval_samples_per_second": 197.636,
      "eval_steps_per_second": 197.636,
      "step": 562913
    },
    {
      "epoch": 43.00664578718203,
      "grad_norm": 4.657359600067139,
      "learning_rate": 1.416112851068164e-05,
      "loss": 1.7208,
      "step": 563000
    },
    {
      "epoch": 43.01428462302345,
      "grad_norm": 6.406615734100342,
      "learning_rate": 1.4154762814147123e-05,
      "loss": 1.5971,
      "step": 563100
    },
    {
      "epoch": 43.02192345886487,
      "grad_norm": 8.430609703063965,
      "learning_rate": 1.414839711761261e-05,
      "loss": 1.6531,
      "step": 563200
    },
    {
      "epoch": 43.02956229470629,
      "grad_norm": 4.817590713500977,
      "learning_rate": 1.4142031421078095e-05,
      "loss": 1.6775,
      "step": 563300
    },
    {
      "epoch": 43.03720113054771,
      "grad_norm": 3.6246249675750732,
      "learning_rate": 1.413566572454358e-05,
      "loss": 1.6919,
      "step": 563400
    },
    {
      "epoch": 43.04483996638912,
      "grad_norm": 6.471918106079102,
      "learning_rate": 1.4129300028009066e-05,
      "loss": 1.7176,
      "step": 563500
    },
    {
      "epoch": 43.05247880223054,
      "grad_norm": 6.7644429206848145,
      "learning_rate": 1.412293433147455e-05,
      "loss": 1.6383,
      "step": 563600
    },
    {
      "epoch": 43.06011763807196,
      "grad_norm": 6.620582103729248,
      "learning_rate": 1.4116568634940036e-05,
      "loss": 1.7174,
      "step": 563700
    },
    {
      "epoch": 43.06775647391338,
      "grad_norm": 6.8845014572143555,
      "learning_rate": 1.411020293840552e-05,
      "loss": 1.5934,
      "step": 563800
    },
    {
      "epoch": 43.075395309754796,
      "grad_norm": 6.08650016784668,
      "learning_rate": 1.4103837241871007e-05,
      "loss": 1.6171,
      "step": 563900
    },
    {
      "epoch": 43.08303414559621,
      "grad_norm": 5.541324615478516,
      "learning_rate": 1.409747154533649e-05,
      "loss": 1.7381,
      "step": 564000
    },
    {
      "epoch": 43.09067298143763,
      "grad_norm": 5.446498870849609,
      "learning_rate": 1.4091105848801977e-05,
      "loss": 1.6787,
      "step": 564100
    },
    {
      "epoch": 43.09831181727905,
      "grad_norm": 4.783714294433594,
      "learning_rate": 1.408474015226746e-05,
      "loss": 1.6994,
      "step": 564200
    },
    {
      "epoch": 43.105950653120466,
      "grad_norm": 5.3294219970703125,
      "learning_rate": 1.4078374455732948e-05,
      "loss": 1.8097,
      "step": 564300
    },
    {
      "epoch": 43.113589488961885,
      "grad_norm": 5.679892539978027,
      "learning_rate": 1.4072008759198433e-05,
      "loss": 1.6916,
      "step": 564400
    },
    {
      "epoch": 43.1212283248033,
      "grad_norm": 7.401609420776367,
      "learning_rate": 1.4065643062663916e-05,
      "loss": 1.8015,
      "step": 564500
    },
    {
      "epoch": 43.12886716064472,
      "grad_norm": 5.127277374267578,
      "learning_rate": 1.4059277366129403e-05,
      "loss": 1.5685,
      "step": 564600
    },
    {
      "epoch": 43.136505996486136,
      "grad_norm": 4.881677150726318,
      "learning_rate": 1.4052911669594887e-05,
      "loss": 1.6501,
      "step": 564700
    },
    {
      "epoch": 43.144144832327555,
      "grad_norm": 5.158393383026123,
      "learning_rate": 1.4046545973060374e-05,
      "loss": 1.662,
      "step": 564800
    },
    {
      "epoch": 43.151783668168974,
      "grad_norm": 5.554840564727783,
      "learning_rate": 1.4040180276525857e-05,
      "loss": 1.6628,
      "step": 564900
    },
    {
      "epoch": 43.15942250401039,
      "grad_norm": 5.072767734527588,
      "learning_rate": 1.4033814579991344e-05,
      "loss": 1.6463,
      "step": 565000
    },
    {
      "epoch": 43.167061339851806,
      "grad_norm": 5.878164768218994,
      "learning_rate": 1.4027448883456828e-05,
      "loss": 1.7162,
      "step": 565100
    },
    {
      "epoch": 43.174700175693225,
      "grad_norm": 5.178486347198486,
      "learning_rate": 1.4021083186922313e-05,
      "loss": 1.7788,
      "step": 565200
    },
    {
      "epoch": 43.182339011534644,
      "grad_norm": 6.0515923500061035,
      "learning_rate": 1.40147174903878e-05,
      "loss": 1.7038,
      "step": 565300
    },
    {
      "epoch": 43.18997784737606,
      "grad_norm": 5.087704181671143,
      "learning_rate": 1.4008351793853284e-05,
      "loss": 1.7423,
      "step": 565400
    },
    {
      "epoch": 43.197616683217475,
      "grad_norm": 5.388148784637451,
      "learning_rate": 1.400198609731877e-05,
      "loss": 1.6709,
      "step": 565500
    },
    {
      "epoch": 43.205255519058895,
      "grad_norm": 5.232319355010986,
      "learning_rate": 1.3995620400784254e-05,
      "loss": 1.6553,
      "step": 565600
    },
    {
      "epoch": 43.212894354900314,
      "grad_norm": 6.030848503112793,
      "learning_rate": 1.3989254704249741e-05,
      "loss": 1.6332,
      "step": 565700
    },
    {
      "epoch": 43.22053319074173,
      "grad_norm": 6.454248428344727,
      "learning_rate": 1.3982889007715224e-05,
      "loss": 1.6278,
      "step": 565800
    },
    {
      "epoch": 43.228172026583145,
      "grad_norm": 4.909491539001465,
      "learning_rate": 1.3976523311180708e-05,
      "loss": 1.7059,
      "step": 565900
    },
    {
      "epoch": 43.235810862424565,
      "grad_norm": 5.520809650421143,
      "learning_rate": 1.3970157614646195e-05,
      "loss": 1.6449,
      "step": 566000
    },
    {
      "epoch": 43.243449698265984,
      "grad_norm": 5.703177452087402,
      "learning_rate": 1.396379191811168e-05,
      "loss": 1.7073,
      "step": 566100
    },
    {
      "epoch": 43.2510885341074,
      "grad_norm": 6.395495414733887,
      "learning_rate": 1.3957426221577165e-05,
      "loss": 1.7056,
      "step": 566200
    },
    {
      "epoch": 43.25872736994882,
      "grad_norm": 6.629772186279297,
      "learning_rate": 1.395106052504265e-05,
      "loss": 1.6416,
      "step": 566300
    },
    {
      "epoch": 43.266366205790234,
      "grad_norm": 6.4083170890808105,
      "learning_rate": 1.3944694828508138e-05,
      "loss": 1.695,
      "step": 566400
    },
    {
      "epoch": 43.27400504163165,
      "grad_norm": 5.759671211242676,
      "learning_rate": 1.3938329131973621e-05,
      "loss": 1.6598,
      "step": 566500
    },
    {
      "epoch": 43.28164387747307,
      "grad_norm": 5.588140964508057,
      "learning_rate": 1.3931963435439108e-05,
      "loss": 1.6447,
      "step": 566600
    },
    {
      "epoch": 43.28928271331449,
      "grad_norm": 6.905984401702881,
      "learning_rate": 1.3925597738904592e-05,
      "loss": 1.6319,
      "step": 566700
    },
    {
      "epoch": 43.29692154915591,
      "grad_norm": 5.336792945861816,
      "learning_rate": 1.3919232042370075e-05,
      "loss": 1.7161,
      "step": 566800
    },
    {
      "epoch": 43.30456038499732,
      "grad_norm": 6.067778587341309,
      "learning_rate": 1.3912866345835562e-05,
      "loss": 1.6675,
      "step": 566900
    },
    {
      "epoch": 43.31219922083874,
      "grad_norm": 5.678388595581055,
      "learning_rate": 1.3906500649301046e-05,
      "loss": 1.6399,
      "step": 567000
    },
    {
      "epoch": 43.31983805668016,
      "grad_norm": 4.6492109298706055,
      "learning_rate": 1.3900134952766532e-05,
      "loss": 1.7132,
      "step": 567100
    },
    {
      "epoch": 43.32747689252158,
      "grad_norm": 6.508328437805176,
      "learning_rate": 1.3893769256232018e-05,
      "loss": 1.6714,
      "step": 567200
    },
    {
      "epoch": 43.335115728363,
      "grad_norm": 8.837386131286621,
      "learning_rate": 1.3887403559697505e-05,
      "loss": 1.6515,
      "step": 567300
    },
    {
      "epoch": 43.34275456420441,
      "grad_norm": 6.315677642822266,
      "learning_rate": 1.3881037863162988e-05,
      "loss": 1.5715,
      "step": 567400
    },
    {
      "epoch": 43.35039340004583,
      "grad_norm": 5.984828948974609,
      "learning_rate": 1.3874672166628472e-05,
      "loss": 1.5786,
      "step": 567500
    },
    {
      "epoch": 43.35803223588725,
      "grad_norm": 6.257064342498779,
      "learning_rate": 1.3868306470093959e-05,
      "loss": 1.7125,
      "step": 567600
    },
    {
      "epoch": 43.36567107172867,
      "grad_norm": 4.976712226867676,
      "learning_rate": 1.3861940773559442e-05,
      "loss": 1.6479,
      "step": 567700
    },
    {
      "epoch": 43.37330990757009,
      "grad_norm": 6.775819778442383,
      "learning_rate": 1.3855575077024929e-05,
      "loss": 1.6434,
      "step": 567800
    },
    {
      "epoch": 43.3809487434115,
      "grad_norm": 4.253622055053711,
      "learning_rate": 1.3849209380490413e-05,
      "loss": 1.6108,
      "step": 567900
    },
    {
      "epoch": 43.38858757925292,
      "grad_norm": 5.2733259201049805,
      "learning_rate": 1.38428436839559e-05,
      "loss": 1.5814,
      "step": 568000
    },
    {
      "epoch": 43.39622641509434,
      "grad_norm": 5.945706367492676,
      "learning_rate": 1.3836477987421385e-05,
      "loss": 1.6528,
      "step": 568100
    },
    {
      "epoch": 43.40386525093576,
      "grad_norm": 6.523749351501465,
      "learning_rate": 1.383011229088687e-05,
      "loss": 1.7343,
      "step": 568200
    },
    {
      "epoch": 43.41150408677718,
      "grad_norm": 6.275683403015137,
      "learning_rate": 1.3823746594352355e-05,
      "loss": 1.7289,
      "step": 568300
    },
    {
      "epoch": 43.41914292261859,
      "grad_norm": 5.373406887054443,
      "learning_rate": 1.3817380897817839e-05,
      "loss": 1.7121,
      "step": 568400
    },
    {
      "epoch": 43.42678175846001,
      "grad_norm": 6.569962978363037,
      "learning_rate": 1.3811015201283326e-05,
      "loss": 1.7306,
      "step": 568500
    },
    {
      "epoch": 43.43442059430143,
      "grad_norm": 6.124303817749023,
      "learning_rate": 1.380464950474881e-05,
      "loss": 1.6994,
      "step": 568600
    },
    {
      "epoch": 43.44205943014285,
      "grad_norm": 5.873958587646484,
      "learning_rate": 1.3798283808214296e-05,
      "loss": 1.6325,
      "step": 568700
    },
    {
      "epoch": 43.44969826598427,
      "grad_norm": 5.308064937591553,
      "learning_rate": 1.379191811167978e-05,
      "loss": 1.7303,
      "step": 568800
    },
    {
      "epoch": 43.45733710182568,
      "grad_norm": 5.2306976318359375,
      "learning_rate": 1.3785552415145267e-05,
      "loss": 1.6434,
      "step": 568900
    },
    {
      "epoch": 43.4649759376671,
      "grad_norm": 4.637258052825928,
      "learning_rate": 1.377918671861075e-05,
      "loss": 1.6691,
      "step": 569000
    },
    {
      "epoch": 43.47261477350852,
      "grad_norm": 5.9822540283203125,
      "learning_rate": 1.3772821022076235e-05,
      "loss": 1.6738,
      "step": 569100
    },
    {
      "epoch": 43.48025360934994,
      "grad_norm": 5.467602729797363,
      "learning_rate": 1.3766455325541722e-05,
      "loss": 1.7325,
      "step": 569200
    },
    {
      "epoch": 43.487892445191356,
      "grad_norm": 3.7243471145629883,
      "learning_rate": 1.3760089629007206e-05,
      "loss": 1.6384,
      "step": 569300
    },
    {
      "epoch": 43.49553128103277,
      "grad_norm": 5.498629570007324,
      "learning_rate": 1.3753723932472693e-05,
      "loss": 1.6469,
      "step": 569400
    },
    {
      "epoch": 43.50317011687419,
      "grad_norm": 4.545355796813965,
      "learning_rate": 1.3747358235938176e-05,
      "loss": 1.6334,
      "step": 569500
    },
    {
      "epoch": 43.51080895271561,
      "grad_norm": 5.641775608062744,
      "learning_rate": 1.3740992539403663e-05,
      "loss": 1.7631,
      "step": 569600
    },
    {
      "epoch": 43.518447788557026,
      "grad_norm": 5.981732368469238,
      "learning_rate": 1.3734626842869147e-05,
      "loss": 1.6172,
      "step": 569700
    },
    {
      "epoch": 43.526086624398445,
      "grad_norm": 10.156503677368164,
      "learning_rate": 1.3728261146334634e-05,
      "loss": 1.659,
      "step": 569800
    },
    {
      "epoch": 43.53372546023986,
      "grad_norm": 4.810032367706299,
      "learning_rate": 1.3721895449800117e-05,
      "loss": 1.6695,
      "step": 569900
    },
    {
      "epoch": 43.54136429608128,
      "grad_norm": 5.298198699951172,
      "learning_rate": 1.3715529753265602e-05,
      "loss": 1.6949,
      "step": 570000
    },
    {
      "epoch": 43.549003131922696,
      "grad_norm": 5.564549922943115,
      "learning_rate": 1.370916405673109e-05,
      "loss": 1.6702,
      "step": 570100
    },
    {
      "epoch": 43.556641967764115,
      "grad_norm": 6.135339260101318,
      "learning_rate": 1.3702798360196573e-05,
      "loss": 1.6245,
      "step": 570200
    },
    {
      "epoch": 43.56428080360553,
      "grad_norm": 5.802757263183594,
      "learning_rate": 1.369643266366206e-05,
      "loss": 1.709,
      "step": 570300
    },
    {
      "epoch": 43.571919639446946,
      "grad_norm": 6.535914421081543,
      "learning_rate": 1.3690066967127543e-05,
      "loss": 1.7302,
      "step": 570400
    },
    {
      "epoch": 43.579558475288366,
      "grad_norm": 5.703641891479492,
      "learning_rate": 1.368370127059303e-05,
      "loss": 1.666,
      "step": 570500
    },
    {
      "epoch": 43.587197311129785,
      "grad_norm": 5.393142223358154,
      "learning_rate": 1.3677335574058514e-05,
      "loss": 1.7321,
      "step": 570600
    },
    {
      "epoch": 43.594836146971204,
      "grad_norm": 4.993222236633301,
      "learning_rate": 1.3670969877523997e-05,
      "loss": 1.7162,
      "step": 570700
    },
    {
      "epoch": 43.602474982812616,
      "grad_norm": 4.955568313598633,
      "learning_rate": 1.3664604180989484e-05,
      "loss": 1.6682,
      "step": 570800
    },
    {
      "epoch": 43.610113818654035,
      "grad_norm": 7.552825450897217,
      "learning_rate": 1.365823848445497e-05,
      "loss": 1.7306,
      "step": 570900
    },
    {
      "epoch": 43.617752654495455,
      "grad_norm": 6.15077018737793,
      "learning_rate": 1.3651872787920455e-05,
      "loss": 1.8152,
      "step": 571000
    },
    {
      "epoch": 43.625391490336874,
      "grad_norm": 5.804305553436279,
      "learning_rate": 1.364550709138594e-05,
      "loss": 1.6771,
      "step": 571100
    },
    {
      "epoch": 43.63303032617829,
      "grad_norm": 5.745118618011475,
      "learning_rate": 1.3639141394851427e-05,
      "loss": 1.7698,
      "step": 571200
    },
    {
      "epoch": 43.640669162019705,
      "grad_norm": 6.172380447387695,
      "learning_rate": 1.363277569831691e-05,
      "loss": 1.6632,
      "step": 571300
    },
    {
      "epoch": 43.648307997861124,
      "grad_norm": 6.054518699645996,
      "learning_rate": 1.3626410001782397e-05,
      "loss": 1.6793,
      "step": 571400
    },
    {
      "epoch": 43.655946833702544,
      "grad_norm": 7.831700801849365,
      "learning_rate": 1.3620044305247881e-05,
      "loss": 1.7353,
      "step": 571500
    },
    {
      "epoch": 43.66358566954396,
      "grad_norm": 5.814819812774658,
      "learning_rate": 1.3613678608713364e-05,
      "loss": 1.7268,
      "step": 571600
    },
    {
      "epoch": 43.67122450538538,
      "grad_norm": 5.8803486824035645,
      "learning_rate": 1.3607312912178851e-05,
      "loss": 1.7327,
      "step": 571700
    },
    {
      "epoch": 43.678863341226794,
      "grad_norm": 5.162229061126709,
      "learning_rate": 1.3600947215644335e-05,
      "loss": 1.686,
      "step": 571800
    },
    {
      "epoch": 43.68650217706821,
      "grad_norm": 4.979755401611328,
      "learning_rate": 1.3594581519109822e-05,
      "loss": 1.6774,
      "step": 571900
    },
    {
      "epoch": 43.69414101290963,
      "grad_norm": 6.414906978607178,
      "learning_rate": 1.3588215822575307e-05,
      "loss": 1.6779,
      "step": 572000
    },
    {
      "epoch": 43.70177984875105,
      "grad_norm": 6.914766311645508,
      "learning_rate": 1.3581850126040794e-05,
      "loss": 1.6966,
      "step": 572100
    },
    {
      "epoch": 43.70941868459247,
      "grad_norm": 5.93441104888916,
      "learning_rate": 1.3575484429506278e-05,
      "loss": 1.6545,
      "step": 572200
    },
    {
      "epoch": 43.71705752043388,
      "grad_norm": 6.496447563171387,
      "learning_rate": 1.3569118732971761e-05,
      "loss": 1.7752,
      "step": 572300
    },
    {
      "epoch": 43.7246963562753,
      "grad_norm": 7.997982025146484,
      "learning_rate": 1.3562753036437248e-05,
      "loss": 1.7112,
      "step": 572400
    },
    {
      "epoch": 43.73233519211672,
      "grad_norm": 8.384111404418945,
      "learning_rate": 1.3556387339902732e-05,
      "loss": 1.6097,
      "step": 572500
    },
    {
      "epoch": 43.73997402795814,
      "grad_norm": 5.670531272888184,
      "learning_rate": 1.3550021643368218e-05,
      "loss": 1.6382,
      "step": 572600
    },
    {
      "epoch": 43.74761286379956,
      "grad_norm": 5.089127540588379,
      "learning_rate": 1.3543655946833702e-05,
      "loss": 1.6505,
      "step": 572700
    },
    {
      "epoch": 43.75525169964097,
      "grad_norm": 5.256466388702393,
      "learning_rate": 1.3537290250299189e-05,
      "loss": 1.7824,
      "step": 572800
    },
    {
      "epoch": 43.76289053548239,
      "grad_norm": 6.374669551849365,
      "learning_rate": 1.3530924553764674e-05,
      "loss": 1.697,
      "step": 572900
    },
    {
      "epoch": 43.77052937132381,
      "grad_norm": 5.449413776397705,
      "learning_rate": 1.352455885723016e-05,
      "loss": 1.7117,
      "step": 573000
    },
    {
      "epoch": 43.77816820716523,
      "grad_norm": 4.316058158874512,
      "learning_rate": 1.3518193160695645e-05,
      "loss": 1.6625,
      "step": 573100
    },
    {
      "epoch": 43.78580704300665,
      "grad_norm": 4.745194435119629,
      "learning_rate": 1.3511827464161128e-05,
      "loss": 1.6993,
      "step": 573200
    },
    {
      "epoch": 43.79344587884806,
      "grad_norm": 4.995669364929199,
      "learning_rate": 1.3505461767626615e-05,
      "loss": 1.6899,
      "step": 573300
    },
    {
      "epoch": 43.80108471468948,
      "grad_norm": 5.058452606201172,
      "learning_rate": 1.3499096071092099e-05,
      "loss": 1.6361,
      "step": 573400
    },
    {
      "epoch": 43.8087235505309,
      "grad_norm": 4.255105495452881,
      "learning_rate": 1.3492730374557586e-05,
      "loss": 1.7135,
      "step": 573500
    },
    {
      "epoch": 43.81636238637232,
      "grad_norm": 6.072693347930908,
      "learning_rate": 1.3486364678023069e-05,
      "loss": 1.667,
      "step": 573600
    },
    {
      "epoch": 43.82400122221374,
      "grad_norm": 6.315856456756592,
      "learning_rate": 1.3479998981488556e-05,
      "loss": 1.6322,
      "step": 573700
    },
    {
      "epoch": 43.83164005805515,
      "grad_norm": 5.7458014488220215,
      "learning_rate": 1.347363328495404e-05,
      "loss": 1.6143,
      "step": 573800
    },
    {
      "epoch": 43.83927889389657,
      "grad_norm": 6.9854559898376465,
      "learning_rate": 1.3467267588419525e-05,
      "loss": 1.8398,
      "step": 573900
    },
    {
      "epoch": 43.84691772973799,
      "grad_norm": 5.240604400634766,
      "learning_rate": 1.3460901891885012e-05,
      "loss": 1.725,
      "step": 574000
    },
    {
      "epoch": 43.85455656557941,
      "grad_norm": 5.970315456390381,
      "learning_rate": 1.3454536195350495e-05,
      "loss": 1.715,
      "step": 574100
    },
    {
      "epoch": 43.86219540142082,
      "grad_norm": 4.7335734367370605,
      "learning_rate": 1.3448170498815982e-05,
      "loss": 1.6721,
      "step": 574200
    },
    {
      "epoch": 43.86983423726224,
      "grad_norm": 5.336263179779053,
      "learning_rate": 1.3441804802281466e-05,
      "loss": 1.7133,
      "step": 574300
    },
    {
      "epoch": 43.87747307310366,
      "grad_norm": 6.048655986785889,
      "learning_rate": 1.3435439105746953e-05,
      "loss": 1.752,
      "step": 574400
    },
    {
      "epoch": 43.88511190894508,
      "grad_norm": 4.592794418334961,
      "learning_rate": 1.3429073409212436e-05,
      "loss": 1.7326,
      "step": 574500
    },
    {
      "epoch": 43.8927507447865,
      "grad_norm": 5.882052898406982,
      "learning_rate": 1.342270771267792e-05,
      "loss": 1.6691,
      "step": 574600
    },
    {
      "epoch": 43.90038958062791,
      "grad_norm": 5.566945552825928,
      "learning_rate": 1.3416342016143407e-05,
      "loss": 1.6366,
      "step": 574700
    },
    {
      "epoch": 43.90802841646933,
      "grad_norm": 7.865756034851074,
      "learning_rate": 1.3409976319608892e-05,
      "loss": 1.6731,
      "step": 574800
    },
    {
      "epoch": 43.91566725231075,
      "grad_norm": 6.617485046386719,
      "learning_rate": 1.3403610623074379e-05,
      "loss": 1.6577,
      "step": 574900
    },
    {
      "epoch": 43.92330608815217,
      "grad_norm": 5.048345565795898,
      "learning_rate": 1.3397244926539862e-05,
      "loss": 1.6599,
      "step": 575000
    },
    {
      "epoch": 43.930944923993586,
      "grad_norm": 6.663213729858398,
      "learning_rate": 1.339087923000535e-05,
      "loss": 1.6874,
      "step": 575100
    },
    {
      "epoch": 43.938583759835,
      "grad_norm": 4.551311492919922,
      "learning_rate": 1.3384513533470833e-05,
      "loss": 1.6224,
      "step": 575200
    },
    {
      "epoch": 43.94622259567642,
      "grad_norm": 4.372865200042725,
      "learning_rate": 1.337814783693632e-05,
      "loss": 1.6204,
      "step": 575300
    },
    {
      "epoch": 43.953861431517836,
      "grad_norm": 5.322026252746582,
      "learning_rate": 1.3371782140401803e-05,
      "loss": 1.6523,
      "step": 575400
    },
    {
      "epoch": 43.961500267359256,
      "grad_norm": 5.268971920013428,
      "learning_rate": 1.3365416443867287e-05,
      "loss": 1.7155,
      "step": 575500
    },
    {
      "epoch": 43.969139103200675,
      "grad_norm": 6.0526251792907715,
      "learning_rate": 1.3359050747332774e-05,
      "loss": 1.6958,
      "step": 575600
    },
    {
      "epoch": 43.97677793904209,
      "grad_norm": 4.966371536254883,
      "learning_rate": 1.3352685050798259e-05,
      "loss": 1.7003,
      "step": 575700
    },
    {
      "epoch": 43.984416774883506,
      "grad_norm": 4.874136447906494,
      "learning_rate": 1.3346319354263744e-05,
      "loss": 1.6753,
      "step": 575800
    },
    {
      "epoch": 43.992055610724925,
      "grad_norm": 6.524861812591553,
      "learning_rate": 1.333995365772923e-05,
      "loss": 1.6846,
      "step": 575900
    },
    {
      "epoch": 43.999694446566345,
      "grad_norm": 5.926547050476074,
      "learning_rate": 1.3333587961194716e-05,
      "loss": 1.6795,
      "step": 576000
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.7720338106155396,
      "eval_runtime": 3.5977,
      "eval_samples_per_second": 191.787,
      "eval_steps_per_second": 191.787,
      "step": 576004
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.4401992559432983,
      "eval_runtime": 65.9462,
      "eval_samples_per_second": 198.51,
      "eval_steps_per_second": 198.51,
      "step": 576004
    },
    {
      "epoch": 44.007333282407764,
      "grad_norm": 8.065813064575195,
      "learning_rate": 1.33272222646602e-05,
      "loss": 1.7366,
      "step": 576100
    },
    {
      "epoch": 44.014972118249176,
      "grad_norm": 3.2900569438934326,
      "learning_rate": 1.3320856568125683e-05,
      "loss": 1.6476,
      "step": 576200
    },
    {
      "epoch": 44.022610954090595,
      "grad_norm": 6.485566139221191,
      "learning_rate": 1.331449087159117e-05,
      "loss": 1.6557,
      "step": 576300
    },
    {
      "epoch": 44.030249789932014,
      "grad_norm": 6.441366195678711,
      "learning_rate": 1.3308125175056654e-05,
      "loss": 1.6476,
      "step": 576400
    },
    {
      "epoch": 44.037888625773434,
      "grad_norm": 4.953311443328857,
      "learning_rate": 1.330175947852214e-05,
      "loss": 1.6835,
      "step": 576500
    },
    {
      "epoch": 44.04552746161485,
      "grad_norm": 5.520931243896484,
      "learning_rate": 1.3295393781987624e-05,
      "loss": 1.6587,
      "step": 576600
    },
    {
      "epoch": 44.053166297456265,
      "grad_norm": 7.246982574462891,
      "learning_rate": 1.3289028085453111e-05,
      "loss": 1.6475,
      "step": 576700
    },
    {
      "epoch": 44.060805133297684,
      "grad_norm": 5.297487735748291,
      "learning_rate": 1.3282662388918596e-05,
      "loss": 1.6895,
      "step": 576800
    },
    {
      "epoch": 44.0684439691391,
      "grad_norm": 4.943332195281982,
      "learning_rate": 1.3276296692384083e-05,
      "loss": 1.6665,
      "step": 576900
    },
    {
      "epoch": 44.07608280498052,
      "grad_norm": 5.106276035308838,
      "learning_rate": 1.3269930995849567e-05,
      "loss": 1.6497,
      "step": 577000
    },
    {
      "epoch": 44.08372164082194,
      "grad_norm": 4.617925643920898,
      "learning_rate": 1.326356529931505e-05,
      "loss": 1.6507,
      "step": 577100
    },
    {
      "epoch": 44.091360476663354,
      "grad_norm": 6.294302463531494,
      "learning_rate": 1.3257199602780537e-05,
      "loss": 1.6367,
      "step": 577200
    },
    {
      "epoch": 44.09899931250477,
      "grad_norm": 3.508192300796509,
      "learning_rate": 1.3250833906246021e-05,
      "loss": 1.6757,
      "step": 577300
    },
    {
      "epoch": 44.10663814834619,
      "grad_norm": 5.609059810638428,
      "learning_rate": 1.3244468209711508e-05,
      "loss": 1.6391,
      "step": 577400
    },
    {
      "epoch": 44.11427698418761,
      "grad_norm": 4.938498020172119,
      "learning_rate": 1.3238102513176991e-05,
      "loss": 1.6403,
      "step": 577500
    },
    {
      "epoch": 44.12191582002903,
      "grad_norm": 5.2195916175842285,
      "learning_rate": 1.3231736816642478e-05,
      "loss": 1.7968,
      "step": 577600
    },
    {
      "epoch": 44.12955465587044,
      "grad_norm": 5.570915222167969,
      "learning_rate": 1.3225371120107964e-05,
      "loss": 1.6672,
      "step": 577700
    },
    {
      "epoch": 44.13719349171186,
      "grad_norm": 6.396505355834961,
      "learning_rate": 1.3219005423573447e-05,
      "loss": 1.5686,
      "step": 577800
    },
    {
      "epoch": 44.14483232755328,
      "grad_norm": 7.73720121383667,
      "learning_rate": 1.3212639727038934e-05,
      "loss": 1.674,
      "step": 577900
    },
    {
      "epoch": 44.1524711633947,
      "grad_norm": 8.58929443359375,
      "learning_rate": 1.3206274030504418e-05,
      "loss": 1.6237,
      "step": 578000
    },
    {
      "epoch": 44.16010999923612,
      "grad_norm": 4.739987850189209,
      "learning_rate": 1.3199908333969904e-05,
      "loss": 1.7042,
      "step": 578100
    },
    {
      "epoch": 44.16774883507753,
      "grad_norm": 5.2390456199646,
      "learning_rate": 1.3193542637435388e-05,
      "loss": 1.6138,
      "step": 578200
    },
    {
      "epoch": 44.17538767091895,
      "grad_norm": 6.952726364135742,
      "learning_rate": 1.3187176940900875e-05,
      "loss": 1.683,
      "step": 578300
    },
    {
      "epoch": 44.18302650676037,
      "grad_norm": 6.156589031219482,
      "learning_rate": 1.3180811244366358e-05,
      "loss": 1.6707,
      "step": 578400
    },
    {
      "epoch": 44.19066534260179,
      "grad_norm": 4.318402290344238,
      "learning_rate": 1.3174445547831845e-05,
      "loss": 1.6374,
      "step": 578500
    },
    {
      "epoch": 44.1983041784432,
      "grad_norm": 5.053143501281738,
      "learning_rate": 1.3168079851297329e-05,
      "loss": 1.6308,
      "step": 578600
    },
    {
      "epoch": 44.20594301428462,
      "grad_norm": 5.409987926483154,
      "learning_rate": 1.3161714154762814e-05,
      "loss": 1.7482,
      "step": 578700
    },
    {
      "epoch": 44.21358185012604,
      "grad_norm": 6.077521324157715,
      "learning_rate": 1.3155348458228301e-05,
      "loss": 1.703,
      "step": 578800
    },
    {
      "epoch": 44.22122068596746,
      "grad_norm": 5.474070072174072,
      "learning_rate": 1.3148982761693785e-05,
      "loss": 1.7112,
      "step": 578900
    },
    {
      "epoch": 44.22885952180888,
      "grad_norm": 5.093750953674316,
      "learning_rate": 1.3142617065159272e-05,
      "loss": 1.6637,
      "step": 579000
    },
    {
      "epoch": 44.23649835765029,
      "grad_norm": 4.817756652832031,
      "learning_rate": 1.3136251368624755e-05,
      "loss": 1.6778,
      "step": 579100
    },
    {
      "epoch": 44.24413719349171,
      "grad_norm": 5.88004732131958,
      "learning_rate": 1.3129885672090242e-05,
      "loss": 1.7776,
      "step": 579200
    },
    {
      "epoch": 44.25177602933313,
      "grad_norm": 8.11834716796875,
      "learning_rate": 1.3123519975555726e-05,
      "loss": 1.6707,
      "step": 579300
    },
    {
      "epoch": 44.25941486517455,
      "grad_norm": 5.137497425079346,
      "learning_rate": 1.3117154279021209e-05,
      "loss": 1.6954,
      "step": 579400
    },
    {
      "epoch": 44.26705370101597,
      "grad_norm": 7.301804065704346,
      "learning_rate": 1.3110788582486696e-05,
      "loss": 1.5629,
      "step": 579500
    },
    {
      "epoch": 44.27469253685738,
      "grad_norm": 8.05537223815918,
      "learning_rate": 1.3104422885952181e-05,
      "loss": 1.7022,
      "step": 579600
    },
    {
      "epoch": 44.2823313726988,
      "grad_norm": 5.171647071838379,
      "learning_rate": 1.3098057189417668e-05,
      "loss": 1.6907,
      "step": 579700
    },
    {
      "epoch": 44.28997020854022,
      "grad_norm": 6.222452640533447,
      "learning_rate": 1.3091691492883152e-05,
      "loss": 1.6738,
      "step": 579800
    },
    {
      "epoch": 44.29760904438164,
      "grad_norm": 6.356319904327393,
      "learning_rate": 1.3085325796348639e-05,
      "loss": 1.5888,
      "step": 579900
    },
    {
      "epoch": 44.30524788022306,
      "grad_norm": 6.876485824584961,
      "learning_rate": 1.3078960099814122e-05,
      "loss": 1.6904,
      "step": 580000
    },
    {
      "epoch": 44.31288671606447,
      "grad_norm": 5.59861946105957,
      "learning_rate": 1.3072594403279609e-05,
      "loss": 1.7054,
      "step": 580100
    },
    {
      "epoch": 44.32052555190589,
      "grad_norm": 4.947333812713623,
      "learning_rate": 1.3066228706745093e-05,
      "loss": 1.6583,
      "step": 580200
    },
    {
      "epoch": 44.32816438774731,
      "grad_norm": 5.3396196365356445,
      "learning_rate": 1.3059863010210576e-05,
      "loss": 1.6882,
      "step": 580300
    },
    {
      "epoch": 44.33580322358873,
      "grad_norm": 6.113770008087158,
      "learning_rate": 1.3053497313676063e-05,
      "loss": 1.7439,
      "step": 580400
    },
    {
      "epoch": 44.343442059430146,
      "grad_norm": 6.397889137268066,
      "learning_rate": 1.3047131617141548e-05,
      "loss": 1.7062,
      "step": 580500
    },
    {
      "epoch": 44.35108089527156,
      "grad_norm": 7.292163848876953,
      "learning_rate": 1.3040765920607034e-05,
      "loss": 1.6838,
      "step": 580600
    },
    {
      "epoch": 44.35871973111298,
      "grad_norm": 6.692040920257568,
      "learning_rate": 1.3034400224072519e-05,
      "loss": 1.6997,
      "step": 580700
    },
    {
      "epoch": 44.366358566954396,
      "grad_norm": 5.90665340423584,
      "learning_rate": 1.3028034527538006e-05,
      "loss": 1.6516,
      "step": 580800
    },
    {
      "epoch": 44.373997402795816,
      "grad_norm": 6.258482933044434,
      "learning_rate": 1.302166883100349e-05,
      "loss": 1.6354,
      "step": 580900
    },
    {
      "epoch": 44.381636238637235,
      "grad_norm": 5.4162774085998535,
      "learning_rate": 1.3015303134468973e-05,
      "loss": 1.7047,
      "step": 581000
    },
    {
      "epoch": 44.38927507447865,
      "grad_norm": 7.254366874694824,
      "learning_rate": 1.300893743793446e-05,
      "loss": 1.6484,
      "step": 581100
    },
    {
      "epoch": 44.396913910320066,
      "grad_norm": 6.015719890594482,
      "learning_rate": 1.3002571741399943e-05,
      "loss": 1.7088,
      "step": 581200
    },
    {
      "epoch": 44.404552746161485,
      "grad_norm": 6.015695571899414,
      "learning_rate": 1.299620604486543e-05,
      "loss": 1.6692,
      "step": 581300
    },
    {
      "epoch": 44.412191582002905,
      "grad_norm": 5.030441761016846,
      "learning_rate": 1.2989840348330914e-05,
      "loss": 1.8165,
      "step": 581400
    },
    {
      "epoch": 44.419830417844324,
      "grad_norm": 5.595301628112793,
      "learning_rate": 1.29834746517964e-05,
      "loss": 1.6575,
      "step": 581500
    },
    {
      "epoch": 44.427469253685736,
      "grad_norm": 5.035435199737549,
      "learning_rate": 1.2977108955261886e-05,
      "loss": 1.5827,
      "step": 581600
    },
    {
      "epoch": 44.435108089527155,
      "grad_norm": 5.020406246185303,
      "learning_rate": 1.2970743258727373e-05,
      "loss": 1.6522,
      "step": 581700
    },
    {
      "epoch": 44.442746925368574,
      "grad_norm": 6.028785228729248,
      "learning_rate": 1.2964377562192856e-05,
      "loss": 1.8121,
      "step": 581800
    },
    {
      "epoch": 44.45038576120999,
      "grad_norm": 5.7223100662231445,
      "learning_rate": 1.295801186565834e-05,
      "loss": 1.6471,
      "step": 581900
    },
    {
      "epoch": 44.45802459705141,
      "grad_norm": 6.472200870513916,
      "learning_rate": 1.2951646169123827e-05,
      "loss": 1.6848,
      "step": 582000
    },
    {
      "epoch": 44.465663432892825,
      "grad_norm": 5.377281665802002,
      "learning_rate": 1.294528047258931e-05,
      "loss": 1.6326,
      "step": 582100
    },
    {
      "epoch": 44.473302268734244,
      "grad_norm": 7.124927520751953,
      "learning_rate": 1.2938914776054797e-05,
      "loss": 1.6905,
      "step": 582200
    },
    {
      "epoch": 44.48094110457566,
      "grad_norm": 5.031324863433838,
      "learning_rate": 1.293254907952028e-05,
      "loss": 1.6666,
      "step": 582300
    },
    {
      "epoch": 44.48857994041708,
      "grad_norm": 5.65596866607666,
      "learning_rate": 1.2926183382985768e-05,
      "loss": 1.7295,
      "step": 582400
    },
    {
      "epoch": 44.4962187762585,
      "grad_norm": 4.848295211791992,
      "learning_rate": 1.2919817686451253e-05,
      "loss": 1.6853,
      "step": 582500
    },
    {
      "epoch": 44.503857612099914,
      "grad_norm": 5.297996520996094,
      "learning_rate": 1.2913451989916736e-05,
      "loss": 1.6258,
      "step": 582600
    },
    {
      "epoch": 44.51149644794133,
      "grad_norm": 7.602203369140625,
      "learning_rate": 1.2907086293382223e-05,
      "loss": 1.6713,
      "step": 582700
    },
    {
      "epoch": 44.51913528378275,
      "grad_norm": 5.788565635681152,
      "learning_rate": 1.2900720596847707e-05,
      "loss": 1.6325,
      "step": 582800
    },
    {
      "epoch": 44.52677411962417,
      "grad_norm": 5.024450778961182,
      "learning_rate": 1.2894354900313194e-05,
      "loss": 1.6263,
      "step": 582900
    },
    {
      "epoch": 44.534412955465584,
      "grad_norm": 4.435013294219971,
      "learning_rate": 1.2887989203778677e-05,
      "loss": 1.7036,
      "step": 583000
    },
    {
      "epoch": 44.542051791307,
      "grad_norm": 5.253452777862549,
      "learning_rate": 1.2881623507244164e-05,
      "loss": 1.6731,
      "step": 583100
    },
    {
      "epoch": 44.54969062714842,
      "grad_norm": 6.509487628936768,
      "learning_rate": 1.2875257810709648e-05,
      "loss": 1.703,
      "step": 583200
    },
    {
      "epoch": 44.55732946298984,
      "grad_norm": 5.255169868469238,
      "learning_rate": 1.2868892114175133e-05,
      "loss": 1.7039,
      "step": 583300
    },
    {
      "epoch": 44.56496829883126,
      "grad_norm": 8.012890815734863,
      "learning_rate": 1.2862526417640618e-05,
      "loss": 1.624,
      "step": 583400
    },
    {
      "epoch": 44.57260713467267,
      "grad_norm": 6.09559965133667,
      "learning_rate": 1.2856160721106104e-05,
      "loss": 1.6873,
      "step": 583500
    },
    {
      "epoch": 44.58024597051409,
      "grad_norm": 5.762030124664307,
      "learning_rate": 1.284979502457159e-05,
      "loss": 1.6679,
      "step": 583600
    },
    {
      "epoch": 44.58788480635551,
      "grad_norm": 5.595786094665527,
      "learning_rate": 1.2843429328037074e-05,
      "loss": 1.6575,
      "step": 583700
    },
    {
      "epoch": 44.59552364219693,
      "grad_norm": 7.332263946533203,
      "learning_rate": 1.2837063631502561e-05,
      "loss": 1.6358,
      "step": 583800
    },
    {
      "epoch": 44.60316247803835,
      "grad_norm": 7.342088222503662,
      "learning_rate": 1.2830697934968044e-05,
      "loss": 1.7095,
      "step": 583900
    },
    {
      "epoch": 44.61080131387976,
      "grad_norm": 5.223987579345703,
      "learning_rate": 1.2824332238433531e-05,
      "loss": 1.6728,
      "step": 584000
    },
    {
      "epoch": 44.61844014972118,
      "grad_norm": 6.313930511474609,
      "learning_rate": 1.2817966541899015e-05,
      "loss": 1.6616,
      "step": 584100
    },
    {
      "epoch": 44.6260789855626,
      "grad_norm": 5.172504901885986,
      "learning_rate": 1.2811600845364498e-05,
      "loss": 1.6623,
      "step": 584200
    },
    {
      "epoch": 44.63371782140402,
      "grad_norm": 6.5596466064453125,
      "learning_rate": 1.2805235148829985e-05,
      "loss": 1.6099,
      "step": 584300
    },
    {
      "epoch": 44.64135665724544,
      "grad_norm": 5.91579532623291,
      "learning_rate": 1.279886945229547e-05,
      "loss": 1.6996,
      "step": 584400
    },
    {
      "epoch": 44.64899549308685,
      "grad_norm": 5.64502477645874,
      "learning_rate": 1.2792503755760958e-05,
      "loss": 1.7027,
      "step": 584500
    },
    {
      "epoch": 44.65663432892827,
      "grad_norm": 6.318057060241699,
      "learning_rate": 1.2786138059226441e-05,
      "loss": 1.661,
      "step": 584600
    },
    {
      "epoch": 44.66427316476969,
      "grad_norm": 5.28687858581543,
      "learning_rate": 1.2779772362691928e-05,
      "loss": 1.666,
      "step": 584700
    },
    {
      "epoch": 44.67191200061111,
      "grad_norm": 5.584472179412842,
      "learning_rate": 1.2773406666157412e-05,
      "loss": 1.6423,
      "step": 584800
    },
    {
      "epoch": 44.67955083645253,
      "grad_norm": 6.742441654205322,
      "learning_rate": 1.2767040969622895e-05,
      "loss": 1.6861,
      "step": 584900
    },
    {
      "epoch": 44.68718967229394,
      "grad_norm": 5.561228275299072,
      "learning_rate": 1.2760675273088382e-05,
      "loss": 1.714,
      "step": 585000
    },
    {
      "epoch": 44.69482850813536,
      "grad_norm": 4.7350850105285645,
      "learning_rate": 1.2754309576553866e-05,
      "loss": 1.7416,
      "step": 585100
    },
    {
      "epoch": 44.70246734397678,
      "grad_norm": 7.011715412139893,
      "learning_rate": 1.2747943880019352e-05,
      "loss": 1.7315,
      "step": 585200
    },
    {
      "epoch": 44.7101061798182,
      "grad_norm": 5.146070957183838,
      "learning_rate": 1.2741578183484838e-05,
      "loss": 1.5759,
      "step": 585300
    },
    {
      "epoch": 44.71774501565962,
      "grad_norm": 5.954077243804932,
      "learning_rate": 1.2735212486950323e-05,
      "loss": 1.6942,
      "step": 585400
    },
    {
      "epoch": 44.72538385150103,
      "grad_norm": 9.222415924072266,
      "learning_rate": 1.2728846790415808e-05,
      "loss": 1.7292,
      "step": 585500
    },
    {
      "epoch": 44.73302268734245,
      "grad_norm": 5.347018718719482,
      "learning_rate": 1.2722481093881295e-05,
      "loss": 1.6324,
      "step": 585600
    },
    {
      "epoch": 44.74066152318387,
      "grad_norm": 7.017146110534668,
      "learning_rate": 1.2716115397346779e-05,
      "loss": 1.6817,
      "step": 585700
    },
    {
      "epoch": 44.748300359025286,
      "grad_norm": 5.455645561218262,
      "learning_rate": 1.2709749700812262e-05,
      "loss": 1.7353,
      "step": 585800
    },
    {
      "epoch": 44.755939194866706,
      "grad_norm": 5.707212924957275,
      "learning_rate": 1.2703384004277749e-05,
      "loss": 1.5812,
      "step": 585900
    },
    {
      "epoch": 44.76357803070812,
      "grad_norm": 5.256403923034668,
      "learning_rate": 1.2697018307743233e-05,
      "loss": 1.5977,
      "step": 586000
    },
    {
      "epoch": 44.77121686654954,
      "grad_norm": 4.784337520599365,
      "learning_rate": 1.269065261120872e-05,
      "loss": 1.6835,
      "step": 586100
    },
    {
      "epoch": 44.778855702390956,
      "grad_norm": 5.938533782958984,
      "learning_rate": 1.2684286914674203e-05,
      "loss": 1.7018,
      "step": 586200
    },
    {
      "epoch": 44.786494538232375,
      "grad_norm": 8.529302597045898,
      "learning_rate": 1.267792121813969e-05,
      "loss": 1.7328,
      "step": 586300
    },
    {
      "epoch": 44.794133374073795,
      "grad_norm": 6.458752632141113,
      "learning_rate": 1.2671555521605175e-05,
      "loss": 1.7417,
      "step": 586400
    },
    {
      "epoch": 44.80177220991521,
      "grad_norm": 5.069871425628662,
      "learning_rate": 1.2665189825070659e-05,
      "loss": 1.6402,
      "step": 586500
    },
    {
      "epoch": 44.809411045756626,
      "grad_norm": 6.2899980545043945,
      "learning_rate": 1.2658824128536146e-05,
      "loss": 1.6759,
      "step": 586600
    },
    {
      "epoch": 44.817049881598045,
      "grad_norm": 6.68769645690918,
      "learning_rate": 1.265245843200163e-05,
      "loss": 1.6351,
      "step": 586700
    },
    {
      "epoch": 44.824688717439464,
      "grad_norm": 5.564553260803223,
      "learning_rate": 1.2646092735467116e-05,
      "loss": 1.8268,
      "step": 586800
    },
    {
      "epoch": 44.83232755328088,
      "grad_norm": 5.433605670928955,
      "learning_rate": 1.26397270389326e-05,
      "loss": 1.6887,
      "step": 586900
    },
    {
      "epoch": 44.839966389122296,
      "grad_norm": 4.577768325805664,
      "learning_rate": 1.2633361342398087e-05,
      "loss": 1.6939,
      "step": 587000
    },
    {
      "epoch": 44.847605224963715,
      "grad_norm": 3.478343963623047,
      "learning_rate": 1.262699564586357e-05,
      "loss": 1.7485,
      "step": 587100
    },
    {
      "epoch": 44.855244060805134,
      "grad_norm": 4.627442359924316,
      "learning_rate": 1.2620629949329057e-05,
      "loss": 1.6809,
      "step": 587200
    },
    {
      "epoch": 44.86288289664655,
      "grad_norm": 4.550380229949951,
      "learning_rate": 1.2614264252794542e-05,
      "loss": 1.6244,
      "step": 587300
    },
    {
      "epoch": 44.870521732487965,
      "grad_norm": 7.511167526245117,
      "learning_rate": 1.2607898556260026e-05,
      "loss": 1.6435,
      "step": 587400
    },
    {
      "epoch": 44.878160568329385,
      "grad_norm": 4.895869731903076,
      "learning_rate": 1.2601532859725513e-05,
      "loss": 1.6921,
      "step": 587500
    },
    {
      "epoch": 44.885799404170804,
      "grad_norm": 6.79847526550293,
      "learning_rate": 1.2595167163190996e-05,
      "loss": 1.7843,
      "step": 587600
    },
    {
      "epoch": 44.89343824001222,
      "grad_norm": 6.633927345275879,
      "learning_rate": 1.2588801466656483e-05,
      "loss": 1.7419,
      "step": 587700
    },
    {
      "epoch": 44.90107707585364,
      "grad_norm": 5.402656555175781,
      "learning_rate": 1.2582435770121967e-05,
      "loss": 1.6694,
      "step": 587800
    },
    {
      "epoch": 44.908715911695055,
      "grad_norm": 5.26518440246582,
      "learning_rate": 1.2576070073587454e-05,
      "loss": 1.6329,
      "step": 587900
    },
    {
      "epoch": 44.916354747536474,
      "grad_norm": 8.213257789611816,
      "learning_rate": 1.2569704377052937e-05,
      "loss": 1.66,
      "step": 588000
    },
    {
      "epoch": 44.92399358337789,
      "grad_norm": 7.4179368019104,
      "learning_rate": 1.2563338680518422e-05,
      "loss": 1.5967,
      "step": 588100
    },
    {
      "epoch": 44.93163241921931,
      "grad_norm": 4.91715145111084,
      "learning_rate": 1.2556972983983908e-05,
      "loss": 1.5912,
      "step": 588200
    },
    {
      "epoch": 44.93927125506073,
      "grad_norm": 5.503509998321533,
      "learning_rate": 1.2550607287449393e-05,
      "loss": 1.7636,
      "step": 588300
    },
    {
      "epoch": 44.94691009090214,
      "grad_norm": 5.577269077301025,
      "learning_rate": 1.254424159091488e-05,
      "loss": 1.7275,
      "step": 588400
    },
    {
      "epoch": 44.95454892674356,
      "grad_norm": 6.331361770629883,
      "learning_rate": 1.2537875894380363e-05,
      "loss": 1.6958,
      "step": 588500
    },
    {
      "epoch": 44.96218776258498,
      "grad_norm": 7.103631973266602,
      "learning_rate": 1.253151019784585e-05,
      "loss": 1.7643,
      "step": 588600
    },
    {
      "epoch": 44.9698265984264,
      "grad_norm": 6.307003974914551,
      "learning_rate": 1.2525144501311334e-05,
      "loss": 1.7162,
      "step": 588700
    },
    {
      "epoch": 44.97746543426782,
      "grad_norm": 5.616539478302002,
      "learning_rate": 1.251877880477682e-05,
      "loss": 1.7143,
      "step": 588800
    },
    {
      "epoch": 44.98510427010923,
      "grad_norm": 5.084558486938477,
      "learning_rate": 1.2512413108242304e-05,
      "loss": 1.7078,
      "step": 588900
    },
    {
      "epoch": 44.99274310595065,
      "grad_norm": 5.683024883270264,
      "learning_rate": 1.2506047411707788e-05,
      "loss": 1.8521,
      "step": 589000
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.765584111213684,
      "eval_runtime": 3.5803,
      "eval_samples_per_second": 192.72,
      "eval_steps_per_second": 192.72,
      "step": 589095
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.4364200830459595,
      "eval_runtime": 64.3158,
      "eval_samples_per_second": 203.543,
      "eval_steps_per_second": 203.543,
      "step": 589095
    },
    {
      "epoch": 45.00038194179207,
      "grad_norm": 5.202277183532715,
      "learning_rate": 1.2499681715173275e-05,
      "loss": 1.6912,
      "step": 589100
    },
    {
      "epoch": 45.00802077763349,
      "grad_norm": 4.728571891784668,
      "learning_rate": 1.249331601863876e-05,
      "loss": 1.6702,
      "step": 589200
    },
    {
      "epoch": 45.01565961347491,
      "grad_norm": 7.520514488220215,
      "learning_rate": 1.2486950322104245e-05,
      "loss": 1.5786,
      "step": 589300
    },
    {
      "epoch": 45.02329844931632,
      "grad_norm": 5.442740440368652,
      "learning_rate": 1.248058462556973e-05,
      "loss": 1.6657,
      "step": 589400
    },
    {
      "epoch": 45.03093728515774,
      "grad_norm": 4.46661376953125,
      "learning_rate": 1.2474218929035216e-05,
      "loss": 1.6572,
      "step": 589500
    },
    {
      "epoch": 45.03857612099916,
      "grad_norm": 5.408418655395508,
      "learning_rate": 1.2467853232500701e-05,
      "loss": 1.6602,
      "step": 589600
    },
    {
      "epoch": 45.04621495684058,
      "grad_norm": 5.918460845947266,
      "learning_rate": 1.2461487535966186e-05,
      "loss": 1.6532,
      "step": 589700
    },
    {
      "epoch": 45.053853792682,
      "grad_norm": 6.59103536605835,
      "learning_rate": 1.2455121839431671e-05,
      "loss": 1.6944,
      "step": 589800
    },
    {
      "epoch": 45.06149262852341,
      "grad_norm": 4.97097110748291,
      "learning_rate": 1.2448756142897157e-05,
      "loss": 1.6364,
      "step": 589900
    },
    {
      "epoch": 45.06913146436483,
      "grad_norm": 5.9120917320251465,
      "learning_rate": 1.2442390446362642e-05,
      "loss": 1.7573,
      "step": 590000
    },
    {
      "epoch": 45.07677030020625,
      "grad_norm": 5.537328720092773,
      "learning_rate": 1.2436024749828127e-05,
      "loss": 1.6032,
      "step": 590100
    },
    {
      "epoch": 45.08440913604767,
      "grad_norm": 5.985818862915039,
      "learning_rate": 1.2429659053293612e-05,
      "loss": 1.735,
      "step": 590200
    },
    {
      "epoch": 45.09204797188909,
      "grad_norm": 9.81231689453125,
      "learning_rate": 1.2423293356759098e-05,
      "loss": 1.7424,
      "step": 590300
    },
    {
      "epoch": 45.0996868077305,
      "grad_norm": 3.3077611923217773,
      "learning_rate": 1.2416927660224583e-05,
      "loss": 1.6101,
      "step": 590400
    },
    {
      "epoch": 45.10732564357192,
      "grad_norm": 5.210785388946533,
      "learning_rate": 1.2410561963690068e-05,
      "loss": 1.6462,
      "step": 590500
    },
    {
      "epoch": 45.11496447941334,
      "grad_norm": 5.347306728363037,
      "learning_rate": 1.2404196267155553e-05,
      "loss": 1.7064,
      "step": 590600
    },
    {
      "epoch": 45.12260331525476,
      "grad_norm": 6.859367847442627,
      "learning_rate": 1.2397830570621038e-05,
      "loss": 1.6573,
      "step": 590700
    },
    {
      "epoch": 45.130242151096176,
      "grad_norm": 5.399704933166504,
      "learning_rate": 1.2391464874086522e-05,
      "loss": 1.6097,
      "step": 590800
    },
    {
      "epoch": 45.13788098693759,
      "grad_norm": 5.2414631843566895,
      "learning_rate": 1.2385099177552007e-05,
      "loss": 1.6508,
      "step": 590900
    },
    {
      "epoch": 45.14551982277901,
      "grad_norm": 5.815143585205078,
      "learning_rate": 1.2378733481017492e-05,
      "loss": 1.6916,
      "step": 591000
    },
    {
      "epoch": 45.15315865862043,
      "grad_norm": 6.685905933380127,
      "learning_rate": 1.237236778448298e-05,
      "loss": 1.699,
      "step": 591100
    },
    {
      "epoch": 45.160797494461846,
      "grad_norm": 6.9991841316223145,
      "learning_rate": 1.2366002087948465e-05,
      "loss": 1.6483,
      "step": 591200
    },
    {
      "epoch": 45.16843633030326,
      "grad_norm": 5.196223735809326,
      "learning_rate": 1.235963639141395e-05,
      "loss": 1.6373,
      "step": 591300
    },
    {
      "epoch": 45.17607516614468,
      "grad_norm": 4.461435794830322,
      "learning_rate": 1.2353270694879435e-05,
      "loss": 1.6617,
      "step": 591400
    },
    {
      "epoch": 45.1837140019861,
      "grad_norm": 6.4085211753845215,
      "learning_rate": 1.234690499834492e-05,
      "loss": 1.6237,
      "step": 591500
    },
    {
      "epoch": 45.191352837827516,
      "grad_norm": 7.538438320159912,
      "learning_rate": 1.2340539301810404e-05,
      "loss": 1.7104,
      "step": 591600
    },
    {
      "epoch": 45.198991673668935,
      "grad_norm": 4.683287620544434,
      "learning_rate": 1.2334173605275889e-05,
      "loss": 1.6519,
      "step": 591700
    },
    {
      "epoch": 45.20663050951035,
      "grad_norm": 5.928033828735352,
      "learning_rate": 1.2327807908741374e-05,
      "loss": 1.6286,
      "step": 591800
    },
    {
      "epoch": 45.21426934535177,
      "grad_norm": 5.265854835510254,
      "learning_rate": 1.232144221220686e-05,
      "loss": 1.7172,
      "step": 591900
    },
    {
      "epoch": 45.221908181193186,
      "grad_norm": 4.8183159828186035,
      "learning_rate": 1.2315076515672345e-05,
      "loss": 1.6864,
      "step": 592000
    },
    {
      "epoch": 45.229547017034605,
      "grad_norm": 5.242882251739502,
      "learning_rate": 1.2308710819137832e-05,
      "loss": 1.6594,
      "step": 592100
    },
    {
      "epoch": 45.237185852876024,
      "grad_norm": 3.1071534156799316,
      "learning_rate": 1.2302345122603317e-05,
      "loss": 1.6043,
      "step": 592200
    },
    {
      "epoch": 45.244824688717436,
      "grad_norm": 7.094574451446533,
      "learning_rate": 1.2295979426068802e-05,
      "loss": 1.7385,
      "step": 592300
    },
    {
      "epoch": 45.252463524558856,
      "grad_norm": 7.086075782775879,
      "learning_rate": 1.2289613729534286e-05,
      "loss": 1.6493,
      "step": 592400
    },
    {
      "epoch": 45.260102360400275,
      "grad_norm": 6.656026363372803,
      "learning_rate": 1.2283248032999771e-05,
      "loss": 1.6274,
      "step": 592500
    },
    {
      "epoch": 45.267741196241694,
      "grad_norm": 4.955245018005371,
      "learning_rate": 1.2276882336465256e-05,
      "loss": 1.5992,
      "step": 592600
    },
    {
      "epoch": 45.27538003208311,
      "grad_norm": 5.66119909286499,
      "learning_rate": 1.2270516639930741e-05,
      "loss": 1.6986,
      "step": 592700
    },
    {
      "epoch": 45.283018867924525,
      "grad_norm": 5.659704208374023,
      "learning_rate": 1.2264150943396227e-05,
      "loss": 1.7988,
      "step": 592800
    },
    {
      "epoch": 45.290657703765945,
      "grad_norm": 5.848198413848877,
      "learning_rate": 1.2257785246861712e-05,
      "loss": 1.6479,
      "step": 592900
    },
    {
      "epoch": 45.298296539607364,
      "grad_norm": 7.023617267608643,
      "learning_rate": 1.2251419550327197e-05,
      "loss": 1.6988,
      "step": 593000
    },
    {
      "epoch": 45.30593537544878,
      "grad_norm": 7.5655317306518555,
      "learning_rate": 1.2245053853792684e-05,
      "loss": 1.6752,
      "step": 593100
    },
    {
      "epoch": 45.3135742112902,
      "grad_norm": 5.304948329925537,
      "learning_rate": 1.2238688157258168e-05,
      "loss": 1.7397,
      "step": 593200
    },
    {
      "epoch": 45.321213047131614,
      "grad_norm": 5.276116847991943,
      "learning_rate": 1.2232322460723653e-05,
      "loss": 1.6288,
      "step": 593300
    },
    {
      "epoch": 45.328851882973034,
      "grad_norm": 4.974532127380371,
      "learning_rate": 1.2225956764189138e-05,
      "loss": 1.6752,
      "step": 593400
    },
    {
      "epoch": 45.33649071881445,
      "grad_norm": 5.949202537536621,
      "learning_rate": 1.2219591067654623e-05,
      "loss": 1.6305,
      "step": 593500
    },
    {
      "epoch": 45.34412955465587,
      "grad_norm": 6.424197673797607,
      "learning_rate": 1.2213225371120108e-05,
      "loss": 1.7077,
      "step": 593600
    },
    {
      "epoch": 45.35176839049729,
      "grad_norm": 5.939901828765869,
      "learning_rate": 1.2206859674585594e-05,
      "loss": 1.807,
      "step": 593700
    },
    {
      "epoch": 45.3594072263387,
      "grad_norm": 8.035174369812012,
      "learning_rate": 1.2200493978051079e-05,
      "loss": 1.6681,
      "step": 593800
    },
    {
      "epoch": 45.36704606218012,
      "grad_norm": 6.5623860359191895,
      "learning_rate": 1.2194128281516564e-05,
      "loss": 1.7882,
      "step": 593900
    },
    {
      "epoch": 45.37468489802154,
      "grad_norm": 6.0400776863098145,
      "learning_rate": 1.218776258498205e-05,
      "loss": 1.684,
      "step": 594000
    },
    {
      "epoch": 45.38232373386296,
      "grad_norm": 5.348831653594971,
      "learning_rate": 1.2181396888447535e-05,
      "loss": 1.7206,
      "step": 594100
    },
    {
      "epoch": 45.38996256970438,
      "grad_norm": 5.619435787200928,
      "learning_rate": 1.217503119191302e-05,
      "loss": 1.7467,
      "step": 594200
    },
    {
      "epoch": 45.39760140554579,
      "grad_norm": 6.40915060043335,
      "learning_rate": 1.2168665495378505e-05,
      "loss": 1.6245,
      "step": 594300
    },
    {
      "epoch": 45.40524024138721,
      "grad_norm": 4.149405479431152,
      "learning_rate": 1.216229979884399e-05,
      "loss": 1.6333,
      "step": 594400
    },
    {
      "epoch": 45.41287907722863,
      "grad_norm": 6.589438438415527,
      "learning_rate": 1.2155934102309476e-05,
      "loss": 1.7084,
      "step": 594500
    },
    {
      "epoch": 45.42051791307005,
      "grad_norm": 6.260027885437012,
      "learning_rate": 1.214956840577496e-05,
      "loss": 1.7299,
      "step": 594600
    },
    {
      "epoch": 45.42815674891147,
      "grad_norm": 7.001885890960693,
      "learning_rate": 1.2143202709240446e-05,
      "loss": 1.655,
      "step": 594700
    },
    {
      "epoch": 45.43579558475288,
      "grad_norm": 5.977808475494385,
      "learning_rate": 1.213683701270593e-05,
      "loss": 1.6699,
      "step": 594800
    },
    {
      "epoch": 45.4434344205943,
      "grad_norm": 5.786457538604736,
      "learning_rate": 1.2130471316171416e-05,
      "loss": 1.7806,
      "step": 594900
    },
    {
      "epoch": 45.45107325643572,
      "grad_norm": 14.473287582397461,
      "learning_rate": 1.2124105619636902e-05,
      "loss": 1.7086,
      "step": 595000
    },
    {
      "epoch": 45.45871209227714,
      "grad_norm": 5.540748596191406,
      "learning_rate": 1.2117739923102387e-05,
      "loss": 1.7417,
      "step": 595100
    },
    {
      "epoch": 45.46635092811856,
      "grad_norm": 4.3391499519348145,
      "learning_rate": 1.2111374226567872e-05,
      "loss": 1.8219,
      "step": 595200
    },
    {
      "epoch": 45.47398976395997,
      "grad_norm": 4.6325788497924805,
      "learning_rate": 1.2105008530033357e-05,
      "loss": 1.659,
      "step": 595300
    },
    {
      "epoch": 45.48162859980139,
      "grad_norm": 3.8387434482574463,
      "learning_rate": 1.2098642833498843e-05,
      "loss": 1.7084,
      "step": 595400
    },
    {
      "epoch": 45.48926743564281,
      "grad_norm": 6.905385971069336,
      "learning_rate": 1.2092277136964328e-05,
      "loss": 1.6598,
      "step": 595500
    },
    {
      "epoch": 45.49690627148423,
      "grad_norm": 6.261168956756592,
      "learning_rate": 1.2085911440429811e-05,
      "loss": 1.6691,
      "step": 595600
    },
    {
      "epoch": 45.50454510732564,
      "grad_norm": 6.329539775848389,
      "learning_rate": 1.2079545743895297e-05,
      "loss": 1.6902,
      "step": 595700
    },
    {
      "epoch": 45.51218394316706,
      "grad_norm": 6.057293891906738,
      "learning_rate": 1.2073180047360782e-05,
      "loss": 1.6761,
      "step": 595800
    },
    {
      "epoch": 45.51982277900848,
      "grad_norm": 5.233827114105225,
      "learning_rate": 1.2066814350826269e-05,
      "loss": 1.6664,
      "step": 595900
    },
    {
      "epoch": 45.5274616148499,
      "grad_norm": 6.14301872253418,
      "learning_rate": 1.2060448654291754e-05,
      "loss": 1.6437,
      "step": 596000
    },
    {
      "epoch": 45.53510045069132,
      "grad_norm": 5.914576530456543,
      "learning_rate": 1.205408295775724e-05,
      "loss": 1.5908,
      "step": 596100
    },
    {
      "epoch": 45.54273928653273,
      "grad_norm": 2.9389190673828125,
      "learning_rate": 1.2047717261222724e-05,
      "loss": 1.68,
      "step": 596200
    },
    {
      "epoch": 45.55037812237415,
      "grad_norm": 6.620063304901123,
      "learning_rate": 1.204135156468821e-05,
      "loss": 1.6216,
      "step": 596300
    },
    {
      "epoch": 45.55801695821557,
      "grad_norm": 5.382903575897217,
      "learning_rate": 1.2034985868153693e-05,
      "loss": 1.6984,
      "step": 596400
    },
    {
      "epoch": 45.56565579405699,
      "grad_norm": 5.190389156341553,
      "learning_rate": 1.2028620171619178e-05,
      "loss": 1.6925,
      "step": 596500
    },
    {
      "epoch": 45.573294629898406,
      "grad_norm": 3.1380114555358887,
      "learning_rate": 1.2022254475084664e-05,
      "loss": 1.6378,
      "step": 596600
    },
    {
      "epoch": 45.58093346573982,
      "grad_norm": 5.880591869354248,
      "learning_rate": 1.2015888778550149e-05,
      "loss": 1.7246,
      "step": 596700
    },
    {
      "epoch": 45.58857230158124,
      "grad_norm": 4.2564496994018555,
      "learning_rate": 1.2009523082015634e-05,
      "loss": 1.6778,
      "step": 596800
    },
    {
      "epoch": 45.59621113742266,
      "grad_norm": 6.668311595916748,
      "learning_rate": 1.200315738548112e-05,
      "loss": 1.6309,
      "step": 596900
    },
    {
      "epoch": 45.603849973264076,
      "grad_norm": 4.971070289611816,
      "learning_rate": 1.1996791688946606e-05,
      "loss": 1.7043,
      "step": 597000
    },
    {
      "epoch": 45.611488809105495,
      "grad_norm": 4.824552536010742,
      "learning_rate": 1.1990425992412092e-05,
      "loss": 1.666,
      "step": 597100
    },
    {
      "epoch": 45.61912764494691,
      "grad_norm": 5.615444183349609,
      "learning_rate": 1.1984060295877575e-05,
      "loss": 1.6081,
      "step": 597200
    },
    {
      "epoch": 45.626766480788326,
      "grad_norm": 5.484137058258057,
      "learning_rate": 1.197769459934306e-05,
      "loss": 1.6912,
      "step": 597300
    },
    {
      "epoch": 45.634405316629746,
      "grad_norm": 3.9295847415924072,
      "learning_rate": 1.1971328902808546e-05,
      "loss": 1.7261,
      "step": 597400
    },
    {
      "epoch": 45.642044152471165,
      "grad_norm": 5.824804782867432,
      "learning_rate": 1.196496320627403e-05,
      "loss": 1.7685,
      "step": 597500
    },
    {
      "epoch": 45.649682988312584,
      "grad_norm": 6.569800853729248,
      "learning_rate": 1.1958597509739516e-05,
      "loss": 1.6397,
      "step": 597600
    },
    {
      "epoch": 45.657321824153996,
      "grad_norm": 6.720727920532227,
      "learning_rate": 1.1952231813205001e-05,
      "loss": 1.7162,
      "step": 597700
    },
    {
      "epoch": 45.664960659995415,
      "grad_norm": 4.619913101196289,
      "learning_rate": 1.1945866116670486e-05,
      "loss": 1.5819,
      "step": 597800
    },
    {
      "epoch": 45.672599495836835,
      "grad_norm": 4.499333381652832,
      "learning_rate": 1.1939500420135972e-05,
      "loss": 1.5964,
      "step": 597900
    },
    {
      "epoch": 45.680238331678254,
      "grad_norm": 5.764429092407227,
      "learning_rate": 1.1933134723601457e-05,
      "loss": 1.7428,
      "step": 598000
    },
    {
      "epoch": 45.68787716751967,
      "grad_norm": 7.047331809997559,
      "learning_rate": 1.1926769027066942e-05,
      "loss": 1.7134,
      "step": 598100
    },
    {
      "epoch": 45.695516003361085,
      "grad_norm": 5.44045352935791,
      "learning_rate": 1.1920403330532427e-05,
      "loss": 1.5891,
      "step": 598200
    },
    {
      "epoch": 45.703154839202504,
      "grad_norm": 7.485546588897705,
      "learning_rate": 1.1914037633997913e-05,
      "loss": 1.6836,
      "step": 598300
    },
    {
      "epoch": 45.710793675043924,
      "grad_norm": 4.662995338439941,
      "learning_rate": 1.1907671937463398e-05,
      "loss": 1.5979,
      "step": 598400
    },
    {
      "epoch": 45.71843251088534,
      "grad_norm": 4.981539249420166,
      "learning_rate": 1.1901306240928883e-05,
      "loss": 1.7381,
      "step": 598500
    },
    {
      "epoch": 45.72607134672676,
      "grad_norm": 6.052089214324951,
      "learning_rate": 1.1894940544394368e-05,
      "loss": 1.7728,
      "step": 598600
    },
    {
      "epoch": 45.733710182568174,
      "grad_norm": 6.0616631507873535,
      "learning_rate": 1.1888574847859854e-05,
      "loss": 1.6431,
      "step": 598700
    },
    {
      "epoch": 45.74134901840959,
      "grad_norm": 4.7763190269470215,
      "learning_rate": 1.1882209151325339e-05,
      "loss": 1.7368,
      "step": 598800
    },
    {
      "epoch": 45.74898785425101,
      "grad_norm": 6.418002128601074,
      "learning_rate": 1.1875843454790824e-05,
      "loss": 1.7096,
      "step": 598900
    },
    {
      "epoch": 45.75662669009243,
      "grad_norm": 4.6281514167785645,
      "learning_rate": 1.186947775825631e-05,
      "loss": 1.5184,
      "step": 599000
    },
    {
      "epoch": 45.76426552593385,
      "grad_norm": 6.747506141662598,
      "learning_rate": 1.1863112061721794e-05,
      "loss": 1.6299,
      "step": 599100
    },
    {
      "epoch": 45.77190436177526,
      "grad_norm": 5.590253829956055,
      "learning_rate": 1.185674636518728e-05,
      "loss": 1.6416,
      "step": 599200
    },
    {
      "epoch": 45.77954319761668,
      "grad_norm": 5.955599308013916,
      "learning_rate": 1.1850380668652765e-05,
      "loss": 1.6425,
      "step": 599300
    },
    {
      "epoch": 45.7871820334581,
      "grad_norm": 4.408376693725586,
      "learning_rate": 1.184401497211825e-05,
      "loss": 1.6381,
      "step": 599400
    },
    {
      "epoch": 45.79482086929952,
      "grad_norm": 6.787402153015137,
      "learning_rate": 1.1837649275583734e-05,
      "loss": 1.6749,
      "step": 599500
    },
    {
      "epoch": 45.80245970514093,
      "grad_norm": 8.48352336883545,
      "learning_rate": 1.1831283579049219e-05,
      "loss": 1.6847,
      "step": 599600
    },
    {
      "epoch": 45.81009854098235,
      "grad_norm": 6.791283130645752,
      "learning_rate": 1.1824917882514706e-05,
      "loss": 1.7285,
      "step": 599700
    },
    {
      "epoch": 45.81773737682377,
      "grad_norm": 5.188232898712158,
      "learning_rate": 1.1818552185980191e-05,
      "loss": 1.6717,
      "step": 599800
    },
    {
      "epoch": 45.82537621266519,
      "grad_norm": 3.8891944885253906,
      "learning_rate": 1.1812186489445676e-05,
      "loss": 1.6707,
      "step": 599900
    },
    {
      "epoch": 45.83301504850661,
      "grad_norm": 6.289470672607422,
      "learning_rate": 1.1805820792911162e-05,
      "loss": 1.7159,
      "step": 600000
    },
    {
      "epoch": 45.84065388434802,
      "grad_norm": 5.342958450317383,
      "learning_rate": 1.1799455096376647e-05,
      "loss": 1.6473,
      "step": 600100
    },
    {
      "epoch": 45.84829272018944,
      "grad_norm": 6.029717445373535,
      "learning_rate": 1.1793089399842132e-05,
      "loss": 1.7137,
      "step": 600200
    },
    {
      "epoch": 45.85593155603086,
      "grad_norm": 5.579313278198242,
      "learning_rate": 1.1786723703307616e-05,
      "loss": 1.7004,
      "step": 600300
    },
    {
      "epoch": 45.86357039187228,
      "grad_norm": 7.3761887550354,
      "learning_rate": 1.17803580067731e-05,
      "loss": 1.6431,
      "step": 600400
    },
    {
      "epoch": 45.8712092277137,
      "grad_norm": 4.850780963897705,
      "learning_rate": 1.1773992310238586e-05,
      "loss": 1.568,
      "step": 600500
    },
    {
      "epoch": 45.87884806355511,
      "grad_norm": 6.412662982940674,
      "learning_rate": 1.1767626613704071e-05,
      "loss": 1.715,
      "step": 600600
    },
    {
      "epoch": 45.88648689939653,
      "grad_norm": 5.8183088302612305,
      "learning_rate": 1.1761260917169556e-05,
      "loss": 1.6154,
      "step": 600700
    },
    {
      "epoch": 45.89412573523795,
      "grad_norm": 6.831126689910889,
      "learning_rate": 1.1754895220635043e-05,
      "loss": 1.6841,
      "step": 600800
    },
    {
      "epoch": 45.90176457107937,
      "grad_norm": 5.653686046600342,
      "learning_rate": 1.1748529524100529e-05,
      "loss": 1.6469,
      "step": 600900
    },
    {
      "epoch": 45.90940340692079,
      "grad_norm": 5.928774833679199,
      "learning_rate": 1.1742163827566014e-05,
      "loss": 1.6415,
      "step": 601000
    },
    {
      "epoch": 45.9170422427622,
      "grad_norm": 6.1427106857299805,
      "learning_rate": 1.1735798131031497e-05,
      "loss": 1.6669,
      "step": 601100
    },
    {
      "epoch": 45.92468107860362,
      "grad_norm": 6.094248294830322,
      "learning_rate": 1.1729432434496983e-05,
      "loss": 1.6842,
      "step": 601200
    },
    {
      "epoch": 45.93231991444504,
      "grad_norm": 6.409873008728027,
      "learning_rate": 1.1723066737962468e-05,
      "loss": 1.6798,
      "step": 601300
    },
    {
      "epoch": 45.93995875028646,
      "grad_norm": 7.575949668884277,
      "learning_rate": 1.1716701041427953e-05,
      "loss": 1.7111,
      "step": 601400
    },
    {
      "epoch": 45.94759758612788,
      "grad_norm": 5.8483381271362305,
      "learning_rate": 1.1710335344893438e-05,
      "loss": 1.6972,
      "step": 601500
    },
    {
      "epoch": 45.95523642196929,
      "grad_norm": 5.376742362976074,
      "learning_rate": 1.1703969648358924e-05,
      "loss": 1.7112,
      "step": 601600
    },
    {
      "epoch": 45.96287525781071,
      "grad_norm": 7.895936965942383,
      "learning_rate": 1.1697603951824409e-05,
      "loss": 1.6877,
      "step": 601700
    },
    {
      "epoch": 45.97051409365213,
      "grad_norm": 5.9198994636535645,
      "learning_rate": 1.1691238255289896e-05,
      "loss": 1.7437,
      "step": 601800
    },
    {
      "epoch": 45.97815292949355,
      "grad_norm": 7.240074634552002,
      "learning_rate": 1.168487255875538e-05,
      "loss": 1.6744,
      "step": 601900
    },
    {
      "epoch": 45.985791765334966,
      "grad_norm": 4.044723987579346,
      "learning_rate": 1.1678506862220864e-05,
      "loss": 1.6877,
      "step": 602000
    },
    {
      "epoch": 45.99343060117638,
      "grad_norm": 7.765266418457031,
      "learning_rate": 1.167214116568635e-05,
      "loss": 1.7551,
      "step": 602100
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.7718254327774048,
      "eval_runtime": 3.3964,
      "eval_samples_per_second": 203.154,
      "eval_steps_per_second": 203.154,
      "step": 602186
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.436265230178833,
      "eval_runtime": 63.1016,
      "eval_samples_per_second": 207.459,
      "eval_steps_per_second": 207.459,
      "step": 602186
    },
    {
      "epoch": 46.0010694370178,
      "grad_norm": 6.372219085693359,
      "learning_rate": 1.1665775469151835e-05,
      "loss": 1.7216,
      "step": 602200
    },
    {
      "epoch": 46.00870827285922,
      "grad_norm": 5.305859565734863,
      "learning_rate": 1.165940977261732e-05,
      "loss": 1.6137,
      "step": 602300
    },
    {
      "epoch": 46.016347108700636,
      "grad_norm": 5.6098246574401855,
      "learning_rate": 1.1653044076082805e-05,
      "loss": 1.7035,
      "step": 602400
    },
    {
      "epoch": 46.023985944542055,
      "grad_norm": 5.634286403656006,
      "learning_rate": 1.164667837954829e-05,
      "loss": 1.6477,
      "step": 602500
    },
    {
      "epoch": 46.03162478038347,
      "grad_norm": 6.332550048828125,
      "learning_rate": 1.1640312683013776e-05,
      "loss": 1.629,
      "step": 602600
    },
    {
      "epoch": 46.039263616224886,
      "grad_norm": 5.930126190185547,
      "learning_rate": 1.1633946986479261e-05,
      "loss": 1.673,
      "step": 602700
    },
    {
      "epoch": 46.046902452066306,
      "grad_norm": 5.000491142272949,
      "learning_rate": 1.1627581289944746e-05,
      "loss": 1.669,
      "step": 602800
    },
    {
      "epoch": 46.054541287907725,
      "grad_norm": 8.798872947692871,
      "learning_rate": 1.1621215593410232e-05,
      "loss": 1.5866,
      "step": 602900
    },
    {
      "epoch": 46.062180123749144,
      "grad_norm": 6.55980110168457,
      "learning_rate": 1.1614849896875717e-05,
      "loss": 1.6367,
      "step": 603000
    },
    {
      "epoch": 46.069818959590556,
      "grad_norm": 6.858345031738281,
      "learning_rate": 1.1608484200341202e-05,
      "loss": 1.6565,
      "step": 603100
    },
    {
      "epoch": 46.077457795431975,
      "grad_norm": 5.718042373657227,
      "learning_rate": 1.1602118503806687e-05,
      "loss": 1.5728,
      "step": 603200
    },
    {
      "epoch": 46.085096631273395,
      "grad_norm": 5.264037132263184,
      "learning_rate": 1.1595752807272172e-05,
      "loss": 1.6789,
      "step": 603300
    },
    {
      "epoch": 46.092735467114814,
      "grad_norm": 5.280074596405029,
      "learning_rate": 1.1589387110737658e-05,
      "loss": 1.6754,
      "step": 603400
    },
    {
      "epoch": 46.10037430295623,
      "grad_norm": 6.170809745788574,
      "learning_rate": 1.1583021414203143e-05,
      "loss": 1.6407,
      "step": 603500
    },
    {
      "epoch": 46.108013138797645,
      "grad_norm": 7.834022045135498,
      "learning_rate": 1.1576655717668628e-05,
      "loss": 1.6787,
      "step": 603600
    },
    {
      "epoch": 46.115651974639064,
      "grad_norm": 6.069819927215576,
      "learning_rate": 1.1570290021134113e-05,
      "loss": 1.6843,
      "step": 603700
    },
    {
      "epoch": 46.12329081048048,
      "grad_norm": 5.44399356842041,
      "learning_rate": 1.1563924324599599e-05,
      "loss": 1.6482,
      "step": 603800
    },
    {
      "epoch": 46.1309296463219,
      "grad_norm": 6.289117336273193,
      "learning_rate": 1.1557558628065084e-05,
      "loss": 1.6733,
      "step": 603900
    },
    {
      "epoch": 46.138568482163315,
      "grad_norm": 5.2188615798950195,
      "learning_rate": 1.1551192931530569e-05,
      "loss": 1.7111,
      "step": 604000
    },
    {
      "epoch": 46.146207318004734,
      "grad_norm": 5.17395544052124,
      "learning_rate": 1.1544827234996054e-05,
      "loss": 1.603,
      "step": 604100
    },
    {
      "epoch": 46.15384615384615,
      "grad_norm": 5.3520331382751465,
      "learning_rate": 1.153846153846154e-05,
      "loss": 1.8287,
      "step": 604200
    },
    {
      "epoch": 46.16148498968757,
      "grad_norm": 6.605811595916748,
      "learning_rate": 1.1532095841927023e-05,
      "loss": 1.609,
      "step": 604300
    },
    {
      "epoch": 46.16912382552899,
      "grad_norm": 4.664291858673096,
      "learning_rate": 1.1525730145392508e-05,
      "loss": 1.6661,
      "step": 604400
    },
    {
      "epoch": 46.176762661370404,
      "grad_norm": 5.2572150230407715,
      "learning_rate": 1.1519364448857994e-05,
      "loss": 1.7595,
      "step": 604500
    },
    {
      "epoch": 46.18440149721182,
      "grad_norm": 4.607580184936523,
      "learning_rate": 1.151299875232348e-05,
      "loss": 1.7582,
      "step": 604600
    },
    {
      "epoch": 46.19204033305324,
      "grad_norm": 5.701722621917725,
      "learning_rate": 1.1506633055788966e-05,
      "loss": 1.7474,
      "step": 604700
    },
    {
      "epoch": 46.19967916889466,
      "grad_norm": 5.361701011657715,
      "learning_rate": 1.1500267359254451e-05,
      "loss": 1.5831,
      "step": 604800
    },
    {
      "epoch": 46.20731800473608,
      "grad_norm": 5.82474946975708,
      "learning_rate": 1.1493901662719936e-05,
      "loss": 1.6552,
      "step": 604900
    },
    {
      "epoch": 46.21495684057749,
      "grad_norm": 6.773707866668701,
      "learning_rate": 1.1487535966185421e-05,
      "loss": 1.6141,
      "step": 605000
    },
    {
      "epoch": 46.22259567641891,
      "grad_norm": 5.8452606201171875,
      "learning_rate": 1.1481170269650905e-05,
      "loss": 1.6116,
      "step": 605100
    },
    {
      "epoch": 46.23023451226033,
      "grad_norm": 6.01795768737793,
      "learning_rate": 1.147480457311639e-05,
      "loss": 1.6864,
      "step": 605200
    },
    {
      "epoch": 46.23787334810175,
      "grad_norm": 6.38190221786499,
      "learning_rate": 1.1468438876581875e-05,
      "loss": 1.6352,
      "step": 605300
    },
    {
      "epoch": 46.24551218394317,
      "grad_norm": 6.70430850982666,
      "learning_rate": 1.146207318004736e-05,
      "loss": 1.6321,
      "step": 605400
    },
    {
      "epoch": 46.25315101978458,
      "grad_norm": 4.6434407234191895,
      "learning_rate": 1.1455707483512846e-05,
      "loss": 1.7473,
      "step": 605500
    },
    {
      "epoch": 46.260789855626,
      "grad_norm": 5.031226634979248,
      "learning_rate": 1.1449341786978333e-05,
      "loss": 1.7551,
      "step": 605600
    },
    {
      "epoch": 46.26842869146742,
      "grad_norm": 4.673890590667725,
      "learning_rate": 1.1442976090443818e-05,
      "loss": 1.5436,
      "step": 605700
    },
    {
      "epoch": 46.27606752730884,
      "grad_norm": 5.857415199279785,
      "learning_rate": 1.1436610393909303e-05,
      "loss": 1.6607,
      "step": 605800
    },
    {
      "epoch": 46.28370636315026,
      "grad_norm": 5.918006896972656,
      "learning_rate": 1.1430244697374787e-05,
      "loss": 1.7179,
      "step": 605900
    },
    {
      "epoch": 46.29134519899167,
      "grad_norm": 7.575809001922607,
      "learning_rate": 1.1423879000840272e-05,
      "loss": 1.684,
      "step": 606000
    },
    {
      "epoch": 46.29898403483309,
      "grad_norm": 2.561359167098999,
      "learning_rate": 1.1417513304305757e-05,
      "loss": 1.6036,
      "step": 606100
    },
    {
      "epoch": 46.30662287067451,
      "grad_norm": 5.953383922576904,
      "learning_rate": 1.1411147607771242e-05,
      "loss": 1.6965,
      "step": 606200
    },
    {
      "epoch": 46.31426170651593,
      "grad_norm": 5.867847442626953,
      "learning_rate": 1.1404781911236728e-05,
      "loss": 1.751,
      "step": 606300
    },
    {
      "epoch": 46.32190054235735,
      "grad_norm": 5.50415563583374,
      "learning_rate": 1.1398416214702213e-05,
      "loss": 1.7246,
      "step": 606400
    },
    {
      "epoch": 46.32953937819876,
      "grad_norm": 4.66446590423584,
      "learning_rate": 1.1392050518167698e-05,
      "loss": 1.7424,
      "step": 606500
    },
    {
      "epoch": 46.33717821404018,
      "grad_norm": 4.445468425750732,
      "learning_rate": 1.1385684821633185e-05,
      "loss": 1.5952,
      "step": 606600
    },
    {
      "epoch": 46.3448170498816,
      "grad_norm": 5.9651923179626465,
      "learning_rate": 1.1379319125098669e-05,
      "loss": 1.625,
      "step": 606700
    },
    {
      "epoch": 46.35245588572302,
      "grad_norm": 5.134461402893066,
      "learning_rate": 1.1372953428564154e-05,
      "loss": 1.665,
      "step": 606800
    },
    {
      "epoch": 46.36009472156444,
      "grad_norm": 5.085220813751221,
      "learning_rate": 1.1366587732029639e-05,
      "loss": 1.7154,
      "step": 606900
    },
    {
      "epoch": 46.36773355740585,
      "grad_norm": 7.308363914489746,
      "learning_rate": 1.1360222035495124e-05,
      "loss": 1.6807,
      "step": 607000
    },
    {
      "epoch": 46.37537239324727,
      "grad_norm": 6.62831974029541,
      "learning_rate": 1.135385633896061e-05,
      "loss": 1.6101,
      "step": 607100
    },
    {
      "epoch": 46.38301122908869,
      "grad_norm": 5.263583660125732,
      "learning_rate": 1.1347490642426095e-05,
      "loss": 1.6067,
      "step": 607200
    },
    {
      "epoch": 46.39065006493011,
      "grad_norm": 6.286208629608154,
      "learning_rate": 1.134112494589158e-05,
      "loss": 1.7321,
      "step": 607300
    },
    {
      "epoch": 46.398288900771526,
      "grad_norm": 6.577287673950195,
      "learning_rate": 1.1334759249357065e-05,
      "loss": 1.7224,
      "step": 607400
    },
    {
      "epoch": 46.40592773661294,
      "grad_norm": 4.613988399505615,
      "learning_rate": 1.132839355282255e-05,
      "loss": 1.5857,
      "step": 607500
    },
    {
      "epoch": 46.41356657245436,
      "grad_norm": 5.4485087394714355,
      "learning_rate": 1.1322027856288036e-05,
      "loss": 1.6594,
      "step": 607600
    },
    {
      "epoch": 46.421205408295776,
      "grad_norm": 5.226426601409912,
      "learning_rate": 1.1315662159753521e-05,
      "loss": 1.6903,
      "step": 607700
    },
    {
      "epoch": 46.428844244137196,
      "grad_norm": 7.9812469482421875,
      "learning_rate": 1.1309296463219006e-05,
      "loss": 1.7129,
      "step": 607800
    },
    {
      "epoch": 46.436483079978615,
      "grad_norm": 5.184629440307617,
      "learning_rate": 1.1302930766684491e-05,
      "loss": 1.7125,
      "step": 607900
    },
    {
      "epoch": 46.44412191582003,
      "grad_norm": 7.319497108459473,
      "learning_rate": 1.1296565070149977e-05,
      "loss": 1.6713,
      "step": 608000
    },
    {
      "epoch": 46.451760751661446,
      "grad_norm": 5.273319244384766,
      "learning_rate": 1.1290199373615462e-05,
      "loss": 1.76,
      "step": 608100
    },
    {
      "epoch": 46.459399587502865,
      "grad_norm": 4.340424060821533,
      "learning_rate": 1.1283833677080945e-05,
      "loss": 1.6649,
      "step": 608200
    },
    {
      "epoch": 46.467038423344285,
      "grad_norm": 5.793267250061035,
      "learning_rate": 1.127746798054643e-05,
      "loss": 1.7733,
      "step": 608300
    },
    {
      "epoch": 46.4746772591857,
      "grad_norm": 5.838311672210693,
      "learning_rate": 1.1271102284011918e-05,
      "loss": 1.6443,
      "step": 608400
    },
    {
      "epoch": 46.482316095027116,
      "grad_norm": 6.494668483734131,
      "learning_rate": 1.1264736587477403e-05,
      "loss": 1.6704,
      "step": 608500
    },
    {
      "epoch": 46.489954930868535,
      "grad_norm": 5.650387763977051,
      "learning_rate": 1.1258370890942888e-05,
      "loss": 1.6209,
      "step": 608600
    },
    {
      "epoch": 46.497593766709954,
      "grad_norm": 5.745364665985107,
      "learning_rate": 1.1252005194408373e-05,
      "loss": 1.7481,
      "step": 608700
    },
    {
      "epoch": 46.505232602551374,
      "grad_norm": 5.030436992645264,
      "learning_rate": 1.1245639497873858e-05,
      "loss": 1.638,
      "step": 608800
    },
    {
      "epoch": 46.512871438392786,
      "grad_norm": 5.2564311027526855,
      "learning_rate": 1.1239273801339344e-05,
      "loss": 1.6407,
      "step": 608900
    },
    {
      "epoch": 46.520510274234205,
      "grad_norm": 8.546146392822266,
      "learning_rate": 1.1232908104804827e-05,
      "loss": 1.6785,
      "step": 609000
    },
    {
      "epoch": 46.528149110075624,
      "grad_norm": 5.8550496101379395,
      "learning_rate": 1.1226542408270312e-05,
      "loss": 1.6931,
      "step": 609100
    },
    {
      "epoch": 46.53578794591704,
      "grad_norm": 5.997428894042969,
      "learning_rate": 1.1220176711735798e-05,
      "loss": 1.7217,
      "step": 609200
    },
    {
      "epoch": 46.54342678175846,
      "grad_norm": 5.594178199768066,
      "learning_rate": 1.1213811015201283e-05,
      "loss": 1.6214,
      "step": 609300
    },
    {
      "epoch": 46.551065617599875,
      "grad_norm": 7.638487815856934,
      "learning_rate": 1.120744531866677e-05,
      "loss": 1.631,
      "step": 609400
    },
    {
      "epoch": 46.558704453441294,
      "grad_norm": 5.664575576782227,
      "learning_rate": 1.1201079622132255e-05,
      "loss": 1.6968,
      "step": 609500
    },
    {
      "epoch": 46.56634328928271,
      "grad_norm": 5.26133394241333,
      "learning_rate": 1.119471392559774e-05,
      "loss": 1.6098,
      "step": 609600
    },
    {
      "epoch": 46.57398212512413,
      "grad_norm": 5.677411079406738,
      "learning_rate": 1.1188348229063226e-05,
      "loss": 1.6345,
      "step": 609700
    },
    {
      "epoch": 46.58162096096555,
      "grad_norm": 6.081544399261475,
      "learning_rate": 1.1181982532528709e-05,
      "loss": 1.6662,
      "step": 609800
    },
    {
      "epoch": 46.589259796806964,
      "grad_norm": 7.244982719421387,
      "learning_rate": 1.1175616835994194e-05,
      "loss": 1.6456,
      "step": 609900
    },
    {
      "epoch": 46.59689863264838,
      "grad_norm": 4.653357982635498,
      "learning_rate": 1.116925113945968e-05,
      "loss": 1.7121,
      "step": 610000
    },
    {
      "epoch": 46.6045374684898,
      "grad_norm": 4.270134925842285,
      "learning_rate": 1.1162885442925165e-05,
      "loss": 1.751,
      "step": 610100
    },
    {
      "epoch": 46.61217630433122,
      "grad_norm": 7.877285003662109,
      "learning_rate": 1.115651974639065e-05,
      "loss": 1.5804,
      "step": 610200
    },
    {
      "epoch": 46.61981514017264,
      "grad_norm": 7.3599772453308105,
      "learning_rate": 1.1150154049856135e-05,
      "loss": 1.7155,
      "step": 610300
    },
    {
      "epoch": 46.62745397601405,
      "grad_norm": 4.215713024139404,
      "learning_rate": 1.1143788353321622e-05,
      "loss": 1.5981,
      "step": 610400
    },
    {
      "epoch": 46.63509281185547,
      "grad_norm": 5.252090930938721,
      "learning_rate": 1.1137422656787107e-05,
      "loss": 1.6152,
      "step": 610500
    },
    {
      "epoch": 46.64273164769689,
      "grad_norm": 5.775940418243408,
      "learning_rate": 1.1131056960252591e-05,
      "loss": 1.6256,
      "step": 610600
    },
    {
      "epoch": 46.65037048353831,
      "grad_norm": 7.10549259185791,
      "learning_rate": 1.1124691263718076e-05,
      "loss": 1.8207,
      "step": 610700
    },
    {
      "epoch": 46.65800931937973,
      "grad_norm": 4.544776916503906,
      "learning_rate": 1.1118325567183561e-05,
      "loss": 1.6661,
      "step": 610800
    },
    {
      "epoch": 46.66564815522114,
      "grad_norm": 5.5146284103393555,
      "learning_rate": 1.1111959870649047e-05,
      "loss": 1.6513,
      "step": 610900
    },
    {
      "epoch": 46.67328699106256,
      "grad_norm": 5.237915515899658,
      "learning_rate": 1.1105594174114532e-05,
      "loss": 1.7054,
      "step": 611000
    },
    {
      "epoch": 46.68092582690398,
      "grad_norm": 6.6677350997924805,
      "learning_rate": 1.1099228477580017e-05,
      "loss": 1.6393,
      "step": 611100
    },
    {
      "epoch": 46.6885646627454,
      "grad_norm": 4.887288570404053,
      "learning_rate": 1.1092862781045502e-05,
      "loss": 1.6486,
      "step": 611200
    },
    {
      "epoch": 46.69620349858682,
      "grad_norm": 5.220915794372559,
      "learning_rate": 1.1086497084510988e-05,
      "loss": 1.6583,
      "step": 611300
    },
    {
      "epoch": 46.70384233442823,
      "grad_norm": 4.468138217926025,
      "learning_rate": 1.1080131387976473e-05,
      "loss": 1.7099,
      "step": 611400
    },
    {
      "epoch": 46.71148117026965,
      "grad_norm": 5.946576118469238,
      "learning_rate": 1.1073765691441958e-05,
      "loss": 1.7076,
      "step": 611500
    },
    {
      "epoch": 46.71912000611107,
      "grad_norm": 3.37235951423645,
      "learning_rate": 1.1067399994907443e-05,
      "loss": 1.7121,
      "step": 611600
    },
    {
      "epoch": 46.72675884195249,
      "grad_norm": 4.938663959503174,
      "learning_rate": 1.1061034298372928e-05,
      "loss": 1.7237,
      "step": 611700
    },
    {
      "epoch": 46.73439767779391,
      "grad_norm": 3.8652541637420654,
      "learning_rate": 1.1054668601838414e-05,
      "loss": 1.7175,
      "step": 611800
    },
    {
      "epoch": 46.74203651363532,
      "grad_norm": 7.193634510040283,
      "learning_rate": 1.1048302905303899e-05,
      "loss": 1.7259,
      "step": 611900
    },
    {
      "epoch": 46.74967534947674,
      "grad_norm": 4.822597980499268,
      "learning_rate": 1.1041937208769384e-05,
      "loss": 1.7074,
      "step": 612000
    },
    {
      "epoch": 46.75731418531816,
      "grad_norm": 5.714976787567139,
      "learning_rate": 1.103557151223487e-05,
      "loss": 1.6951,
      "step": 612100
    },
    {
      "epoch": 46.76495302115958,
      "grad_norm": 6.105940341949463,
      "learning_rate": 1.1029205815700355e-05,
      "loss": 1.7682,
      "step": 612200
    },
    {
      "epoch": 46.77259185700099,
      "grad_norm": 6.813441276550293,
      "learning_rate": 1.102284011916584e-05,
      "loss": 1.6985,
      "step": 612300
    },
    {
      "epoch": 46.78023069284241,
      "grad_norm": 5.933877468109131,
      "learning_rate": 1.1016474422631325e-05,
      "loss": 1.6671,
      "step": 612400
    },
    {
      "epoch": 46.78786952868383,
      "grad_norm": 6.375633716583252,
      "learning_rate": 1.101010872609681e-05,
      "loss": 1.647,
      "step": 612500
    },
    {
      "epoch": 46.79550836452525,
      "grad_norm": 7.34278678894043,
      "learning_rate": 1.1003743029562296e-05,
      "loss": 1.7041,
      "step": 612600
    },
    {
      "epoch": 46.803147200366666,
      "grad_norm": 7.223268985748291,
      "learning_rate": 1.099737733302778e-05,
      "loss": 1.6743,
      "step": 612700
    },
    {
      "epoch": 46.81078603620808,
      "grad_norm": 4.258143901824951,
      "learning_rate": 1.0991011636493266e-05,
      "loss": 1.6815,
      "step": 612800
    },
    {
      "epoch": 46.8184248720495,
      "grad_norm": 9.021712303161621,
      "learning_rate": 1.0984645939958751e-05,
      "loss": 1.6437,
      "step": 612900
    },
    {
      "epoch": 46.82606370789092,
      "grad_norm": 5.42758846282959,
      "learning_rate": 1.0978280243424235e-05,
      "loss": 1.6514,
      "step": 613000
    },
    {
      "epoch": 46.833702543732336,
      "grad_norm": 5.873457908630371,
      "learning_rate": 1.097191454688972e-05,
      "loss": 1.6448,
      "step": 613100
    },
    {
      "epoch": 46.841341379573755,
      "grad_norm": 6.556370735168457,
      "learning_rate": 1.0965548850355207e-05,
      "loss": 1.705,
      "step": 613200
    },
    {
      "epoch": 46.84898021541517,
      "grad_norm": 5.316837310791016,
      "learning_rate": 1.0959183153820692e-05,
      "loss": 1.679,
      "step": 613300
    },
    {
      "epoch": 46.85661905125659,
      "grad_norm": 5.34821081161499,
      "learning_rate": 1.0952817457286177e-05,
      "loss": 1.6141,
      "step": 613400
    },
    {
      "epoch": 46.864257887098006,
      "grad_norm": 5.4716691970825195,
      "learning_rate": 1.0946451760751663e-05,
      "loss": 1.7236,
      "step": 613500
    },
    {
      "epoch": 46.871896722939425,
      "grad_norm": 9.282125473022461,
      "learning_rate": 1.0940086064217148e-05,
      "loss": 1.7317,
      "step": 613600
    },
    {
      "epoch": 46.879535558780844,
      "grad_norm": 6.109384536743164,
      "learning_rate": 1.0933720367682633e-05,
      "loss": 1.6595,
      "step": 613700
    },
    {
      "epoch": 46.88717439462226,
      "grad_norm": 6.537565231323242,
      "learning_rate": 1.0927354671148117e-05,
      "loss": 1.6428,
      "step": 613800
    },
    {
      "epoch": 46.894813230463676,
      "grad_norm": 4.696020126342773,
      "learning_rate": 1.0920988974613602e-05,
      "loss": 1.6456,
      "step": 613900
    },
    {
      "epoch": 46.902452066305095,
      "grad_norm": 5.893305778503418,
      "learning_rate": 1.0914623278079087e-05,
      "loss": 1.7047,
      "step": 614000
    },
    {
      "epoch": 46.910090902146514,
      "grad_norm": 5.550114154815674,
      "learning_rate": 1.0908257581544572e-05,
      "loss": 1.5839,
      "step": 614100
    },
    {
      "epoch": 46.91772973798793,
      "grad_norm": 6.797854423522949,
      "learning_rate": 1.090189188501006e-05,
      "loss": 1.7337,
      "step": 614200
    },
    {
      "epoch": 46.925368573829346,
      "grad_norm": 6.537629127502441,
      "learning_rate": 1.0895526188475544e-05,
      "loss": 1.6153,
      "step": 614300
    },
    {
      "epoch": 46.933007409670765,
      "grad_norm": 5.230928897857666,
      "learning_rate": 1.088916049194103e-05,
      "loss": 1.6164,
      "step": 614400
    },
    {
      "epoch": 46.940646245512184,
      "grad_norm": 6.458341121673584,
      "learning_rate": 1.0882794795406515e-05,
      "loss": 1.7353,
      "step": 614500
    },
    {
      "epoch": 46.9482850813536,
      "grad_norm": 5.32715368270874,
      "learning_rate": 1.0876429098871998e-05,
      "loss": 1.7809,
      "step": 614600
    },
    {
      "epoch": 46.95592391719502,
      "grad_norm": 8.43335247039795,
      "learning_rate": 1.0870063402337484e-05,
      "loss": 1.6384,
      "step": 614700
    },
    {
      "epoch": 46.963562753036435,
      "grad_norm": 6.607231140136719,
      "learning_rate": 1.0863697705802969e-05,
      "loss": 1.7057,
      "step": 614800
    },
    {
      "epoch": 46.971201588877854,
      "grad_norm": 4.6296586990356445,
      "learning_rate": 1.0857332009268454e-05,
      "loss": 1.6972,
      "step": 614900
    },
    {
      "epoch": 46.97884042471927,
      "grad_norm": 5.245601177215576,
      "learning_rate": 1.085096631273394e-05,
      "loss": 1.6249,
      "step": 615000
    },
    {
      "epoch": 46.98647926056069,
      "grad_norm": 5.830678939819336,
      "learning_rate": 1.0844600616199425e-05,
      "loss": 1.7545,
      "step": 615100
    },
    {
      "epoch": 46.99411809640211,
      "grad_norm": 7.1934494972229,
      "learning_rate": 1.0838234919664912e-05,
      "loss": 1.7118,
      "step": 615200
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.7696313858032227,
      "eval_runtime": 3.2814,
      "eval_samples_per_second": 210.276,
      "eval_steps_per_second": 210.276,
      "step": 615277
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.4354355335235596,
      "eval_runtime": 63.2313,
      "eval_samples_per_second": 207.034,
      "eval_steps_per_second": 207.034,
      "step": 615277
    },
    {
      "epoch": 47.001756932243524,
      "grad_norm": 4.878353595733643,
      "learning_rate": 1.0831869223130395e-05,
      "loss": 1.6587,
      "step": 615300
    },
    {
      "epoch": 47.00939576808494,
      "grad_norm": 5.0090742111206055,
      "learning_rate": 1.082550352659588e-05,
      "loss": 1.678,
      "step": 615400
    },
    {
      "epoch": 47.01703460392636,
      "grad_norm": 5.1005659103393555,
      "learning_rate": 1.0819137830061366e-05,
      "loss": 1.5937,
      "step": 615500
    },
    {
      "epoch": 47.02467343976778,
      "grad_norm": 6.076723575592041,
      "learning_rate": 1.081277213352685e-05,
      "loss": 1.6835,
      "step": 615600
    },
    {
      "epoch": 47.0323122756092,
      "grad_norm": 5.820812225341797,
      "learning_rate": 1.0806406436992336e-05,
      "loss": 1.7057,
      "step": 615700
    },
    {
      "epoch": 47.03995111145061,
      "grad_norm": 6.6675872802734375,
      "learning_rate": 1.0800040740457821e-05,
      "loss": 1.6532,
      "step": 615800
    },
    {
      "epoch": 47.04758994729203,
      "grad_norm": 6.143783092498779,
      "learning_rate": 1.0793675043923306e-05,
      "loss": 1.5897,
      "step": 615900
    },
    {
      "epoch": 47.05522878313345,
      "grad_norm": 4.49727201461792,
      "learning_rate": 1.0787309347388792e-05,
      "loss": 1.664,
      "step": 616000
    },
    {
      "epoch": 47.06286761897487,
      "grad_norm": 5.270390033721924,
      "learning_rate": 1.0780943650854277e-05,
      "loss": 1.6766,
      "step": 616100
    },
    {
      "epoch": 47.07050645481629,
      "grad_norm": 4.891238689422607,
      "learning_rate": 1.0774577954319762e-05,
      "loss": 1.6853,
      "step": 616200
    },
    {
      "epoch": 47.0781452906577,
      "grad_norm": 5.974268436431885,
      "learning_rate": 1.0768212257785247e-05,
      "loss": 1.5991,
      "step": 616300
    },
    {
      "epoch": 47.08578412649912,
      "grad_norm": 4.758352279663086,
      "learning_rate": 1.0761846561250733e-05,
      "loss": 1.6532,
      "step": 616400
    },
    {
      "epoch": 47.09342296234054,
      "grad_norm": 5.692986965179443,
      "learning_rate": 1.0755480864716218e-05,
      "loss": 1.694,
      "step": 616500
    },
    {
      "epoch": 47.10106179818196,
      "grad_norm": 8.559891700744629,
      "learning_rate": 1.0749115168181703e-05,
      "loss": 1.6945,
      "step": 616600
    },
    {
      "epoch": 47.10870063402337,
      "grad_norm": 8.49687671661377,
      "learning_rate": 1.0742749471647188e-05,
      "loss": 1.6362,
      "step": 616700
    },
    {
      "epoch": 47.11633946986479,
      "grad_norm": 7.602470397949219,
      "learning_rate": 1.0736383775112674e-05,
      "loss": 1.7685,
      "step": 616800
    },
    {
      "epoch": 47.12397830570621,
      "grad_norm": 6.1348419189453125,
      "learning_rate": 1.0730018078578157e-05,
      "loss": 1.7227,
      "step": 616900
    },
    {
      "epoch": 47.13161714154763,
      "grad_norm": 4.942792892456055,
      "learning_rate": 1.0723652382043644e-05,
      "loss": 1.6716,
      "step": 617000
    },
    {
      "epoch": 47.13925597738905,
      "grad_norm": 4.208699703216553,
      "learning_rate": 1.071728668550913e-05,
      "loss": 1.6236,
      "step": 617100
    },
    {
      "epoch": 47.14689481323046,
      "grad_norm": 6.957225799560547,
      "learning_rate": 1.0710920988974614e-05,
      "loss": 1.6611,
      "step": 617200
    },
    {
      "epoch": 47.15453364907188,
      "grad_norm": 4.261774063110352,
      "learning_rate": 1.07045552924401e-05,
      "loss": 1.6103,
      "step": 617300
    },
    {
      "epoch": 47.1621724849133,
      "grad_norm": 4.2502641677856445,
      "learning_rate": 1.0698189595905585e-05,
      "loss": 1.6879,
      "step": 617400
    },
    {
      "epoch": 47.16981132075472,
      "grad_norm": 5.017292499542236,
      "learning_rate": 1.069182389937107e-05,
      "loss": 1.6978,
      "step": 617500
    },
    {
      "epoch": 47.17745015659614,
      "grad_norm": 5.760812759399414,
      "learning_rate": 1.0685458202836555e-05,
      "loss": 1.724,
      "step": 617600
    },
    {
      "epoch": 47.18508899243755,
      "grad_norm": 6.619508743286133,
      "learning_rate": 1.0679092506302039e-05,
      "loss": 1.7453,
      "step": 617700
    },
    {
      "epoch": 47.19272782827897,
      "grad_norm": 7.043161392211914,
      "learning_rate": 1.0672726809767524e-05,
      "loss": 1.7018,
      "step": 617800
    },
    {
      "epoch": 47.20036666412039,
      "grad_norm": 5.017159938812256,
      "learning_rate": 1.066636111323301e-05,
      "loss": 1.7159,
      "step": 617900
    },
    {
      "epoch": 47.20800549996181,
      "grad_norm": 4.34565544128418,
      "learning_rate": 1.0659995416698496e-05,
      "loss": 1.6956,
      "step": 618000
    },
    {
      "epoch": 47.215644335803226,
      "grad_norm": 5.101362228393555,
      "learning_rate": 1.0653629720163982e-05,
      "loss": 1.6024,
      "step": 618100
    },
    {
      "epoch": 47.22328317164464,
      "grad_norm": 5.4243998527526855,
      "learning_rate": 1.0647264023629467e-05,
      "loss": 1.6944,
      "step": 618200
    },
    {
      "epoch": 47.23092200748606,
      "grad_norm": 3.956402540206909,
      "learning_rate": 1.0640898327094952e-05,
      "loss": 1.7364,
      "step": 618300
    },
    {
      "epoch": 47.23856084332748,
      "grad_norm": 6.734538555145264,
      "learning_rate": 1.0634532630560437e-05,
      "loss": 1.6642,
      "step": 618400
    },
    {
      "epoch": 47.246199679168896,
      "grad_norm": 5.41572380065918,
      "learning_rate": 1.062816693402592e-05,
      "loss": 1.672,
      "step": 618500
    },
    {
      "epoch": 47.253838515010315,
      "grad_norm": 4.8663716316223145,
      "learning_rate": 1.0621801237491406e-05,
      "loss": 1.6916,
      "step": 618600
    },
    {
      "epoch": 47.26147735085173,
      "grad_norm": 4.882279872894287,
      "learning_rate": 1.0615435540956891e-05,
      "loss": 1.6983,
      "step": 618700
    },
    {
      "epoch": 47.26911618669315,
      "grad_norm": 4.905909538269043,
      "learning_rate": 1.0609069844422376e-05,
      "loss": 1.7093,
      "step": 618800
    },
    {
      "epoch": 47.276755022534566,
      "grad_norm": 5.504658222198486,
      "learning_rate": 1.0602704147887862e-05,
      "loss": 1.6617,
      "step": 618900
    },
    {
      "epoch": 47.284393858375985,
      "grad_norm": 6.4927191734313965,
      "learning_rate": 1.0596338451353349e-05,
      "loss": 1.7512,
      "step": 619000
    },
    {
      "epoch": 47.292032694217404,
      "grad_norm": 5.251181125640869,
      "learning_rate": 1.0589972754818834e-05,
      "loss": 1.7017,
      "step": 619100
    },
    {
      "epoch": 47.299671530058816,
      "grad_norm": 6.465775966644287,
      "learning_rate": 1.0583607058284319e-05,
      "loss": 1.7717,
      "step": 619200
    },
    {
      "epoch": 47.307310365900236,
      "grad_norm": 5.50642728805542,
      "learning_rate": 1.0577241361749803e-05,
      "loss": 1.6714,
      "step": 619300
    },
    {
      "epoch": 47.314949201741655,
      "grad_norm": 4.548752784729004,
      "learning_rate": 1.0570875665215288e-05,
      "loss": 1.7027,
      "step": 619400
    },
    {
      "epoch": 47.322588037583074,
      "grad_norm": 5.352508544921875,
      "learning_rate": 1.0564509968680773e-05,
      "loss": 1.6875,
      "step": 619500
    },
    {
      "epoch": 47.33022687342449,
      "grad_norm": 5.940009593963623,
      "learning_rate": 1.0558144272146258e-05,
      "loss": 1.6818,
      "step": 619600
    },
    {
      "epoch": 47.337865709265905,
      "grad_norm": 10.100730895996094,
      "learning_rate": 1.0551778575611744e-05,
      "loss": 1.7444,
      "step": 619700
    },
    {
      "epoch": 47.345504545107325,
      "grad_norm": 4.519016742706299,
      "learning_rate": 1.0545412879077229e-05,
      "loss": 1.7378,
      "step": 619800
    },
    {
      "epoch": 47.353143380948744,
      "grad_norm": 6.607736587524414,
      "learning_rate": 1.0539047182542714e-05,
      "loss": 1.6676,
      "step": 619900
    },
    {
      "epoch": 47.36078221679016,
      "grad_norm": 4.204545021057129,
      "learning_rate": 1.0532681486008201e-05,
      "loss": 1.7126,
      "step": 620000
    },
    {
      "epoch": 47.36842105263158,
      "grad_norm": 6.426143169403076,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.6408,
      "step": 620100
    },
    {
      "epoch": 47.376059888472994,
      "grad_norm": 6.201984882354736,
      "learning_rate": 1.051995009293917e-05,
      "loss": 1.6565,
      "step": 620200
    },
    {
      "epoch": 47.383698724314414,
      "grad_norm": 6.180161476135254,
      "learning_rate": 1.0513584396404655e-05,
      "loss": 1.6917,
      "step": 620300
    },
    {
      "epoch": 47.39133756015583,
      "grad_norm": 6.746631145477295,
      "learning_rate": 1.050721869987014e-05,
      "loss": 1.64,
      "step": 620400
    },
    {
      "epoch": 47.39897639599725,
      "grad_norm": 6.513095855712891,
      "learning_rate": 1.0500853003335625e-05,
      "loss": 1.6353,
      "step": 620500
    },
    {
      "epoch": 47.40661523183867,
      "grad_norm": 6.792156219482422,
      "learning_rate": 1.049448730680111e-05,
      "loss": 1.6017,
      "step": 620600
    },
    {
      "epoch": 47.41425406768008,
      "grad_norm": 5.474059104919434,
      "learning_rate": 1.0488121610266596e-05,
      "loss": 1.6563,
      "step": 620700
    },
    {
      "epoch": 47.4218929035215,
      "grad_norm": 5.5847015380859375,
      "learning_rate": 1.0481755913732081e-05,
      "loss": 1.7004,
      "step": 620800
    },
    {
      "epoch": 47.42953173936292,
      "grad_norm": 5.338504314422607,
      "learning_rate": 1.0475390217197566e-05,
      "loss": 1.6742,
      "step": 620900
    },
    {
      "epoch": 47.43717057520434,
      "grad_norm": 4.239241123199463,
      "learning_rate": 1.0469024520663052e-05,
      "loss": 1.6995,
      "step": 621000
    },
    {
      "epoch": 47.44480941104575,
      "grad_norm": 3.481687545776367,
      "learning_rate": 1.0462658824128537e-05,
      "loss": 1.5294,
      "step": 621100
    },
    {
      "epoch": 47.45244824688717,
      "grad_norm": 5.195860862731934,
      "learning_rate": 1.0456293127594022e-05,
      "loss": 1.6151,
      "step": 621200
    },
    {
      "epoch": 47.46008708272859,
      "grad_norm": 5.325191974639893,
      "learning_rate": 1.0449927431059507e-05,
      "loss": 1.6485,
      "step": 621300
    },
    {
      "epoch": 47.46772591857001,
      "grad_norm": 8.576597213745117,
      "learning_rate": 1.0443561734524992e-05,
      "loss": 1.6007,
      "step": 621400
    },
    {
      "epoch": 47.47536475441143,
      "grad_norm": 6.573062896728516,
      "learning_rate": 1.0437196037990478e-05,
      "loss": 1.7389,
      "step": 621500
    },
    {
      "epoch": 47.48300359025284,
      "grad_norm": 4.66713809967041,
      "learning_rate": 1.0430830341455963e-05,
      "loss": 1.6351,
      "step": 621600
    },
    {
      "epoch": 47.49064242609426,
      "grad_norm": 4.8236846923828125,
      "learning_rate": 1.0424464644921446e-05,
      "loss": 1.6814,
      "step": 621700
    },
    {
      "epoch": 47.49828126193568,
      "grad_norm": 5.517549991607666,
      "learning_rate": 1.0418098948386933e-05,
      "loss": 1.6949,
      "step": 621800
    },
    {
      "epoch": 47.5059200977771,
      "grad_norm": 5.413139820098877,
      "learning_rate": 1.0411733251852419e-05,
      "loss": 1.7137,
      "step": 621900
    },
    {
      "epoch": 47.51355893361852,
      "grad_norm": 7.080410957336426,
      "learning_rate": 1.0405367555317904e-05,
      "loss": 1.5986,
      "step": 622000
    },
    {
      "epoch": 47.52119776945993,
      "grad_norm": 3.638075113296509,
      "learning_rate": 1.0399001858783389e-05,
      "loss": 1.633,
      "step": 622100
    },
    {
      "epoch": 47.52883660530135,
      "grad_norm": 5.545544624328613,
      "learning_rate": 1.0392636162248874e-05,
      "loss": 1.7259,
      "step": 622200
    },
    {
      "epoch": 47.53647544114277,
      "grad_norm": 7.242941379547119,
      "learning_rate": 1.038627046571436e-05,
      "loss": 1.6236,
      "step": 622300
    },
    {
      "epoch": 47.54411427698419,
      "grad_norm": 3.0804708003997803,
      "learning_rate": 1.0379904769179845e-05,
      "loss": 1.6676,
      "step": 622400
    },
    {
      "epoch": 47.55175311282561,
      "grad_norm": 5.296783924102783,
      "learning_rate": 1.0373539072645328e-05,
      "loss": 1.6856,
      "step": 622500
    },
    {
      "epoch": 47.55939194866702,
      "grad_norm": 6.1483049392700195,
      "learning_rate": 1.0367173376110814e-05,
      "loss": 1.6437,
      "step": 622600
    },
    {
      "epoch": 47.56703078450844,
      "grad_norm": 8.152826309204102,
      "learning_rate": 1.0360807679576299e-05,
      "loss": 1.6924,
      "step": 622700
    },
    {
      "epoch": 47.57466962034986,
      "grad_norm": 5.539251327514648,
      "learning_rate": 1.0354441983041786e-05,
      "loss": 1.6925,
      "step": 622800
    },
    {
      "epoch": 47.58230845619128,
      "grad_norm": 5.368374347686768,
      "learning_rate": 1.0348076286507271e-05,
      "loss": 1.7429,
      "step": 622900
    },
    {
      "epoch": 47.5899472920327,
      "grad_norm": 4.6451897621154785,
      "learning_rate": 1.0341710589972756e-05,
      "loss": 1.7474,
      "step": 623000
    },
    {
      "epoch": 47.59758612787411,
      "grad_norm": 7.667295455932617,
      "learning_rate": 1.0335344893438241e-05,
      "loss": 1.6937,
      "step": 623100
    },
    {
      "epoch": 47.60522496371553,
      "grad_norm": 4.852086544036865,
      "learning_rate": 1.0328979196903725e-05,
      "loss": 1.6559,
      "step": 623200
    },
    {
      "epoch": 47.61286379955695,
      "grad_norm": 5.549003601074219,
      "learning_rate": 1.032261350036921e-05,
      "loss": 1.6646,
      "step": 623300
    },
    {
      "epoch": 47.62050263539837,
      "grad_norm": 5.158308982849121,
      "learning_rate": 1.0316247803834695e-05,
      "loss": 1.7031,
      "step": 623400
    },
    {
      "epoch": 47.628141471239786,
      "grad_norm": 4.451132297515869,
      "learning_rate": 1.030988210730018e-05,
      "loss": 1.6402,
      "step": 623500
    },
    {
      "epoch": 47.6357803070812,
      "grad_norm": 7.332211971282959,
      "learning_rate": 1.0303516410765666e-05,
      "loss": 1.6783,
      "step": 623600
    },
    {
      "epoch": 47.64341914292262,
      "grad_norm": 5.0822601318359375,
      "learning_rate": 1.0297150714231151e-05,
      "loss": 1.7109,
      "step": 623700
    },
    {
      "epoch": 47.65105797876404,
      "grad_norm": 2.967374324798584,
      "learning_rate": 1.0290785017696638e-05,
      "loss": 1.6343,
      "step": 623800
    },
    {
      "epoch": 47.658696814605456,
      "grad_norm": 11.93215560913086,
      "learning_rate": 1.0284419321162123e-05,
      "loss": 1.749,
      "step": 623900
    },
    {
      "epoch": 47.666335650446875,
      "grad_norm": 5.0289082527160645,
      "learning_rate": 1.0278053624627607e-05,
      "loss": 1.7022,
      "step": 624000
    },
    {
      "epoch": 47.67397448628829,
      "grad_norm": 7.310097694396973,
      "learning_rate": 1.0271687928093092e-05,
      "loss": 1.7341,
      "step": 624100
    },
    {
      "epoch": 47.68161332212971,
      "grad_norm": 6.665048599243164,
      "learning_rate": 1.0265322231558577e-05,
      "loss": 1.6125,
      "step": 624200
    },
    {
      "epoch": 47.689252157971126,
      "grad_norm": 5.577174186706543,
      "learning_rate": 1.0258956535024062e-05,
      "loss": 1.6429,
      "step": 624300
    },
    {
      "epoch": 47.696890993812545,
      "grad_norm": 6.723477363586426,
      "learning_rate": 1.0252590838489548e-05,
      "loss": 1.652,
      "step": 624400
    },
    {
      "epoch": 47.704529829653964,
      "grad_norm": 6.063966751098633,
      "learning_rate": 1.0246225141955033e-05,
      "loss": 1.5837,
      "step": 624500
    },
    {
      "epoch": 47.712168665495376,
      "grad_norm": 5.539443492889404,
      "learning_rate": 1.0239859445420518e-05,
      "loss": 1.6366,
      "step": 624600
    },
    {
      "epoch": 47.719807501336795,
      "grad_norm": 5.744282245635986,
      "learning_rate": 1.0233493748886003e-05,
      "loss": 1.73,
      "step": 624700
    },
    {
      "epoch": 47.727446337178215,
      "grad_norm": 3.1515769958496094,
      "learning_rate": 1.0227128052351489e-05,
      "loss": 1.6514,
      "step": 624800
    },
    {
      "epoch": 47.735085173019634,
      "grad_norm": 6.059731960296631,
      "learning_rate": 1.0220762355816974e-05,
      "loss": 1.6156,
      "step": 624900
    },
    {
      "epoch": 47.742724008861046,
      "grad_norm": 5.375224590301514,
      "learning_rate": 1.0214396659282459e-05,
      "loss": 1.7393,
      "step": 625000
    },
    {
      "epoch": 47.750362844702465,
      "grad_norm": 6.093752384185791,
      "learning_rate": 1.0208030962747944e-05,
      "loss": 1.6267,
      "step": 625100
    },
    {
      "epoch": 47.758001680543885,
      "grad_norm": 5.896709442138672,
      "learning_rate": 1.020166526621343e-05,
      "loss": 1.6913,
      "step": 625200
    },
    {
      "epoch": 47.765640516385304,
      "grad_norm": 4.380159854888916,
      "learning_rate": 1.0195299569678915e-05,
      "loss": 1.6686,
      "step": 625300
    },
    {
      "epoch": 47.77327935222672,
      "grad_norm": 4.98752498626709,
      "learning_rate": 1.01889338731444e-05,
      "loss": 1.7406,
      "step": 625400
    },
    {
      "epoch": 47.780918188068135,
      "grad_norm": 6.032670497894287,
      "learning_rate": 1.0182568176609885e-05,
      "loss": 1.7001,
      "step": 625500
    },
    {
      "epoch": 47.788557023909554,
      "grad_norm": 5.508749961853027,
      "learning_rate": 1.017620248007537e-05,
      "loss": 1.6959,
      "step": 625600
    },
    {
      "epoch": 47.79619585975097,
      "grad_norm": 5.722550868988037,
      "learning_rate": 1.0169836783540856e-05,
      "loss": 1.7378,
      "step": 625700
    },
    {
      "epoch": 47.80383469559239,
      "grad_norm": 5.678709983825684,
      "learning_rate": 1.0163471087006341e-05,
      "loss": 1.7311,
      "step": 625800
    },
    {
      "epoch": 47.81147353143381,
      "grad_norm": 6.0058512687683105,
      "learning_rate": 1.0157105390471826e-05,
      "loss": 1.6292,
      "step": 625900
    },
    {
      "epoch": 47.819112367275224,
      "grad_norm": 5.837588310241699,
      "learning_rate": 1.0150739693937311e-05,
      "loss": 1.6719,
      "step": 626000
    },
    {
      "epoch": 47.82675120311664,
      "grad_norm": 6.040148735046387,
      "learning_rate": 1.0144373997402797e-05,
      "loss": 1.6824,
      "step": 626100
    },
    {
      "epoch": 47.83439003895806,
      "grad_norm": 5.363310813903809,
      "learning_rate": 1.0138008300868282e-05,
      "loss": 1.7348,
      "step": 626200
    },
    {
      "epoch": 47.84202887479948,
      "grad_norm": 5.346041202545166,
      "learning_rate": 1.0131642604333767e-05,
      "loss": 1.7192,
      "step": 626300
    },
    {
      "epoch": 47.8496677106409,
      "grad_norm": 5.992053985595703,
      "learning_rate": 1.012527690779925e-05,
      "loss": 1.5896,
      "step": 626400
    },
    {
      "epoch": 47.85730654648231,
      "grad_norm": 7.040773868560791,
      "learning_rate": 1.0118911211264736e-05,
      "loss": 1.6355,
      "step": 626500
    },
    {
      "epoch": 47.86494538232373,
      "grad_norm": 5.748354434967041,
      "learning_rate": 1.0112545514730223e-05,
      "loss": 1.6098,
      "step": 626600
    },
    {
      "epoch": 47.87258421816515,
      "grad_norm": 7.653101921081543,
      "learning_rate": 1.0106179818195708e-05,
      "loss": 1.6118,
      "step": 626700
    },
    {
      "epoch": 47.88022305400657,
      "grad_norm": 7.009582042694092,
      "learning_rate": 1.0099814121661193e-05,
      "loss": 1.6448,
      "step": 626800
    },
    {
      "epoch": 47.88786188984799,
      "grad_norm": 4.617142677307129,
      "learning_rate": 1.0093448425126678e-05,
      "loss": 1.6562,
      "step": 626900
    },
    {
      "epoch": 47.8955007256894,
      "grad_norm": 5.509178161621094,
      "learning_rate": 1.0087082728592164e-05,
      "loss": 1.6232,
      "step": 627000
    },
    {
      "epoch": 47.90313956153082,
      "grad_norm": 3.8733291625976562,
      "learning_rate": 1.0080717032057649e-05,
      "loss": 1.6824,
      "step": 627100
    },
    {
      "epoch": 47.91077839737224,
      "grad_norm": 5.481777191162109,
      "learning_rate": 1.0074351335523132e-05,
      "loss": 1.7148,
      "step": 627200
    },
    {
      "epoch": 47.91841723321366,
      "grad_norm": 5.383998394012451,
      "learning_rate": 1.0067985638988618e-05,
      "loss": 1.5935,
      "step": 627300
    },
    {
      "epoch": 47.92605606905508,
      "grad_norm": 4.946466445922852,
      "learning_rate": 1.0061619942454103e-05,
      "loss": 1.6544,
      "step": 627400
    },
    {
      "epoch": 47.93369490489649,
      "grad_norm": 5.831068515777588,
      "learning_rate": 1.0055254245919588e-05,
      "loss": 1.6327,
      "step": 627500
    },
    {
      "epoch": 47.94133374073791,
      "grad_norm": 4.142380714416504,
      "learning_rate": 1.0048888549385075e-05,
      "loss": 1.6463,
      "step": 627600
    },
    {
      "epoch": 47.94897257657933,
      "grad_norm": 6.423245429992676,
      "learning_rate": 1.004252285285056e-05,
      "loss": 1.6416,
      "step": 627700
    },
    {
      "epoch": 47.95661141242075,
      "grad_norm": 6.1499481201171875,
      "learning_rate": 1.0036157156316046e-05,
      "loss": 1.674,
      "step": 627800
    },
    {
      "epoch": 47.96425024826217,
      "grad_norm": 5.407371997833252,
      "learning_rate": 1.002979145978153e-05,
      "loss": 1.6603,
      "step": 627900
    },
    {
      "epoch": 47.97188908410358,
      "grad_norm": 4.933300971984863,
      "learning_rate": 1.0023425763247014e-05,
      "loss": 1.6495,
      "step": 628000
    },
    {
      "epoch": 47.979527919945,
      "grad_norm": 5.713621139526367,
      "learning_rate": 1.00170600667125e-05,
      "loss": 1.7445,
      "step": 628100
    },
    {
      "epoch": 47.98716675578642,
      "grad_norm": 5.558666706085205,
      "learning_rate": 1.0010694370177985e-05,
      "loss": 1.6263,
      "step": 628200
    },
    {
      "epoch": 47.99480559162784,
      "grad_norm": 4.7775678634643555,
      "learning_rate": 1.000432867364347e-05,
      "loss": 1.833,
      "step": 628300
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.7651077508926392,
      "eval_runtime": 3.4457,
      "eval_samples_per_second": 200.25,
      "eval_steps_per_second": 200.25,
      "step": 628368
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.4312525987625122,
      "eval_runtime": 66.1987,
      "eval_samples_per_second": 197.753,
      "eval_steps_per_second": 197.753,
      "step": 628368
    },
    {
      "epoch": 48.00244442746926,
      "grad_norm": 4.28258752822876,
      "learning_rate": 9.997962977108955e-06,
      "loss": 1.6798,
      "step": 628400
    },
    {
      "epoch": 48.01008326331067,
      "grad_norm": 6.215147972106934,
      "learning_rate": 9.99159728057444e-06,
      "loss": 1.6268,
      "step": 628500
    },
    {
      "epoch": 48.01772209915209,
      "grad_norm": 6.108238697052002,
      "learning_rate": 9.985231584039927e-06,
      "loss": 1.6835,
      "step": 628600
    },
    {
      "epoch": 48.02536093499351,
      "grad_norm": 3.0489161014556885,
      "learning_rate": 9.978865887505413e-06,
      "loss": 1.5654,
      "step": 628700
    },
    {
      "epoch": 48.03299977083493,
      "grad_norm": 5.129830837249756,
      "learning_rate": 9.972500190970896e-06,
      "loss": 1.5875,
      "step": 628800
    },
    {
      "epoch": 48.040638606676346,
      "grad_norm": 5.195532321929932,
      "learning_rate": 9.966134494436381e-06,
      "loss": 1.6485,
      "step": 628900
    },
    {
      "epoch": 48.04827744251776,
      "grad_norm": 6.571280479431152,
      "learning_rate": 9.959768797901867e-06,
      "loss": 1.6481,
      "step": 629000
    },
    {
      "epoch": 48.05591627835918,
      "grad_norm": 6.069522857666016,
      "learning_rate": 9.953403101367352e-06,
      "loss": 1.6663,
      "step": 629100
    },
    {
      "epoch": 48.0635551142006,
      "grad_norm": 5.193500995635986,
      "learning_rate": 9.947037404832837e-06,
      "loss": 1.7466,
      "step": 629200
    },
    {
      "epoch": 48.071193950042016,
      "grad_norm": 5.372504234313965,
      "learning_rate": 9.940671708298322e-06,
      "loss": 1.6709,
      "step": 629300
    },
    {
      "epoch": 48.07883278588343,
      "grad_norm": 6.813499450683594,
      "learning_rate": 9.934306011763808e-06,
      "loss": 1.7019,
      "step": 629400
    },
    {
      "epoch": 48.08647162172485,
      "grad_norm": 5.929594993591309,
      "learning_rate": 9.927940315229293e-06,
      "loss": 1.7099,
      "step": 629500
    },
    {
      "epoch": 48.094110457566266,
      "grad_norm": 5.032452583312988,
      "learning_rate": 9.921574618694778e-06,
      "loss": 1.5975,
      "step": 629600
    },
    {
      "epoch": 48.101749293407686,
      "grad_norm": 6.485461711883545,
      "learning_rate": 9.915208922160263e-06,
      "loss": 1.7186,
      "step": 629700
    },
    {
      "epoch": 48.109388129249105,
      "grad_norm": 5.9071760177612305,
      "learning_rate": 9.908843225625748e-06,
      "loss": 1.6094,
      "step": 629800
    },
    {
      "epoch": 48.11702696509052,
      "grad_norm": 5.8265910148620605,
      "learning_rate": 9.902477529091234e-06,
      "loss": 1.669,
      "step": 629900
    },
    {
      "epoch": 48.124665800931936,
      "grad_norm": 4.672089099884033,
      "learning_rate": 9.896111832556719e-06,
      "loss": 1.6953,
      "step": 630000
    },
    {
      "epoch": 48.132304636773355,
      "grad_norm": 8.218777656555176,
      "learning_rate": 9.889746136022204e-06,
      "loss": 1.6773,
      "step": 630100
    },
    {
      "epoch": 48.139943472614775,
      "grad_norm": 4.995782375335693,
      "learning_rate": 9.88338043948769e-06,
      "loss": 1.6511,
      "step": 630200
    },
    {
      "epoch": 48.147582308456194,
      "grad_norm": 7.63792085647583,
      "learning_rate": 9.877014742953175e-06,
      "loss": 1.7399,
      "step": 630300
    },
    {
      "epoch": 48.155221144297606,
      "grad_norm": 5.1824870109558105,
      "learning_rate": 9.87064904641866e-06,
      "loss": 1.689,
      "step": 630400
    },
    {
      "epoch": 48.162859980139025,
      "grad_norm": 6.073306560516357,
      "learning_rate": 9.864283349884145e-06,
      "loss": 1.613,
      "step": 630500
    },
    {
      "epoch": 48.170498815980444,
      "grad_norm": 4.923702239990234,
      "learning_rate": 9.85791765334963e-06,
      "loss": 1.5993,
      "step": 630600
    },
    {
      "epoch": 48.178137651821864,
      "grad_norm": 5.520125389099121,
      "learning_rate": 9.851551956815116e-06,
      "loss": 1.7153,
      "step": 630700
    },
    {
      "epoch": 48.18577648766328,
      "grad_norm": 6.063218116760254,
      "learning_rate": 9.8451862602806e-06,
      "loss": 1.6732,
      "step": 630800
    },
    {
      "epoch": 48.193415323504695,
      "grad_norm": 6.473487854003906,
      "learning_rate": 9.838820563746086e-06,
      "loss": 1.7316,
      "step": 630900
    },
    {
      "epoch": 48.201054159346114,
      "grad_norm": 5.841305255889893,
      "learning_rate": 9.832454867211571e-06,
      "loss": 1.5799,
      "step": 631000
    },
    {
      "epoch": 48.20869299518753,
      "grad_norm": 8.713576316833496,
      "learning_rate": 9.826089170677056e-06,
      "loss": 1.7333,
      "step": 631100
    },
    {
      "epoch": 48.21633183102895,
      "grad_norm": 6.9818925857543945,
      "learning_rate": 9.81972347414254e-06,
      "loss": 1.642,
      "step": 631200
    },
    {
      "epoch": 48.22397066687037,
      "grad_norm": 5.761997699737549,
      "learning_rate": 9.813357777608025e-06,
      "loss": 1.6641,
      "step": 631300
    },
    {
      "epoch": 48.231609502711784,
      "grad_norm": 5.072281837463379,
      "learning_rate": 9.806992081073512e-06,
      "loss": 1.6313,
      "step": 631400
    },
    {
      "epoch": 48.2392483385532,
      "grad_norm": 5.785048007965088,
      "learning_rate": 9.800626384538997e-06,
      "loss": 1.6906,
      "step": 631500
    },
    {
      "epoch": 48.24688717439462,
      "grad_norm": 4.731751441955566,
      "learning_rate": 9.794260688004483e-06,
      "loss": 1.6201,
      "step": 631600
    },
    {
      "epoch": 48.25452601023604,
      "grad_norm": 6.5588059425354,
      "learning_rate": 9.787894991469968e-06,
      "loss": 1.6847,
      "step": 631700
    },
    {
      "epoch": 48.26216484607746,
      "grad_norm": 7.005298137664795,
      "learning_rate": 9.781529294935453e-06,
      "loss": 1.663,
      "step": 631800
    },
    {
      "epoch": 48.26980368191887,
      "grad_norm": 7.253459453582764,
      "learning_rate": 9.775163598400937e-06,
      "loss": 1.6756,
      "step": 631900
    },
    {
      "epoch": 48.27744251776029,
      "grad_norm": 5.365832328796387,
      "learning_rate": 9.768797901866422e-06,
      "loss": 1.6576,
      "step": 632000
    },
    {
      "epoch": 48.28508135360171,
      "grad_norm": 4.442868709564209,
      "learning_rate": 9.762432205331907e-06,
      "loss": 1.7278,
      "step": 632100
    },
    {
      "epoch": 48.29272018944313,
      "grad_norm": 7.0942888259887695,
      "learning_rate": 9.756066508797392e-06,
      "loss": 1.6553,
      "step": 632200
    },
    {
      "epoch": 48.30035902528455,
      "grad_norm": 6.48536491394043,
      "learning_rate": 9.749700812262878e-06,
      "loss": 1.7008,
      "step": 632300
    },
    {
      "epoch": 48.30799786112596,
      "grad_norm": 5.56050968170166,
      "learning_rate": 9.743335115728364e-06,
      "loss": 1.7334,
      "step": 632400
    },
    {
      "epoch": 48.31563669696738,
      "grad_norm": 4.336082458496094,
      "learning_rate": 9.73696941919385e-06,
      "loss": 1.6136,
      "step": 632500
    },
    {
      "epoch": 48.3232755328088,
      "grad_norm": 4.423134803771973,
      "learning_rate": 9.730603722659335e-06,
      "loss": 1.6269,
      "step": 632600
    },
    {
      "epoch": 48.33091436865022,
      "grad_norm": 4.887508392333984,
      "learning_rate": 9.724238026124818e-06,
      "loss": 1.6427,
      "step": 632700
    },
    {
      "epoch": 48.33855320449164,
      "grad_norm": 3.8508920669555664,
      "learning_rate": 9.717872329590304e-06,
      "loss": 1.659,
      "step": 632800
    },
    {
      "epoch": 48.34619204033305,
      "grad_norm": 5.240368843078613,
      "learning_rate": 9.711506633055789e-06,
      "loss": 1.7051,
      "step": 632900
    },
    {
      "epoch": 48.35383087617447,
      "grad_norm": 6.250096321105957,
      "learning_rate": 9.705140936521274e-06,
      "loss": 1.5839,
      "step": 633000
    },
    {
      "epoch": 48.36146971201589,
      "grad_norm": 5.115534782409668,
      "learning_rate": 9.69877523998676e-06,
      "loss": 1.7114,
      "step": 633100
    },
    {
      "epoch": 48.36910854785731,
      "grad_norm": 5.708251476287842,
      "learning_rate": 9.692409543452245e-06,
      "loss": 1.6893,
      "step": 633200
    },
    {
      "epoch": 48.37674738369873,
      "grad_norm": 5.992188453674316,
      "learning_rate": 9.68604384691773e-06,
      "loss": 1.6467,
      "step": 633300
    },
    {
      "epoch": 48.38438621954014,
      "grad_norm": 4.7251973152160645,
      "learning_rate": 9.679678150383217e-06,
      "loss": 1.7006,
      "step": 633400
    },
    {
      "epoch": 48.39202505538156,
      "grad_norm": 4.170095920562744,
      "learning_rate": 9.6733124538487e-06,
      "loss": 1.6424,
      "step": 633500
    },
    {
      "epoch": 48.39966389122298,
      "grad_norm": 5.242519378662109,
      "learning_rate": 9.666946757314186e-06,
      "loss": 1.7342,
      "step": 633600
    },
    {
      "epoch": 48.4073027270644,
      "grad_norm": 3.79638409614563,
      "learning_rate": 9.66058106077967e-06,
      "loss": 1.6909,
      "step": 633700
    },
    {
      "epoch": 48.41494156290581,
      "grad_norm": 4.928607940673828,
      "learning_rate": 9.654215364245156e-06,
      "loss": 1.6,
      "step": 633800
    },
    {
      "epoch": 48.42258039874723,
      "grad_norm": 6.688182830810547,
      "learning_rate": 9.647849667710641e-06,
      "loss": 1.7299,
      "step": 633900
    },
    {
      "epoch": 48.43021923458865,
      "grad_norm": 4.832962512969971,
      "learning_rate": 9.641483971176126e-06,
      "loss": 1.6291,
      "step": 634000
    },
    {
      "epoch": 48.43785807043007,
      "grad_norm": 5.401906490325928,
      "learning_rate": 9.635118274641612e-06,
      "loss": 1.6265,
      "step": 634100
    },
    {
      "epoch": 48.44549690627149,
      "grad_norm": 4.76840877532959,
      "learning_rate": 9.628752578107097e-06,
      "loss": 1.7048,
      "step": 634200
    },
    {
      "epoch": 48.4531357421129,
      "grad_norm": 4.970560550689697,
      "learning_rate": 9.622386881572582e-06,
      "loss": 1.7362,
      "step": 634300
    },
    {
      "epoch": 48.46077457795432,
      "grad_norm": 5.535584926605225,
      "learning_rate": 9.616021185038067e-06,
      "loss": 1.7144,
      "step": 634400
    },
    {
      "epoch": 48.46841341379574,
      "grad_norm": 5.176870346069336,
      "learning_rate": 9.609655488503553e-06,
      "loss": 1.6987,
      "step": 634500
    },
    {
      "epoch": 48.476052249637156,
      "grad_norm": 9.105125427246094,
      "learning_rate": 9.603289791969038e-06,
      "loss": 1.6722,
      "step": 634600
    },
    {
      "epoch": 48.483691085478576,
      "grad_norm": 4.832175254821777,
      "learning_rate": 9.596924095434523e-06,
      "loss": 1.6512,
      "step": 634700
    },
    {
      "epoch": 48.49132992131999,
      "grad_norm": 8.192996978759766,
      "learning_rate": 9.590558398900008e-06,
      "loss": 1.8123,
      "step": 634800
    },
    {
      "epoch": 48.49896875716141,
      "grad_norm": 6.374805450439453,
      "learning_rate": 9.584192702365494e-06,
      "loss": 1.6514,
      "step": 634900
    },
    {
      "epoch": 48.506607593002826,
      "grad_norm": 4.570394039154053,
      "learning_rate": 9.577827005830979e-06,
      "loss": 1.7242,
      "step": 635000
    },
    {
      "epoch": 48.514246428844245,
      "grad_norm": 4.462533950805664,
      "learning_rate": 9.571461309296462e-06,
      "loss": 1.5923,
      "step": 635100
    },
    {
      "epoch": 48.521885264685665,
      "grad_norm": 5.8239336013793945,
      "learning_rate": 9.56509561276195e-06,
      "loss": 1.7282,
      "step": 635200
    },
    {
      "epoch": 48.52952410052708,
      "grad_norm": 6.641139030456543,
      "learning_rate": 9.558729916227434e-06,
      "loss": 1.6748,
      "step": 635300
    },
    {
      "epoch": 48.537162936368496,
      "grad_norm": 4.948571681976318,
      "learning_rate": 9.55236421969292e-06,
      "loss": 1.7324,
      "step": 635400
    },
    {
      "epoch": 48.544801772209915,
      "grad_norm": 6.3028340339660645,
      "learning_rate": 9.545998523158405e-06,
      "loss": 1.6761,
      "step": 635500
    },
    {
      "epoch": 48.552440608051334,
      "grad_norm": 7.1274027824401855,
      "learning_rate": 9.53963282662389e-06,
      "loss": 1.7268,
      "step": 635600
    },
    {
      "epoch": 48.560079443892754,
      "grad_norm": 6.98466157913208,
      "learning_rate": 9.533267130089375e-06,
      "loss": 1.6829,
      "step": 635700
    },
    {
      "epoch": 48.567718279734166,
      "grad_norm": 5.84000825881958,
      "learning_rate": 9.52690143355486e-06,
      "loss": 1.6585,
      "step": 635800
    },
    {
      "epoch": 48.575357115575585,
      "grad_norm": 5.800010681152344,
      "learning_rate": 9.520535737020344e-06,
      "loss": 1.7383,
      "step": 635900
    },
    {
      "epoch": 48.582995951417004,
      "grad_norm": 4.873678207397461,
      "learning_rate": 9.51417004048583e-06,
      "loss": 1.5898,
      "step": 636000
    },
    {
      "epoch": 48.59063478725842,
      "grad_norm": 5.295515537261963,
      "learning_rate": 9.507804343951315e-06,
      "loss": 1.6251,
      "step": 636100
    },
    {
      "epoch": 48.59827362309984,
      "grad_norm": 5.986955165863037,
      "learning_rate": 9.501438647416802e-06,
      "loss": 1.6619,
      "step": 636200
    },
    {
      "epoch": 48.605912458941255,
      "grad_norm": 4.740330219268799,
      "learning_rate": 9.495072950882287e-06,
      "loss": 1.6782,
      "step": 636300
    },
    {
      "epoch": 48.613551294782674,
      "grad_norm": 4.994925498962402,
      "learning_rate": 9.488707254347772e-06,
      "loss": 1.6183,
      "step": 636400
    },
    {
      "epoch": 48.62119013062409,
      "grad_norm": 5.304251670837402,
      "learning_rate": 9.482341557813257e-06,
      "loss": 1.6185,
      "step": 636500
    },
    {
      "epoch": 48.62882896646551,
      "grad_norm": 4.649903297424316,
      "learning_rate": 9.475975861278742e-06,
      "loss": 1.609,
      "step": 636600
    },
    {
      "epoch": 48.63646780230693,
      "grad_norm": 8.29078483581543,
      "learning_rate": 9.469610164744226e-06,
      "loss": 1.612,
      "step": 636700
    },
    {
      "epoch": 48.644106638148344,
      "grad_norm": 6.639185905456543,
      "learning_rate": 9.463244468209711e-06,
      "loss": 1.8138,
      "step": 636800
    },
    {
      "epoch": 48.65174547398976,
      "grad_norm": 6.848357677459717,
      "learning_rate": 9.456878771675196e-06,
      "loss": 1.7169,
      "step": 636900
    },
    {
      "epoch": 48.65938430983118,
      "grad_norm": 5.4868364334106445,
      "learning_rate": 9.450513075140682e-06,
      "loss": 1.6494,
      "step": 637000
    },
    {
      "epoch": 48.6670231456726,
      "grad_norm": 5.852152347564697,
      "learning_rate": 9.444147378606167e-06,
      "loss": 1.7911,
      "step": 637100
    },
    {
      "epoch": 48.67466198151402,
      "grad_norm": 5.947052478790283,
      "learning_rate": 9.437781682071654e-06,
      "loss": 1.7397,
      "step": 637200
    },
    {
      "epoch": 48.68230081735543,
      "grad_norm": 8.956380844116211,
      "learning_rate": 9.431415985537139e-06,
      "loss": 1.6976,
      "step": 637300
    },
    {
      "epoch": 48.68993965319685,
      "grad_norm": 3.795023202896118,
      "learning_rate": 9.425050289002624e-06,
      "loss": 1.6184,
      "step": 637400
    },
    {
      "epoch": 48.69757848903827,
      "grad_norm": 5.874826431274414,
      "learning_rate": 9.418684592468108e-06,
      "loss": 1.6303,
      "step": 637500
    },
    {
      "epoch": 48.70521732487969,
      "grad_norm": 8.270111083984375,
      "learning_rate": 9.412318895933593e-06,
      "loss": 1.6286,
      "step": 637600
    },
    {
      "epoch": 48.7128561607211,
      "grad_norm": 5.011717796325684,
      "learning_rate": 9.405953199399078e-06,
      "loss": 1.6352,
      "step": 637700
    },
    {
      "epoch": 48.72049499656252,
      "grad_norm": 7.276413917541504,
      "learning_rate": 9.399587502864564e-06,
      "loss": 1.6278,
      "step": 637800
    },
    {
      "epoch": 48.72813383240394,
      "grad_norm": 6.0092010498046875,
      "learning_rate": 9.393221806330049e-06,
      "loss": 1.7594,
      "step": 637900
    },
    {
      "epoch": 48.73577266824536,
      "grad_norm": 4.328963756561279,
      "learning_rate": 9.386856109795534e-06,
      "loss": 1.6831,
      "step": 638000
    },
    {
      "epoch": 48.74341150408678,
      "grad_norm": 7.2095046043396,
      "learning_rate": 9.38049041326102e-06,
      "loss": 1.7043,
      "step": 638100
    },
    {
      "epoch": 48.75105033992819,
      "grad_norm": 6.630940914154053,
      "learning_rate": 9.374124716726506e-06,
      "loss": 1.6244,
      "step": 638200
    },
    {
      "epoch": 48.75868917576961,
      "grad_norm": 6.3517279624938965,
      "learning_rate": 9.36775902019199e-06,
      "loss": 1.6984,
      "step": 638300
    },
    {
      "epoch": 48.76632801161103,
      "grad_norm": 5.079900741577148,
      "learning_rate": 9.361393323657475e-06,
      "loss": 1.6603,
      "step": 638400
    },
    {
      "epoch": 48.77396684745245,
      "grad_norm": 5.729778289794922,
      "learning_rate": 9.35502762712296e-06,
      "loss": 1.6985,
      "step": 638500
    },
    {
      "epoch": 48.78160568329387,
      "grad_norm": 4.888319969177246,
      "learning_rate": 9.348661930588445e-06,
      "loss": 1.6884,
      "step": 638600
    },
    {
      "epoch": 48.78924451913528,
      "grad_norm": 5.568778991699219,
      "learning_rate": 9.34229623405393e-06,
      "loss": 1.6771,
      "step": 638700
    },
    {
      "epoch": 48.7968833549767,
      "grad_norm": 4.866215705871582,
      "learning_rate": 9.335930537519416e-06,
      "loss": 1.6818,
      "step": 638800
    },
    {
      "epoch": 48.80452219081812,
      "grad_norm": 5.999023914337158,
      "learning_rate": 9.329564840984901e-06,
      "loss": 1.7755,
      "step": 638900
    },
    {
      "epoch": 48.81216102665954,
      "grad_norm": 4.619223594665527,
      "learning_rate": 9.323199144450386e-06,
      "loss": 1.6593,
      "step": 639000
    },
    {
      "epoch": 48.81979986250096,
      "grad_norm": 4.662407875061035,
      "learning_rate": 9.316833447915872e-06,
      "loss": 1.6438,
      "step": 639100
    },
    {
      "epoch": 48.82743869834237,
      "grad_norm": 3.8577768802642822,
      "learning_rate": 9.310467751381357e-06,
      "loss": 1.6971,
      "step": 639200
    },
    {
      "epoch": 48.83507753418379,
      "grad_norm": 5.083370208740234,
      "learning_rate": 9.304102054846842e-06,
      "loss": 1.6556,
      "step": 639300
    },
    {
      "epoch": 48.84271637002521,
      "grad_norm": 4.551042079925537,
      "learning_rate": 9.297736358312327e-06,
      "loss": 1.7347,
      "step": 639400
    },
    {
      "epoch": 48.85035520586663,
      "grad_norm": 5.789947509765625,
      "learning_rate": 9.291370661777812e-06,
      "loss": 1.6737,
      "step": 639500
    },
    {
      "epoch": 48.85799404170805,
      "grad_norm": 5.8204779624938965,
      "learning_rate": 9.285004965243298e-06,
      "loss": 1.5875,
      "step": 639600
    },
    {
      "epoch": 48.86563287754946,
      "grad_norm": 4.274491786956787,
      "learning_rate": 9.278639268708783e-06,
      "loss": 1.6747,
      "step": 639700
    },
    {
      "epoch": 48.87327171339088,
      "grad_norm": 3.8026089668273926,
      "learning_rate": 9.272273572174266e-06,
      "loss": 1.6439,
      "step": 639800
    },
    {
      "epoch": 48.8809105492323,
      "grad_norm": 6.471221446990967,
      "learning_rate": 9.265907875639752e-06,
      "loss": 1.6197,
      "step": 639900
    },
    {
      "epoch": 48.888549385073716,
      "grad_norm": 5.129705905914307,
      "learning_rate": 9.259542179105239e-06,
      "loss": 1.6737,
      "step": 640000
    },
    {
      "epoch": 48.896188220915136,
      "grad_norm": 6.122276306152344,
      "learning_rate": 9.253176482570724e-06,
      "loss": 1.714,
      "step": 640100
    },
    {
      "epoch": 48.90382705675655,
      "grad_norm": 6.691046714782715,
      "learning_rate": 9.246810786036209e-06,
      "loss": 1.6177,
      "step": 640200
    },
    {
      "epoch": 48.91146589259797,
      "grad_norm": 5.466975688934326,
      "learning_rate": 9.240445089501694e-06,
      "loss": 1.7171,
      "step": 640300
    },
    {
      "epoch": 48.919104728439386,
      "grad_norm": 4.670409679412842,
      "learning_rate": 9.23407939296718e-06,
      "loss": 1.6214,
      "step": 640400
    },
    {
      "epoch": 48.926743564280805,
      "grad_norm": 3.806514024734497,
      "learning_rate": 9.227713696432665e-06,
      "loss": 1.6122,
      "step": 640500
    },
    {
      "epoch": 48.934382400122225,
      "grad_norm": 5.199855804443359,
      "learning_rate": 9.221347999898148e-06,
      "loss": 1.6624,
      "step": 640600
    },
    {
      "epoch": 48.94202123596364,
      "grad_norm": 4.2381391525268555,
      "learning_rate": 9.214982303363634e-06,
      "loss": 1.6675,
      "step": 640700
    },
    {
      "epoch": 48.949660071805056,
      "grad_norm": 8.332963943481445,
      "learning_rate": 9.208616606829119e-06,
      "loss": 1.6809,
      "step": 640800
    },
    {
      "epoch": 48.957298907646475,
      "grad_norm": 6.093944072723389,
      "learning_rate": 9.202250910294604e-06,
      "loss": 1.6042,
      "step": 640900
    },
    {
      "epoch": 48.964937743487894,
      "grad_norm": 4.925325393676758,
      "learning_rate": 9.195885213760091e-06,
      "loss": 1.6076,
      "step": 641000
    },
    {
      "epoch": 48.97257657932931,
      "grad_norm": 5.8530683517456055,
      "learning_rate": 9.189519517225576e-06,
      "loss": 1.7206,
      "step": 641100
    },
    {
      "epoch": 48.980215415170726,
      "grad_norm": 4.272193431854248,
      "learning_rate": 9.183153820691061e-06,
      "loss": 1.5881,
      "step": 641200
    },
    {
      "epoch": 48.987854251012145,
      "grad_norm": 5.490666389465332,
      "learning_rate": 9.176788124156547e-06,
      "loss": 1.6989,
      "step": 641300
    },
    {
      "epoch": 48.995493086853564,
      "grad_norm": 7.793294906616211,
      "learning_rate": 9.17042242762203e-06,
      "loss": 1.716,
      "step": 641400
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.7686673402786255,
      "eval_runtime": 3.0194,
      "eval_samples_per_second": 228.52,
      "eval_steps_per_second": 228.52,
      "step": 641459
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.4295639991760254,
      "eval_runtime": 57.3975,
      "eval_samples_per_second": 228.076,
      "eval_steps_per_second": 228.076,
      "step": 641459
    },
    {
      "epoch": 49.00313192269498,
      "grad_norm": 5.492871284484863,
      "learning_rate": 9.164056731087515e-06,
      "loss": 1.6678,
      "step": 641500
    },
    {
      "epoch": 49.0107707585364,
      "grad_norm": 6.843557357788086,
      "learning_rate": 9.157691034553e-06,
      "loss": 1.7194,
      "step": 641600
    },
    {
      "epoch": 49.018409594377815,
      "grad_norm": 4.524519920349121,
      "learning_rate": 9.151325338018486e-06,
      "loss": 1.6072,
      "step": 641700
    },
    {
      "epoch": 49.026048430219234,
      "grad_norm": 5.376636981964111,
      "learning_rate": 9.144959641483971e-06,
      "loss": 1.6642,
      "step": 641800
    },
    {
      "epoch": 49.03368726606065,
      "grad_norm": 4.704803466796875,
      "learning_rate": 9.138593944949456e-06,
      "loss": 1.6099,
      "step": 641900
    },
    {
      "epoch": 49.04132610190207,
      "grad_norm": 5.779706001281738,
      "learning_rate": 9.132228248414943e-06,
      "loss": 1.7101,
      "step": 642000
    },
    {
      "epoch": 49.048964937743484,
      "grad_norm": 11.115053176879883,
      "learning_rate": 9.125862551880428e-06,
      "loss": 1.7927,
      "step": 642100
    },
    {
      "epoch": 49.056603773584904,
      "grad_norm": 5.526963710784912,
      "learning_rate": 9.119496855345912e-06,
      "loss": 1.7581,
      "step": 642200
    },
    {
      "epoch": 49.06424260942632,
      "grad_norm": 6.156510829925537,
      "learning_rate": 9.113131158811397e-06,
      "loss": 1.6295,
      "step": 642300
    },
    {
      "epoch": 49.07188144526774,
      "grad_norm": 6.256163120269775,
      "learning_rate": 9.106765462276882e-06,
      "loss": 1.6146,
      "step": 642400
    },
    {
      "epoch": 49.07952028110916,
      "grad_norm": 4.235997676849365,
      "learning_rate": 9.100399765742368e-06,
      "loss": 1.7099,
      "step": 642500
    },
    {
      "epoch": 49.08715911695057,
      "grad_norm": 6.864741325378418,
      "learning_rate": 9.094034069207853e-06,
      "loss": 1.6794,
      "step": 642600
    },
    {
      "epoch": 49.09479795279199,
      "grad_norm": 4.248346328735352,
      "learning_rate": 9.087668372673338e-06,
      "loss": 1.6227,
      "step": 642700
    },
    {
      "epoch": 49.10243678863341,
      "grad_norm": 5.127954006195068,
      "learning_rate": 9.081302676138823e-06,
      "loss": 1.6397,
      "step": 642800
    },
    {
      "epoch": 49.11007562447483,
      "grad_norm": 5.472870349884033,
      "learning_rate": 9.074936979604309e-06,
      "loss": 1.6642,
      "step": 642900
    },
    {
      "epoch": 49.11771446031625,
      "grad_norm": 5.182826519012451,
      "learning_rate": 9.068571283069794e-06,
      "loss": 1.6807,
      "step": 643000
    },
    {
      "epoch": 49.12535329615766,
      "grad_norm": 4.730987071990967,
      "learning_rate": 9.062205586535279e-06,
      "loss": 1.6299,
      "step": 643100
    },
    {
      "epoch": 49.13299213199908,
      "grad_norm": 8.04514217376709,
      "learning_rate": 9.055839890000764e-06,
      "loss": 1.6627,
      "step": 643200
    },
    {
      "epoch": 49.1406309678405,
      "grad_norm": 7.25179386138916,
      "learning_rate": 9.04947419346625e-06,
      "loss": 1.6102,
      "step": 643300
    },
    {
      "epoch": 49.14826980368192,
      "grad_norm": 6.143752574920654,
      "learning_rate": 9.043108496931735e-06,
      "loss": 1.6178,
      "step": 643400
    },
    {
      "epoch": 49.15590863952334,
      "grad_norm": 6.02653694152832,
      "learning_rate": 9.03674280039722e-06,
      "loss": 1.6439,
      "step": 643500
    },
    {
      "epoch": 49.16354747536475,
      "grad_norm": 7.133213043212891,
      "learning_rate": 9.030377103862705e-06,
      "loss": 1.6819,
      "step": 643600
    },
    {
      "epoch": 49.17118631120617,
      "grad_norm": 4.045809268951416,
      "learning_rate": 9.02401140732819e-06,
      "loss": 1.6116,
      "step": 643700
    },
    {
      "epoch": 49.17882514704759,
      "grad_norm": 6.053730010986328,
      "learning_rate": 9.017645710793676e-06,
      "loss": 1.6194,
      "step": 643800
    },
    {
      "epoch": 49.18646398288901,
      "grad_norm": 6.469773292541504,
      "learning_rate": 9.011280014259161e-06,
      "loss": 1.7107,
      "step": 643900
    },
    {
      "epoch": 49.19410281873043,
      "grad_norm": 6.439405918121338,
      "learning_rate": 9.004914317724646e-06,
      "loss": 1.628,
      "step": 644000
    },
    {
      "epoch": 49.20174165457184,
      "grad_norm": 10.341625213623047,
      "learning_rate": 8.998548621190131e-06,
      "loss": 1.7057,
      "step": 644100
    },
    {
      "epoch": 49.20938049041326,
      "grad_norm": 5.739377498626709,
      "learning_rate": 8.992182924655617e-06,
      "loss": 1.6671,
      "step": 644200
    },
    {
      "epoch": 49.21701932625468,
      "grad_norm": 6.72314977645874,
      "learning_rate": 8.985817228121102e-06,
      "loss": 1.6106,
      "step": 644300
    },
    {
      "epoch": 49.2246581620961,
      "grad_norm": 6.277027606964111,
      "learning_rate": 8.979451531586587e-06,
      "loss": 1.5545,
      "step": 644400
    },
    {
      "epoch": 49.23229699793752,
      "grad_norm": 11.500717163085938,
      "learning_rate": 8.973085835052072e-06,
      "loss": 1.6713,
      "step": 644500
    },
    {
      "epoch": 49.23993583377893,
      "grad_norm": 5.934829235076904,
      "learning_rate": 8.966720138517556e-06,
      "loss": 1.695,
      "step": 644600
    },
    {
      "epoch": 49.24757466962035,
      "grad_norm": 5.758574962615967,
      "learning_rate": 8.960354441983041e-06,
      "loss": 1.7187,
      "step": 644700
    },
    {
      "epoch": 49.25521350546177,
      "grad_norm": 7.279116630554199,
      "learning_rate": 8.953988745448528e-06,
      "loss": 1.6821,
      "step": 644800
    },
    {
      "epoch": 49.26285234130319,
      "grad_norm": 6.252087593078613,
      "learning_rate": 8.947623048914013e-06,
      "loss": 1.8014,
      "step": 644900
    },
    {
      "epoch": 49.270491177144606,
      "grad_norm": 5.212798118591309,
      "learning_rate": 8.941257352379498e-06,
      "loss": 1.6961,
      "step": 645000
    },
    {
      "epoch": 49.27813001298602,
      "grad_norm": 5.372674465179443,
      "learning_rate": 8.934891655844984e-06,
      "loss": 1.7052,
      "step": 645100
    },
    {
      "epoch": 49.28576884882744,
      "grad_norm": 6.010117530822754,
      "learning_rate": 8.928525959310469e-06,
      "loss": 1.7047,
      "step": 645200
    },
    {
      "epoch": 49.29340768466886,
      "grad_norm": 6.819607257843018,
      "learning_rate": 8.922160262775954e-06,
      "loss": 1.7603,
      "step": 645300
    },
    {
      "epoch": 49.301046520510276,
      "grad_norm": 5.543334484100342,
      "learning_rate": 8.915794566241438e-06,
      "loss": 1.5473,
      "step": 645400
    },
    {
      "epoch": 49.308685356351695,
      "grad_norm": 6.382333278656006,
      "learning_rate": 8.909428869706923e-06,
      "loss": 1.5806,
      "step": 645500
    },
    {
      "epoch": 49.31632419219311,
      "grad_norm": 6.3606038093566895,
      "learning_rate": 8.903063173172408e-06,
      "loss": 1.6897,
      "step": 645600
    },
    {
      "epoch": 49.32396302803453,
      "grad_norm": 4.426272392272949,
      "learning_rate": 8.896697476637893e-06,
      "loss": 1.759,
      "step": 645700
    },
    {
      "epoch": 49.331601863875946,
      "grad_norm": 8.288125991821289,
      "learning_rate": 8.89033178010338e-06,
      "loss": 1.695,
      "step": 645800
    },
    {
      "epoch": 49.339240699717365,
      "grad_norm": 6.943077087402344,
      "learning_rate": 8.883966083568866e-06,
      "loss": 1.7137,
      "step": 645900
    },
    {
      "epoch": 49.346879535558784,
      "grad_norm": 6.772149562835693,
      "learning_rate": 8.87760038703435e-06,
      "loss": 1.689,
      "step": 646000
    },
    {
      "epoch": 49.3545183714002,
      "grad_norm": 4.449424743652344,
      "learning_rate": 8.871234690499836e-06,
      "loss": 1.6196,
      "step": 646100
    },
    {
      "epoch": 49.362157207241616,
      "grad_norm": 7.176319599151611,
      "learning_rate": 8.86486899396532e-06,
      "loss": 1.688,
      "step": 646200
    },
    {
      "epoch": 49.369796043083035,
      "grad_norm": 7.449068546295166,
      "learning_rate": 8.858503297430805e-06,
      "loss": 1.648,
      "step": 646300
    },
    {
      "epoch": 49.377434878924454,
      "grad_norm": 5.408481597900391,
      "learning_rate": 8.85213760089629e-06,
      "loss": 1.6746,
      "step": 646400
    },
    {
      "epoch": 49.385073714765866,
      "grad_norm": 6.677781105041504,
      "learning_rate": 8.845771904361775e-06,
      "loss": 1.5576,
      "step": 646500
    },
    {
      "epoch": 49.392712550607285,
      "grad_norm": 5.7841339111328125,
      "learning_rate": 8.83940620782726e-06,
      "loss": 1.6992,
      "step": 646600
    },
    {
      "epoch": 49.400351386448705,
      "grad_norm": 4.475447654724121,
      "learning_rate": 8.833040511292746e-06,
      "loss": 1.8204,
      "step": 646700
    },
    {
      "epoch": 49.407990222290124,
      "grad_norm": 5.594250202178955,
      "learning_rate": 8.826674814758233e-06,
      "loss": 1.6874,
      "step": 646800
    },
    {
      "epoch": 49.41562905813154,
      "grad_norm": 5.8777384757995605,
      "learning_rate": 8.820309118223718e-06,
      "loss": 1.6899,
      "step": 646900
    },
    {
      "epoch": 49.423267893972955,
      "grad_norm": 5.862396240234375,
      "learning_rate": 8.813943421689201e-06,
      "loss": 1.6805,
      "step": 647000
    },
    {
      "epoch": 49.430906729814375,
      "grad_norm": 6.081191539764404,
      "learning_rate": 8.807577725154687e-06,
      "loss": 1.6631,
      "step": 647100
    },
    {
      "epoch": 49.438545565655794,
      "grad_norm": 5.5343499183654785,
      "learning_rate": 8.801212028620172e-06,
      "loss": 1.7262,
      "step": 647200
    },
    {
      "epoch": 49.44618440149721,
      "grad_norm": 8.391581535339355,
      "learning_rate": 8.794846332085657e-06,
      "loss": 1.6766,
      "step": 647300
    },
    {
      "epoch": 49.45382323733863,
      "grad_norm": 6.767543792724609,
      "learning_rate": 8.788480635551142e-06,
      "loss": 1.7023,
      "step": 647400
    },
    {
      "epoch": 49.461462073180044,
      "grad_norm": 5.1481170654296875,
      "learning_rate": 8.782114939016628e-06,
      "loss": 1.6891,
      "step": 647500
    },
    {
      "epoch": 49.46910090902146,
      "grad_norm": 5.292346000671387,
      "learning_rate": 8.775749242482113e-06,
      "loss": 1.581,
      "step": 647600
    },
    {
      "epoch": 49.47673974486288,
      "grad_norm": 5.139919757843018,
      "learning_rate": 8.769383545947598e-06,
      "loss": 1.652,
      "step": 647700
    },
    {
      "epoch": 49.4843785807043,
      "grad_norm": 4.878640174865723,
      "learning_rate": 8.763017849413083e-06,
      "loss": 1.6317,
      "step": 647800
    },
    {
      "epoch": 49.49201741654572,
      "grad_norm": 7.724509239196777,
      "learning_rate": 8.756652152878568e-06,
      "loss": 1.6616,
      "step": 647900
    },
    {
      "epoch": 49.49965625238713,
      "grad_norm": 6.290921211242676,
      "learning_rate": 8.750286456344054e-06,
      "loss": 1.6455,
      "step": 648000
    },
    {
      "epoch": 49.50729508822855,
      "grad_norm": 6.207122325897217,
      "learning_rate": 8.743920759809539e-06,
      "loss": 1.6517,
      "step": 648100
    },
    {
      "epoch": 49.51493392406997,
      "grad_norm": 5.898125648498535,
      "learning_rate": 8.737555063275024e-06,
      "loss": 1.7067,
      "step": 648200
    },
    {
      "epoch": 49.52257275991139,
      "grad_norm": 6.25136661529541,
      "learning_rate": 8.73118936674051e-06,
      "loss": 1.6004,
      "step": 648300
    },
    {
      "epoch": 49.53021159575281,
      "grad_norm": 6.160559177398682,
      "learning_rate": 8.724823670205995e-06,
      "loss": 1.6465,
      "step": 648400
    },
    {
      "epoch": 49.53785043159422,
      "grad_norm": 5.1351494789123535,
      "learning_rate": 8.718457973671478e-06,
      "loss": 1.6585,
      "step": 648500
    },
    {
      "epoch": 49.54548926743564,
      "grad_norm": 6.2219109535217285,
      "learning_rate": 8.712092277136965e-06,
      "loss": 1.7049,
      "step": 648600
    },
    {
      "epoch": 49.55312810327706,
      "grad_norm": 6.297394275665283,
      "learning_rate": 8.70572658060245e-06,
      "loss": 1.6885,
      "step": 648700
    },
    {
      "epoch": 49.56076693911848,
      "grad_norm": 5.030143737792969,
      "learning_rate": 8.699360884067936e-06,
      "loss": 1.6683,
      "step": 648800
    },
    {
      "epoch": 49.5684057749599,
      "grad_norm": 5.823544025421143,
      "learning_rate": 8.69299518753342e-06,
      "loss": 1.6702,
      "step": 648900
    },
    {
      "epoch": 49.57604461080131,
      "grad_norm": 6.9110307693481445,
      "learning_rate": 8.686629490998906e-06,
      "loss": 1.6946,
      "step": 649000
    },
    {
      "epoch": 49.58368344664273,
      "grad_norm": 6.221914291381836,
      "learning_rate": 8.680263794464391e-06,
      "loss": 1.5182,
      "step": 649100
    },
    {
      "epoch": 49.59132228248415,
      "grad_norm": 6.444257736206055,
      "learning_rate": 8.673898097929876e-06,
      "loss": 1.6123,
      "step": 649200
    },
    {
      "epoch": 49.59896111832557,
      "grad_norm": 6.345627307891846,
      "learning_rate": 8.66753240139536e-06,
      "loss": 1.674,
      "step": 649300
    },
    {
      "epoch": 49.60659995416699,
      "grad_norm": 7.480443000793457,
      "learning_rate": 8.661166704860845e-06,
      "loss": 1.7259,
      "step": 649400
    },
    {
      "epoch": 49.6142387900084,
      "grad_norm": 5.572721481323242,
      "learning_rate": 8.65480100832633e-06,
      "loss": 1.7406,
      "step": 649500
    },
    {
      "epoch": 49.62187762584982,
      "grad_norm": 6.615174293518066,
      "learning_rate": 8.648435311791817e-06,
      "loss": 1.7306,
      "step": 649600
    },
    {
      "epoch": 49.62951646169124,
      "grad_norm": 5.34755277633667,
      "learning_rate": 8.642069615257303e-06,
      "loss": 1.5701,
      "step": 649700
    },
    {
      "epoch": 49.63715529753266,
      "grad_norm": 6.046187877655029,
      "learning_rate": 8.635703918722788e-06,
      "loss": 1.7842,
      "step": 649800
    },
    {
      "epoch": 49.64479413337408,
      "grad_norm": 5.724435329437256,
      "learning_rate": 8.629338222188273e-06,
      "loss": 1.7341,
      "step": 649900
    },
    {
      "epoch": 49.65243296921549,
      "grad_norm": 5.070894241333008,
      "learning_rate": 8.622972525653758e-06,
      "loss": 1.6764,
      "step": 650000
    },
    {
      "epoch": 49.66007180505691,
      "grad_norm": 4.538473129272461,
      "learning_rate": 8.616606829119242e-06,
      "loss": 1.6487,
      "step": 650100
    },
    {
      "epoch": 49.66771064089833,
      "grad_norm": 5.12246561050415,
      "learning_rate": 8.610241132584727e-06,
      "loss": 1.5945,
      "step": 650200
    },
    {
      "epoch": 49.67534947673975,
      "grad_norm": 5.25440788269043,
      "learning_rate": 8.603875436050212e-06,
      "loss": 1.6685,
      "step": 650300
    },
    {
      "epoch": 49.68298831258116,
      "grad_norm": 6.20920467376709,
      "learning_rate": 8.597509739515698e-06,
      "loss": 1.709,
      "step": 650400
    },
    {
      "epoch": 49.69062714842258,
      "grad_norm": 5.002234935760498,
      "learning_rate": 8.591144042981183e-06,
      "loss": 1.7814,
      "step": 650500
    },
    {
      "epoch": 49.698265984264,
      "grad_norm": 5.351170063018799,
      "learning_rate": 8.58477834644667e-06,
      "loss": 1.6457,
      "step": 650600
    },
    {
      "epoch": 49.70590482010542,
      "grad_norm": 6.936562538146973,
      "learning_rate": 8.578412649912155e-06,
      "loss": 1.6613,
      "step": 650700
    },
    {
      "epoch": 49.713543655946836,
      "grad_norm": 6.959283351898193,
      "learning_rate": 8.57204695337764e-06,
      "loss": 1.5899,
      "step": 650800
    },
    {
      "epoch": 49.72118249178825,
      "grad_norm": 5.299860000610352,
      "learning_rate": 8.565681256843124e-06,
      "loss": 1.6546,
      "step": 650900
    },
    {
      "epoch": 49.72882132762967,
      "grad_norm": 6.523656845092773,
      "learning_rate": 8.559315560308609e-06,
      "loss": 1.6228,
      "step": 651000
    },
    {
      "epoch": 49.73646016347109,
      "grad_norm": 5.398313522338867,
      "learning_rate": 8.552949863774094e-06,
      "loss": 1.6735,
      "step": 651100
    },
    {
      "epoch": 49.744098999312506,
      "grad_norm": 7.673612117767334,
      "learning_rate": 8.54658416723958e-06,
      "loss": 1.676,
      "step": 651200
    },
    {
      "epoch": 49.751737835153925,
      "grad_norm": 4.677674770355225,
      "learning_rate": 8.540218470705065e-06,
      "loss": 1.6441,
      "step": 651300
    },
    {
      "epoch": 49.75937667099534,
      "grad_norm": 5.862667560577393,
      "learning_rate": 8.53385277417055e-06,
      "loss": 1.6525,
      "step": 651400
    },
    {
      "epoch": 49.767015506836756,
      "grad_norm": 4.778517723083496,
      "learning_rate": 8.527487077636035e-06,
      "loss": 1.7002,
      "step": 651500
    },
    {
      "epoch": 49.774654342678176,
      "grad_norm": 4.37764835357666,
      "learning_rate": 8.521121381101522e-06,
      "loss": 1.6227,
      "step": 651600
    },
    {
      "epoch": 49.782293178519595,
      "grad_norm": 7.072505474090576,
      "learning_rate": 8.514755684567006e-06,
      "loss": 1.6939,
      "step": 651700
    },
    {
      "epoch": 49.789932014361014,
      "grad_norm": 3.628490686416626,
      "learning_rate": 8.50838998803249e-06,
      "loss": 1.7308,
      "step": 651800
    },
    {
      "epoch": 49.797570850202426,
      "grad_norm": 8.681468963623047,
      "learning_rate": 8.502024291497976e-06,
      "loss": 1.7068,
      "step": 651900
    },
    {
      "epoch": 49.805209686043845,
      "grad_norm": 5.668773651123047,
      "learning_rate": 8.495658594963461e-06,
      "loss": 1.6408,
      "step": 652000
    },
    {
      "epoch": 49.812848521885265,
      "grad_norm": 5.78593635559082,
      "learning_rate": 8.489292898428946e-06,
      "loss": 1.6862,
      "step": 652100
    },
    {
      "epoch": 49.820487357726684,
      "grad_norm": 7.469154357910156,
      "learning_rate": 8.482927201894432e-06,
      "loss": 1.6059,
      "step": 652200
    },
    {
      "epoch": 49.8281261935681,
      "grad_norm": 6.057969570159912,
      "learning_rate": 8.476561505359917e-06,
      "loss": 1.579,
      "step": 652300
    },
    {
      "epoch": 49.835765029409515,
      "grad_norm": 8.992391586303711,
      "learning_rate": 8.470195808825402e-06,
      "loss": 1.7307,
      "step": 652400
    },
    {
      "epoch": 49.843403865250934,
      "grad_norm": 5.649016380310059,
      "learning_rate": 8.463830112290887e-06,
      "loss": 1.6607,
      "step": 652500
    },
    {
      "epoch": 49.851042701092354,
      "grad_norm": 5.840665817260742,
      "learning_rate": 8.457464415756373e-06,
      "loss": 1.6733,
      "step": 652600
    },
    {
      "epoch": 49.85868153693377,
      "grad_norm": 5.248531341552734,
      "learning_rate": 8.451098719221858e-06,
      "loss": 1.6913,
      "step": 652700
    },
    {
      "epoch": 49.86632037277519,
      "grad_norm": 4.643907070159912,
      "learning_rate": 8.444733022687343e-06,
      "loss": 1.6472,
      "step": 652800
    },
    {
      "epoch": 49.873959208616604,
      "grad_norm": 5.3818254470825195,
      "learning_rate": 8.438367326152828e-06,
      "loss": 1.6733,
      "step": 652900
    },
    {
      "epoch": 49.88159804445802,
      "grad_norm": 6.225611686706543,
      "learning_rate": 8.432001629618314e-06,
      "loss": 1.6535,
      "step": 653000
    },
    {
      "epoch": 49.88923688029944,
      "grad_norm": 4.772578716278076,
      "learning_rate": 8.425635933083799e-06,
      "loss": 1.5079,
      "step": 653100
    },
    {
      "epoch": 49.89687571614086,
      "grad_norm": 7.07700777053833,
      "learning_rate": 8.419270236549284e-06,
      "loss": 1.689,
      "step": 653200
    },
    {
      "epoch": 49.90451455198228,
      "grad_norm": 5.167425632476807,
      "learning_rate": 8.412904540014768e-06,
      "loss": 1.7604,
      "step": 653300
    },
    {
      "epoch": 49.91215338782369,
      "grad_norm": 7.47617244720459,
      "learning_rate": 8.406538843480254e-06,
      "loss": 1.6995,
      "step": 653400
    },
    {
      "epoch": 49.91979222366511,
      "grad_norm": 6.722566604614258,
      "learning_rate": 8.40017314694574e-06,
      "loss": 1.6869,
      "step": 653500
    },
    {
      "epoch": 49.92743105950653,
      "grad_norm": 8.687090873718262,
      "learning_rate": 8.393807450411225e-06,
      "loss": 1.6805,
      "step": 653600
    },
    {
      "epoch": 49.93506989534795,
      "grad_norm": 4.86493444442749,
      "learning_rate": 8.38744175387671e-06,
      "loss": 1.6329,
      "step": 653700
    },
    {
      "epoch": 49.94270873118937,
      "grad_norm": 6.143304824829102,
      "learning_rate": 8.381076057342195e-06,
      "loss": 1.7357,
      "step": 653800
    },
    {
      "epoch": 49.95034756703078,
      "grad_norm": 5.747901916503906,
      "learning_rate": 8.37471036080768e-06,
      "loss": 1.6578,
      "step": 653900
    },
    {
      "epoch": 49.9579864028722,
      "grad_norm": 4.083413124084473,
      "learning_rate": 8.368344664273166e-06,
      "loss": 1.6523,
      "step": 654000
    },
    {
      "epoch": 49.96562523871362,
      "grad_norm": 6.273045539855957,
      "learning_rate": 8.36197896773865e-06,
      "loss": 1.6395,
      "step": 654100
    },
    {
      "epoch": 49.97326407455504,
      "grad_norm": 5.069854259490967,
      "learning_rate": 8.355613271204135e-06,
      "loss": 1.5453,
      "step": 654200
    },
    {
      "epoch": 49.98090291039646,
      "grad_norm": 5.261346817016602,
      "learning_rate": 8.34924757466962e-06,
      "loss": 1.7667,
      "step": 654300
    },
    {
      "epoch": 49.98854174623787,
      "grad_norm": 4.931425094604492,
      "learning_rate": 8.342881878135107e-06,
      "loss": 1.6938,
      "step": 654400
    },
    {
      "epoch": 49.99618058207929,
      "grad_norm": 5.336640357971191,
      "learning_rate": 8.336516181600592e-06,
      "loss": 1.6983,
      "step": 654500
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.7675817012786865,
      "eval_runtime": 3.0383,
      "eval_samples_per_second": 227.1,
      "eval_steps_per_second": 227.1,
      "step": 654550
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.4279674291610718,
      "eval_runtime": 57.6271,
      "eval_samples_per_second": 227.168,
      "eval_steps_per_second": 227.168,
      "step": 654550
    },
    {
      "epoch": 50.00381941792071,
      "grad_norm": 7.665390491485596,
      "learning_rate": 8.330150485066077e-06,
      "loss": 1.6611,
      "step": 654600
    },
    {
      "epoch": 50.01145825376213,
      "grad_norm": 6.060481548309326,
      "learning_rate": 8.323784788531562e-06,
      "loss": 1.7764,
      "step": 654700
    },
    {
      "epoch": 50.01909708960354,
      "grad_norm": 6.348606109619141,
      "learning_rate": 8.317419091997048e-06,
      "loss": 1.6676,
      "step": 654800
    },
    {
      "epoch": 50.02673592544496,
      "grad_norm": 4.867170810699463,
      "learning_rate": 8.311053395462531e-06,
      "loss": 1.6905,
      "step": 654900
    },
    {
      "epoch": 50.03437476128638,
      "grad_norm": 5.184966564178467,
      "learning_rate": 8.304687698928016e-06,
      "loss": 1.665,
      "step": 655000
    },
    {
      "epoch": 50.0420135971278,
      "grad_norm": 5.596755504608154,
      "learning_rate": 8.298322002393502e-06,
      "loss": 1.5347,
      "step": 655100
    },
    {
      "epoch": 50.04965243296922,
      "grad_norm": 6.177641868591309,
      "learning_rate": 8.291956305858987e-06,
      "loss": 1.6387,
      "step": 655200
    },
    {
      "epoch": 50.05729126881063,
      "grad_norm": 5.4564619064331055,
      "learning_rate": 8.285590609324472e-06,
      "loss": 1.6464,
      "step": 655300
    },
    {
      "epoch": 50.06493010465205,
      "grad_norm": 4.5737481117248535,
      "learning_rate": 8.279224912789959e-06,
      "loss": 1.6963,
      "step": 655400
    },
    {
      "epoch": 50.07256894049347,
      "grad_norm": 5.113501071929932,
      "learning_rate": 8.272859216255444e-06,
      "loss": 1.6683,
      "step": 655500
    },
    {
      "epoch": 50.08020777633489,
      "grad_norm": 5.760585308074951,
      "learning_rate": 8.26649351972093e-06,
      "loss": 1.7874,
      "step": 655600
    },
    {
      "epoch": 50.08784661217631,
      "grad_norm": 6.352732181549072,
      "learning_rate": 8.260127823186413e-06,
      "loss": 1.6169,
      "step": 655700
    },
    {
      "epoch": 50.09548544801772,
      "grad_norm": 4.6772871017456055,
      "learning_rate": 8.253762126651898e-06,
      "loss": 1.5875,
      "step": 655800
    },
    {
      "epoch": 50.10312428385914,
      "grad_norm": 6.540454864501953,
      "learning_rate": 8.247396430117384e-06,
      "loss": 1.6523,
      "step": 655900
    },
    {
      "epoch": 50.11076311970056,
      "grad_norm": 6.230140686035156,
      "learning_rate": 8.241030733582869e-06,
      "loss": 1.6355,
      "step": 656000
    },
    {
      "epoch": 50.11840195554198,
      "grad_norm": 7.3319010734558105,
      "learning_rate": 8.234665037048354e-06,
      "loss": 1.7733,
      "step": 656100
    },
    {
      "epoch": 50.126040791383396,
      "grad_norm": 6.233104705810547,
      "learning_rate": 8.22829934051384e-06,
      "loss": 1.7117,
      "step": 656200
    },
    {
      "epoch": 50.13367962722481,
      "grad_norm": 6.138639450073242,
      "learning_rate": 8.221933643979324e-06,
      "loss": 1.663,
      "step": 656300
    },
    {
      "epoch": 50.14131846306623,
      "grad_norm": 5.199666500091553,
      "learning_rate": 8.21556794744481e-06,
      "loss": 1.598,
      "step": 656400
    },
    {
      "epoch": 50.148957298907646,
      "grad_norm": 6.203488826751709,
      "learning_rate": 8.209202250910295e-06,
      "loss": 1.6153,
      "step": 656500
    },
    {
      "epoch": 50.156596134749066,
      "grad_norm": 4.638282775878906,
      "learning_rate": 8.20283655437578e-06,
      "loss": 1.5899,
      "step": 656600
    },
    {
      "epoch": 50.164234970590485,
      "grad_norm": 5.480164051055908,
      "learning_rate": 8.196470857841265e-06,
      "loss": 1.7469,
      "step": 656700
    },
    {
      "epoch": 50.1718738064319,
      "grad_norm": 5.513218402862549,
      "learning_rate": 8.19010516130675e-06,
      "loss": 1.7193,
      "step": 656800
    },
    {
      "epoch": 50.179512642273316,
      "grad_norm": 8.683313369750977,
      "learning_rate": 8.183739464772236e-06,
      "loss": 1.7675,
      "step": 656900
    },
    {
      "epoch": 50.187151478114735,
      "grad_norm": 5.014987945556641,
      "learning_rate": 8.177373768237721e-06,
      "loss": 1.7136,
      "step": 657000
    },
    {
      "epoch": 50.194790313956155,
      "grad_norm": 5.534670829772949,
      "learning_rate": 8.171008071703206e-06,
      "loss": 1.7738,
      "step": 657100
    },
    {
      "epoch": 50.202429149797574,
      "grad_norm": 5.579684257507324,
      "learning_rate": 8.164642375168692e-06,
      "loss": 1.5738,
      "step": 657200
    },
    {
      "epoch": 50.210067985638986,
      "grad_norm": 4.959832668304443,
      "learning_rate": 8.158276678634177e-06,
      "loss": 1.5792,
      "step": 657300
    },
    {
      "epoch": 50.217706821480405,
      "grad_norm": 4.315649509429932,
      "learning_rate": 8.151910982099662e-06,
      "loss": 1.674,
      "step": 657400
    },
    {
      "epoch": 50.225345657321824,
      "grad_norm": 3.1436076164245605,
      "learning_rate": 8.145545285565147e-06,
      "loss": 1.5524,
      "step": 657500
    },
    {
      "epoch": 50.232984493163244,
      "grad_norm": 5.573809623718262,
      "learning_rate": 8.139179589030632e-06,
      "loss": 1.6854,
      "step": 657600
    },
    {
      "epoch": 50.24062332900466,
      "grad_norm": 3.889279842376709,
      "learning_rate": 8.132813892496118e-06,
      "loss": 1.6197,
      "step": 657700
    },
    {
      "epoch": 50.248262164846075,
      "grad_norm": 7.119768142700195,
      "learning_rate": 8.126448195961603e-06,
      "loss": 1.6576,
      "step": 657800
    },
    {
      "epoch": 50.255901000687494,
      "grad_norm": 8.611199378967285,
      "learning_rate": 8.120082499427088e-06,
      "loss": 1.6582,
      "step": 657900
    },
    {
      "epoch": 50.26353983652891,
      "grad_norm": 5.106244087219238,
      "learning_rate": 8.113716802892572e-06,
      "loss": 1.6754,
      "step": 658000
    },
    {
      "epoch": 50.27117867237033,
      "grad_norm": 5.80056095123291,
      "learning_rate": 8.107351106358057e-06,
      "loss": 1.5773,
      "step": 658100
    },
    {
      "epoch": 50.27881750821175,
      "grad_norm": 4.123693466186523,
      "learning_rate": 8.100985409823544e-06,
      "loss": 1.6488,
      "step": 658200
    },
    {
      "epoch": 50.286456344053164,
      "grad_norm": 5.198522567749023,
      "learning_rate": 8.094619713289029e-06,
      "loss": 1.6763,
      "step": 658300
    },
    {
      "epoch": 50.29409517989458,
      "grad_norm": 5.989585876464844,
      "learning_rate": 8.088254016754514e-06,
      "loss": 1.6318,
      "step": 658400
    },
    {
      "epoch": 50.301734015736,
      "grad_norm": 7.059685707092285,
      "learning_rate": 8.08188832022e-06,
      "loss": 1.6551,
      "step": 658500
    },
    {
      "epoch": 50.30937285157742,
      "grad_norm": 6.593732833862305,
      "learning_rate": 8.075522623685485e-06,
      "loss": 1.7191,
      "step": 658600
    },
    {
      "epoch": 50.31701168741884,
      "grad_norm": 5.942458629608154,
      "learning_rate": 8.06915692715097e-06,
      "loss": 1.6559,
      "step": 658700
    },
    {
      "epoch": 50.32465052326025,
      "grad_norm": 6.219500541687012,
      "learning_rate": 8.062791230616453e-06,
      "loss": 1.6077,
      "step": 658800
    },
    {
      "epoch": 50.33228935910167,
      "grad_norm": 4.623891353607178,
      "learning_rate": 8.056425534081939e-06,
      "loss": 1.5276,
      "step": 658900
    },
    {
      "epoch": 50.33992819494309,
      "grad_norm": 4.977240085601807,
      "learning_rate": 8.050059837547424e-06,
      "loss": 1.6935,
      "step": 659000
    },
    {
      "epoch": 50.34756703078451,
      "grad_norm": 9.542304992675781,
      "learning_rate": 8.04369414101291e-06,
      "loss": 1.6604,
      "step": 659100
    },
    {
      "epoch": 50.35520586662592,
      "grad_norm": 4.484404563903809,
      "learning_rate": 8.037328444478396e-06,
      "loss": 1.5976,
      "step": 659200
    },
    {
      "epoch": 50.36284470246734,
      "grad_norm": 5.8319902420043945,
      "learning_rate": 8.030962747943881e-06,
      "loss": 1.7087,
      "step": 659300
    },
    {
      "epoch": 50.37048353830876,
      "grad_norm": 9.067705154418945,
      "learning_rate": 8.024597051409367e-06,
      "loss": 1.6534,
      "step": 659400
    },
    {
      "epoch": 50.37812237415018,
      "grad_norm": 4.908513069152832,
      "learning_rate": 8.018231354874852e-06,
      "loss": 1.7617,
      "step": 659500
    },
    {
      "epoch": 50.3857612099916,
      "grad_norm": 6.134512424468994,
      "learning_rate": 8.011865658340335e-06,
      "loss": 1.6539,
      "step": 659600
    },
    {
      "epoch": 50.39340004583301,
      "grad_norm": 11.696722984313965,
      "learning_rate": 8.00549996180582e-06,
      "loss": 1.7485,
      "step": 659700
    },
    {
      "epoch": 50.40103888167443,
      "grad_norm": 6.096528053283691,
      "learning_rate": 7.999134265271306e-06,
      "loss": 1.6323,
      "step": 659800
    },
    {
      "epoch": 50.40867771751585,
      "grad_norm": 4.863277912139893,
      "learning_rate": 7.992768568736791e-06,
      "loss": 1.6028,
      "step": 659900
    },
    {
      "epoch": 50.41631655335727,
      "grad_norm": 6.097926616668701,
      "learning_rate": 7.986402872202276e-06,
      "loss": 1.6505,
      "step": 660000
    },
    {
      "epoch": 50.42395538919869,
      "grad_norm": 5.464179992675781,
      "learning_rate": 7.980037175667761e-06,
      "loss": 1.7822,
      "step": 660100
    },
    {
      "epoch": 50.4315942250401,
      "grad_norm": 5.377533912658691,
      "learning_rate": 7.973671479133248e-06,
      "loss": 1.6602,
      "step": 660200
    },
    {
      "epoch": 50.43923306088152,
      "grad_norm": 7.783638000488281,
      "learning_rate": 7.967305782598734e-06,
      "loss": 1.6715,
      "step": 660300
    },
    {
      "epoch": 50.44687189672294,
      "grad_norm": 5.63162088394165,
      "learning_rate": 7.960940086064217e-06,
      "loss": 1.6655,
      "step": 660400
    },
    {
      "epoch": 50.45451073256436,
      "grad_norm": 7.68012809753418,
      "learning_rate": 7.954574389529702e-06,
      "loss": 1.6395,
      "step": 660500
    },
    {
      "epoch": 50.46214956840578,
      "grad_norm": 6.815326690673828,
      "learning_rate": 7.948208692995188e-06,
      "loss": 1.6373,
      "step": 660600
    },
    {
      "epoch": 50.46978840424719,
      "grad_norm": 7.167413711547852,
      "learning_rate": 7.941842996460673e-06,
      "loss": 1.6619,
      "step": 660700
    },
    {
      "epoch": 50.47742724008861,
      "grad_norm": 8.864025115966797,
      "learning_rate": 7.935477299926158e-06,
      "loss": 1.6711,
      "step": 660800
    },
    {
      "epoch": 50.48506607593003,
      "grad_norm": 6.017006874084473,
      "learning_rate": 7.929111603391643e-06,
      "loss": 1.649,
      "step": 660900
    },
    {
      "epoch": 50.49270491177145,
      "grad_norm": 5.149747371673584,
      "learning_rate": 7.922745906857129e-06,
      "loss": 1.6412,
      "step": 661000
    },
    {
      "epoch": 50.50034374761287,
      "grad_norm": 6.806668758392334,
      "learning_rate": 7.916380210322614e-06,
      "loss": 1.6985,
      "step": 661100
    },
    {
      "epoch": 50.50798258345428,
      "grad_norm": 7.2788214683532715,
      "learning_rate": 7.910014513788099e-06,
      "loss": 1.677,
      "step": 661200
    },
    {
      "epoch": 50.5156214192957,
      "grad_norm": 7.926620960235596,
      "learning_rate": 7.903648817253584e-06,
      "loss": 1.6525,
      "step": 661300
    },
    {
      "epoch": 50.52326025513712,
      "grad_norm": 5.918035984039307,
      "learning_rate": 7.89728312071907e-06,
      "loss": 1.6268,
      "step": 661400
    },
    {
      "epoch": 50.53089909097854,
      "grad_norm": 6.453835964202881,
      "learning_rate": 7.890917424184555e-06,
      "loss": 1.6589,
      "step": 661500
    },
    {
      "epoch": 50.538537926819956,
      "grad_norm": 4.949940204620361,
      "learning_rate": 7.88455172765004e-06,
      "loss": 1.6254,
      "step": 661600
    },
    {
      "epoch": 50.54617676266137,
      "grad_norm": 10.586743354797363,
      "learning_rate": 7.878186031115525e-06,
      "loss": 1.6467,
      "step": 661700
    },
    {
      "epoch": 50.55381559850279,
      "grad_norm": 6.2972540855407715,
      "learning_rate": 7.87182033458101e-06,
      "loss": 1.7099,
      "step": 661800
    },
    {
      "epoch": 50.561454434344206,
      "grad_norm": 7.237600326538086,
      "learning_rate": 7.865454638046496e-06,
      "loss": 1.6482,
      "step": 661900
    },
    {
      "epoch": 50.569093270185625,
      "grad_norm": 6.80472993850708,
      "learning_rate": 7.859088941511981e-06,
      "loss": 1.7364,
      "step": 662000
    },
    {
      "epoch": 50.576732106027045,
      "grad_norm": 5.703480243682861,
      "learning_rate": 7.852723244977466e-06,
      "loss": 1.691,
      "step": 662100
    },
    {
      "epoch": 50.58437094186846,
      "grad_norm": 6.409777641296387,
      "learning_rate": 7.846357548442951e-06,
      "loss": 1.6751,
      "step": 662200
    },
    {
      "epoch": 50.592009777709876,
      "grad_norm": 6.334525108337402,
      "learning_rate": 7.839991851908437e-06,
      "loss": 1.6979,
      "step": 662300
    },
    {
      "epoch": 50.599648613551295,
      "grad_norm": 4.991335391998291,
      "learning_rate": 7.833626155373922e-06,
      "loss": 1.6655,
      "step": 662400
    },
    {
      "epoch": 50.607287449392715,
      "grad_norm": 5.905850410461426,
      "learning_rate": 7.827260458839407e-06,
      "loss": 1.7602,
      "step": 662500
    },
    {
      "epoch": 50.614926285234134,
      "grad_norm": 6.688587188720703,
      "learning_rate": 7.820894762304892e-06,
      "loss": 1.6801,
      "step": 662600
    },
    {
      "epoch": 50.622565121075546,
      "grad_norm": 8.17046070098877,
      "learning_rate": 7.814529065770377e-06,
      "loss": 1.6451,
      "step": 662700
    },
    {
      "epoch": 50.630203956916965,
      "grad_norm": 6.659950256347656,
      "learning_rate": 7.808163369235861e-06,
      "loss": 1.6905,
      "step": 662800
    },
    {
      "epoch": 50.637842792758384,
      "grad_norm": 4.532078742980957,
      "learning_rate": 7.801797672701346e-06,
      "loss": 1.6756,
      "step": 662900
    },
    {
      "epoch": 50.6454816285998,
      "grad_norm": 4.587836265563965,
      "learning_rate": 7.795431976166833e-06,
      "loss": 1.6808,
      "step": 663000
    },
    {
      "epoch": 50.653120464441216,
      "grad_norm": 7.022619247436523,
      "learning_rate": 7.789066279632318e-06,
      "loss": 1.6258,
      "step": 663100
    },
    {
      "epoch": 50.660759300282635,
      "grad_norm": 5.994460582733154,
      "learning_rate": 7.782700583097804e-06,
      "loss": 1.8066,
      "step": 663200
    },
    {
      "epoch": 50.668398136124054,
      "grad_norm": 3.6228764057159424,
      "learning_rate": 7.776334886563289e-06,
      "loss": 1.7209,
      "step": 663300
    },
    {
      "epoch": 50.67603697196547,
      "grad_norm": 5.192248344421387,
      "learning_rate": 7.769969190028774e-06,
      "loss": 1.6459,
      "step": 663400
    },
    {
      "epoch": 50.68367580780689,
      "grad_norm": 5.479316711425781,
      "learning_rate": 7.76360349349426e-06,
      "loss": 1.7009,
      "step": 663500
    },
    {
      "epoch": 50.691314643648305,
      "grad_norm": 5.0274338722229,
      "learning_rate": 7.757237796959743e-06,
      "loss": 1.5523,
      "step": 663600
    },
    {
      "epoch": 50.698953479489724,
      "grad_norm": 7.486623764038086,
      "learning_rate": 7.750872100425228e-06,
      "loss": 1.6493,
      "step": 663700
    },
    {
      "epoch": 50.70659231533114,
      "grad_norm": 4.851303577423096,
      "learning_rate": 7.744506403890713e-06,
      "loss": 1.7278,
      "step": 663800
    },
    {
      "epoch": 50.71423115117256,
      "grad_norm": 5.422630786895752,
      "learning_rate": 7.738140707356199e-06,
      "loss": 1.6391,
      "step": 663900
    },
    {
      "epoch": 50.72186998701398,
      "grad_norm": 3.6581549644470215,
      "learning_rate": 7.731775010821685e-06,
      "loss": 1.614,
      "step": 664000
    },
    {
      "epoch": 50.729508822855394,
      "grad_norm": 6.229951858520508,
      "learning_rate": 7.72540931428717e-06,
      "loss": 1.6052,
      "step": 664100
    },
    {
      "epoch": 50.73714765869681,
      "grad_norm": 5.516754150390625,
      "learning_rate": 7.719043617752656e-06,
      "loss": 1.7549,
      "step": 664200
    },
    {
      "epoch": 50.74478649453823,
      "grad_norm": 6.198404788970947,
      "learning_rate": 7.71267792121814e-06,
      "loss": 1.6423,
      "step": 664300
    },
    {
      "epoch": 50.75242533037965,
      "grad_norm": 5.645606994628906,
      "learning_rate": 7.706312224683625e-06,
      "loss": 1.6218,
      "step": 664400
    },
    {
      "epoch": 50.76006416622107,
      "grad_norm": 4.8919782638549805,
      "learning_rate": 7.69994652814911e-06,
      "loss": 1.7095,
      "step": 664500
    },
    {
      "epoch": 50.76770300206248,
      "grad_norm": 5.0478386878967285,
      "learning_rate": 7.693580831614595e-06,
      "loss": 1.7042,
      "step": 664600
    },
    {
      "epoch": 50.7753418379039,
      "grad_norm": 6.153415203094482,
      "learning_rate": 7.68721513508008e-06,
      "loss": 1.6134,
      "step": 664700
    },
    {
      "epoch": 50.78298067374532,
      "grad_norm": 5.741031646728516,
      "learning_rate": 7.680849438545566e-06,
      "loss": 1.6415,
      "step": 664800
    },
    {
      "epoch": 50.79061950958674,
      "grad_norm": 3.7410993576049805,
      "learning_rate": 7.674483742011051e-06,
      "loss": 1.6109,
      "step": 664900
    },
    {
      "epoch": 50.79825834542816,
      "grad_norm": 7.849706172943115,
      "learning_rate": 7.668118045476536e-06,
      "loss": 1.6543,
      "step": 665000
    },
    {
      "epoch": 50.80589718126957,
      "grad_norm": 5.133119583129883,
      "learning_rate": 7.661752348942021e-06,
      "loss": 1.6774,
      "step": 665100
    },
    {
      "epoch": 50.81353601711099,
      "grad_norm": 6.335470199584961,
      "learning_rate": 7.655386652407507e-06,
      "loss": 1.7796,
      "step": 665200
    },
    {
      "epoch": 50.82117485295241,
      "grad_norm": 5.095435619354248,
      "learning_rate": 7.649020955872992e-06,
      "loss": 1.7357,
      "step": 665300
    },
    {
      "epoch": 50.82881368879383,
      "grad_norm": 6.595631122589111,
      "learning_rate": 7.642655259338477e-06,
      "loss": 1.6983,
      "step": 665400
    },
    {
      "epoch": 50.83645252463525,
      "grad_norm": 4.936891555786133,
      "learning_rate": 7.636289562803962e-06,
      "loss": 1.7184,
      "step": 665500
    },
    {
      "epoch": 50.84409136047666,
      "grad_norm": 8.4594087600708,
      "learning_rate": 7.629923866269447e-06,
      "loss": 1.6686,
      "step": 665600
    },
    {
      "epoch": 50.85173019631808,
      "grad_norm": 7.237072467803955,
      "learning_rate": 7.623558169734933e-06,
      "loss": 1.6274,
      "step": 665700
    },
    {
      "epoch": 50.8593690321595,
      "grad_norm": 4.863776683807373,
      "learning_rate": 7.617192473200419e-06,
      "loss": 1.6577,
      "step": 665800
    },
    {
      "epoch": 50.86700786800092,
      "grad_norm": 4.534554481506348,
      "learning_rate": 7.610826776665902e-06,
      "loss": 1.6719,
      "step": 665900
    },
    {
      "epoch": 50.87464670384234,
      "grad_norm": 4.8004913330078125,
      "learning_rate": 7.6044610801313876e-06,
      "loss": 1.6518,
      "step": 666000
    },
    {
      "epoch": 50.88228553968375,
      "grad_norm": 7.452777862548828,
      "learning_rate": 7.598095383596874e-06,
      "loss": 1.7107,
      "step": 666100
    },
    {
      "epoch": 50.88992437552517,
      "grad_norm": 5.44984245300293,
      "learning_rate": 7.591729687062359e-06,
      "loss": 1.5899,
      "step": 666200
    },
    {
      "epoch": 50.89756321136659,
      "grad_norm": 6.581993103027344,
      "learning_rate": 7.585363990527844e-06,
      "loss": 1.6672,
      "step": 666300
    },
    {
      "epoch": 50.90520204720801,
      "grad_norm": 5.94862699508667,
      "learning_rate": 7.578998293993329e-06,
      "loss": 1.6169,
      "step": 666400
    },
    {
      "epoch": 50.91284088304943,
      "grad_norm": 6.5272111892700195,
      "learning_rate": 7.5726325974588146e-06,
      "loss": 1.7014,
      "step": 666500
    },
    {
      "epoch": 50.92047971889084,
      "grad_norm": 6.922396659851074,
      "learning_rate": 7.5662669009243e-06,
      "loss": 1.6532,
      "step": 666600
    },
    {
      "epoch": 50.92811855473226,
      "grad_norm": 5.101492881774902,
      "learning_rate": 7.559901204389784e-06,
      "loss": 1.6718,
      "step": 666700
    },
    {
      "epoch": 50.93575739057368,
      "grad_norm": 5.563222885131836,
      "learning_rate": 7.553535507855269e-06,
      "loss": 1.6752,
      "step": 666800
    },
    {
      "epoch": 50.943396226415096,
      "grad_norm": 4.745147705078125,
      "learning_rate": 7.547169811320755e-06,
      "loss": 1.6062,
      "step": 666900
    },
    {
      "epoch": 50.951035062256516,
      "grad_norm": 7.113264560699463,
      "learning_rate": 7.54080411478624e-06,
      "loss": 1.6485,
      "step": 667000
    },
    {
      "epoch": 50.95867389809793,
      "grad_norm": 4.124155521392822,
      "learning_rate": 7.534438418251725e-06,
      "loss": 1.7222,
      "step": 667100
    },
    {
      "epoch": 50.96631273393935,
      "grad_norm": 6.087859630584717,
      "learning_rate": 7.528072721717211e-06,
      "loss": 1.747,
      "step": 667200
    },
    {
      "epoch": 50.973951569780766,
      "grad_norm": 6.040027618408203,
      "learning_rate": 7.521707025182696e-06,
      "loss": 1.6526,
      "step": 667300
    },
    {
      "epoch": 50.981590405622185,
      "grad_norm": 5.990872859954834,
      "learning_rate": 7.515341328648182e-06,
      "loss": 1.7091,
      "step": 667400
    },
    {
      "epoch": 50.9892292414636,
      "grad_norm": 4.310040473937988,
      "learning_rate": 7.508975632113666e-06,
      "loss": 1.6359,
      "step": 667500
    },
    {
      "epoch": 50.99686807730502,
      "grad_norm": 7.4748969078063965,
      "learning_rate": 7.502609935579151e-06,
      "loss": 1.7104,
      "step": 667600
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.7665959596633911,
      "eval_runtime": 3.0416,
      "eval_samples_per_second": 226.857,
      "eval_steps_per_second": 226.857,
      "step": 667641
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.4249317646026611,
      "eval_runtime": 57.6934,
      "eval_samples_per_second": 226.906,
      "eval_steps_per_second": 226.906,
      "step": 667641
    },
    {
      "epoch": 51.004506913146436,
      "grad_norm": 5.014420509338379,
      "learning_rate": 7.4962442390446365e-06,
      "loss": 1.632,
      "step": 667700
    },
    {
      "epoch": 51.012145748987855,
      "grad_norm": 4.869631767272949,
      "learning_rate": 7.489878542510122e-06,
      "loss": 1.6939,
      "step": 667800
    },
    {
      "epoch": 51.019784584829274,
      "grad_norm": 7.176863670349121,
      "learning_rate": 7.483512845975607e-06,
      "loss": 1.6959,
      "step": 667900
    },
    {
      "epoch": 51.02742342067069,
      "grad_norm": 6.417855739593506,
      "learning_rate": 7.477147149441092e-06,
      "loss": 1.6564,
      "step": 668000
    },
    {
      "epoch": 51.035062256512106,
      "grad_norm": 5.036755084991455,
      "learning_rate": 7.470781452906577e-06,
      "loss": 1.6751,
      "step": 668100
    },
    {
      "epoch": 51.042701092353525,
      "grad_norm": 6.469101428985596,
      "learning_rate": 7.4644157563720635e-06,
      "loss": 1.6156,
      "step": 668200
    },
    {
      "epoch": 51.050339928194944,
      "grad_norm": 4.481647968292236,
      "learning_rate": 7.458050059837547e-06,
      "loss": 1.7442,
      "step": 668300
    },
    {
      "epoch": 51.05797876403636,
      "grad_norm": 4.216817855834961,
      "learning_rate": 7.451684363303032e-06,
      "loss": 1.6704,
      "step": 668400
    },
    {
      "epoch": 51.065617599877775,
      "grad_norm": 5.1177754402160645,
      "learning_rate": 7.4453186667685175e-06,
      "loss": 1.6994,
      "step": 668500
    },
    {
      "epoch": 51.073256435719195,
      "grad_norm": 5.596358776092529,
      "learning_rate": 7.4389529702340036e-06,
      "loss": 1.6797,
      "step": 668600
    },
    {
      "epoch": 51.080895271560614,
      "grad_norm": 6.885013103485107,
      "learning_rate": 7.432587273699489e-06,
      "loss": 1.6408,
      "step": 668700
    },
    {
      "epoch": 51.08853410740203,
      "grad_norm": 4.445962905883789,
      "learning_rate": 7.426221577164974e-06,
      "loss": 1.7028,
      "step": 668800
    },
    {
      "epoch": 51.09617294324345,
      "grad_norm": 5.268774509429932,
      "learning_rate": 7.419855880630459e-06,
      "loss": 1.5694,
      "step": 668900
    },
    {
      "epoch": 51.103811779084864,
      "grad_norm": 5.812099933624268,
      "learning_rate": 7.4134901840959445e-06,
      "loss": 1.6546,
      "step": 669000
    },
    {
      "epoch": 51.111450614926284,
      "grad_norm": 8.185001373291016,
      "learning_rate": 7.407124487561429e-06,
      "loss": 1.5864,
      "step": 669100
    },
    {
      "epoch": 51.1190894507677,
      "grad_norm": 5.359178066253662,
      "learning_rate": 7.400758791026914e-06,
      "loss": 1.598,
      "step": 669200
    },
    {
      "epoch": 51.12672828660912,
      "grad_norm": 5.369293212890625,
      "learning_rate": 7.394393094492399e-06,
      "loss": 1.7086,
      "step": 669300
    },
    {
      "epoch": 51.13436712245054,
      "grad_norm": 7.391408443450928,
      "learning_rate": 7.3880273979578846e-06,
      "loss": 1.6777,
      "step": 669400
    },
    {
      "epoch": 51.14200595829195,
      "grad_norm": 6.664477825164795,
      "learning_rate": 7.38166170142337e-06,
      "loss": 1.6511,
      "step": 669500
    },
    {
      "epoch": 51.14964479413337,
      "grad_norm": 5.1227898597717285,
      "learning_rate": 7.375296004888856e-06,
      "loss": 1.6832,
      "step": 669600
    },
    {
      "epoch": 51.15728362997479,
      "grad_norm": 5.852409362792969,
      "learning_rate": 7.368930308354341e-06,
      "loss": 1.6589,
      "step": 669700
    },
    {
      "epoch": 51.16492246581621,
      "grad_norm": 6.816143035888672,
      "learning_rate": 7.362564611819826e-06,
      "loss": 1.7318,
      "step": 669800
    },
    {
      "epoch": 51.17256130165763,
      "grad_norm": 7.698111534118652,
      "learning_rate": 7.356198915285311e-06,
      "loss": 1.5546,
      "step": 669900
    },
    {
      "epoch": 51.18020013749904,
      "grad_norm": 5.669102191925049,
      "learning_rate": 7.349833218750796e-06,
      "loss": 1.7137,
      "step": 670000
    },
    {
      "epoch": 51.18783897334046,
      "grad_norm": 5.822607517242432,
      "learning_rate": 7.343467522216281e-06,
      "loss": 1.6916,
      "step": 670100
    },
    {
      "epoch": 51.19547780918188,
      "grad_norm": 6.8082098960876465,
      "learning_rate": 7.337101825681766e-06,
      "loss": 1.6823,
      "step": 670200
    },
    {
      "epoch": 51.2031166450233,
      "grad_norm": 6.644167900085449,
      "learning_rate": 7.330736129147252e-06,
      "loss": 1.6854,
      "step": 670300
    },
    {
      "epoch": 51.21075548086472,
      "grad_norm": 8.137953758239746,
      "learning_rate": 7.324370432612737e-06,
      "loss": 1.6259,
      "step": 670400
    },
    {
      "epoch": 51.21839431670613,
      "grad_norm": 7.467419624328613,
      "learning_rate": 7.318004736078222e-06,
      "loss": 1.6428,
      "step": 670500
    },
    {
      "epoch": 51.22603315254755,
      "grad_norm": 6.118802070617676,
      "learning_rate": 7.311639039543708e-06,
      "loss": 1.6762,
      "step": 670600
    },
    {
      "epoch": 51.23367198838897,
      "grad_norm": 4.731357097625732,
      "learning_rate": 7.305273343009192e-06,
      "loss": 1.6151,
      "step": 670700
    },
    {
      "epoch": 51.24131082423039,
      "grad_norm": 5.5948896408081055,
      "learning_rate": 7.298907646474677e-06,
      "loss": 1.6757,
      "step": 670800
    },
    {
      "epoch": 51.24894966007181,
      "grad_norm": 5.300180435180664,
      "learning_rate": 7.292541949940162e-06,
      "loss": 1.6265,
      "step": 670900
    },
    {
      "epoch": 51.25658849591322,
      "grad_norm": 4.706467628479004,
      "learning_rate": 7.286176253405648e-06,
      "loss": 1.6861,
      "step": 671000
    },
    {
      "epoch": 51.26422733175464,
      "grad_norm": 6.880863189697266,
      "learning_rate": 7.2798105568711335e-06,
      "loss": 1.6596,
      "step": 671100
    },
    {
      "epoch": 51.27186616759606,
      "grad_norm": 7.722949504852295,
      "learning_rate": 7.273444860336619e-06,
      "loss": 1.7263,
      "step": 671200
    },
    {
      "epoch": 51.27950500343748,
      "grad_norm": 6.257556915283203,
      "learning_rate": 7.267079163802104e-06,
      "loss": 1.6837,
      "step": 671300
    },
    {
      "epoch": 51.2871438392789,
      "grad_norm": 6.230011463165283,
      "learning_rate": 7.260713467267589e-06,
      "loss": 1.7403,
      "step": 671400
    },
    {
      "epoch": 51.29478267512031,
      "grad_norm": 6.59585428237915,
      "learning_rate": 7.2543477707330736e-06,
      "loss": 1.7179,
      "step": 671500
    },
    {
      "epoch": 51.30242151096173,
      "grad_norm": 10.450965881347656,
      "learning_rate": 7.247982074198559e-06,
      "loss": 1.6993,
      "step": 671600
    },
    {
      "epoch": 51.31006034680315,
      "grad_norm": 5.440919399261475,
      "learning_rate": 7.241616377664044e-06,
      "loss": 1.6417,
      "step": 671700
    },
    {
      "epoch": 51.31769918264457,
      "grad_norm": 5.80851411819458,
      "learning_rate": 7.235250681129529e-06,
      "loss": 1.6458,
      "step": 671800
    },
    {
      "epoch": 51.32533801848598,
      "grad_norm": 4.9510817527771,
      "learning_rate": 7.2288849845950145e-06,
      "loss": 1.6163,
      "step": 671900
    },
    {
      "epoch": 51.3329768543274,
      "grad_norm": 5.0735650062561035,
      "learning_rate": 7.2225192880605006e-06,
      "loss": 1.6771,
      "step": 672000
    },
    {
      "epoch": 51.34061569016882,
      "grad_norm": 4.512588024139404,
      "learning_rate": 7.216153591525986e-06,
      "loss": 1.5498,
      "step": 672100
    },
    {
      "epoch": 51.34825452601024,
      "grad_norm": 5.521089553833008,
      "learning_rate": 7.209787894991471e-06,
      "loss": 1.6632,
      "step": 672200
    },
    {
      "epoch": 51.355893361851656,
      "grad_norm": 5.042962551116943,
      "learning_rate": 7.2034221984569546e-06,
      "loss": 1.6623,
      "step": 672300
    },
    {
      "epoch": 51.36353219769307,
      "grad_norm": 6.142600059509277,
      "learning_rate": 7.197056501922441e-06,
      "loss": 1.6308,
      "step": 672400
    },
    {
      "epoch": 51.37117103353449,
      "grad_norm": 6.5862507820129395,
      "learning_rate": 7.190690805387926e-06,
      "loss": 1.6477,
      "step": 672500
    },
    {
      "epoch": 51.37880986937591,
      "grad_norm": 3.7190229892730713,
      "learning_rate": 7.184325108853411e-06,
      "loss": 1.6636,
      "step": 672600
    },
    {
      "epoch": 51.386448705217326,
      "grad_norm": 6.943399906158447,
      "learning_rate": 7.177959412318896e-06,
      "loss": 1.6024,
      "step": 672700
    },
    {
      "epoch": 51.394087541058745,
      "grad_norm": 5.045365333557129,
      "learning_rate": 7.1715937157843816e-06,
      "loss": 1.6969,
      "step": 672800
    },
    {
      "epoch": 51.40172637690016,
      "grad_norm": 5.183690547943115,
      "learning_rate": 7.165228019249867e-06,
      "loss": 1.7243,
      "step": 672900
    },
    {
      "epoch": 51.40936521274158,
      "grad_norm": 6.573738098144531,
      "learning_rate": 7.158862322715351e-06,
      "loss": 1.7071,
      "step": 673000
    },
    {
      "epoch": 51.417004048582996,
      "grad_norm": 6.303290843963623,
      "learning_rate": 7.152496626180836e-06,
      "loss": 1.6213,
      "step": 673100
    },
    {
      "epoch": 51.424642884424415,
      "grad_norm": 6.170448303222656,
      "learning_rate": 7.146130929646322e-06,
      "loss": 1.6554,
      "step": 673200
    },
    {
      "epoch": 51.432281720265834,
      "grad_norm": 5.3869524002075195,
      "learning_rate": 7.139765233111807e-06,
      "loss": 1.8387,
      "step": 673300
    },
    {
      "epoch": 51.439920556107246,
      "grad_norm": 5.341297626495361,
      "learning_rate": 7.133399536577293e-06,
      "loss": 1.6662,
      "step": 673400
    },
    {
      "epoch": 51.447559391948666,
      "grad_norm": 5.532390117645264,
      "learning_rate": 7.127033840042778e-06,
      "loss": 1.6026,
      "step": 673500
    },
    {
      "epoch": 51.455198227790085,
      "grad_norm": 5.669171333312988,
      "learning_rate": 7.120668143508263e-06,
      "loss": 1.7521,
      "step": 673600
    },
    {
      "epoch": 51.462837063631504,
      "grad_norm": 9.271395683288574,
      "learning_rate": 7.114302446973749e-06,
      "loss": 1.7016,
      "step": 673700
    },
    {
      "epoch": 51.47047589947292,
      "grad_norm": 5.449870586395264,
      "learning_rate": 7.107936750439233e-06,
      "loss": 1.626,
      "step": 673800
    },
    {
      "epoch": 51.478114735314335,
      "grad_norm": 5.75205135345459,
      "learning_rate": 7.101571053904718e-06,
      "loss": 1.737,
      "step": 673900
    },
    {
      "epoch": 51.485753571155755,
      "grad_norm": 4.304020881652832,
      "learning_rate": 7.0952053573702035e-06,
      "loss": 1.6497,
      "step": 674000
    },
    {
      "epoch": 51.493392406997174,
      "grad_norm": 4.500977993011475,
      "learning_rate": 7.088839660835689e-06,
      "loss": 1.7432,
      "step": 674100
    },
    {
      "epoch": 51.50103124283859,
      "grad_norm": 6.264891624450684,
      "learning_rate": 7.082473964301174e-06,
      "loss": 1.6833,
      "step": 674200
    },
    {
      "epoch": 51.50867007868001,
      "grad_norm": 4.848044395446777,
      "learning_rate": 7.076108267766659e-06,
      "loss": 1.7348,
      "step": 674300
    },
    {
      "epoch": 51.516308914521424,
      "grad_norm": 3.785609483718872,
      "learning_rate": 7.069742571232145e-06,
      "loss": 1.7155,
      "step": 674400
    },
    {
      "epoch": 51.523947750362844,
      "grad_norm": 5.752864837646484,
      "learning_rate": 7.0633768746976305e-06,
      "loss": 1.6573,
      "step": 674500
    },
    {
      "epoch": 51.53158658620426,
      "grad_norm": 5.266960144042969,
      "learning_rate": 7.057011178163114e-06,
      "loss": 1.7285,
      "step": 674600
    },
    {
      "epoch": 51.53922542204568,
      "grad_norm": 6.136749267578125,
      "learning_rate": 7.050645481628599e-06,
      "loss": 1.7156,
      "step": 674700
    },
    {
      "epoch": 51.5468642578871,
      "grad_norm": 3.727055549621582,
      "learning_rate": 7.044279785094085e-06,
      "loss": 1.7082,
      "step": 674800
    },
    {
      "epoch": 51.55450309372851,
      "grad_norm": 5.586435317993164,
      "learning_rate": 7.0379140885595706e-06,
      "loss": 1.6646,
      "step": 674900
    },
    {
      "epoch": 51.56214192956993,
      "grad_norm": 4.888583660125732,
      "learning_rate": 7.031548392025056e-06,
      "loss": 1.7293,
      "step": 675000
    },
    {
      "epoch": 51.56978076541135,
      "grad_norm": 4.9046525955200195,
      "learning_rate": 7.025182695490541e-06,
      "loss": 1.7296,
      "step": 675100
    },
    {
      "epoch": 51.57741960125277,
      "grad_norm": 5.284517765045166,
      "learning_rate": 7.018816998956026e-06,
      "loss": 1.7196,
      "step": 675200
    },
    {
      "epoch": 51.58505843709419,
      "grad_norm": 5.811095714569092,
      "learning_rate": 7.0124513024215115e-06,
      "loss": 1.6188,
      "step": 675300
    },
    {
      "epoch": 51.5926972729356,
      "grad_norm": 5.4890289306640625,
      "learning_rate": 7.006085605886996e-06,
      "loss": 1.6684,
      "step": 675400
    },
    {
      "epoch": 51.60033610877702,
      "grad_norm": 7.3102898597717285,
      "learning_rate": 6.999719909352481e-06,
      "loss": 1.69,
      "step": 675500
    },
    {
      "epoch": 51.60797494461844,
      "grad_norm": 7.131279945373535,
      "learning_rate": 6.993354212817966e-06,
      "loss": 1.6753,
      "step": 675600
    },
    {
      "epoch": 51.61561378045986,
      "grad_norm": 5.722874641418457,
      "learning_rate": 6.9869885162834516e-06,
      "loss": 1.6741,
      "step": 675700
    },
    {
      "epoch": 51.62325261630127,
      "grad_norm": 6.700299263000488,
      "learning_rate": 6.980622819748938e-06,
      "loss": 1.6834,
      "step": 675800
    },
    {
      "epoch": 51.63089145214269,
      "grad_norm": 4.981940746307373,
      "learning_rate": 6.974257123214423e-06,
      "loss": 1.7157,
      "step": 675900
    },
    {
      "epoch": 51.63853028798411,
      "grad_norm": 4.109084606170654,
      "learning_rate": 6.967891426679908e-06,
      "loss": 1.6508,
      "step": 676000
    },
    {
      "epoch": 51.64616912382553,
      "grad_norm": 6.9126362800598145,
      "learning_rate": 6.961525730145393e-06,
      "loss": 1.6223,
      "step": 676100
    },
    {
      "epoch": 51.65380795966695,
      "grad_norm": 6.183560371398926,
      "learning_rate": 6.955160033610878e-06,
      "loss": 1.6777,
      "step": 676200
    },
    {
      "epoch": 51.66144679550836,
      "grad_norm": 4.462488174438477,
      "learning_rate": 6.948794337076363e-06,
      "loss": 1.6553,
      "step": 676300
    },
    {
      "epoch": 51.66908563134978,
      "grad_norm": 5.2922682762146,
      "learning_rate": 6.942428640541848e-06,
      "loss": 1.7386,
      "step": 676400
    },
    {
      "epoch": 51.6767244671912,
      "grad_norm": 4.8211750984191895,
      "learning_rate": 6.936062944007333e-06,
      "loss": 1.6507,
      "step": 676500
    },
    {
      "epoch": 51.68436330303262,
      "grad_norm": 4.607859134674072,
      "learning_rate": 6.929697247472819e-06,
      "loss": 1.7112,
      "step": 676600
    },
    {
      "epoch": 51.69200213887404,
      "grad_norm": 10.551873207092285,
      "learning_rate": 6.923331550938304e-06,
      "loss": 1.7018,
      "step": 676700
    },
    {
      "epoch": 51.69964097471545,
      "grad_norm": 6.245595455169678,
      "learning_rate": 6.91696585440379e-06,
      "loss": 1.6528,
      "step": 676800
    },
    {
      "epoch": 51.70727981055687,
      "grad_norm": 7.053083419799805,
      "learning_rate": 6.910600157869275e-06,
      "loss": 1.6847,
      "step": 676900
    },
    {
      "epoch": 51.71491864639829,
      "grad_norm": 6.934109210968018,
      "learning_rate": 6.904234461334759e-06,
      "loss": 1.6663,
      "step": 677000
    },
    {
      "epoch": 51.72255748223971,
      "grad_norm": 6.605268955230713,
      "learning_rate": 6.897868764800244e-06,
      "loss": 1.6428,
      "step": 677100
    },
    {
      "epoch": 51.73019631808113,
      "grad_norm": 5.190280437469482,
      "learning_rate": 6.89150306826573e-06,
      "loss": 1.6628,
      "step": 677200
    },
    {
      "epoch": 51.73783515392254,
      "grad_norm": 6.191761016845703,
      "learning_rate": 6.885137371731215e-06,
      "loss": 1.661,
      "step": 677300
    },
    {
      "epoch": 51.74547398976396,
      "grad_norm": 5.6177167892456055,
      "learning_rate": 6.8787716751967005e-06,
      "loss": 1.714,
      "step": 677400
    },
    {
      "epoch": 51.75311282560538,
      "grad_norm": 5.561858654022217,
      "learning_rate": 6.872405978662186e-06,
      "loss": 1.6969,
      "step": 677500
    },
    {
      "epoch": 51.7607516614468,
      "grad_norm": 5.381412029266357,
      "learning_rate": 6.866040282127671e-06,
      "loss": 1.6122,
      "step": 677600
    },
    {
      "epoch": 51.768390497288216,
      "grad_norm": 2.3730628490448,
      "learning_rate": 6.859674585593156e-06,
      "loss": 1.6251,
      "step": 677700
    },
    {
      "epoch": 51.77602933312963,
      "grad_norm": 6.737569808959961,
      "learning_rate": 6.8533088890586406e-06,
      "loss": 1.6601,
      "step": 677800
    },
    {
      "epoch": 51.78366816897105,
      "grad_norm": 6.353808879852295,
      "learning_rate": 6.846943192524126e-06,
      "loss": 1.6634,
      "step": 677900
    },
    {
      "epoch": 51.79130700481247,
      "grad_norm": 4.896389961242676,
      "learning_rate": 6.840577495989611e-06,
      "loss": 1.6841,
      "step": 678000
    },
    {
      "epoch": 51.798945840653886,
      "grad_norm": 4.831046104431152,
      "learning_rate": 6.834211799455096e-06,
      "loss": 1.6361,
      "step": 678100
    },
    {
      "epoch": 51.806584676495305,
      "grad_norm": 6.091353893280029,
      "learning_rate": 6.827846102920582e-06,
      "loss": 1.5201,
      "step": 678200
    },
    {
      "epoch": 51.81422351233672,
      "grad_norm": 4.424503803253174,
      "learning_rate": 6.8214804063860676e-06,
      "loss": 1.6798,
      "step": 678300
    },
    {
      "epoch": 51.821862348178136,
      "grad_norm": 5.036162376403809,
      "learning_rate": 6.815114709851553e-06,
      "loss": 1.7129,
      "step": 678400
    },
    {
      "epoch": 51.829501184019556,
      "grad_norm": 4.509875774383545,
      "learning_rate": 6.808749013317038e-06,
      "loss": 1.7159,
      "step": 678500
    },
    {
      "epoch": 51.837140019860975,
      "grad_norm": 4.236762046813965,
      "learning_rate": 6.802383316782522e-06,
      "loss": 1.6374,
      "step": 678600
    },
    {
      "epoch": 51.844778855702394,
      "grad_norm": 5.341888904571533,
      "learning_rate": 6.796017620248008e-06,
      "loss": 1.5937,
      "step": 678700
    },
    {
      "epoch": 51.852417691543806,
      "grad_norm": 3.919027328491211,
      "learning_rate": 6.789651923713493e-06,
      "loss": 1.6312,
      "step": 678800
    },
    {
      "epoch": 51.860056527385225,
      "grad_norm": 6.830878257751465,
      "learning_rate": 6.783286227178978e-06,
      "loss": 1.5955,
      "step": 678900
    },
    {
      "epoch": 51.867695363226645,
      "grad_norm": 5.884898662567139,
      "learning_rate": 6.776920530644463e-06,
      "loss": 1.6572,
      "step": 679000
    },
    {
      "epoch": 51.875334199068064,
      "grad_norm": 5.529935836791992,
      "learning_rate": 6.7705548341099486e-06,
      "loss": 1.7417,
      "step": 679100
    },
    {
      "epoch": 51.88297303490948,
      "grad_norm": 6.53312873840332,
      "learning_rate": 6.764189137575435e-06,
      "loss": 1.7179,
      "step": 679200
    },
    {
      "epoch": 51.890611870750895,
      "grad_norm": 5.593925952911377,
      "learning_rate": 6.75782344104092e-06,
      "loss": 1.6193,
      "step": 679300
    },
    {
      "epoch": 51.898250706592314,
      "grad_norm": 4.638153553009033,
      "learning_rate": 6.751457744506403e-06,
      "loss": 1.5829,
      "step": 679400
    },
    {
      "epoch": 51.905889542433734,
      "grad_norm": 7.393383502960205,
      "learning_rate": 6.745092047971889e-06,
      "loss": 1.6514,
      "step": 679500
    },
    {
      "epoch": 51.91352837827515,
      "grad_norm": 4.885377407073975,
      "learning_rate": 6.738726351437375e-06,
      "loss": 1.6364,
      "step": 679600
    },
    {
      "epoch": 51.92116721411657,
      "grad_norm": 5.185352325439453,
      "learning_rate": 6.73236065490286e-06,
      "loss": 1.6805,
      "step": 679700
    },
    {
      "epoch": 51.928806049957984,
      "grad_norm": 6.705924987792969,
      "learning_rate": 6.725994958368345e-06,
      "loss": 1.6111,
      "step": 679800
    },
    {
      "epoch": 51.9364448857994,
      "grad_norm": 4.613678932189941,
      "learning_rate": 6.71962926183383e-06,
      "loss": 1.6248,
      "step": 679900
    },
    {
      "epoch": 51.94408372164082,
      "grad_norm": 6.348213195800781,
      "learning_rate": 6.713263565299316e-06,
      "loss": 1.6227,
      "step": 680000
    },
    {
      "epoch": 51.95172255748224,
      "grad_norm": 3.0541398525238037,
      "learning_rate": 6.706897868764801e-06,
      "loss": 1.7004,
      "step": 680100
    },
    {
      "epoch": 51.959361393323654,
      "grad_norm": 4.8856000900268555,
      "learning_rate": 6.700532172230285e-06,
      "loss": 1.6616,
      "step": 680200
    },
    {
      "epoch": 51.96700022916507,
      "grad_norm": 6.25784969329834,
      "learning_rate": 6.6941664756957705e-06,
      "loss": 1.7067,
      "step": 680300
    },
    {
      "epoch": 51.97463906500649,
      "grad_norm": 5.404641151428223,
      "learning_rate": 6.687800779161256e-06,
      "loss": 1.684,
      "step": 680400
    },
    {
      "epoch": 51.98227790084791,
      "grad_norm": 6.275862216949463,
      "learning_rate": 6.681435082626741e-06,
      "loss": 1.5907,
      "step": 680500
    },
    {
      "epoch": 51.98991673668933,
      "grad_norm": 5.189094543457031,
      "learning_rate": 6.675069386092227e-06,
      "loss": 1.6575,
      "step": 680600
    },
    {
      "epoch": 51.99755557253074,
      "grad_norm": 6.161994934082031,
      "learning_rate": 6.668703689557712e-06,
      "loss": 1.654,
      "step": 680700
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.7679051160812378,
      "eval_runtime": 3.0745,
      "eval_samples_per_second": 224.428,
      "eval_steps_per_second": 224.428,
      "step": 680732
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.426196575164795,
      "eval_runtime": 57.3477,
      "eval_samples_per_second": 228.274,
      "eval_steps_per_second": 228.274,
      "step": 680732
    },
    {
      "epoch": 52.00519440837216,
      "grad_norm": 5.645890235900879,
      "learning_rate": 6.6623379930231975e-06,
      "loss": 1.6392,
      "step": 680800
    },
    {
      "epoch": 52.01283324421358,
      "grad_norm": 5.993499279022217,
      "learning_rate": 6.655972296488681e-06,
      "loss": 1.74,
      "step": 680900
    },
    {
      "epoch": 52.020472080055,
      "grad_norm": 6.641808986663818,
      "learning_rate": 6.649606599954167e-06,
      "loss": 1.6057,
      "step": 681000
    },
    {
      "epoch": 52.02811091589642,
      "grad_norm": 7.223964214324951,
      "learning_rate": 6.643240903419652e-06,
      "loss": 1.6653,
      "step": 681100
    },
    {
      "epoch": 52.03574975173783,
      "grad_norm": 4.9288129806518555,
      "learning_rate": 6.6368752068851376e-06,
      "loss": 1.6358,
      "step": 681200
    },
    {
      "epoch": 52.04338858757925,
      "grad_norm": 5.7113776206970215,
      "learning_rate": 6.630509510350623e-06,
      "loss": 1.6715,
      "step": 681300
    },
    {
      "epoch": 52.05102742342067,
      "grad_norm": 5.121915817260742,
      "learning_rate": 6.624143813816108e-06,
      "loss": 1.607,
      "step": 681400
    },
    {
      "epoch": 52.05866625926209,
      "grad_norm": 7.0187859535217285,
      "learning_rate": 6.617778117281593e-06,
      "loss": 1.8201,
      "step": 681500
    },
    {
      "epoch": 52.06630509510351,
      "grad_norm": 5.110859394073486,
      "learning_rate": 6.611412420747079e-06,
      "loss": 1.713,
      "step": 681600
    },
    {
      "epoch": 52.07394393094492,
      "grad_norm": 4.899158954620361,
      "learning_rate": 6.605046724212563e-06,
      "loss": 1.5483,
      "step": 681700
    },
    {
      "epoch": 52.08158276678634,
      "grad_norm": 4.899007320404053,
      "learning_rate": 6.598681027678048e-06,
      "loss": 1.5897,
      "step": 681800
    },
    {
      "epoch": 52.08922160262776,
      "grad_norm": 6.264943599700928,
      "learning_rate": 6.592315331143533e-06,
      "loss": 1.6352,
      "step": 681900
    },
    {
      "epoch": 52.09686043846918,
      "grad_norm": 5.788131237030029,
      "learning_rate": 6.585949634609019e-06,
      "loss": 1.7293,
      "step": 682000
    },
    {
      "epoch": 52.1044992743106,
      "grad_norm": 4.823431491851807,
      "learning_rate": 6.579583938074505e-06,
      "loss": 1.6441,
      "step": 682100
    },
    {
      "epoch": 52.11213811015201,
      "grad_norm": 6.554244518280029,
      "learning_rate": 6.57321824153999e-06,
      "loss": 1.6525,
      "step": 682200
    },
    {
      "epoch": 52.11977694599343,
      "grad_norm": 6.192212104797363,
      "learning_rate": 6.566852545005475e-06,
      "loss": 1.613,
      "step": 682300
    },
    {
      "epoch": 52.12741578183485,
      "grad_norm": 8.354736328125,
      "learning_rate": 6.56048684847096e-06,
      "loss": 1.675,
      "step": 682400
    },
    {
      "epoch": 52.13505461767627,
      "grad_norm": 4.641676902770996,
      "learning_rate": 6.554121151936445e-06,
      "loss": 1.7275,
      "step": 682500
    },
    {
      "epoch": 52.14269345351769,
      "grad_norm": 5.656129360198975,
      "learning_rate": 6.54775545540193e-06,
      "loss": 1.6829,
      "step": 682600
    },
    {
      "epoch": 52.1503322893591,
      "grad_norm": 5.25619649887085,
      "learning_rate": 6.541389758867415e-06,
      "loss": 1.5742,
      "step": 682700
    },
    {
      "epoch": 52.15797112520052,
      "grad_norm": 4.848607063293457,
      "learning_rate": 6.5350240623329e-06,
      "loss": 1.7442,
      "step": 682800
    },
    {
      "epoch": 52.16560996104194,
      "grad_norm": 5.4767842292785645,
      "learning_rate": 6.528658365798386e-06,
      "loss": 1.7413,
      "step": 682900
    },
    {
      "epoch": 52.17324879688336,
      "grad_norm": 6.618401050567627,
      "learning_rate": 6.522292669263872e-06,
      "loss": 1.6134,
      "step": 683000
    },
    {
      "epoch": 52.180887632724776,
      "grad_norm": 5.260737419128418,
      "learning_rate": 6.515926972729357e-06,
      "loss": 1.683,
      "step": 683100
    },
    {
      "epoch": 52.18852646856619,
      "grad_norm": 8.103092193603516,
      "learning_rate": 6.509561276194842e-06,
      "loss": 1.7593,
      "step": 683200
    },
    {
      "epoch": 52.19616530440761,
      "grad_norm": 7.360860824584961,
      "learning_rate": 6.503195579660326e-06,
      "loss": 1.6779,
      "step": 683300
    },
    {
      "epoch": 52.20380414024903,
      "grad_norm": 3.3998830318450928,
      "learning_rate": 6.496829883125812e-06,
      "loss": 1.7156,
      "step": 683400
    },
    {
      "epoch": 52.211442976090446,
      "grad_norm": 4.057453632354736,
      "learning_rate": 6.490464186591297e-06,
      "loss": 1.6122,
      "step": 683500
    },
    {
      "epoch": 52.219081811931865,
      "grad_norm": 6.795844554901123,
      "learning_rate": 6.484098490056782e-06,
      "loss": 1.5961,
      "step": 683600
    },
    {
      "epoch": 52.22672064777328,
      "grad_norm": 5.239668369293213,
      "learning_rate": 6.4777327935222675e-06,
      "loss": 1.5561,
      "step": 683700
    },
    {
      "epoch": 52.234359483614696,
      "grad_norm": 7.03793478012085,
      "learning_rate": 6.471367096987753e-06,
      "loss": 1.6188,
      "step": 683800
    },
    {
      "epoch": 52.241998319456115,
      "grad_norm": 5.961772441864014,
      "learning_rate": 6.465001400453238e-06,
      "loss": 1.7386,
      "step": 683900
    },
    {
      "epoch": 52.249637155297535,
      "grad_norm": 3.9924004077911377,
      "learning_rate": 6.458635703918724e-06,
      "loss": 1.6705,
      "step": 684000
    },
    {
      "epoch": 52.257275991138954,
      "grad_norm": 8.039294242858887,
      "learning_rate": 6.4522700073842076e-06,
      "loss": 1.6627,
      "step": 684100
    },
    {
      "epoch": 52.264914826980366,
      "grad_norm": 6.9546613693237305,
      "learning_rate": 6.445904310849693e-06,
      "loss": 1.6928,
      "step": 684200
    },
    {
      "epoch": 52.272553662821785,
      "grad_norm": 5.125645160675049,
      "learning_rate": 6.439538614315178e-06,
      "loss": 1.6646,
      "step": 684300
    },
    {
      "epoch": 52.280192498663205,
      "grad_norm": 7.3009514808654785,
      "learning_rate": 6.433172917780664e-06,
      "loss": 1.6793,
      "step": 684400
    },
    {
      "epoch": 52.287831334504624,
      "grad_norm": 5.352451801300049,
      "learning_rate": 6.426807221246149e-06,
      "loss": 1.7191,
      "step": 684500
    },
    {
      "epoch": 52.295470170346036,
      "grad_norm": 5.280102252960205,
      "learning_rate": 6.4204415247116346e-06,
      "loss": 1.6545,
      "step": 684600
    },
    {
      "epoch": 52.303109006187455,
      "grad_norm": 5.328249454498291,
      "learning_rate": 6.41407582817712e-06,
      "loss": 1.6911,
      "step": 684700
    },
    {
      "epoch": 52.310747842028874,
      "grad_norm": 6.190421104431152,
      "learning_rate": 6.407710131642605e-06,
      "loss": 1.6575,
      "step": 684800
    },
    {
      "epoch": 52.31838667787029,
      "grad_norm": 6.136380195617676,
      "learning_rate": 6.401344435108089e-06,
      "loss": 1.6732,
      "step": 684900
    },
    {
      "epoch": 52.32602551371171,
      "grad_norm": 6.725803375244141,
      "learning_rate": 6.394978738573575e-06,
      "loss": 1.6697,
      "step": 685000
    },
    {
      "epoch": 52.333664349553125,
      "grad_norm": 4.280093193054199,
      "learning_rate": 6.38861304203906e-06,
      "loss": 1.7128,
      "step": 685100
    },
    {
      "epoch": 52.341303185394544,
      "grad_norm": 6.904796123504639,
      "learning_rate": 6.382247345504545e-06,
      "loss": 1.6828,
      "step": 685200
    },
    {
      "epoch": 52.34894202123596,
      "grad_norm": 6.283822536468506,
      "learning_rate": 6.37588164897003e-06,
      "loss": 1.7178,
      "step": 685300
    },
    {
      "epoch": 52.35658085707738,
      "grad_norm": 4.8696441650390625,
      "learning_rate": 6.369515952435516e-06,
      "loss": 1.6489,
      "step": 685400
    },
    {
      "epoch": 52.3642196929188,
      "grad_norm": 5.849899768829346,
      "learning_rate": 6.363150255901002e-06,
      "loss": 1.6724,
      "step": 685500
    },
    {
      "epoch": 52.371858528760214,
      "grad_norm": 6.366905689239502,
      "learning_rate": 6.356784559366487e-06,
      "loss": 1.628,
      "step": 685600
    },
    {
      "epoch": 52.37949736460163,
      "grad_norm": 5.694220066070557,
      "learning_rate": 6.35041886283197e-06,
      "loss": 1.7111,
      "step": 685700
    },
    {
      "epoch": 52.38713620044305,
      "grad_norm": 6.076289176940918,
      "learning_rate": 6.3440531662974565e-06,
      "loss": 1.6895,
      "step": 685800
    },
    {
      "epoch": 52.39477503628447,
      "grad_norm": 6.039805889129639,
      "learning_rate": 6.337687469762942e-06,
      "loss": 1.5594,
      "step": 685900
    },
    {
      "epoch": 52.40241387212589,
      "grad_norm": 4.93049955368042,
      "learning_rate": 6.331321773228427e-06,
      "loss": 1.647,
      "step": 686000
    },
    {
      "epoch": 52.4100527079673,
      "grad_norm": 5.262991905212402,
      "learning_rate": 6.324956076693912e-06,
      "loss": 1.6621,
      "step": 686100
    },
    {
      "epoch": 52.41769154380872,
      "grad_norm": 5.210890293121338,
      "learning_rate": 6.318590380159397e-06,
      "loss": 1.6264,
      "step": 686200
    },
    {
      "epoch": 52.42533037965014,
      "grad_norm": 7.623476505279541,
      "learning_rate": 6.312224683624883e-06,
      "loss": 1.6147,
      "step": 686300
    },
    {
      "epoch": 52.43296921549156,
      "grad_norm": 5.061221122741699,
      "learning_rate": 6.305858987090369e-06,
      "loss": 1.6405,
      "step": 686400
    },
    {
      "epoch": 52.44060805133298,
      "grad_norm": 4.780667781829834,
      "learning_rate": 6.299493290555852e-06,
      "loss": 1.6642,
      "step": 686500
    },
    {
      "epoch": 52.44824688717439,
      "grad_norm": 7.674021244049072,
      "learning_rate": 6.2931275940213375e-06,
      "loss": 1.7377,
      "step": 686600
    },
    {
      "epoch": 52.45588572301581,
      "grad_norm": 6.418912887573242,
      "learning_rate": 6.286761897486823e-06,
      "loss": 1.7108,
      "step": 686700
    },
    {
      "epoch": 52.46352455885723,
      "grad_norm": 7.60066556930542,
      "learning_rate": 6.280396200952309e-06,
      "loss": 1.7367,
      "step": 686800
    },
    {
      "epoch": 52.47116339469865,
      "grad_norm": 6.141528606414795,
      "learning_rate": 6.274030504417794e-06,
      "loss": 1.6578,
      "step": 686900
    },
    {
      "epoch": 52.47880223054007,
      "grad_norm": 6.773213863372803,
      "learning_rate": 6.267664807883279e-06,
      "loss": 1.7597,
      "step": 687000
    },
    {
      "epoch": 52.48644106638148,
      "grad_norm": 6.186466217041016,
      "learning_rate": 6.2612991113487645e-06,
      "loss": 1.6632,
      "step": 687100
    },
    {
      "epoch": 52.4940799022229,
      "grad_norm": 5.5622687339782715,
      "learning_rate": 6.25493341481425e-06,
      "loss": 1.676,
      "step": 687200
    },
    {
      "epoch": 52.50171873806432,
      "grad_norm": 7.199466705322266,
      "learning_rate": 6.248567718279735e-06,
      "loss": 1.6336,
      "step": 687300
    },
    {
      "epoch": 52.50935757390574,
      "grad_norm": 4.330127716064453,
      "learning_rate": 6.24220202174522e-06,
      "loss": 1.7051,
      "step": 687400
    },
    {
      "epoch": 52.51699640974716,
      "grad_norm": 4.776598930358887,
      "learning_rate": 6.2358363252107046e-06,
      "loss": 1.6281,
      "step": 687500
    },
    {
      "epoch": 52.52463524558857,
      "grad_norm": 6.713372707366943,
      "learning_rate": 6.22947062867619e-06,
      "loss": 1.6095,
      "step": 687600
    },
    {
      "epoch": 52.53227408142999,
      "grad_norm": 6.643784999847412,
      "learning_rate": 6.223104932141675e-06,
      "loss": 1.5841,
      "step": 687700
    },
    {
      "epoch": 52.53991291727141,
      "grad_norm": 5.458785057067871,
      "learning_rate": 6.216739235607161e-06,
      "loss": 1.5666,
      "step": 687800
    },
    {
      "epoch": 52.54755175311283,
      "grad_norm": 7.783427715301514,
      "learning_rate": 6.2103735390726455e-06,
      "loss": 1.6164,
      "step": 687900
    },
    {
      "epoch": 52.55519058895425,
      "grad_norm": 5.005687713623047,
      "learning_rate": 6.204007842538131e-06,
      "loss": 1.6469,
      "step": 688000
    },
    {
      "epoch": 52.56282942479566,
      "grad_norm": 6.591892719268799,
      "learning_rate": 6.197642146003616e-06,
      "loss": 1.605,
      "step": 688100
    },
    {
      "epoch": 52.57046826063708,
      "grad_norm": 5.658939838409424,
      "learning_rate": 6.191276449469101e-06,
      "loss": 1.6689,
      "step": 688200
    },
    {
      "epoch": 52.5781070964785,
      "grad_norm": 6.341914176940918,
      "learning_rate": 6.184910752934586e-06,
      "loss": 1.6782,
      "step": 688300
    },
    {
      "epoch": 52.58574593231992,
      "grad_norm": 7.5698957443237305,
      "learning_rate": 6.178545056400072e-06,
      "loss": 1.5738,
      "step": 688400
    },
    {
      "epoch": 52.59338476816133,
      "grad_norm": 7.348686218261719,
      "learning_rate": 6.172179359865557e-06,
      "loss": 1.7179,
      "step": 688500
    },
    {
      "epoch": 52.60102360400275,
      "grad_norm": 5.696482181549072,
      "learning_rate": 6.165813663331042e-06,
      "loss": 1.6542,
      "step": 688600
    },
    {
      "epoch": 52.60866243984417,
      "grad_norm": 5.041801452636719,
      "learning_rate": 6.159447966796527e-06,
      "loss": 1.6232,
      "step": 688700
    },
    {
      "epoch": 52.616301275685586,
      "grad_norm": 5.294833660125732,
      "learning_rate": 6.1530822702620126e-06,
      "loss": 1.6579,
      "step": 688800
    },
    {
      "epoch": 52.623940111527006,
      "grad_norm": 4.9936676025390625,
      "learning_rate": 6.146716573727498e-06,
      "loss": 1.7472,
      "step": 688900
    },
    {
      "epoch": 52.63157894736842,
      "grad_norm": 5.810636520385742,
      "learning_rate": 6.140350877192982e-06,
      "loss": 1.6764,
      "step": 689000
    },
    {
      "epoch": 52.63921778320984,
      "grad_norm": 5.364856719970703,
      "learning_rate": 6.133985180658467e-06,
      "loss": 1.6472,
      "step": 689100
    },
    {
      "epoch": 52.646856619051256,
      "grad_norm": 6.856464385986328,
      "learning_rate": 6.1276194841239535e-06,
      "loss": 1.6308,
      "step": 689200
    },
    {
      "epoch": 52.654495454892675,
      "grad_norm": 6.340748310089111,
      "learning_rate": 6.121253787589439e-06,
      "loss": 1.6871,
      "step": 689300
    },
    {
      "epoch": 52.662134290734095,
      "grad_norm": 5.957707405090332,
      "learning_rate": 6.114888091054923e-06,
      "loss": 1.681,
      "step": 689400
    },
    {
      "epoch": 52.66977312657551,
      "grad_norm": 6.286112308502197,
      "learning_rate": 6.108522394520408e-06,
      "loss": 1.6328,
      "step": 689500
    },
    {
      "epoch": 52.677411962416926,
      "grad_norm": 6.6109514236450195,
      "learning_rate": 6.1021566979858936e-06,
      "loss": 1.5924,
      "step": 689600
    },
    {
      "epoch": 52.685050798258345,
      "grad_norm": 5.657927989959717,
      "learning_rate": 6.09579100145138e-06,
      "loss": 1.537,
      "step": 689700
    },
    {
      "epoch": 52.692689634099764,
      "grad_norm": 6.4826579093933105,
      "learning_rate": 6.089425304916864e-06,
      "loss": 1.6304,
      "step": 689800
    },
    {
      "epoch": 52.700328469941184,
      "grad_norm": 5.845702648162842,
      "learning_rate": 6.083059608382349e-06,
      "loss": 1.6085,
      "step": 689900
    },
    {
      "epoch": 52.707967305782596,
      "grad_norm": 5.291395664215088,
      "learning_rate": 6.0766939118478345e-06,
      "loss": 1.6772,
      "step": 690000
    },
    {
      "epoch": 52.715606141624015,
      "grad_norm": 4.269700527191162,
      "learning_rate": 6.07032821531332e-06,
      "loss": 1.6656,
      "step": 690100
    },
    {
      "epoch": 52.723244977465434,
      "grad_norm": 7.680747985839844,
      "learning_rate": 6.063962518778805e-06,
      "loss": 1.7223,
      "step": 690200
    },
    {
      "epoch": 52.73088381330685,
      "grad_norm": 8.765625,
      "learning_rate": 6.05759682224429e-06,
      "loss": 1.6926,
      "step": 690300
    },
    {
      "epoch": 52.73852264914827,
      "grad_norm": 4.254753112792969,
      "learning_rate": 6.051231125709775e-06,
      "loss": 1.5992,
      "step": 690400
    },
    {
      "epoch": 52.746161484989685,
      "grad_norm": 3.8939266204833984,
      "learning_rate": 6.044865429175261e-06,
      "loss": 1.6073,
      "step": 690500
    },
    {
      "epoch": 52.753800320831104,
      "grad_norm": 6.782395839691162,
      "learning_rate": 6.038499732640746e-06,
      "loss": 1.7455,
      "step": 690600
    },
    {
      "epoch": 52.76143915667252,
      "grad_norm": 5.8443284034729,
      "learning_rate": 6.032134036106231e-06,
      "loss": 1.6762,
      "step": 690700
    },
    {
      "epoch": 52.76907799251394,
      "grad_norm": 5.9302215576171875,
      "learning_rate": 6.025768339571716e-06,
      "loss": 1.7158,
      "step": 690800
    },
    {
      "epoch": 52.77671682835536,
      "grad_norm": 6.203545570373535,
      "learning_rate": 6.0194026430372016e-06,
      "loss": 1.7234,
      "step": 690900
    },
    {
      "epoch": 52.784355664196774,
      "grad_norm": 5.5619378089904785,
      "learning_rate": 6.013036946502686e-06,
      "loss": 1.6683,
      "step": 691000
    },
    {
      "epoch": 52.79199450003819,
      "grad_norm": 6.901512622833252,
      "learning_rate": 6.006671249968172e-06,
      "loss": 1.6833,
      "step": 691100
    },
    {
      "epoch": 52.79963333587961,
      "grad_norm": 4.842649936676025,
      "learning_rate": 6.000305553433657e-06,
      "loss": 1.6935,
      "step": 691200
    },
    {
      "epoch": 52.80727217172103,
      "grad_norm": 5.028113842010498,
      "learning_rate": 5.9939398568991425e-06,
      "loss": 1.6953,
      "step": 691300
    },
    {
      "epoch": 52.81491100756245,
      "grad_norm": 5.021380424499512,
      "learning_rate": 5.987574160364627e-06,
      "loss": 1.7428,
      "step": 691400
    },
    {
      "epoch": 52.82254984340386,
      "grad_norm": 7.4515886306762695,
      "learning_rate": 5.981208463830112e-06,
      "loss": 1.6404,
      "step": 691500
    },
    {
      "epoch": 52.83018867924528,
      "grad_norm": 5.767804145812988,
      "learning_rate": 5.974842767295598e-06,
      "loss": 1.6791,
      "step": 691600
    },
    {
      "epoch": 52.8378275150867,
      "grad_norm": 5.915269374847412,
      "learning_rate": 5.968477070761083e-06,
      "loss": 1.8112,
      "step": 691700
    },
    {
      "epoch": 52.84546635092812,
      "grad_norm": 6.259686470031738,
      "learning_rate": 5.962111374226568e-06,
      "loss": 1.6779,
      "step": 691800
    },
    {
      "epoch": 52.85310518676954,
      "grad_norm": 4.566655158996582,
      "learning_rate": 5.955745677692053e-06,
      "loss": 1.6681,
      "step": 691900
    },
    {
      "epoch": 52.86074402261095,
      "grad_norm": 6.421591281890869,
      "learning_rate": 5.949379981157538e-06,
      "loss": 1.7829,
      "step": 692000
    },
    {
      "epoch": 52.86838285845237,
      "grad_norm": 7.018477916717529,
      "learning_rate": 5.943014284623024e-06,
      "loss": 1.7033,
      "step": 692100
    },
    {
      "epoch": 52.87602169429379,
      "grad_norm": 5.5193867683410645,
      "learning_rate": 5.936648588088509e-06,
      "loss": 1.6987,
      "step": 692200
    },
    {
      "epoch": 52.88366053013521,
      "grad_norm": 6.421962738037109,
      "learning_rate": 5.930282891553994e-06,
      "loss": 1.7369,
      "step": 692300
    },
    {
      "epoch": 52.89129936597663,
      "grad_norm": 7.1382646560668945,
      "learning_rate": 5.923917195019479e-06,
      "loss": 1.6541,
      "step": 692400
    },
    {
      "epoch": 52.89893820181804,
      "grad_norm": 6.022456169128418,
      "learning_rate": 5.917551498484964e-06,
      "loss": 1.6386,
      "step": 692500
    },
    {
      "epoch": 52.90657703765946,
      "grad_norm": 5.042779922485352,
      "learning_rate": 5.91118580195045e-06,
      "loss": 1.6701,
      "step": 692600
    },
    {
      "epoch": 52.91421587350088,
      "grad_norm": 5.434022426605225,
      "learning_rate": 5.904820105415935e-06,
      "loss": 1.6327,
      "step": 692700
    },
    {
      "epoch": 52.9218547093423,
      "grad_norm": 5.967252254486084,
      "learning_rate": 5.89845440888142e-06,
      "loss": 1.6966,
      "step": 692800
    },
    {
      "epoch": 52.92949354518371,
      "grad_norm": 5.357983589172363,
      "learning_rate": 5.892088712346905e-06,
      "loss": 1.6385,
      "step": 692900
    },
    {
      "epoch": 52.93713238102513,
      "grad_norm": 6.050188064575195,
      "learning_rate": 5.8857230158123906e-06,
      "loss": 1.7031,
      "step": 693000
    },
    {
      "epoch": 52.94477121686655,
      "grad_norm": 4.771517276763916,
      "learning_rate": 5.879357319277876e-06,
      "loss": 1.5487,
      "step": 693100
    },
    {
      "epoch": 52.95241005270797,
      "grad_norm": 5.740580081939697,
      "learning_rate": 5.872991622743361e-06,
      "loss": 1.625,
      "step": 693200
    },
    {
      "epoch": 52.96004888854939,
      "grad_norm": 4.502198696136475,
      "learning_rate": 5.866625926208846e-06,
      "loss": 1.7035,
      "step": 693300
    },
    {
      "epoch": 52.9676877243908,
      "grad_norm": 4.648904323577881,
      "learning_rate": 5.860260229674331e-06,
      "loss": 1.555,
      "step": 693400
    },
    {
      "epoch": 52.97532656023222,
      "grad_norm": 9.105415344238281,
      "learning_rate": 5.853894533139817e-06,
      "loss": 1.745,
      "step": 693500
    },
    {
      "epoch": 52.98296539607364,
      "grad_norm": 6.306145668029785,
      "learning_rate": 5.847528836605302e-06,
      "loss": 1.6863,
      "step": 693600
    },
    {
      "epoch": 52.99060423191506,
      "grad_norm": 5.886040210723877,
      "learning_rate": 5.841163140070787e-06,
      "loss": 1.706,
      "step": 693700
    },
    {
      "epoch": 52.998243067756476,
      "grad_norm": 5.985062122344971,
      "learning_rate": 5.8347974435362716e-06,
      "loss": 1.5877,
      "step": 693800
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.7683583498001099,
      "eval_runtime": 3.0552,
      "eval_samples_per_second": 225.847,
      "eval_steps_per_second": 225.847,
      "step": 693823
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.4247822761535645,
      "eval_runtime": 57.7018,
      "eval_samples_per_second": 226.874,
      "eval_steps_per_second": 226.874,
      "step": 693823
    },
    {
      "epoch": 53.00588190359789,
      "grad_norm": 4.160023212432861,
      "learning_rate": 5.828431747001757e-06,
      "loss": 1.5261,
      "step": 693900
    },
    {
      "epoch": 53.01352073943931,
      "grad_norm": 6.234644412994385,
      "learning_rate": 5.822066050467243e-06,
      "loss": 1.6184,
      "step": 694000
    },
    {
      "epoch": 53.02115957528073,
      "grad_norm": 4.978917121887207,
      "learning_rate": 5.815700353932728e-06,
      "loss": 1.6033,
      "step": 694100
    },
    {
      "epoch": 53.028798411122146,
      "grad_norm": 5.525883674621582,
      "learning_rate": 5.8093346573982125e-06,
      "loss": 1.6706,
      "step": 694200
    },
    {
      "epoch": 53.036437246963565,
      "grad_norm": 4.85263729095459,
      "learning_rate": 5.802968960863698e-06,
      "loss": 1.6476,
      "step": 694300
    },
    {
      "epoch": 53.04407608280498,
      "grad_norm": 3.7375309467315674,
      "learning_rate": 5.796603264329183e-06,
      "loss": 1.7072,
      "step": 694400
    },
    {
      "epoch": 53.0517149186464,
      "grad_norm": 4.997615814208984,
      "learning_rate": 5.790237567794669e-06,
      "loss": 1.7763,
      "step": 694500
    },
    {
      "epoch": 53.059353754487816,
      "grad_norm": 5.07525634765625,
      "learning_rate": 5.783871871260153e-06,
      "loss": 1.5771,
      "step": 694600
    },
    {
      "epoch": 53.066992590329235,
      "grad_norm": 4.9594244956970215,
      "learning_rate": 5.777506174725639e-06,
      "loss": 1.6995,
      "step": 694700
    },
    {
      "epoch": 53.074631426170654,
      "grad_norm": 7.907371520996094,
      "learning_rate": 5.771140478191124e-06,
      "loss": 1.6649,
      "step": 694800
    },
    {
      "epoch": 53.08227026201207,
      "grad_norm": 5.130833148956299,
      "learning_rate": 5.764774781656609e-06,
      "loss": 1.7334,
      "step": 694900
    },
    {
      "epoch": 53.089909097853486,
      "grad_norm": 6.978509426116943,
      "learning_rate": 5.758409085122094e-06,
      "loss": 1.5801,
      "step": 695000
    },
    {
      "epoch": 53.097547933694905,
      "grad_norm": 3.9670486450195312,
      "learning_rate": 5.7520433885875796e-06,
      "loss": 1.619,
      "step": 695100
    },
    {
      "epoch": 53.105186769536324,
      "grad_norm": 6.821423053741455,
      "learning_rate": 5.745677692053065e-06,
      "loss": 1.7235,
      "step": 695200
    },
    {
      "epoch": 53.11282560537774,
      "grad_norm": 6.841426849365234,
      "learning_rate": 5.73931199551855e-06,
      "loss": 1.7551,
      "step": 695300
    },
    {
      "epoch": 53.120464441219156,
      "grad_norm": 5.781994342803955,
      "learning_rate": 5.732946298984035e-06,
      "loss": 1.7059,
      "step": 695400
    },
    {
      "epoch": 53.128103277060575,
      "grad_norm": 5.768599987030029,
      "learning_rate": 5.7265806024495205e-06,
      "loss": 1.6086,
      "step": 695500
    },
    {
      "epoch": 53.135742112901994,
      "grad_norm": 5.803096294403076,
      "learning_rate": 5.720214905915006e-06,
      "loss": 1.6614,
      "step": 695600
    },
    {
      "epoch": 53.14338094874341,
      "grad_norm": 3.7420032024383545,
      "learning_rate": 5.713849209380491e-06,
      "loss": 1.7211,
      "step": 695700
    },
    {
      "epoch": 53.15101978458483,
      "grad_norm": 4.243455410003662,
      "learning_rate": 5.707483512845975e-06,
      "loss": 1.6875,
      "step": 695800
    },
    {
      "epoch": 53.158658620426245,
      "grad_norm": 4.250053405761719,
      "learning_rate": 5.701117816311461e-06,
      "loss": 1.6043,
      "step": 695900
    },
    {
      "epoch": 53.166297456267664,
      "grad_norm": 5.570467948913574,
      "learning_rate": 5.694752119776947e-06,
      "loss": 1.8199,
      "step": 696000
    },
    {
      "epoch": 53.17393629210908,
      "grad_norm": 4.731317520141602,
      "learning_rate": 5.688386423242432e-06,
      "loss": 1.6494,
      "step": 696100
    },
    {
      "epoch": 53.1815751279505,
      "grad_norm": 5.911611080169678,
      "learning_rate": 5.682020726707916e-06,
      "loss": 1.5769,
      "step": 696200
    },
    {
      "epoch": 53.18921396379192,
      "grad_norm": 5.137813091278076,
      "learning_rate": 5.6756550301734015e-06,
      "loss": 1.6723,
      "step": 696300
    },
    {
      "epoch": 53.196852799633334,
      "grad_norm": 5.327342510223389,
      "learning_rate": 5.6692893336388876e-06,
      "loss": 1.6114,
      "step": 696400
    },
    {
      "epoch": 53.20449163547475,
      "grad_norm": 6.624953269958496,
      "learning_rate": 5.662923637104373e-06,
      "loss": 1.7551,
      "step": 696500
    },
    {
      "epoch": 53.21213047131617,
      "grad_norm": 5.343706130981445,
      "learning_rate": 5.656557940569857e-06,
      "loss": 1.6884,
      "step": 696600
    },
    {
      "epoch": 53.21976930715759,
      "grad_norm": 5.788687705993652,
      "learning_rate": 5.650192244035342e-06,
      "loss": 1.6619,
      "step": 696700
    },
    {
      "epoch": 53.22740814299901,
      "grad_norm": 5.433036804199219,
      "learning_rate": 5.643826547500828e-06,
      "loss": 1.647,
      "step": 696800
    },
    {
      "epoch": 53.23504697884042,
      "grad_norm": 5.188383102416992,
      "learning_rate": 5.637460850966314e-06,
      "loss": 1.6165,
      "step": 696900
    },
    {
      "epoch": 53.24268581468184,
      "grad_norm": 5.244858741760254,
      "learning_rate": 5.631095154431798e-06,
      "loss": 1.6392,
      "step": 697000
    },
    {
      "epoch": 53.25032465052326,
      "grad_norm": 5.490683555603027,
      "learning_rate": 5.624729457897283e-06,
      "loss": 1.6447,
      "step": 697100
    },
    {
      "epoch": 53.25796348636468,
      "grad_norm": 7.308954238891602,
      "learning_rate": 5.6183637613627686e-06,
      "loss": 1.7309,
      "step": 697200
    },
    {
      "epoch": 53.26560232220609,
      "grad_norm": 6.262969970703125,
      "learning_rate": 5.611998064828254e-06,
      "loss": 1.6411,
      "step": 697300
    },
    {
      "epoch": 53.27324115804751,
      "grad_norm": 11.452204704284668,
      "learning_rate": 5.605632368293739e-06,
      "loss": 1.6035,
      "step": 697400
    },
    {
      "epoch": 53.28087999388893,
      "grad_norm": 9.510866165161133,
      "learning_rate": 5.599266671759224e-06,
      "loss": 1.595,
      "step": 697500
    },
    {
      "epoch": 53.28851882973035,
      "grad_norm": 4.82388162612915,
      "learning_rate": 5.5929009752247095e-06,
      "loss": 1.739,
      "step": 697600
    },
    {
      "epoch": 53.29615766557177,
      "grad_norm": 4.831125259399414,
      "learning_rate": 5.586535278690194e-06,
      "loss": 1.6468,
      "step": 697700
    },
    {
      "epoch": 53.30379650141318,
      "grad_norm": 4.794821262359619,
      "learning_rate": 5.58016958215568e-06,
      "loss": 1.6366,
      "step": 697800
    },
    {
      "epoch": 53.3114353372546,
      "grad_norm": 5.80039644241333,
      "learning_rate": 5.573803885621165e-06,
      "loss": 1.6936,
      "step": 697900
    },
    {
      "epoch": 53.31907417309602,
      "grad_norm": 4.831349849700928,
      "learning_rate": 5.56743818908665e-06,
      "loss": 1.5957,
      "step": 698000
    },
    {
      "epoch": 53.32671300893744,
      "grad_norm": 5.822379112243652,
      "learning_rate": 5.561072492552135e-06,
      "loss": 1.6779,
      "step": 698100
    },
    {
      "epoch": 53.33435184477886,
      "grad_norm": 8.04554271697998,
      "learning_rate": 5.55470679601762e-06,
      "loss": 1.6506,
      "step": 698200
    },
    {
      "epoch": 53.34199068062027,
      "grad_norm": 5.017741680145264,
      "learning_rate": 5.548341099483106e-06,
      "loss": 1.6,
      "step": 698300
    },
    {
      "epoch": 53.34962951646169,
      "grad_norm": 6.287872791290283,
      "learning_rate": 5.541975402948591e-06,
      "loss": 1.6322,
      "step": 698400
    },
    {
      "epoch": 53.35726835230311,
      "grad_norm": 5.687722206115723,
      "learning_rate": 5.535609706414076e-06,
      "loss": 1.708,
      "step": 698500
    },
    {
      "epoch": 53.36490718814453,
      "grad_norm": 5.220081806182861,
      "learning_rate": 5.529244009879561e-06,
      "loss": 1.6479,
      "step": 698600
    },
    {
      "epoch": 53.37254602398595,
      "grad_norm": 5.782938480377197,
      "learning_rate": 5.522878313345046e-06,
      "loss": 1.7536,
      "step": 698700
    },
    {
      "epoch": 53.38018485982736,
      "grad_norm": 3.968456983566284,
      "learning_rate": 5.516512616810532e-06,
      "loss": 1.6107,
      "step": 698800
    },
    {
      "epoch": 53.38782369566878,
      "grad_norm": 5.318431854248047,
      "learning_rate": 5.510146920276017e-06,
      "loss": 1.611,
      "step": 698900
    },
    {
      "epoch": 53.3954625315102,
      "grad_norm": 4.933444499969482,
      "learning_rate": 5.503781223741502e-06,
      "loss": 1.5874,
      "step": 699000
    },
    {
      "epoch": 53.40310136735162,
      "grad_norm": 5.380294322967529,
      "learning_rate": 5.497415527206987e-06,
      "loss": 1.6371,
      "step": 699100
    },
    {
      "epoch": 53.410740203193036,
      "grad_norm": 5.441801071166992,
      "learning_rate": 5.491049830672472e-06,
      "loss": 1.7148,
      "step": 699200
    },
    {
      "epoch": 53.41837903903445,
      "grad_norm": 3.952193260192871,
      "learning_rate": 5.4846841341379575e-06,
      "loss": 1.6968,
      "step": 699300
    },
    {
      "epoch": 53.42601787487587,
      "grad_norm": 7.654212951660156,
      "learning_rate": 5.478318437603443e-06,
      "loss": 1.7086,
      "step": 699400
    },
    {
      "epoch": 53.43365671071729,
      "grad_norm": 4.918219566345215,
      "learning_rate": 5.471952741068928e-06,
      "loss": 1.6886,
      "step": 699500
    },
    {
      "epoch": 53.441295546558706,
      "grad_norm": 6.171566486358643,
      "learning_rate": 5.465587044534413e-06,
      "loss": 1.5854,
      "step": 699600
    },
    {
      "epoch": 53.448934382400125,
      "grad_norm": 6.650501251220703,
      "learning_rate": 5.4592213479998985e-06,
      "loss": 1.6792,
      "step": 699700
    },
    {
      "epoch": 53.45657321824154,
      "grad_norm": 6.022382736206055,
      "learning_rate": 5.452855651465384e-06,
      "loss": 1.7319,
      "step": 699800
    },
    {
      "epoch": 53.46421205408296,
      "grad_norm": 7.043496131896973,
      "learning_rate": 5.446489954930869e-06,
      "loss": 1.6954,
      "step": 699900
    },
    {
      "epoch": 53.471850889924376,
      "grad_norm": 5.5987067222595215,
      "learning_rate": 5.440124258396354e-06,
      "loss": 1.6354,
      "step": 700000
    },
    {
      "epoch": 53.479489725765795,
      "grad_norm": 4.242290019989014,
      "learning_rate": 5.4337585618618385e-06,
      "loss": 1.7439,
      "step": 700100
    },
    {
      "epoch": 53.487128561607214,
      "grad_norm": 5.128252029418945,
      "learning_rate": 5.427392865327325e-06,
      "loss": 1.6915,
      "step": 700200
    },
    {
      "epoch": 53.494767397448626,
      "grad_norm": 5.870030879974365,
      "learning_rate": 5.42102716879281e-06,
      "loss": 1.6215,
      "step": 700300
    },
    {
      "epoch": 53.502406233290046,
      "grad_norm": 6.710471153259277,
      "learning_rate": 5.414661472258295e-06,
      "loss": 1.6592,
      "step": 700400
    },
    {
      "epoch": 53.510045069131465,
      "grad_norm": 7.5249924659729,
      "learning_rate": 5.4082957757237795e-06,
      "loss": 1.7243,
      "step": 700500
    },
    {
      "epoch": 53.517683904972884,
      "grad_norm": 9.265822410583496,
      "learning_rate": 5.401930079189265e-06,
      "loss": 1.6318,
      "step": 700600
    },
    {
      "epoch": 53.5253227408143,
      "grad_norm": 5.3929123878479,
      "learning_rate": 5.395564382654751e-06,
      "loss": 1.7448,
      "step": 700700
    },
    {
      "epoch": 53.532961576655715,
      "grad_norm": 4.7769670486450195,
      "learning_rate": 5.389198686120236e-06,
      "loss": 1.6474,
      "step": 700800
    },
    {
      "epoch": 53.540600412497135,
      "grad_norm": 5.175009727478027,
      "learning_rate": 5.38283298958572e-06,
      "loss": 1.5969,
      "step": 700900
    },
    {
      "epoch": 53.548239248338554,
      "grad_norm": 6.2137346267700195,
      "learning_rate": 5.376467293051206e-06,
      "loss": 1.6862,
      "step": 701000
    },
    {
      "epoch": 53.55587808417997,
      "grad_norm": 5.241001605987549,
      "learning_rate": 5.370101596516691e-06,
      "loss": 1.5898,
      "step": 701100
    },
    {
      "epoch": 53.563516920021385,
      "grad_norm": 5.685648441314697,
      "learning_rate": 5.363735899982177e-06,
      "loss": 1.6887,
      "step": 701200
    },
    {
      "epoch": 53.571155755862804,
      "grad_norm": 6.690186500549316,
      "learning_rate": 5.357370203447661e-06,
      "loss": 1.564,
      "step": 701300
    },
    {
      "epoch": 53.578794591704224,
      "grad_norm": 5.288109302520752,
      "learning_rate": 5.3510045069131465e-06,
      "loss": 1.6225,
      "step": 701400
    },
    {
      "epoch": 53.58643342754564,
      "grad_norm": 4.171234607696533,
      "learning_rate": 5.344638810378632e-06,
      "loss": 1.6247,
      "step": 701500
    },
    {
      "epoch": 53.59407226338706,
      "grad_norm": 6.134993076324463,
      "learning_rate": 5.338273113844117e-06,
      "loss": 1.6036,
      "step": 701600
    },
    {
      "epoch": 53.601711099228474,
      "grad_norm": 6.66055965423584,
      "learning_rate": 5.331907417309602e-06,
      "loss": 1.6533,
      "step": 701700
    },
    {
      "epoch": 53.60934993506989,
      "grad_norm": 4.833213806152344,
      "learning_rate": 5.3255417207750875e-06,
      "loss": 1.7031,
      "step": 701800
    },
    {
      "epoch": 53.61698877091131,
      "grad_norm": 6.948662281036377,
      "learning_rate": 5.319176024240573e-06,
      "loss": 1.7588,
      "step": 701900
    },
    {
      "epoch": 53.62462760675273,
      "grad_norm": 6.288259983062744,
      "learning_rate": 5.312810327706058e-06,
      "loss": 1.6769,
      "step": 702000
    },
    {
      "epoch": 53.63226644259415,
      "grad_norm": 5.479709625244141,
      "learning_rate": 5.306444631171543e-06,
      "loss": 1.7188,
      "step": 702100
    },
    {
      "epoch": 53.63990527843556,
      "grad_norm": 4.554489612579346,
      "learning_rate": 5.300078934637028e-06,
      "loss": 1.6281,
      "step": 702200
    },
    {
      "epoch": 53.64754411427698,
      "grad_norm": 4.828105926513672,
      "learning_rate": 5.293713238102514e-06,
      "loss": 1.6711,
      "step": 702300
    },
    {
      "epoch": 53.6551829501184,
      "grad_norm": 9.195488929748535,
      "learning_rate": 5.287347541567999e-06,
      "loss": 1.6828,
      "step": 702400
    },
    {
      "epoch": 53.66282178595982,
      "grad_norm": 7.57483434677124,
      "learning_rate": 5.280981845033483e-06,
      "loss": 1.6632,
      "step": 702500
    },
    {
      "epoch": 53.67046062180124,
      "grad_norm": 5.461429119110107,
      "learning_rate": 5.274616148498969e-06,
      "loss": 1.6974,
      "step": 702600
    },
    {
      "epoch": 53.67809945764265,
      "grad_norm": 5.937380790710449,
      "learning_rate": 5.2682504519644545e-06,
      "loss": 1.695,
      "step": 702700
    },
    {
      "epoch": 53.68573829348407,
      "grad_norm": 5.958353519439697,
      "learning_rate": 5.26188475542994e-06,
      "loss": 1.7039,
      "step": 702800
    },
    {
      "epoch": 53.69337712932549,
      "grad_norm": 5.33692741394043,
      "learning_rate": 5.255519058895424e-06,
      "loss": 1.5446,
      "step": 702900
    },
    {
      "epoch": 53.70101596516691,
      "grad_norm": 6.475208282470703,
      "learning_rate": 5.249153362360909e-06,
      "loss": 1.6861,
      "step": 703000
    },
    {
      "epoch": 53.70865480100833,
      "grad_norm": 5.149153709411621,
      "learning_rate": 5.2427876658263955e-06,
      "loss": 1.6088,
      "step": 703100
    },
    {
      "epoch": 53.71629363684974,
      "grad_norm": 5.392803192138672,
      "learning_rate": 5.236421969291881e-06,
      "loss": 1.6457,
      "step": 703200
    },
    {
      "epoch": 53.72393247269116,
      "grad_norm": 5.87557315826416,
      "learning_rate": 5.230056272757365e-06,
      "loss": 1.6573,
      "step": 703300
    },
    {
      "epoch": 53.73157130853258,
      "grad_norm": 6.8576979637146,
      "learning_rate": 5.22369057622285e-06,
      "loss": 1.6834,
      "step": 703400
    },
    {
      "epoch": 53.739210144374,
      "grad_norm": 5.453713417053223,
      "learning_rate": 5.2173248796883355e-06,
      "loss": 1.6964,
      "step": 703500
    },
    {
      "epoch": 53.74684898021542,
      "grad_norm": 6.01716423034668,
      "learning_rate": 5.210959183153822e-06,
      "loss": 1.6529,
      "step": 703600
    },
    {
      "epoch": 53.75448781605683,
      "grad_norm": 6.564359188079834,
      "learning_rate": 5.204593486619306e-06,
      "loss": 1.6544,
      "step": 703700
    },
    {
      "epoch": 53.76212665189825,
      "grad_norm": 5.633203029632568,
      "learning_rate": 5.198227790084791e-06,
      "loss": 1.7184,
      "step": 703800
    },
    {
      "epoch": 53.76976548773967,
      "grad_norm": 5.728443145751953,
      "learning_rate": 5.1918620935502765e-06,
      "loss": 1.618,
      "step": 703900
    },
    {
      "epoch": 53.77740432358109,
      "grad_norm": 6.220523357391357,
      "learning_rate": 5.185496397015762e-06,
      "loss": 1.8028,
      "step": 704000
    },
    {
      "epoch": 53.78504315942251,
      "grad_norm": 5.38453483581543,
      "learning_rate": 5.179130700481247e-06,
      "loss": 1.6113,
      "step": 704100
    },
    {
      "epoch": 53.79268199526392,
      "grad_norm": 5.064483165740967,
      "learning_rate": 5.172765003946732e-06,
      "loss": 1.6366,
      "step": 704200
    },
    {
      "epoch": 53.80032083110534,
      "grad_norm": 5.685464859008789,
      "learning_rate": 5.166399307412217e-06,
      "loss": 1.6782,
      "step": 704300
    },
    {
      "epoch": 53.80795966694676,
      "grad_norm": 7.239526271820068,
      "learning_rate": 5.160033610877703e-06,
      "loss": 1.7346,
      "step": 704400
    },
    {
      "epoch": 53.81559850278818,
      "grad_norm": 3.7133491039276123,
      "learning_rate": 5.153667914343188e-06,
      "loss": 1.7174,
      "step": 704500
    },
    {
      "epoch": 53.823237338629596,
      "grad_norm": 4.709740161895752,
      "learning_rate": 5.147302217808673e-06,
      "loss": 1.6317,
      "step": 704600
    },
    {
      "epoch": 53.83087617447101,
      "grad_norm": 5.2505621910095215,
      "learning_rate": 5.140936521274158e-06,
      "loss": 1.6692,
      "step": 704700
    },
    {
      "epoch": 53.83851501031243,
      "grad_norm": 6.268980026245117,
      "learning_rate": 5.1345708247396435e-06,
      "loss": 1.5837,
      "step": 704800
    },
    {
      "epoch": 53.84615384615385,
      "grad_norm": 4.660510063171387,
      "learning_rate": 5.128205128205128e-06,
      "loss": 1.6781,
      "step": 704900
    },
    {
      "epoch": 53.853792681995266,
      "grad_norm": 4.8282389640808105,
      "learning_rate": 5.121839431670614e-06,
      "loss": 1.6763,
      "step": 705000
    },
    {
      "epoch": 53.861431517836685,
      "grad_norm": 5.992502212524414,
      "learning_rate": 5.115473735136099e-06,
      "loss": 1.752,
      "step": 705100
    },
    {
      "epoch": 53.8690703536781,
      "grad_norm": 4.660850524902344,
      "learning_rate": 5.109108038601584e-06,
      "loss": 1.6316,
      "step": 705200
    },
    {
      "epoch": 53.87670918951952,
      "grad_norm": 7.6030144691467285,
      "learning_rate": 5.102742342067069e-06,
      "loss": 1.6879,
      "step": 705300
    },
    {
      "epoch": 53.884348025360936,
      "grad_norm": 4.857193470001221,
      "learning_rate": 5.096376645532554e-06,
      "loss": 1.635,
      "step": 705400
    },
    {
      "epoch": 53.891986861202355,
      "grad_norm": 6.60988187789917,
      "learning_rate": 5.09001094899804e-06,
      "loss": 1.6963,
      "step": 705500
    },
    {
      "epoch": 53.89962569704377,
      "grad_norm": 8.625320434570312,
      "learning_rate": 5.0836452524635245e-06,
      "loss": 1.6651,
      "step": 705600
    },
    {
      "epoch": 53.907264532885186,
      "grad_norm": 5.073952674865723,
      "learning_rate": 5.07727955592901e-06,
      "loss": 1.6982,
      "step": 705700
    },
    {
      "epoch": 53.914903368726605,
      "grad_norm": 6.030569076538086,
      "learning_rate": 5.070913859394495e-06,
      "loss": 1.5728,
      "step": 705800
    },
    {
      "epoch": 53.922542204568025,
      "grad_norm": 4.3978118896484375,
      "learning_rate": 5.06454816285998e-06,
      "loss": 1.6034,
      "step": 705900
    },
    {
      "epoch": 53.930181040409444,
      "grad_norm": 6.734593868255615,
      "learning_rate": 5.0581824663254655e-06,
      "loss": 1.7061,
      "step": 706000
    },
    {
      "epoch": 53.937819876250856,
      "grad_norm": 5.148995399475098,
      "learning_rate": 5.051816769790951e-06,
      "loss": 1.6854,
      "step": 706100
    },
    {
      "epoch": 53.945458712092275,
      "grad_norm": 6.02133846282959,
      "learning_rate": 5.045451073256436e-06,
      "loss": 1.6098,
      "step": 706200
    },
    {
      "epoch": 53.953097547933694,
      "grad_norm": 5.376541614532471,
      "learning_rate": 5.039085376721921e-06,
      "loss": 1.635,
      "step": 706300
    },
    {
      "epoch": 53.960736383775114,
      "grad_norm": 6.485936641693115,
      "learning_rate": 5.032719680187406e-06,
      "loss": 1.6373,
      "step": 706400
    },
    {
      "epoch": 53.96837521961653,
      "grad_norm": 5.896373271942139,
      "learning_rate": 5.026353983652892e-06,
      "loss": 1.7269,
      "step": 706500
    },
    {
      "epoch": 53.976014055457945,
      "grad_norm": 8.934886932373047,
      "learning_rate": 5.019988287118377e-06,
      "loss": 1.7513,
      "step": 706600
    },
    {
      "epoch": 53.983652891299364,
      "grad_norm": 5.940446853637695,
      "learning_rate": 5.013622590583862e-06,
      "loss": 1.616,
      "step": 706700
    },
    {
      "epoch": 53.99129172714078,
      "grad_norm": 5.171688079833984,
      "learning_rate": 5.0072568940493465e-06,
      "loss": 1.6039,
      "step": 706800
    },
    {
      "epoch": 53.9989305629822,
      "grad_norm": 6.296419620513916,
      "learning_rate": 5.0008911975148325e-06,
      "loss": 1.6048,
      "step": 706900
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.766911506652832,
      "eval_runtime": 3.0436,
      "eval_samples_per_second": 226.703,
      "eval_steps_per_second": 226.703,
      "step": 706914
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.4230681657791138,
      "eval_runtime": 57.9102,
      "eval_samples_per_second": 226.057,
      "eval_steps_per_second": 226.057,
      "step": 706914
    },
    {
      "epoch": 54.00656939882362,
      "grad_norm": 5.160341262817383,
      "learning_rate": 4.994525500980318e-06,
      "loss": 1.711,
      "step": 707000
    },
    {
      "epoch": 54.014208234665034,
      "grad_norm": 6.520318031311035,
      "learning_rate": 4.988159804445803e-06,
      "loss": 1.6657,
      "step": 707100
    },
    {
      "epoch": 54.02184707050645,
      "grad_norm": 7.364297866821289,
      "learning_rate": 4.981794107911287e-06,
      "loss": 1.6849,
      "step": 707200
    },
    {
      "epoch": 54.02948590634787,
      "grad_norm": 6.7065887451171875,
      "learning_rate": 4.975428411376773e-06,
      "loss": 1.5895,
      "step": 707300
    },
    {
      "epoch": 54.03712474218929,
      "grad_norm": 7.1193952560424805,
      "learning_rate": 4.969062714842259e-06,
      "loss": 1.6467,
      "step": 707400
    },
    {
      "epoch": 54.04476357803071,
      "grad_norm": 6.00725793838501,
      "learning_rate": 4.962697018307744e-06,
      "loss": 1.6646,
      "step": 707500
    },
    {
      "epoch": 54.05240241387212,
      "grad_norm": 5.423215866088867,
      "learning_rate": 4.956331321773228e-06,
      "loss": 1.6644,
      "step": 707600
    },
    {
      "epoch": 54.06004124971354,
      "grad_norm": 5.432493209838867,
      "learning_rate": 4.9499656252387135e-06,
      "loss": 1.5706,
      "step": 707700
    },
    {
      "epoch": 54.06768008555496,
      "grad_norm": 5.7952470779418945,
      "learning_rate": 4.943599928704199e-06,
      "loss": 1.6527,
      "step": 707800
    },
    {
      "epoch": 54.07531892139638,
      "grad_norm": 6.98494291305542,
      "learning_rate": 4.937234232169685e-06,
      "loss": 1.6213,
      "step": 707900
    },
    {
      "epoch": 54.0829577572378,
      "grad_norm": 5.701110363006592,
      "learning_rate": 4.930868535635169e-06,
      "loss": 1.6838,
      "step": 708000
    },
    {
      "epoch": 54.09059659307921,
      "grad_norm": 6.038256645202637,
      "learning_rate": 4.9245028391006545e-06,
      "loss": 1.6748,
      "step": 708100
    },
    {
      "epoch": 54.09823542892063,
      "grad_norm": 5.841527462005615,
      "learning_rate": 4.91813714256614e-06,
      "loss": 1.6765,
      "step": 708200
    },
    {
      "epoch": 54.10587426476205,
      "grad_norm": 7.7247161865234375,
      "learning_rate": 4.911771446031625e-06,
      "loss": 1.7149,
      "step": 708300
    },
    {
      "epoch": 54.11351310060347,
      "grad_norm": 6.955260753631592,
      "learning_rate": 4.90540574949711e-06,
      "loss": 1.6501,
      "step": 708400
    },
    {
      "epoch": 54.12115193644489,
      "grad_norm": 5.127742290496826,
      "learning_rate": 4.899040052962595e-06,
      "loss": 1.5636,
      "step": 708500
    },
    {
      "epoch": 54.1287907722863,
      "grad_norm": 7.465822696685791,
      "learning_rate": 4.892674356428081e-06,
      "loss": 1.7173,
      "step": 708600
    },
    {
      "epoch": 54.13642960812772,
      "grad_norm": 6.8048858642578125,
      "learning_rate": 4.886308659893566e-06,
      "loss": 1.5289,
      "step": 708700
    },
    {
      "epoch": 54.14406844396914,
      "grad_norm": 7.111835956573486,
      "learning_rate": 4.879942963359051e-06,
      "loss": 1.6885,
      "step": 708800
    },
    {
      "epoch": 54.15170727981056,
      "grad_norm": 7.220579624176025,
      "learning_rate": 4.873577266824536e-06,
      "loss": 1.6249,
      "step": 708900
    },
    {
      "epoch": 54.15934611565198,
      "grad_norm": 5.3769450187683105,
      "learning_rate": 4.8672115702900215e-06,
      "loss": 1.6877,
      "step": 709000
    },
    {
      "epoch": 54.16698495149339,
      "grad_norm": 7.076661109924316,
      "learning_rate": 4.860845873755507e-06,
      "loss": 1.7379,
      "step": 709100
    },
    {
      "epoch": 54.17462378733481,
      "grad_norm": 5.1670403480529785,
      "learning_rate": 4.854480177220991e-06,
      "loss": 1.6072,
      "step": 709200
    },
    {
      "epoch": 54.18226262317623,
      "grad_norm": 4.872208595275879,
      "learning_rate": 4.848114480686477e-06,
      "loss": 1.6448,
      "step": 709300
    },
    {
      "epoch": 54.18990145901765,
      "grad_norm": 5.404850006103516,
      "learning_rate": 4.8417487841519625e-06,
      "loss": 1.7067,
      "step": 709400
    },
    {
      "epoch": 54.19754029485907,
      "grad_norm": 6.018180847167969,
      "learning_rate": 4.835383087617448e-06,
      "loss": 1.6094,
      "step": 709500
    },
    {
      "epoch": 54.20517913070048,
      "grad_norm": 4.26843786239624,
      "learning_rate": 4.829017391082932e-06,
      "loss": 1.6627,
      "step": 709600
    },
    {
      "epoch": 54.2128179665419,
      "grad_norm": 4.9512038230896,
      "learning_rate": 4.822651694548417e-06,
      "loss": 1.6415,
      "step": 709700
    },
    {
      "epoch": 54.22045680238332,
      "grad_norm": 7.000278949737549,
      "learning_rate": 4.816285998013903e-06,
      "loss": 1.731,
      "step": 709800
    },
    {
      "epoch": 54.22809563822474,
      "grad_norm": 6.577754020690918,
      "learning_rate": 4.809920301479389e-06,
      "loss": 1.5697,
      "step": 709900
    },
    {
      "epoch": 54.23573447406615,
      "grad_norm": 5.885952472686768,
      "learning_rate": 4.803554604944873e-06,
      "loss": 1.6297,
      "step": 710000
    },
    {
      "epoch": 54.24337330990757,
      "grad_norm": 5.931832313537598,
      "learning_rate": 4.797188908410358e-06,
      "loss": 1.6256,
      "step": 710100
    },
    {
      "epoch": 54.25101214574899,
      "grad_norm": 4.686436653137207,
      "learning_rate": 4.7908232118758435e-06,
      "loss": 1.5786,
      "step": 710200
    },
    {
      "epoch": 54.25865098159041,
      "grad_norm": 4.294750213623047,
      "learning_rate": 4.7844575153413295e-06,
      "loss": 1.7095,
      "step": 710300
    },
    {
      "epoch": 54.266289817431826,
      "grad_norm": 7.491374969482422,
      "learning_rate": 4.778091818806814e-06,
      "loss": 1.6215,
      "step": 710400
    },
    {
      "epoch": 54.27392865327324,
      "grad_norm": 6.204489231109619,
      "learning_rate": 4.771726122272299e-06,
      "loss": 1.7553,
      "step": 710500
    },
    {
      "epoch": 54.28156748911466,
      "grad_norm": 6.224453926086426,
      "learning_rate": 4.765360425737784e-06,
      "loss": 1.6608,
      "step": 710600
    },
    {
      "epoch": 54.289206324956076,
      "grad_norm": 5.838784217834473,
      "learning_rate": 4.75899472920327e-06,
      "loss": 1.6297,
      "step": 710700
    },
    {
      "epoch": 54.296845160797496,
      "grad_norm": 6.232291221618652,
      "learning_rate": 4.752629032668755e-06,
      "loss": 1.6859,
      "step": 710800
    },
    {
      "epoch": 54.304483996638915,
      "grad_norm": 5.361569881439209,
      "learning_rate": 4.74626333613424e-06,
      "loss": 1.5979,
      "step": 710900
    },
    {
      "epoch": 54.31212283248033,
      "grad_norm": 4.3937668800354,
      "learning_rate": 4.739897639599725e-06,
      "loss": 1.7463,
      "step": 711000
    },
    {
      "epoch": 54.319761668321746,
      "grad_norm": 7.8672943115234375,
      "learning_rate": 4.7335319430652105e-06,
      "loss": 1.6167,
      "step": 711100
    },
    {
      "epoch": 54.327400504163165,
      "grad_norm": 5.220357418060303,
      "learning_rate": 4.727166246530696e-06,
      "loss": 1.6187,
      "step": 711200
    },
    {
      "epoch": 54.335039340004585,
      "grad_norm": 6.083699703216553,
      "learning_rate": 4.720800549996181e-06,
      "loss": 1.7547,
      "step": 711300
    },
    {
      "epoch": 54.342678175846004,
      "grad_norm": 6.054345607757568,
      "learning_rate": 4.714434853461666e-06,
      "loss": 1.7148,
      "step": 711400
    },
    {
      "epoch": 54.350317011687416,
      "grad_norm": 4.724027633666992,
      "learning_rate": 4.7080691569271515e-06,
      "loss": 1.6541,
      "step": 711500
    },
    {
      "epoch": 54.357955847528835,
      "grad_norm": 5.514564514160156,
      "learning_rate": 4.701703460392636e-06,
      "loss": 1.7032,
      "step": 711600
    },
    {
      "epoch": 54.365594683370254,
      "grad_norm": 6.757510185241699,
      "learning_rate": 4.695337763858122e-06,
      "loss": 1.7234,
      "step": 711700
    },
    {
      "epoch": 54.373233519211674,
      "grad_norm": 5.650080680847168,
      "learning_rate": 4.688972067323607e-06,
      "loss": 1.5537,
      "step": 711800
    },
    {
      "epoch": 54.38087235505309,
      "grad_norm": 5.6018171310424805,
      "learning_rate": 4.682606370789092e-06,
      "loss": 1.6966,
      "step": 711900
    },
    {
      "epoch": 54.388511190894505,
      "grad_norm": 7.194591522216797,
      "learning_rate": 4.676240674254577e-06,
      "loss": 1.7212,
      "step": 712000
    },
    {
      "epoch": 54.396150026735924,
      "grad_norm": 5.716732978820801,
      "learning_rate": 4.669874977720062e-06,
      "loss": 1.6796,
      "step": 712100
    },
    {
      "epoch": 54.40378886257734,
      "grad_norm": 6.176423072814941,
      "learning_rate": 4.663509281185548e-06,
      "loss": 1.6772,
      "step": 712200
    },
    {
      "epoch": 54.41142769841876,
      "grad_norm": 5.851450443267822,
      "learning_rate": 4.657143584651033e-06,
      "loss": 1.7017,
      "step": 712300
    },
    {
      "epoch": 54.41906653426018,
      "grad_norm": 3.845834732055664,
      "learning_rate": 4.650777888116518e-06,
      "loss": 1.5801,
      "step": 712400
    },
    {
      "epoch": 54.426705370101594,
      "grad_norm": 6.055948734283447,
      "learning_rate": 4.644412191582003e-06,
      "loss": 1.6947,
      "step": 712500
    },
    {
      "epoch": 54.43434420594301,
      "grad_norm": 4.614181041717529,
      "learning_rate": 4.638046495047488e-06,
      "loss": 1.6248,
      "step": 712600
    },
    {
      "epoch": 54.44198304178443,
      "grad_norm": 6.149637222290039,
      "learning_rate": 4.631680798512974e-06,
      "loss": 1.8025,
      "step": 712700
    },
    {
      "epoch": 54.44962187762585,
      "grad_norm": 6.100334644317627,
      "learning_rate": 4.625315101978459e-06,
      "loss": 1.5964,
      "step": 712800
    },
    {
      "epoch": 54.45726071346727,
      "grad_norm": 6.0194196701049805,
      "learning_rate": 4.618949405443944e-06,
      "loss": 1.6238,
      "step": 712900
    },
    {
      "epoch": 54.46489954930868,
      "grad_norm": 6.226385593414307,
      "learning_rate": 4.612583708909429e-06,
      "loss": 1.7122,
      "step": 713000
    },
    {
      "epoch": 54.4725383851501,
      "grad_norm": 5.738065242767334,
      "learning_rate": 4.606218012374914e-06,
      "loss": 1.6405,
      "step": 713100
    },
    {
      "epoch": 54.48017722099152,
      "grad_norm": 7.291013240814209,
      "learning_rate": 4.5998523158403995e-06,
      "loss": 1.6583,
      "step": 713200
    },
    {
      "epoch": 54.48781605683294,
      "grad_norm": 4.685400485992432,
      "learning_rate": 4.593486619305885e-06,
      "loss": 1.6977,
      "step": 713300
    },
    {
      "epoch": 54.49545489267436,
      "grad_norm": 6.9114227294921875,
      "learning_rate": 4.58712092277137e-06,
      "loss": 1.6477,
      "step": 713400
    },
    {
      "epoch": 54.50309372851577,
      "grad_norm": 5.332272052764893,
      "learning_rate": 4.580755226236854e-06,
      "loss": 1.6612,
      "step": 713500
    },
    {
      "epoch": 54.51073256435719,
      "grad_norm": 6.6113972663879395,
      "learning_rate": 4.5743895297023405e-06,
      "loss": 1.7294,
      "step": 713600
    },
    {
      "epoch": 54.51837140019861,
      "grad_norm": 6.744396686553955,
      "learning_rate": 4.568023833167826e-06,
      "loss": 1.6689,
      "step": 713700
    },
    {
      "epoch": 54.52601023604003,
      "grad_norm": 5.202552795410156,
      "learning_rate": 4.561658136633311e-06,
      "loss": 1.6516,
      "step": 713800
    },
    {
      "epoch": 54.53364907188144,
      "grad_norm": 5.127214431762695,
      "learning_rate": 4.555292440098795e-06,
      "loss": 1.8411,
      "step": 713900
    },
    {
      "epoch": 54.54128790772286,
      "grad_norm": 5.516728401184082,
      "learning_rate": 4.5489267435642805e-06,
      "loss": 1.6315,
      "step": 714000
    },
    {
      "epoch": 54.54892674356428,
      "grad_norm": 4.684859752655029,
      "learning_rate": 4.542561047029767e-06,
      "loss": 1.5494,
      "step": 714100
    },
    {
      "epoch": 54.5565655794057,
      "grad_norm": 4.031318664550781,
      "learning_rate": 4.536195350495252e-06,
      "loss": 1.6537,
      "step": 714200
    },
    {
      "epoch": 54.56420441524712,
      "grad_norm": 6.2639970779418945,
      "learning_rate": 4.529829653960736e-06,
      "loss": 1.692,
      "step": 714300
    },
    {
      "epoch": 54.57184325108853,
      "grad_norm": 5.41891622543335,
      "learning_rate": 4.5234639574262215e-06,
      "loss": 1.636,
      "step": 714400
    },
    {
      "epoch": 54.57948208692995,
      "grad_norm": 5.631979942321777,
      "learning_rate": 4.517098260891707e-06,
      "loss": 1.6384,
      "step": 714500
    },
    {
      "epoch": 54.58712092277137,
      "grad_norm": 5.6884307861328125,
      "learning_rate": 4.510732564357193e-06,
      "loss": 1.6396,
      "step": 714600
    },
    {
      "epoch": 54.59475975861279,
      "grad_norm": 4.922634124755859,
      "learning_rate": 4.504366867822677e-06,
      "loss": 1.5931,
      "step": 714700
    },
    {
      "epoch": 54.60239859445421,
      "grad_norm": 5.246068954467773,
      "learning_rate": 4.498001171288162e-06,
      "loss": 1.7197,
      "step": 714800
    },
    {
      "epoch": 54.61003743029562,
      "grad_norm": 5.593333721160889,
      "learning_rate": 4.491635474753648e-06,
      "loss": 1.5842,
      "step": 714900
    },
    {
      "epoch": 54.61767626613704,
      "grad_norm": 5.648776531219482,
      "learning_rate": 4.485269778219133e-06,
      "loss": 1.6154,
      "step": 715000
    },
    {
      "epoch": 54.62531510197846,
      "grad_norm": 7.071779727935791,
      "learning_rate": 4.478904081684618e-06,
      "loss": 1.7183,
      "step": 715100
    },
    {
      "epoch": 54.63295393781988,
      "grad_norm": 5.025323867797852,
      "learning_rate": 4.472538385150103e-06,
      "loss": 1.6588,
      "step": 715200
    },
    {
      "epoch": 54.6405927736613,
      "grad_norm": 6.163576602935791,
      "learning_rate": 4.4661726886155885e-06,
      "loss": 1.6388,
      "step": 715300
    },
    {
      "epoch": 54.64823160950271,
      "grad_norm": 5.581357955932617,
      "learning_rate": 4.459806992081074e-06,
      "loss": 1.6564,
      "step": 715400
    },
    {
      "epoch": 54.65587044534413,
      "grad_norm": 4.798847198486328,
      "learning_rate": 4.453441295546559e-06,
      "loss": 1.6248,
      "step": 715500
    },
    {
      "epoch": 54.66350928118555,
      "grad_norm": 6.038216590881348,
      "learning_rate": 4.447075599012044e-06,
      "loss": 1.5901,
      "step": 715600
    },
    {
      "epoch": 54.671148117026966,
      "grad_norm": 4.956783771514893,
      "learning_rate": 4.4407099024775295e-06,
      "loss": 1.7048,
      "step": 715700
    },
    {
      "epoch": 54.678786952868386,
      "grad_norm": 6.404382705688477,
      "learning_rate": 4.434344205943015e-06,
      "loss": 1.7372,
      "step": 715800
    },
    {
      "epoch": 54.6864257887098,
      "grad_norm": 4.619614601135254,
      "learning_rate": 4.427978509408499e-06,
      "loss": 1.6646,
      "step": 715900
    },
    {
      "epoch": 54.69406462455122,
      "grad_norm": 4.0869245529174805,
      "learning_rate": 4.421612812873985e-06,
      "loss": 1.6154,
      "step": 716000
    },
    {
      "epoch": 54.701703460392636,
      "grad_norm": 5.318358898162842,
      "learning_rate": 4.41524711633947e-06,
      "loss": 1.6456,
      "step": 716100
    },
    {
      "epoch": 54.709342296234055,
      "grad_norm": 5.45546817779541,
      "learning_rate": 4.408881419804956e-06,
      "loss": 1.728,
      "step": 716200
    },
    {
      "epoch": 54.716981132075475,
      "grad_norm": 5.731074333190918,
      "learning_rate": 4.40251572327044e-06,
      "loss": 1.5362,
      "step": 716300
    },
    {
      "epoch": 54.72461996791689,
      "grad_norm": 4.779626369476318,
      "learning_rate": 4.396150026735925e-06,
      "loss": 1.5713,
      "step": 716400
    },
    {
      "epoch": 54.732258803758306,
      "grad_norm": 7.345141887664795,
      "learning_rate": 4.389784330201411e-06,
      "loss": 1.7061,
      "step": 716500
    },
    {
      "epoch": 54.739897639599725,
      "grad_norm": 4.589358329772949,
      "learning_rate": 4.3834186336668965e-06,
      "loss": 1.7251,
      "step": 716600
    },
    {
      "epoch": 54.747536475441144,
      "grad_norm": 5.303734302520752,
      "learning_rate": 4.377052937132381e-06,
      "loss": 1.6534,
      "step": 716700
    },
    {
      "epoch": 54.755175311282564,
      "grad_norm": 4.201170921325684,
      "learning_rate": 4.370687240597866e-06,
      "loss": 1.5363,
      "step": 716800
    },
    {
      "epoch": 54.762814147123976,
      "grad_norm": 6.330554008483887,
      "learning_rate": 4.364321544063351e-06,
      "loss": 1.7914,
      "step": 716900
    },
    {
      "epoch": 54.770452982965395,
      "grad_norm": 5.861571311950684,
      "learning_rate": 4.3579558475288375e-06,
      "loss": 1.6501,
      "step": 717000
    },
    {
      "epoch": 54.778091818806814,
      "grad_norm": 6.659517765045166,
      "learning_rate": 4.351590150994322e-06,
      "loss": 1.6303,
      "step": 717100
    },
    {
      "epoch": 54.78573065464823,
      "grad_norm": 6.948970794677734,
      "learning_rate": 4.345224454459807e-06,
      "loss": 1.6047,
      "step": 717200
    },
    {
      "epoch": 54.79336949048965,
      "grad_norm": 6.463131427764893,
      "learning_rate": 4.338858757925292e-06,
      "loss": 1.6641,
      "step": 717300
    },
    {
      "epoch": 54.801008326331065,
      "grad_norm": 7.884331703186035,
      "learning_rate": 4.3324930613907775e-06,
      "loss": 1.6569,
      "step": 717400
    },
    {
      "epoch": 54.808647162172484,
      "grad_norm": 5.078915596008301,
      "learning_rate": 4.326127364856263e-06,
      "loss": 1.6579,
      "step": 717500
    },
    {
      "epoch": 54.8162859980139,
      "grad_norm": 4.058579444885254,
      "learning_rate": 4.319761668321748e-06,
      "loss": 1.6292,
      "step": 717600
    },
    {
      "epoch": 54.82392483385532,
      "grad_norm": 5.398921489715576,
      "learning_rate": 4.313395971787233e-06,
      "loss": 1.6987,
      "step": 717700
    },
    {
      "epoch": 54.83156366969674,
      "grad_norm": 5.718168258666992,
      "learning_rate": 4.3070302752527185e-06,
      "loss": 1.6431,
      "step": 717800
    },
    {
      "epoch": 54.839202505538154,
      "grad_norm": 6.770698547363281,
      "learning_rate": 4.300664578718204e-06,
      "loss": 1.6839,
      "step": 717900
    },
    {
      "epoch": 54.84684134137957,
      "grad_norm": 5.106060981750488,
      "learning_rate": 4.294298882183689e-06,
      "loss": 1.5972,
      "step": 718000
    },
    {
      "epoch": 54.85448017722099,
      "grad_norm": 6.249147891998291,
      "learning_rate": 4.287933185649174e-06,
      "loss": 1.6169,
      "step": 718100
    },
    {
      "epoch": 54.86211901306241,
      "grad_norm": 5.490262508392334,
      "learning_rate": 4.281567489114659e-06,
      "loss": 1.6474,
      "step": 718200
    },
    {
      "epoch": 54.869757848903824,
      "grad_norm": 5.1532206535339355,
      "learning_rate": 4.275201792580144e-06,
      "loss": 1.6037,
      "step": 718300
    },
    {
      "epoch": 54.87739668474524,
      "grad_norm": 5.917208194732666,
      "learning_rate": 4.26883609604563e-06,
      "loss": 1.7138,
      "step": 718400
    },
    {
      "epoch": 54.88503552058666,
      "grad_norm": 7.562472820281982,
      "learning_rate": 4.262470399511115e-06,
      "loss": 1.6951,
      "step": 718500
    },
    {
      "epoch": 54.89267435642808,
      "grad_norm": 6.800601482391357,
      "learning_rate": 4.2561047029766e-06,
      "loss": 1.607,
      "step": 718600
    },
    {
      "epoch": 54.9003131922695,
      "grad_norm": 5.942862510681152,
      "learning_rate": 4.249739006442085e-06,
      "loss": 1.7253,
      "step": 718700
    },
    {
      "epoch": 54.90795202811091,
      "grad_norm": 4.654082775115967,
      "learning_rate": 4.24337330990757e-06,
      "loss": 1.7091,
      "step": 718800
    },
    {
      "epoch": 54.91559086395233,
      "grad_norm": 5.112153053283691,
      "learning_rate": 4.237007613373056e-06,
      "loss": 1.7095,
      "step": 718900
    },
    {
      "epoch": 54.92322969979375,
      "grad_norm": 6.677208423614502,
      "learning_rate": 4.230641916838541e-06,
      "loss": 1.7594,
      "step": 719000
    },
    {
      "epoch": 54.93086853563517,
      "grad_norm": 4.65085506439209,
      "learning_rate": 4.224276220304026e-06,
      "loss": 1.7206,
      "step": 719100
    },
    {
      "epoch": 54.93850737147659,
      "grad_norm": 4.658579349517822,
      "learning_rate": 4.217910523769511e-06,
      "loss": 1.5661,
      "step": 719200
    },
    {
      "epoch": 54.946146207318,
      "grad_norm": 3.795258045196533,
      "learning_rate": 4.211544827234996e-06,
      "loss": 1.6538,
      "step": 719300
    },
    {
      "epoch": 54.95378504315942,
      "grad_norm": 6.565566062927246,
      "learning_rate": 4.205179130700482e-06,
      "loss": 1.8059,
      "step": 719400
    },
    {
      "epoch": 54.96142387900084,
      "grad_norm": 5.624305248260498,
      "learning_rate": 4.1988134341659665e-06,
      "loss": 1.6604,
      "step": 719500
    },
    {
      "epoch": 54.96906271484226,
      "grad_norm": 5.479617595672607,
      "learning_rate": 4.192447737631452e-06,
      "loss": 1.5923,
      "step": 719600
    },
    {
      "epoch": 54.97670155068368,
      "grad_norm": 6.470591068267822,
      "learning_rate": 4.186082041096937e-06,
      "loss": 1.6453,
      "step": 719700
    },
    {
      "epoch": 54.98434038652509,
      "grad_norm": 5.850996494293213,
      "learning_rate": 4.179716344562422e-06,
      "loss": 1.6719,
      "step": 719800
    },
    {
      "epoch": 54.99197922236651,
      "grad_norm": 5.225100040435791,
      "learning_rate": 4.1733506480279075e-06,
      "loss": 1.6794,
      "step": 719900
    },
    {
      "epoch": 54.99961805820793,
      "grad_norm": 5.043039798736572,
      "learning_rate": 4.166984951493393e-06,
      "loss": 1.6153,
      "step": 720000
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.769400715827942,
      "eval_runtime": 3.0053,
      "eval_samples_per_second": 229.592,
      "eval_steps_per_second": 229.592,
      "step": 720005
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.423486351966858,
      "eval_runtime": 57.5863,
      "eval_samples_per_second": 227.328,
      "eval_steps_per_second": 227.328,
      "step": 720005
    },
    {
      "epoch": 55.00725689404935,
      "grad_norm": 5.101141929626465,
      "learning_rate": 4.160619254958878e-06,
      "loss": 1.707,
      "step": 720100
    },
    {
      "epoch": 55.01489572989077,
      "grad_norm": 5.339540481567383,
      "learning_rate": 4.154253558424363e-06,
      "loss": 1.6476,
      "step": 720200
    },
    {
      "epoch": 55.02253456573218,
      "grad_norm": 6.242185592651367,
      "learning_rate": 4.147887861889848e-06,
      "loss": 1.7044,
      "step": 720300
    },
    {
      "epoch": 55.0301734015736,
      "grad_norm": 6.163277626037598,
      "learning_rate": 4.141522165355334e-06,
      "loss": 1.6996,
      "step": 720400
    },
    {
      "epoch": 55.03781223741502,
      "grad_norm": 5.448181629180908,
      "learning_rate": 4.135156468820819e-06,
      "loss": 1.659,
      "step": 720500
    },
    {
      "epoch": 55.04545107325644,
      "grad_norm": 6.630251407623291,
      "learning_rate": 4.128790772286304e-06,
      "loss": 1.6752,
      "step": 720600
    },
    {
      "epoch": 55.05308990909786,
      "grad_norm": 5.438547611236572,
      "learning_rate": 4.1224250757517885e-06,
      "loss": 1.6195,
      "step": 720700
    },
    {
      "epoch": 55.06072874493927,
      "grad_norm": 5.804901123046875,
      "learning_rate": 4.1160593792172745e-06,
      "loss": 1.7218,
      "step": 720800
    },
    {
      "epoch": 55.06836758078069,
      "grad_norm": 6.128392696380615,
      "learning_rate": 4.10969368268276e-06,
      "loss": 1.7206,
      "step": 720900
    },
    {
      "epoch": 55.07600641662211,
      "grad_norm": 6.055934906005859,
      "learning_rate": 4.103327986148245e-06,
      "loss": 1.653,
      "step": 721000
    },
    {
      "epoch": 55.083645252463526,
      "grad_norm": 6.043670177459717,
      "learning_rate": 4.096962289613729e-06,
      "loss": 1.6196,
      "step": 721100
    },
    {
      "epoch": 55.091284088304945,
      "grad_norm": 7.346596717834473,
      "learning_rate": 4.090596593079215e-06,
      "loss": 1.6433,
      "step": 721200
    },
    {
      "epoch": 55.09892292414636,
      "grad_norm": 5.49358606338501,
      "learning_rate": 4.084230896544701e-06,
      "loss": 1.6689,
      "step": 721300
    },
    {
      "epoch": 55.10656175998778,
      "grad_norm": 9.700754165649414,
      "learning_rate": 4.077865200010186e-06,
      "loss": 1.6958,
      "step": 721400
    },
    {
      "epoch": 55.114200595829196,
      "grad_norm": 5.858145236968994,
      "learning_rate": 4.07149950347567e-06,
      "loss": 1.7067,
      "step": 721500
    },
    {
      "epoch": 55.121839431670615,
      "grad_norm": 6.901357650756836,
      "learning_rate": 4.0651338069411555e-06,
      "loss": 1.6866,
      "step": 721600
    },
    {
      "epoch": 55.129478267512035,
      "grad_norm": 6.234379768371582,
      "learning_rate": 4.058768110406641e-06,
      "loss": 1.6713,
      "step": 721700
    },
    {
      "epoch": 55.13711710335345,
      "grad_norm": 6.2590765953063965,
      "learning_rate": 4.052402413872126e-06,
      "loss": 1.6003,
      "step": 721800
    },
    {
      "epoch": 55.144755939194866,
      "grad_norm": 4.795740127563477,
      "learning_rate": 4.046036717337611e-06,
      "loss": 1.671,
      "step": 721900
    },
    {
      "epoch": 55.152394775036285,
      "grad_norm": 5.15263032913208,
      "learning_rate": 4.0396710208030965e-06,
      "loss": 1.5865,
      "step": 722000
    },
    {
      "epoch": 55.160033610877704,
      "grad_norm": 7.261315822601318,
      "learning_rate": 4.033305324268582e-06,
      "loss": 1.6731,
      "step": 722100
    },
    {
      "epoch": 55.16767244671912,
      "grad_norm": 6.862951278686523,
      "learning_rate": 4.026939627734067e-06,
      "loss": 1.674,
      "step": 722200
    },
    {
      "epoch": 55.175311282560536,
      "grad_norm": 6.402537822723389,
      "learning_rate": 4.020573931199552e-06,
      "loss": 1.6833,
      "step": 722300
    },
    {
      "epoch": 55.182950118401955,
      "grad_norm": 5.112146854400635,
      "learning_rate": 4.014208234665037e-06,
      "loss": 1.6236,
      "step": 722400
    },
    {
      "epoch": 55.190588954243374,
      "grad_norm": 5.962298393249512,
      "learning_rate": 4.007842538130523e-06,
      "loss": 1.5749,
      "step": 722500
    },
    {
      "epoch": 55.19822779008479,
      "grad_norm": 5.211469650268555,
      "learning_rate": 4.001476841596007e-06,
      "loss": 1.6376,
      "step": 722600
    },
    {
      "epoch": 55.205866625926205,
      "grad_norm": 6.497713565826416,
      "learning_rate": 3.995111145061493e-06,
      "loss": 1.6472,
      "step": 722700
    },
    {
      "epoch": 55.213505461767625,
      "grad_norm": 5.458552837371826,
      "learning_rate": 3.988745448526978e-06,
      "loss": 1.65,
      "step": 722800
    },
    {
      "epoch": 55.221144297609044,
      "grad_norm": 5.230023384094238,
      "learning_rate": 3.9823797519924635e-06,
      "loss": 1.6529,
      "step": 722900
    },
    {
      "epoch": 55.22878313345046,
      "grad_norm": 4.361122131347656,
      "learning_rate": 3.976014055457948e-06,
      "loss": 1.6465,
      "step": 723000
    },
    {
      "epoch": 55.23642196929188,
      "grad_norm": 4.29431676864624,
      "learning_rate": 3.969648358923433e-06,
      "loss": 1.661,
      "step": 723100
    },
    {
      "epoch": 55.244060805133294,
      "grad_norm": 5.8495707511901855,
      "learning_rate": 3.963282662388919e-06,
      "loss": 1.7239,
      "step": 723200
    },
    {
      "epoch": 55.251699640974714,
      "grad_norm": 6.486269474029541,
      "learning_rate": 3.9569169658544045e-06,
      "loss": 1.6382,
      "step": 723300
    },
    {
      "epoch": 55.25933847681613,
      "grad_norm": 7.832284450531006,
      "learning_rate": 3.950551269319889e-06,
      "loss": 1.6967,
      "step": 723400
    },
    {
      "epoch": 55.26697731265755,
      "grad_norm": 5.345885276794434,
      "learning_rate": 3.944185572785374e-06,
      "loss": 1.6823,
      "step": 723500
    },
    {
      "epoch": 55.27461614849897,
      "grad_norm": 5.194597244262695,
      "learning_rate": 3.937819876250859e-06,
      "loss": 1.6796,
      "step": 723600
    },
    {
      "epoch": 55.28225498434038,
      "grad_norm": 5.243821144104004,
      "learning_rate": 3.931454179716345e-06,
      "loss": 1.796,
      "step": 723700
    },
    {
      "epoch": 55.2898938201818,
      "grad_norm": 6.9870500564575195,
      "learning_rate": 3.92508848318183e-06,
      "loss": 1.6034,
      "step": 723800
    },
    {
      "epoch": 55.29753265602322,
      "grad_norm": 6.302748203277588,
      "learning_rate": 3.918722786647315e-06,
      "loss": 1.6486,
      "step": 723900
    },
    {
      "epoch": 55.30517149186464,
      "grad_norm": 5.999128818511963,
      "learning_rate": 3.9123570901128e-06,
      "loss": 1.6669,
      "step": 724000
    },
    {
      "epoch": 55.31281032770606,
      "grad_norm": 6.5787177085876465,
      "learning_rate": 3.9059913935782855e-06,
      "loss": 1.5963,
      "step": 724100
    },
    {
      "epoch": 55.32044916354747,
      "grad_norm": 6.7464823722839355,
      "learning_rate": 3.899625697043771e-06,
      "loss": 1.6592,
      "step": 724200
    },
    {
      "epoch": 55.32808799938889,
      "grad_norm": 5.819195747375488,
      "learning_rate": 3.893260000509256e-06,
      "loss": 1.6139,
      "step": 724300
    },
    {
      "epoch": 55.33572683523031,
      "grad_norm": 4.683496952056885,
      "learning_rate": 3.886894303974741e-06,
      "loss": 1.6307,
      "step": 724400
    },
    {
      "epoch": 55.34336567107173,
      "grad_norm": 4.818039894104004,
      "learning_rate": 3.880528607440226e-06,
      "loss": 1.7574,
      "step": 724500
    },
    {
      "epoch": 55.35100450691315,
      "grad_norm": 6.2573137283325195,
      "learning_rate": 3.874162910905712e-06,
      "loss": 1.655,
      "step": 724600
    },
    {
      "epoch": 55.35864334275456,
      "grad_norm": 6.35595178604126,
      "learning_rate": 3.867797214371197e-06,
      "loss": 1.6299,
      "step": 724700
    },
    {
      "epoch": 55.36628217859598,
      "grad_norm": 4.670665740966797,
      "learning_rate": 3.861431517836682e-06,
      "loss": 1.6613,
      "step": 724800
    },
    {
      "epoch": 55.3739210144374,
      "grad_norm": 7.75575065612793,
      "learning_rate": 3.855065821302167e-06,
      "loss": 1.6901,
      "step": 724900
    },
    {
      "epoch": 55.38155985027882,
      "grad_norm": 7.393832206726074,
      "learning_rate": 3.848700124767652e-06,
      "loss": 1.7119,
      "step": 725000
    },
    {
      "epoch": 55.38919868612024,
      "grad_norm": 6.313008785247803,
      "learning_rate": 3.842334428233138e-06,
      "loss": 1.5863,
      "step": 725100
    },
    {
      "epoch": 55.39683752196165,
      "grad_norm": 5.398622989654541,
      "learning_rate": 3.835968731698623e-06,
      "loss": 1.6347,
      "step": 725200
    },
    {
      "epoch": 55.40447635780307,
      "grad_norm": 5.661993026733398,
      "learning_rate": 3.829603035164108e-06,
      "loss": 1.7582,
      "step": 725300
    },
    {
      "epoch": 55.41211519364449,
      "grad_norm": 4.989962577819824,
      "learning_rate": 3.823237338629593e-06,
      "loss": 1.5666,
      "step": 725400
    },
    {
      "epoch": 55.41975402948591,
      "grad_norm": 5.761322975158691,
      "learning_rate": 3.816871642095078e-06,
      "loss": 1.6028,
      "step": 725500
    },
    {
      "epoch": 55.42739286532733,
      "grad_norm": 4.615773677825928,
      "learning_rate": 3.8105059455605635e-06,
      "loss": 1.6294,
      "step": 725600
    },
    {
      "epoch": 55.43503170116874,
      "grad_norm": 5.53812837600708,
      "learning_rate": 3.8041402490260487e-06,
      "loss": 1.6881,
      "step": 725700
    },
    {
      "epoch": 55.44267053701016,
      "grad_norm": 8.107499122619629,
      "learning_rate": 3.7977745524915335e-06,
      "loss": 1.6788,
      "step": 725800
    },
    {
      "epoch": 55.45030937285158,
      "grad_norm": 5.7425217628479,
      "learning_rate": 3.7914088559570188e-06,
      "loss": 1.5899,
      "step": 725900
    },
    {
      "epoch": 55.457948208693,
      "grad_norm": 6.397339344024658,
      "learning_rate": 3.7850431594225044e-06,
      "loss": 1.7627,
      "step": 726000
    },
    {
      "epoch": 55.465587044534416,
      "grad_norm": 6.148181438446045,
      "learning_rate": 3.7786774628879896e-06,
      "loss": 1.5793,
      "step": 726100
    },
    {
      "epoch": 55.47322588037583,
      "grad_norm": 4.965124607086182,
      "learning_rate": 3.7723117663534745e-06,
      "loss": 1.6718,
      "step": 726200
    },
    {
      "epoch": 55.48086471621725,
      "grad_norm": 5.0691752433776855,
      "learning_rate": 3.7659460698189597e-06,
      "loss": 1.71,
      "step": 726300
    },
    {
      "epoch": 55.48850355205867,
      "grad_norm": 4.916019916534424,
      "learning_rate": 3.759580373284445e-06,
      "loss": 1.6632,
      "step": 726400
    },
    {
      "epoch": 55.496142387900086,
      "grad_norm": 5.63309383392334,
      "learning_rate": 3.7532146767499306e-06,
      "loss": 1.6918,
      "step": 726500
    },
    {
      "epoch": 55.5037812237415,
      "grad_norm": 6.20958948135376,
      "learning_rate": 3.746848980215415e-06,
      "loss": 1.6552,
      "step": 726600
    },
    {
      "epoch": 55.51142005958292,
      "grad_norm": 7.961966037750244,
      "learning_rate": 3.7404832836809006e-06,
      "loss": 1.5714,
      "step": 726700
    },
    {
      "epoch": 55.51905889542434,
      "grad_norm": 6.536621570587158,
      "learning_rate": 3.734117587146386e-06,
      "loss": 1.7191,
      "step": 726800
    },
    {
      "epoch": 55.526697731265756,
      "grad_norm": 5.947134494781494,
      "learning_rate": 3.727751890611871e-06,
      "loss": 1.7007,
      "step": 726900
    },
    {
      "epoch": 55.534336567107175,
      "grad_norm": 5.235934734344482,
      "learning_rate": 3.721386194077356e-06,
      "loss": 1.7817,
      "step": 727000
    },
    {
      "epoch": 55.54197540294859,
      "grad_norm": 6.2180094718933105,
      "learning_rate": 3.715020497542841e-06,
      "loss": 1.6585,
      "step": 727100
    },
    {
      "epoch": 55.54961423879001,
      "grad_norm": 5.875663757324219,
      "learning_rate": 3.7086548010083268e-06,
      "loss": 1.6589,
      "step": 727200
    },
    {
      "epoch": 55.557253074631426,
      "grad_norm": 5.598463535308838,
      "learning_rate": 3.702289104473812e-06,
      "loss": 1.6044,
      "step": 727300
    },
    {
      "epoch": 55.564891910472845,
      "grad_norm": 5.013166427612305,
      "learning_rate": 3.695923407939297e-06,
      "loss": 1.7321,
      "step": 727400
    },
    {
      "epoch": 55.572530746314264,
      "grad_norm": 5.260261058807373,
      "learning_rate": 3.689557711404782e-06,
      "loss": 1.6353,
      "step": 727500
    },
    {
      "epoch": 55.580169582155676,
      "grad_norm": 5.83621883392334,
      "learning_rate": 3.6831920148702673e-06,
      "loss": 1.6147,
      "step": 727600
    },
    {
      "epoch": 55.587808417997095,
      "grad_norm": 4.433786869049072,
      "learning_rate": 3.676826318335753e-06,
      "loss": 1.6222,
      "step": 727700
    },
    {
      "epoch": 55.595447253838515,
      "grad_norm": 5.510095596313477,
      "learning_rate": 3.6704606218012373e-06,
      "loss": 1.6485,
      "step": 727800
    },
    {
      "epoch": 55.603086089679934,
      "grad_norm": 4.8749799728393555,
      "learning_rate": 3.664094925266723e-06,
      "loss": 1.6735,
      "step": 727900
    },
    {
      "epoch": 55.61072492552135,
      "grad_norm": 5.128319263458252,
      "learning_rate": 3.657729228732208e-06,
      "loss": 1.5941,
      "step": 728000
    },
    {
      "epoch": 55.618363761362765,
      "grad_norm": 6.539769649505615,
      "learning_rate": 3.6513635321976934e-06,
      "loss": 1.6869,
      "step": 728100
    },
    {
      "epoch": 55.626002597204184,
      "grad_norm": 6.640518665313721,
      "learning_rate": 3.6449978356631782e-06,
      "loss": 1.6798,
      "step": 728200
    },
    {
      "epoch": 55.633641433045604,
      "grad_norm": 4.697530269622803,
      "learning_rate": 3.6386321391286635e-06,
      "loss": 1.655,
      "step": 728300
    },
    {
      "epoch": 55.64128026888702,
      "grad_norm": 6.524223327636719,
      "learning_rate": 3.632266442594149e-06,
      "loss": 1.6203,
      "step": 728400
    },
    {
      "epoch": 55.64891910472844,
      "grad_norm": 7.183069229125977,
      "learning_rate": 3.6259007460596343e-06,
      "loss": 1.5282,
      "step": 728500
    },
    {
      "epoch": 55.656557940569854,
      "grad_norm": 7.533753395080566,
      "learning_rate": 3.619535049525119e-06,
      "loss": 1.5938,
      "step": 728600
    },
    {
      "epoch": 55.66419677641127,
      "grad_norm": 7.527103424072266,
      "learning_rate": 3.6131693529906044e-06,
      "loss": 1.5844,
      "step": 728700
    },
    {
      "epoch": 55.67183561225269,
      "grad_norm": 6.378932952880859,
      "learning_rate": 3.6068036564560896e-06,
      "loss": 1.6682,
      "step": 728800
    },
    {
      "epoch": 55.67947444809411,
      "grad_norm": 5.93471097946167,
      "learning_rate": 3.6004379599215753e-06,
      "loss": 1.6458,
      "step": 728900
    },
    {
      "epoch": 55.68711328393553,
      "grad_norm": 5.483569622039795,
      "learning_rate": 3.5940722633870596e-06,
      "loss": 1.6933,
      "step": 729000
    },
    {
      "epoch": 55.69475211977694,
      "grad_norm": 5.086124897003174,
      "learning_rate": 3.5877065668525453e-06,
      "loss": 1.6248,
      "step": 729100
    },
    {
      "epoch": 55.70239095561836,
      "grad_norm": 4.555093765258789,
      "learning_rate": 3.5813408703180305e-06,
      "loss": 1.6824,
      "step": 729200
    },
    {
      "epoch": 55.71002979145978,
      "grad_norm": 5.613065719604492,
      "learning_rate": 3.5749751737835158e-06,
      "loss": 1.5671,
      "step": 729300
    },
    {
      "epoch": 55.7176686273012,
      "grad_norm": 4.165313720703125,
      "learning_rate": 3.5686094772490006e-06,
      "loss": 1.5618,
      "step": 729400
    },
    {
      "epoch": 55.72530746314262,
      "grad_norm": 4.998035907745361,
      "learning_rate": 3.562243780714486e-06,
      "loss": 1.6784,
      "step": 729500
    },
    {
      "epoch": 55.73294629898403,
      "grad_norm": 5.4176130294799805,
      "learning_rate": 3.5558780841799715e-06,
      "loss": 1.6513,
      "step": 729600
    },
    {
      "epoch": 55.74058513482545,
      "grad_norm": 7.729305744171143,
      "learning_rate": 3.5495123876454567e-06,
      "loss": 1.6822,
      "step": 729700
    },
    {
      "epoch": 55.74822397066687,
      "grad_norm": 5.0827412605285645,
      "learning_rate": 3.5431466911109415e-06,
      "loss": 1.7425,
      "step": 729800
    },
    {
      "epoch": 55.75586280650829,
      "grad_norm": 5.260871887207031,
      "learning_rate": 3.5367809945764267e-06,
      "loss": 1.685,
      "step": 729900
    },
    {
      "epoch": 55.76350164234971,
      "grad_norm": 7.745272159576416,
      "learning_rate": 3.530415298041912e-06,
      "loss": 1.8081,
      "step": 730000
    },
    {
      "epoch": 55.77114047819112,
      "grad_norm": 5.43470573425293,
      "learning_rate": 3.5240496015073968e-06,
      "loss": 1.63,
      "step": 730100
    },
    {
      "epoch": 55.77877931403254,
      "grad_norm": 6.6633100509643555,
      "learning_rate": 3.517683904972882e-06,
      "loss": 1.6821,
      "step": 730200
    },
    {
      "epoch": 55.78641814987396,
      "grad_norm": 5.4162139892578125,
      "learning_rate": 3.5113182084383676e-06,
      "loss": 1.6843,
      "step": 730300
    },
    {
      "epoch": 55.79405698571538,
      "grad_norm": 6.017086029052734,
      "learning_rate": 3.504952511903853e-06,
      "loss": 1.6817,
      "step": 730400
    },
    {
      "epoch": 55.8016958215568,
      "grad_norm": 6.0378098487854,
      "learning_rate": 3.4985868153693377e-06,
      "loss": 1.7265,
      "step": 730500
    },
    {
      "epoch": 55.80933465739821,
      "grad_norm": 7.87346887588501,
      "learning_rate": 3.492221118834823e-06,
      "loss": 1.6277,
      "step": 730600
    },
    {
      "epoch": 55.81697349323963,
      "grad_norm": 6.049696922302246,
      "learning_rate": 3.485855422300308e-06,
      "loss": 1.5953,
      "step": 730700
    },
    {
      "epoch": 55.82461232908105,
      "grad_norm": 6.162407875061035,
      "learning_rate": 3.479489725765794e-06,
      "loss": 1.6972,
      "step": 730800
    },
    {
      "epoch": 55.83225116492247,
      "grad_norm": 6.820120334625244,
      "learning_rate": 3.473124029231278e-06,
      "loss": 1.6715,
      "step": 730900
    },
    {
      "epoch": 55.83989000076388,
      "grad_norm": 5.790234088897705,
      "learning_rate": 3.466758332696764e-06,
      "loss": 1.666,
      "step": 731000
    },
    {
      "epoch": 55.8475288366053,
      "grad_norm": 5.308929920196533,
      "learning_rate": 3.460392636162249e-06,
      "loss": 1.6295,
      "step": 731100
    },
    {
      "epoch": 55.85516767244672,
      "grad_norm": 6.671043872833252,
      "learning_rate": 3.4540269396277343e-06,
      "loss": 1.6659,
      "step": 731200
    },
    {
      "epoch": 55.86280650828814,
      "grad_norm": 5.39039421081543,
      "learning_rate": 3.447661243093219e-06,
      "loss": 1.7097,
      "step": 731300
    },
    {
      "epoch": 55.87044534412956,
      "grad_norm": 11.689075469970703,
      "learning_rate": 3.4412955465587043e-06,
      "loss": 1.7104,
      "step": 731400
    },
    {
      "epoch": 55.87808417997097,
      "grad_norm": 6.5910868644714355,
      "learning_rate": 3.43492985002419e-06,
      "loss": 1.6131,
      "step": 731500
    },
    {
      "epoch": 55.88572301581239,
      "grad_norm": 4.700501441955566,
      "learning_rate": 3.4285641534896752e-06,
      "loss": 1.639,
      "step": 731600
    },
    {
      "epoch": 55.89336185165381,
      "grad_norm": 5.774579048156738,
      "learning_rate": 3.42219845695516e-06,
      "loss": 1.7366,
      "step": 731700
    },
    {
      "epoch": 55.90100068749523,
      "grad_norm": 6.023842811584473,
      "learning_rate": 3.4158327604206453e-06,
      "loss": 1.6738,
      "step": 731800
    },
    {
      "epoch": 55.908639523336646,
      "grad_norm": 6.769016265869141,
      "learning_rate": 3.4094670638861305e-06,
      "loss": 1.752,
      "step": 731900
    },
    {
      "epoch": 55.91627835917806,
      "grad_norm": 5.46060037612915,
      "learning_rate": 3.403101367351616e-06,
      "loss": 1.6593,
      "step": 732000
    },
    {
      "epoch": 55.92391719501948,
      "grad_norm": 11.59928035736084,
      "learning_rate": 3.3967356708171005e-06,
      "loss": 1.6204,
      "step": 732100
    },
    {
      "epoch": 55.9315560308609,
      "grad_norm": 5.195320129394531,
      "learning_rate": 3.390369974282586e-06,
      "loss": 1.6962,
      "step": 732200
    },
    {
      "epoch": 55.939194866702316,
      "grad_norm": 4.946882247924805,
      "learning_rate": 3.3840042777480714e-06,
      "loss": 1.6004,
      "step": 732300
    },
    {
      "epoch": 55.946833702543735,
      "grad_norm": 5.2401323318481445,
      "learning_rate": 3.3776385812135566e-06,
      "loss": 1.6129,
      "step": 732400
    },
    {
      "epoch": 55.95447253838515,
      "grad_norm": 7.351644992828369,
      "learning_rate": 3.3712728846790415e-06,
      "loss": 1.6235,
      "step": 732500
    },
    {
      "epoch": 55.962111374226566,
      "grad_norm": 6.116562843322754,
      "learning_rate": 3.3649071881445267e-06,
      "loss": 1.7421,
      "step": 732600
    },
    {
      "epoch": 55.969750210067986,
      "grad_norm": 5.431849956512451,
      "learning_rate": 3.3585414916100123e-06,
      "loss": 1.5943,
      "step": 732700
    },
    {
      "epoch": 55.977389045909405,
      "grad_norm": 6.887990474700928,
      "learning_rate": 3.3521757950754976e-06,
      "loss": 1.6555,
      "step": 732800
    },
    {
      "epoch": 55.985027881750824,
      "grad_norm": 7.183782577514648,
      "learning_rate": 3.3458100985409824e-06,
      "loss": 1.7152,
      "step": 732900
    },
    {
      "epoch": 55.992666717592236,
      "grad_norm": 5.433837413787842,
      "learning_rate": 3.3394444020064676e-06,
      "loss": 1.6799,
      "step": 733000
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.7680532932281494,
      "eval_runtime": 3.022,
      "eval_samples_per_second": 228.325,
      "eval_steps_per_second": 228.325,
      "step": 733096
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.422179102897644,
      "eval_runtime": 57.116,
      "eval_samples_per_second": 229.2,
      "eval_steps_per_second": 229.2,
      "step": 733096
    },
    {
      "epoch": 56.000305553433655,
      "grad_norm": 4.768153190612793,
      "learning_rate": 3.333078705471953e-06,
      "loss": 1.5903,
      "step": 733100
    },
    {
      "epoch": 56.007944389275075,
      "grad_norm": 5.937540054321289,
      "learning_rate": 3.3267130089374385e-06,
      "loss": 1.7527,
      "step": 733200
    },
    {
      "epoch": 56.015583225116494,
      "grad_norm": 6.8082356452941895,
      "learning_rate": 3.320347312402923e-06,
      "loss": 1.6088,
      "step": 733300
    },
    {
      "epoch": 56.02322206095791,
      "grad_norm": 4.720379829406738,
      "learning_rate": 3.3139816158684085e-06,
      "loss": 1.6437,
      "step": 733400
    },
    {
      "epoch": 56.030860896799325,
      "grad_norm": 4.320838451385498,
      "learning_rate": 3.3076159193338938e-06,
      "loss": 1.5671,
      "step": 733500
    },
    {
      "epoch": 56.038499732640744,
      "grad_norm": 5.59210205078125,
      "learning_rate": 3.301250222799379e-06,
      "loss": 1.6507,
      "step": 733600
    },
    {
      "epoch": 56.046138568482164,
      "grad_norm": 5.324517726898193,
      "learning_rate": 3.294884526264864e-06,
      "loss": 1.6117,
      "step": 733700
    },
    {
      "epoch": 56.05377740432358,
      "grad_norm": 4.803516864776611,
      "learning_rate": 3.288518829730349e-06,
      "loss": 1.6678,
      "step": 733800
    },
    {
      "epoch": 56.061416240165,
      "grad_norm": 6.267271518707275,
      "learning_rate": 3.2821531331958347e-06,
      "loss": 1.6637,
      "step": 733900
    },
    {
      "epoch": 56.069055076006414,
      "grad_norm": 4.841135025024414,
      "learning_rate": 3.27578743666132e-06,
      "loss": 1.6851,
      "step": 734000
    },
    {
      "epoch": 56.07669391184783,
      "grad_norm": 5.557135581970215,
      "learning_rate": 3.2694217401268047e-06,
      "loss": 1.6192,
      "step": 734100
    },
    {
      "epoch": 56.08433274768925,
      "grad_norm": 6.57951545715332,
      "learning_rate": 3.26305604359229e-06,
      "loss": 1.6741,
      "step": 734200
    },
    {
      "epoch": 56.09197158353067,
      "grad_norm": 5.440262317657471,
      "learning_rate": 3.256690347057775e-06,
      "loss": 1.6913,
      "step": 734300
    },
    {
      "epoch": 56.09961041937209,
      "grad_norm": 5.715848445892334,
      "learning_rate": 3.250324650523261e-06,
      "loss": 1.6166,
      "step": 734400
    },
    {
      "epoch": 56.1072492552135,
      "grad_norm": 6.674252986907959,
      "learning_rate": 3.2439589539887452e-06,
      "loss": 1.6535,
      "step": 734500
    },
    {
      "epoch": 56.11488809105492,
      "grad_norm": 5.210897922515869,
      "learning_rate": 3.237593257454231e-06,
      "loss": 1.695,
      "step": 734600
    },
    {
      "epoch": 56.12252692689634,
      "grad_norm": 8.4357328414917,
      "learning_rate": 3.231227560919716e-06,
      "loss": 1.6522,
      "step": 734700
    },
    {
      "epoch": 56.13016576273776,
      "grad_norm": 7.063627243041992,
      "learning_rate": 3.2248618643852013e-06,
      "loss": 1.6836,
      "step": 734800
    },
    {
      "epoch": 56.13780459857918,
      "grad_norm": 5.454204082489014,
      "learning_rate": 3.218496167850686e-06,
      "loss": 1.7275,
      "step": 734900
    },
    {
      "epoch": 56.14544343442059,
      "grad_norm": 5.972026348114014,
      "learning_rate": 3.2121304713161714e-06,
      "loss": 1.6652,
      "step": 735000
    },
    {
      "epoch": 56.15308227026201,
      "grad_norm": 4.7065839767456055,
      "learning_rate": 3.205764774781657e-06,
      "loss": 1.6243,
      "step": 735100
    },
    {
      "epoch": 56.16072110610343,
      "grad_norm": 7.711634159088135,
      "learning_rate": 3.1993990782471423e-06,
      "loss": 1.6254,
      "step": 735200
    },
    {
      "epoch": 56.16835994194485,
      "grad_norm": 6.901363849639893,
      "learning_rate": 3.193033381712627e-06,
      "loss": 1.5975,
      "step": 735300
    },
    {
      "epoch": 56.17599877778626,
      "grad_norm": 6.481245040893555,
      "learning_rate": 3.1866676851781123e-06,
      "loss": 1.6579,
      "step": 735400
    },
    {
      "epoch": 56.18363761362768,
      "grad_norm": 6.052633762359619,
      "learning_rate": 3.1803019886435975e-06,
      "loss": 1.6231,
      "step": 735500
    },
    {
      "epoch": 56.1912764494691,
      "grad_norm": 4.352400302886963,
      "learning_rate": 3.173936292109083e-06,
      "loss": 1.5889,
      "step": 735600
    },
    {
      "epoch": 56.19891528531052,
      "grad_norm": 6.198676109313965,
      "learning_rate": 3.1675705955745676e-06,
      "loss": 1.6667,
      "step": 735700
    },
    {
      "epoch": 56.20655412115194,
      "grad_norm": 4.664207458496094,
      "learning_rate": 3.1612048990400532e-06,
      "loss": 1.6612,
      "step": 735800
    },
    {
      "epoch": 56.21419295699335,
      "grad_norm": 5.129711627960205,
      "learning_rate": 3.1548392025055384e-06,
      "loss": 1.7219,
      "step": 735900
    },
    {
      "epoch": 56.22183179283477,
      "grad_norm": 5.769312858581543,
      "learning_rate": 3.1484735059710237e-06,
      "loss": 1.5991,
      "step": 736000
    },
    {
      "epoch": 56.22947062867619,
      "grad_norm": 4.761446952819824,
      "learning_rate": 3.1421078094365085e-06,
      "loss": 1.6402,
      "step": 736100
    },
    {
      "epoch": 56.23710946451761,
      "grad_norm": 4.34386682510376,
      "learning_rate": 3.1357421129019937e-06,
      "loss": 1.6299,
      "step": 736200
    },
    {
      "epoch": 56.24474830035903,
      "grad_norm": 5.989083766937256,
      "learning_rate": 3.1293764163674794e-06,
      "loss": 1.6694,
      "step": 736300
    },
    {
      "epoch": 56.25238713620044,
      "grad_norm": 5.513697147369385,
      "learning_rate": 3.123010719832964e-06,
      "loss": 1.5799,
      "step": 736400
    },
    {
      "epoch": 56.26002597204186,
      "grad_norm": 5.5183587074279785,
      "learning_rate": 3.1166450232984494e-06,
      "loss": 1.6758,
      "step": 736500
    },
    {
      "epoch": 56.26766480788328,
      "grad_norm": 5.217335224151611,
      "learning_rate": 3.1102793267639346e-06,
      "loss": 1.652,
      "step": 736600
    },
    {
      "epoch": 56.2753036437247,
      "grad_norm": 6.057732582092285,
      "learning_rate": 3.10391363022942e-06,
      "loss": 1.6849,
      "step": 736700
    },
    {
      "epoch": 56.28294247956612,
      "grad_norm": 5.138778209686279,
      "learning_rate": 3.097547933694905e-06,
      "loss": 1.658,
      "step": 736800
    },
    {
      "epoch": 56.29058131540753,
      "grad_norm": 9.265316009521484,
      "learning_rate": 3.0911822371603903e-06,
      "loss": 1.5809,
      "step": 736900
    },
    {
      "epoch": 56.29822015124895,
      "grad_norm": 6.707977771759033,
      "learning_rate": 3.0848165406258756e-06,
      "loss": 1.7035,
      "step": 737000
    },
    {
      "epoch": 56.30585898709037,
      "grad_norm": 3.6922874450683594,
      "learning_rate": 3.078450844091361e-06,
      "loss": 1.7059,
      "step": 737100
    },
    {
      "epoch": 56.31349782293179,
      "grad_norm": 5.051612377166748,
      "learning_rate": 3.0720851475568456e-06,
      "loss": 1.6627,
      "step": 737200
    },
    {
      "epoch": 56.321136658773206,
      "grad_norm": 6.239876747131348,
      "learning_rate": 3.0657194510223313e-06,
      "loss": 1.639,
      "step": 737300
    },
    {
      "epoch": 56.32877549461462,
      "grad_norm": 8.006744384765625,
      "learning_rate": 3.059353754487816e-06,
      "loss": 1.6899,
      "step": 737400
    },
    {
      "epoch": 56.33641433045604,
      "grad_norm": 6.023315906524658,
      "learning_rate": 3.0529880579533017e-06,
      "loss": 1.7252,
      "step": 737500
    },
    {
      "epoch": 56.344053166297456,
      "grad_norm": 5.516744613647461,
      "learning_rate": 3.0466223614187865e-06,
      "loss": 1.6505,
      "step": 737600
    },
    {
      "epoch": 56.351692002138876,
      "grad_norm": 6.4368896484375,
      "learning_rate": 3.0402566648842718e-06,
      "loss": 1.6411,
      "step": 737700
    },
    {
      "epoch": 56.359330837980295,
      "grad_norm": 6.0037126541137695,
      "learning_rate": 3.033890968349757e-06,
      "loss": 1.6688,
      "step": 737800
    },
    {
      "epoch": 56.36696967382171,
      "grad_norm": 5.238569259643555,
      "learning_rate": 3.0275252718152422e-06,
      "loss": 1.6442,
      "step": 737900
    },
    {
      "epoch": 56.374608509663126,
      "grad_norm": 5.804466247558594,
      "learning_rate": 3.0211595752807274e-06,
      "loss": 1.6409,
      "step": 738000
    },
    {
      "epoch": 56.382247345504545,
      "grad_norm": 5.204783916473389,
      "learning_rate": 3.0147938787462123e-06,
      "loss": 1.5893,
      "step": 738100
    },
    {
      "epoch": 56.389886181345965,
      "grad_norm": 8.77600383758545,
      "learning_rate": 3.008428182211698e-06,
      "loss": 1.7239,
      "step": 738200
    },
    {
      "epoch": 56.397525017187384,
      "grad_norm": 5.36644983291626,
      "learning_rate": 3.0020624856771827e-06,
      "loss": 1.7316,
      "step": 738300
    },
    {
      "epoch": 56.405163853028796,
      "grad_norm": 5.304215431213379,
      "learning_rate": 2.995696789142668e-06,
      "loss": 1.7046,
      "step": 738400
    },
    {
      "epoch": 56.412802688870215,
      "grad_norm": 6.363797187805176,
      "learning_rate": 2.989331092608153e-06,
      "loss": 1.6846,
      "step": 738500
    },
    {
      "epoch": 56.420441524711634,
      "grad_norm": 6.072504043579102,
      "learning_rate": 2.9829653960736384e-06,
      "loss": 1.7629,
      "step": 738600
    },
    {
      "epoch": 56.428080360553054,
      "grad_norm": 6.394786834716797,
      "learning_rate": 2.9765996995391236e-06,
      "loss": 1.6166,
      "step": 738700
    },
    {
      "epoch": 56.43571919639447,
      "grad_norm": 5.559258937835693,
      "learning_rate": 2.970234003004609e-06,
      "loss": 1.6385,
      "step": 738800
    },
    {
      "epoch": 56.443358032235885,
      "grad_norm": 5.264664649963379,
      "learning_rate": 2.963868306470094e-06,
      "loss": 1.6587,
      "step": 738900
    },
    {
      "epoch": 56.450996868077304,
      "grad_norm": 4.24843692779541,
      "learning_rate": 2.9575026099355793e-06,
      "loss": 1.6844,
      "step": 739000
    },
    {
      "epoch": 56.45863570391872,
      "grad_norm": 6.821393966674805,
      "learning_rate": 2.951136913401064e-06,
      "loss": 1.7097,
      "step": 739100
    },
    {
      "epoch": 56.46627453976014,
      "grad_norm": 5.175167083740234,
      "learning_rate": 2.94477121686655e-06,
      "loss": 1.6222,
      "step": 739200
    },
    {
      "epoch": 56.473913375601555,
      "grad_norm": 6.768109321594238,
      "learning_rate": 2.9384055203320346e-06,
      "loss": 1.6886,
      "step": 739300
    },
    {
      "epoch": 56.481552211442974,
      "grad_norm": 7.5719380378723145,
      "learning_rate": 2.9320398237975203e-06,
      "loss": 1.7538,
      "step": 739400
    },
    {
      "epoch": 56.48919104728439,
      "grad_norm": 3.422048807144165,
      "learning_rate": 2.925674127263005e-06,
      "loss": 1.7039,
      "step": 739500
    },
    {
      "epoch": 56.49682988312581,
      "grad_norm": 6.912776470184326,
      "learning_rate": 2.9193084307284903e-06,
      "loss": 1.6665,
      "step": 739600
    },
    {
      "epoch": 56.50446871896723,
      "grad_norm": 6.197654724121094,
      "learning_rate": 2.9129427341939755e-06,
      "loss": 1.6158,
      "step": 739700
    },
    {
      "epoch": 56.512107554808644,
      "grad_norm": 5.906466960906982,
      "learning_rate": 2.9065770376594608e-06,
      "loss": 1.7122,
      "step": 739800
    },
    {
      "epoch": 56.51974639065006,
      "grad_norm": 4.189670085906982,
      "learning_rate": 2.900211341124946e-06,
      "loss": 1.6696,
      "step": 739900
    },
    {
      "epoch": 56.52738522649148,
      "grad_norm": 6.28016471862793,
      "learning_rate": 2.8938456445904312e-06,
      "loss": 1.6779,
      "step": 740000
    },
    {
      "epoch": 56.5350240623329,
      "grad_norm": 6.1137614250183105,
      "learning_rate": 2.8874799480559164e-06,
      "loss": 1.6579,
      "step": 740100
    },
    {
      "epoch": 56.54266289817432,
      "grad_norm": 5.211909770965576,
      "learning_rate": 2.8811142515214017e-06,
      "loss": 1.7231,
      "step": 740200
    },
    {
      "epoch": 56.55030173401573,
      "grad_norm": 6.315410614013672,
      "learning_rate": 2.8747485549868865e-06,
      "loss": 1.5847,
      "step": 740300
    },
    {
      "epoch": 56.55794056985715,
      "grad_norm": 6.446758270263672,
      "learning_rate": 2.868382858452372e-06,
      "loss": 1.631,
      "step": 740400
    },
    {
      "epoch": 56.56557940569857,
      "grad_norm": 4.962820053100586,
      "learning_rate": 2.862017161917857e-06,
      "loss": 1.6669,
      "step": 740500
    },
    {
      "epoch": 56.57321824153999,
      "grad_norm": 6.11265230178833,
      "learning_rate": 2.8556514653833426e-06,
      "loss": 1.663,
      "step": 740600
    },
    {
      "epoch": 56.58085707738141,
      "grad_norm": 5.133054733276367,
      "learning_rate": 2.8492857688488274e-06,
      "loss": 1.6986,
      "step": 740700
    },
    {
      "epoch": 56.58849591322282,
      "grad_norm": 4.753196716308594,
      "learning_rate": 2.8429200723143126e-06,
      "loss": 1.6376,
      "step": 740800
    },
    {
      "epoch": 56.59613474906424,
      "grad_norm": 5.642092227935791,
      "learning_rate": 2.836554375779798e-06,
      "loss": 1.7031,
      "step": 740900
    },
    {
      "epoch": 56.60377358490566,
      "grad_norm": 4.890718460083008,
      "learning_rate": 2.830188679245283e-06,
      "loss": 1.635,
      "step": 741000
    },
    {
      "epoch": 56.61141242074708,
      "grad_norm": 9.723201751708984,
      "learning_rate": 2.8238229827107683e-06,
      "loss": 1.6809,
      "step": 741100
    },
    {
      "epoch": 56.6190512565885,
      "grad_norm": 9.399515151977539,
      "learning_rate": 2.8174572861762536e-06,
      "loss": 1.6979,
      "step": 741200
    },
    {
      "epoch": 56.62669009242991,
      "grad_norm": 5.85469913482666,
      "learning_rate": 2.811091589641739e-06,
      "loss": 1.5958,
      "step": 741300
    },
    {
      "epoch": 56.63432892827133,
      "grad_norm": 5.457273006439209,
      "learning_rate": 2.804725893107224e-06,
      "loss": 1.6743,
      "step": 741400
    },
    {
      "epoch": 56.64196776411275,
      "grad_norm": 6.439059257507324,
      "learning_rate": 2.798360196572709e-06,
      "loss": 1.6814,
      "step": 741500
    },
    {
      "epoch": 56.64960659995417,
      "grad_norm": 6.019657135009766,
      "learning_rate": 2.7919945000381945e-06,
      "loss": 1.655,
      "step": 741600
    },
    {
      "epoch": 56.65724543579559,
      "grad_norm": 5.71403169631958,
      "learning_rate": 2.7856288035036793e-06,
      "loss": 1.7133,
      "step": 741700
    },
    {
      "epoch": 56.664884271637,
      "grad_norm": 6.027581214904785,
      "learning_rate": 2.779263106969165e-06,
      "loss": 1.6103,
      "step": 741800
    },
    {
      "epoch": 56.67252310747842,
      "grad_norm": 6.914031982421875,
      "learning_rate": 2.7728974104346498e-06,
      "loss": 1.6358,
      "step": 741900
    },
    {
      "epoch": 56.68016194331984,
      "grad_norm": 4.652510166168213,
      "learning_rate": 2.766531713900135e-06,
      "loss": 1.6751,
      "step": 742000
    },
    {
      "epoch": 56.68780077916126,
      "grad_norm": 4.87141227722168,
      "learning_rate": 2.7601660173656202e-06,
      "loss": 1.5676,
      "step": 742100
    },
    {
      "epoch": 56.69543961500268,
      "grad_norm": 4.553814888000488,
      "learning_rate": 2.7538003208311054e-06,
      "loss": 1.7349,
      "step": 742200
    },
    {
      "epoch": 56.70307845084409,
      "grad_norm": 6.703137397766113,
      "learning_rate": 2.7474346242965907e-06,
      "loss": 1.6564,
      "step": 742300
    },
    {
      "epoch": 56.71071728668551,
      "grad_norm": 5.623235702514648,
      "learning_rate": 2.741068927762076e-06,
      "loss": 1.7041,
      "step": 742400
    },
    {
      "epoch": 56.71835612252693,
      "grad_norm": 6.038280010223389,
      "learning_rate": 2.734703231227561e-06,
      "loss": 1.5702,
      "step": 742500
    },
    {
      "epoch": 56.72599495836835,
      "grad_norm": 7.1913957595825195,
      "learning_rate": 2.7283375346930464e-06,
      "loss": 1.6293,
      "step": 742600
    },
    {
      "epoch": 56.733633794209766,
      "grad_norm": 6.831264495849609,
      "learning_rate": 2.721971838158531e-06,
      "loss": 1.5089,
      "step": 742700
    },
    {
      "epoch": 56.74127263005118,
      "grad_norm": 10.142657279968262,
      "learning_rate": 2.715606141624017e-06,
      "loss": 1.5759,
      "step": 742800
    },
    {
      "epoch": 56.7489114658926,
      "grad_norm": 6.318375587463379,
      "learning_rate": 2.7092404450895016e-06,
      "loss": 1.7346,
      "step": 742900
    },
    {
      "epoch": 56.756550301734016,
      "grad_norm": 4.736691474914551,
      "learning_rate": 2.7028747485549873e-06,
      "loss": 1.6415,
      "step": 743000
    },
    {
      "epoch": 56.764189137575435,
      "grad_norm": 7.052783966064453,
      "learning_rate": 2.696509052020472e-06,
      "loss": 1.6644,
      "step": 743100
    },
    {
      "epoch": 56.771827973416855,
      "grad_norm": 6.002240180969238,
      "learning_rate": 2.6901433554859573e-06,
      "loss": 1.5969,
      "step": 743200
    },
    {
      "epoch": 56.77946680925827,
      "grad_norm": 4.814122676849365,
      "learning_rate": 2.6837776589514426e-06,
      "loss": 1.6079,
      "step": 743300
    },
    {
      "epoch": 56.787105645099686,
      "grad_norm": 4.480914115905762,
      "learning_rate": 2.677411962416928e-06,
      "loss": 1.6418,
      "step": 743400
    },
    {
      "epoch": 56.794744480941105,
      "grad_norm": 7.300108432769775,
      "learning_rate": 2.671046265882413e-06,
      "loss": 1.6071,
      "step": 743500
    },
    {
      "epoch": 56.802383316782525,
      "grad_norm": 5.852034091949463,
      "learning_rate": 2.6646805693478983e-06,
      "loss": 1.6363,
      "step": 743600
    },
    {
      "epoch": 56.81002215262394,
      "grad_norm": 4.4145050048828125,
      "learning_rate": 2.6583148728133835e-06,
      "loss": 1.6857,
      "step": 743700
    },
    {
      "epoch": 56.817660988465356,
      "grad_norm": 3.702946901321411,
      "learning_rate": 2.6519491762788687e-06,
      "loss": 1.7249,
      "step": 743800
    },
    {
      "epoch": 56.825299824306775,
      "grad_norm": 7.375652313232422,
      "learning_rate": 2.6455834797443535e-06,
      "loss": 1.6992,
      "step": 743900
    },
    {
      "epoch": 56.832938660148194,
      "grad_norm": 5.54459810256958,
      "learning_rate": 2.639217783209839e-06,
      "loss": 1.6809,
      "step": 744000
    },
    {
      "epoch": 56.84057749598961,
      "grad_norm": 3.867820978164673,
      "learning_rate": 2.632852086675324e-06,
      "loss": 1.7041,
      "step": 744100
    },
    {
      "epoch": 56.848216331831026,
      "grad_norm": 5.820812702178955,
      "learning_rate": 2.6264863901408096e-06,
      "loss": 1.5567,
      "step": 744200
    },
    {
      "epoch": 56.855855167672445,
      "grad_norm": 4.780124187469482,
      "learning_rate": 2.6201206936062944e-06,
      "loss": 1.6284,
      "step": 744300
    },
    {
      "epoch": 56.863494003513864,
      "grad_norm": 5.217427730560303,
      "learning_rate": 2.6137549970717797e-06,
      "loss": 1.7922,
      "step": 744400
    },
    {
      "epoch": 56.87113283935528,
      "grad_norm": 5.717187881469727,
      "learning_rate": 2.607389300537265e-06,
      "loss": 1.6828,
      "step": 744500
    },
    {
      "epoch": 56.8787716751967,
      "grad_norm": 5.112090110778809,
      "learning_rate": 2.60102360400275e-06,
      "loss": 1.6758,
      "step": 744600
    },
    {
      "epoch": 56.886410511038115,
      "grad_norm": 6.057891845703125,
      "learning_rate": 2.5946579074682354e-06,
      "loss": 1.7441,
      "step": 744700
    },
    {
      "epoch": 56.894049346879534,
      "grad_norm": 3.86385178565979,
      "learning_rate": 2.5882922109337206e-06,
      "loss": 1.6449,
      "step": 744800
    },
    {
      "epoch": 56.90168818272095,
      "grad_norm": 5.174170970916748,
      "learning_rate": 2.581926514399206e-06,
      "loss": 1.6861,
      "step": 744900
    },
    {
      "epoch": 56.90932701856237,
      "grad_norm": 6.481746196746826,
      "learning_rate": 2.575560817864691e-06,
      "loss": 1.7064,
      "step": 745000
    },
    {
      "epoch": 56.91696585440379,
      "grad_norm": 4.823950290679932,
      "learning_rate": 2.569195121330176e-06,
      "loss": 1.6366,
      "step": 745100
    },
    {
      "epoch": 56.924604690245204,
      "grad_norm": 6.732917785644531,
      "learning_rate": 2.5628294247956615e-06,
      "loss": 1.5372,
      "step": 745200
    },
    {
      "epoch": 56.93224352608662,
      "grad_norm": 6.894320487976074,
      "learning_rate": 2.5564637282611463e-06,
      "loss": 1.6636,
      "step": 745300
    },
    {
      "epoch": 56.93988236192804,
      "grad_norm": 5.166409969329834,
      "learning_rate": 2.550098031726632e-06,
      "loss": 1.6847,
      "step": 745400
    },
    {
      "epoch": 56.94752119776946,
      "grad_norm": 6.531200408935547,
      "learning_rate": 2.5437323351921168e-06,
      "loss": 1.6088,
      "step": 745500
    },
    {
      "epoch": 56.95516003361088,
      "grad_norm": 7.507292747497559,
      "learning_rate": 2.537366638657602e-06,
      "loss": 1.704,
      "step": 745600
    },
    {
      "epoch": 56.96279886945229,
      "grad_norm": 4.430426120758057,
      "learning_rate": 2.5310009421230872e-06,
      "loss": 1.6552,
      "step": 745700
    },
    {
      "epoch": 56.97043770529371,
      "grad_norm": 7.077213287353516,
      "learning_rate": 2.5246352455885725e-06,
      "loss": 1.6831,
      "step": 745800
    },
    {
      "epoch": 56.97807654113513,
      "grad_norm": 6.49446439743042,
      "learning_rate": 2.5182695490540577e-06,
      "loss": 1.6093,
      "step": 745900
    },
    {
      "epoch": 56.98571537697655,
      "grad_norm": 6.144032955169678,
      "learning_rate": 2.511903852519543e-06,
      "loss": 1.6103,
      "step": 746000
    },
    {
      "epoch": 56.99335421281797,
      "grad_norm": 4.5679850578308105,
      "learning_rate": 2.505538155985028e-06,
      "loss": 1.6366,
      "step": 746100
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.7655606269836426,
      "eval_runtime": 3.0174,
      "eval_samples_per_second": 228.67,
      "eval_steps_per_second": 228.67,
      "step": 746187
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.4200043678283691,
      "eval_runtime": 57.5853,
      "eval_samples_per_second": 227.332,
      "eval_steps_per_second": 227.332,
      "step": 746187
    },
    {
      "epoch": 57.00099304865938,
      "grad_norm": 5.461700439453125,
      "learning_rate": 2.499172459450513e-06,
      "loss": 1.6873,
      "step": 746200
    },
    {
      "epoch": 57.0086318845008,
      "grad_norm": 5.773086071014404,
      "learning_rate": 2.492806762915998e-06,
      "loss": 1.6264,
      "step": 746300
    },
    {
      "epoch": 57.01627072034222,
      "grad_norm": 5.1888556480407715,
      "learning_rate": 2.4864410663814834e-06,
      "loss": 1.6343,
      "step": 746400
    },
    {
      "epoch": 57.02390955618364,
      "grad_norm": 11.727595329284668,
      "learning_rate": 2.4800753698469687e-06,
      "loss": 1.714,
      "step": 746500
    },
    {
      "epoch": 57.03154839202506,
      "grad_norm": 5.720734596252441,
      "learning_rate": 2.473709673312454e-06,
      "loss": 1.6086,
      "step": 746600
    },
    {
      "epoch": 57.03918722786647,
      "grad_norm": 7.202116966247559,
      "learning_rate": 2.467343976777939e-06,
      "loss": 1.5464,
      "step": 746700
    },
    {
      "epoch": 57.04682606370789,
      "grad_norm": 5.234673023223877,
      "learning_rate": 2.4609782802434244e-06,
      "loss": 1.6631,
      "step": 746800
    },
    {
      "epoch": 57.05446489954931,
      "grad_norm": 5.609129428863525,
      "learning_rate": 2.4546125837089096e-06,
      "loss": 1.6672,
      "step": 746900
    },
    {
      "epoch": 57.06210373539073,
      "grad_norm": 6.151725769042969,
      "learning_rate": 2.4482468871743944e-06,
      "loss": 1.6981,
      "step": 747000
    },
    {
      "epoch": 57.06974257123215,
      "grad_norm": 7.660589218139648,
      "learning_rate": 2.44188119063988e-06,
      "loss": 1.6575,
      "step": 747100
    },
    {
      "epoch": 57.07738140707356,
      "grad_norm": 5.028860569000244,
      "learning_rate": 2.435515494105365e-06,
      "loss": 1.6719,
      "step": 747200
    },
    {
      "epoch": 57.08502024291498,
      "grad_norm": 6.1495680809021,
      "learning_rate": 2.4291497975708505e-06,
      "loss": 1.6934,
      "step": 747300
    },
    {
      "epoch": 57.0926590787564,
      "grad_norm": 5.908535480499268,
      "learning_rate": 2.4227841010363353e-06,
      "loss": 1.6883,
      "step": 747400
    },
    {
      "epoch": 57.10029791459782,
      "grad_norm": 6.318461894989014,
      "learning_rate": 2.4164184045018206e-06,
      "loss": 1.6199,
      "step": 747500
    },
    {
      "epoch": 57.10793675043924,
      "grad_norm": 7.29660701751709,
      "learning_rate": 2.4100527079673058e-06,
      "loss": 1.7273,
      "step": 747600
    },
    {
      "epoch": 57.11557558628065,
      "grad_norm": 8.370750427246094,
      "learning_rate": 2.403687011432791e-06,
      "loss": 1.5721,
      "step": 747700
    },
    {
      "epoch": 57.12321442212207,
      "grad_norm": 6.844699382781982,
      "learning_rate": 2.3973213148982762e-06,
      "loss": 1.6378,
      "step": 747800
    },
    {
      "epoch": 57.13085325796349,
      "grad_norm": 6.0894999504089355,
      "learning_rate": 2.3909556183637615e-06,
      "loss": 1.5819,
      "step": 747900
    },
    {
      "epoch": 57.138492093804906,
      "grad_norm": 4.965878009796143,
      "learning_rate": 2.3845899218292467e-06,
      "loss": 1.6962,
      "step": 748000
    },
    {
      "epoch": 57.14613092964632,
      "grad_norm": 6.266096591949463,
      "learning_rate": 2.378224225294732e-06,
      "loss": 1.6656,
      "step": 748100
    },
    {
      "epoch": 57.15376976548774,
      "grad_norm": 4.792496681213379,
      "learning_rate": 2.3718585287602167e-06,
      "loss": 1.6394,
      "step": 748200
    },
    {
      "epoch": 57.16140860132916,
      "grad_norm": 7.293784141540527,
      "learning_rate": 2.3654928322257024e-06,
      "loss": 1.7441,
      "step": 748300
    },
    {
      "epoch": 57.169047437170576,
      "grad_norm": 4.995838642120361,
      "learning_rate": 2.359127135691187e-06,
      "loss": 1.5567,
      "step": 748400
    },
    {
      "epoch": 57.176686273011995,
      "grad_norm": 6.641321182250977,
      "learning_rate": 2.352761439156673e-06,
      "loss": 1.6706,
      "step": 748500
    },
    {
      "epoch": 57.18432510885341,
      "grad_norm": 6.095525741577148,
      "learning_rate": 2.3463957426221577e-06,
      "loss": 1.6135,
      "step": 748600
    },
    {
      "epoch": 57.19196394469483,
      "grad_norm": 4.573644638061523,
      "learning_rate": 2.340030046087643e-06,
      "loss": 1.7109,
      "step": 748700
    },
    {
      "epoch": 57.199602780536246,
      "grad_norm": 5.635073184967041,
      "learning_rate": 2.333664349553128e-06,
      "loss": 1.6904,
      "step": 748800
    },
    {
      "epoch": 57.207241616377665,
      "grad_norm": 5.279994964599609,
      "learning_rate": 2.3272986530186134e-06,
      "loss": 1.6054,
      "step": 748900
    },
    {
      "epoch": 57.214880452219084,
      "grad_norm": 5.403153419494629,
      "learning_rate": 2.3209329564840986e-06,
      "loss": 1.729,
      "step": 749000
    },
    {
      "epoch": 57.2225192880605,
      "grad_norm": 5.7006330490112305,
      "learning_rate": 2.314567259949584e-06,
      "loss": 1.6649,
      "step": 749100
    },
    {
      "epoch": 57.230158123901916,
      "grad_norm": 6.894267559051514,
      "learning_rate": 2.308201563415069e-06,
      "loss": 1.6469,
      "step": 749200
    },
    {
      "epoch": 57.237796959743335,
      "grad_norm": 6.288120746612549,
      "learning_rate": 2.3018358668805543e-06,
      "loss": 1.6945,
      "step": 749300
    },
    {
      "epoch": 57.245435795584754,
      "grad_norm": 9.920001029968262,
      "learning_rate": 2.295470170346039e-06,
      "loss": 1.665,
      "step": 749400
    },
    {
      "epoch": 57.25307463142617,
      "grad_norm": 5.033909320831299,
      "learning_rate": 2.2891044738115247e-06,
      "loss": 1.6146,
      "step": 749500
    },
    {
      "epoch": 57.260713467267585,
      "grad_norm": 5.86557674407959,
      "learning_rate": 2.2827387772770096e-06,
      "loss": 1.5353,
      "step": 749600
    },
    {
      "epoch": 57.268352303109005,
      "grad_norm": 6.867630958557129,
      "learning_rate": 2.276373080742495e-06,
      "loss": 1.7504,
      "step": 749700
    },
    {
      "epoch": 57.275991138950424,
      "grad_norm": 9.021961212158203,
      "learning_rate": 2.27000738420798e-06,
      "loss": 1.6925,
      "step": 749800
    },
    {
      "epoch": 57.28362997479184,
      "grad_norm": 4.596912384033203,
      "learning_rate": 2.2636416876734652e-06,
      "loss": 1.642,
      "step": 749900
    },
    {
      "epoch": 57.29126881063326,
      "grad_norm": 5.419638633728027,
      "learning_rate": 2.2572759911389505e-06,
      "loss": 1.691,
      "step": 750000
    },
    {
      "epoch": 57.298907646474674,
      "grad_norm": 8.094590187072754,
      "learning_rate": 2.2509102946044357e-06,
      "loss": 1.7561,
      "step": 750100
    },
    {
      "epoch": 57.306546482316094,
      "grad_norm": 5.151345729827881,
      "learning_rate": 2.244544598069921e-06,
      "loss": 1.619,
      "step": 750200
    },
    {
      "epoch": 57.31418531815751,
      "grad_norm": 6.789736270904541,
      "learning_rate": 2.238178901535406e-06,
      "loss": 1.6652,
      "step": 750300
    },
    {
      "epoch": 57.32182415399893,
      "grad_norm": 5.633062362670898,
      "learning_rate": 2.2318132050008914e-06,
      "loss": 1.638,
      "step": 750400
    },
    {
      "epoch": 57.32946298984035,
      "grad_norm": 6.802534103393555,
      "learning_rate": 2.2254475084663766e-06,
      "loss": 1.5583,
      "step": 750500
    },
    {
      "epoch": 57.33710182568176,
      "grad_norm": 3.9357874393463135,
      "learning_rate": 2.2190818119318614e-06,
      "loss": 1.6335,
      "step": 750600
    },
    {
      "epoch": 57.34474066152318,
      "grad_norm": 5.472509384155273,
      "learning_rate": 2.212716115397347e-06,
      "loss": 1.572,
      "step": 750700
    },
    {
      "epoch": 57.3523794973646,
      "grad_norm": 6.186545372009277,
      "learning_rate": 2.206350418862832e-06,
      "loss": 1.7032,
      "step": 750800
    },
    {
      "epoch": 57.36001833320602,
      "grad_norm": 7.072136402130127,
      "learning_rate": 2.1999847223283176e-06,
      "loss": 1.6395,
      "step": 750900
    },
    {
      "epoch": 57.36765716904744,
      "grad_norm": 7.440654277801514,
      "learning_rate": 2.1936190257938024e-06,
      "loss": 1.7263,
      "step": 751000
    },
    {
      "epoch": 57.37529600488885,
      "grad_norm": 6.9363837242126465,
      "learning_rate": 2.1872533292592876e-06,
      "loss": 1.6715,
      "step": 751100
    },
    {
      "epoch": 57.38293484073027,
      "grad_norm": 5.710132122039795,
      "learning_rate": 2.180887632724773e-06,
      "loss": 1.6366,
      "step": 751200
    },
    {
      "epoch": 57.39057367657169,
      "grad_norm": 4.929948806762695,
      "learning_rate": 2.174521936190258e-06,
      "loss": 1.5906,
      "step": 751300
    },
    {
      "epoch": 57.39821251241311,
      "grad_norm": 6.812678813934326,
      "learning_rate": 2.1681562396557433e-06,
      "loss": 1.6568,
      "step": 751400
    },
    {
      "epoch": 57.40585134825453,
      "grad_norm": 5.227017402648926,
      "learning_rate": 2.1617905431212285e-06,
      "loss": 1.6719,
      "step": 751500
    },
    {
      "epoch": 57.41349018409594,
      "grad_norm": 5.337823867797852,
      "learning_rate": 2.1554248465867137e-06,
      "loss": 1.6892,
      "step": 751600
    },
    {
      "epoch": 57.42112901993736,
      "grad_norm": 6.837067604064941,
      "learning_rate": 2.149059150052199e-06,
      "loss": 1.6432,
      "step": 751700
    },
    {
      "epoch": 57.42876785577878,
      "grad_norm": 5.332702159881592,
      "learning_rate": 2.1426934535176838e-06,
      "loss": 1.6886,
      "step": 751800
    },
    {
      "epoch": 57.4364066916202,
      "grad_norm": 6.238533973693848,
      "learning_rate": 2.1363277569831694e-06,
      "loss": 1.669,
      "step": 751900
    },
    {
      "epoch": 57.44404552746161,
      "grad_norm": 4.779510974884033,
      "learning_rate": 2.1299620604486542e-06,
      "loss": 1.6891,
      "step": 752000
    },
    {
      "epoch": 57.45168436330303,
      "grad_norm": 5.484791278839111,
      "learning_rate": 2.12359636391414e-06,
      "loss": 1.6804,
      "step": 752100
    },
    {
      "epoch": 57.45932319914445,
      "grad_norm": 6.2871198654174805,
      "learning_rate": 2.1172306673796247e-06,
      "loss": 1.6396,
      "step": 752200
    },
    {
      "epoch": 57.46696203498587,
      "grad_norm": 5.178253650665283,
      "learning_rate": 2.11086497084511e-06,
      "loss": 1.7185,
      "step": 752300
    },
    {
      "epoch": 57.47460087082729,
      "grad_norm": 5.111154079437256,
      "learning_rate": 2.104499274310595e-06,
      "loss": 1.6385,
      "step": 752400
    },
    {
      "epoch": 57.4822397066687,
      "grad_norm": 7.260732173919678,
      "learning_rate": 2.0981335777760804e-06,
      "loss": 1.7656,
      "step": 752500
    },
    {
      "epoch": 57.48987854251012,
      "grad_norm": 6.15248966217041,
      "learning_rate": 2.0917678812415656e-06,
      "loss": 1.6606,
      "step": 752600
    },
    {
      "epoch": 57.49751737835154,
      "grad_norm": 6.224547863006592,
      "learning_rate": 2.085402184707051e-06,
      "loss": 1.6488,
      "step": 752700
    },
    {
      "epoch": 57.50515621419296,
      "grad_norm": 7.035947799682617,
      "learning_rate": 2.079036488172536e-06,
      "loss": 1.6763,
      "step": 752800
    },
    {
      "epoch": 57.51279505003438,
      "grad_norm": 5.800795555114746,
      "learning_rate": 2.0726707916380213e-06,
      "loss": 1.6471,
      "step": 752900
    },
    {
      "epoch": 57.52043388587579,
      "grad_norm": 5.009486198425293,
      "learning_rate": 2.066305095103506e-06,
      "loss": 1.7615,
      "step": 753000
    },
    {
      "epoch": 57.52807272171721,
      "grad_norm": 5.330407619476318,
      "learning_rate": 2.0599393985689918e-06,
      "loss": 1.573,
      "step": 753100
    },
    {
      "epoch": 57.53571155755863,
      "grad_norm": 5.09058141708374,
      "learning_rate": 2.0535737020344766e-06,
      "loss": 1.7047,
      "step": 753200
    },
    {
      "epoch": 57.54335039340005,
      "grad_norm": 5.2063751220703125,
      "learning_rate": 2.0472080054999622e-06,
      "loss": 1.6632,
      "step": 753300
    },
    {
      "epoch": 57.550989229241466,
      "grad_norm": 5.498964309692383,
      "learning_rate": 2.040842308965447e-06,
      "loss": 1.6719,
      "step": 753400
    },
    {
      "epoch": 57.55862806508288,
      "grad_norm": 6.638993263244629,
      "learning_rate": 2.0344766124309323e-06,
      "loss": 1.6894,
      "step": 753500
    },
    {
      "epoch": 57.5662669009243,
      "grad_norm": 7.125654220581055,
      "learning_rate": 2.0281109158964175e-06,
      "loss": 1.7069,
      "step": 753600
    },
    {
      "epoch": 57.57390573676572,
      "grad_norm": 5.4990153312683105,
      "learning_rate": 2.0217452193619027e-06,
      "loss": 1.6619,
      "step": 753700
    },
    {
      "epoch": 57.581544572607136,
      "grad_norm": 5.344854831695557,
      "learning_rate": 2.015379522827388e-06,
      "loss": 1.5737,
      "step": 753800
    },
    {
      "epoch": 57.589183408448555,
      "grad_norm": 5.659837245941162,
      "learning_rate": 2.009013826292873e-06,
      "loss": 1.6768,
      "step": 753900
    },
    {
      "epoch": 57.59682224428997,
      "grad_norm": 7.860769748687744,
      "learning_rate": 2.0026481297583584e-06,
      "loss": 1.6584,
      "step": 754000
    },
    {
      "epoch": 57.60446108013139,
      "grad_norm": 6.181825160980225,
      "learning_rate": 1.9962824332238437e-06,
      "loss": 1.6842,
      "step": 754100
    },
    {
      "epoch": 57.612099915972806,
      "grad_norm": 5.150535583496094,
      "learning_rate": 1.9899167366893285e-06,
      "loss": 1.7277,
      "step": 754200
    },
    {
      "epoch": 57.619738751814225,
      "grad_norm": 4.851866245269775,
      "learning_rate": 1.983551040154814e-06,
      "loss": 1.6366,
      "step": 754300
    },
    {
      "epoch": 57.627377587655644,
      "grad_norm": 4.159414291381836,
      "learning_rate": 1.977185343620299e-06,
      "loss": 1.6753,
      "step": 754400
    },
    {
      "epoch": 57.635016423497056,
      "grad_norm": 5.6532979011535645,
      "learning_rate": 1.970819647085784e-06,
      "loss": 1.652,
      "step": 754500
    },
    {
      "epoch": 57.642655259338476,
      "grad_norm": 5.089677333831787,
      "learning_rate": 1.9644539505512694e-06,
      "loss": 1.6549,
      "step": 754600
    },
    {
      "epoch": 57.650294095179895,
      "grad_norm": 5.431359767913818,
      "learning_rate": 1.9580882540167546e-06,
      "loss": 1.5952,
      "step": 754700
    },
    {
      "epoch": 57.657932931021314,
      "grad_norm": 5.671353816986084,
      "learning_rate": 1.95172255748224e-06,
      "loss": 1.5605,
      "step": 754800
    },
    {
      "epoch": 57.66557176686273,
      "grad_norm": 7.669556617736816,
      "learning_rate": 1.9453568609477247e-06,
      "loss": 1.6683,
      "step": 754900
    },
    {
      "epoch": 57.673210602704145,
      "grad_norm": 6.225969314575195,
      "learning_rate": 1.9389911644132103e-06,
      "loss": 1.5905,
      "step": 755000
    },
    {
      "epoch": 57.680849438545565,
      "grad_norm": 6.788818836212158,
      "learning_rate": 1.932625467878695e-06,
      "loss": 1.6853,
      "step": 755100
    },
    {
      "epoch": 57.688488274386984,
      "grad_norm": 5.683050155639648,
      "learning_rate": 1.9262597713441808e-06,
      "loss": 1.6206,
      "step": 755200
    },
    {
      "epoch": 57.6961271102284,
      "grad_norm": 8.09142780303955,
      "learning_rate": 1.9198940748096656e-06,
      "loss": 1.611,
      "step": 755300
    },
    {
      "epoch": 57.70376594606982,
      "grad_norm": 7.372729301452637,
      "learning_rate": 1.913528378275151e-06,
      "loss": 1.7206,
      "step": 755400
    },
    {
      "epoch": 57.711404781911234,
      "grad_norm": 6.789959907531738,
      "learning_rate": 1.907162681740636e-06,
      "loss": 1.648,
      "step": 755500
    },
    {
      "epoch": 57.719043617752654,
      "grad_norm": 4.796362400054932,
      "learning_rate": 1.9007969852061213e-06,
      "loss": 1.6212,
      "step": 755600
    },
    {
      "epoch": 57.72668245359407,
      "grad_norm": 6.9831037521362305,
      "learning_rate": 1.8944312886716063e-06,
      "loss": 1.7182,
      "step": 755700
    },
    {
      "epoch": 57.73432128943549,
      "grad_norm": 5.89168643951416,
      "learning_rate": 1.8880655921370917e-06,
      "loss": 1.6785,
      "step": 755800
    },
    {
      "epoch": 57.74196012527691,
      "grad_norm": 5.176660060882568,
      "learning_rate": 1.8816998956025768e-06,
      "loss": 1.6751,
      "step": 755900
    },
    {
      "epoch": 57.74959896111832,
      "grad_norm": 8.428021430969238,
      "learning_rate": 1.8753341990680622e-06,
      "loss": 1.7891,
      "step": 756000
    },
    {
      "epoch": 57.75723779695974,
      "grad_norm": 3.7057130336761475,
      "learning_rate": 1.8689685025335472e-06,
      "loss": 1.5898,
      "step": 756100
    },
    {
      "epoch": 57.76487663280116,
      "grad_norm": 6.606159687042236,
      "learning_rate": 1.8626028059990325e-06,
      "loss": 1.6416,
      "step": 756200
    },
    {
      "epoch": 57.77251546864258,
      "grad_norm": 7.204043388366699,
      "learning_rate": 1.8562371094645175e-06,
      "loss": 1.6725,
      "step": 756300
    },
    {
      "epoch": 57.78015430448399,
      "grad_norm": 5.5310750007629395,
      "learning_rate": 1.849871412930003e-06,
      "loss": 1.6581,
      "step": 756400
    },
    {
      "epoch": 57.78779314032541,
      "grad_norm": 9.178092956542969,
      "learning_rate": 1.843505716395488e-06,
      "loss": 1.6248,
      "step": 756500
    },
    {
      "epoch": 57.79543197616683,
      "grad_norm": 4.329946041107178,
      "learning_rate": 1.8371400198609734e-06,
      "loss": 1.65,
      "step": 756600
    },
    {
      "epoch": 57.80307081200825,
      "grad_norm": 5.230401992797852,
      "learning_rate": 1.8307743233264584e-06,
      "loss": 1.6724,
      "step": 756700
    },
    {
      "epoch": 57.81070964784967,
      "grad_norm": 6.524346828460693,
      "learning_rate": 1.8244086267919436e-06,
      "loss": 1.6007,
      "step": 756800
    },
    {
      "epoch": 57.81834848369108,
      "grad_norm": 4.115842819213867,
      "learning_rate": 1.8180429302574286e-06,
      "loss": 1.6013,
      "step": 756900
    },
    {
      "epoch": 57.8259873195325,
      "grad_norm": 4.574665546417236,
      "learning_rate": 1.811677233722914e-06,
      "loss": 1.6473,
      "step": 757000
    },
    {
      "epoch": 57.83362615537392,
      "grad_norm": 6.848022937774658,
      "learning_rate": 1.8053115371883991e-06,
      "loss": 1.6026,
      "step": 757100
    },
    {
      "epoch": 57.84126499121534,
      "grad_norm": 3.923269748687744,
      "learning_rate": 1.7989458406538845e-06,
      "loss": 1.6887,
      "step": 757200
    },
    {
      "epoch": 57.84890382705676,
      "grad_norm": 4.882192611694336,
      "learning_rate": 1.7925801441193696e-06,
      "loss": 1.7726,
      "step": 757300
    },
    {
      "epoch": 57.85654266289817,
      "grad_norm": 5.367194175720215,
      "learning_rate": 1.7862144475848548e-06,
      "loss": 1.6475,
      "step": 757400
    },
    {
      "epoch": 57.86418149873959,
      "grad_norm": 7.398985385894775,
      "learning_rate": 1.7798487510503398e-06,
      "loss": 1.6487,
      "step": 757500
    },
    {
      "epoch": 57.87182033458101,
      "grad_norm": 4.061198711395264,
      "learning_rate": 1.7734830545158253e-06,
      "loss": 1.5973,
      "step": 757600
    },
    {
      "epoch": 57.87945917042243,
      "grad_norm": 5.681895732879639,
      "learning_rate": 1.7671173579813103e-06,
      "loss": 1.6746,
      "step": 757700
    },
    {
      "epoch": 57.88709800626385,
      "grad_norm": 5.850834369659424,
      "learning_rate": 1.7607516614467957e-06,
      "loss": 1.7386,
      "step": 757800
    },
    {
      "epoch": 57.89473684210526,
      "grad_norm": 5.914535045623779,
      "learning_rate": 1.7543859649122807e-06,
      "loss": 1.653,
      "step": 757900
    },
    {
      "epoch": 57.90237567794668,
      "grad_norm": 6.892632007598877,
      "learning_rate": 1.748020268377766e-06,
      "loss": 1.6219,
      "step": 758000
    },
    {
      "epoch": 57.9100145137881,
      "grad_norm": 9.905842781066895,
      "learning_rate": 1.741654571843251e-06,
      "loss": 1.6569,
      "step": 758100
    },
    {
      "epoch": 57.91765334962952,
      "grad_norm": 5.841408729553223,
      "learning_rate": 1.7352888753087364e-06,
      "loss": 1.6735,
      "step": 758200
    },
    {
      "epoch": 57.92529218547094,
      "grad_norm": 5.188521385192871,
      "learning_rate": 1.7289231787742215e-06,
      "loss": 1.6728,
      "step": 758300
    },
    {
      "epoch": 57.93293102131235,
      "grad_norm": 4.955285549163818,
      "learning_rate": 1.722557482239707e-06,
      "loss": 1.6738,
      "step": 758400
    },
    {
      "epoch": 57.94056985715377,
      "grad_norm": 6.421579837799072,
      "learning_rate": 1.716191785705192e-06,
      "loss": 1.6393,
      "step": 758500
    },
    {
      "epoch": 57.94820869299519,
      "grad_norm": 7.935885429382324,
      "learning_rate": 1.7098260891706771e-06,
      "loss": 1.575,
      "step": 758600
    },
    {
      "epoch": 57.95584752883661,
      "grad_norm": 5.12671422958374,
      "learning_rate": 1.7034603926361622e-06,
      "loss": 1.645,
      "step": 758700
    },
    {
      "epoch": 57.963486364678026,
      "grad_norm": 6.071629047393799,
      "learning_rate": 1.6970946961016476e-06,
      "loss": 1.6537,
      "step": 758800
    },
    {
      "epoch": 57.97112520051944,
      "grad_norm": 6.325619220733643,
      "learning_rate": 1.6907289995671326e-06,
      "loss": 1.7023,
      "step": 758900
    },
    {
      "epoch": 57.97876403636086,
      "grad_norm": 5.9041361808776855,
      "learning_rate": 1.684363303032618e-06,
      "loss": 1.6966,
      "step": 759000
    },
    {
      "epoch": 57.98640287220228,
      "grad_norm": 6.31923246383667,
      "learning_rate": 1.677997606498103e-06,
      "loss": 1.7035,
      "step": 759100
    },
    {
      "epoch": 57.994041708043696,
      "grad_norm": 7.037151336669922,
      "learning_rate": 1.6716319099635883e-06,
      "loss": 1.7453,
      "step": 759200
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.7662101984024048,
      "eval_runtime": 3.0267,
      "eval_samples_per_second": 227.968,
      "eval_steps_per_second": 227.968,
      "step": 759278
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.419494867324829,
      "eval_runtime": 57.7449,
      "eval_samples_per_second": 226.704,
      "eval_steps_per_second": 226.704,
      "step": 759278
    },
    {
      "epoch": 58.001680543885115,
      "grad_norm": 6.8949103355407715,
      "learning_rate": 1.6652662134290733e-06,
      "loss": 1.5174,
      "step": 759300
    },
    {
      "epoch": 58.00931937972653,
      "grad_norm": 8.209230422973633,
      "learning_rate": 1.6589005168945588e-06,
      "loss": 1.7367,
      "step": 759400
    },
    {
      "epoch": 58.016958215567946,
      "grad_norm": 4.773022174835205,
      "learning_rate": 1.6525348203600438e-06,
      "loss": 1.6619,
      "step": 759500
    },
    {
      "epoch": 58.024597051409366,
      "grad_norm": 6.358884811401367,
      "learning_rate": 1.6461691238255292e-06,
      "loss": 1.5535,
      "step": 759600
    },
    {
      "epoch": 58.032235887250785,
      "grad_norm": 6.431318283081055,
      "learning_rate": 1.6398034272910143e-06,
      "loss": 1.7082,
      "step": 759700
    },
    {
      "epoch": 58.039874723092204,
      "grad_norm": 5.869907379150391,
      "learning_rate": 1.6334377307564995e-06,
      "loss": 1.7009,
      "step": 759800
    },
    {
      "epoch": 58.047513558933616,
      "grad_norm": 5.533761978149414,
      "learning_rate": 1.6270720342219845e-06,
      "loss": 1.7186,
      "step": 759900
    },
    {
      "epoch": 58.055152394775035,
      "grad_norm": 5.772053241729736,
      "learning_rate": 1.62070633768747e-06,
      "loss": 1.6186,
      "step": 760000
    },
    {
      "epoch": 58.062791230616455,
      "grad_norm": 5.230722427368164,
      "learning_rate": 1.614340641152955e-06,
      "loss": 1.6321,
      "step": 760100
    },
    {
      "epoch": 58.070430066457874,
      "grad_norm": 5.724600315093994,
      "learning_rate": 1.6079749446184404e-06,
      "loss": 1.5855,
      "step": 760200
    },
    {
      "epoch": 58.07806890229929,
      "grad_norm": 5.6853928565979,
      "learning_rate": 1.6016092480839254e-06,
      "loss": 1.6495,
      "step": 760300
    },
    {
      "epoch": 58.085707738140705,
      "grad_norm": 6.310353755950928,
      "learning_rate": 1.5952435515494107e-06,
      "loss": 1.7089,
      "step": 760400
    },
    {
      "epoch": 58.093346573982124,
      "grad_norm": 4.3390421867370605,
      "learning_rate": 1.5888778550148957e-06,
      "loss": 1.6303,
      "step": 760500
    },
    {
      "epoch": 58.100985409823544,
      "grad_norm": 4.934926986694336,
      "learning_rate": 1.5825121584803811e-06,
      "loss": 1.5959,
      "step": 760600
    },
    {
      "epoch": 58.10862424566496,
      "grad_norm": 7.009948253631592,
      "learning_rate": 1.5761464619458661e-06,
      "loss": 1.6547,
      "step": 760700
    },
    {
      "epoch": 58.116263081506375,
      "grad_norm": 6.993702411651611,
      "learning_rate": 1.5697807654113516e-06,
      "loss": 1.6455,
      "step": 760800
    },
    {
      "epoch": 58.123901917347794,
      "grad_norm": 5.504286289215088,
      "learning_rate": 1.5634150688768366e-06,
      "loss": 1.6907,
      "step": 760900
    },
    {
      "epoch": 58.13154075318921,
      "grad_norm": 6.308470726013184,
      "learning_rate": 1.5570493723423218e-06,
      "loss": 1.5923,
      "step": 761000
    },
    {
      "epoch": 58.13917958903063,
      "grad_norm": 5.232071399688721,
      "learning_rate": 1.5506836758078069e-06,
      "loss": 1.597,
      "step": 761100
    },
    {
      "epoch": 58.14681842487205,
      "grad_norm": 5.180421829223633,
      "learning_rate": 1.544317979273292e-06,
      "loss": 1.7017,
      "step": 761200
    },
    {
      "epoch": 58.154457260713464,
      "grad_norm": 5.261650085449219,
      "learning_rate": 1.5379522827387773e-06,
      "loss": 1.6534,
      "step": 761300
    },
    {
      "epoch": 58.16209609655488,
      "grad_norm": 8.527509689331055,
      "learning_rate": 1.5315865862042625e-06,
      "loss": 1.709,
      "step": 761400
    },
    {
      "epoch": 58.1697349323963,
      "grad_norm": 6.339090824127197,
      "learning_rate": 1.5252208896697478e-06,
      "loss": 1.5861,
      "step": 761500
    },
    {
      "epoch": 58.17737376823772,
      "grad_norm": 5.217146396636963,
      "learning_rate": 1.518855193135233e-06,
      "loss": 1.638,
      "step": 761600
    },
    {
      "epoch": 58.18501260407914,
      "grad_norm": 4.396206378936768,
      "learning_rate": 1.512489496600718e-06,
      "loss": 1.6256,
      "step": 761700
    },
    {
      "epoch": 58.19265143992055,
      "grad_norm": 7.256394386291504,
      "learning_rate": 1.5061238000662033e-06,
      "loss": 1.7432,
      "step": 761800
    },
    {
      "epoch": 58.20029027576197,
      "grad_norm": 6.063029766082764,
      "learning_rate": 1.4997581035316885e-06,
      "loss": 1.673,
      "step": 761900
    },
    {
      "epoch": 58.20792911160339,
      "grad_norm": 6.6350202560424805,
      "learning_rate": 1.4933924069971737e-06,
      "loss": 1.5703,
      "step": 762000
    },
    {
      "epoch": 58.21556794744481,
      "grad_norm": 5.642764568328857,
      "learning_rate": 1.487026710462659e-06,
      "loss": 1.6154,
      "step": 762100
    },
    {
      "epoch": 58.22320678328623,
      "grad_norm": 5.7283501625061035,
      "learning_rate": 1.4806610139281442e-06,
      "loss": 1.6285,
      "step": 762200
    },
    {
      "epoch": 58.23084561912764,
      "grad_norm": 5.145875930786133,
      "learning_rate": 1.4742953173936292e-06,
      "loss": 1.6519,
      "step": 762300
    },
    {
      "epoch": 58.23848445496906,
      "grad_norm": 7.670156955718994,
      "learning_rate": 1.4679296208591144e-06,
      "loss": 1.5916,
      "step": 762400
    },
    {
      "epoch": 58.24612329081048,
      "grad_norm": 5.085388660430908,
      "learning_rate": 1.4615639243245997e-06,
      "loss": 1.6634,
      "step": 762500
    },
    {
      "epoch": 58.2537621266519,
      "grad_norm": 6.3093366622924805,
      "learning_rate": 1.4551982277900849e-06,
      "loss": 1.7123,
      "step": 762600
    },
    {
      "epoch": 58.26140096249332,
      "grad_norm": 5.388332366943359,
      "learning_rate": 1.4488325312555701e-06,
      "loss": 1.6089,
      "step": 762700
    },
    {
      "epoch": 58.26903979833473,
      "grad_norm": 7.620651721954346,
      "learning_rate": 1.4424668347210554e-06,
      "loss": 1.665,
      "step": 762800
    },
    {
      "epoch": 58.27667863417615,
      "grad_norm": 5.352286338806152,
      "learning_rate": 1.4361011381865404e-06,
      "loss": 1.6662,
      "step": 762900
    },
    {
      "epoch": 58.28431747001757,
      "grad_norm": 5.565343856811523,
      "learning_rate": 1.4297354416520256e-06,
      "loss": 1.6199,
      "step": 763000
    },
    {
      "epoch": 58.29195630585899,
      "grad_norm": 5.058483600616455,
      "learning_rate": 1.4233697451175108e-06,
      "loss": 1.7124,
      "step": 763100
    },
    {
      "epoch": 58.29959514170041,
      "grad_norm": 7.970149040222168,
      "learning_rate": 1.417004048582996e-06,
      "loss": 1.5596,
      "step": 763200
    },
    {
      "epoch": 58.30723397754182,
      "grad_norm": 5.555758953094482,
      "learning_rate": 1.4106383520484813e-06,
      "loss": 1.6027,
      "step": 763300
    },
    {
      "epoch": 58.31487281338324,
      "grad_norm": 3.718132972717285,
      "learning_rate": 1.4042726555139665e-06,
      "loss": 1.6523,
      "step": 763400
    },
    {
      "epoch": 58.32251164922466,
      "grad_norm": 6.5234270095825195,
      "learning_rate": 1.3979069589794515e-06,
      "loss": 1.6056,
      "step": 763500
    },
    {
      "epoch": 58.33015048506608,
      "grad_norm": 5.445178508758545,
      "learning_rate": 1.3915412624449368e-06,
      "loss": 1.7084,
      "step": 763600
    },
    {
      "epoch": 58.3377893209075,
      "grad_norm": 5.138778209686279,
      "learning_rate": 1.385175565910422e-06,
      "loss": 1.7027,
      "step": 763700
    },
    {
      "epoch": 58.34542815674891,
      "grad_norm": 5.346354961395264,
      "learning_rate": 1.3788098693759072e-06,
      "loss": 1.6161,
      "step": 763800
    },
    {
      "epoch": 58.35306699259033,
      "grad_norm": 5.54856538772583,
      "learning_rate": 1.3724441728413925e-06,
      "loss": 1.6389,
      "step": 763900
    },
    {
      "epoch": 58.36070582843175,
      "grad_norm": 4.657378196716309,
      "learning_rate": 1.3660784763068777e-06,
      "loss": 1.6964,
      "step": 764000
    },
    {
      "epoch": 58.36834466427317,
      "grad_norm": 6.146982192993164,
      "learning_rate": 1.3597127797723627e-06,
      "loss": 1.7383,
      "step": 764100
    },
    {
      "epoch": 58.375983500114586,
      "grad_norm": 6.0909905433654785,
      "learning_rate": 1.353347083237848e-06,
      "loss": 1.6487,
      "step": 764200
    },
    {
      "epoch": 58.383622335956,
      "grad_norm": 5.546685218811035,
      "learning_rate": 1.3469813867033332e-06,
      "loss": 1.6584,
      "step": 764300
    },
    {
      "epoch": 58.39126117179742,
      "grad_norm": 7.783483982086182,
      "learning_rate": 1.3406156901688184e-06,
      "loss": 1.6243,
      "step": 764400
    },
    {
      "epoch": 58.39890000763884,
      "grad_norm": 13.16552734375,
      "learning_rate": 1.3342499936343036e-06,
      "loss": 1.7112,
      "step": 764500
    },
    {
      "epoch": 58.406538843480256,
      "grad_norm": 5.491532325744629,
      "learning_rate": 1.3278842970997889e-06,
      "loss": 1.705,
      "step": 764600
    },
    {
      "epoch": 58.41417767932167,
      "grad_norm": 5.064395904541016,
      "learning_rate": 1.3215186005652739e-06,
      "loss": 1.6789,
      "step": 764700
    },
    {
      "epoch": 58.42181651516309,
      "grad_norm": 5.771672248840332,
      "learning_rate": 1.3151529040307591e-06,
      "loss": 1.6086,
      "step": 764800
    },
    {
      "epoch": 58.429455351004506,
      "grad_norm": 4.508591651916504,
      "learning_rate": 1.3087872074962444e-06,
      "loss": 1.675,
      "step": 764900
    },
    {
      "epoch": 58.437094186845925,
      "grad_norm": 4.3237199783325195,
      "learning_rate": 1.3024215109617296e-06,
      "loss": 1.6492,
      "step": 765000
    },
    {
      "epoch": 58.444733022687345,
      "grad_norm": 4.354304313659668,
      "learning_rate": 1.2960558144272148e-06,
      "loss": 1.6302,
      "step": 765100
    },
    {
      "epoch": 58.45237185852876,
      "grad_norm": 4.725539207458496,
      "learning_rate": 1.2896901178926998e-06,
      "loss": 1.695,
      "step": 765200
    },
    {
      "epoch": 58.460010694370176,
      "grad_norm": 6.294522285461426,
      "learning_rate": 1.283324421358185e-06,
      "loss": 1.6369,
      "step": 765300
    },
    {
      "epoch": 58.467649530211595,
      "grad_norm": 8.263193130493164,
      "learning_rate": 1.2769587248236703e-06,
      "loss": 1.6596,
      "step": 765400
    },
    {
      "epoch": 58.475288366053014,
      "grad_norm": 6.6061482429504395,
      "learning_rate": 1.2705930282891555e-06,
      "loss": 1.7034,
      "step": 765500
    },
    {
      "epoch": 58.482927201894434,
      "grad_norm": 5.4934492111206055,
      "learning_rate": 1.2642273317546408e-06,
      "loss": 1.6401,
      "step": 765600
    },
    {
      "epoch": 58.490566037735846,
      "grad_norm": 5.891183853149414,
      "learning_rate": 1.257861635220126e-06,
      "loss": 1.6273,
      "step": 765700
    },
    {
      "epoch": 58.498204873577265,
      "grad_norm": 5.757681846618652,
      "learning_rate": 1.251495938685611e-06,
      "loss": 1.6749,
      "step": 765800
    },
    {
      "epoch": 58.505843709418684,
      "grad_norm": 6.484023094177246,
      "learning_rate": 1.2451302421510962e-06,
      "loss": 1.6456,
      "step": 765900
    },
    {
      "epoch": 58.5134825452601,
      "grad_norm": 5.6319403648376465,
      "learning_rate": 1.2387645456165815e-06,
      "loss": 1.6058,
      "step": 766000
    },
    {
      "epoch": 58.52112138110152,
      "grad_norm": 4.419945240020752,
      "learning_rate": 1.2323988490820667e-06,
      "loss": 1.5814,
      "step": 766100
    },
    {
      "epoch": 58.528760216942935,
      "grad_norm": 5.197119235992432,
      "learning_rate": 1.226033152547552e-06,
      "loss": 1.6873,
      "step": 766200
    },
    {
      "epoch": 58.536399052784354,
      "grad_norm": 6.65122652053833,
      "learning_rate": 1.2196674560130372e-06,
      "loss": 1.638,
      "step": 766300
    },
    {
      "epoch": 58.54403788862577,
      "grad_norm": 6.061364650726318,
      "learning_rate": 1.2133017594785222e-06,
      "loss": 1.6285,
      "step": 766400
    },
    {
      "epoch": 58.55167672446719,
      "grad_norm": 6.454477787017822,
      "learning_rate": 1.2069360629440074e-06,
      "loss": 1.6399,
      "step": 766500
    },
    {
      "epoch": 58.55931556030861,
      "grad_norm": 6.57393741607666,
      "learning_rate": 1.2005703664094926e-06,
      "loss": 1.6163,
      "step": 766600
    },
    {
      "epoch": 58.566954396150024,
      "grad_norm": 6.474884510040283,
      "learning_rate": 1.1942046698749777e-06,
      "loss": 1.7218,
      "step": 766700
    },
    {
      "epoch": 58.57459323199144,
      "grad_norm": 4.740698337554932,
      "learning_rate": 1.1878389733404629e-06,
      "loss": 1.6418,
      "step": 766800
    },
    {
      "epoch": 58.58223206783286,
      "grad_norm": 7.308356285095215,
      "learning_rate": 1.1814732768059481e-06,
      "loss": 1.5961,
      "step": 766900
    },
    {
      "epoch": 58.58987090367428,
      "grad_norm": 5.572506904602051,
      "learning_rate": 1.1751075802714334e-06,
      "loss": 1.675,
      "step": 767000
    },
    {
      "epoch": 58.5975097395157,
      "grad_norm": 5.416477203369141,
      "learning_rate": 1.1687418837369184e-06,
      "loss": 1.6397,
      "step": 767100
    },
    {
      "epoch": 58.60514857535711,
      "grad_norm": 4.322419166564941,
      "learning_rate": 1.1623761872024036e-06,
      "loss": 1.6924,
      "step": 767200
    },
    {
      "epoch": 58.61278741119853,
      "grad_norm": 7.019276142120361,
      "learning_rate": 1.1560104906678888e-06,
      "loss": 1.7182,
      "step": 767300
    },
    {
      "epoch": 58.62042624703995,
      "grad_norm": 6.9344682693481445,
      "learning_rate": 1.149644794133374e-06,
      "loss": 1.6396,
      "step": 767400
    },
    {
      "epoch": 58.62806508288137,
      "grad_norm": 5.109040260314941,
      "learning_rate": 1.1432790975988593e-06,
      "loss": 1.7216,
      "step": 767500
    },
    {
      "epoch": 58.63570391872279,
      "grad_norm": 7.006357669830322,
      "learning_rate": 1.1369134010643445e-06,
      "loss": 1.7233,
      "step": 767600
    },
    {
      "epoch": 58.6433427545642,
      "grad_norm": 6.153194427490234,
      "learning_rate": 1.1305477045298295e-06,
      "loss": 1.6319,
      "step": 767700
    },
    {
      "epoch": 58.65098159040562,
      "grad_norm": 6.421050548553467,
      "learning_rate": 1.1241820079953148e-06,
      "loss": 1.6604,
      "step": 767800
    },
    {
      "epoch": 58.65862042624704,
      "grad_norm": 6.0877227783203125,
      "learning_rate": 1.1178163114608e-06,
      "loss": 1.6315,
      "step": 767900
    },
    {
      "epoch": 58.66625926208846,
      "grad_norm": 6.1811676025390625,
      "learning_rate": 1.1114506149262852e-06,
      "loss": 1.714,
      "step": 768000
    },
    {
      "epoch": 58.67389809792988,
      "grad_norm": 6.867175102233887,
      "learning_rate": 1.1050849183917705e-06,
      "loss": 1.5992,
      "step": 768100
    },
    {
      "epoch": 58.68153693377129,
      "grad_norm": 5.0997748374938965,
      "learning_rate": 1.0987192218572557e-06,
      "loss": 1.6583,
      "step": 768200
    },
    {
      "epoch": 58.68917576961271,
      "grad_norm": 5.459279537200928,
      "learning_rate": 1.0923535253227407e-06,
      "loss": 1.7571,
      "step": 768300
    },
    {
      "epoch": 58.69681460545413,
      "grad_norm": 4.93020486831665,
      "learning_rate": 1.085987828788226e-06,
      "loss": 1.6893,
      "step": 768400
    },
    {
      "epoch": 58.70445344129555,
      "grad_norm": 5.459554672241211,
      "learning_rate": 1.0796221322537112e-06,
      "loss": 1.7449,
      "step": 768500
    },
    {
      "epoch": 58.71209227713697,
      "grad_norm": 4.575467586517334,
      "learning_rate": 1.0732564357191964e-06,
      "loss": 1.6941,
      "step": 768600
    },
    {
      "epoch": 58.71973111297838,
      "grad_norm": 5.738361835479736,
      "learning_rate": 1.0668907391846816e-06,
      "loss": 1.6603,
      "step": 768700
    },
    {
      "epoch": 58.7273699488198,
      "grad_norm": 3.685606002807617,
      "learning_rate": 1.0605250426501669e-06,
      "loss": 1.6813,
      "step": 768800
    },
    {
      "epoch": 58.73500878466122,
      "grad_norm": 5.931684494018555,
      "learning_rate": 1.0541593461156519e-06,
      "loss": 1.6238,
      "step": 768900
    },
    {
      "epoch": 58.74264762050264,
      "grad_norm": 7.458081245422363,
      "learning_rate": 1.0477936495811371e-06,
      "loss": 1.6387,
      "step": 769000
    },
    {
      "epoch": 58.75028645634406,
      "grad_norm": 5.069918155670166,
      "learning_rate": 1.0414279530466223e-06,
      "loss": 1.5435,
      "step": 769100
    },
    {
      "epoch": 58.75792529218547,
      "grad_norm": 6.899303913116455,
      "learning_rate": 1.0350622565121076e-06,
      "loss": 1.6689,
      "step": 769200
    },
    {
      "epoch": 58.76556412802689,
      "grad_norm": 7.1286211013793945,
      "learning_rate": 1.0286965599775928e-06,
      "loss": 1.6375,
      "step": 769300
    },
    {
      "epoch": 58.77320296386831,
      "grad_norm": 3.902646064758301,
      "learning_rate": 1.022330863443078e-06,
      "loss": 1.5877,
      "step": 769400
    },
    {
      "epoch": 58.78084179970973,
      "grad_norm": 4.707862377166748,
      "learning_rate": 1.015965166908563e-06,
      "loss": 1.6629,
      "step": 769500
    },
    {
      "epoch": 58.78848063555114,
      "grad_norm": 4.71596622467041,
      "learning_rate": 1.0095994703740483e-06,
      "loss": 1.6188,
      "step": 769600
    },
    {
      "epoch": 58.79611947139256,
      "grad_norm": 8.113243103027344,
      "learning_rate": 1.0032337738395335e-06,
      "loss": 1.69,
      "step": 769700
    },
    {
      "epoch": 58.80375830723398,
      "grad_norm": 5.351583480834961,
      "learning_rate": 9.968680773050188e-07,
      "loss": 1.5288,
      "step": 769800
    },
    {
      "epoch": 58.811397143075396,
      "grad_norm": 5.972075462341309,
      "learning_rate": 9.90502380770504e-07,
      "loss": 1.6767,
      "step": 769900
    },
    {
      "epoch": 58.819035978916816,
      "grad_norm": 5.180877208709717,
      "learning_rate": 9.841366842359892e-07,
      "loss": 1.669,
      "step": 770000
    },
    {
      "epoch": 58.82667481475823,
      "grad_norm": 5.4834089279174805,
      "learning_rate": 9.777709877014742e-07,
      "loss": 1.7376,
      "step": 770100
    },
    {
      "epoch": 58.83431365059965,
      "grad_norm": 6.691640377044678,
      "learning_rate": 9.714052911669595e-07,
      "loss": 1.6447,
      "step": 770200
    },
    {
      "epoch": 58.841952486441066,
      "grad_norm": 5.466052532196045,
      "learning_rate": 9.650395946324447e-07,
      "loss": 1.6533,
      "step": 770300
    },
    {
      "epoch": 58.849591322282485,
      "grad_norm": 7.064538478851318,
      "learning_rate": 9.5867389809793e-07,
      "loss": 1.6367,
      "step": 770400
    },
    {
      "epoch": 58.857230158123905,
      "grad_norm": 5.161160469055176,
      "learning_rate": 9.523082015634152e-07,
      "loss": 1.6169,
      "step": 770500
    },
    {
      "epoch": 58.86486899396532,
      "grad_norm": 5.460132598876953,
      "learning_rate": 9.459425050289003e-07,
      "loss": 1.724,
      "step": 770600
    },
    {
      "epoch": 58.872507829806736,
      "grad_norm": 5.6812567710876465,
      "learning_rate": 9.395768084943855e-07,
      "loss": 1.632,
      "step": 770700
    },
    {
      "epoch": 58.880146665648155,
      "grad_norm": 6.459299564361572,
      "learning_rate": 9.332111119598707e-07,
      "loss": 1.6317,
      "step": 770800
    },
    {
      "epoch": 58.887785501489574,
      "grad_norm": 5.409079551696777,
      "learning_rate": 9.268454154253559e-07,
      "loss": 1.6918,
      "step": 770900
    },
    {
      "epoch": 58.895424337330994,
      "grad_norm": 6.011091709136963,
      "learning_rate": 9.204797188908411e-07,
      "loss": 1.7705,
      "step": 771000
    },
    {
      "epoch": 58.903063173172406,
      "grad_norm": 2.866039991378784,
      "learning_rate": 9.141140223563263e-07,
      "loss": 1.6834,
      "step": 771100
    },
    {
      "epoch": 58.910702009013825,
      "grad_norm": 5.925498962402344,
      "learning_rate": 9.077483258218115e-07,
      "loss": 1.5698,
      "step": 771200
    },
    {
      "epoch": 58.918340844855244,
      "grad_norm": 4.848118782043457,
      "learning_rate": 9.013826292872967e-07,
      "loss": 1.5557,
      "step": 771300
    },
    {
      "epoch": 58.92597968069666,
      "grad_norm": 5.22537899017334,
      "learning_rate": 8.950169327527819e-07,
      "loss": 1.6314,
      "step": 771400
    },
    {
      "epoch": 58.93361851653808,
      "grad_norm": 5.281287670135498,
      "learning_rate": 8.88651236218267e-07,
      "loss": 1.694,
      "step": 771500
    },
    {
      "epoch": 58.941257352379495,
      "grad_norm": 6.239500522613525,
      "learning_rate": 8.822855396837523e-07,
      "loss": 1.622,
      "step": 771600
    },
    {
      "epoch": 58.948896188220914,
      "grad_norm": 4.205190181732178,
      "learning_rate": 8.759198431492375e-07,
      "loss": 1.5913,
      "step": 771700
    },
    {
      "epoch": 58.95653502406233,
      "grad_norm": 5.79352331161499,
      "learning_rate": 8.695541466147226e-07,
      "loss": 1.6562,
      "step": 771800
    },
    {
      "epoch": 58.96417385990375,
      "grad_norm": 5.83949613571167,
      "learning_rate": 8.631884500802079e-07,
      "loss": 1.7371,
      "step": 771900
    },
    {
      "epoch": 58.97181269574517,
      "grad_norm": 7.102807998657227,
      "learning_rate": 8.568227535456931e-07,
      "loss": 1.618,
      "step": 772000
    },
    {
      "epoch": 58.979451531586584,
      "grad_norm": 6.763718605041504,
      "learning_rate": 8.504570570111782e-07,
      "loss": 1.7581,
      "step": 772100
    },
    {
      "epoch": 58.987090367428,
      "grad_norm": 7.941994667053223,
      "learning_rate": 8.440913604766634e-07,
      "loss": 1.636,
      "step": 772200
    },
    {
      "epoch": 58.99472920326942,
      "grad_norm": 5.056000709533691,
      "learning_rate": 8.377256639421487e-07,
      "loss": 1.74,
      "step": 772300
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.7656278610229492,
      "eval_runtime": 3.6524,
      "eval_samples_per_second": 188.918,
      "eval_steps_per_second": 188.918,
      "step": 772369
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.419402837753296,
      "eval_runtime": 68.4175,
      "eval_samples_per_second": 191.34,
      "eval_steps_per_second": 191.34,
      "step": 772369
    },
    {
      "epoch": 59.00236803911084,
      "grad_norm": 7.15889310836792,
      "learning_rate": 8.313599674076338e-07,
      "loss": 1.7543,
      "step": 772400
    },
    {
      "epoch": 59.01000687495226,
      "grad_norm": 4.2385759353637695,
      "learning_rate": 8.24994270873119e-07,
      "loss": 1.6581,
      "step": 772500
    },
    {
      "epoch": 59.01764571079367,
      "grad_norm": 5.816093921661377,
      "learning_rate": 8.186285743386043e-07,
      "loss": 1.6375,
      "step": 772600
    },
    {
      "epoch": 59.02528454663509,
      "grad_norm": 5.274614334106445,
      "learning_rate": 8.122628778040894e-07,
      "loss": 1.6547,
      "step": 772700
    },
    {
      "epoch": 59.03292338247651,
      "grad_norm": 7.092513084411621,
      "learning_rate": 8.058971812695746e-07,
      "loss": 1.731,
      "step": 772800
    },
    {
      "epoch": 59.04056221831793,
      "grad_norm": 6.747623443603516,
      "learning_rate": 7.995314847350598e-07,
      "loss": 1.62,
      "step": 772900
    },
    {
      "epoch": 59.04820105415935,
      "grad_norm": 4.5476908683776855,
      "learning_rate": 7.93165788200545e-07,
      "loss": 1.5759,
      "step": 773000
    },
    {
      "epoch": 59.05583989000076,
      "grad_norm": 5.655548572540283,
      "learning_rate": 7.868000916660302e-07,
      "loss": 1.6754,
      "step": 773100
    },
    {
      "epoch": 59.06347872584218,
      "grad_norm": 5.090417385101318,
      "learning_rate": 7.804343951315153e-07,
      "loss": 1.6768,
      "step": 773200
    },
    {
      "epoch": 59.0711175616836,
      "grad_norm": 4.0665693283081055,
      "learning_rate": 7.740686985970005e-07,
      "loss": 1.536,
      "step": 773300
    },
    {
      "epoch": 59.07875639752502,
      "grad_norm": 7.891992568969727,
      "learning_rate": 7.677030020624857e-07,
      "loss": 1.7019,
      "step": 773400
    },
    {
      "epoch": 59.08639523336643,
      "grad_norm": 7.772146224975586,
      "learning_rate": 7.613373055279709e-07,
      "loss": 1.6195,
      "step": 773500
    },
    {
      "epoch": 59.09403406920785,
      "grad_norm": 10.00039005279541,
      "learning_rate": 7.54971608993456e-07,
      "loss": 1.6757,
      "step": 773600
    },
    {
      "epoch": 59.10167290504927,
      "grad_norm": 6.059822082519531,
      "learning_rate": 7.486059124589413e-07,
      "loss": 1.7199,
      "step": 773700
    },
    {
      "epoch": 59.10931174089069,
      "grad_norm": 5.860372066497803,
      "learning_rate": 7.422402159244265e-07,
      "loss": 1.6549,
      "step": 773800
    },
    {
      "epoch": 59.11695057673211,
      "grad_norm": 4.833215713500977,
      "learning_rate": 7.358745193899116e-07,
      "loss": 1.7024,
      "step": 773900
    },
    {
      "epoch": 59.12458941257352,
      "grad_norm": 5.597489356994629,
      "learning_rate": 7.295088228553969e-07,
      "loss": 1.6467,
      "step": 774000
    },
    {
      "epoch": 59.13222824841494,
      "grad_norm": 5.807828426361084,
      "learning_rate": 7.231431263208821e-07,
      "loss": 1.717,
      "step": 774100
    },
    {
      "epoch": 59.13986708425636,
      "grad_norm": 5.561272621154785,
      "learning_rate": 7.167774297863672e-07,
      "loss": 1.5966,
      "step": 774200
    },
    {
      "epoch": 59.14750592009778,
      "grad_norm": 5.700578212738037,
      "learning_rate": 7.104117332518524e-07,
      "loss": 1.6684,
      "step": 774300
    },
    {
      "epoch": 59.1551447559392,
      "grad_norm": 4.879953861236572,
      "learning_rate": 7.040460367173377e-07,
      "loss": 1.6421,
      "step": 774400
    },
    {
      "epoch": 59.16278359178061,
      "grad_norm": 9.503366470336914,
      "learning_rate": 6.976803401828228e-07,
      "loss": 1.7641,
      "step": 774500
    },
    {
      "epoch": 59.17042242762203,
      "grad_norm": 5.412872314453125,
      "learning_rate": 6.91314643648308e-07,
      "loss": 1.7299,
      "step": 774600
    },
    {
      "epoch": 59.17806126346345,
      "grad_norm": 3.675290584564209,
      "learning_rate": 6.849489471137933e-07,
      "loss": 1.5781,
      "step": 774700
    },
    {
      "epoch": 59.18570009930487,
      "grad_norm": 5.302492141723633,
      "learning_rate": 6.785832505792784e-07,
      "loss": 1.6609,
      "step": 774800
    },
    {
      "epoch": 59.193338935146286,
      "grad_norm": 6.256106853485107,
      "learning_rate": 6.722175540447636e-07,
      "loss": 1.5718,
      "step": 774900
    },
    {
      "epoch": 59.2009777709877,
      "grad_norm": 4.8666276931762695,
      "learning_rate": 6.658518575102488e-07,
      "loss": 1.5987,
      "step": 775000
    },
    {
      "epoch": 59.20861660682912,
      "grad_norm": 4.193001747131348,
      "learning_rate": 6.59486160975734e-07,
      "loss": 1.6681,
      "step": 775100
    },
    {
      "epoch": 59.21625544267054,
      "grad_norm": 6.617309093475342,
      "learning_rate": 6.531204644412192e-07,
      "loss": 1.6296,
      "step": 775200
    },
    {
      "epoch": 59.223894278511956,
      "grad_norm": 4.849087715148926,
      "learning_rate": 6.467547679067044e-07,
      "loss": 1.7316,
      "step": 775300
    },
    {
      "epoch": 59.231533114353375,
      "grad_norm": 4.0277252197265625,
      "learning_rate": 6.403890713721896e-07,
      "loss": 1.6339,
      "step": 775400
    },
    {
      "epoch": 59.23917195019479,
      "grad_norm": 6.460297107696533,
      "learning_rate": 6.340233748376748e-07,
      "loss": 1.6814,
      "step": 775500
    },
    {
      "epoch": 59.24681078603621,
      "grad_norm": 4.762042999267578,
      "learning_rate": 6.2765767830316e-07,
      "loss": 1.7581,
      "step": 775600
    },
    {
      "epoch": 59.254449621877626,
      "grad_norm": 5.754872798919678,
      "learning_rate": 6.212919817686451e-07,
      "loss": 1.6484,
      "step": 775700
    },
    {
      "epoch": 59.262088457719045,
      "grad_norm": 4.4079132080078125,
      "learning_rate": 6.149262852341304e-07,
      "loss": 1.6586,
      "step": 775800
    },
    {
      "epoch": 59.269727293560464,
      "grad_norm": 5.87803840637207,
      "learning_rate": 6.085605886996156e-07,
      "loss": 1.6143,
      "step": 775900
    },
    {
      "epoch": 59.27736612940188,
      "grad_norm": 5.189731121063232,
      "learning_rate": 6.021948921651007e-07,
      "loss": 1.6522,
      "step": 776000
    },
    {
      "epoch": 59.285004965243296,
      "grad_norm": 8.948629379272461,
      "learning_rate": 5.95829195630586e-07,
      "loss": 1.5998,
      "step": 776100
    },
    {
      "epoch": 59.292643801084715,
      "grad_norm": 5.162638187408447,
      "learning_rate": 5.894634990960712e-07,
      "loss": 1.6997,
      "step": 776200
    },
    {
      "epoch": 59.300282636926134,
      "grad_norm": 9.832096099853516,
      "learning_rate": 5.830978025615563e-07,
      "loss": 1.6821,
      "step": 776300
    },
    {
      "epoch": 59.30792147276755,
      "grad_norm": 6.307126998901367,
      "learning_rate": 5.767321060270415e-07,
      "loss": 1.6477,
      "step": 776400
    },
    {
      "epoch": 59.315560308608966,
      "grad_norm": 5.5324859619140625,
      "learning_rate": 5.703664094925268e-07,
      "loss": 1.6393,
      "step": 776500
    },
    {
      "epoch": 59.323199144450385,
      "grad_norm": 6.351873397827148,
      "learning_rate": 5.640007129580119e-07,
      "loss": 1.6772,
      "step": 776600
    },
    {
      "epoch": 59.330837980291804,
      "grad_norm": 4.2968034744262695,
      "learning_rate": 5.576350164234971e-07,
      "loss": 1.6242,
      "step": 776700
    },
    {
      "epoch": 59.33847681613322,
      "grad_norm": 5.700830459594727,
      "learning_rate": 5.512693198889824e-07,
      "loss": 1.6464,
      "step": 776800
    },
    {
      "epoch": 59.34611565197464,
      "grad_norm": 6.635672569274902,
      "learning_rate": 5.449036233544674e-07,
      "loss": 1.6782,
      "step": 776900
    },
    {
      "epoch": 59.353754487816055,
      "grad_norm": 5.891359806060791,
      "learning_rate": 5.385379268199526e-07,
      "loss": 1.6731,
      "step": 777000
    },
    {
      "epoch": 59.361393323657474,
      "grad_norm": 5.39751672744751,
      "learning_rate": 5.321722302854378e-07,
      "loss": 1.7352,
      "step": 777100
    },
    {
      "epoch": 59.36903215949889,
      "grad_norm": 6.709139347076416,
      "learning_rate": 5.25806533750923e-07,
      "loss": 1.6294,
      "step": 777200
    },
    {
      "epoch": 59.37667099534031,
      "grad_norm": 4.613731861114502,
      "learning_rate": 5.194408372164082e-07,
      "loss": 1.5917,
      "step": 777300
    },
    {
      "epoch": 59.384309831181724,
      "grad_norm": 5.460576057434082,
      "learning_rate": 5.130751406818934e-07,
      "loss": 1.6836,
      "step": 777400
    },
    {
      "epoch": 59.391948667023144,
      "grad_norm": 4.221390247344971,
      "learning_rate": 5.067094441473786e-07,
      "loss": 1.6957,
      "step": 777500
    },
    {
      "epoch": 59.39958750286456,
      "grad_norm": 5.652078628540039,
      "learning_rate": 5.003437476128638e-07,
      "loss": 1.6439,
      "step": 777600
    },
    {
      "epoch": 59.40722633870598,
      "grad_norm": 5.569116592407227,
      "learning_rate": 4.93978051078349e-07,
      "loss": 1.541,
      "step": 777700
    },
    {
      "epoch": 59.4148651745474,
      "grad_norm": 8.474854469299316,
      "learning_rate": 4.876123545438341e-07,
      "loss": 1.7119,
      "step": 777800
    },
    {
      "epoch": 59.42250401038881,
      "grad_norm": 5.928569316864014,
      "learning_rate": 4.812466580093194e-07,
      "loss": 1.6718,
      "step": 777900
    },
    {
      "epoch": 59.43014284623023,
      "grad_norm": 5.265827655792236,
      "learning_rate": 4.748809614748046e-07,
      "loss": 1.5742,
      "step": 778000
    },
    {
      "epoch": 59.43778168207165,
      "grad_norm": 5.7166619300842285,
      "learning_rate": 4.685152649402898e-07,
      "loss": 1.6981,
      "step": 778100
    },
    {
      "epoch": 59.44542051791307,
      "grad_norm": 5.0237627029418945,
      "learning_rate": 4.6214956840577496e-07,
      "loss": 1.6199,
      "step": 778200
    },
    {
      "epoch": 59.45305935375449,
      "grad_norm": 4.593656063079834,
      "learning_rate": 4.5578387187126014e-07,
      "loss": 1.6628,
      "step": 778300
    },
    {
      "epoch": 59.4606981895959,
      "grad_norm": 5.157220840454102,
      "learning_rate": 4.4941817533674537e-07,
      "loss": 1.7674,
      "step": 778400
    },
    {
      "epoch": 59.46833702543732,
      "grad_norm": 4.842380523681641,
      "learning_rate": 4.4305247880223054e-07,
      "loss": 1.6444,
      "step": 778500
    },
    {
      "epoch": 59.47597586127874,
      "grad_norm": 5.64108419418335,
      "learning_rate": 4.366867822677157e-07,
      "loss": 1.672,
      "step": 778600
    },
    {
      "epoch": 59.48361469712016,
      "grad_norm": 7.094460964202881,
      "learning_rate": 4.3032108573320095e-07,
      "loss": 1.6035,
      "step": 778700
    },
    {
      "epoch": 59.49125353296158,
      "grad_norm": 5.133694171905518,
      "learning_rate": 4.2395538919868613e-07,
      "loss": 1.6002,
      "step": 778800
    },
    {
      "epoch": 59.49889236880299,
      "grad_norm": 3.0466785430908203,
      "learning_rate": 4.175896926641713e-07,
      "loss": 1.5807,
      "step": 778900
    },
    {
      "epoch": 59.50653120464441,
      "grad_norm": 5.845396518707275,
      "learning_rate": 4.1122399612965654e-07,
      "loss": 1.6053,
      "step": 779000
    },
    {
      "epoch": 59.51417004048583,
      "grad_norm": 8.349569320678711,
      "learning_rate": 4.048582995951417e-07,
      "loss": 1.652,
      "step": 779100
    },
    {
      "epoch": 59.52180887632725,
      "grad_norm": 5.4508538246154785,
      "learning_rate": 3.984926030606269e-07,
      "loss": 1.5594,
      "step": 779200
    },
    {
      "epoch": 59.52944771216867,
      "grad_norm": 5.121043682098389,
      "learning_rate": 3.921269065261121e-07,
      "loss": 1.6759,
      "step": 779300
    },
    {
      "epoch": 59.53708654801008,
      "grad_norm": 5.393668174743652,
      "learning_rate": 3.857612099915973e-07,
      "loss": 1.699,
      "step": 779400
    },
    {
      "epoch": 59.5447253838515,
      "grad_norm": 5.887543201446533,
      "learning_rate": 3.793955134570825e-07,
      "loss": 1.6673,
      "step": 779500
    },
    {
      "epoch": 59.55236421969292,
      "grad_norm": 5.705017566680908,
      "learning_rate": 3.730298169225677e-07,
      "loss": 1.6645,
      "step": 779600
    },
    {
      "epoch": 59.56000305553434,
      "grad_norm": 4.828662872314453,
      "learning_rate": 3.666641203880529e-07,
      "loss": 1.6147,
      "step": 779700
    },
    {
      "epoch": 59.56764189137576,
      "grad_norm": 6.566033840179443,
      "learning_rate": 3.6029842385353807e-07,
      "loss": 1.6611,
      "step": 779800
    },
    {
      "epoch": 59.57528072721717,
      "grad_norm": 5.717811584472656,
      "learning_rate": 3.539327273190233e-07,
      "loss": 1.6079,
      "step": 779900
    },
    {
      "epoch": 59.58291956305859,
      "grad_norm": 5.916950702667236,
      "learning_rate": 3.475670307845084e-07,
      "loss": 1.6965,
      "step": 780000
    },
    {
      "epoch": 59.59055839890001,
      "grad_norm": 7.219481945037842,
      "learning_rate": 3.4120133424999365e-07,
      "loss": 1.714,
      "step": 780100
    },
    {
      "epoch": 59.59819723474143,
      "grad_norm": 7.10008430480957,
      "learning_rate": 3.3483563771547883e-07,
      "loss": 1.6837,
      "step": 780200
    },
    {
      "epoch": 59.605836070582846,
      "grad_norm": 6.189722537994385,
      "learning_rate": 3.28469941180964e-07,
      "loss": 1.6352,
      "step": 780300
    },
    {
      "epoch": 59.61347490642426,
      "grad_norm": 5.862355709075928,
      "learning_rate": 3.2210424464644924e-07,
      "loss": 1.6306,
      "step": 780400
    },
    {
      "epoch": 59.62111374226568,
      "grad_norm": 6.040161609649658,
      "learning_rate": 3.157385481119344e-07,
      "loss": 1.6676,
      "step": 780500
    },
    {
      "epoch": 59.6287525781071,
      "grad_norm": 5.634494304656982,
      "learning_rate": 3.093728515774196e-07,
      "loss": 1.7336,
      "step": 780600
    },
    {
      "epoch": 59.636391413948516,
      "grad_norm": 3.984654426574707,
      "learning_rate": 3.0300715504290483e-07,
      "loss": 1.5867,
      "step": 780700
    },
    {
      "epoch": 59.644030249789935,
      "grad_norm": 5.712482452392578,
      "learning_rate": 2.9664145850839e-07,
      "loss": 1.7626,
      "step": 780800
    },
    {
      "epoch": 59.65166908563135,
      "grad_norm": 5.936000823974609,
      "learning_rate": 2.902757619738752e-07,
      "loss": 1.6778,
      "step": 780900
    },
    {
      "epoch": 59.65930792147277,
      "grad_norm": 5.93980073928833,
      "learning_rate": 2.839100654393604e-07,
      "loss": 1.6692,
      "step": 781000
    },
    {
      "epoch": 59.666946757314186,
      "grad_norm": 5.716855049133301,
      "learning_rate": 2.775443689048456e-07,
      "loss": 1.6006,
      "step": 781100
    },
    {
      "epoch": 59.674585593155605,
      "grad_norm": 5.849826335906982,
      "learning_rate": 2.7117867237033077e-07,
      "loss": 1.7574,
      "step": 781200
    },
    {
      "epoch": 59.682224428997024,
      "grad_norm": 5.6581621170043945,
      "learning_rate": 2.64812975835816e-07,
      "loss": 1.5627,
      "step": 781300
    },
    {
      "epoch": 59.689863264838436,
      "grad_norm": 5.566331386566162,
      "learning_rate": 2.584472793013012e-07,
      "loss": 1.6393,
      "step": 781400
    },
    {
      "epoch": 59.697502100679856,
      "grad_norm": 4.435230731964111,
      "learning_rate": 2.5208158276678636e-07,
      "loss": 1.6494,
      "step": 781500
    },
    {
      "epoch": 59.705140936521275,
      "grad_norm": 5.670032501220703,
      "learning_rate": 2.457158862322716e-07,
      "loss": 1.7261,
      "step": 781600
    },
    {
      "epoch": 59.712779772362694,
      "grad_norm": 6.807234764099121,
      "learning_rate": 2.3935018969775676e-07,
      "loss": 1.6668,
      "step": 781700
    },
    {
      "epoch": 59.72041860820411,
      "grad_norm": 5.040987014770508,
      "learning_rate": 2.3298449316324194e-07,
      "loss": 1.5801,
      "step": 781800
    },
    {
      "epoch": 59.728057444045525,
      "grad_norm": 4.585339546203613,
      "learning_rate": 2.2661879662872715e-07,
      "loss": 1.7427,
      "step": 781900
    },
    {
      "epoch": 59.735696279886945,
      "grad_norm": 5.625013828277588,
      "learning_rate": 2.202531000942123e-07,
      "loss": 1.7391,
      "step": 782000
    },
    {
      "epoch": 59.743335115728364,
      "grad_norm": 5.060871124267578,
      "learning_rate": 2.138874035596975e-07,
      "loss": 1.6065,
      "step": 782100
    },
    {
      "epoch": 59.75097395156978,
      "grad_norm": 4.913209915161133,
      "learning_rate": 2.0752170702518268e-07,
      "loss": 1.6637,
      "step": 782200
    },
    {
      "epoch": 59.758612787411195,
      "grad_norm": 5.07354736328125,
      "learning_rate": 2.0115601049066788e-07,
      "loss": 1.6628,
      "step": 782300
    },
    {
      "epoch": 59.766251623252614,
      "grad_norm": 6.760327339172363,
      "learning_rate": 1.947903139561531e-07,
      "loss": 1.6736,
      "step": 782400
    },
    {
      "epoch": 59.773890459094034,
      "grad_norm": 4.309381484985352,
      "learning_rate": 1.8842461742163827e-07,
      "loss": 1.6797,
      "step": 782500
    },
    {
      "epoch": 59.78152929493545,
      "grad_norm": 4.16554069519043,
      "learning_rate": 1.8205892088712347e-07,
      "loss": 1.6692,
      "step": 782600
    },
    {
      "epoch": 59.78916813077687,
      "grad_norm": 5.553100109100342,
      "learning_rate": 1.7569322435260867e-07,
      "loss": 1.7095,
      "step": 782700
    },
    {
      "epoch": 59.796806966618284,
      "grad_norm": 5.158059120178223,
      "learning_rate": 1.6932752781809385e-07,
      "loss": 1.688,
      "step": 782800
    },
    {
      "epoch": 59.8044458024597,
      "grad_norm": 4.8582563400268555,
      "learning_rate": 1.6296183128357906e-07,
      "loss": 1.6163,
      "step": 782900
    },
    {
      "epoch": 59.81208463830112,
      "grad_norm": 5.649268627166748,
      "learning_rate": 1.5659613474906426e-07,
      "loss": 1.6945,
      "step": 783000
    },
    {
      "epoch": 59.81972347414254,
      "grad_norm": 7.737738132476807,
      "learning_rate": 1.5023043821454944e-07,
      "loss": 1.6237,
      "step": 783100
    },
    {
      "epoch": 59.82736230998396,
      "grad_norm": 6.737875461578369,
      "learning_rate": 1.4386474168003464e-07,
      "loss": 1.6321,
      "step": 783200
    },
    {
      "epoch": 59.83500114582537,
      "grad_norm": 4.524415016174316,
      "learning_rate": 1.3749904514551985e-07,
      "loss": 1.6605,
      "step": 783300
    },
    {
      "epoch": 59.84263998166679,
      "grad_norm": 6.829458236694336,
      "learning_rate": 1.3113334861100502e-07,
      "loss": 1.6282,
      "step": 783400
    },
    {
      "epoch": 59.85027881750821,
      "grad_norm": 5.301250457763672,
      "learning_rate": 1.2476765207649023e-07,
      "loss": 1.6977,
      "step": 783500
    },
    {
      "epoch": 59.85791765334963,
      "grad_norm": 5.746587753295898,
      "learning_rate": 1.184019555419754e-07,
      "loss": 1.6316,
      "step": 783600
    },
    {
      "epoch": 59.86555648919105,
      "grad_norm": 9.533252716064453,
      "learning_rate": 1.120362590074606e-07,
      "loss": 1.6101,
      "step": 783700
    },
    {
      "epoch": 59.87319532503246,
      "grad_norm": 5.8593668937683105,
      "learning_rate": 1.0567056247294579e-07,
      "loss": 1.6539,
      "step": 783800
    },
    {
      "epoch": 59.88083416087388,
      "grad_norm": 5.492667198181152,
      "learning_rate": 9.930486593843098e-08,
      "loss": 1.7091,
      "step": 783900
    },
    {
      "epoch": 59.8884729967153,
      "grad_norm": 5.822889804840088,
      "learning_rate": 9.293916940391618e-08,
      "loss": 1.6794,
      "step": 784000
    },
    {
      "epoch": 59.89611183255672,
      "grad_norm": 4.874628067016602,
      "learning_rate": 8.657347286940138e-08,
      "loss": 1.6509,
      "step": 784100
    },
    {
      "epoch": 59.90375066839814,
      "grad_norm": 4.3855438232421875,
      "learning_rate": 8.020777633488657e-08,
      "loss": 1.5895,
      "step": 784200
    },
    {
      "epoch": 59.91138950423955,
      "grad_norm": 7.407977104187012,
      "learning_rate": 7.384207980037176e-08,
      "loss": 1.656,
      "step": 784300
    },
    {
      "epoch": 59.91902834008097,
      "grad_norm": 8.792720794677734,
      "learning_rate": 6.747638326585695e-08,
      "loss": 1.6587,
      "step": 784400
    },
    {
      "epoch": 59.92666717592239,
      "grad_norm": 6.256693363189697,
      "learning_rate": 6.111068673134214e-08,
      "loss": 1.5463,
      "step": 784500
    },
    {
      "epoch": 59.93430601176381,
      "grad_norm": 5.763101100921631,
      "learning_rate": 5.4744990196827344e-08,
      "loss": 1.6254,
      "step": 784600
    },
    {
      "epoch": 59.94194484760523,
      "grad_norm": 6.1437201499938965,
      "learning_rate": 4.837929366231253e-08,
      "loss": 1.6353,
      "step": 784700
    },
    {
      "epoch": 59.94958368344664,
      "grad_norm": 4.173142910003662,
      "learning_rate": 4.2013597127797726e-08,
      "loss": 1.6227,
      "step": 784800
    },
    {
      "epoch": 59.95722251928806,
      "grad_norm": 5.919569492340088,
      "learning_rate": 3.5647900593282924e-08,
      "loss": 1.6286,
      "step": 784900
    },
    {
      "epoch": 59.96486135512948,
      "grad_norm": 5.664323329925537,
      "learning_rate": 2.928220405876811e-08,
      "loss": 1.6176,
      "step": 785000
    },
    {
      "epoch": 59.9725001909709,
      "grad_norm": 5.093532562255859,
      "learning_rate": 2.2916507524253306e-08,
      "loss": 1.6141,
      "step": 785100
    },
    {
      "epoch": 59.98013902681232,
      "grad_norm": 5.6722941398620605,
      "learning_rate": 1.65508109897385e-08,
      "loss": 1.7383,
      "step": 785200
    },
    {
      "epoch": 59.98777786265373,
      "grad_norm": 5.176349639892578,
      "learning_rate": 1.0185114455223691e-08,
      "loss": 1.6267,
      "step": 785300
    },
    {
      "epoch": 59.99541669849515,
      "grad_norm": 5.499408721923828,
      "learning_rate": 3.819417920708884e-09,
      "loss": 1.7076,
      "step": 785400
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.7666943073272705,
      "eval_runtime": 2.9492,
      "eval_samples_per_second": 233.964,
      "eval_steps_per_second": 233.964,
      "step": 785460
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.4196614027023315,
      "eval_runtime": 68.3807,
      "eval_samples_per_second": 191.443,
      "eval_steps_per_second": 191.443,
      "step": 785460
    }
  ],
  "logging_steps": 100,
  "max_steps": 785460,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 486049183027200.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
