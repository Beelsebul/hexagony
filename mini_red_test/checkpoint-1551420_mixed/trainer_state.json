{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 60.0,
  "eval_steps": 500,
  "global_step": 1551420,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00386742468190432,
      "grad_norm": 2.2242796421051025,
      "learning_rate": 4.999677714609842e-05,
      "loss": 4.8217,
      "step": 100
    },
    {
      "epoch": 0.00773484936380864,
      "grad_norm": 4.00025749206543,
      "learning_rate": 4.9993554292196825e-05,
      "loss": 4.6794,
      "step": 200
    },
    {
      "epoch": 0.01160227404571296,
      "grad_norm": 4.104935169219971,
      "learning_rate": 4.999033143829524e-05,
      "loss": 4.5559,
      "step": 300
    },
    {
      "epoch": 0.01546969872761728,
      "grad_norm": 4.368701934814453,
      "learning_rate": 4.9987108584393654e-05,
      "loss": 4.4746,
      "step": 400
    },
    {
      "epoch": 0.0193371234095216,
      "grad_norm": 5.557064533233643,
      "learning_rate": 4.998388573049207e-05,
      "loss": 4.385,
      "step": 500
    },
    {
      "epoch": 0.02320454809142592,
      "grad_norm": 4.856510639190674,
      "learning_rate": 4.998066287659048e-05,
      "loss": 4.3089,
      "step": 600
    },
    {
      "epoch": 0.02707197277333024,
      "grad_norm": 5.217531681060791,
      "learning_rate": 4.997744002268889e-05,
      "loss": 4.2255,
      "step": 700
    },
    {
      "epoch": 0.03093939745523456,
      "grad_norm": 4.637568950653076,
      "learning_rate": 4.9974217168787307e-05,
      "loss": 4.1791,
      "step": 800
    },
    {
      "epoch": 0.03480682213713888,
      "grad_norm": 3.0582213401794434,
      "learning_rate": 4.997099431488572e-05,
      "loss": 4.1092,
      "step": 900
    },
    {
      "epoch": 0.0386742468190432,
      "grad_norm": 5.075669288635254,
      "learning_rate": 4.996777146098413e-05,
      "loss": 4.0391,
      "step": 1000
    },
    {
      "epoch": 0.042541671500947516,
      "grad_norm": 5.160064220428467,
      "learning_rate": 4.9964548607082544e-05,
      "loss": 4.0181,
      "step": 1100
    },
    {
      "epoch": 0.04640909618285184,
      "grad_norm": 4.883174419403076,
      "learning_rate": 4.996132575318096e-05,
      "loss": 3.9815,
      "step": 1200
    },
    {
      "epoch": 0.05027652086475616,
      "grad_norm": 3.7997310161590576,
      "learning_rate": 4.9958102899279373e-05,
      "loss": 3.9478,
      "step": 1300
    },
    {
      "epoch": 0.05414394554666048,
      "grad_norm": 3.7105050086975098,
      "learning_rate": 4.995488004537778e-05,
      "loss": 3.9228,
      "step": 1400
    },
    {
      "epoch": 0.0580113702285648,
      "grad_norm": 6.453140735626221,
      "learning_rate": 4.9951657191476196e-05,
      "loss": 3.8971,
      "step": 1500
    },
    {
      "epoch": 0.06187879491046912,
      "grad_norm": 4.975737571716309,
      "learning_rate": 4.994843433757461e-05,
      "loss": 3.8464,
      "step": 1600
    },
    {
      "epoch": 0.06574621959237344,
      "grad_norm": 4.278599262237549,
      "learning_rate": 4.9945211483673026e-05,
      "loss": 3.832,
      "step": 1700
    },
    {
      "epoch": 0.06961364427427776,
      "grad_norm": 4.978128433227539,
      "learning_rate": 4.9941988629771433e-05,
      "loss": 3.8288,
      "step": 1800
    },
    {
      "epoch": 0.07348106895618207,
      "grad_norm": 6.901026248931885,
      "learning_rate": 4.993876577586985e-05,
      "loss": 3.7735,
      "step": 1900
    },
    {
      "epoch": 0.0773484936380864,
      "grad_norm": 3.7524824142456055,
      "learning_rate": 4.993554292196826e-05,
      "loss": 3.7595,
      "step": 2000
    },
    {
      "epoch": 0.08121591831999071,
      "grad_norm": 3.611414909362793,
      "learning_rate": 4.993232006806668e-05,
      "loss": 3.7494,
      "step": 2100
    },
    {
      "epoch": 0.08508334300189503,
      "grad_norm": 6.67779541015625,
      "learning_rate": 4.9929097214165086e-05,
      "loss": 3.7252,
      "step": 2200
    },
    {
      "epoch": 0.08895076768379936,
      "grad_norm": 3.9687623977661133,
      "learning_rate": 4.99258743602635e-05,
      "loss": 3.6972,
      "step": 2300
    },
    {
      "epoch": 0.09281819236570368,
      "grad_norm": 5.950343132019043,
      "learning_rate": 4.9922651506361915e-05,
      "loss": 3.7226,
      "step": 2400
    },
    {
      "epoch": 0.096685617047608,
      "grad_norm": 5.172548770904541,
      "learning_rate": 4.991942865246033e-05,
      "loss": 3.7067,
      "step": 2500
    },
    {
      "epoch": 0.10055304172951232,
      "grad_norm": 4.089677333831787,
      "learning_rate": 4.991620579855874e-05,
      "loss": 3.6825,
      "step": 2600
    },
    {
      "epoch": 0.10442046641141664,
      "grad_norm": 4.208201885223389,
      "learning_rate": 4.991298294465715e-05,
      "loss": 3.6478,
      "step": 2700
    },
    {
      "epoch": 0.10828789109332096,
      "grad_norm": 6.2388787269592285,
      "learning_rate": 4.990976009075557e-05,
      "loss": 3.6572,
      "step": 2800
    },
    {
      "epoch": 0.11215531577522528,
      "grad_norm": 4.840461730957031,
      "learning_rate": 4.990653723685398e-05,
      "loss": 3.6415,
      "step": 2900
    },
    {
      "epoch": 0.1160227404571296,
      "grad_norm": 8.013736724853516,
      "learning_rate": 4.990331438295239e-05,
      "loss": 3.5868,
      "step": 3000
    },
    {
      "epoch": 0.11989016513903392,
      "grad_norm": 4.94861364364624,
      "learning_rate": 4.9900091529050804e-05,
      "loss": 3.6055,
      "step": 3100
    },
    {
      "epoch": 0.12375758982093824,
      "grad_norm": 5.515712738037109,
      "learning_rate": 4.989686867514922e-05,
      "loss": 3.5781,
      "step": 3200
    },
    {
      "epoch": 0.12762501450284255,
      "grad_norm": 5.224508762359619,
      "learning_rate": 4.9893645821247634e-05,
      "loss": 3.624,
      "step": 3300
    },
    {
      "epoch": 0.13149243918474687,
      "grad_norm": 6.632035255432129,
      "learning_rate": 4.989042296734605e-05,
      "loss": 3.6122,
      "step": 3400
    },
    {
      "epoch": 0.1353598638666512,
      "grad_norm": 10.370353698730469,
      "learning_rate": 4.988720011344446e-05,
      "loss": 3.5625,
      "step": 3500
    },
    {
      "epoch": 0.1392272885485555,
      "grad_norm": 5.958041191101074,
      "learning_rate": 4.988397725954287e-05,
      "loss": 3.5564,
      "step": 3600
    },
    {
      "epoch": 0.14309471323045983,
      "grad_norm": 4.620528221130371,
      "learning_rate": 4.9880754405641286e-05,
      "loss": 3.5592,
      "step": 3700
    },
    {
      "epoch": 0.14696213791236415,
      "grad_norm": 5.524900913238525,
      "learning_rate": 4.98775315517397e-05,
      "loss": 3.5784,
      "step": 3800
    },
    {
      "epoch": 0.15082956259426847,
      "grad_norm": 6.01129150390625,
      "learning_rate": 4.9874308697838115e-05,
      "loss": 3.505,
      "step": 3900
    },
    {
      "epoch": 0.1546969872761728,
      "grad_norm": 6.645888805389404,
      "learning_rate": 4.987108584393653e-05,
      "loss": 3.5275,
      "step": 4000
    },
    {
      "epoch": 0.1585644119580771,
      "grad_norm": 7.222827434539795,
      "learning_rate": 4.986786299003494e-05,
      "loss": 3.5606,
      "step": 4100
    },
    {
      "epoch": 0.16243183663998143,
      "grad_norm": 5.5867462158203125,
      "learning_rate": 4.986464013613335e-05,
      "loss": 3.4968,
      "step": 4200
    },
    {
      "epoch": 0.16629926132188574,
      "grad_norm": 6.453340530395508,
      "learning_rate": 4.986141728223177e-05,
      "loss": 3.5031,
      "step": 4300
    },
    {
      "epoch": 0.17016668600379006,
      "grad_norm": 3.976266860961914,
      "learning_rate": 4.985819442833018e-05,
      "loss": 3.5137,
      "step": 4400
    },
    {
      "epoch": 0.1740341106856944,
      "grad_norm": 6.720580577850342,
      "learning_rate": 4.985497157442859e-05,
      "loss": 3.4871,
      "step": 4500
    },
    {
      "epoch": 0.17790153536759873,
      "grad_norm": 5.676624298095703,
      "learning_rate": 4.9851748720527005e-05,
      "loss": 3.4938,
      "step": 4600
    },
    {
      "epoch": 0.18176896004950305,
      "grad_norm": 5.962072849273682,
      "learning_rate": 4.984852586662542e-05,
      "loss": 3.4716,
      "step": 4700
    },
    {
      "epoch": 0.18563638473140737,
      "grad_norm": 4.767612934112549,
      "learning_rate": 4.9845303012723834e-05,
      "loss": 3.4572,
      "step": 4800
    },
    {
      "epoch": 0.18950380941331169,
      "grad_norm": 5.283890247344971,
      "learning_rate": 4.984208015882224e-05,
      "loss": 3.4832,
      "step": 4900
    },
    {
      "epoch": 0.193371234095216,
      "grad_norm": 7.926885604858398,
      "learning_rate": 4.983885730492066e-05,
      "loss": 3.4749,
      "step": 5000
    },
    {
      "epoch": 0.19723865877712032,
      "grad_norm": 5.902615070343018,
      "learning_rate": 4.983563445101907e-05,
      "loss": 3.472,
      "step": 5100
    },
    {
      "epoch": 0.20110608345902464,
      "grad_norm": 5.764787673950195,
      "learning_rate": 4.9832411597117486e-05,
      "loss": 3.4103,
      "step": 5200
    },
    {
      "epoch": 0.20497350814092896,
      "grad_norm": 6.988839626312256,
      "learning_rate": 4.9829188743215894e-05,
      "loss": 3.4524,
      "step": 5300
    },
    {
      "epoch": 0.20884093282283328,
      "grad_norm": 5.804410934448242,
      "learning_rate": 4.982596588931431e-05,
      "loss": 3.434,
      "step": 5400
    },
    {
      "epoch": 0.2127083575047376,
      "grad_norm": 5.391355991363525,
      "learning_rate": 4.9822743035412724e-05,
      "loss": 3.4573,
      "step": 5500
    },
    {
      "epoch": 0.21657578218664192,
      "grad_norm": 5.926646709442139,
      "learning_rate": 4.981952018151114e-05,
      "loss": 3.4638,
      "step": 5600
    },
    {
      "epoch": 0.22044320686854624,
      "grad_norm": 6.195850849151611,
      "learning_rate": 4.9816297327609546e-05,
      "loss": 3.4334,
      "step": 5700
    },
    {
      "epoch": 0.22431063155045056,
      "grad_norm": 17.188232421875,
      "learning_rate": 4.981307447370796e-05,
      "loss": 3.4122,
      "step": 5800
    },
    {
      "epoch": 0.22817805623235488,
      "grad_norm": 5.675980567932129,
      "learning_rate": 4.9809851619806376e-05,
      "loss": 3.4451,
      "step": 5900
    },
    {
      "epoch": 0.2320454809142592,
      "grad_norm": 9.39175796508789,
      "learning_rate": 4.9806628765904784e-05,
      "loss": 3.4726,
      "step": 6000
    },
    {
      "epoch": 0.2359129055961635,
      "grad_norm": 5.350219249725342,
      "learning_rate": 4.98034059120032e-05,
      "loss": 3.4417,
      "step": 6100
    },
    {
      "epoch": 0.23978033027806783,
      "grad_norm": 6.1121954917907715,
      "learning_rate": 4.980018305810161e-05,
      "loss": 3.43,
      "step": 6200
    },
    {
      "epoch": 0.24364775495997215,
      "grad_norm": 7.094221115112305,
      "learning_rate": 4.979696020420003e-05,
      "loss": 3.3852,
      "step": 6300
    },
    {
      "epoch": 0.24751517964187647,
      "grad_norm": 5.140956878662109,
      "learning_rate": 4.9793737350298436e-05,
      "loss": 3.3981,
      "step": 6400
    },
    {
      "epoch": 0.2513826043237808,
      "grad_norm": 6.612984657287598,
      "learning_rate": 4.979051449639685e-05,
      "loss": 3.4254,
      "step": 6500
    },
    {
      "epoch": 0.2552500290056851,
      "grad_norm": 7.211151123046875,
      "learning_rate": 4.9787291642495265e-05,
      "loss": 3.38,
      "step": 6600
    },
    {
      "epoch": 0.2591174536875894,
      "grad_norm": 6.347944259643555,
      "learning_rate": 4.978406878859368e-05,
      "loss": 3.3778,
      "step": 6700
    },
    {
      "epoch": 0.26298487836949375,
      "grad_norm": 7.545178413391113,
      "learning_rate": 4.978084593469209e-05,
      "loss": 3.3827,
      "step": 6800
    },
    {
      "epoch": 0.26685230305139807,
      "grad_norm": 7.801607608795166,
      "learning_rate": 4.97776230807905e-05,
      "loss": 3.3919,
      "step": 6900
    },
    {
      "epoch": 0.2707197277333024,
      "grad_norm": 7.0675811767578125,
      "learning_rate": 4.977440022688892e-05,
      "loss": 3.3559,
      "step": 7000
    },
    {
      "epoch": 0.2745871524152067,
      "grad_norm": 6.567689895629883,
      "learning_rate": 4.977117737298733e-05,
      "loss": 3.408,
      "step": 7100
    },
    {
      "epoch": 0.278454577097111,
      "grad_norm": 6.322761535644531,
      "learning_rate": 4.976795451908574e-05,
      "loss": 3.3179,
      "step": 7200
    },
    {
      "epoch": 0.28232200177901534,
      "grad_norm": 5.865058898925781,
      "learning_rate": 4.9764731665184155e-05,
      "loss": 3.3174,
      "step": 7300
    },
    {
      "epoch": 0.28618942646091966,
      "grad_norm": 10.074893951416016,
      "learning_rate": 4.976150881128257e-05,
      "loss": 3.3819,
      "step": 7400
    },
    {
      "epoch": 0.290056851142824,
      "grad_norm": 6.39840841293335,
      "learning_rate": 4.9758285957380984e-05,
      "loss": 3.3498,
      "step": 7500
    },
    {
      "epoch": 0.2939242758247283,
      "grad_norm": 7.63909912109375,
      "learning_rate": 4.975506310347939e-05,
      "loss": 3.264,
      "step": 7600
    },
    {
      "epoch": 0.2977917005066326,
      "grad_norm": 9.480682373046875,
      "learning_rate": 4.975184024957781e-05,
      "loss": 3.387,
      "step": 7700
    },
    {
      "epoch": 0.30165912518853694,
      "grad_norm": 8.144660949707031,
      "learning_rate": 4.974861739567622e-05,
      "loss": 3.3779,
      "step": 7800
    },
    {
      "epoch": 0.30552654987044126,
      "grad_norm": 8.389481544494629,
      "learning_rate": 4.9745394541774636e-05,
      "loss": 3.3582,
      "step": 7900
    },
    {
      "epoch": 0.3093939745523456,
      "grad_norm": 6.469958782196045,
      "learning_rate": 4.9742171687873044e-05,
      "loss": 3.3478,
      "step": 8000
    },
    {
      "epoch": 0.3132613992342499,
      "grad_norm": 7.464743614196777,
      "learning_rate": 4.973894883397146e-05,
      "loss": 3.3737,
      "step": 8100
    },
    {
      "epoch": 0.3171288239161542,
      "grad_norm": 9.653414726257324,
      "learning_rate": 4.9735725980069874e-05,
      "loss": 3.3824,
      "step": 8200
    },
    {
      "epoch": 0.32099624859805853,
      "grad_norm": 7.062738418579102,
      "learning_rate": 4.973250312616829e-05,
      "loss": 3.3509,
      "step": 8300
    },
    {
      "epoch": 0.32486367327996285,
      "grad_norm": 6.851805210113525,
      "learning_rate": 4.9729280272266696e-05,
      "loss": 3.3354,
      "step": 8400
    },
    {
      "epoch": 0.32873109796186717,
      "grad_norm": 5.241024494171143,
      "learning_rate": 4.972605741836511e-05,
      "loss": 3.3267,
      "step": 8500
    },
    {
      "epoch": 0.3325985226437715,
      "grad_norm": 6.771374225616455,
      "learning_rate": 4.9722834564463526e-05,
      "loss": 3.2686,
      "step": 8600
    },
    {
      "epoch": 0.3364659473256758,
      "grad_norm": 6.463833332061768,
      "learning_rate": 4.971961171056194e-05,
      "loss": 3.2317,
      "step": 8700
    },
    {
      "epoch": 0.3403333720075801,
      "grad_norm": 5.955878257751465,
      "learning_rate": 4.971638885666035e-05,
      "loss": 3.283,
      "step": 8800
    },
    {
      "epoch": 0.34420079668948445,
      "grad_norm": 10.497576713562012,
      "learning_rate": 4.971316600275876e-05,
      "loss": 3.3221,
      "step": 8900
    },
    {
      "epoch": 0.3480682213713888,
      "grad_norm": 7.210210800170898,
      "learning_rate": 4.970994314885718e-05,
      "loss": 3.3326,
      "step": 9000
    },
    {
      "epoch": 0.35193564605329314,
      "grad_norm": 8.48784351348877,
      "learning_rate": 4.9706720294955586e-05,
      "loss": 3.2459,
      "step": 9100
    },
    {
      "epoch": 0.35580307073519746,
      "grad_norm": 7.547479629516602,
      "learning_rate": 4.9703497441054e-05,
      "loss": 3.2772,
      "step": 9200
    },
    {
      "epoch": 0.3596704954171018,
      "grad_norm": 6.232669353485107,
      "learning_rate": 4.9700274587152415e-05,
      "loss": 3.295,
      "step": 9300
    },
    {
      "epoch": 0.3635379200990061,
      "grad_norm": 7.856706142425537,
      "learning_rate": 4.969705173325083e-05,
      "loss": 3.3516,
      "step": 9400
    },
    {
      "epoch": 0.3674053447809104,
      "grad_norm": 7.027344226837158,
      "learning_rate": 4.969382887934924e-05,
      "loss": 3.3144,
      "step": 9500
    },
    {
      "epoch": 0.37127276946281473,
      "grad_norm": 8.706036567687988,
      "learning_rate": 4.969060602544765e-05,
      "loss": 3.2498,
      "step": 9600
    },
    {
      "epoch": 0.37514019414471905,
      "grad_norm": 6.025588035583496,
      "learning_rate": 4.968738317154607e-05,
      "loss": 3.2845,
      "step": 9700
    },
    {
      "epoch": 0.37900761882662337,
      "grad_norm": 6.4031982421875,
      "learning_rate": 4.968416031764448e-05,
      "loss": 3.2782,
      "step": 9800
    },
    {
      "epoch": 0.3828750435085277,
      "grad_norm": 6.541450500488281,
      "learning_rate": 4.96809374637429e-05,
      "loss": 3.3295,
      "step": 9900
    },
    {
      "epoch": 0.386742468190432,
      "grad_norm": 7.1404900550842285,
      "learning_rate": 4.9677714609841305e-05,
      "loss": 3.2794,
      "step": 10000
    },
    {
      "epoch": 0.39060989287233633,
      "grad_norm": 6.5013813972473145,
      "learning_rate": 4.967449175593972e-05,
      "loss": 3.2802,
      "step": 10100
    },
    {
      "epoch": 0.39447731755424065,
      "grad_norm": 12.384428024291992,
      "learning_rate": 4.9671268902038134e-05,
      "loss": 3.2701,
      "step": 10200
    },
    {
      "epoch": 0.39834474223614497,
      "grad_norm": 7.786991596221924,
      "learning_rate": 4.966804604813655e-05,
      "loss": 3.3194,
      "step": 10300
    },
    {
      "epoch": 0.4022121669180493,
      "grad_norm": 7.381240367889404,
      "learning_rate": 4.9664823194234964e-05,
      "loss": 3.2986,
      "step": 10400
    },
    {
      "epoch": 0.4060795915999536,
      "grad_norm": 8.774725914001465,
      "learning_rate": 4.966160034033338e-05,
      "loss": 3.2889,
      "step": 10500
    },
    {
      "epoch": 0.4099470162818579,
      "grad_norm": 7.538899898529053,
      "learning_rate": 4.9658377486431786e-05,
      "loss": 3.2532,
      "step": 10600
    },
    {
      "epoch": 0.41381444096376224,
      "grad_norm": 9.620382308959961,
      "learning_rate": 4.96551546325302e-05,
      "loss": 3.227,
      "step": 10700
    },
    {
      "epoch": 0.41768186564566656,
      "grad_norm": 8.227100372314453,
      "learning_rate": 4.9651931778628616e-05,
      "loss": 3.2576,
      "step": 10800
    },
    {
      "epoch": 0.4215492903275709,
      "grad_norm": 5.748256206512451,
      "learning_rate": 4.964870892472703e-05,
      "loss": 3.2826,
      "step": 10900
    },
    {
      "epoch": 0.4254167150094752,
      "grad_norm": 9.117942810058594,
      "learning_rate": 4.9645486070825445e-05,
      "loss": 3.2414,
      "step": 11000
    },
    {
      "epoch": 0.4292841396913795,
      "grad_norm": 9.39196491241455,
      "learning_rate": 4.964226321692385e-05,
      "loss": 3.2465,
      "step": 11100
    },
    {
      "epoch": 0.43315156437328384,
      "grad_norm": 9.392311096191406,
      "learning_rate": 4.963904036302227e-05,
      "loss": 3.2486,
      "step": 11200
    },
    {
      "epoch": 0.43701898905518816,
      "grad_norm": 10.657303810119629,
      "learning_rate": 4.963581750912068e-05,
      "loss": 3.247,
      "step": 11300
    },
    {
      "epoch": 0.4408864137370925,
      "grad_norm": 7.559788703918457,
      "learning_rate": 4.96325946552191e-05,
      "loss": 3.1931,
      "step": 11400
    },
    {
      "epoch": 0.4447538384189968,
      "grad_norm": 7.416409969329834,
      "learning_rate": 4.9629371801317505e-05,
      "loss": 3.222,
      "step": 11500
    },
    {
      "epoch": 0.4486212631009011,
      "grad_norm": 6.96967887878418,
      "learning_rate": 4.962614894741592e-05,
      "loss": 3.2967,
      "step": 11600
    },
    {
      "epoch": 0.45248868778280543,
      "grad_norm": 8.38740062713623,
      "learning_rate": 4.9622926093514335e-05,
      "loss": 3.1733,
      "step": 11700
    },
    {
      "epoch": 0.45635611246470975,
      "grad_norm": 9.53358268737793,
      "learning_rate": 4.961970323961275e-05,
      "loss": 3.2343,
      "step": 11800
    },
    {
      "epoch": 0.46022353714661407,
      "grad_norm": 7.919357776641846,
      "learning_rate": 4.961648038571116e-05,
      "loss": 3.2577,
      "step": 11900
    },
    {
      "epoch": 0.4640909618285184,
      "grad_norm": 7.678158760070801,
      "learning_rate": 4.961325753180957e-05,
      "loss": 3.2302,
      "step": 12000
    },
    {
      "epoch": 0.4679583865104227,
      "grad_norm": 8.107439041137695,
      "learning_rate": 4.961003467790799e-05,
      "loss": 3.2342,
      "step": 12100
    },
    {
      "epoch": 0.471825811192327,
      "grad_norm": 6.092660427093506,
      "learning_rate": 4.9606811824006395e-05,
      "loss": 3.2286,
      "step": 12200
    },
    {
      "epoch": 0.47569323587423135,
      "grad_norm": 7.833790302276611,
      "learning_rate": 4.960358897010481e-05,
      "loss": 3.2568,
      "step": 12300
    },
    {
      "epoch": 0.47956066055613567,
      "grad_norm": 6.242001056671143,
      "learning_rate": 4.9600366116203224e-05,
      "loss": 3.2447,
      "step": 12400
    },
    {
      "epoch": 0.48342808523804,
      "grad_norm": 7.935070514678955,
      "learning_rate": 4.959714326230164e-05,
      "loss": 3.2496,
      "step": 12500
    },
    {
      "epoch": 0.4872955099199443,
      "grad_norm": 7.294278621673584,
      "learning_rate": 4.959392040840005e-05,
      "loss": 3.2666,
      "step": 12600
    },
    {
      "epoch": 0.4911629346018486,
      "grad_norm": 6.602031707763672,
      "learning_rate": 4.959069755449846e-05,
      "loss": 3.2281,
      "step": 12700
    },
    {
      "epoch": 0.49503035928375294,
      "grad_norm": 6.3521881103515625,
      "learning_rate": 4.9587474700596876e-05,
      "loss": 3.2009,
      "step": 12800
    },
    {
      "epoch": 0.49889778396565726,
      "grad_norm": 8.472763061523438,
      "learning_rate": 4.958425184669529e-05,
      "loss": 3.1833,
      "step": 12900
    },
    {
      "epoch": 0.5027652086475616,
      "grad_norm": 7.101345539093018,
      "learning_rate": 4.95810289927937e-05,
      "loss": 3.1953,
      "step": 13000
    },
    {
      "epoch": 0.506632633329466,
      "grad_norm": 8.63314437866211,
      "learning_rate": 4.9577806138892114e-05,
      "loss": 3.22,
      "step": 13100
    },
    {
      "epoch": 0.5105000580113702,
      "grad_norm": 8.261184692382812,
      "learning_rate": 4.957458328499053e-05,
      "loss": 3.2254,
      "step": 13200
    },
    {
      "epoch": 0.5143674826932746,
      "grad_norm": 8.131250381469727,
      "learning_rate": 4.957136043108894e-05,
      "loss": 3.1379,
      "step": 13300
    },
    {
      "epoch": 0.5182349073751789,
      "grad_norm": 7.424875736236572,
      "learning_rate": 4.956813757718735e-05,
      "loss": 3.2232,
      "step": 13400
    },
    {
      "epoch": 0.5221023320570832,
      "grad_norm": 7.653655529022217,
      "learning_rate": 4.9564914723285766e-05,
      "loss": 3.1965,
      "step": 13500
    },
    {
      "epoch": 0.5259697567389875,
      "grad_norm": 6.400707244873047,
      "learning_rate": 4.956169186938418e-05,
      "loss": 3.2685,
      "step": 13600
    },
    {
      "epoch": 0.5298371814208919,
      "grad_norm": 9.125215530395508,
      "learning_rate": 4.9558469015482595e-05,
      "loss": 3.1634,
      "step": 13700
    },
    {
      "epoch": 0.5337046061027961,
      "grad_norm": 7.580578327178955,
      "learning_rate": 4.9555246161581e-05,
      "loss": 3.1709,
      "step": 13800
    },
    {
      "epoch": 0.5375720307847005,
      "grad_norm": 7.810330867767334,
      "learning_rate": 4.955202330767942e-05,
      "loss": 3.1946,
      "step": 13900
    },
    {
      "epoch": 0.5414394554666048,
      "grad_norm": 8.145174026489258,
      "learning_rate": 4.954880045377783e-05,
      "loss": 3.1838,
      "step": 14000
    },
    {
      "epoch": 0.5453068801485091,
      "grad_norm": 7.00197696685791,
      "learning_rate": 4.954557759987625e-05,
      "loss": 3.2359,
      "step": 14100
    },
    {
      "epoch": 0.5491743048304134,
      "grad_norm": 10.3578462600708,
      "learning_rate": 4.9542354745974655e-05,
      "loss": 3.1824,
      "step": 14200
    },
    {
      "epoch": 0.5530417295123178,
      "grad_norm": 11.705028533935547,
      "learning_rate": 4.953913189207307e-05,
      "loss": 3.1781,
      "step": 14300
    },
    {
      "epoch": 0.556909154194222,
      "grad_norm": 9.932175636291504,
      "learning_rate": 4.9535909038171485e-05,
      "loss": 3.21,
      "step": 14400
    },
    {
      "epoch": 0.5607765788761264,
      "grad_norm": 8.649901390075684,
      "learning_rate": 4.95326861842699e-05,
      "loss": 3.1974,
      "step": 14500
    },
    {
      "epoch": 0.5646440035580307,
      "grad_norm": 9.705351829528809,
      "learning_rate": 4.952946333036831e-05,
      "loss": 3.1878,
      "step": 14600
    },
    {
      "epoch": 0.5685114282399351,
      "grad_norm": 8.49486255645752,
      "learning_rate": 4.952624047646672e-05,
      "loss": 3.2258,
      "step": 14700
    },
    {
      "epoch": 0.5723788529218393,
      "grad_norm": 11.243165016174316,
      "learning_rate": 4.952301762256514e-05,
      "loss": 3.1974,
      "step": 14800
    },
    {
      "epoch": 0.5762462776037437,
      "grad_norm": 8.886362075805664,
      "learning_rate": 4.9519794768663545e-05,
      "loss": 3.2202,
      "step": 14900
    },
    {
      "epoch": 0.580113702285648,
      "grad_norm": 11.38934326171875,
      "learning_rate": 4.951657191476196e-05,
      "loss": 3.1796,
      "step": 15000
    },
    {
      "epoch": 0.5839811269675523,
      "grad_norm": 8.605443000793457,
      "learning_rate": 4.9513349060860374e-05,
      "loss": 3.1606,
      "step": 15100
    },
    {
      "epoch": 0.5878485516494566,
      "grad_norm": 12.248119354248047,
      "learning_rate": 4.951012620695879e-05,
      "loss": 3.1723,
      "step": 15200
    },
    {
      "epoch": 0.591715976331361,
      "grad_norm": 11.738093376159668,
      "learning_rate": 4.95069033530572e-05,
      "loss": 3.2102,
      "step": 15300
    },
    {
      "epoch": 0.5955834010132652,
      "grad_norm": 11.382428169250488,
      "learning_rate": 4.950368049915561e-05,
      "loss": 3.2022,
      "step": 15400
    },
    {
      "epoch": 0.5994508256951696,
      "grad_norm": 11.782538414001465,
      "learning_rate": 4.9500457645254026e-05,
      "loss": 3.1273,
      "step": 15500
    },
    {
      "epoch": 0.6033182503770739,
      "grad_norm": 8.020842552185059,
      "learning_rate": 4.949723479135244e-05,
      "loss": 3.1504,
      "step": 15600
    },
    {
      "epoch": 0.6071856750589782,
      "grad_norm": 8.320785522460938,
      "learning_rate": 4.949401193745085e-05,
      "loss": 3.1115,
      "step": 15700
    },
    {
      "epoch": 0.6110530997408825,
      "grad_norm": 10.376381874084473,
      "learning_rate": 4.9490789083549264e-05,
      "loss": 3.1482,
      "step": 15800
    },
    {
      "epoch": 0.6149205244227869,
      "grad_norm": 8.184329986572266,
      "learning_rate": 4.948756622964768e-05,
      "loss": 3.1527,
      "step": 15900
    },
    {
      "epoch": 0.6187879491046911,
      "grad_norm": 6.558215141296387,
      "learning_rate": 4.948434337574609e-05,
      "loss": 3.1883,
      "step": 16000
    },
    {
      "epoch": 0.6226553737865955,
      "grad_norm": 7.348842144012451,
      "learning_rate": 4.94811205218445e-05,
      "loss": 3.1981,
      "step": 16100
    },
    {
      "epoch": 0.6265227984684998,
      "grad_norm": 9.35085678100586,
      "learning_rate": 4.9477897667942916e-05,
      "loss": 3.0683,
      "step": 16200
    },
    {
      "epoch": 0.6303902231504042,
      "grad_norm": 7.531129837036133,
      "learning_rate": 4.947467481404133e-05,
      "loss": 3.1283,
      "step": 16300
    },
    {
      "epoch": 0.6342576478323084,
      "grad_norm": 9.584077835083008,
      "learning_rate": 4.9471451960139745e-05,
      "loss": 3.1304,
      "step": 16400
    },
    {
      "epoch": 0.6381250725142128,
      "grad_norm": 8.81094741821289,
      "learning_rate": 4.946822910623815e-05,
      "loss": 3.1368,
      "step": 16500
    },
    {
      "epoch": 0.6419924971961171,
      "grad_norm": 11.763800621032715,
      "learning_rate": 4.946500625233657e-05,
      "loss": 3.0897,
      "step": 16600
    },
    {
      "epoch": 0.6458599218780214,
      "grad_norm": 9.694778442382812,
      "learning_rate": 4.946178339843498e-05,
      "loss": 3.1501,
      "step": 16700
    },
    {
      "epoch": 0.6497273465599257,
      "grad_norm": 8.444746017456055,
      "learning_rate": 4.94585605445334e-05,
      "loss": 3.2064,
      "step": 16800
    },
    {
      "epoch": 0.6535947712418301,
      "grad_norm": 7.263978958129883,
      "learning_rate": 4.945533769063181e-05,
      "loss": 3.1396,
      "step": 16900
    },
    {
      "epoch": 0.6574621959237343,
      "grad_norm": 7.232344150543213,
      "learning_rate": 4.945211483673023e-05,
      "loss": 3.0931,
      "step": 17000
    },
    {
      "epoch": 0.6613296206056387,
      "grad_norm": 7.855003833770752,
      "learning_rate": 4.9448891982828635e-05,
      "loss": 3.1369,
      "step": 17100
    },
    {
      "epoch": 0.665197045287543,
      "grad_norm": 13.994126319885254,
      "learning_rate": 4.944566912892705e-05,
      "loss": 3.1407,
      "step": 17200
    },
    {
      "epoch": 0.6690644699694474,
      "grad_norm": 9.395435333251953,
      "learning_rate": 4.9442446275025464e-05,
      "loss": 3.1192,
      "step": 17300
    },
    {
      "epoch": 0.6729318946513516,
      "grad_norm": 9.757431983947754,
      "learning_rate": 4.943922342112388e-05,
      "loss": 3.1236,
      "step": 17400
    },
    {
      "epoch": 0.676799319333256,
      "grad_norm": 8.960302352905273,
      "learning_rate": 4.9436000567222294e-05,
      "loss": 3.1243,
      "step": 17500
    },
    {
      "epoch": 0.6806667440151603,
      "grad_norm": 9.0440034866333,
      "learning_rate": 4.94327777133207e-05,
      "loss": 3.1776,
      "step": 17600
    },
    {
      "epoch": 0.6845341686970646,
      "grad_norm": 7.692118167877197,
      "learning_rate": 4.9429554859419116e-05,
      "loss": 3.2006,
      "step": 17700
    },
    {
      "epoch": 0.6884015933789689,
      "grad_norm": 7.621280670166016,
      "learning_rate": 4.942633200551753e-05,
      "loss": 3.1131,
      "step": 17800
    },
    {
      "epoch": 0.6922690180608733,
      "grad_norm": 10.384078025817871,
      "learning_rate": 4.9423109151615946e-05,
      "loss": 3.1269,
      "step": 17900
    },
    {
      "epoch": 0.6961364427427776,
      "grad_norm": 6.298161029815674,
      "learning_rate": 4.9419886297714354e-05,
      "loss": 3.1269,
      "step": 18000
    },
    {
      "epoch": 0.7000038674246819,
      "grad_norm": 7.731649875640869,
      "learning_rate": 4.941666344381277e-05,
      "loss": 3.0992,
      "step": 18100
    },
    {
      "epoch": 0.7038712921065863,
      "grad_norm": 7.029819011688232,
      "learning_rate": 4.941344058991118e-05,
      "loss": 3.1817,
      "step": 18200
    },
    {
      "epoch": 0.7077387167884905,
      "grad_norm": 7.8553242683410645,
      "learning_rate": 4.94102177360096e-05,
      "loss": 3.1518,
      "step": 18300
    },
    {
      "epoch": 0.7116061414703949,
      "grad_norm": 8.5288724899292,
      "learning_rate": 4.9406994882108006e-05,
      "loss": 3.0321,
      "step": 18400
    },
    {
      "epoch": 0.7154735661522992,
      "grad_norm": 9.880796432495117,
      "learning_rate": 4.940377202820642e-05,
      "loss": 3.1294,
      "step": 18500
    },
    {
      "epoch": 0.7193409908342036,
      "grad_norm": 8.846461296081543,
      "learning_rate": 4.9400549174304835e-05,
      "loss": 3.153,
      "step": 18600
    },
    {
      "epoch": 0.7232084155161078,
      "grad_norm": 13.046731948852539,
      "learning_rate": 4.939732632040325e-05,
      "loss": 3.1678,
      "step": 18700
    },
    {
      "epoch": 0.7270758401980122,
      "grad_norm": 11.064437866210938,
      "learning_rate": 4.939410346650166e-05,
      "loss": 3.0581,
      "step": 18800
    },
    {
      "epoch": 0.7309432648799165,
      "grad_norm": 11.691305160522461,
      "learning_rate": 4.939088061260007e-05,
      "loss": 3.0757,
      "step": 18900
    },
    {
      "epoch": 0.7348106895618208,
      "grad_norm": 9.556395530700684,
      "learning_rate": 4.938765775869849e-05,
      "loss": 3.118,
      "step": 19000
    },
    {
      "epoch": 0.7386781142437251,
      "grad_norm": 9.873581886291504,
      "learning_rate": 4.93844349047969e-05,
      "loss": 3.0902,
      "step": 19100
    },
    {
      "epoch": 0.7425455389256295,
      "grad_norm": 12.307035446166992,
      "learning_rate": 4.938121205089531e-05,
      "loss": 3.1408,
      "step": 19200
    },
    {
      "epoch": 0.7464129636075337,
      "grad_norm": 9.143632888793945,
      "learning_rate": 4.9377989196993725e-05,
      "loss": 3.0901,
      "step": 19300
    },
    {
      "epoch": 0.7502803882894381,
      "grad_norm": 9.54712963104248,
      "learning_rate": 4.937476634309214e-05,
      "loss": 3.0904,
      "step": 19400
    },
    {
      "epoch": 0.7541478129713424,
      "grad_norm": 8.904016494750977,
      "learning_rate": 4.9371543489190554e-05,
      "loss": 3.0626,
      "step": 19500
    },
    {
      "epoch": 0.7580152376532467,
      "grad_norm": 10.77379035949707,
      "learning_rate": 4.936832063528896e-05,
      "loss": 3.0682,
      "step": 19600
    },
    {
      "epoch": 0.761882662335151,
      "grad_norm": 9.257044792175293,
      "learning_rate": 4.936509778138738e-05,
      "loss": 3.0829,
      "step": 19700
    },
    {
      "epoch": 0.7657500870170554,
      "grad_norm": 12.217463493347168,
      "learning_rate": 4.936187492748579e-05,
      "loss": 3.0497,
      "step": 19800
    },
    {
      "epoch": 0.7696175116989596,
      "grad_norm": 9.115326881408691,
      "learning_rate": 4.9358652073584206e-05,
      "loss": 3.0757,
      "step": 19900
    },
    {
      "epoch": 0.773484936380864,
      "grad_norm": 10.53512191772461,
      "learning_rate": 4.9355429219682614e-05,
      "loss": 3.0706,
      "step": 20000
    },
    {
      "epoch": 0.7773523610627683,
      "grad_norm": 8.460957527160645,
      "learning_rate": 4.935220636578103e-05,
      "loss": 3.0552,
      "step": 20100
    },
    {
      "epoch": 0.7812197857446727,
      "grad_norm": 8.550858497619629,
      "learning_rate": 4.9348983511879443e-05,
      "loss": 3.0865,
      "step": 20200
    },
    {
      "epoch": 0.7850872104265769,
      "grad_norm": 9.465072631835938,
      "learning_rate": 4.934576065797786e-05,
      "loss": 3.0572,
      "step": 20300
    },
    {
      "epoch": 0.7889546351084813,
      "grad_norm": 12.80776309967041,
      "learning_rate": 4.9342537804076266e-05,
      "loss": 3.1051,
      "step": 20400
    },
    {
      "epoch": 0.7928220597903856,
      "grad_norm": 13.361125946044922,
      "learning_rate": 4.933931495017468e-05,
      "loss": 3.0731,
      "step": 20500
    },
    {
      "epoch": 0.7966894844722899,
      "grad_norm": 9.527655601501465,
      "learning_rate": 4.9336092096273096e-05,
      "loss": 3.1165,
      "step": 20600
    },
    {
      "epoch": 0.8005569091541942,
      "grad_norm": 10.810237884521484,
      "learning_rate": 4.9332869242371504e-05,
      "loss": 3.0713,
      "step": 20700
    },
    {
      "epoch": 0.8044243338360986,
      "grad_norm": 12.511951446533203,
      "learning_rate": 4.932964638846992e-05,
      "loss": 3.115,
      "step": 20800
    },
    {
      "epoch": 0.8082917585180028,
      "grad_norm": 10.07791519165039,
      "learning_rate": 4.932642353456833e-05,
      "loss": 3.0334,
      "step": 20900
    },
    {
      "epoch": 0.8121591831999072,
      "grad_norm": 10.866104125976562,
      "learning_rate": 4.932320068066675e-05,
      "loss": 3.0586,
      "step": 21000
    },
    {
      "epoch": 0.8160266078818115,
      "grad_norm": 7.930206298828125,
      "learning_rate": 4.9319977826765156e-05,
      "loss": 3.0439,
      "step": 21100
    },
    {
      "epoch": 0.8198940325637158,
      "grad_norm": 10.718666076660156,
      "learning_rate": 4.931675497286357e-05,
      "loss": 3.06,
      "step": 21200
    },
    {
      "epoch": 0.8237614572456201,
      "grad_norm": 7.619448661804199,
      "learning_rate": 4.9313532118961985e-05,
      "loss": 3.1347,
      "step": 21300
    },
    {
      "epoch": 0.8276288819275245,
      "grad_norm": 8.639686584472656,
      "learning_rate": 4.93103092650604e-05,
      "loss": 3.102,
      "step": 21400
    },
    {
      "epoch": 0.8314963066094287,
      "grad_norm": 8.965747833251953,
      "learning_rate": 4.930708641115881e-05,
      "loss": 3.0327,
      "step": 21500
    },
    {
      "epoch": 0.8353637312913331,
      "grad_norm": 9.923988342285156,
      "learning_rate": 4.930386355725722e-05,
      "loss": 3.1185,
      "step": 21600
    },
    {
      "epoch": 0.8392311559732374,
      "grad_norm": 9.867907524108887,
      "learning_rate": 4.930064070335564e-05,
      "loss": 3.0505,
      "step": 21700
    },
    {
      "epoch": 0.8430985806551418,
      "grad_norm": 10.203326225280762,
      "learning_rate": 4.929741784945405e-05,
      "loss": 3.0997,
      "step": 21800
    },
    {
      "epoch": 0.846966005337046,
      "grad_norm": 8.573594093322754,
      "learning_rate": 4.929419499555246e-05,
      "loss": 3.0169,
      "step": 21900
    },
    {
      "epoch": 0.8508334300189504,
      "grad_norm": 9.09321403503418,
      "learning_rate": 4.9290972141650875e-05,
      "loss": 3.0658,
      "step": 22000
    },
    {
      "epoch": 0.8547008547008547,
      "grad_norm": 11.276224136352539,
      "learning_rate": 4.928774928774929e-05,
      "loss": 3.0439,
      "step": 22100
    },
    {
      "epoch": 0.858568279382759,
      "grad_norm": 10.964778900146484,
      "learning_rate": 4.9284526433847704e-05,
      "loss": 3.0312,
      "step": 22200
    },
    {
      "epoch": 0.8624357040646633,
      "grad_norm": 10.652155876159668,
      "learning_rate": 4.928130357994611e-05,
      "loss": 3.0465,
      "step": 22300
    },
    {
      "epoch": 0.8663031287465677,
      "grad_norm": 12.545928001403809,
      "learning_rate": 4.927808072604453e-05,
      "loss": 3.06,
      "step": 22400
    },
    {
      "epoch": 0.8701705534284719,
      "grad_norm": 9.032703399658203,
      "learning_rate": 4.927485787214294e-05,
      "loss": 3.0487,
      "step": 22500
    },
    {
      "epoch": 0.8740379781103763,
      "grad_norm": 9.755168914794922,
      "learning_rate": 4.9271635018241356e-05,
      "loss": 3.0347,
      "step": 22600
    },
    {
      "epoch": 0.8779054027922806,
      "grad_norm": 11.832676887512207,
      "learning_rate": 4.9268412164339764e-05,
      "loss": 2.977,
      "step": 22700
    },
    {
      "epoch": 0.881772827474185,
      "grad_norm": 9.718785285949707,
      "learning_rate": 4.926518931043818e-05,
      "loss": 3.0254,
      "step": 22800
    },
    {
      "epoch": 0.8856402521560892,
      "grad_norm": 7.588074207305908,
      "learning_rate": 4.9261966456536593e-05,
      "loss": 3.0537,
      "step": 22900
    },
    {
      "epoch": 0.8895076768379936,
      "grad_norm": 9.329320907592773,
      "learning_rate": 4.925874360263501e-05,
      "loss": 2.9936,
      "step": 23000
    },
    {
      "epoch": 0.8933751015198979,
      "grad_norm": 9.94555377960205,
      "learning_rate": 4.9255520748733416e-05,
      "loss": 3.0505,
      "step": 23100
    },
    {
      "epoch": 0.8972425262018022,
      "grad_norm": 11.352953910827637,
      "learning_rate": 4.925229789483183e-05,
      "loss": 3.0094,
      "step": 23200
    },
    {
      "epoch": 0.9011099508837065,
      "grad_norm": 10.240994453430176,
      "learning_rate": 4.9249075040930246e-05,
      "loss": 2.9841,
      "step": 23300
    },
    {
      "epoch": 0.9049773755656109,
      "grad_norm": 9.45578384399414,
      "learning_rate": 4.924585218702866e-05,
      "loss": 2.988,
      "step": 23400
    },
    {
      "epoch": 0.9088448002475151,
      "grad_norm": 10.612102508544922,
      "learning_rate": 4.9242629333127075e-05,
      "loss": 3.0878,
      "step": 23500
    },
    {
      "epoch": 0.9127122249294195,
      "grad_norm": 9.774772644042969,
      "learning_rate": 4.923940647922548e-05,
      "loss": 3.0307,
      "step": 23600
    },
    {
      "epoch": 0.9165796496113238,
      "grad_norm": 11.416166305541992,
      "learning_rate": 4.92361836253239e-05,
      "loss": 3.0072,
      "step": 23700
    },
    {
      "epoch": 0.9204470742932281,
      "grad_norm": 9.749472618103027,
      "learning_rate": 4.923296077142231e-05,
      "loss": 2.9859,
      "step": 23800
    },
    {
      "epoch": 0.9243144989751325,
      "grad_norm": 10.236063957214355,
      "learning_rate": 4.922973791752073e-05,
      "loss": 2.9855,
      "step": 23900
    },
    {
      "epoch": 0.9281819236570368,
      "grad_norm": 10.56482982635498,
      "learning_rate": 4.922651506361914e-05,
      "loss": 3.0134,
      "step": 24000
    },
    {
      "epoch": 0.9320493483389412,
      "grad_norm": 14.204108238220215,
      "learning_rate": 4.922329220971755e-05,
      "loss": 2.9435,
      "step": 24100
    },
    {
      "epoch": 0.9359167730208454,
      "grad_norm": 8.570218086242676,
      "learning_rate": 4.9220069355815964e-05,
      "loss": 3.1059,
      "step": 24200
    },
    {
      "epoch": 0.9397841977027498,
      "grad_norm": 15.756704330444336,
      "learning_rate": 4.921684650191438e-05,
      "loss": 3.0195,
      "step": 24300
    },
    {
      "epoch": 0.943651622384654,
      "grad_norm": 10.16104793548584,
      "learning_rate": 4.9213623648012794e-05,
      "loss": 3.0332,
      "step": 24400
    },
    {
      "epoch": 0.9475190470665584,
      "grad_norm": 8.664031028747559,
      "learning_rate": 4.921040079411121e-05,
      "loss": 3.0418,
      "step": 24500
    },
    {
      "epoch": 0.9513864717484627,
      "grad_norm": 9.63821029663086,
      "learning_rate": 4.9207177940209617e-05,
      "loss": 3.0121,
      "step": 24600
    },
    {
      "epoch": 0.9552538964303671,
      "grad_norm": 8.92183780670166,
      "learning_rate": 4.920395508630803e-05,
      "loss": 2.9896,
      "step": 24700
    },
    {
      "epoch": 0.9591213211122713,
      "grad_norm": 11.530024528503418,
      "learning_rate": 4.9200732232406446e-05,
      "loss": 3.0373,
      "step": 24800
    },
    {
      "epoch": 0.9629887457941757,
      "grad_norm": 10.783599853515625,
      "learning_rate": 4.919750937850486e-05,
      "loss": 3.0643,
      "step": 24900
    },
    {
      "epoch": 0.96685617047608,
      "grad_norm": 9.599602699279785,
      "learning_rate": 4.919428652460327e-05,
      "loss": 3.0562,
      "step": 25000
    },
    {
      "epoch": 0.9707235951579843,
      "grad_norm": 10.881058692932129,
      "learning_rate": 4.919106367070168e-05,
      "loss": 3.0305,
      "step": 25100
    },
    {
      "epoch": 0.9745910198398886,
      "grad_norm": 9.58672046661377,
      "learning_rate": 4.91878408168001e-05,
      "loss": 2.9935,
      "step": 25200
    },
    {
      "epoch": 0.978458444521793,
      "grad_norm": 12.201854705810547,
      "learning_rate": 4.918461796289851e-05,
      "loss": 2.9588,
      "step": 25300
    },
    {
      "epoch": 0.9823258692036972,
      "grad_norm": 10.248329162597656,
      "learning_rate": 4.918139510899692e-05,
      "loss": 3.0175,
      "step": 25400
    },
    {
      "epoch": 0.9861932938856016,
      "grad_norm": 9.7299165725708,
      "learning_rate": 4.9178172255095335e-05,
      "loss": 3.0331,
      "step": 25500
    },
    {
      "epoch": 0.9900607185675059,
      "grad_norm": 11.31818962097168,
      "learning_rate": 4.917494940119375e-05,
      "loss": 2.9646,
      "step": 25600
    },
    {
      "epoch": 0.9939281432494103,
      "grad_norm": 10.2388334274292,
      "learning_rate": 4.9171726547292165e-05,
      "loss": 2.9876,
      "step": 25700
    },
    {
      "epoch": 0.9977955679313145,
      "grad_norm": 11.25092601776123,
      "learning_rate": 4.916850369339057e-05,
      "loss": 2.9772,
      "step": 25800
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.929396867752075,
      "eval_runtime": 3.3608,
      "eval_samples_per_second": 404.969,
      "eval_steps_per_second": 404.969,
      "step": 25857
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.945615768432617,
      "eval_runtime": 85.6384,
      "eval_samples_per_second": 301.932,
      "eval_steps_per_second": 301.932,
      "step": 25857
    },
    {
      "epoch": 1.0016629926132188,
      "grad_norm": 9.68046760559082,
      "learning_rate": 4.916528083948899e-05,
      "loss": 3.013,
      "step": 25900
    },
    {
      "epoch": 1.0055304172951232,
      "grad_norm": 10.495505332946777,
      "learning_rate": 4.91620579855874e-05,
      "loss": 3.0051,
      "step": 26000
    },
    {
      "epoch": 1.0093978419770275,
      "grad_norm": 11.2944917678833,
      "learning_rate": 4.915883513168582e-05,
      "loss": 3.046,
      "step": 26100
    },
    {
      "epoch": 1.013265266658932,
      "grad_norm": 16.086706161499023,
      "learning_rate": 4.9155612277784225e-05,
      "loss": 3.0284,
      "step": 26200
    },
    {
      "epoch": 1.017132691340836,
      "grad_norm": 8.994538307189941,
      "learning_rate": 4.915238942388264e-05,
      "loss": 2.9942,
      "step": 26300
    },
    {
      "epoch": 1.0210001160227404,
      "grad_norm": 11.203361511230469,
      "learning_rate": 4.9149166569981054e-05,
      "loss": 2.9783,
      "step": 26400
    },
    {
      "epoch": 1.0248675407046448,
      "grad_norm": 9.605382919311523,
      "learning_rate": 4.914594371607947e-05,
      "loss": 2.9699,
      "step": 26500
    },
    {
      "epoch": 1.0287349653865492,
      "grad_norm": 10.655733108520508,
      "learning_rate": 4.914272086217788e-05,
      "loss": 2.9464,
      "step": 26600
    },
    {
      "epoch": 1.0326023900684533,
      "grad_norm": 9.869318962097168,
      "learning_rate": 4.913949800827629e-05,
      "loss": 2.9652,
      "step": 26700
    },
    {
      "epoch": 1.0364698147503577,
      "grad_norm": 8.236629486083984,
      "learning_rate": 4.9136275154374706e-05,
      "loss": 2.9321,
      "step": 26800
    },
    {
      "epoch": 1.040337239432262,
      "grad_norm": 9.798447608947754,
      "learning_rate": 4.9133052300473114e-05,
      "loss": 2.8627,
      "step": 26900
    },
    {
      "epoch": 1.0442046641141665,
      "grad_norm": 11.397940635681152,
      "learning_rate": 4.912982944657153e-05,
      "loss": 3.0125,
      "step": 27000
    },
    {
      "epoch": 1.0480720887960706,
      "grad_norm": 13.056947708129883,
      "learning_rate": 4.9126606592669944e-05,
      "loss": 2.9399,
      "step": 27100
    },
    {
      "epoch": 1.051939513477975,
      "grad_norm": 9.963944435119629,
      "learning_rate": 4.912338373876836e-05,
      "loss": 2.9049,
      "step": 27200
    },
    {
      "epoch": 1.0558069381598794,
      "grad_norm": 10.029145240783691,
      "learning_rate": 4.9120160884866767e-05,
      "loss": 2.9816,
      "step": 27300
    },
    {
      "epoch": 1.0596743628417837,
      "grad_norm": 12.099961280822754,
      "learning_rate": 4.911693803096518e-05,
      "loss": 2.9302,
      "step": 27400
    },
    {
      "epoch": 1.0635417875236879,
      "grad_norm": 10.104100227355957,
      "learning_rate": 4.9113715177063596e-05,
      "loss": 2.8921,
      "step": 27500
    },
    {
      "epoch": 1.0674092122055923,
      "grad_norm": 10.381487846374512,
      "learning_rate": 4.911049232316201e-05,
      "loss": 3.0039,
      "step": 27600
    },
    {
      "epoch": 1.0712766368874966,
      "grad_norm": 10.010738372802734,
      "learning_rate": 4.910726946926042e-05,
      "loss": 2.8858,
      "step": 27700
    },
    {
      "epoch": 1.075144061569401,
      "grad_norm": 12.207826614379883,
      "learning_rate": 4.910404661535883e-05,
      "loss": 2.9588,
      "step": 27800
    },
    {
      "epoch": 1.0790114862513052,
      "grad_norm": 11.588973045349121,
      "learning_rate": 4.910082376145725e-05,
      "loss": 2.9188,
      "step": 27900
    },
    {
      "epoch": 1.0828789109332095,
      "grad_norm": 16.207408905029297,
      "learning_rate": 4.909760090755566e-05,
      "loss": 2.9753,
      "step": 28000
    },
    {
      "epoch": 1.086746335615114,
      "grad_norm": 9.448533058166504,
      "learning_rate": 4.909437805365407e-05,
      "loss": 2.9317,
      "step": 28100
    },
    {
      "epoch": 1.0906137602970183,
      "grad_norm": 12.87701416015625,
      "learning_rate": 4.9091155199752485e-05,
      "loss": 2.9889,
      "step": 28200
    },
    {
      "epoch": 1.0944811849789224,
      "grad_norm": 11.336146354675293,
      "learning_rate": 4.90879323458509e-05,
      "loss": 2.9233,
      "step": 28300
    },
    {
      "epoch": 1.0983486096608268,
      "grad_norm": 10.187177658081055,
      "learning_rate": 4.9084709491949315e-05,
      "loss": 2.9538,
      "step": 28400
    },
    {
      "epoch": 1.1022160343427312,
      "grad_norm": 12.677289962768555,
      "learning_rate": 4.908148663804772e-05,
      "loss": 2.9128,
      "step": 28500
    },
    {
      "epoch": 1.1060834590246356,
      "grad_norm": 10.400110244750977,
      "learning_rate": 4.907826378414614e-05,
      "loss": 2.8979,
      "step": 28600
    },
    {
      "epoch": 1.1099508837065397,
      "grad_norm": 11.225016593933105,
      "learning_rate": 4.907504093024455e-05,
      "loss": 3.0013,
      "step": 28700
    },
    {
      "epoch": 1.113818308388444,
      "grad_norm": 9.968836784362793,
      "learning_rate": 4.907181807634297e-05,
      "loss": 2.977,
      "step": 28800
    },
    {
      "epoch": 1.1176857330703485,
      "grad_norm": 9.763652801513672,
      "learning_rate": 4.9068595222441375e-05,
      "loss": 2.9756,
      "step": 28900
    },
    {
      "epoch": 1.1215531577522528,
      "grad_norm": 10.433131217956543,
      "learning_rate": 4.906537236853979e-05,
      "loss": 2.9194,
      "step": 29000
    },
    {
      "epoch": 1.125420582434157,
      "grad_norm": 8.489563941955566,
      "learning_rate": 4.9062149514638204e-05,
      "loss": 3.0017,
      "step": 29100
    },
    {
      "epoch": 1.1292880071160614,
      "grad_norm": 9.809876441955566,
      "learning_rate": 4.905892666073662e-05,
      "loss": 2.9972,
      "step": 29200
    },
    {
      "epoch": 1.1331554317979657,
      "grad_norm": 15.66970157623291,
      "learning_rate": 4.905570380683503e-05,
      "loss": 3.0429,
      "step": 29300
    },
    {
      "epoch": 1.1370228564798701,
      "grad_norm": 11.213406562805176,
      "learning_rate": 4.905248095293344e-05,
      "loss": 2.9762,
      "step": 29400
    },
    {
      "epoch": 1.1408902811617745,
      "grad_norm": 8.526177406311035,
      "learning_rate": 4.9049258099031856e-05,
      "loss": 2.8773,
      "step": 29500
    },
    {
      "epoch": 1.1447577058436786,
      "grad_norm": 10.654688835144043,
      "learning_rate": 4.9046035245130264e-05,
      "loss": 2.8903,
      "step": 29600
    },
    {
      "epoch": 1.148625130525583,
      "grad_norm": 11.085805892944336,
      "learning_rate": 4.904281239122868e-05,
      "loss": 2.959,
      "step": 29700
    },
    {
      "epoch": 1.1524925552074874,
      "grad_norm": 12.408997535705566,
      "learning_rate": 4.9039589537327094e-05,
      "loss": 2.9435,
      "step": 29800
    },
    {
      "epoch": 1.1563599798893915,
      "grad_norm": 11.450925827026367,
      "learning_rate": 4.903636668342551e-05,
      "loss": 2.9121,
      "step": 29900
    },
    {
      "epoch": 1.160227404571296,
      "grad_norm": 12.747930526733398,
      "learning_rate": 4.9033143829523916e-05,
      "loss": 2.9243,
      "step": 30000
    },
    {
      "epoch": 1.1640948292532003,
      "grad_norm": 15.25787353515625,
      "learning_rate": 4.902992097562233e-05,
      "loss": 2.924,
      "step": 30100
    },
    {
      "epoch": 1.1679622539351047,
      "grad_norm": 9.237811088562012,
      "learning_rate": 4.9026698121720746e-05,
      "loss": 2.9108,
      "step": 30200
    },
    {
      "epoch": 1.171829678617009,
      "grad_norm": 14.154156684875488,
      "learning_rate": 4.902347526781916e-05,
      "loss": 2.9053,
      "step": 30300
    },
    {
      "epoch": 1.1756971032989132,
      "grad_norm": 11.327510833740234,
      "learning_rate": 4.9020252413917575e-05,
      "loss": 2.9195,
      "step": 30400
    },
    {
      "epoch": 1.1795645279808176,
      "grad_norm": 9.56159782409668,
      "learning_rate": 4.901702956001599e-05,
      "loss": 2.9185,
      "step": 30500
    },
    {
      "epoch": 1.183431952662722,
      "grad_norm": 9.7981538772583,
      "learning_rate": 4.90138067061144e-05,
      "loss": 2.925,
      "step": 30600
    },
    {
      "epoch": 1.187299377344626,
      "grad_norm": 12.059347152709961,
      "learning_rate": 4.901058385221281e-05,
      "loss": 2.9135,
      "step": 30700
    },
    {
      "epoch": 1.1911668020265305,
      "grad_norm": 11.048664093017578,
      "learning_rate": 4.900736099831123e-05,
      "loss": 2.9195,
      "step": 30800
    },
    {
      "epoch": 1.1950342267084348,
      "grad_norm": 9.402969360351562,
      "learning_rate": 4.900413814440964e-05,
      "loss": 2.999,
      "step": 30900
    },
    {
      "epoch": 1.1989016513903392,
      "grad_norm": 12.254706382751465,
      "learning_rate": 4.900091529050806e-05,
      "loss": 2.9174,
      "step": 31000
    },
    {
      "epoch": 1.2027690760722436,
      "grad_norm": 9.272831916809082,
      "learning_rate": 4.899769243660647e-05,
      "loss": 2.8811,
      "step": 31100
    },
    {
      "epoch": 1.2066365007541477,
      "grad_norm": 13.067997932434082,
      "learning_rate": 4.899446958270488e-05,
      "loss": 2.924,
      "step": 31200
    },
    {
      "epoch": 1.2105039254360521,
      "grad_norm": 16.073278427124023,
      "learning_rate": 4.8991246728803294e-05,
      "loss": 2.9789,
      "step": 31300
    },
    {
      "epoch": 1.2143713501179565,
      "grad_norm": 10.95952033996582,
      "learning_rate": 4.898802387490171e-05,
      "loss": 2.8899,
      "step": 31400
    },
    {
      "epoch": 1.2182387747998609,
      "grad_norm": 13.48935604095459,
      "learning_rate": 4.8984801021000124e-05,
      "loss": 2.8938,
      "step": 31500
    },
    {
      "epoch": 1.222106199481765,
      "grad_norm": 10.2596435546875,
      "learning_rate": 4.898157816709853e-05,
      "loss": 2.8521,
      "step": 31600
    },
    {
      "epoch": 1.2259736241636694,
      "grad_norm": 13.971383094787598,
      "learning_rate": 4.8978355313196946e-05,
      "loss": 2.9238,
      "step": 31700
    },
    {
      "epoch": 1.2298410488455738,
      "grad_norm": 11.763291358947754,
      "learning_rate": 4.897513245929536e-05,
      "loss": 2.9618,
      "step": 31800
    },
    {
      "epoch": 1.2337084735274781,
      "grad_norm": 9.795754432678223,
      "learning_rate": 4.8971909605393776e-05,
      "loss": 2.9298,
      "step": 31900
    },
    {
      "epoch": 1.2375758982093823,
      "grad_norm": 11.691205978393555,
      "learning_rate": 4.8968686751492184e-05,
      "loss": 2.9644,
      "step": 32000
    },
    {
      "epoch": 1.2414433228912867,
      "grad_norm": 9.05426025390625,
      "learning_rate": 4.89654638975906e-05,
      "loss": 2.9485,
      "step": 32100
    },
    {
      "epoch": 1.245310747573191,
      "grad_norm": 13.45604419708252,
      "learning_rate": 4.896224104368901e-05,
      "loss": 2.9121,
      "step": 32200
    },
    {
      "epoch": 1.2491781722550954,
      "grad_norm": 9.236865997314453,
      "learning_rate": 4.895901818978743e-05,
      "loss": 2.9452,
      "step": 32300
    },
    {
      "epoch": 1.2530455969369996,
      "grad_norm": 9.139429092407227,
      "learning_rate": 4.8955795335885836e-05,
      "loss": 2.8266,
      "step": 32400
    },
    {
      "epoch": 1.256913021618904,
      "grad_norm": 14.845593452453613,
      "learning_rate": 4.895257248198425e-05,
      "loss": 2.9183,
      "step": 32500
    },
    {
      "epoch": 1.2607804463008083,
      "grad_norm": 9.92809772491455,
      "learning_rate": 4.8949349628082665e-05,
      "loss": 2.8634,
      "step": 32600
    },
    {
      "epoch": 1.2646478709827127,
      "grad_norm": 15.897668838500977,
      "learning_rate": 4.894612677418107e-05,
      "loss": 2.8427,
      "step": 32700
    },
    {
      "epoch": 1.2685152956646168,
      "grad_norm": 10.215909004211426,
      "learning_rate": 4.894290392027949e-05,
      "loss": 2.9844,
      "step": 32800
    },
    {
      "epoch": 1.2723827203465212,
      "grad_norm": 9.139297485351562,
      "learning_rate": 4.89396810663779e-05,
      "loss": 2.8795,
      "step": 32900
    },
    {
      "epoch": 1.2762501450284256,
      "grad_norm": 12.263065338134766,
      "learning_rate": 4.893645821247632e-05,
      "loss": 2.9195,
      "step": 33000
    },
    {
      "epoch": 1.28011756971033,
      "grad_norm": 11.207759857177734,
      "learning_rate": 4.8933235358574725e-05,
      "loss": 2.8842,
      "step": 33100
    },
    {
      "epoch": 1.2839849943922341,
      "grad_norm": 13.613802909851074,
      "learning_rate": 4.893001250467314e-05,
      "loss": 2.9328,
      "step": 33200
    },
    {
      "epoch": 1.2878524190741385,
      "grad_norm": 11.039318084716797,
      "learning_rate": 4.8926789650771555e-05,
      "loss": 2.8888,
      "step": 33300
    },
    {
      "epoch": 1.2917198437560429,
      "grad_norm": 15.20668888092041,
      "learning_rate": 4.892356679686997e-05,
      "loss": 2.9015,
      "step": 33400
    },
    {
      "epoch": 1.2955872684379472,
      "grad_norm": 9.6165771484375,
      "learning_rate": 4.892034394296838e-05,
      "loss": 2.8468,
      "step": 33500
    },
    {
      "epoch": 1.2994546931198516,
      "grad_norm": 9.95697021484375,
      "learning_rate": 4.891712108906679e-05,
      "loss": 2.8952,
      "step": 33600
    },
    {
      "epoch": 1.3033221178017558,
      "grad_norm": 9.780007362365723,
      "learning_rate": 4.891389823516521e-05,
      "loss": 2.8917,
      "step": 33700
    },
    {
      "epoch": 1.3071895424836601,
      "grad_norm": 14.048711776733398,
      "learning_rate": 4.891067538126362e-05,
      "loss": 2.9279,
      "step": 33800
    },
    {
      "epoch": 1.3110569671655645,
      "grad_norm": 10.80257511138916,
      "learning_rate": 4.890745252736203e-05,
      "loss": 2.9318,
      "step": 33900
    },
    {
      "epoch": 1.3149243918474687,
      "grad_norm": 10.559666633605957,
      "learning_rate": 4.8904229673460444e-05,
      "loss": 2.9084,
      "step": 34000
    },
    {
      "epoch": 1.318791816529373,
      "grad_norm": 13.673538208007812,
      "learning_rate": 4.890100681955886e-05,
      "loss": 2.944,
      "step": 34100
    },
    {
      "epoch": 1.3226592412112774,
      "grad_norm": 10.253246307373047,
      "learning_rate": 4.8897783965657274e-05,
      "loss": 2.8415,
      "step": 34200
    },
    {
      "epoch": 1.3265266658931818,
      "grad_norm": 10.819576263427734,
      "learning_rate": 4.889456111175568e-05,
      "loss": 2.9188,
      "step": 34300
    },
    {
      "epoch": 1.3303940905750862,
      "grad_norm": 8.137179374694824,
      "learning_rate": 4.8891338257854096e-05,
      "loss": 2.916,
      "step": 34400
    },
    {
      "epoch": 1.3342615152569903,
      "grad_norm": 10.606954574584961,
      "learning_rate": 4.888811540395251e-05,
      "loss": 3.011,
      "step": 34500
    },
    {
      "epoch": 1.3381289399388947,
      "grad_norm": 7.85199499130249,
      "learning_rate": 4.8884892550050926e-05,
      "loss": 2.891,
      "step": 34600
    },
    {
      "epoch": 1.341996364620799,
      "grad_norm": 10.793598175048828,
      "learning_rate": 4.8881669696149334e-05,
      "loss": 2.8718,
      "step": 34700
    },
    {
      "epoch": 1.3458637893027032,
      "grad_norm": 10.436274528503418,
      "learning_rate": 4.887844684224775e-05,
      "loss": 2.9831,
      "step": 34800
    },
    {
      "epoch": 1.3497312139846076,
      "grad_norm": 12.019898414611816,
      "learning_rate": 4.887522398834616e-05,
      "loss": 2.934,
      "step": 34900
    },
    {
      "epoch": 1.353598638666512,
      "grad_norm": 9.603578567504883,
      "learning_rate": 4.887200113444458e-05,
      "loss": 2.8173,
      "step": 35000
    },
    {
      "epoch": 1.3574660633484164,
      "grad_norm": 11.255072593688965,
      "learning_rate": 4.8868778280542986e-05,
      "loss": 2.8393,
      "step": 35100
    },
    {
      "epoch": 1.3613334880303207,
      "grad_norm": 12.065092086791992,
      "learning_rate": 4.88655554266414e-05,
      "loss": 2.877,
      "step": 35200
    },
    {
      "epoch": 1.3652009127122249,
      "grad_norm": 11.876753807067871,
      "learning_rate": 4.8862332572739815e-05,
      "loss": 2.8971,
      "step": 35300
    },
    {
      "epoch": 1.3690683373941293,
      "grad_norm": 10.22796630859375,
      "learning_rate": 4.885910971883823e-05,
      "loss": 2.8901,
      "step": 35400
    },
    {
      "epoch": 1.3729357620760336,
      "grad_norm": 11.085182189941406,
      "learning_rate": 4.885588686493664e-05,
      "loss": 2.9318,
      "step": 35500
    },
    {
      "epoch": 1.3768031867579378,
      "grad_norm": 12.190752029418945,
      "learning_rate": 4.885266401103505e-05,
      "loss": 2.8433,
      "step": 35600
    },
    {
      "epoch": 1.3806706114398422,
      "grad_norm": 11.110892295837402,
      "learning_rate": 4.884944115713347e-05,
      "loss": 2.8768,
      "step": 35700
    },
    {
      "epoch": 1.3845380361217465,
      "grad_norm": 11.036306381225586,
      "learning_rate": 4.8846218303231875e-05,
      "loss": 2.9113,
      "step": 35800
    },
    {
      "epoch": 1.388405460803651,
      "grad_norm": 11.443995475769043,
      "learning_rate": 4.884299544933029e-05,
      "loss": 2.7756,
      "step": 35900
    },
    {
      "epoch": 1.3922728854855553,
      "grad_norm": 12.680521011352539,
      "learning_rate": 4.8839772595428705e-05,
      "loss": 2.8603,
      "step": 36000
    },
    {
      "epoch": 1.3961403101674594,
      "grad_norm": 12.415080070495605,
      "learning_rate": 4.883654974152712e-05,
      "loss": 2.8773,
      "step": 36100
    },
    {
      "epoch": 1.4000077348493638,
      "grad_norm": 12.234031677246094,
      "learning_rate": 4.883332688762553e-05,
      "loss": 2.8666,
      "step": 36200
    },
    {
      "epoch": 1.4038751595312682,
      "grad_norm": 14.336424827575684,
      "learning_rate": 4.883010403372394e-05,
      "loss": 2.9252,
      "step": 36300
    },
    {
      "epoch": 1.4077425842131723,
      "grad_norm": 12.52529525756836,
      "learning_rate": 4.882688117982236e-05,
      "loss": 2.778,
      "step": 36400
    },
    {
      "epoch": 1.4116100088950767,
      "grad_norm": 11.189495086669922,
      "learning_rate": 4.882365832592077e-05,
      "loss": 2.8362,
      "step": 36500
    },
    {
      "epoch": 1.415477433576981,
      "grad_norm": 11.279738426208496,
      "learning_rate": 4.882043547201918e-05,
      "loss": 2.8826,
      "step": 36600
    },
    {
      "epoch": 1.4193448582588855,
      "grad_norm": 11.601266860961914,
      "learning_rate": 4.8817212618117594e-05,
      "loss": 2.868,
      "step": 36700
    },
    {
      "epoch": 1.4232122829407898,
      "grad_norm": 9.803698539733887,
      "learning_rate": 4.881398976421601e-05,
      "loss": 2.8546,
      "step": 36800
    },
    {
      "epoch": 1.427079707622694,
      "grad_norm": 8.869645118713379,
      "learning_rate": 4.8810766910314424e-05,
      "loss": 2.8531,
      "step": 36900
    },
    {
      "epoch": 1.4309471323045984,
      "grad_norm": 9.921470642089844,
      "learning_rate": 4.880754405641284e-05,
      "loss": 2.8116,
      "step": 37000
    },
    {
      "epoch": 1.4348145569865027,
      "grad_norm": 12.820967674255371,
      "learning_rate": 4.8804321202511246e-05,
      "loss": 2.8715,
      "step": 37100
    },
    {
      "epoch": 1.4386819816684069,
      "grad_norm": 10.125819206237793,
      "learning_rate": 4.880109834860966e-05,
      "loss": 2.8693,
      "step": 37200
    },
    {
      "epoch": 1.4425494063503113,
      "grad_norm": 9.234274864196777,
      "learning_rate": 4.8797875494708076e-05,
      "loss": 2.751,
      "step": 37300
    },
    {
      "epoch": 1.4464168310322156,
      "grad_norm": 10.84761905670166,
      "learning_rate": 4.879465264080649e-05,
      "loss": 2.8757,
      "step": 37400
    },
    {
      "epoch": 1.45028425571412,
      "grad_norm": 10.399983406066895,
      "learning_rate": 4.8791429786904905e-05,
      "loss": 2.8515,
      "step": 37500
    },
    {
      "epoch": 1.4541516803960244,
      "grad_norm": 8.715995788574219,
      "learning_rate": 4.878820693300331e-05,
      "loss": 2.8406,
      "step": 37600
    },
    {
      "epoch": 1.4580191050779285,
      "grad_norm": 11.069428443908691,
      "learning_rate": 4.878498407910173e-05,
      "loss": 2.827,
      "step": 37700
    },
    {
      "epoch": 1.461886529759833,
      "grad_norm": 11.116776466369629,
      "learning_rate": 4.878176122520014e-05,
      "loss": 2.8368,
      "step": 37800
    },
    {
      "epoch": 1.4657539544417373,
      "grad_norm": 9.112192153930664,
      "learning_rate": 4.877853837129856e-05,
      "loss": 2.8687,
      "step": 37900
    },
    {
      "epoch": 1.4696213791236414,
      "grad_norm": 10.44295883178711,
      "learning_rate": 4.877531551739697e-05,
      "loss": 2.8018,
      "step": 38000
    },
    {
      "epoch": 1.4734888038055458,
      "grad_norm": 9.86604118347168,
      "learning_rate": 4.877209266349539e-05,
      "loss": 2.8782,
      "step": 38100
    },
    {
      "epoch": 1.4773562284874502,
      "grad_norm": 14.672070503234863,
      "learning_rate": 4.8768869809593795e-05,
      "loss": 2.8512,
      "step": 38200
    },
    {
      "epoch": 1.4812236531693546,
      "grad_norm": 13.35692024230957,
      "learning_rate": 4.876564695569221e-05,
      "loss": 2.8651,
      "step": 38300
    },
    {
      "epoch": 1.485091077851259,
      "grad_norm": 10.667649269104004,
      "learning_rate": 4.8762424101790624e-05,
      "loss": 2.907,
      "step": 38400
    },
    {
      "epoch": 1.488958502533163,
      "grad_norm": 13.689168930053711,
      "learning_rate": 4.875920124788903e-05,
      "loss": 2.8193,
      "step": 38500
    },
    {
      "epoch": 1.4928259272150675,
      "grad_norm": 11.22292709350586,
      "learning_rate": 4.875597839398745e-05,
      "loss": 2.7688,
      "step": 38600
    },
    {
      "epoch": 1.4966933518969718,
      "grad_norm": 8.580955505371094,
      "learning_rate": 4.875275554008586e-05,
      "loss": 2.8192,
      "step": 38700
    },
    {
      "epoch": 1.500560776578876,
      "grad_norm": 10.12331485748291,
      "learning_rate": 4.8749532686184276e-05,
      "loss": 2.8582,
      "step": 38800
    },
    {
      "epoch": 1.5044282012607804,
      "grad_norm": 12.831135749816895,
      "learning_rate": 4.8746309832282684e-05,
      "loss": 2.8285,
      "step": 38900
    },
    {
      "epoch": 1.5082956259426847,
      "grad_norm": 10.933977127075195,
      "learning_rate": 4.87430869783811e-05,
      "loss": 2.8393,
      "step": 39000
    },
    {
      "epoch": 1.5121630506245891,
      "grad_norm": 15.262316703796387,
      "learning_rate": 4.8739864124479514e-05,
      "loss": 2.8474,
      "step": 39100
    },
    {
      "epoch": 1.5160304753064935,
      "grad_norm": 12.575278282165527,
      "learning_rate": 4.873664127057793e-05,
      "loss": 2.8412,
      "step": 39200
    },
    {
      "epoch": 1.5198978999883979,
      "grad_norm": 12.920973777770996,
      "learning_rate": 4.8733418416676336e-05,
      "loss": 2.777,
      "step": 39300
    },
    {
      "epoch": 1.523765324670302,
      "grad_norm": 10.36455249786377,
      "learning_rate": 4.873019556277475e-05,
      "loss": 2.8727,
      "step": 39400
    },
    {
      "epoch": 1.5276327493522064,
      "grad_norm": 11.569990158081055,
      "learning_rate": 4.8726972708873166e-05,
      "loss": 2.8452,
      "step": 39500
    },
    {
      "epoch": 1.5315001740341105,
      "grad_norm": 12.251331329345703,
      "learning_rate": 4.872374985497158e-05,
      "loss": 2.8895,
      "step": 39600
    },
    {
      "epoch": 1.535367598716015,
      "grad_norm": 10.315520286560059,
      "learning_rate": 4.872052700106999e-05,
      "loss": 2.8656,
      "step": 39700
    },
    {
      "epoch": 1.5392350233979193,
      "grad_norm": 9.877409934997559,
      "learning_rate": 4.87173041471684e-05,
      "loss": 2.8373,
      "step": 39800
    },
    {
      "epoch": 1.5431024480798237,
      "grad_norm": 9.78903865814209,
      "learning_rate": 4.871408129326682e-05,
      "loss": 2.8803,
      "step": 39900
    },
    {
      "epoch": 1.546969872761728,
      "grad_norm": 10.335355758666992,
      "learning_rate": 4.871085843936523e-05,
      "loss": 2.8883,
      "step": 40000
    },
    {
      "epoch": 1.5508372974436324,
      "grad_norm": 10.682903289794922,
      "learning_rate": 4.870763558546364e-05,
      "loss": 2.7979,
      "step": 40100
    },
    {
      "epoch": 1.5547047221255366,
      "grad_norm": 9.539783477783203,
      "learning_rate": 4.8704412731562055e-05,
      "loss": 2.7447,
      "step": 40200
    },
    {
      "epoch": 1.558572146807441,
      "grad_norm": 8.876877784729004,
      "learning_rate": 4.870118987766047e-05,
      "loss": 2.8584,
      "step": 40300
    },
    {
      "epoch": 1.562439571489345,
      "grad_norm": 11.34116268157959,
      "learning_rate": 4.8697967023758885e-05,
      "loss": 2.7684,
      "step": 40400
    },
    {
      "epoch": 1.5663069961712495,
      "grad_norm": 14.165547370910645,
      "learning_rate": 4.869474416985729e-05,
      "loss": 2.8277,
      "step": 40500
    },
    {
      "epoch": 1.5701744208531538,
      "grad_norm": 11.092350006103516,
      "learning_rate": 4.869152131595571e-05,
      "loss": 2.9001,
      "step": 40600
    },
    {
      "epoch": 1.5740418455350582,
      "grad_norm": 11.500643730163574,
      "learning_rate": 4.868829846205412e-05,
      "loss": 2.8395,
      "step": 40700
    },
    {
      "epoch": 1.5779092702169626,
      "grad_norm": 10.536563873291016,
      "learning_rate": 4.868507560815254e-05,
      "loss": 2.8474,
      "step": 40800
    },
    {
      "epoch": 1.581776694898867,
      "grad_norm": 11.835408210754395,
      "learning_rate": 4.8681852754250945e-05,
      "loss": 2.7209,
      "step": 40900
    },
    {
      "epoch": 1.5856441195807711,
      "grad_norm": 11.244421005249023,
      "learning_rate": 4.867862990034936e-05,
      "loss": 2.8501,
      "step": 41000
    },
    {
      "epoch": 1.5895115442626755,
      "grad_norm": 13.619446754455566,
      "learning_rate": 4.8675407046447774e-05,
      "loss": 2.8433,
      "step": 41100
    },
    {
      "epoch": 1.5933789689445796,
      "grad_norm": 15.670763969421387,
      "learning_rate": 4.867218419254619e-05,
      "loss": 2.8007,
      "step": 41200
    },
    {
      "epoch": 1.597246393626484,
      "grad_norm": 14.985188484191895,
      "learning_rate": 4.86689613386446e-05,
      "loss": 2.7622,
      "step": 41300
    },
    {
      "epoch": 1.6011138183083884,
      "grad_norm": 12.33414363861084,
      "learning_rate": 4.866573848474301e-05,
      "loss": 2.7765,
      "step": 41400
    },
    {
      "epoch": 1.6049812429902928,
      "grad_norm": 12.764215469360352,
      "learning_rate": 4.8662515630841426e-05,
      "loss": 2.784,
      "step": 41500
    },
    {
      "epoch": 1.6088486676721971,
      "grad_norm": 9.749333381652832,
      "learning_rate": 4.8659292776939834e-05,
      "loss": 2.7869,
      "step": 41600
    },
    {
      "epoch": 1.6127160923541015,
      "grad_norm": 10.541728019714355,
      "learning_rate": 4.865606992303825e-05,
      "loss": 2.7907,
      "step": 41700
    },
    {
      "epoch": 1.6165835170360057,
      "grad_norm": 11.063837051391602,
      "learning_rate": 4.8652847069136664e-05,
      "loss": 2.8591,
      "step": 41800
    },
    {
      "epoch": 1.62045094171791,
      "grad_norm": 12.021734237670898,
      "learning_rate": 4.864962421523508e-05,
      "loss": 2.8207,
      "step": 41900
    },
    {
      "epoch": 1.6243183663998144,
      "grad_norm": 10.698655128479004,
      "learning_rate": 4.8646401361333486e-05,
      "loss": 2.8038,
      "step": 42000
    },
    {
      "epoch": 1.6281857910817186,
      "grad_norm": 13.051753044128418,
      "learning_rate": 4.86431785074319e-05,
      "loss": 2.7967,
      "step": 42100
    },
    {
      "epoch": 1.632053215763623,
      "grad_norm": 11.931554794311523,
      "learning_rate": 4.8639955653530316e-05,
      "loss": 2.8244,
      "step": 42200
    },
    {
      "epoch": 1.6359206404455273,
      "grad_norm": 11.690330505371094,
      "learning_rate": 4.863673279962873e-05,
      "loss": 2.7932,
      "step": 42300
    },
    {
      "epoch": 1.6397880651274317,
      "grad_norm": 12.30509090423584,
      "learning_rate": 4.863350994572714e-05,
      "loss": 2.8878,
      "step": 42400
    },
    {
      "epoch": 1.643655489809336,
      "grad_norm": 13.186769485473633,
      "learning_rate": 4.863028709182555e-05,
      "loss": 2.8064,
      "step": 42500
    },
    {
      "epoch": 1.6475229144912404,
      "grad_norm": 10.897762298583984,
      "learning_rate": 4.862706423792397e-05,
      "loss": 2.8269,
      "step": 42600
    },
    {
      "epoch": 1.6513903391731446,
      "grad_norm": 12.431841850280762,
      "learning_rate": 4.862384138402238e-05,
      "loss": 2.8597,
      "step": 42700
    },
    {
      "epoch": 1.655257763855049,
      "grad_norm": 11.814749717712402,
      "learning_rate": 4.862061853012079e-05,
      "loss": 2.8069,
      "step": 42800
    },
    {
      "epoch": 1.6591251885369531,
      "grad_norm": 12.463223457336426,
      "learning_rate": 4.8617395676219205e-05,
      "loss": 2.8229,
      "step": 42900
    },
    {
      "epoch": 1.6629926132188575,
      "grad_norm": 15.819334030151367,
      "learning_rate": 4.861417282231762e-05,
      "loss": 2.7532,
      "step": 43000
    },
    {
      "epoch": 1.6668600379007619,
      "grad_norm": 12.712223052978516,
      "learning_rate": 4.8610949968416035e-05,
      "loss": 2.8371,
      "step": 43100
    },
    {
      "epoch": 1.6707274625826662,
      "grad_norm": 9.868736267089844,
      "learning_rate": 4.860772711451444e-05,
      "loss": 2.8205,
      "step": 43200
    },
    {
      "epoch": 1.6745948872645706,
      "grad_norm": 10.931950569152832,
      "learning_rate": 4.860450426061286e-05,
      "loss": 2.8482,
      "step": 43300
    },
    {
      "epoch": 1.678462311946475,
      "grad_norm": 12.578097343444824,
      "learning_rate": 4.860128140671127e-05,
      "loss": 2.818,
      "step": 43400
    },
    {
      "epoch": 1.6823297366283791,
      "grad_norm": 13.053266525268555,
      "learning_rate": 4.859805855280969e-05,
      "loss": 2.7648,
      "step": 43500
    },
    {
      "epoch": 1.6861971613102835,
      "grad_norm": 9.757497787475586,
      "learning_rate": 4.8594835698908095e-05,
      "loss": 2.8231,
      "step": 43600
    },
    {
      "epoch": 1.6900645859921877,
      "grad_norm": 11.218828201293945,
      "learning_rate": 4.859161284500651e-05,
      "loss": 2.8127,
      "step": 43700
    },
    {
      "epoch": 1.693932010674092,
      "grad_norm": 10.181017875671387,
      "learning_rate": 4.8588389991104924e-05,
      "loss": 2.8038,
      "step": 43800
    },
    {
      "epoch": 1.6977994353559964,
      "grad_norm": 11.976357460021973,
      "learning_rate": 4.858516713720334e-05,
      "loss": 2.774,
      "step": 43900
    },
    {
      "epoch": 1.7016668600379008,
      "grad_norm": 14.347623825073242,
      "learning_rate": 4.8581944283301753e-05,
      "loss": 2.8299,
      "step": 44000
    },
    {
      "epoch": 1.7055342847198052,
      "grad_norm": 10.004572868347168,
      "learning_rate": 4.857872142940016e-05,
      "loss": 2.8193,
      "step": 44100
    },
    {
      "epoch": 1.7094017094017095,
      "grad_norm": 16.05431365966797,
      "learning_rate": 4.8575498575498576e-05,
      "loss": 2.8764,
      "step": 44200
    },
    {
      "epoch": 1.7132691340836137,
      "grad_norm": 11.014318466186523,
      "learning_rate": 4.857227572159699e-05,
      "loss": 2.6902,
      "step": 44300
    },
    {
      "epoch": 1.717136558765518,
      "grad_norm": 15.206827163696289,
      "learning_rate": 4.8569052867695406e-05,
      "loss": 2.7301,
      "step": 44400
    },
    {
      "epoch": 1.7210039834474222,
      "grad_norm": 13.729058265686035,
      "learning_rate": 4.856583001379382e-05,
      "loss": 2.7232,
      "step": 44500
    },
    {
      "epoch": 1.7248714081293266,
      "grad_norm": 14.317293167114258,
      "learning_rate": 4.8562607159892235e-05,
      "loss": 2.7367,
      "step": 44600
    },
    {
      "epoch": 1.728738832811231,
      "grad_norm": 8.872166633605957,
      "learning_rate": 4.855938430599064e-05,
      "loss": 2.7165,
      "step": 44700
    },
    {
      "epoch": 1.7326062574931353,
      "grad_norm": 9.891863822937012,
      "learning_rate": 4.855616145208906e-05,
      "loss": 2.7384,
      "step": 44800
    },
    {
      "epoch": 1.7364736821750397,
      "grad_norm": 10.971586227416992,
      "learning_rate": 4.855293859818747e-05,
      "loss": 2.7385,
      "step": 44900
    },
    {
      "epoch": 1.740341106856944,
      "grad_norm": 10.711504936218262,
      "learning_rate": 4.854971574428589e-05,
      "loss": 2.777,
      "step": 45000
    },
    {
      "epoch": 1.7442085315388483,
      "grad_norm": 14.233291625976562,
      "learning_rate": 4.8546492890384295e-05,
      "loss": 2.7338,
      "step": 45100
    },
    {
      "epoch": 1.7480759562207526,
      "grad_norm": 11.656879425048828,
      "learning_rate": 4.854327003648271e-05,
      "loss": 2.7492,
      "step": 45200
    },
    {
      "epoch": 1.7519433809026568,
      "grad_norm": 10.946054458618164,
      "learning_rate": 4.8540047182581124e-05,
      "loss": 2.7933,
      "step": 45300
    },
    {
      "epoch": 1.7558108055845612,
      "grad_norm": 8.032022476196289,
      "learning_rate": 4.853682432867954e-05,
      "loss": 2.7128,
      "step": 45400
    },
    {
      "epoch": 1.7596782302664655,
      "grad_norm": 11.714090347290039,
      "learning_rate": 4.853360147477795e-05,
      "loss": 2.7788,
      "step": 45500
    },
    {
      "epoch": 1.76354565494837,
      "grad_norm": 10.292919158935547,
      "learning_rate": 4.853037862087636e-05,
      "loss": 2.7256,
      "step": 45600
    },
    {
      "epoch": 1.7674130796302743,
      "grad_norm": 8.59425163269043,
      "learning_rate": 4.8527155766974777e-05,
      "loss": 2.7587,
      "step": 45700
    },
    {
      "epoch": 1.7712805043121786,
      "grad_norm": 11.001283645629883,
      "learning_rate": 4.852393291307319e-05,
      "loss": 2.796,
      "step": 45800
    },
    {
      "epoch": 1.7751479289940828,
      "grad_norm": 12.615269660949707,
      "learning_rate": 4.85207100591716e-05,
      "loss": 2.8042,
      "step": 45900
    },
    {
      "epoch": 1.7790153536759872,
      "grad_norm": 12.616223335266113,
      "learning_rate": 4.8517487205270014e-05,
      "loss": 2.8097,
      "step": 46000
    },
    {
      "epoch": 1.7828827783578913,
      "grad_norm": 11.871819496154785,
      "learning_rate": 4.851426435136843e-05,
      "loss": 2.7482,
      "step": 46100
    },
    {
      "epoch": 1.7867502030397957,
      "grad_norm": 11.827109336853027,
      "learning_rate": 4.851104149746684e-05,
      "loss": 2.787,
      "step": 46200
    },
    {
      "epoch": 1.7906176277217,
      "grad_norm": 12.909256935119629,
      "learning_rate": 4.850781864356525e-05,
      "loss": 2.8062,
      "step": 46300
    },
    {
      "epoch": 1.7944850524036045,
      "grad_norm": 10.99797248840332,
      "learning_rate": 4.8504595789663666e-05,
      "loss": 2.7298,
      "step": 46400
    },
    {
      "epoch": 1.7983524770855088,
      "grad_norm": 12.475669860839844,
      "learning_rate": 4.850137293576208e-05,
      "loss": 2.7006,
      "step": 46500
    },
    {
      "epoch": 1.8022199017674132,
      "grad_norm": 12.36352252960205,
      "learning_rate": 4.8498150081860495e-05,
      "loss": 2.7592,
      "step": 46600
    },
    {
      "epoch": 1.8060873264493174,
      "grad_norm": 10.904006958007812,
      "learning_rate": 4.8494927227958903e-05,
      "loss": 2.7001,
      "step": 46700
    },
    {
      "epoch": 1.8099547511312217,
      "grad_norm": 10.514335632324219,
      "learning_rate": 4.849170437405732e-05,
      "loss": 2.7862,
      "step": 46800
    },
    {
      "epoch": 1.8138221758131259,
      "grad_norm": 10.687511444091797,
      "learning_rate": 4.848848152015573e-05,
      "loss": 2.8166,
      "step": 46900
    },
    {
      "epoch": 1.8176896004950303,
      "grad_norm": 12.674626350402832,
      "learning_rate": 4.848525866625415e-05,
      "loss": 2.7337,
      "step": 47000
    },
    {
      "epoch": 1.8215570251769346,
      "grad_norm": 15.229828834533691,
      "learning_rate": 4.8482035812352555e-05,
      "loss": 2.8101,
      "step": 47100
    },
    {
      "epoch": 1.825424449858839,
      "grad_norm": 10.853320121765137,
      "learning_rate": 4.847881295845097e-05,
      "loss": 2.7792,
      "step": 47200
    },
    {
      "epoch": 1.8292918745407434,
      "grad_norm": 12.385266304016113,
      "learning_rate": 4.8475590104549385e-05,
      "loss": 2.7117,
      "step": 47300
    },
    {
      "epoch": 1.8331592992226478,
      "grad_norm": 11.261083602905273,
      "learning_rate": 4.847236725064779e-05,
      "loss": 2.7791,
      "step": 47400
    },
    {
      "epoch": 1.837026723904552,
      "grad_norm": 9.415427207946777,
      "learning_rate": 4.846914439674621e-05,
      "loss": 2.7701,
      "step": 47500
    },
    {
      "epoch": 1.8408941485864563,
      "grad_norm": 11.843681335449219,
      "learning_rate": 4.846592154284462e-05,
      "loss": 2.7071,
      "step": 47600
    },
    {
      "epoch": 1.8447615732683604,
      "grad_norm": 11.945456504821777,
      "learning_rate": 4.846269868894304e-05,
      "loss": 2.7205,
      "step": 47700
    },
    {
      "epoch": 1.8486289979502648,
      "grad_norm": 13.343491554260254,
      "learning_rate": 4.8459475835041445e-05,
      "loss": 2.7888,
      "step": 47800
    },
    {
      "epoch": 1.8524964226321692,
      "grad_norm": 10.96035385131836,
      "learning_rate": 4.845625298113986e-05,
      "loss": 2.7368,
      "step": 47900
    },
    {
      "epoch": 1.8563638473140736,
      "grad_norm": 12.230255126953125,
      "learning_rate": 4.8453030127238274e-05,
      "loss": 2.6613,
      "step": 48000
    },
    {
      "epoch": 1.860231271995978,
      "grad_norm": 10.845033645629883,
      "learning_rate": 4.844980727333669e-05,
      "loss": 2.7398,
      "step": 48100
    },
    {
      "epoch": 1.8640986966778823,
      "grad_norm": 12.102655410766602,
      "learning_rate": 4.84465844194351e-05,
      "loss": 2.7495,
      "step": 48200
    },
    {
      "epoch": 1.8679661213597867,
      "grad_norm": 9.606693267822266,
      "learning_rate": 4.844336156553351e-05,
      "loss": 2.7711,
      "step": 48300
    },
    {
      "epoch": 1.8718335460416908,
      "grad_norm": 10.29539680480957,
      "learning_rate": 4.8440138711631926e-05,
      "loss": 2.7788,
      "step": 48400
    },
    {
      "epoch": 1.8757009707235952,
      "grad_norm": 11.059228897094727,
      "learning_rate": 4.843691585773034e-05,
      "loss": 2.8158,
      "step": 48500
    },
    {
      "epoch": 1.8795683954054994,
      "grad_norm": 12.094185829162598,
      "learning_rate": 4.843369300382875e-05,
      "loss": 2.8328,
      "step": 48600
    },
    {
      "epoch": 1.8834358200874037,
      "grad_norm": 14.285297393798828,
      "learning_rate": 4.8430470149927164e-05,
      "loss": 2.7585,
      "step": 48700
    },
    {
      "epoch": 1.887303244769308,
      "grad_norm": 11.478860855102539,
      "learning_rate": 4.842724729602558e-05,
      "loss": 2.6982,
      "step": 48800
    },
    {
      "epoch": 1.8911706694512125,
      "grad_norm": 11.149827003479004,
      "learning_rate": 4.842402444212399e-05,
      "loss": 2.7633,
      "step": 48900
    },
    {
      "epoch": 1.8950380941331169,
      "grad_norm": 16.072635650634766,
      "learning_rate": 4.84208015882224e-05,
      "loss": 2.6988,
      "step": 49000
    },
    {
      "epoch": 1.8989055188150212,
      "grad_norm": 10.497143745422363,
      "learning_rate": 4.8417578734320816e-05,
      "loss": 2.7116,
      "step": 49100
    },
    {
      "epoch": 1.9027729434969254,
      "grad_norm": 10.60454273223877,
      "learning_rate": 4.841435588041923e-05,
      "loss": 2.8099,
      "step": 49200
    },
    {
      "epoch": 1.9066403681788298,
      "grad_norm": 12.398513793945312,
      "learning_rate": 4.8411133026517645e-05,
      "loss": 2.7337,
      "step": 49300
    },
    {
      "epoch": 1.910507792860734,
      "grad_norm": 11.105875015258789,
      "learning_rate": 4.840791017261605e-05,
      "loss": 2.7515,
      "step": 49400
    },
    {
      "epoch": 1.9143752175426383,
      "grad_norm": 9.76968002319336,
      "learning_rate": 4.840468731871447e-05,
      "loss": 2.7865,
      "step": 49500
    },
    {
      "epoch": 1.9182426422245427,
      "grad_norm": 12.714951515197754,
      "learning_rate": 4.840146446481288e-05,
      "loss": 2.722,
      "step": 49600
    },
    {
      "epoch": 1.922110066906447,
      "grad_norm": 9.893155097961426,
      "learning_rate": 4.83982416109113e-05,
      "loss": 2.7406,
      "step": 49700
    },
    {
      "epoch": 1.9259774915883514,
      "grad_norm": 12.908356666564941,
      "learning_rate": 4.8395018757009705e-05,
      "loss": 2.7466,
      "step": 49800
    },
    {
      "epoch": 1.9298449162702558,
      "grad_norm": 15.392721176147461,
      "learning_rate": 4.839179590310812e-05,
      "loss": 2.7733,
      "step": 49900
    },
    {
      "epoch": 1.93371234095216,
      "grad_norm": 10.969168663024902,
      "learning_rate": 4.8388573049206535e-05,
      "loss": 2.7966,
      "step": 50000
    },
    {
      "epoch": 1.9375797656340643,
      "grad_norm": 12.062667846679688,
      "learning_rate": 4.838535019530495e-05,
      "loss": 2.7583,
      "step": 50100
    },
    {
      "epoch": 1.9414471903159685,
      "grad_norm": 10.956388473510742,
      "learning_rate": 4.838212734140336e-05,
      "loss": 2.7711,
      "step": 50200
    },
    {
      "epoch": 1.9453146149978728,
      "grad_norm": 13.246088981628418,
      "learning_rate": 4.837890448750177e-05,
      "loss": 2.7874,
      "step": 50300
    },
    {
      "epoch": 1.9491820396797772,
      "grad_norm": 9.107651710510254,
      "learning_rate": 4.837568163360019e-05,
      "loss": 2.7325,
      "step": 50400
    },
    {
      "epoch": 1.9530494643616816,
      "grad_norm": 12.079833984375,
      "learning_rate": 4.83724587796986e-05,
      "loss": 2.661,
      "step": 50500
    },
    {
      "epoch": 1.956916889043586,
      "grad_norm": 11.058603286743164,
      "learning_rate": 4.836923592579701e-05,
      "loss": 2.7718,
      "step": 50600
    },
    {
      "epoch": 1.9607843137254903,
      "grad_norm": 12.016042709350586,
      "learning_rate": 4.8366013071895424e-05,
      "loss": 2.7233,
      "step": 50700
    },
    {
      "epoch": 1.9646517384073945,
      "grad_norm": 11.532064437866211,
      "learning_rate": 4.836279021799384e-05,
      "loss": 2.6396,
      "step": 50800
    },
    {
      "epoch": 1.9685191630892989,
      "grad_norm": 10.554731369018555,
      "learning_rate": 4.8359567364092254e-05,
      "loss": 2.6654,
      "step": 50900
    },
    {
      "epoch": 1.972386587771203,
      "grad_norm": 10.66648006439209,
      "learning_rate": 4.835634451019067e-05,
      "loss": 2.7931,
      "step": 51000
    },
    {
      "epoch": 1.9762540124531074,
      "grad_norm": 10.720507621765137,
      "learning_rate": 4.835312165628908e-05,
      "loss": 2.7552,
      "step": 51100
    },
    {
      "epoch": 1.9801214371350118,
      "grad_norm": 13.401927947998047,
      "learning_rate": 4.834989880238749e-05,
      "loss": 2.7452,
      "step": 51200
    },
    {
      "epoch": 1.9839888618169161,
      "grad_norm": 10.115129470825195,
      "learning_rate": 4.8346675948485906e-05,
      "loss": 2.7094,
      "step": 51300
    },
    {
      "epoch": 1.9878562864988205,
      "grad_norm": 10.20948314666748,
      "learning_rate": 4.834345309458432e-05,
      "loss": 2.6928,
      "step": 51400
    },
    {
      "epoch": 1.9917237111807249,
      "grad_norm": 11.011829376220703,
      "learning_rate": 4.8340230240682735e-05,
      "loss": 2.6996,
      "step": 51500
    },
    {
      "epoch": 1.995591135862629,
      "grad_norm": 13.642552375793457,
      "learning_rate": 4.833700738678115e-05,
      "loss": 2.7508,
      "step": 51600
    },
    {
      "epoch": 1.9994585605445334,
      "grad_norm": 14.194305419921875,
      "learning_rate": 4.833378453287956e-05,
      "loss": 2.8247,
      "step": 51700
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.595015525817871,
      "eval_runtime": 6.0034,
      "eval_samples_per_second": 226.704,
      "eval_steps_per_second": 226.704,
      "step": 51714
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.609187364578247,
      "eval_runtime": 112.084,
      "eval_samples_per_second": 230.693,
      "eval_steps_per_second": 230.693,
      "step": 51714
    },
    {
      "epoch": 2.0033259852264376,
      "grad_norm": 11.984580993652344,
      "learning_rate": 4.833056167897797e-05,
      "loss": 2.7482,
      "step": 51800
    },
    {
      "epoch": 2.007193409908342,
      "grad_norm": 12.016136169433594,
      "learning_rate": 4.832733882507639e-05,
      "loss": 2.7736,
      "step": 51900
    },
    {
      "epoch": 2.0110608345902463,
      "grad_norm": 15.326830863952637,
      "learning_rate": 4.83241159711748e-05,
      "loss": 2.7145,
      "step": 52000
    },
    {
      "epoch": 2.0149282592721507,
      "grad_norm": 12.515507698059082,
      "learning_rate": 4.832089311727321e-05,
      "loss": 2.7009,
      "step": 52100
    },
    {
      "epoch": 2.018795683954055,
      "grad_norm": 11.829208374023438,
      "learning_rate": 4.8317670263371625e-05,
      "loss": 2.803,
      "step": 52200
    },
    {
      "epoch": 2.0226631086359594,
      "grad_norm": 13.735088348388672,
      "learning_rate": 4.831444740947004e-05,
      "loss": 2.7164,
      "step": 52300
    },
    {
      "epoch": 2.026530533317864,
      "grad_norm": 12.156318664550781,
      "learning_rate": 4.8311224555568454e-05,
      "loss": 2.7611,
      "step": 52400
    },
    {
      "epoch": 2.0303979579997677,
      "grad_norm": 10.349760055541992,
      "learning_rate": 4.830800170166686e-05,
      "loss": 2.7326,
      "step": 52500
    },
    {
      "epoch": 2.034265382681672,
      "grad_norm": 10.16445255279541,
      "learning_rate": 4.830477884776528e-05,
      "loss": 2.7262,
      "step": 52600
    },
    {
      "epoch": 2.0381328073635765,
      "grad_norm": 12.533638000488281,
      "learning_rate": 4.830155599386369e-05,
      "loss": 2.6374,
      "step": 52700
    },
    {
      "epoch": 2.042000232045481,
      "grad_norm": 11.863341331481934,
      "learning_rate": 4.8298333139962106e-05,
      "loss": 2.7648,
      "step": 52800
    },
    {
      "epoch": 2.0458676567273852,
      "grad_norm": 9.663450241088867,
      "learning_rate": 4.8295110286060514e-05,
      "loss": 2.6638,
      "step": 52900
    },
    {
      "epoch": 2.0497350814092896,
      "grad_norm": 12.181743621826172,
      "learning_rate": 4.829188743215893e-05,
      "loss": 2.723,
      "step": 53000
    },
    {
      "epoch": 2.053602506091194,
      "grad_norm": 12.14089298248291,
      "learning_rate": 4.8288664578257344e-05,
      "loss": 2.7245,
      "step": 53100
    },
    {
      "epoch": 2.0574699307730984,
      "grad_norm": 15.383132934570312,
      "learning_rate": 4.828544172435575e-05,
      "loss": 2.6265,
      "step": 53200
    },
    {
      "epoch": 2.0613373554550023,
      "grad_norm": 11.329533576965332,
      "learning_rate": 4.8282218870454166e-05,
      "loss": 2.7692,
      "step": 53300
    },
    {
      "epoch": 2.0652047801369067,
      "grad_norm": 11.346879959106445,
      "learning_rate": 4.827899601655258e-05,
      "loss": 2.7151,
      "step": 53400
    },
    {
      "epoch": 2.069072204818811,
      "grad_norm": 10.660216331481934,
      "learning_rate": 4.8275773162650996e-05,
      "loss": 2.6723,
      "step": 53500
    },
    {
      "epoch": 2.0729396295007154,
      "grad_norm": 11.176191329956055,
      "learning_rate": 4.8272550308749404e-05,
      "loss": 2.7147,
      "step": 53600
    },
    {
      "epoch": 2.07680705418262,
      "grad_norm": 11.647812843322754,
      "learning_rate": 4.826932745484782e-05,
      "loss": 2.7052,
      "step": 53700
    },
    {
      "epoch": 2.080674478864524,
      "grad_norm": 10.711576461791992,
      "learning_rate": 4.826610460094623e-05,
      "loss": 2.7261,
      "step": 53800
    },
    {
      "epoch": 2.0845419035464285,
      "grad_norm": 12.698695182800293,
      "learning_rate": 4.826288174704465e-05,
      "loss": 2.7512,
      "step": 53900
    },
    {
      "epoch": 2.088409328228333,
      "grad_norm": 11.651325225830078,
      "learning_rate": 4.8259658893143056e-05,
      "loss": 2.7391,
      "step": 54000
    },
    {
      "epoch": 2.0922767529102373,
      "grad_norm": 19.072397232055664,
      "learning_rate": 4.825643603924147e-05,
      "loss": 2.7881,
      "step": 54100
    },
    {
      "epoch": 2.0961441775921412,
      "grad_norm": 10.619026184082031,
      "learning_rate": 4.8253213185339885e-05,
      "loss": 2.6894,
      "step": 54200
    },
    {
      "epoch": 2.1000116022740456,
      "grad_norm": 15.566765785217285,
      "learning_rate": 4.82499903314383e-05,
      "loss": 2.7081,
      "step": 54300
    },
    {
      "epoch": 2.10387902695595,
      "grad_norm": 12.469735145568848,
      "learning_rate": 4.824676747753671e-05,
      "loss": 2.642,
      "step": 54400
    },
    {
      "epoch": 2.1077464516378543,
      "grad_norm": 17.6705322265625,
      "learning_rate": 4.824354462363512e-05,
      "loss": 2.6877,
      "step": 54500
    },
    {
      "epoch": 2.1116138763197587,
      "grad_norm": 18.29848289489746,
      "learning_rate": 4.824032176973354e-05,
      "loss": 2.6651,
      "step": 54600
    },
    {
      "epoch": 2.115481301001663,
      "grad_norm": 13.218013763427734,
      "learning_rate": 4.823709891583195e-05,
      "loss": 2.714,
      "step": 54700
    },
    {
      "epoch": 2.1193487256835675,
      "grad_norm": 11.64716625213623,
      "learning_rate": 4.823387606193036e-05,
      "loss": 2.6326,
      "step": 54800
    },
    {
      "epoch": 2.123216150365472,
      "grad_norm": 14.604146957397461,
      "learning_rate": 4.8230653208028775e-05,
      "loss": 2.7182,
      "step": 54900
    },
    {
      "epoch": 2.1270835750473758,
      "grad_norm": 13.453182220458984,
      "learning_rate": 4.822743035412719e-05,
      "loss": 2.745,
      "step": 55000
    },
    {
      "epoch": 2.13095099972928,
      "grad_norm": 11.890019416809082,
      "learning_rate": 4.8224207500225604e-05,
      "loss": 2.671,
      "step": 55100
    },
    {
      "epoch": 2.1348184244111845,
      "grad_norm": 11.557160377502441,
      "learning_rate": 4.822098464632401e-05,
      "loss": 2.6996,
      "step": 55200
    },
    {
      "epoch": 2.138685849093089,
      "grad_norm": 13.321207046508789,
      "learning_rate": 4.821776179242243e-05,
      "loss": 2.7415,
      "step": 55300
    },
    {
      "epoch": 2.1425532737749933,
      "grad_norm": 14.988072395324707,
      "learning_rate": 4.821453893852084e-05,
      "loss": 2.6335,
      "step": 55400
    },
    {
      "epoch": 2.1464206984568976,
      "grad_norm": 13.240316390991211,
      "learning_rate": 4.8211316084619256e-05,
      "loss": 2.6759,
      "step": 55500
    },
    {
      "epoch": 2.150288123138802,
      "grad_norm": 11.46268367767334,
      "learning_rate": 4.8208093230717664e-05,
      "loss": 2.7472,
      "step": 55600
    },
    {
      "epoch": 2.1541555478207064,
      "grad_norm": 17.874496459960938,
      "learning_rate": 4.820487037681608e-05,
      "loss": 2.6075,
      "step": 55700
    },
    {
      "epoch": 2.1580229725026103,
      "grad_norm": 10.14795970916748,
      "learning_rate": 4.8201647522914494e-05,
      "loss": 2.7182,
      "step": 55800
    },
    {
      "epoch": 2.1618903971845147,
      "grad_norm": 11.704095840454102,
      "learning_rate": 4.819842466901291e-05,
      "loss": 2.7569,
      "step": 55900
    },
    {
      "epoch": 2.165757821866419,
      "grad_norm": 13.336108207702637,
      "learning_rate": 4.8195201815111316e-05,
      "loss": 2.6399,
      "step": 56000
    },
    {
      "epoch": 2.1696252465483234,
      "grad_norm": 10.03001594543457,
      "learning_rate": 4.819197896120973e-05,
      "loss": 2.6313,
      "step": 56100
    },
    {
      "epoch": 2.173492671230228,
      "grad_norm": 13.17245101928711,
      "learning_rate": 4.8188756107308146e-05,
      "loss": 2.761,
      "step": 56200
    },
    {
      "epoch": 2.177360095912132,
      "grad_norm": 18.42361831665039,
      "learning_rate": 4.8185533253406554e-05,
      "loss": 2.7425,
      "step": 56300
    },
    {
      "epoch": 2.1812275205940366,
      "grad_norm": 12.480350494384766,
      "learning_rate": 4.818231039950497e-05,
      "loss": 2.6741,
      "step": 56400
    },
    {
      "epoch": 2.185094945275941,
      "grad_norm": 10.44747257232666,
      "learning_rate": 4.817908754560338e-05,
      "loss": 2.7149,
      "step": 56500
    },
    {
      "epoch": 2.188962369957845,
      "grad_norm": 10.611279487609863,
      "learning_rate": 4.81758646917018e-05,
      "loss": 2.7012,
      "step": 56600
    },
    {
      "epoch": 2.1928297946397493,
      "grad_norm": 10.859256744384766,
      "learning_rate": 4.8172641837800206e-05,
      "loss": 2.704,
      "step": 56700
    },
    {
      "epoch": 2.1966972193216536,
      "grad_norm": 12.669916152954102,
      "learning_rate": 4.816941898389862e-05,
      "loss": 2.7113,
      "step": 56800
    },
    {
      "epoch": 2.200564644003558,
      "grad_norm": 9.507461547851562,
      "learning_rate": 4.8166196129997035e-05,
      "loss": 2.6606,
      "step": 56900
    },
    {
      "epoch": 2.2044320686854624,
      "grad_norm": 9.751797676086426,
      "learning_rate": 4.816297327609545e-05,
      "loss": 2.6453,
      "step": 57000
    },
    {
      "epoch": 2.2082994933673668,
      "grad_norm": 10.6864652633667,
      "learning_rate": 4.815975042219386e-05,
      "loss": 2.6886,
      "step": 57100
    },
    {
      "epoch": 2.212166918049271,
      "grad_norm": 8.38883113861084,
      "learning_rate": 4.815652756829227e-05,
      "loss": 2.6771,
      "step": 57200
    },
    {
      "epoch": 2.2160343427311755,
      "grad_norm": 11.253654479980469,
      "learning_rate": 4.815330471439069e-05,
      "loss": 2.6303,
      "step": 57300
    },
    {
      "epoch": 2.2199017674130794,
      "grad_norm": 10.684015274047852,
      "learning_rate": 4.81500818604891e-05,
      "loss": 2.6619,
      "step": 57400
    },
    {
      "epoch": 2.223769192094984,
      "grad_norm": 11.487302780151367,
      "learning_rate": 4.814685900658752e-05,
      "loss": 2.6149,
      "step": 57500
    },
    {
      "epoch": 2.227636616776888,
      "grad_norm": 10.70810604095459,
      "learning_rate": 4.8143636152685925e-05,
      "loss": 2.6966,
      "step": 57600
    },
    {
      "epoch": 2.2315040414587926,
      "grad_norm": 12.34349250793457,
      "learning_rate": 4.814041329878434e-05,
      "loss": 2.7089,
      "step": 57700
    },
    {
      "epoch": 2.235371466140697,
      "grad_norm": 10.943300247192383,
      "learning_rate": 4.8137190444882754e-05,
      "loss": 2.6513,
      "step": 57800
    },
    {
      "epoch": 2.2392388908226013,
      "grad_norm": 10.568330764770508,
      "learning_rate": 4.813396759098117e-05,
      "loss": 2.6663,
      "step": 57900
    },
    {
      "epoch": 2.2431063155045057,
      "grad_norm": 12.61851978302002,
      "learning_rate": 4.8130744737079584e-05,
      "loss": 2.6513,
      "step": 58000
    },
    {
      "epoch": 2.24697374018641,
      "grad_norm": 11.259453773498535,
      "learning_rate": 4.8127521883178e-05,
      "loss": 2.6021,
      "step": 58100
    },
    {
      "epoch": 2.250841164868314,
      "grad_norm": 11.618033409118652,
      "learning_rate": 4.8124299029276406e-05,
      "loss": 2.6745,
      "step": 58200
    },
    {
      "epoch": 2.2547085895502184,
      "grad_norm": 11.033696174621582,
      "learning_rate": 4.812107617537482e-05,
      "loss": 2.5997,
      "step": 58300
    },
    {
      "epoch": 2.2585760142321227,
      "grad_norm": 12.168313980102539,
      "learning_rate": 4.8117853321473236e-05,
      "loss": 2.7007,
      "step": 58400
    },
    {
      "epoch": 2.262443438914027,
      "grad_norm": 10.667349815368652,
      "learning_rate": 4.811463046757165e-05,
      "loss": 2.6298,
      "step": 58500
    },
    {
      "epoch": 2.2663108635959315,
      "grad_norm": 9.54935073852539,
      "learning_rate": 4.8111407613670065e-05,
      "loss": 2.6598,
      "step": 58600
    },
    {
      "epoch": 2.270178288277836,
      "grad_norm": 12.127628326416016,
      "learning_rate": 4.810818475976847e-05,
      "loss": 2.6703,
      "step": 58700
    },
    {
      "epoch": 2.2740457129597402,
      "grad_norm": 11.128009796142578,
      "learning_rate": 4.810496190586689e-05,
      "loss": 2.7063,
      "step": 58800
    },
    {
      "epoch": 2.2779131376416446,
      "grad_norm": 13.315823554992676,
      "learning_rate": 4.81017390519653e-05,
      "loss": 2.629,
      "step": 58900
    },
    {
      "epoch": 2.281780562323549,
      "grad_norm": 11.8663969039917,
      "learning_rate": 4.809851619806372e-05,
      "loss": 2.7002,
      "step": 59000
    },
    {
      "epoch": 2.285647987005453,
      "grad_norm": 10.755072593688965,
      "learning_rate": 4.8095293344162125e-05,
      "loss": 2.7723,
      "step": 59100
    },
    {
      "epoch": 2.2895154116873573,
      "grad_norm": 12.644771575927734,
      "learning_rate": 4.809207049026054e-05,
      "loss": 2.6555,
      "step": 59200
    },
    {
      "epoch": 2.2933828363692617,
      "grad_norm": 15.348128318786621,
      "learning_rate": 4.8088847636358955e-05,
      "loss": 2.6873,
      "step": 59300
    },
    {
      "epoch": 2.297250261051166,
      "grad_norm": 11.539701461791992,
      "learning_rate": 4.808562478245736e-05,
      "loss": 2.6873,
      "step": 59400
    },
    {
      "epoch": 2.3011176857330704,
      "grad_norm": 12.475747108459473,
      "learning_rate": 4.808240192855578e-05,
      "loss": 2.6339,
      "step": 59500
    },
    {
      "epoch": 2.304985110414975,
      "grad_norm": 12.081379890441895,
      "learning_rate": 4.807917907465419e-05,
      "loss": 2.7379,
      "step": 59600
    },
    {
      "epoch": 2.308852535096879,
      "grad_norm": 13.30543041229248,
      "learning_rate": 4.807595622075261e-05,
      "loss": 2.6654,
      "step": 59700
    },
    {
      "epoch": 2.312719959778783,
      "grad_norm": 13.048297882080078,
      "learning_rate": 4.8072733366851015e-05,
      "loss": 2.6585,
      "step": 59800
    },
    {
      "epoch": 2.3165873844606875,
      "grad_norm": 13.492883682250977,
      "learning_rate": 4.806951051294943e-05,
      "loss": 2.6656,
      "step": 59900
    },
    {
      "epoch": 2.320454809142592,
      "grad_norm": 10.17243480682373,
      "learning_rate": 4.8066287659047844e-05,
      "loss": 2.7144,
      "step": 60000
    },
    {
      "epoch": 2.324322233824496,
      "grad_norm": 9.794583320617676,
      "learning_rate": 4.806306480514626e-05,
      "loss": 2.7005,
      "step": 60100
    },
    {
      "epoch": 2.3281896585064006,
      "grad_norm": 13.101433753967285,
      "learning_rate": 4.805984195124467e-05,
      "loss": 2.6103,
      "step": 60200
    },
    {
      "epoch": 2.332057083188305,
      "grad_norm": 15.619622230529785,
      "learning_rate": 4.805661909734308e-05,
      "loss": 2.6302,
      "step": 60300
    },
    {
      "epoch": 2.3359245078702093,
      "grad_norm": 13.868456840515137,
      "learning_rate": 4.8053396243441496e-05,
      "loss": 2.6706,
      "step": 60400
    },
    {
      "epoch": 2.3397919325521137,
      "grad_norm": 11.371696472167969,
      "learning_rate": 4.805017338953991e-05,
      "loss": 2.6362,
      "step": 60500
    },
    {
      "epoch": 2.343659357234018,
      "grad_norm": 12.953911781311035,
      "learning_rate": 4.804695053563832e-05,
      "loss": 2.7313,
      "step": 60600
    },
    {
      "epoch": 2.347526781915922,
      "grad_norm": 11.361368179321289,
      "learning_rate": 4.8043727681736734e-05,
      "loss": 2.6288,
      "step": 60700
    },
    {
      "epoch": 2.3513942065978264,
      "grad_norm": 12.840725898742676,
      "learning_rate": 4.804050482783515e-05,
      "loss": 2.6741,
      "step": 60800
    },
    {
      "epoch": 2.3552616312797308,
      "grad_norm": 13.811330795288086,
      "learning_rate": 4.803728197393356e-05,
      "loss": 2.6397,
      "step": 60900
    },
    {
      "epoch": 2.359129055961635,
      "grad_norm": 11.930586814880371,
      "learning_rate": 4.803405912003197e-05,
      "loss": 2.6612,
      "step": 61000
    },
    {
      "epoch": 2.3629964806435395,
      "grad_norm": 16.239990234375,
      "learning_rate": 4.8030836266130386e-05,
      "loss": 2.5789,
      "step": 61100
    },
    {
      "epoch": 2.366863905325444,
      "grad_norm": 11.765795707702637,
      "learning_rate": 4.80276134122288e-05,
      "loss": 2.6209,
      "step": 61200
    },
    {
      "epoch": 2.3707313300073483,
      "grad_norm": 9.180022239685059,
      "learning_rate": 4.8024390558327215e-05,
      "loss": 2.671,
      "step": 61300
    },
    {
      "epoch": 2.374598754689252,
      "grad_norm": 10.61334228515625,
      "learning_rate": 4.802116770442562e-05,
      "loss": 2.6507,
      "step": 61400
    },
    {
      "epoch": 2.3784661793711566,
      "grad_norm": 13.73450756072998,
      "learning_rate": 4.801794485052404e-05,
      "loss": 2.6799,
      "step": 61500
    },
    {
      "epoch": 2.382333604053061,
      "grad_norm": 10.256061553955078,
      "learning_rate": 4.801472199662245e-05,
      "loss": 2.6404,
      "step": 61600
    },
    {
      "epoch": 2.3862010287349653,
      "grad_norm": 11.206364631652832,
      "learning_rate": 4.801149914272087e-05,
      "loss": 2.6456,
      "step": 61700
    },
    {
      "epoch": 2.3900684534168697,
      "grad_norm": 10.770604133605957,
      "learning_rate": 4.8008276288819275e-05,
      "loss": 2.6863,
      "step": 61800
    },
    {
      "epoch": 2.393935878098774,
      "grad_norm": 11.765501022338867,
      "learning_rate": 4.800505343491769e-05,
      "loss": 2.6376,
      "step": 61900
    },
    {
      "epoch": 2.3978033027806784,
      "grad_norm": 10.644545555114746,
      "learning_rate": 4.8001830581016105e-05,
      "loss": 2.6152,
      "step": 62000
    },
    {
      "epoch": 2.401670727462583,
      "grad_norm": 12.169551849365234,
      "learning_rate": 4.799860772711451e-05,
      "loss": 2.5885,
      "step": 62100
    },
    {
      "epoch": 2.405538152144487,
      "grad_norm": 10.368197441101074,
      "learning_rate": 4.799538487321293e-05,
      "loss": 2.6564,
      "step": 62200
    },
    {
      "epoch": 2.409405576826391,
      "grad_norm": 14.011360168457031,
      "learning_rate": 4.799216201931134e-05,
      "loss": 2.623,
      "step": 62300
    },
    {
      "epoch": 2.4132730015082955,
      "grad_norm": 12.5914945602417,
      "learning_rate": 4.798893916540976e-05,
      "loss": 2.6382,
      "step": 62400
    },
    {
      "epoch": 2.4171404261902,
      "grad_norm": 19.815645217895508,
      "learning_rate": 4.7985716311508165e-05,
      "loss": 2.6798,
      "step": 62500
    },
    {
      "epoch": 2.4210078508721042,
      "grad_norm": 14.28756332397461,
      "learning_rate": 4.798249345760658e-05,
      "loss": 2.5752,
      "step": 62600
    },
    {
      "epoch": 2.4248752755540086,
      "grad_norm": 12.627927780151367,
      "learning_rate": 4.7979270603704994e-05,
      "loss": 2.6686,
      "step": 62700
    },
    {
      "epoch": 2.428742700235913,
      "grad_norm": 12.407604217529297,
      "learning_rate": 4.797604774980341e-05,
      "loss": 2.6866,
      "step": 62800
    },
    {
      "epoch": 2.4326101249178174,
      "grad_norm": 8.708335876464844,
      "learning_rate": 4.797282489590182e-05,
      "loss": 2.6583,
      "step": 62900
    },
    {
      "epoch": 2.4364775495997217,
      "grad_norm": 12.672526359558105,
      "learning_rate": 4.796960204200023e-05,
      "loss": 2.585,
      "step": 63000
    },
    {
      "epoch": 2.4403449742816257,
      "grad_norm": 8.976652145385742,
      "learning_rate": 4.7966379188098646e-05,
      "loss": 2.6422,
      "step": 63100
    },
    {
      "epoch": 2.44421239896353,
      "grad_norm": 10.132478713989258,
      "learning_rate": 4.796315633419706e-05,
      "loss": 2.589,
      "step": 63200
    },
    {
      "epoch": 2.4480798236454344,
      "grad_norm": 11.933313369750977,
      "learning_rate": 4.795993348029547e-05,
      "loss": 2.6253,
      "step": 63300
    },
    {
      "epoch": 2.451947248327339,
      "grad_norm": 10.748333930969238,
      "learning_rate": 4.7956710626393884e-05,
      "loss": 2.6165,
      "step": 63400
    },
    {
      "epoch": 2.455814673009243,
      "grad_norm": 11.565808296203613,
      "learning_rate": 4.79534877724923e-05,
      "loss": 2.5967,
      "step": 63500
    },
    {
      "epoch": 2.4596820976911475,
      "grad_norm": 12.241741180419922,
      "learning_rate": 4.795026491859071e-05,
      "loss": 2.6651,
      "step": 63600
    },
    {
      "epoch": 2.463549522373052,
      "grad_norm": 10.670074462890625,
      "learning_rate": 4.794704206468912e-05,
      "loss": 2.6568,
      "step": 63700
    },
    {
      "epoch": 2.4674169470549563,
      "grad_norm": 10.585684776306152,
      "learning_rate": 4.7943819210787536e-05,
      "loss": 2.6247,
      "step": 63800
    },
    {
      "epoch": 2.4712843717368607,
      "grad_norm": 10.18244743347168,
      "learning_rate": 4.794059635688595e-05,
      "loss": 2.6421,
      "step": 63900
    },
    {
      "epoch": 2.4751517964187646,
      "grad_norm": 11.6072416305542,
      "learning_rate": 4.7937373502984365e-05,
      "loss": 2.7413,
      "step": 64000
    },
    {
      "epoch": 2.479019221100669,
      "grad_norm": 12.07791519165039,
      "learning_rate": 4.793415064908277e-05,
      "loss": 2.6467,
      "step": 64100
    },
    {
      "epoch": 2.4828866457825733,
      "grad_norm": 13.033596992492676,
      "learning_rate": 4.793092779518119e-05,
      "loss": 2.6816,
      "step": 64200
    },
    {
      "epoch": 2.4867540704644777,
      "grad_norm": 10.637701988220215,
      "learning_rate": 4.79277049412796e-05,
      "loss": 2.5812,
      "step": 64300
    },
    {
      "epoch": 2.490621495146382,
      "grad_norm": 12.29122543334961,
      "learning_rate": 4.792448208737802e-05,
      "loss": 2.5599,
      "step": 64400
    },
    {
      "epoch": 2.4944889198282865,
      "grad_norm": 12.425097465515137,
      "learning_rate": 4.792125923347643e-05,
      "loss": 2.6031,
      "step": 64500
    },
    {
      "epoch": 2.498356344510191,
      "grad_norm": 15.165380477905273,
      "learning_rate": 4.7918036379574847e-05,
      "loss": 2.6371,
      "step": 64600
    },
    {
      "epoch": 2.5022237691920948,
      "grad_norm": 10.708800315856934,
      "learning_rate": 4.7914813525673255e-05,
      "loss": 2.6742,
      "step": 64700
    },
    {
      "epoch": 2.506091193873999,
      "grad_norm": 10.927173614501953,
      "learning_rate": 4.791159067177167e-05,
      "loss": 2.6183,
      "step": 64800
    },
    {
      "epoch": 2.5099586185559035,
      "grad_norm": 11.429464340209961,
      "learning_rate": 4.7908367817870084e-05,
      "loss": 2.6913,
      "step": 64900
    },
    {
      "epoch": 2.513826043237808,
      "grad_norm": 18.32984733581543,
      "learning_rate": 4.79051449639685e-05,
      "loss": 2.5838,
      "step": 65000
    },
    {
      "epoch": 2.5176934679197123,
      "grad_norm": 11.967634201049805,
      "learning_rate": 4.7901922110066913e-05,
      "loss": 2.6361,
      "step": 65100
    },
    {
      "epoch": 2.5215608926016166,
      "grad_norm": 11.211578369140625,
      "learning_rate": 4.789869925616532e-05,
      "loss": 2.7024,
      "step": 65200
    },
    {
      "epoch": 2.525428317283521,
      "grad_norm": 11.061905860900879,
      "learning_rate": 4.7895476402263736e-05,
      "loss": 2.6162,
      "step": 65300
    },
    {
      "epoch": 2.5292957419654254,
      "grad_norm": 10.961190223693848,
      "learning_rate": 4.789225354836215e-05,
      "loss": 2.7054,
      "step": 65400
    },
    {
      "epoch": 2.5331631666473298,
      "grad_norm": 8.625204086303711,
      "learning_rate": 4.7889030694460566e-05,
      "loss": 2.6131,
      "step": 65500
    },
    {
      "epoch": 2.5370305913292337,
      "grad_norm": 15.732843399047852,
      "learning_rate": 4.7885807840558973e-05,
      "loss": 2.6321,
      "step": 65600
    },
    {
      "epoch": 2.540898016011138,
      "grad_norm": 10.900118827819824,
      "learning_rate": 4.788258498665739e-05,
      "loss": 2.6414,
      "step": 65700
    },
    {
      "epoch": 2.5447654406930424,
      "grad_norm": 12.45571231842041,
      "learning_rate": 4.78793621327558e-05,
      "loss": 2.5936,
      "step": 65800
    },
    {
      "epoch": 2.548632865374947,
      "grad_norm": 12.199542999267578,
      "learning_rate": 4.787613927885422e-05,
      "loss": 2.6715,
      "step": 65900
    },
    {
      "epoch": 2.552500290056851,
      "grad_norm": 9.84536075592041,
      "learning_rate": 4.7872916424952626e-05,
      "loss": 2.6721,
      "step": 66000
    },
    {
      "epoch": 2.5563677147387556,
      "grad_norm": 13.98239803314209,
      "learning_rate": 4.786969357105104e-05,
      "loss": 2.7432,
      "step": 66100
    },
    {
      "epoch": 2.56023513942066,
      "grad_norm": 9.302152633666992,
      "learning_rate": 4.7866470717149455e-05,
      "loss": 2.6671,
      "step": 66200
    },
    {
      "epoch": 2.564102564102564,
      "grad_norm": 12.707266807556152,
      "learning_rate": 4.786324786324787e-05,
      "loss": 2.5541,
      "step": 66300
    },
    {
      "epoch": 2.5679699887844682,
      "grad_norm": 9.391818046569824,
      "learning_rate": 4.786002500934628e-05,
      "loss": 2.5415,
      "step": 66400
    },
    {
      "epoch": 2.5718374134663726,
      "grad_norm": 14.916263580322266,
      "learning_rate": 4.785680215544469e-05,
      "loss": 2.68,
      "step": 66500
    },
    {
      "epoch": 2.575704838148277,
      "grad_norm": 10.196688652038574,
      "learning_rate": 4.785357930154311e-05,
      "loss": 2.6555,
      "step": 66600
    },
    {
      "epoch": 2.5795722628301814,
      "grad_norm": 13.10278606414795,
      "learning_rate": 4.785035644764152e-05,
      "loss": 2.6827,
      "step": 66700
    },
    {
      "epoch": 2.5834396875120857,
      "grad_norm": 13.820043563842773,
      "learning_rate": 4.784713359373993e-05,
      "loss": 2.688,
      "step": 66800
    },
    {
      "epoch": 2.58730711219399,
      "grad_norm": 11.35772705078125,
      "learning_rate": 4.7843910739838344e-05,
      "loss": 2.6477,
      "step": 66900
    },
    {
      "epoch": 2.5911745368758945,
      "grad_norm": 11.485821723937988,
      "learning_rate": 4.784068788593676e-05,
      "loss": 2.6174,
      "step": 67000
    },
    {
      "epoch": 2.595041961557799,
      "grad_norm": 11.44229793548584,
      "learning_rate": 4.7837465032035174e-05,
      "loss": 2.672,
      "step": 67100
    },
    {
      "epoch": 2.5989093862397032,
      "grad_norm": 10.780437469482422,
      "learning_rate": 4.783424217813358e-05,
      "loss": 2.6075,
      "step": 67200
    },
    {
      "epoch": 2.602776810921607,
      "grad_norm": 12.615857124328613,
      "learning_rate": 4.7831019324231997e-05,
      "loss": 2.6929,
      "step": 67300
    },
    {
      "epoch": 2.6066442356035116,
      "grad_norm": 14.196723937988281,
      "learning_rate": 4.782779647033041e-05,
      "loss": 2.5973,
      "step": 67400
    },
    {
      "epoch": 2.610511660285416,
      "grad_norm": 13.747245788574219,
      "learning_rate": 4.7824573616428826e-05,
      "loss": 2.6018,
      "step": 67500
    },
    {
      "epoch": 2.6143790849673203,
      "grad_norm": 13.821662902832031,
      "learning_rate": 4.7821350762527234e-05,
      "loss": 2.5791,
      "step": 67600
    },
    {
      "epoch": 2.6182465096492247,
      "grad_norm": 11.905004501342773,
      "learning_rate": 4.781812790862565e-05,
      "loss": 2.6478,
      "step": 67700
    },
    {
      "epoch": 2.622113934331129,
      "grad_norm": 15.82669448852539,
      "learning_rate": 4.781490505472406e-05,
      "loss": 2.6333,
      "step": 67800
    },
    {
      "epoch": 2.625981359013033,
      "grad_norm": 13.252135276794434,
      "learning_rate": 4.781168220082247e-05,
      "loss": 2.6069,
      "step": 67900
    },
    {
      "epoch": 2.6298487836949374,
      "grad_norm": 13.625860214233398,
      "learning_rate": 4.7808459346920886e-05,
      "loss": 2.5293,
      "step": 68000
    },
    {
      "epoch": 2.6337162083768417,
      "grad_norm": 11.018630981445312,
      "learning_rate": 4.78052364930193e-05,
      "loss": 2.5835,
      "step": 68100
    },
    {
      "epoch": 2.637583633058746,
      "grad_norm": 11.497334480285645,
      "learning_rate": 4.7802013639117715e-05,
      "loss": 2.6624,
      "step": 68200
    },
    {
      "epoch": 2.6414510577406505,
      "grad_norm": 22.177955627441406,
      "learning_rate": 4.7798790785216123e-05,
      "loss": 2.6182,
      "step": 68300
    },
    {
      "epoch": 2.645318482422555,
      "grad_norm": 10.850152969360352,
      "learning_rate": 4.779556793131454e-05,
      "loss": 2.5679,
      "step": 68400
    },
    {
      "epoch": 2.6491859071044592,
      "grad_norm": 15.505343437194824,
      "learning_rate": 4.779234507741295e-05,
      "loss": 2.6324,
      "step": 68500
    },
    {
      "epoch": 2.6530533317863636,
      "grad_norm": 14.365192413330078,
      "learning_rate": 4.778912222351137e-05,
      "loss": 2.6405,
      "step": 68600
    },
    {
      "epoch": 2.656920756468268,
      "grad_norm": 12.317343711853027,
      "learning_rate": 4.7785899369609776e-05,
      "loss": 2.5765,
      "step": 68700
    },
    {
      "epoch": 2.6607881811501723,
      "grad_norm": 9.635002136230469,
      "learning_rate": 4.778267651570819e-05,
      "loss": 2.5474,
      "step": 68800
    },
    {
      "epoch": 2.6646556058320763,
      "grad_norm": 9.94176197052002,
      "learning_rate": 4.7779453661806605e-05,
      "loss": 2.6072,
      "step": 68900
    },
    {
      "epoch": 2.6685230305139807,
      "grad_norm": 14.309548377990723,
      "learning_rate": 4.777623080790502e-05,
      "loss": 2.5482,
      "step": 69000
    },
    {
      "epoch": 2.672390455195885,
      "grad_norm": 11.347043991088867,
      "learning_rate": 4.777300795400343e-05,
      "loss": 2.5954,
      "step": 69100
    },
    {
      "epoch": 2.6762578798777894,
      "grad_norm": 8.998818397521973,
      "learning_rate": 4.776978510010184e-05,
      "loss": 2.5942,
      "step": 69200
    },
    {
      "epoch": 2.6801253045596938,
      "grad_norm": 12.189281463623047,
      "learning_rate": 4.776656224620026e-05,
      "loss": 2.6418,
      "step": 69300
    },
    {
      "epoch": 2.683992729241598,
      "grad_norm": 13.267939567565918,
      "learning_rate": 4.776333939229867e-05,
      "loss": 2.5703,
      "step": 69400
    },
    {
      "epoch": 2.687860153923502,
      "grad_norm": 10.625880241394043,
      "learning_rate": 4.776011653839708e-05,
      "loss": 2.5773,
      "step": 69500
    },
    {
      "epoch": 2.6917275786054065,
      "grad_norm": 10.345649719238281,
      "learning_rate": 4.7756893684495494e-05,
      "loss": 2.5637,
      "step": 69600
    },
    {
      "epoch": 2.695595003287311,
      "grad_norm": 11.454219818115234,
      "learning_rate": 4.775367083059391e-05,
      "loss": 2.6084,
      "step": 69700
    },
    {
      "epoch": 2.699462427969215,
      "grad_norm": 9.728167533874512,
      "learning_rate": 4.7750447976692324e-05,
      "loss": 2.6229,
      "step": 69800
    },
    {
      "epoch": 2.7033298526511196,
      "grad_norm": 17.2287540435791,
      "learning_rate": 4.774722512279073e-05,
      "loss": 2.5041,
      "step": 69900
    },
    {
      "epoch": 2.707197277333024,
      "grad_norm": 13.822603225708008,
      "learning_rate": 4.7744002268889147e-05,
      "loss": 2.6229,
      "step": 70000
    },
    {
      "epoch": 2.7110647020149283,
      "grad_norm": 16.368249893188477,
      "learning_rate": 4.774077941498756e-05,
      "loss": 2.6352,
      "step": 70100
    },
    {
      "epoch": 2.7149321266968327,
      "grad_norm": 12.718688011169434,
      "learning_rate": 4.7737556561085976e-05,
      "loss": 2.5431,
      "step": 70200
    },
    {
      "epoch": 2.718799551378737,
      "grad_norm": 13.866657257080078,
      "learning_rate": 4.7734333707184384e-05,
      "loss": 2.6717,
      "step": 70300
    },
    {
      "epoch": 2.7226669760606415,
      "grad_norm": 14.100528717041016,
      "learning_rate": 4.77311108532828e-05,
      "loss": 2.6503,
      "step": 70400
    },
    {
      "epoch": 2.7265344007425454,
      "grad_norm": 13.702670097351074,
      "learning_rate": 4.772788799938121e-05,
      "loss": 2.6401,
      "step": 70500
    },
    {
      "epoch": 2.7304018254244498,
      "grad_norm": 12.392752647399902,
      "learning_rate": 4.772466514547963e-05,
      "loss": 2.6647,
      "step": 70600
    },
    {
      "epoch": 2.734269250106354,
      "grad_norm": 12.551030158996582,
      "learning_rate": 4.7721442291578036e-05,
      "loss": 2.4865,
      "step": 70700
    },
    {
      "epoch": 2.7381366747882585,
      "grad_norm": 11.684564590454102,
      "learning_rate": 4.771821943767645e-05,
      "loss": 2.5182,
      "step": 70800
    },
    {
      "epoch": 2.742004099470163,
      "grad_norm": 11.566561698913574,
      "learning_rate": 4.7714996583774865e-05,
      "loss": 2.6393,
      "step": 70900
    },
    {
      "epoch": 2.7458715241520673,
      "grad_norm": 12.331571578979492,
      "learning_rate": 4.771177372987328e-05,
      "loss": 2.6373,
      "step": 71000
    },
    {
      "epoch": 2.749738948833971,
      "grad_norm": 11.670929908752441,
      "learning_rate": 4.770855087597169e-05,
      "loss": 2.6704,
      "step": 71100
    },
    {
      "epoch": 2.7536063735158756,
      "grad_norm": 11.946353912353516,
      "learning_rate": 4.77053280220701e-05,
      "loss": 2.6026,
      "step": 71200
    },
    {
      "epoch": 2.75747379819778,
      "grad_norm": 10.202155113220215,
      "learning_rate": 4.770210516816852e-05,
      "loss": 2.6527,
      "step": 71300
    },
    {
      "epoch": 2.7613412228796843,
      "grad_norm": 13.841562271118164,
      "learning_rate": 4.769888231426693e-05,
      "loss": 2.6725,
      "step": 71400
    },
    {
      "epoch": 2.7652086475615887,
      "grad_norm": 11.935789108276367,
      "learning_rate": 4.769565946036535e-05,
      "loss": 2.581,
      "step": 71500
    },
    {
      "epoch": 2.769076072243493,
      "grad_norm": 11.469494819641113,
      "learning_rate": 4.769243660646376e-05,
      "loss": 2.5502,
      "step": 71600
    },
    {
      "epoch": 2.7729434969253974,
      "grad_norm": 12.965470314025879,
      "learning_rate": 4.768921375256217e-05,
      "loss": 2.6548,
      "step": 71700
    },
    {
      "epoch": 2.776810921607302,
      "grad_norm": 11.582527160644531,
      "learning_rate": 4.7685990898660584e-05,
      "loss": 2.6098,
      "step": 71800
    },
    {
      "epoch": 2.780678346289206,
      "grad_norm": 9.153876304626465,
      "learning_rate": 4.7682768044759e-05,
      "loss": 2.6195,
      "step": 71900
    },
    {
      "epoch": 2.7845457709711106,
      "grad_norm": 12.308762550354004,
      "learning_rate": 4.7679545190857414e-05,
      "loss": 2.5598,
      "step": 72000
    },
    {
      "epoch": 2.788413195653015,
      "grad_norm": 13.285557746887207,
      "learning_rate": 4.767632233695583e-05,
      "loss": 2.5998,
      "step": 72100
    },
    {
      "epoch": 2.792280620334919,
      "grad_norm": 11.969847679138184,
      "learning_rate": 4.7673099483054236e-05,
      "loss": 2.6089,
      "step": 72200
    },
    {
      "epoch": 2.7961480450168232,
      "grad_norm": 14.056909561157227,
      "learning_rate": 4.766987662915265e-05,
      "loss": 2.5787,
      "step": 72300
    },
    {
      "epoch": 2.8000154696987276,
      "grad_norm": 13.523787498474121,
      "learning_rate": 4.7666653775251066e-05,
      "loss": 2.5288,
      "step": 72400
    },
    {
      "epoch": 2.803882894380632,
      "grad_norm": 11.093809127807617,
      "learning_rate": 4.766343092134948e-05,
      "loss": 2.5493,
      "step": 72500
    },
    {
      "epoch": 2.8077503190625364,
      "grad_norm": 12.120494842529297,
      "learning_rate": 4.766020806744789e-05,
      "loss": 2.5639,
      "step": 72600
    },
    {
      "epoch": 2.8116177437444407,
      "grad_norm": 10.608753204345703,
      "learning_rate": 4.76569852135463e-05,
      "loss": 2.7122,
      "step": 72700
    },
    {
      "epoch": 2.8154851684263447,
      "grad_norm": 10.612672805786133,
      "learning_rate": 4.765376235964472e-05,
      "loss": 2.5751,
      "step": 72800
    },
    {
      "epoch": 2.819352593108249,
      "grad_norm": 14.796998977661133,
      "learning_rate": 4.765053950574313e-05,
      "loss": 2.5666,
      "step": 72900
    },
    {
      "epoch": 2.8232200177901534,
      "grad_norm": 13.473142623901367,
      "learning_rate": 4.764731665184154e-05,
      "loss": 2.5482,
      "step": 73000
    },
    {
      "epoch": 2.827087442472058,
      "grad_norm": 12.43746280670166,
      "learning_rate": 4.7644093797939955e-05,
      "loss": 2.5881,
      "step": 73100
    },
    {
      "epoch": 2.830954867153962,
      "grad_norm": 15.04827880859375,
      "learning_rate": 4.764087094403837e-05,
      "loss": 2.5389,
      "step": 73200
    },
    {
      "epoch": 2.8348222918358665,
      "grad_norm": 18.438838958740234,
      "learning_rate": 4.7637648090136785e-05,
      "loss": 2.6569,
      "step": 73300
    },
    {
      "epoch": 2.838689716517771,
      "grad_norm": 11.534834861755371,
      "learning_rate": 4.763442523623519e-05,
      "loss": 2.5911,
      "step": 73400
    },
    {
      "epoch": 2.8425571411996753,
      "grad_norm": 12.229121208190918,
      "learning_rate": 4.763120238233361e-05,
      "loss": 2.6686,
      "step": 73500
    },
    {
      "epoch": 2.8464245658815797,
      "grad_norm": 13.597604751586914,
      "learning_rate": 4.762797952843202e-05,
      "loss": 2.6594,
      "step": 73600
    },
    {
      "epoch": 2.850291990563484,
      "grad_norm": 13.217788696289062,
      "learning_rate": 4.762475667453044e-05,
      "loss": 2.6382,
      "step": 73700
    },
    {
      "epoch": 2.854159415245388,
      "grad_norm": 11.30639362335205,
      "learning_rate": 4.7621533820628845e-05,
      "loss": 2.5706,
      "step": 73800
    },
    {
      "epoch": 2.8580268399272923,
      "grad_norm": 10.768976211547852,
      "learning_rate": 4.761831096672726e-05,
      "loss": 2.5801,
      "step": 73900
    },
    {
      "epoch": 2.8618942646091967,
      "grad_norm": 14.812049865722656,
      "learning_rate": 4.7615088112825674e-05,
      "loss": 2.6389,
      "step": 74000
    },
    {
      "epoch": 2.865761689291101,
      "grad_norm": 12.376669883728027,
      "learning_rate": 4.761186525892408e-05,
      "loss": 2.6408,
      "step": 74100
    },
    {
      "epoch": 2.8696291139730055,
      "grad_norm": 13.962846755981445,
      "learning_rate": 4.76086424050225e-05,
      "loss": 2.5441,
      "step": 74200
    },
    {
      "epoch": 2.87349653865491,
      "grad_norm": 13.21922779083252,
      "learning_rate": 4.760541955112091e-05,
      "loss": 2.6101,
      "step": 74300
    },
    {
      "epoch": 2.8773639633368138,
      "grad_norm": 12.471378326416016,
      "learning_rate": 4.7602196697219326e-05,
      "loss": 2.5488,
      "step": 74400
    },
    {
      "epoch": 2.881231388018718,
      "grad_norm": 10.967215538024902,
      "learning_rate": 4.7598973843317734e-05,
      "loss": 2.5509,
      "step": 74500
    },
    {
      "epoch": 2.8850988127006225,
      "grad_norm": 12.395756721496582,
      "learning_rate": 4.759575098941615e-05,
      "loss": 2.5798,
      "step": 74600
    },
    {
      "epoch": 2.888966237382527,
      "grad_norm": 11.969383239746094,
      "learning_rate": 4.7592528135514564e-05,
      "loss": 2.589,
      "step": 74700
    },
    {
      "epoch": 2.8928336620644313,
      "grad_norm": 11.262697219848633,
      "learning_rate": 4.758930528161298e-05,
      "loss": 2.5906,
      "step": 74800
    },
    {
      "epoch": 2.8967010867463356,
      "grad_norm": 12.264019012451172,
      "learning_rate": 4.7586082427711386e-05,
      "loss": 2.5621,
      "step": 74900
    },
    {
      "epoch": 2.90056851142824,
      "grad_norm": 10.8200044631958,
      "learning_rate": 4.75828595738098e-05,
      "loss": 2.6383,
      "step": 75000
    },
    {
      "epoch": 2.9044359361101444,
      "grad_norm": 9.477106094360352,
      "learning_rate": 4.7579636719908216e-05,
      "loss": 2.5762,
      "step": 75100
    },
    {
      "epoch": 2.9083033607920488,
      "grad_norm": 13.523917198181152,
      "learning_rate": 4.757641386600663e-05,
      "loss": 2.5369,
      "step": 75200
    },
    {
      "epoch": 2.912170785473953,
      "grad_norm": 12.788148880004883,
      "learning_rate": 4.757319101210504e-05,
      "loss": 2.58,
      "step": 75300
    },
    {
      "epoch": 2.916038210155857,
      "grad_norm": 11.2503662109375,
      "learning_rate": 4.756996815820345e-05,
      "loss": 2.5411,
      "step": 75400
    },
    {
      "epoch": 2.9199056348377614,
      "grad_norm": 12.10870361328125,
      "learning_rate": 4.756674530430187e-05,
      "loss": 2.5773,
      "step": 75500
    },
    {
      "epoch": 2.923773059519666,
      "grad_norm": 13.004871368408203,
      "learning_rate": 4.756352245040028e-05,
      "loss": 2.6223,
      "step": 75600
    },
    {
      "epoch": 2.92764048420157,
      "grad_norm": 20.098302841186523,
      "learning_rate": 4.756029959649869e-05,
      "loss": 2.5982,
      "step": 75700
    },
    {
      "epoch": 2.9315079088834746,
      "grad_norm": 11.566576957702637,
      "learning_rate": 4.7557076742597105e-05,
      "loss": 2.5605,
      "step": 75800
    },
    {
      "epoch": 2.935375333565379,
      "grad_norm": 10.019816398620605,
      "learning_rate": 4.755385388869552e-05,
      "loss": 2.5369,
      "step": 75900
    },
    {
      "epoch": 2.939242758247283,
      "grad_norm": 15.088028907775879,
      "learning_rate": 4.7550631034793935e-05,
      "loss": 2.5759,
      "step": 76000
    },
    {
      "epoch": 2.9431101829291872,
      "grad_norm": 12.9610013961792,
      "learning_rate": 4.754740818089234e-05,
      "loss": 2.5995,
      "step": 76100
    },
    {
      "epoch": 2.9469776076110916,
      "grad_norm": 11.049814224243164,
      "learning_rate": 4.754418532699076e-05,
      "loss": 2.5219,
      "step": 76200
    },
    {
      "epoch": 2.950845032292996,
      "grad_norm": 13.644680976867676,
      "learning_rate": 4.754096247308917e-05,
      "loss": 2.6602,
      "step": 76300
    },
    {
      "epoch": 2.9547124569749004,
      "grad_norm": 12.753107070922852,
      "learning_rate": 4.753773961918759e-05,
      "loss": 2.5812,
      "step": 76400
    },
    {
      "epoch": 2.9585798816568047,
      "grad_norm": 10.570812225341797,
      "learning_rate": 4.7534516765285995e-05,
      "loss": 2.6359,
      "step": 76500
    },
    {
      "epoch": 2.962447306338709,
      "grad_norm": 20.14385223388672,
      "learning_rate": 4.753129391138441e-05,
      "loss": 2.594,
      "step": 76600
    },
    {
      "epoch": 2.9663147310206135,
      "grad_norm": 11.036210060119629,
      "learning_rate": 4.7528071057482824e-05,
      "loss": 2.526,
      "step": 76700
    },
    {
      "epoch": 2.970182155702518,
      "grad_norm": 12.635332107543945,
      "learning_rate": 4.752484820358123e-05,
      "loss": 2.6089,
      "step": 76800
    },
    {
      "epoch": 2.9740495803844222,
      "grad_norm": 10.951274871826172,
      "learning_rate": 4.752162534967965e-05,
      "loss": 2.6134,
      "step": 76900
    },
    {
      "epoch": 2.977917005066326,
      "grad_norm": 13.21359634399414,
      "learning_rate": 4.751840249577806e-05,
      "loss": 2.5917,
      "step": 77000
    },
    {
      "epoch": 2.9817844297482305,
      "grad_norm": 11.702211380004883,
      "learning_rate": 4.7515179641876476e-05,
      "loss": 2.5459,
      "step": 77100
    },
    {
      "epoch": 2.985651854430135,
      "grad_norm": 12.881514549255371,
      "learning_rate": 4.7511956787974884e-05,
      "loss": 2.5343,
      "step": 77200
    },
    {
      "epoch": 2.9895192791120393,
      "grad_norm": 12.774600982666016,
      "learning_rate": 4.75087339340733e-05,
      "loss": 2.5695,
      "step": 77300
    },
    {
      "epoch": 2.9933867037939437,
      "grad_norm": 11.617778778076172,
      "learning_rate": 4.7505511080171714e-05,
      "loss": 2.6077,
      "step": 77400
    },
    {
      "epoch": 2.997254128475848,
      "grad_norm": 9.65219497680664,
      "learning_rate": 4.750228822627013e-05,
      "loss": 2.5838,
      "step": 77500
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.4243993759155273,
      "eval_runtime": 6.0157,
      "eval_samples_per_second": 226.241,
      "eval_steps_per_second": 226.241,
      "step": 77571
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.4324233531951904,
      "eval_runtime": 113.8356,
      "eval_samples_per_second": 227.143,
      "eval_steps_per_second": 227.143,
      "step": 77571
    },
    {
      "epoch": 3.0011215531577524,
      "grad_norm": 11.331966400146484,
      "learning_rate": 4.7499065372368536e-05,
      "loss": 2.5549,
      "step": 77600
    },
    {
      "epoch": 3.004988977839657,
      "grad_norm": 14.648524284362793,
      "learning_rate": 4.749584251846695e-05,
      "loss": 2.5969,
      "step": 77700
    },
    {
      "epoch": 3.0088564025215607,
      "grad_norm": 14.125096321105957,
      "learning_rate": 4.7492619664565366e-05,
      "loss": 2.6364,
      "step": 77800
    },
    {
      "epoch": 3.012723827203465,
      "grad_norm": 10.491825103759766,
      "learning_rate": 4.748939681066378e-05,
      "loss": 2.5448,
      "step": 77900
    },
    {
      "epoch": 3.0165912518853695,
      "grad_norm": 13.733137130737305,
      "learning_rate": 4.7486173956762195e-05,
      "loss": 2.631,
      "step": 78000
    },
    {
      "epoch": 3.020458676567274,
      "grad_norm": 10.405007362365723,
      "learning_rate": 4.748295110286061e-05,
      "loss": 2.5545,
      "step": 78100
    },
    {
      "epoch": 3.0243261012491782,
      "grad_norm": 11.63265609741211,
      "learning_rate": 4.747972824895902e-05,
      "loss": 2.5346,
      "step": 78200
    },
    {
      "epoch": 3.0281935259310826,
      "grad_norm": 10.04670524597168,
      "learning_rate": 4.747650539505743e-05,
      "loss": 2.5169,
      "step": 78300
    },
    {
      "epoch": 3.032060950612987,
      "grad_norm": 10.220405578613281,
      "learning_rate": 4.747328254115585e-05,
      "loss": 2.5437,
      "step": 78400
    },
    {
      "epoch": 3.0359283752948913,
      "grad_norm": 13.132667541503906,
      "learning_rate": 4.747005968725426e-05,
      "loss": 2.6502,
      "step": 78500
    },
    {
      "epoch": 3.0397957999767953,
      "grad_norm": 9.686088562011719,
      "learning_rate": 4.746683683335268e-05,
      "loss": 2.5353,
      "step": 78600
    },
    {
      "epoch": 3.0436632246586997,
      "grad_norm": 11.315986633300781,
      "learning_rate": 4.746361397945109e-05,
      "loss": 2.5598,
      "step": 78700
    },
    {
      "epoch": 3.047530649340604,
      "grad_norm": 10.424201965332031,
      "learning_rate": 4.74603911255495e-05,
      "loss": 2.4782,
      "step": 78800
    },
    {
      "epoch": 3.0513980740225084,
      "grad_norm": 13.446880340576172,
      "learning_rate": 4.7457168271647914e-05,
      "loss": 2.5117,
      "step": 78900
    },
    {
      "epoch": 3.0552654987044128,
      "grad_norm": 13.686201095581055,
      "learning_rate": 4.745394541774633e-05,
      "loss": 2.5882,
      "step": 79000
    },
    {
      "epoch": 3.059132923386317,
      "grad_norm": 13.278970718383789,
      "learning_rate": 4.7450722563844744e-05,
      "loss": 2.5807,
      "step": 79100
    },
    {
      "epoch": 3.0630003480682215,
      "grad_norm": 11.953218460083008,
      "learning_rate": 4.744749970994315e-05,
      "loss": 2.4852,
      "step": 79200
    },
    {
      "epoch": 3.066867772750126,
      "grad_norm": 10.75196647644043,
      "learning_rate": 4.7444276856041566e-05,
      "loss": 2.601,
      "step": 79300
    },
    {
      "epoch": 3.07073519743203,
      "grad_norm": 13.92008113861084,
      "learning_rate": 4.744105400213998e-05,
      "loss": 2.5437,
      "step": 79400
    },
    {
      "epoch": 3.074602622113934,
      "grad_norm": 12.43371295928955,
      "learning_rate": 4.7437831148238396e-05,
      "loss": 2.5724,
      "step": 79500
    },
    {
      "epoch": 3.0784700467958386,
      "grad_norm": 12.427750587463379,
      "learning_rate": 4.7434608294336804e-05,
      "loss": 2.5828,
      "step": 79600
    },
    {
      "epoch": 3.082337471477743,
      "grad_norm": 13.189301490783691,
      "learning_rate": 4.743138544043522e-05,
      "loss": 2.5693,
      "step": 79700
    },
    {
      "epoch": 3.0862048961596473,
      "grad_norm": 11.221919059753418,
      "learning_rate": 4.742816258653363e-05,
      "loss": 2.5593,
      "step": 79800
    },
    {
      "epoch": 3.0900723208415517,
      "grad_norm": 9.391475677490234,
      "learning_rate": 4.742493973263204e-05,
      "loss": 2.4892,
      "step": 79900
    },
    {
      "epoch": 3.093939745523456,
      "grad_norm": 10.377046585083008,
      "learning_rate": 4.7421716878730456e-05,
      "loss": 2.5026,
      "step": 80000
    },
    {
      "epoch": 3.0978071702053604,
      "grad_norm": 12.376091003417969,
      "learning_rate": 4.741849402482887e-05,
      "loss": 2.5215,
      "step": 80100
    },
    {
      "epoch": 3.1016745948872644,
      "grad_norm": 14.129363059997559,
      "learning_rate": 4.7415271170927285e-05,
      "loss": 2.5494,
      "step": 80200
    },
    {
      "epoch": 3.1055420195691688,
      "grad_norm": 14.622543334960938,
      "learning_rate": 4.741204831702569e-05,
      "loss": 2.5559,
      "step": 80300
    },
    {
      "epoch": 3.109409444251073,
      "grad_norm": 11.944047927856445,
      "learning_rate": 4.740882546312411e-05,
      "loss": 2.5417,
      "step": 80400
    },
    {
      "epoch": 3.1132768689329775,
      "grad_norm": 11.693864822387695,
      "learning_rate": 4.740560260922252e-05,
      "loss": 2.5794,
      "step": 80500
    },
    {
      "epoch": 3.117144293614882,
      "grad_norm": 12.172306060791016,
      "learning_rate": 4.740237975532094e-05,
      "loss": 2.4814,
      "step": 80600
    },
    {
      "epoch": 3.1210117182967863,
      "grad_norm": 11.721125602722168,
      "learning_rate": 4.7399156901419345e-05,
      "loss": 2.6276,
      "step": 80700
    },
    {
      "epoch": 3.1248791429786906,
      "grad_norm": 9.571009635925293,
      "learning_rate": 4.739593404751776e-05,
      "loss": 2.4596,
      "step": 80800
    },
    {
      "epoch": 3.128746567660595,
      "grad_norm": 11.311132431030273,
      "learning_rate": 4.7392711193616175e-05,
      "loss": 2.5465,
      "step": 80900
    },
    {
      "epoch": 3.132613992342499,
      "grad_norm": 12.318622589111328,
      "learning_rate": 4.738948833971459e-05,
      "loss": 2.5767,
      "step": 81000
    },
    {
      "epoch": 3.1364814170244033,
      "grad_norm": 11.993478775024414,
      "learning_rate": 4.7386265485813e-05,
      "loss": 2.5198,
      "step": 81100
    },
    {
      "epoch": 3.1403488417063077,
      "grad_norm": 12.301976203918457,
      "learning_rate": 4.738304263191141e-05,
      "loss": 2.5538,
      "step": 81200
    },
    {
      "epoch": 3.144216266388212,
      "grad_norm": 10.640873908996582,
      "learning_rate": 4.737981977800983e-05,
      "loss": 2.619,
      "step": 81300
    },
    {
      "epoch": 3.1480836910701164,
      "grad_norm": 10.954164505004883,
      "learning_rate": 4.737659692410824e-05,
      "loss": 2.4697,
      "step": 81400
    },
    {
      "epoch": 3.151951115752021,
      "grad_norm": 12.141908645629883,
      "learning_rate": 4.737337407020665e-05,
      "loss": 2.5312,
      "step": 81500
    },
    {
      "epoch": 3.155818540433925,
      "grad_norm": 11.628993034362793,
      "learning_rate": 4.7370151216305064e-05,
      "loss": 2.5564,
      "step": 81600
    },
    {
      "epoch": 3.1596859651158296,
      "grad_norm": 13.032670021057129,
      "learning_rate": 4.736692836240348e-05,
      "loss": 2.5758,
      "step": 81700
    },
    {
      "epoch": 3.1635533897977335,
      "grad_norm": 10.235810279846191,
      "learning_rate": 4.7363705508501894e-05,
      "loss": 2.5691,
      "step": 81800
    },
    {
      "epoch": 3.167420814479638,
      "grad_norm": 13.909031867980957,
      "learning_rate": 4.73604826546003e-05,
      "loss": 2.6046,
      "step": 81900
    },
    {
      "epoch": 3.1712882391615422,
      "grad_norm": 9.206428527832031,
      "learning_rate": 4.7357259800698716e-05,
      "loss": 2.5998,
      "step": 82000
    },
    {
      "epoch": 3.1751556638434466,
      "grad_norm": 11.876690864562988,
      "learning_rate": 4.735403694679713e-05,
      "loss": 2.5542,
      "step": 82100
    },
    {
      "epoch": 3.179023088525351,
      "grad_norm": 12.0932035446167,
      "learning_rate": 4.7350814092895546e-05,
      "loss": 2.5445,
      "step": 82200
    },
    {
      "epoch": 3.1828905132072554,
      "grad_norm": 11.579883575439453,
      "learning_rate": 4.7347591238993954e-05,
      "loss": 2.5008,
      "step": 82300
    },
    {
      "epoch": 3.1867579378891597,
      "grad_norm": 10.800305366516113,
      "learning_rate": 4.734436838509237e-05,
      "loss": 2.4405,
      "step": 82400
    },
    {
      "epoch": 3.190625362571064,
      "grad_norm": 8.485188484191895,
      "learning_rate": 4.734114553119078e-05,
      "loss": 2.5593,
      "step": 82500
    },
    {
      "epoch": 3.194492787252968,
      "grad_norm": 10.683402061462402,
      "learning_rate": 4.73379226772892e-05,
      "loss": 2.4876,
      "step": 82600
    },
    {
      "epoch": 3.1983602119348724,
      "grad_norm": 10.968940734863281,
      "learning_rate": 4.7334699823387606e-05,
      "loss": 2.5114,
      "step": 82700
    },
    {
      "epoch": 3.202227636616777,
      "grad_norm": 10.957895278930664,
      "learning_rate": 4.733147696948602e-05,
      "loss": 2.5685,
      "step": 82800
    },
    {
      "epoch": 3.206095061298681,
      "grad_norm": 12.925724983215332,
      "learning_rate": 4.7328254115584435e-05,
      "loss": 2.5804,
      "step": 82900
    },
    {
      "epoch": 3.2099624859805855,
      "grad_norm": 11.027142524719238,
      "learning_rate": 4.732503126168284e-05,
      "loss": 2.4618,
      "step": 83000
    },
    {
      "epoch": 3.21382991066249,
      "grad_norm": 11.892701148986816,
      "learning_rate": 4.732180840778126e-05,
      "loss": 2.5623,
      "step": 83100
    },
    {
      "epoch": 3.2176973353443943,
      "grad_norm": 12.931811332702637,
      "learning_rate": 4.731858555387967e-05,
      "loss": 2.5826,
      "step": 83200
    },
    {
      "epoch": 3.2215647600262987,
      "grad_norm": 15.847238540649414,
      "learning_rate": 4.731536269997809e-05,
      "loss": 2.5462,
      "step": 83300
    },
    {
      "epoch": 3.225432184708203,
      "grad_norm": 15.08557415008545,
      "learning_rate": 4.7312139846076495e-05,
      "loss": 2.5189,
      "step": 83400
    },
    {
      "epoch": 3.229299609390107,
      "grad_norm": 12.483834266662598,
      "learning_rate": 4.730891699217491e-05,
      "loss": 2.5248,
      "step": 83500
    },
    {
      "epoch": 3.2331670340720113,
      "grad_norm": 12.54214096069336,
      "learning_rate": 4.7305694138273325e-05,
      "loss": 2.4826,
      "step": 83600
    },
    {
      "epoch": 3.2370344587539157,
      "grad_norm": 11.619196891784668,
      "learning_rate": 4.730247128437174e-05,
      "loss": 2.4943,
      "step": 83700
    },
    {
      "epoch": 3.24090188343582,
      "grad_norm": 11.193713188171387,
      "learning_rate": 4.729924843047015e-05,
      "loss": 2.5629,
      "step": 83800
    },
    {
      "epoch": 3.2447693081177245,
      "grad_norm": 10.572447776794434,
      "learning_rate": 4.729602557656856e-05,
      "loss": 2.5695,
      "step": 83900
    },
    {
      "epoch": 3.248636732799629,
      "grad_norm": 15.296239852905273,
      "learning_rate": 4.729280272266698e-05,
      "loss": 2.5603,
      "step": 84000
    },
    {
      "epoch": 3.252504157481533,
      "grad_norm": 10.721091270446777,
      "learning_rate": 4.728957986876539e-05,
      "loss": 2.549,
      "step": 84100
    },
    {
      "epoch": 3.256371582163437,
      "grad_norm": 13.108878135681152,
      "learning_rate": 4.72863570148638e-05,
      "loss": 2.5283,
      "step": 84200
    },
    {
      "epoch": 3.2602390068453415,
      "grad_norm": 12.033175468444824,
      "learning_rate": 4.7283134160962214e-05,
      "loss": 2.5251,
      "step": 84300
    },
    {
      "epoch": 3.264106431527246,
      "grad_norm": 13.041966438293457,
      "learning_rate": 4.727991130706063e-05,
      "loss": 2.5277,
      "step": 84400
    },
    {
      "epoch": 3.2679738562091503,
      "grad_norm": 12.740574836730957,
      "learning_rate": 4.7276688453159044e-05,
      "loss": 2.5331,
      "step": 84500
    },
    {
      "epoch": 3.2718412808910546,
      "grad_norm": 12.23105239868164,
      "learning_rate": 4.727346559925746e-05,
      "loss": 2.5562,
      "step": 84600
    },
    {
      "epoch": 3.275708705572959,
      "grad_norm": 13.42733383178711,
      "learning_rate": 4.7270242745355866e-05,
      "loss": 2.4954,
      "step": 84700
    },
    {
      "epoch": 3.2795761302548634,
      "grad_norm": 13.497089385986328,
      "learning_rate": 4.726701989145428e-05,
      "loss": 2.5482,
      "step": 84800
    },
    {
      "epoch": 3.2834435549367678,
      "grad_norm": 13.359333038330078,
      "learning_rate": 4.7263797037552696e-05,
      "loss": 2.5243,
      "step": 84900
    },
    {
      "epoch": 3.287310979618672,
      "grad_norm": 10.380036354064941,
      "learning_rate": 4.726057418365111e-05,
      "loss": 2.4965,
      "step": 85000
    },
    {
      "epoch": 3.291178404300576,
      "grad_norm": 12.369914054870605,
      "learning_rate": 4.7257351329749525e-05,
      "loss": 2.5434,
      "step": 85100
    },
    {
      "epoch": 3.2950458289824804,
      "grad_norm": 12.835641860961914,
      "learning_rate": 4.725412847584793e-05,
      "loss": 2.4517,
      "step": 85200
    },
    {
      "epoch": 3.298913253664385,
      "grad_norm": 12.88876724243164,
      "learning_rate": 4.725090562194635e-05,
      "loss": 2.5356,
      "step": 85300
    },
    {
      "epoch": 3.302780678346289,
      "grad_norm": 11.678826332092285,
      "learning_rate": 4.724768276804476e-05,
      "loss": 2.468,
      "step": 85400
    },
    {
      "epoch": 3.3066481030281936,
      "grad_norm": 12.118416786193848,
      "learning_rate": 4.724445991414318e-05,
      "loss": 2.533,
      "step": 85500
    },
    {
      "epoch": 3.310515527710098,
      "grad_norm": 13.57384204864502,
      "learning_rate": 4.724123706024159e-05,
      "loss": 2.6559,
      "step": 85600
    },
    {
      "epoch": 3.3143829523920023,
      "grad_norm": 12.076716423034668,
      "learning_rate": 4.723801420634e-05,
      "loss": 2.621,
      "step": 85700
    },
    {
      "epoch": 3.3182503770739062,
      "grad_norm": 13.635428428649902,
      "learning_rate": 4.7234791352438415e-05,
      "loss": 2.6212,
      "step": 85800
    },
    {
      "epoch": 3.3221178017558106,
      "grad_norm": 16.034225463867188,
      "learning_rate": 4.723156849853683e-05,
      "loss": 2.5333,
      "step": 85900
    },
    {
      "epoch": 3.325985226437715,
      "grad_norm": 11.990154266357422,
      "learning_rate": 4.7228345644635244e-05,
      "loss": 2.4966,
      "step": 86000
    },
    {
      "epoch": 3.3298526511196194,
      "grad_norm": 11.69874095916748,
      "learning_rate": 4.722512279073365e-05,
      "loss": 2.5229,
      "step": 86100
    },
    {
      "epoch": 3.3337200758015237,
      "grad_norm": 10.759678840637207,
      "learning_rate": 4.722189993683207e-05,
      "loss": 2.4822,
      "step": 86200
    },
    {
      "epoch": 3.337587500483428,
      "grad_norm": 13.491080284118652,
      "learning_rate": 4.721867708293048e-05,
      "loss": 2.5521,
      "step": 86300
    },
    {
      "epoch": 3.3414549251653325,
      "grad_norm": 10.773896217346191,
      "learning_rate": 4.7215454229028896e-05,
      "loss": 2.4767,
      "step": 86400
    },
    {
      "epoch": 3.345322349847237,
      "grad_norm": 12.030342102050781,
      "learning_rate": 4.7212231375127304e-05,
      "loss": 2.6129,
      "step": 86500
    },
    {
      "epoch": 3.3491897745291412,
      "grad_norm": 12.945026397705078,
      "learning_rate": 4.720900852122572e-05,
      "loss": 2.4848,
      "step": 86600
    },
    {
      "epoch": 3.3530571992110456,
      "grad_norm": 11.259561538696289,
      "learning_rate": 4.7205785667324133e-05,
      "loss": 2.6044,
      "step": 86700
    },
    {
      "epoch": 3.3569246238929495,
      "grad_norm": 10.251480102539062,
      "learning_rate": 4.720256281342255e-05,
      "loss": 2.5247,
      "step": 86800
    },
    {
      "epoch": 3.360792048574854,
      "grad_norm": 14.318562507629395,
      "learning_rate": 4.7199339959520956e-05,
      "loss": 2.5094,
      "step": 86900
    },
    {
      "epoch": 3.3646594732567583,
      "grad_norm": 11.989217758178711,
      "learning_rate": 4.719611710561937e-05,
      "loss": 2.4218,
      "step": 87000
    },
    {
      "epoch": 3.3685268979386627,
      "grad_norm": 11.830093383789062,
      "learning_rate": 4.7192894251717786e-05,
      "loss": 2.4914,
      "step": 87100
    },
    {
      "epoch": 3.372394322620567,
      "grad_norm": 13.959319114685059,
      "learning_rate": 4.71896713978162e-05,
      "loss": 2.5346,
      "step": 87200
    },
    {
      "epoch": 3.3762617473024714,
      "grad_norm": 12.947421073913574,
      "learning_rate": 4.718644854391461e-05,
      "loss": 2.5522,
      "step": 87300
    },
    {
      "epoch": 3.380129171984376,
      "grad_norm": 12.774435997009277,
      "learning_rate": 4.718322569001302e-05,
      "loss": 2.5948,
      "step": 87400
    },
    {
      "epoch": 3.3839965966662797,
      "grad_norm": 14.040682792663574,
      "learning_rate": 4.718000283611144e-05,
      "loss": 2.5587,
      "step": 87500
    },
    {
      "epoch": 3.387864021348184,
      "grad_norm": 10.938764572143555,
      "learning_rate": 4.717677998220985e-05,
      "loss": 2.5974,
      "step": 87600
    },
    {
      "epoch": 3.3917314460300885,
      "grad_norm": 14.035453796386719,
      "learning_rate": 4.717355712830826e-05,
      "loss": 2.4967,
      "step": 87700
    },
    {
      "epoch": 3.395598870711993,
      "grad_norm": 16.034337997436523,
      "learning_rate": 4.7170334274406675e-05,
      "loss": 2.5537,
      "step": 87800
    },
    {
      "epoch": 3.399466295393897,
      "grad_norm": 12.012704849243164,
      "learning_rate": 4.716711142050509e-05,
      "loss": 2.5634,
      "step": 87900
    },
    {
      "epoch": 3.4033337200758016,
      "grad_norm": 14.963208198547363,
      "learning_rate": 4.7163888566603504e-05,
      "loss": 2.5046,
      "step": 88000
    },
    {
      "epoch": 3.407201144757706,
      "grad_norm": 12.939169883728027,
      "learning_rate": 4.716066571270191e-05,
      "loss": 2.5076,
      "step": 88100
    },
    {
      "epoch": 3.4110685694396103,
      "grad_norm": 14.149730682373047,
      "learning_rate": 4.715744285880033e-05,
      "loss": 2.5048,
      "step": 88200
    },
    {
      "epoch": 3.4149359941215147,
      "grad_norm": 15.346272468566895,
      "learning_rate": 4.715422000489874e-05,
      "loss": 2.5582,
      "step": 88300
    },
    {
      "epoch": 3.4188034188034186,
      "grad_norm": 10.883710861206055,
      "learning_rate": 4.7150997150997157e-05,
      "loss": 2.4748,
      "step": 88400
    },
    {
      "epoch": 3.422670843485323,
      "grad_norm": 10.292684555053711,
      "learning_rate": 4.7147774297095565e-05,
      "loss": 2.524,
      "step": 88500
    },
    {
      "epoch": 3.4265382681672274,
      "grad_norm": 11.884941101074219,
      "learning_rate": 4.714455144319398e-05,
      "loss": 2.4756,
      "step": 88600
    },
    {
      "epoch": 3.4304056928491318,
      "grad_norm": 10.663677215576172,
      "learning_rate": 4.7141328589292394e-05,
      "loss": 2.499,
      "step": 88700
    },
    {
      "epoch": 3.434273117531036,
      "grad_norm": 10.787541389465332,
      "learning_rate": 4.71381057353908e-05,
      "loss": 2.5451,
      "step": 88800
    },
    {
      "epoch": 3.4381405422129405,
      "grad_norm": 11.391544342041016,
      "learning_rate": 4.7134882881489217e-05,
      "loss": 2.4622,
      "step": 88900
    },
    {
      "epoch": 3.442007966894845,
      "grad_norm": 11.75012493133545,
      "learning_rate": 4.713166002758763e-05,
      "loss": 2.4613,
      "step": 89000
    },
    {
      "epoch": 3.445875391576749,
      "grad_norm": 12.353778839111328,
      "learning_rate": 4.7128437173686046e-05,
      "loss": 2.555,
      "step": 89100
    },
    {
      "epoch": 3.449742816258653,
      "grad_norm": 12.851856231689453,
      "learning_rate": 4.7125214319784454e-05,
      "loss": 2.4649,
      "step": 89200
    },
    {
      "epoch": 3.4536102409405576,
      "grad_norm": 12.363053321838379,
      "learning_rate": 4.712199146588287e-05,
      "loss": 2.524,
      "step": 89300
    },
    {
      "epoch": 3.457477665622462,
      "grad_norm": 11.510493278503418,
      "learning_rate": 4.7118768611981283e-05,
      "loss": 2.58,
      "step": 89400
    },
    {
      "epoch": 3.4613450903043663,
      "grad_norm": 10.605661392211914,
      "learning_rate": 4.71155457580797e-05,
      "loss": 2.5533,
      "step": 89500
    },
    {
      "epoch": 3.4652125149862707,
      "grad_norm": 12.303016662597656,
      "learning_rate": 4.7112322904178106e-05,
      "loss": 2.5013,
      "step": 89600
    },
    {
      "epoch": 3.469079939668175,
      "grad_norm": 12.682515144348145,
      "learning_rate": 4.710910005027652e-05,
      "loss": 2.5617,
      "step": 89700
    },
    {
      "epoch": 3.4729473643500794,
      "grad_norm": 10.708253860473633,
      "learning_rate": 4.7105877196374936e-05,
      "loss": 2.5146,
      "step": 89800
    },
    {
      "epoch": 3.476814789031984,
      "grad_norm": 8.645657539367676,
      "learning_rate": 4.710265434247335e-05,
      "loss": 2.4538,
      "step": 89900
    },
    {
      "epoch": 3.4806822137138878,
      "grad_norm": 12.650653839111328,
      "learning_rate": 4.709943148857176e-05,
      "loss": 2.5133,
      "step": 90000
    },
    {
      "epoch": 3.484549638395792,
      "grad_norm": 10.06093692779541,
      "learning_rate": 4.709620863467017e-05,
      "loss": 2.5088,
      "step": 90100
    },
    {
      "epoch": 3.4884170630776965,
      "grad_norm": 13.007017135620117,
      "learning_rate": 4.709298578076859e-05,
      "loss": 2.5033,
      "step": 90200
    },
    {
      "epoch": 3.492284487759601,
      "grad_norm": 10.70387077331543,
      "learning_rate": 4.7089762926867e-05,
      "loss": 2.5046,
      "step": 90300
    },
    {
      "epoch": 3.4961519124415052,
      "grad_norm": 13.02193546295166,
      "learning_rate": 4.708654007296541e-05,
      "loss": 2.5589,
      "step": 90400
    },
    {
      "epoch": 3.5000193371234096,
      "grad_norm": 12.75492000579834,
      "learning_rate": 4.7083317219063825e-05,
      "loss": 2.5268,
      "step": 90500
    },
    {
      "epoch": 3.503886761805314,
      "grad_norm": 12.122307777404785,
      "learning_rate": 4.708009436516224e-05,
      "loss": 2.4931,
      "step": 90600
    },
    {
      "epoch": 3.507754186487218,
      "grad_norm": 10.921128273010254,
      "learning_rate": 4.7076871511260654e-05,
      "loss": 2.5533,
      "step": 90700
    },
    {
      "epoch": 3.5116216111691223,
      "grad_norm": 13.157118797302246,
      "learning_rate": 4.707364865735906e-05,
      "loss": 2.6164,
      "step": 90800
    },
    {
      "epoch": 3.5154890358510267,
      "grad_norm": 11.018210411071777,
      "learning_rate": 4.707042580345748e-05,
      "loss": 2.4972,
      "step": 90900
    },
    {
      "epoch": 3.519356460532931,
      "grad_norm": 15.972672462463379,
      "learning_rate": 4.706720294955589e-05,
      "loss": 2.4873,
      "step": 91000
    },
    {
      "epoch": 3.5232238852148354,
      "grad_norm": 11.702130317687988,
      "learning_rate": 4.7063980095654307e-05,
      "loss": 2.4611,
      "step": 91100
    },
    {
      "epoch": 3.52709130989674,
      "grad_norm": 10.561568260192871,
      "learning_rate": 4.7060757241752714e-05,
      "loss": 2.5237,
      "step": 91200
    },
    {
      "epoch": 3.530958734578644,
      "grad_norm": 11.153508186340332,
      "learning_rate": 4.705753438785113e-05,
      "loss": 2.4779,
      "step": 91300
    },
    {
      "epoch": 3.5348261592605486,
      "grad_norm": 15.56576919555664,
      "learning_rate": 4.7054311533949544e-05,
      "loss": 2.5581,
      "step": 91400
    },
    {
      "epoch": 3.538693583942453,
      "grad_norm": 15.21600341796875,
      "learning_rate": 4.705108868004796e-05,
      "loss": 2.4503,
      "step": 91500
    },
    {
      "epoch": 3.5425610086243573,
      "grad_norm": 11.932960510253906,
      "learning_rate": 4.704786582614637e-05,
      "loss": 2.5082,
      "step": 91600
    },
    {
      "epoch": 3.5464284333062612,
      "grad_norm": 14.342846870422363,
      "learning_rate": 4.704464297224478e-05,
      "loss": 2.4777,
      "step": 91700
    },
    {
      "epoch": 3.5502958579881656,
      "grad_norm": 14.777765274047852,
      "learning_rate": 4.7041420118343196e-05,
      "loss": 2.4236,
      "step": 91800
    },
    {
      "epoch": 3.55416328267007,
      "grad_norm": 11.24562931060791,
      "learning_rate": 4.703819726444161e-05,
      "loss": 2.5107,
      "step": 91900
    },
    {
      "epoch": 3.5580307073519744,
      "grad_norm": 11.8204345703125,
      "learning_rate": 4.7034974410540025e-05,
      "loss": 2.5116,
      "step": 92000
    },
    {
      "epoch": 3.5618981320338787,
      "grad_norm": 10.771537780761719,
      "learning_rate": 4.703175155663844e-05,
      "loss": 2.5697,
      "step": 92100
    },
    {
      "epoch": 3.565765556715783,
      "grad_norm": 14.816344261169434,
      "learning_rate": 4.7028528702736855e-05,
      "loss": 2.5193,
      "step": 92200
    },
    {
      "epoch": 3.569632981397687,
      "grad_norm": 19.46860122680664,
      "learning_rate": 4.702530584883526e-05,
      "loss": 2.4982,
      "step": 92300
    },
    {
      "epoch": 3.5735004060795914,
      "grad_norm": 8.79142951965332,
      "learning_rate": 4.702208299493368e-05,
      "loss": 2.5092,
      "step": 92400
    },
    {
      "epoch": 3.577367830761496,
      "grad_norm": 17.510421752929688,
      "learning_rate": 4.701886014103209e-05,
      "loss": 2.487,
      "step": 92500
    },
    {
      "epoch": 3.5812352554434,
      "grad_norm": 12.237537384033203,
      "learning_rate": 4.701563728713051e-05,
      "loss": 2.5344,
      "step": 92600
    },
    {
      "epoch": 3.5851026801253045,
      "grad_norm": 15.293196678161621,
      "learning_rate": 4.7012414433228915e-05,
      "loss": 2.4907,
      "step": 92700
    },
    {
      "epoch": 3.588970104807209,
      "grad_norm": 13.116704940795898,
      "learning_rate": 4.700919157932733e-05,
      "loss": 2.5326,
      "step": 92800
    },
    {
      "epoch": 3.5928375294891133,
      "grad_norm": 13.0936918258667,
      "learning_rate": 4.7005968725425744e-05,
      "loss": 2.4955,
      "step": 92900
    },
    {
      "epoch": 3.5967049541710177,
      "grad_norm": 10.888387680053711,
      "learning_rate": 4.700274587152416e-05,
      "loss": 2.5028,
      "step": 93000
    },
    {
      "epoch": 3.600572378852922,
      "grad_norm": 12.975375175476074,
      "learning_rate": 4.699952301762257e-05,
      "loss": 2.4439,
      "step": 93100
    },
    {
      "epoch": 3.6044398035348264,
      "grad_norm": 11.633943557739258,
      "learning_rate": 4.699630016372098e-05,
      "loss": 2.4767,
      "step": 93200
    },
    {
      "epoch": 3.6083072282167303,
      "grad_norm": 9.770415306091309,
      "learning_rate": 4.6993077309819396e-05,
      "loss": 2.4606,
      "step": 93300
    },
    {
      "epoch": 3.6121746528986347,
      "grad_norm": 11.249017715454102,
      "learning_rate": 4.698985445591781e-05,
      "loss": 2.5941,
      "step": 93400
    },
    {
      "epoch": 3.616042077580539,
      "grad_norm": 13.191757202148438,
      "learning_rate": 4.698663160201622e-05,
      "loss": 2.4403,
      "step": 93500
    },
    {
      "epoch": 3.6199095022624435,
      "grad_norm": 10.60651969909668,
      "learning_rate": 4.6983408748114634e-05,
      "loss": 2.5546,
      "step": 93600
    },
    {
      "epoch": 3.623776926944348,
      "grad_norm": 12.307530403137207,
      "learning_rate": 4.698018589421305e-05,
      "loss": 2.4765,
      "step": 93700
    },
    {
      "epoch": 3.627644351626252,
      "grad_norm": 11.775721549987793,
      "learning_rate": 4.697696304031146e-05,
      "loss": 2.4585,
      "step": 93800
    },
    {
      "epoch": 3.631511776308156,
      "grad_norm": 11.649757385253906,
      "learning_rate": 4.697374018640987e-05,
      "loss": 2.5033,
      "step": 93900
    },
    {
      "epoch": 3.6353792009900605,
      "grad_norm": 13.2531156539917,
      "learning_rate": 4.6970517332508286e-05,
      "loss": 2.4845,
      "step": 94000
    },
    {
      "epoch": 3.639246625671965,
      "grad_norm": 11.26230239868164,
      "learning_rate": 4.69672944786067e-05,
      "loss": 2.4682,
      "step": 94100
    },
    {
      "epoch": 3.6431140503538693,
      "grad_norm": 12.196091651916504,
      "learning_rate": 4.6964071624705115e-05,
      "loss": 2.4427,
      "step": 94200
    },
    {
      "epoch": 3.6469814750357736,
      "grad_norm": 12.737295150756836,
      "learning_rate": 4.696084877080352e-05,
      "loss": 2.4797,
      "step": 94300
    },
    {
      "epoch": 3.650848899717678,
      "grad_norm": 9.805950164794922,
      "learning_rate": 4.695762591690194e-05,
      "loss": 2.4021,
      "step": 94400
    },
    {
      "epoch": 3.6547163243995824,
      "grad_norm": 13.50197982788086,
      "learning_rate": 4.695440306300035e-05,
      "loss": 2.4818,
      "step": 94500
    },
    {
      "epoch": 3.6585837490814868,
      "grad_norm": 11.089695930480957,
      "learning_rate": 4.695118020909876e-05,
      "loss": 2.4797,
      "step": 94600
    },
    {
      "epoch": 3.662451173763391,
      "grad_norm": 11.79837703704834,
      "learning_rate": 4.6947957355197175e-05,
      "loss": 2.5001,
      "step": 94700
    },
    {
      "epoch": 3.6663185984452955,
      "grad_norm": 11.058259010314941,
      "learning_rate": 4.694473450129559e-05,
      "loss": 2.5101,
      "step": 94800
    },
    {
      "epoch": 3.6701860231271994,
      "grad_norm": 11.342411994934082,
      "learning_rate": 4.6941511647394005e-05,
      "loss": 2.5056,
      "step": 94900
    },
    {
      "epoch": 3.674053447809104,
      "grad_norm": 11.293076515197754,
      "learning_rate": 4.693828879349241e-05,
      "loss": 2.4986,
      "step": 95000
    },
    {
      "epoch": 3.677920872491008,
      "grad_norm": 14.329245567321777,
      "learning_rate": 4.693506593959083e-05,
      "loss": 2.4711,
      "step": 95100
    },
    {
      "epoch": 3.6817882971729126,
      "grad_norm": 10.015339851379395,
      "learning_rate": 4.693184308568924e-05,
      "loss": 2.558,
      "step": 95200
    },
    {
      "epoch": 3.685655721854817,
      "grad_norm": 12.097323417663574,
      "learning_rate": 4.692862023178766e-05,
      "loss": 2.5395,
      "step": 95300
    },
    {
      "epoch": 3.6895231465367213,
      "grad_norm": 11.595786094665527,
      "learning_rate": 4.6925397377886065e-05,
      "loss": 2.4583,
      "step": 95400
    },
    {
      "epoch": 3.6933905712186252,
      "grad_norm": 11.852326393127441,
      "learning_rate": 4.692217452398448e-05,
      "loss": 2.4872,
      "step": 95500
    },
    {
      "epoch": 3.6972579959005296,
      "grad_norm": 17.564640045166016,
      "learning_rate": 4.6918951670082894e-05,
      "loss": 2.5458,
      "step": 95600
    },
    {
      "epoch": 3.701125420582434,
      "grad_norm": 11.084656715393066,
      "learning_rate": 4.691572881618131e-05,
      "loss": 2.48,
      "step": 95700
    },
    {
      "epoch": 3.7049928452643384,
      "grad_norm": 8.17834186553955,
      "learning_rate": 4.691250596227972e-05,
      "loss": 2.4717,
      "step": 95800
    },
    {
      "epoch": 3.7088602699462427,
      "grad_norm": 11.948101997375488,
      "learning_rate": 4.690928310837813e-05,
      "loss": 2.5171,
      "step": 95900
    },
    {
      "epoch": 3.712727694628147,
      "grad_norm": 14.055557250976562,
      "learning_rate": 4.6906060254476546e-05,
      "loss": 2.4553,
      "step": 96000
    },
    {
      "epoch": 3.7165951193100515,
      "grad_norm": 10.758610725402832,
      "learning_rate": 4.690283740057496e-05,
      "loss": 2.5546,
      "step": 96100
    },
    {
      "epoch": 3.720462543991956,
      "grad_norm": 13.96855354309082,
      "learning_rate": 4.689961454667337e-05,
      "loss": 2.4398,
      "step": 96200
    },
    {
      "epoch": 3.7243299686738602,
      "grad_norm": 12.478446006774902,
      "learning_rate": 4.6896391692771784e-05,
      "loss": 2.3856,
      "step": 96300
    },
    {
      "epoch": 3.7281973933557646,
      "grad_norm": 11.0077543258667,
      "learning_rate": 4.68931688388702e-05,
      "loss": 2.4623,
      "step": 96400
    },
    {
      "epoch": 3.732064818037669,
      "grad_norm": 16.550752639770508,
      "learning_rate": 4.688994598496861e-05,
      "loss": 2.513,
      "step": 96500
    },
    {
      "epoch": 3.735932242719573,
      "grad_norm": 13.608443260192871,
      "learning_rate": 4.688672313106702e-05,
      "loss": 2.4735,
      "step": 96600
    },
    {
      "epoch": 3.7397996674014773,
      "grad_norm": 11.640388488769531,
      "learning_rate": 4.6883500277165436e-05,
      "loss": 2.4733,
      "step": 96700
    },
    {
      "epoch": 3.7436670920833817,
      "grad_norm": 11.52885913848877,
      "learning_rate": 4.688027742326385e-05,
      "loss": 2.5144,
      "step": 96800
    },
    {
      "epoch": 3.747534516765286,
      "grad_norm": 12.576054573059082,
      "learning_rate": 4.6877054569362265e-05,
      "loss": 2.4503,
      "step": 96900
    },
    {
      "epoch": 3.7514019414471904,
      "grad_norm": 11.937113761901855,
      "learning_rate": 4.687383171546067e-05,
      "loss": 2.5731,
      "step": 97000
    },
    {
      "epoch": 3.755269366129095,
      "grad_norm": 12.328171730041504,
      "learning_rate": 4.687060886155909e-05,
      "loss": 2.4705,
      "step": 97100
    },
    {
      "epoch": 3.7591367908109987,
      "grad_norm": 10.631997108459473,
      "learning_rate": 4.68673860076575e-05,
      "loss": 2.4954,
      "step": 97200
    },
    {
      "epoch": 3.763004215492903,
      "grad_norm": 9.654637336730957,
      "learning_rate": 4.686416315375592e-05,
      "loss": 2.4948,
      "step": 97300
    },
    {
      "epoch": 3.7668716401748075,
      "grad_norm": 13.920283317565918,
      "learning_rate": 4.6860940299854325e-05,
      "loss": 2.469,
      "step": 97400
    },
    {
      "epoch": 3.770739064856712,
      "grad_norm": 11.272799491882324,
      "learning_rate": 4.685771744595274e-05,
      "loss": 2.429,
      "step": 97500
    },
    {
      "epoch": 3.774606489538616,
      "grad_norm": 13.166943550109863,
      "learning_rate": 4.6854494592051155e-05,
      "loss": 2.4861,
      "step": 97600
    },
    {
      "epoch": 3.7784739142205206,
      "grad_norm": 15.15464973449707,
      "learning_rate": 4.685127173814956e-05,
      "loss": 2.548,
      "step": 97700
    },
    {
      "epoch": 3.782341338902425,
      "grad_norm": 9.64553165435791,
      "learning_rate": 4.684804888424798e-05,
      "loss": 2.4754,
      "step": 97800
    },
    {
      "epoch": 3.7862087635843293,
      "grad_norm": 11.657025337219238,
      "learning_rate": 4.684482603034639e-05,
      "loss": 2.416,
      "step": 97900
    },
    {
      "epoch": 3.7900761882662337,
      "grad_norm": 12.375982284545898,
      "learning_rate": 4.684160317644481e-05,
      "loss": 2.5077,
      "step": 98000
    },
    {
      "epoch": 3.793943612948138,
      "grad_norm": 12.11016845703125,
      "learning_rate": 4.683838032254322e-05,
      "loss": 2.5797,
      "step": 98100
    },
    {
      "epoch": 3.797811037630042,
      "grad_norm": 17.616260528564453,
      "learning_rate": 4.683515746864163e-05,
      "loss": 2.5269,
      "step": 98200
    },
    {
      "epoch": 3.8016784623119464,
      "grad_norm": 8.140048027038574,
      "learning_rate": 4.6831934614740044e-05,
      "loss": 2.5432,
      "step": 98300
    },
    {
      "epoch": 3.8055458869938508,
      "grad_norm": 11.055126190185547,
      "learning_rate": 4.682871176083846e-05,
      "loss": 2.4585,
      "step": 98400
    },
    {
      "epoch": 3.809413311675755,
      "grad_norm": 10.811854362487793,
      "learning_rate": 4.6825488906936874e-05,
      "loss": 2.6511,
      "step": 98500
    },
    {
      "epoch": 3.8132807363576595,
      "grad_norm": 10.988290786743164,
      "learning_rate": 4.682226605303529e-05,
      "loss": 2.53,
      "step": 98600
    },
    {
      "epoch": 3.817148161039564,
      "grad_norm": 13.794465065002441,
      "learning_rate": 4.6819043199133696e-05,
      "loss": 2.5049,
      "step": 98700
    },
    {
      "epoch": 3.821015585721468,
      "grad_norm": 10.657241821289062,
      "learning_rate": 4.681582034523211e-05,
      "loss": 2.5535,
      "step": 98800
    },
    {
      "epoch": 3.824883010403372,
      "grad_norm": 10.285845756530762,
      "learning_rate": 4.6812597491330526e-05,
      "loss": 2.413,
      "step": 98900
    },
    {
      "epoch": 3.8287504350852766,
      "grad_norm": 12.255842208862305,
      "learning_rate": 4.680937463742894e-05,
      "loss": 2.4833,
      "step": 99000
    },
    {
      "epoch": 3.832617859767181,
      "grad_norm": 16.443696975708008,
      "learning_rate": 4.6806151783527355e-05,
      "loss": 2.4875,
      "step": 99100
    },
    {
      "epoch": 3.8364852844490853,
      "grad_norm": 11.997361183166504,
      "learning_rate": 4.680292892962577e-05,
      "loss": 2.3904,
      "step": 99200
    },
    {
      "epoch": 3.8403527091309897,
      "grad_norm": 11.013986587524414,
      "learning_rate": 4.679970607572418e-05,
      "loss": 2.5248,
      "step": 99300
    },
    {
      "epoch": 3.844220133812894,
      "grad_norm": 11.868213653564453,
      "learning_rate": 4.679648322182259e-05,
      "loss": 2.487,
      "step": 99400
    },
    {
      "epoch": 3.8480875584947984,
      "grad_norm": 11.81403923034668,
      "learning_rate": 4.679326036792101e-05,
      "loss": 2.496,
      "step": 99500
    },
    {
      "epoch": 3.851954983176703,
      "grad_norm": 14.402790069580078,
      "learning_rate": 4.679003751401942e-05,
      "loss": 2.5147,
      "step": 99600
    },
    {
      "epoch": 3.855822407858607,
      "grad_norm": 13.35611343383789,
      "learning_rate": 4.678681466011783e-05,
      "loss": 2.4474,
      "step": 99700
    },
    {
      "epoch": 3.859689832540511,
      "grad_norm": 16.27056121826172,
      "learning_rate": 4.6783591806216245e-05,
      "loss": 2.4316,
      "step": 99800
    },
    {
      "epoch": 3.8635572572224155,
      "grad_norm": 12.231403350830078,
      "learning_rate": 4.678036895231466e-05,
      "loss": 2.5203,
      "step": 99900
    },
    {
      "epoch": 3.86742468190432,
      "grad_norm": 13.4855375289917,
      "learning_rate": 4.6777146098413074e-05,
      "loss": 2.4756,
      "step": 100000
    },
    {
      "epoch": 3.8712921065862242,
      "grad_norm": 12.598390579223633,
      "learning_rate": 4.677392324451148e-05,
      "loss": 2.5032,
      "step": 100100
    },
    {
      "epoch": 3.8751595312681286,
      "grad_norm": 10.624122619628906,
      "learning_rate": 4.67707003906099e-05,
      "loss": 2.3878,
      "step": 100200
    },
    {
      "epoch": 3.879026955950033,
      "grad_norm": 12.794489860534668,
      "learning_rate": 4.676747753670831e-05,
      "loss": 2.4423,
      "step": 100300
    },
    {
      "epoch": 3.882894380631937,
      "grad_norm": 12.695862770080566,
      "learning_rate": 4.676425468280672e-05,
      "loss": 2.4968,
      "step": 100400
    },
    {
      "epoch": 3.8867618053138413,
      "grad_norm": 9.464838027954102,
      "learning_rate": 4.6761031828905134e-05,
      "loss": 2.4203,
      "step": 100500
    },
    {
      "epoch": 3.8906292299957457,
      "grad_norm": 13.549962997436523,
      "learning_rate": 4.675780897500355e-05,
      "loss": 2.4971,
      "step": 100600
    },
    {
      "epoch": 3.89449665467765,
      "grad_norm": 14.581608772277832,
      "learning_rate": 4.6754586121101964e-05,
      "loss": 2.4713,
      "step": 100700
    },
    {
      "epoch": 3.8983640793595544,
      "grad_norm": 12.461280822753906,
      "learning_rate": 4.675136326720037e-05,
      "loss": 2.4659,
      "step": 100800
    },
    {
      "epoch": 3.902231504041459,
      "grad_norm": 12.213085174560547,
      "learning_rate": 4.6748140413298786e-05,
      "loss": 2.5059,
      "step": 100900
    },
    {
      "epoch": 3.906098928723363,
      "grad_norm": 11.966547012329102,
      "learning_rate": 4.67449175593972e-05,
      "loss": 2.4618,
      "step": 101000
    },
    {
      "epoch": 3.9099663534052675,
      "grad_norm": 11.300568580627441,
      "learning_rate": 4.6741694705495616e-05,
      "loss": 2.4917,
      "step": 101100
    },
    {
      "epoch": 3.913833778087172,
      "grad_norm": 11.05372428894043,
      "learning_rate": 4.6738471851594024e-05,
      "loss": 2.4477,
      "step": 101200
    },
    {
      "epoch": 3.9177012027690763,
      "grad_norm": 10.575620651245117,
      "learning_rate": 4.673524899769244e-05,
      "loss": 2.4499,
      "step": 101300
    },
    {
      "epoch": 3.9215686274509802,
      "grad_norm": 13.648691177368164,
      "learning_rate": 4.673202614379085e-05,
      "loss": 2.469,
      "step": 101400
    },
    {
      "epoch": 3.9254360521328846,
      "grad_norm": 13.328571319580078,
      "learning_rate": 4.672880328988927e-05,
      "loss": 2.4384,
      "step": 101500
    },
    {
      "epoch": 3.929303476814789,
      "grad_norm": 10.497634887695312,
      "learning_rate": 4.6725580435987676e-05,
      "loss": 2.3532,
      "step": 101600
    },
    {
      "epoch": 3.9331709014966934,
      "grad_norm": 11.458486557006836,
      "learning_rate": 4.672235758208609e-05,
      "loss": 2.4336,
      "step": 101700
    },
    {
      "epoch": 3.9370383261785977,
      "grad_norm": 14.403416633605957,
      "learning_rate": 4.6719134728184505e-05,
      "loss": 2.3829,
      "step": 101800
    },
    {
      "epoch": 3.940905750860502,
      "grad_norm": 16.459877014160156,
      "learning_rate": 4.671591187428292e-05,
      "loss": 2.5199,
      "step": 101900
    },
    {
      "epoch": 3.9447731755424065,
      "grad_norm": 12.318603515625,
      "learning_rate": 4.671268902038133e-05,
      "loss": 2.4691,
      "step": 102000
    },
    {
      "epoch": 3.9486406002243104,
      "grad_norm": 11.3916015625,
      "learning_rate": 4.670946616647974e-05,
      "loss": 2.4397,
      "step": 102100
    },
    {
      "epoch": 3.952508024906215,
      "grad_norm": 12.723882675170898,
      "learning_rate": 4.670624331257816e-05,
      "loss": 2.4256,
      "step": 102200
    },
    {
      "epoch": 3.956375449588119,
      "grad_norm": 12.7306489944458,
      "learning_rate": 4.670302045867657e-05,
      "loss": 2.5152,
      "step": 102300
    },
    {
      "epoch": 3.9602428742700235,
      "grad_norm": 10.714763641357422,
      "learning_rate": 4.669979760477498e-05,
      "loss": 2.4272,
      "step": 102400
    },
    {
      "epoch": 3.964110298951928,
      "grad_norm": 12.715547561645508,
      "learning_rate": 4.6696574750873395e-05,
      "loss": 2.5076,
      "step": 102500
    },
    {
      "epoch": 3.9679777236338323,
      "grad_norm": 11.27103042602539,
      "learning_rate": 4.669335189697181e-05,
      "loss": 2.4679,
      "step": 102600
    },
    {
      "epoch": 3.9718451483157367,
      "grad_norm": 13.100988388061523,
      "learning_rate": 4.6690129043070224e-05,
      "loss": 2.4854,
      "step": 102700
    },
    {
      "epoch": 3.975712572997641,
      "grad_norm": 10.559436798095703,
      "learning_rate": 4.668690618916863e-05,
      "loss": 2.438,
      "step": 102800
    },
    {
      "epoch": 3.9795799976795454,
      "grad_norm": 20.387514114379883,
      "learning_rate": 4.668368333526705e-05,
      "loss": 2.5179,
      "step": 102900
    },
    {
      "epoch": 3.9834474223614498,
      "grad_norm": 12.236836433410645,
      "learning_rate": 4.668046048136546e-05,
      "loss": 2.5086,
      "step": 103000
    },
    {
      "epoch": 3.9873148470433537,
      "grad_norm": 9.608447074890137,
      "learning_rate": 4.6677237627463876e-05,
      "loss": 2.4797,
      "step": 103100
    },
    {
      "epoch": 3.991182271725258,
      "grad_norm": 12.058476448059082,
      "learning_rate": 4.6674014773562284e-05,
      "loss": 2.4845,
      "step": 103200
    },
    {
      "epoch": 3.9950496964071625,
      "grad_norm": 14.500772476196289,
      "learning_rate": 4.66707919196607e-05,
      "loss": 2.3865,
      "step": 103300
    },
    {
      "epoch": 3.998917121089067,
      "grad_norm": 13.8099365234375,
      "learning_rate": 4.6667569065759114e-05,
      "loss": 2.4813,
      "step": 103400
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.320185899734497,
      "eval_runtime": 6.1196,
      "eval_samples_per_second": 222.401,
      "eval_steps_per_second": 222.401,
      "step": 103428
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.3188793659210205,
      "eval_runtime": 113.8913,
      "eval_samples_per_second": 227.032,
      "eval_steps_per_second": 227.032,
      "step": 103428
    },
    {
      "epoch": 4.002784545770971,
      "grad_norm": 9.778238296508789,
      "learning_rate": 4.666434621185752e-05,
      "loss": 2.4683,
      "step": 103500
    },
    {
      "epoch": 4.006651970452875,
      "grad_norm": 10.7366943359375,
      "learning_rate": 4.6661123357955936e-05,
      "loss": 2.435,
      "step": 103600
    },
    {
      "epoch": 4.0105193951347795,
      "grad_norm": 12.80128288269043,
      "learning_rate": 4.665790050405435e-05,
      "loss": 2.4594,
      "step": 103700
    },
    {
      "epoch": 4.014386819816684,
      "grad_norm": 10.993231773376465,
      "learning_rate": 4.6654677650152766e-05,
      "loss": 2.4496,
      "step": 103800
    },
    {
      "epoch": 4.018254244498588,
      "grad_norm": 11.658554077148438,
      "learning_rate": 4.6651454796251174e-05,
      "loss": 2.4485,
      "step": 103900
    },
    {
      "epoch": 4.022121669180493,
      "grad_norm": 10.525105476379395,
      "learning_rate": 4.664823194234959e-05,
      "loss": 2.4735,
      "step": 104000
    },
    {
      "epoch": 4.025989093862397,
      "grad_norm": 10.901859283447266,
      "learning_rate": 4.6645009088448e-05,
      "loss": 2.4596,
      "step": 104100
    },
    {
      "epoch": 4.029856518544301,
      "grad_norm": 10.574847221374512,
      "learning_rate": 4.664178623454642e-05,
      "loss": 2.4885,
      "step": 104200
    },
    {
      "epoch": 4.033723943226206,
      "grad_norm": 11.107726097106934,
      "learning_rate": 4.6638563380644826e-05,
      "loss": 2.4853,
      "step": 104300
    },
    {
      "epoch": 4.03759136790811,
      "grad_norm": 12.926016807556152,
      "learning_rate": 4.663534052674324e-05,
      "loss": 2.4814,
      "step": 104400
    },
    {
      "epoch": 4.0414587925900145,
      "grad_norm": 10.634356498718262,
      "learning_rate": 4.6632117672841655e-05,
      "loss": 2.4926,
      "step": 104500
    },
    {
      "epoch": 4.045326217271919,
      "grad_norm": 18.11391258239746,
      "learning_rate": 4.662889481894007e-05,
      "loss": 2.4085,
      "step": 104600
    },
    {
      "epoch": 4.049193641953823,
      "grad_norm": 12.41186809539795,
      "learning_rate": 4.662567196503848e-05,
      "loss": 2.3868,
      "step": 104700
    },
    {
      "epoch": 4.053061066635728,
      "grad_norm": 13.85928726196289,
      "learning_rate": 4.662244911113689e-05,
      "loss": 2.5329,
      "step": 104800
    },
    {
      "epoch": 4.056928491317632,
      "grad_norm": 16.304580688476562,
      "learning_rate": 4.661922625723531e-05,
      "loss": 2.4419,
      "step": 104900
    },
    {
      "epoch": 4.0607959159995355,
      "grad_norm": 10.984184265136719,
      "learning_rate": 4.661600340333372e-05,
      "loss": 2.4472,
      "step": 105000
    },
    {
      "epoch": 4.06466334068144,
      "grad_norm": 12.277637481689453,
      "learning_rate": 4.661278054943214e-05,
      "loss": 2.5072,
      "step": 105100
    },
    {
      "epoch": 4.068530765363344,
      "grad_norm": 12.395037651062012,
      "learning_rate": 4.6609557695530545e-05,
      "loss": 2.5134,
      "step": 105200
    },
    {
      "epoch": 4.072398190045249,
      "grad_norm": 13.40284252166748,
      "learning_rate": 4.660633484162896e-05,
      "loss": 2.399,
      "step": 105300
    },
    {
      "epoch": 4.076265614727153,
      "grad_norm": 11.148368835449219,
      "learning_rate": 4.6603111987727374e-05,
      "loss": 2.5346,
      "step": 105400
    },
    {
      "epoch": 4.080133039409057,
      "grad_norm": 11.19209098815918,
      "learning_rate": 4.659988913382579e-05,
      "loss": 2.4229,
      "step": 105500
    },
    {
      "epoch": 4.084000464090962,
      "grad_norm": 15.133734703063965,
      "learning_rate": 4.6596666279924204e-05,
      "loss": 2.3898,
      "step": 105600
    },
    {
      "epoch": 4.087867888772866,
      "grad_norm": 11.900274276733398,
      "learning_rate": 4.659344342602262e-05,
      "loss": 2.4579,
      "step": 105700
    },
    {
      "epoch": 4.0917353134547705,
      "grad_norm": 23.753339767456055,
      "learning_rate": 4.6590220572121026e-05,
      "loss": 2.4628,
      "step": 105800
    },
    {
      "epoch": 4.095602738136675,
      "grad_norm": 11.312374114990234,
      "learning_rate": 4.658699771821944e-05,
      "loss": 2.4886,
      "step": 105900
    },
    {
      "epoch": 4.099470162818579,
      "grad_norm": 7.319274425506592,
      "learning_rate": 4.6583774864317856e-05,
      "loss": 2.4428,
      "step": 106000
    },
    {
      "epoch": 4.103337587500484,
      "grad_norm": 11.435356140136719,
      "learning_rate": 4.658055201041627e-05,
      "loss": 2.4383,
      "step": 106100
    },
    {
      "epoch": 4.107205012182388,
      "grad_norm": 12.355931282043457,
      "learning_rate": 4.6577329156514685e-05,
      "loss": 2.4552,
      "step": 106200
    },
    {
      "epoch": 4.111072436864292,
      "grad_norm": 13.597196578979492,
      "learning_rate": 4.657410630261309e-05,
      "loss": 2.3997,
      "step": 106300
    },
    {
      "epoch": 4.114939861546197,
      "grad_norm": 13.227070808410645,
      "learning_rate": 4.657088344871151e-05,
      "loss": 2.4737,
      "step": 106400
    },
    {
      "epoch": 4.118807286228101,
      "grad_norm": 11.699151992797852,
      "learning_rate": 4.656766059480992e-05,
      "loss": 2.4143,
      "step": 106500
    },
    {
      "epoch": 4.122674710910005,
      "grad_norm": 11.81743049621582,
      "learning_rate": 4.656443774090833e-05,
      "loss": 2.4137,
      "step": 106600
    },
    {
      "epoch": 4.126542135591909,
      "grad_norm": 11.676580429077148,
      "learning_rate": 4.6561214887006745e-05,
      "loss": 2.4098,
      "step": 106700
    },
    {
      "epoch": 4.130409560273813,
      "grad_norm": 14.353161811828613,
      "learning_rate": 4.655799203310516e-05,
      "loss": 2.404,
      "step": 106800
    },
    {
      "epoch": 4.134276984955718,
      "grad_norm": 11.257654190063477,
      "learning_rate": 4.6554769179203575e-05,
      "loss": 2.5182,
      "step": 106900
    },
    {
      "epoch": 4.138144409637622,
      "grad_norm": 10.449263572692871,
      "learning_rate": 4.655154632530198e-05,
      "loss": 2.3903,
      "step": 107000
    },
    {
      "epoch": 4.1420118343195265,
      "grad_norm": 15.700368881225586,
      "learning_rate": 4.65483234714004e-05,
      "loss": 2.3378,
      "step": 107100
    },
    {
      "epoch": 4.145879259001431,
      "grad_norm": 8.919455528259277,
      "learning_rate": 4.654510061749881e-05,
      "loss": 2.4928,
      "step": 107200
    },
    {
      "epoch": 4.149746683683335,
      "grad_norm": 11.943994522094727,
      "learning_rate": 4.654187776359723e-05,
      "loss": 2.5131,
      "step": 107300
    },
    {
      "epoch": 4.15361410836524,
      "grad_norm": 11.613554954528809,
      "learning_rate": 4.6538654909695635e-05,
      "loss": 2.4457,
      "step": 107400
    },
    {
      "epoch": 4.157481533047144,
      "grad_norm": 11.190828323364258,
      "learning_rate": 4.653543205579405e-05,
      "loss": 2.4356,
      "step": 107500
    },
    {
      "epoch": 4.161348957729048,
      "grad_norm": 11.50714111328125,
      "learning_rate": 4.6532209201892464e-05,
      "loss": 2.394,
      "step": 107600
    },
    {
      "epoch": 4.165216382410953,
      "grad_norm": 10.63207721710205,
      "learning_rate": 4.652898634799088e-05,
      "loss": 2.4189,
      "step": 107700
    },
    {
      "epoch": 4.169083807092857,
      "grad_norm": 12.89371395111084,
      "learning_rate": 4.652576349408929e-05,
      "loss": 2.4221,
      "step": 107800
    },
    {
      "epoch": 4.1729512317747615,
      "grad_norm": 10.24938678741455,
      "learning_rate": 4.65225406401877e-05,
      "loss": 2.5126,
      "step": 107900
    },
    {
      "epoch": 4.176818656456666,
      "grad_norm": 12.418906211853027,
      "learning_rate": 4.6519317786286116e-05,
      "loss": 2.5059,
      "step": 108000
    },
    {
      "epoch": 4.18068608113857,
      "grad_norm": 13.6846923828125,
      "learning_rate": 4.651609493238453e-05,
      "loss": 2.392,
      "step": 108100
    },
    {
      "epoch": 4.184553505820475,
      "grad_norm": 12.811842918395996,
      "learning_rate": 4.651287207848294e-05,
      "loss": 2.3891,
      "step": 108200
    },
    {
      "epoch": 4.188420930502378,
      "grad_norm": 12.528547286987305,
      "learning_rate": 4.6509649224581353e-05,
      "loss": 2.4368,
      "step": 108300
    },
    {
      "epoch": 4.1922883551842824,
      "grad_norm": 13.185543060302734,
      "learning_rate": 4.650642637067977e-05,
      "loss": 2.4263,
      "step": 108400
    },
    {
      "epoch": 4.196155779866187,
      "grad_norm": 10.629875183105469,
      "learning_rate": 4.650320351677818e-05,
      "loss": 2.4706,
      "step": 108500
    },
    {
      "epoch": 4.200023204548091,
      "grad_norm": 13.922647476196289,
      "learning_rate": 4.649998066287659e-05,
      "loss": 2.4221,
      "step": 108600
    },
    {
      "epoch": 4.203890629229996,
      "grad_norm": 13.081168174743652,
      "learning_rate": 4.6496757808975006e-05,
      "loss": 2.392,
      "step": 108700
    },
    {
      "epoch": 4.2077580539119,
      "grad_norm": 11.037858009338379,
      "learning_rate": 4.649353495507342e-05,
      "loss": 2.3799,
      "step": 108800
    },
    {
      "epoch": 4.211625478593804,
      "grad_norm": 13.634016990661621,
      "learning_rate": 4.6490312101171835e-05,
      "loss": 2.4802,
      "step": 108900
    },
    {
      "epoch": 4.215492903275709,
      "grad_norm": 13.321236610412598,
      "learning_rate": 4.648708924727024e-05,
      "loss": 2.4594,
      "step": 109000
    },
    {
      "epoch": 4.219360327957613,
      "grad_norm": 11.700807571411133,
      "learning_rate": 4.648386639336866e-05,
      "loss": 2.4404,
      "step": 109100
    },
    {
      "epoch": 4.223227752639517,
      "grad_norm": 11.655987739562988,
      "learning_rate": 4.648064353946707e-05,
      "loss": 2.4265,
      "step": 109200
    },
    {
      "epoch": 4.227095177321422,
      "grad_norm": 13.091019630432129,
      "learning_rate": 4.647742068556548e-05,
      "loss": 2.4916,
      "step": 109300
    },
    {
      "epoch": 4.230962602003326,
      "grad_norm": 11.270697593688965,
      "learning_rate": 4.6474197831663895e-05,
      "loss": 2.3783,
      "step": 109400
    },
    {
      "epoch": 4.234830026685231,
      "grad_norm": 10.383971214294434,
      "learning_rate": 4.647097497776231e-05,
      "loss": 2.4575,
      "step": 109500
    },
    {
      "epoch": 4.238697451367135,
      "grad_norm": 11.360053062438965,
      "learning_rate": 4.6467752123860724e-05,
      "loss": 2.4753,
      "step": 109600
    },
    {
      "epoch": 4.242564876049039,
      "grad_norm": 12.72242259979248,
      "learning_rate": 4.646452926995913e-05,
      "loss": 2.3545,
      "step": 109700
    },
    {
      "epoch": 4.246432300730944,
      "grad_norm": 12.316884994506836,
      "learning_rate": 4.646130641605755e-05,
      "loss": 2.4354,
      "step": 109800
    },
    {
      "epoch": 4.250299725412847,
      "grad_norm": 11.015069007873535,
      "learning_rate": 4.645808356215596e-05,
      "loss": 2.3985,
      "step": 109900
    },
    {
      "epoch": 4.2541671500947515,
      "grad_norm": 13.075430870056152,
      "learning_rate": 4.6454860708254377e-05,
      "loss": 2.3743,
      "step": 110000
    },
    {
      "epoch": 4.258034574776656,
      "grad_norm": 11.892017364501953,
      "learning_rate": 4.6451637854352785e-05,
      "loss": 2.5039,
      "step": 110100
    },
    {
      "epoch": 4.26190199945856,
      "grad_norm": 13.17197322845459,
      "learning_rate": 4.64484150004512e-05,
      "loss": 2.5063,
      "step": 110200
    },
    {
      "epoch": 4.265769424140465,
      "grad_norm": 10.688498497009277,
      "learning_rate": 4.6445192146549614e-05,
      "loss": 2.4441,
      "step": 110300
    },
    {
      "epoch": 4.269636848822369,
      "grad_norm": 10.923091888427734,
      "learning_rate": 4.644196929264803e-05,
      "loss": 2.439,
      "step": 110400
    },
    {
      "epoch": 4.273504273504273,
      "grad_norm": 11.924664497375488,
      "learning_rate": 4.643874643874644e-05,
      "loss": 2.3647,
      "step": 110500
    },
    {
      "epoch": 4.277371698186178,
      "grad_norm": 13.03878116607666,
      "learning_rate": 4.643552358484485e-05,
      "loss": 2.4744,
      "step": 110600
    },
    {
      "epoch": 4.281239122868082,
      "grad_norm": 13.064327239990234,
      "learning_rate": 4.6432300730943266e-05,
      "loss": 2.5145,
      "step": 110700
    },
    {
      "epoch": 4.2851065475499865,
      "grad_norm": 11.394965171813965,
      "learning_rate": 4.642907787704168e-05,
      "loss": 2.457,
      "step": 110800
    },
    {
      "epoch": 4.288973972231891,
      "grad_norm": 11.598432540893555,
      "learning_rate": 4.642585502314009e-05,
      "loss": 2.4424,
      "step": 110900
    },
    {
      "epoch": 4.292841396913795,
      "grad_norm": 11.139634132385254,
      "learning_rate": 4.6422632169238503e-05,
      "loss": 2.4488,
      "step": 111000
    },
    {
      "epoch": 4.2967088215957,
      "grad_norm": 13.171236991882324,
      "learning_rate": 4.641940931533692e-05,
      "loss": 2.4248,
      "step": 111100
    },
    {
      "epoch": 4.300576246277604,
      "grad_norm": 14.080887794494629,
      "learning_rate": 4.641618646143533e-05,
      "loss": 2.4436,
      "step": 111200
    },
    {
      "epoch": 4.304443670959508,
      "grad_norm": 11.868490219116211,
      "learning_rate": 4.641296360753374e-05,
      "loss": 2.5203,
      "step": 111300
    },
    {
      "epoch": 4.308311095641413,
      "grad_norm": 14.021581649780273,
      "learning_rate": 4.6409740753632156e-05,
      "loss": 2.424,
      "step": 111400
    },
    {
      "epoch": 4.312178520323316,
      "grad_norm": 9.431987762451172,
      "learning_rate": 4.640651789973057e-05,
      "loss": 2.4155,
      "step": 111500
    },
    {
      "epoch": 4.316045945005221,
      "grad_norm": 12.786911010742188,
      "learning_rate": 4.6403295045828985e-05,
      "loss": 2.4658,
      "step": 111600
    },
    {
      "epoch": 4.319913369687125,
      "grad_norm": 10.019388198852539,
      "learning_rate": 4.640007219192739e-05,
      "loss": 2.4098,
      "step": 111700
    },
    {
      "epoch": 4.323780794369029,
      "grad_norm": 12.173807144165039,
      "learning_rate": 4.639684933802581e-05,
      "loss": 2.5042,
      "step": 111800
    },
    {
      "epoch": 4.327648219050934,
      "grad_norm": 13.145914077758789,
      "learning_rate": 4.639362648412422e-05,
      "loss": 2.3321,
      "step": 111900
    },
    {
      "epoch": 4.331515643732838,
      "grad_norm": 9.516462326049805,
      "learning_rate": 4.639040363022264e-05,
      "loss": 2.3851,
      "step": 112000
    },
    {
      "epoch": 4.3353830684147425,
      "grad_norm": 13.256109237670898,
      "learning_rate": 4.638718077632105e-05,
      "loss": 2.3634,
      "step": 112100
    },
    {
      "epoch": 4.339250493096647,
      "grad_norm": 12.707517623901367,
      "learning_rate": 4.6383957922419467e-05,
      "loss": 2.4328,
      "step": 112200
    },
    {
      "epoch": 4.343117917778551,
      "grad_norm": 11.908676147460938,
      "learning_rate": 4.6380735068517874e-05,
      "loss": 2.359,
      "step": 112300
    },
    {
      "epoch": 4.346985342460456,
      "grad_norm": 10.535346031188965,
      "learning_rate": 4.637751221461629e-05,
      "loss": 2.4146,
      "step": 112400
    },
    {
      "epoch": 4.35085276714236,
      "grad_norm": 12.475851058959961,
      "learning_rate": 4.6374289360714704e-05,
      "loss": 2.4093,
      "step": 112500
    },
    {
      "epoch": 4.354720191824264,
      "grad_norm": 10.923019409179688,
      "learning_rate": 4.637106650681312e-05,
      "loss": 2.3574,
      "step": 112600
    },
    {
      "epoch": 4.358587616506169,
      "grad_norm": 10.836874961853027,
      "learning_rate": 4.636784365291153e-05,
      "loss": 2.4918,
      "step": 112700
    },
    {
      "epoch": 4.362455041188073,
      "grad_norm": 12.564202308654785,
      "learning_rate": 4.636462079900994e-05,
      "loss": 2.474,
      "step": 112800
    },
    {
      "epoch": 4.3663224658699775,
      "grad_norm": 13.357836723327637,
      "learning_rate": 4.6361397945108356e-05,
      "loss": 2.3911,
      "step": 112900
    },
    {
      "epoch": 4.370189890551882,
      "grad_norm": 13.108006477355957,
      "learning_rate": 4.635817509120677e-05,
      "loss": 2.4608,
      "step": 113000
    },
    {
      "epoch": 4.374057315233786,
      "grad_norm": 12.23215103149414,
      "learning_rate": 4.6354952237305185e-05,
      "loss": 2.3634,
      "step": 113100
    },
    {
      "epoch": 4.37792473991569,
      "grad_norm": 12.300082206726074,
      "learning_rate": 4.635172938340359e-05,
      "loss": 2.4171,
      "step": 113200
    },
    {
      "epoch": 4.381792164597594,
      "grad_norm": 11.4401216506958,
      "learning_rate": 4.634850652950201e-05,
      "loss": 2.3985,
      "step": 113300
    },
    {
      "epoch": 4.3856595892794985,
      "grad_norm": 11.788360595703125,
      "learning_rate": 4.634528367560042e-05,
      "loss": 2.444,
      "step": 113400
    },
    {
      "epoch": 4.389527013961403,
      "grad_norm": 11.3988676071167,
      "learning_rate": 4.634206082169884e-05,
      "loss": 2.4479,
      "step": 113500
    },
    {
      "epoch": 4.393394438643307,
      "grad_norm": 13.699382781982422,
      "learning_rate": 4.6338837967797245e-05,
      "loss": 2.4073,
      "step": 113600
    },
    {
      "epoch": 4.397261863325212,
      "grad_norm": 14.701923370361328,
      "learning_rate": 4.633561511389566e-05,
      "loss": 2.4338,
      "step": 113700
    },
    {
      "epoch": 4.401129288007116,
      "grad_norm": 13.398966789245605,
      "learning_rate": 4.6332392259994075e-05,
      "loss": 2.455,
      "step": 113800
    },
    {
      "epoch": 4.40499671268902,
      "grad_norm": 11.225510597229004,
      "learning_rate": 4.632916940609249e-05,
      "loss": 2.3753,
      "step": 113900
    },
    {
      "epoch": 4.408864137370925,
      "grad_norm": 10.786487579345703,
      "learning_rate": 4.63259465521909e-05,
      "loss": 2.3745,
      "step": 114000
    },
    {
      "epoch": 4.412731562052829,
      "grad_norm": 11.3790864944458,
      "learning_rate": 4.632272369828931e-05,
      "loss": 2.4222,
      "step": 114100
    },
    {
      "epoch": 4.4165989867347335,
      "grad_norm": 13.67520809173584,
      "learning_rate": 4.631950084438773e-05,
      "loss": 2.4675,
      "step": 114200
    },
    {
      "epoch": 4.420466411416638,
      "grad_norm": 10.944571495056152,
      "learning_rate": 4.631627799048614e-05,
      "loss": 2.504,
      "step": 114300
    },
    {
      "epoch": 4.424333836098542,
      "grad_norm": 14.053335189819336,
      "learning_rate": 4.631305513658455e-05,
      "loss": 2.4734,
      "step": 114400
    },
    {
      "epoch": 4.428201260780447,
      "grad_norm": 10.782526969909668,
      "learning_rate": 4.6309832282682964e-05,
      "loss": 2.4184,
      "step": 114500
    },
    {
      "epoch": 4.432068685462351,
      "grad_norm": 10.759801864624023,
      "learning_rate": 4.630660942878138e-05,
      "loss": 2.4694,
      "step": 114600
    },
    {
      "epoch": 4.4359361101442545,
      "grad_norm": 13.842698097229004,
      "learning_rate": 4.6303386574879794e-05,
      "loss": 2.344,
      "step": 114700
    },
    {
      "epoch": 4.439803534826159,
      "grad_norm": 12.591568946838379,
      "learning_rate": 4.63001637209782e-05,
      "loss": 2.3726,
      "step": 114800
    },
    {
      "epoch": 4.443670959508063,
      "grad_norm": 10.081822395324707,
      "learning_rate": 4.6296940867076616e-05,
      "loss": 2.4316,
      "step": 114900
    },
    {
      "epoch": 4.447538384189968,
      "grad_norm": 13.368429183959961,
      "learning_rate": 4.629371801317503e-05,
      "loss": 2.3872,
      "step": 115000
    },
    {
      "epoch": 4.451405808871872,
      "grad_norm": 14.725486755371094,
      "learning_rate": 4.629049515927344e-05,
      "loss": 2.3767,
      "step": 115100
    },
    {
      "epoch": 4.455273233553776,
      "grad_norm": 11.387945175170898,
      "learning_rate": 4.6287272305371854e-05,
      "loss": 2.364,
      "step": 115200
    },
    {
      "epoch": 4.459140658235681,
      "grad_norm": 16.061229705810547,
      "learning_rate": 4.628404945147027e-05,
      "loss": 2.3768,
      "step": 115300
    },
    {
      "epoch": 4.463008082917585,
      "grad_norm": 12.40393352508545,
      "learning_rate": 4.628082659756868e-05,
      "loss": 2.3692,
      "step": 115400
    },
    {
      "epoch": 4.4668755075994895,
      "grad_norm": 14.301135063171387,
      "learning_rate": 4.627760374366709e-05,
      "loss": 2.4401,
      "step": 115500
    },
    {
      "epoch": 4.470742932281394,
      "grad_norm": 11.464287757873535,
      "learning_rate": 4.6274380889765506e-05,
      "loss": 2.3912,
      "step": 115600
    },
    {
      "epoch": 4.474610356963298,
      "grad_norm": 13.436341285705566,
      "learning_rate": 4.627115803586392e-05,
      "loss": 2.3592,
      "step": 115700
    },
    {
      "epoch": 4.478477781645203,
      "grad_norm": 13.924737930297852,
      "learning_rate": 4.6267935181962335e-05,
      "loss": 2.4733,
      "step": 115800
    },
    {
      "epoch": 4.482345206327107,
      "grad_norm": 12.244709014892578,
      "learning_rate": 4.626471232806074e-05,
      "loss": 2.3787,
      "step": 115900
    },
    {
      "epoch": 4.486212631009011,
      "grad_norm": 8.935629844665527,
      "learning_rate": 4.626148947415916e-05,
      "loss": 2.4424,
      "step": 116000
    },
    {
      "epoch": 4.490080055690916,
      "grad_norm": 11.26055908203125,
      "learning_rate": 4.625826662025757e-05,
      "loss": 2.4142,
      "step": 116100
    },
    {
      "epoch": 4.49394748037282,
      "grad_norm": 10.227204322814941,
      "learning_rate": 4.625504376635599e-05,
      "loss": 2.3963,
      "step": 116200
    },
    {
      "epoch": 4.4978149050547245,
      "grad_norm": 12.494834899902344,
      "learning_rate": 4.6251820912454395e-05,
      "loss": 2.418,
      "step": 116300
    },
    {
      "epoch": 4.501682329736628,
      "grad_norm": 10.888087272644043,
      "learning_rate": 4.624859805855281e-05,
      "loss": 2.3638,
      "step": 116400
    },
    {
      "epoch": 4.505549754418532,
      "grad_norm": 11.372318267822266,
      "learning_rate": 4.6245375204651225e-05,
      "loss": 2.4166,
      "step": 116500
    },
    {
      "epoch": 4.509417179100437,
      "grad_norm": 11.028681755065918,
      "learning_rate": 4.624215235074964e-05,
      "loss": 2.4626,
      "step": 116600
    },
    {
      "epoch": 4.513284603782341,
      "grad_norm": 14.405777931213379,
      "learning_rate": 4.623892949684805e-05,
      "loss": 2.3896,
      "step": 116700
    },
    {
      "epoch": 4.5171520284642455,
      "grad_norm": 13.507003784179688,
      "learning_rate": 4.623570664294646e-05,
      "loss": 2.4912,
      "step": 116800
    },
    {
      "epoch": 4.52101945314615,
      "grad_norm": 11.763433456420898,
      "learning_rate": 4.623248378904488e-05,
      "loss": 2.371,
      "step": 116900
    },
    {
      "epoch": 4.524886877828054,
      "grad_norm": 15.882091522216797,
      "learning_rate": 4.622926093514329e-05,
      "loss": 2.5101,
      "step": 117000
    },
    {
      "epoch": 4.528754302509959,
      "grad_norm": 14.996663093566895,
      "learning_rate": 4.62260380812417e-05,
      "loss": 2.4216,
      "step": 117100
    },
    {
      "epoch": 4.532621727191863,
      "grad_norm": 11.891451835632324,
      "learning_rate": 4.6222815227340114e-05,
      "loss": 2.3966,
      "step": 117200
    },
    {
      "epoch": 4.536489151873767,
      "grad_norm": 11.03209400177002,
      "learning_rate": 4.621959237343853e-05,
      "loss": 2.3325,
      "step": 117300
    },
    {
      "epoch": 4.540356576555672,
      "grad_norm": 12.696417808532715,
      "learning_rate": 4.6216369519536944e-05,
      "loss": 2.3577,
      "step": 117400
    },
    {
      "epoch": 4.544224001237576,
      "grad_norm": 10.928132057189941,
      "learning_rate": 4.621314666563535e-05,
      "loss": 2.3985,
      "step": 117500
    },
    {
      "epoch": 4.5480914259194805,
      "grad_norm": 14.60015869140625,
      "learning_rate": 4.6209923811733766e-05,
      "loss": 2.4966,
      "step": 117600
    },
    {
      "epoch": 4.551958850601385,
      "grad_norm": 10.06252384185791,
      "learning_rate": 4.620670095783218e-05,
      "loss": 2.4194,
      "step": 117700
    },
    {
      "epoch": 4.555826275283289,
      "grad_norm": 13.056982040405273,
      "learning_rate": 4.6203478103930596e-05,
      "loss": 2.4452,
      "step": 117800
    },
    {
      "epoch": 4.559693699965193,
      "grad_norm": 12.25284194946289,
      "learning_rate": 4.6200255250029004e-05,
      "loss": 2.4506,
      "step": 117900
    },
    {
      "epoch": 4.563561124647098,
      "grad_norm": 10.496893882751465,
      "learning_rate": 4.619703239612742e-05,
      "loss": 2.3883,
      "step": 118000
    },
    {
      "epoch": 4.567428549329001,
      "grad_norm": 11.390835762023926,
      "learning_rate": 4.619380954222583e-05,
      "loss": 2.4192,
      "step": 118100
    },
    {
      "epoch": 4.571295974010906,
      "grad_norm": 12.465291976928711,
      "learning_rate": 4.619058668832424e-05,
      "loss": 2.5036,
      "step": 118200
    },
    {
      "epoch": 4.57516339869281,
      "grad_norm": 12.643993377685547,
      "learning_rate": 4.6187363834422656e-05,
      "loss": 2.4304,
      "step": 118300
    },
    {
      "epoch": 4.579030823374715,
      "grad_norm": 13.53988265991211,
      "learning_rate": 4.618414098052107e-05,
      "loss": 2.3286,
      "step": 118400
    },
    {
      "epoch": 4.582898248056619,
      "grad_norm": 13.221341133117676,
      "learning_rate": 4.6180918126619485e-05,
      "loss": 2.3855,
      "step": 118500
    },
    {
      "epoch": 4.586765672738523,
      "grad_norm": 12.763028144836426,
      "learning_rate": 4.61776952727179e-05,
      "loss": 2.3715,
      "step": 118600
    },
    {
      "epoch": 4.590633097420428,
      "grad_norm": 11.437664985656738,
      "learning_rate": 4.617447241881631e-05,
      "loss": 2.4515,
      "step": 118700
    },
    {
      "epoch": 4.594500522102332,
      "grad_norm": 12.725462913513184,
      "learning_rate": 4.617124956491472e-05,
      "loss": 2.3817,
      "step": 118800
    },
    {
      "epoch": 4.598367946784236,
      "grad_norm": 13.713122367858887,
      "learning_rate": 4.616802671101314e-05,
      "loss": 2.4327,
      "step": 118900
    },
    {
      "epoch": 4.602235371466141,
      "grad_norm": 13.224522590637207,
      "learning_rate": 4.616480385711155e-05,
      "loss": 2.4787,
      "step": 119000
    },
    {
      "epoch": 4.606102796148045,
      "grad_norm": 10.558768272399902,
      "learning_rate": 4.616158100320997e-05,
      "loss": 2.4356,
      "step": 119100
    },
    {
      "epoch": 4.60997022082995,
      "grad_norm": 11.487655639648438,
      "learning_rate": 4.615835814930838e-05,
      "loss": 2.3513,
      "step": 119200
    },
    {
      "epoch": 4.613837645511854,
      "grad_norm": 12.40176010131836,
      "learning_rate": 4.615513529540679e-05,
      "loss": 2.4123,
      "step": 119300
    },
    {
      "epoch": 4.617705070193758,
      "grad_norm": 11.964447975158691,
      "learning_rate": 4.6151912441505204e-05,
      "loss": 2.4051,
      "step": 119400
    },
    {
      "epoch": 4.621572494875663,
      "grad_norm": 11.640883445739746,
      "learning_rate": 4.614868958760362e-05,
      "loss": 2.3538,
      "step": 119500
    },
    {
      "epoch": 4.625439919557566,
      "grad_norm": 15.079926490783691,
      "learning_rate": 4.6145466733702034e-05,
      "loss": 2.4396,
      "step": 119600
    },
    {
      "epoch": 4.629307344239471,
      "grad_norm": 12.54332447052002,
      "learning_rate": 4.614224387980045e-05,
      "loss": 2.4316,
      "step": 119700
    },
    {
      "epoch": 4.633174768921375,
      "grad_norm": 9.79453182220459,
      "learning_rate": 4.6139021025898856e-05,
      "loss": 2.524,
      "step": 119800
    },
    {
      "epoch": 4.637042193603279,
      "grad_norm": 11.653656005859375,
      "learning_rate": 4.613579817199727e-05,
      "loss": 2.4433,
      "step": 119900
    },
    {
      "epoch": 4.640909618285184,
      "grad_norm": 14.710819244384766,
      "learning_rate": 4.6132575318095686e-05,
      "loss": 2.43,
      "step": 120000
    },
    {
      "epoch": 4.644777042967088,
      "grad_norm": 10.939803123474121,
      "learning_rate": 4.61293524641941e-05,
      "loss": 2.4059,
      "step": 120100
    },
    {
      "epoch": 4.648644467648992,
      "grad_norm": 12.826651573181152,
      "learning_rate": 4.612612961029251e-05,
      "loss": 2.4455,
      "step": 120200
    },
    {
      "epoch": 4.652511892330897,
      "grad_norm": 12.49959945678711,
      "learning_rate": 4.612290675639092e-05,
      "loss": 2.456,
      "step": 120300
    },
    {
      "epoch": 4.656379317012801,
      "grad_norm": 12.544316291809082,
      "learning_rate": 4.611968390248934e-05,
      "loss": 2.5104,
      "step": 120400
    },
    {
      "epoch": 4.6602467416947055,
      "grad_norm": 13.28805923461914,
      "learning_rate": 4.611646104858775e-05,
      "loss": 2.4327,
      "step": 120500
    },
    {
      "epoch": 4.66411416637661,
      "grad_norm": 11.335453987121582,
      "learning_rate": 4.611323819468616e-05,
      "loss": 2.4597,
      "step": 120600
    },
    {
      "epoch": 4.667981591058514,
      "grad_norm": 11.58237075805664,
      "learning_rate": 4.6110015340784575e-05,
      "loss": 2.3686,
      "step": 120700
    },
    {
      "epoch": 4.671849015740419,
      "grad_norm": 11.617168426513672,
      "learning_rate": 4.610679248688299e-05,
      "loss": 2.3114,
      "step": 120800
    },
    {
      "epoch": 4.675716440422323,
      "grad_norm": 10.602211952209473,
      "learning_rate": 4.6103569632981405e-05,
      "loss": 2.436,
      "step": 120900
    },
    {
      "epoch": 4.679583865104227,
      "grad_norm": 13.518969535827637,
      "learning_rate": 4.610034677907981e-05,
      "loss": 2.3686,
      "step": 121000
    },
    {
      "epoch": 4.683451289786132,
      "grad_norm": 11.041003227233887,
      "learning_rate": 4.609712392517823e-05,
      "loss": 2.3882,
      "step": 121100
    },
    {
      "epoch": 4.687318714468036,
      "grad_norm": 11.209014892578125,
      "learning_rate": 4.609390107127664e-05,
      "loss": 2.3335,
      "step": 121200
    },
    {
      "epoch": 4.69118613914994,
      "grad_norm": 14.358569145202637,
      "learning_rate": 4.609067821737505e-05,
      "loss": 2.4841,
      "step": 121300
    },
    {
      "epoch": 4.695053563831844,
      "grad_norm": 14.652589797973633,
      "learning_rate": 4.6087455363473465e-05,
      "loss": 2.3389,
      "step": 121400
    },
    {
      "epoch": 4.698920988513748,
      "grad_norm": 14.923538208007812,
      "learning_rate": 4.608423250957188e-05,
      "loss": 2.3686,
      "step": 121500
    },
    {
      "epoch": 4.702788413195653,
      "grad_norm": 12.938556671142578,
      "learning_rate": 4.6081009655670294e-05,
      "loss": 2.5167,
      "step": 121600
    },
    {
      "epoch": 4.706655837877557,
      "grad_norm": 9.334694862365723,
      "learning_rate": 4.60777868017687e-05,
      "loss": 2.4591,
      "step": 121700
    },
    {
      "epoch": 4.7105232625594615,
      "grad_norm": 13.869953155517578,
      "learning_rate": 4.607456394786712e-05,
      "loss": 2.3672,
      "step": 121800
    },
    {
      "epoch": 4.714390687241366,
      "grad_norm": 9.684259414672852,
      "learning_rate": 4.607134109396553e-05,
      "loss": 2.485,
      "step": 121900
    },
    {
      "epoch": 4.71825811192327,
      "grad_norm": 12.625104904174805,
      "learning_rate": 4.6068118240063946e-05,
      "loss": 2.3944,
      "step": 122000
    },
    {
      "epoch": 4.722125536605175,
      "grad_norm": 12.590402603149414,
      "learning_rate": 4.6064895386162354e-05,
      "loss": 2.3465,
      "step": 122100
    },
    {
      "epoch": 4.725992961287079,
      "grad_norm": 10.158049583435059,
      "learning_rate": 4.606167253226077e-05,
      "loss": 2.3855,
      "step": 122200
    },
    {
      "epoch": 4.729860385968983,
      "grad_norm": 14.98021125793457,
      "learning_rate": 4.6058449678359184e-05,
      "loss": 2.4433,
      "step": 122300
    },
    {
      "epoch": 4.733727810650888,
      "grad_norm": 11.853074073791504,
      "learning_rate": 4.60552268244576e-05,
      "loss": 2.3377,
      "step": 122400
    },
    {
      "epoch": 4.737595235332792,
      "grad_norm": 13.746198654174805,
      "learning_rate": 4.6052003970556006e-05,
      "loss": 2.3299,
      "step": 122500
    },
    {
      "epoch": 4.7414626600146965,
      "grad_norm": 10.823535919189453,
      "learning_rate": 4.604878111665442e-05,
      "loss": 2.3583,
      "step": 122600
    },
    {
      "epoch": 4.745330084696601,
      "grad_norm": 10.775053024291992,
      "learning_rate": 4.6045558262752836e-05,
      "loss": 2.3738,
      "step": 122700
    },
    {
      "epoch": 4.749197509378504,
      "grad_norm": 11.116486549377441,
      "learning_rate": 4.604233540885125e-05,
      "loss": 2.4338,
      "step": 122800
    },
    {
      "epoch": 4.75306493406041,
      "grad_norm": 11.156011581420898,
      "learning_rate": 4.603911255494966e-05,
      "loss": 2.4085,
      "step": 122900
    },
    {
      "epoch": 4.756932358742313,
      "grad_norm": 11.033803939819336,
      "learning_rate": 4.603588970104807e-05,
      "loss": 2.3378,
      "step": 123000
    },
    {
      "epoch": 4.7607997834242175,
      "grad_norm": 16.966873168945312,
      "learning_rate": 4.603266684714649e-05,
      "loss": 2.4305,
      "step": 123100
    },
    {
      "epoch": 4.764667208106122,
      "grad_norm": 13.003959655761719,
      "learning_rate": 4.60294439932449e-05,
      "loss": 2.411,
      "step": 123200
    },
    {
      "epoch": 4.768534632788026,
      "grad_norm": 9.74387264251709,
      "learning_rate": 4.602622113934331e-05,
      "loss": 2.475,
      "step": 123300
    },
    {
      "epoch": 4.772402057469931,
      "grad_norm": 10.481627464294434,
      "learning_rate": 4.6022998285441725e-05,
      "loss": 2.3565,
      "step": 123400
    },
    {
      "epoch": 4.776269482151835,
      "grad_norm": 9.407973289489746,
      "learning_rate": 4.601977543154014e-05,
      "loss": 2.4222,
      "step": 123500
    },
    {
      "epoch": 4.780136906833739,
      "grad_norm": 11.105690956115723,
      "learning_rate": 4.6016552577638555e-05,
      "loss": 2.3938,
      "step": 123600
    },
    {
      "epoch": 4.784004331515644,
      "grad_norm": 11.446649551391602,
      "learning_rate": 4.601332972373696e-05,
      "loss": 2.378,
      "step": 123700
    },
    {
      "epoch": 4.787871756197548,
      "grad_norm": 13.75274658203125,
      "learning_rate": 4.601010686983538e-05,
      "loss": 2.401,
      "step": 123800
    },
    {
      "epoch": 4.7917391808794525,
      "grad_norm": 16.834178924560547,
      "learning_rate": 4.600688401593379e-05,
      "loss": 2.3732,
      "step": 123900
    },
    {
      "epoch": 4.795606605561357,
      "grad_norm": 14.374767303466797,
      "learning_rate": 4.60036611620322e-05,
      "loss": 2.4153,
      "step": 124000
    },
    {
      "epoch": 4.799474030243261,
      "grad_norm": 12.714994430541992,
      "learning_rate": 4.6000438308130615e-05,
      "loss": 2.3841,
      "step": 124100
    },
    {
      "epoch": 4.803341454925166,
      "grad_norm": 13.297703742980957,
      "learning_rate": 4.599721545422903e-05,
      "loss": 2.3706,
      "step": 124200
    },
    {
      "epoch": 4.80720887960707,
      "grad_norm": 11.100730895996094,
      "learning_rate": 4.5993992600327444e-05,
      "loss": 2.3448,
      "step": 124300
    },
    {
      "epoch": 4.811076304288974,
      "grad_norm": 11.025723457336426,
      "learning_rate": 4.599076974642585e-05,
      "loss": 2.4041,
      "step": 124400
    },
    {
      "epoch": 4.814943728970878,
      "grad_norm": 14.684630393981934,
      "learning_rate": 4.598754689252427e-05,
      "loss": 2.4049,
      "step": 124500
    },
    {
      "epoch": 4.818811153652782,
      "grad_norm": 9.191145896911621,
      "learning_rate": 4.598432403862268e-05,
      "loss": 2.35,
      "step": 124600
    },
    {
      "epoch": 4.822678578334687,
      "grad_norm": 12.132938385009766,
      "learning_rate": 4.5981101184721096e-05,
      "loss": 2.4465,
      "step": 124700
    },
    {
      "epoch": 4.826546003016591,
      "grad_norm": 10.210366249084473,
      "learning_rate": 4.5977878330819504e-05,
      "loss": 2.3908,
      "step": 124800
    },
    {
      "epoch": 4.830413427698495,
      "grad_norm": 12.95992660522461,
      "learning_rate": 4.597465547691792e-05,
      "loss": 2.4646,
      "step": 124900
    },
    {
      "epoch": 4.8342808523804,
      "grad_norm": 11.378332138061523,
      "learning_rate": 4.5971432623016334e-05,
      "loss": 2.4145,
      "step": 125000
    },
    {
      "epoch": 4.838148277062304,
      "grad_norm": 10.68790340423584,
      "learning_rate": 4.596820976911475e-05,
      "loss": 2.3929,
      "step": 125100
    },
    {
      "epoch": 4.8420157017442085,
      "grad_norm": 11.600547790527344,
      "learning_rate": 4.5964986915213156e-05,
      "loss": 2.3922,
      "step": 125200
    },
    {
      "epoch": 4.845883126426113,
      "grad_norm": 13.15573501586914,
      "learning_rate": 4.596176406131157e-05,
      "loss": 2.4016,
      "step": 125300
    },
    {
      "epoch": 4.849750551108017,
      "grad_norm": 12.645048141479492,
      "learning_rate": 4.5958541207409986e-05,
      "loss": 2.404,
      "step": 125400
    },
    {
      "epoch": 4.853617975789922,
      "grad_norm": 12.08794116973877,
      "learning_rate": 4.59553183535084e-05,
      "loss": 2.5223,
      "step": 125500
    },
    {
      "epoch": 4.857485400471826,
      "grad_norm": 18.65300750732422,
      "learning_rate": 4.5952095499606815e-05,
      "loss": 2.374,
      "step": 125600
    },
    {
      "epoch": 4.86135282515373,
      "grad_norm": 13.817039489746094,
      "learning_rate": 4.594887264570523e-05,
      "loss": 2.3293,
      "step": 125700
    },
    {
      "epoch": 4.865220249835635,
      "grad_norm": 11.837453842163086,
      "learning_rate": 4.594564979180364e-05,
      "loss": 2.4677,
      "step": 125800
    },
    {
      "epoch": 4.869087674517539,
      "grad_norm": 10.434643745422363,
      "learning_rate": 4.594242693790205e-05,
      "loss": 2.4383,
      "step": 125900
    },
    {
      "epoch": 4.8729550991994435,
      "grad_norm": 11.129546165466309,
      "learning_rate": 4.593920408400047e-05,
      "loss": 2.4506,
      "step": 126000
    },
    {
      "epoch": 4.876822523881348,
      "grad_norm": 9.161874771118164,
      "learning_rate": 4.593598123009888e-05,
      "loss": 2.4287,
      "step": 126100
    },
    {
      "epoch": 4.880689948563251,
      "grad_norm": 10.543747901916504,
      "learning_rate": 4.59327583761973e-05,
      "loss": 2.4166,
      "step": 126200
    },
    {
      "epoch": 4.884557373245156,
      "grad_norm": 8.209606170654297,
      "learning_rate": 4.5929535522295705e-05,
      "loss": 2.3931,
      "step": 126300
    },
    {
      "epoch": 4.88842479792706,
      "grad_norm": 12.219804763793945,
      "learning_rate": 4.592631266839412e-05,
      "loss": 2.449,
      "step": 126400
    },
    {
      "epoch": 4.8922922226089645,
      "grad_norm": 11.271222114562988,
      "learning_rate": 4.5923089814492534e-05,
      "loss": 2.3508,
      "step": 126500
    },
    {
      "epoch": 4.896159647290869,
      "grad_norm": 11.264196395874023,
      "learning_rate": 4.591986696059095e-05,
      "loss": 2.3987,
      "step": 126600
    },
    {
      "epoch": 4.900027071972773,
      "grad_norm": 17.65947151184082,
      "learning_rate": 4.5916644106689364e-05,
      "loss": 2.3674,
      "step": 126700
    },
    {
      "epoch": 4.903894496654678,
      "grad_norm": 15.566792488098145,
      "learning_rate": 4.591342125278777e-05,
      "loss": 2.3643,
      "step": 126800
    },
    {
      "epoch": 4.907761921336582,
      "grad_norm": 13.653643608093262,
      "learning_rate": 4.5910198398886186e-05,
      "loss": 2.3239,
      "step": 126900
    },
    {
      "epoch": 4.911629346018486,
      "grad_norm": 12.656281471252441,
      "learning_rate": 4.59069755449846e-05,
      "loss": 2.4479,
      "step": 127000
    },
    {
      "epoch": 4.915496770700391,
      "grad_norm": 14.562944412231445,
      "learning_rate": 4.590375269108301e-05,
      "loss": 2.3656,
      "step": 127100
    },
    {
      "epoch": 4.919364195382295,
      "grad_norm": 9.084761619567871,
      "learning_rate": 4.5900529837181424e-05,
      "loss": 2.4348,
      "step": 127200
    },
    {
      "epoch": 4.9232316200641995,
      "grad_norm": 12.82398796081543,
      "learning_rate": 4.589730698327984e-05,
      "loss": 2.327,
      "step": 127300
    },
    {
      "epoch": 4.927099044746104,
      "grad_norm": 12.458486557006836,
      "learning_rate": 4.589408412937825e-05,
      "loss": 2.3945,
      "step": 127400
    },
    {
      "epoch": 4.930966469428008,
      "grad_norm": 12.384858131408691,
      "learning_rate": 4.589086127547666e-05,
      "loss": 2.388,
      "step": 127500
    },
    {
      "epoch": 4.934833894109913,
      "grad_norm": 13.223518371582031,
      "learning_rate": 4.5887638421575076e-05,
      "loss": 2.3852,
      "step": 127600
    },
    {
      "epoch": 4.938701318791816,
      "grad_norm": 14.407257080078125,
      "learning_rate": 4.588441556767349e-05,
      "loss": 2.3138,
      "step": 127700
    },
    {
      "epoch": 4.942568743473721,
      "grad_norm": 14.387578010559082,
      "learning_rate": 4.5881192713771905e-05,
      "loss": 2.441,
      "step": 127800
    },
    {
      "epoch": 4.946436168155625,
      "grad_norm": 11.411210060119629,
      "learning_rate": 4.587796985987031e-05,
      "loss": 2.3924,
      "step": 127900
    },
    {
      "epoch": 4.950303592837529,
      "grad_norm": 8.94438648223877,
      "learning_rate": 4.587474700596873e-05,
      "loss": 2.472,
      "step": 128000
    },
    {
      "epoch": 4.954171017519434,
      "grad_norm": 11.209029197692871,
      "learning_rate": 4.587152415206714e-05,
      "loss": 2.4125,
      "step": 128100
    },
    {
      "epoch": 4.958038442201338,
      "grad_norm": 8.466691017150879,
      "learning_rate": 4.586830129816556e-05,
      "loss": 2.4038,
      "step": 128200
    },
    {
      "epoch": 4.961905866883242,
      "grad_norm": 12.21420955657959,
      "learning_rate": 4.5865078444263965e-05,
      "loss": 2.4409,
      "step": 128300
    },
    {
      "epoch": 4.965773291565147,
      "grad_norm": 12.728645324707031,
      "learning_rate": 4.586185559036238e-05,
      "loss": 2.3834,
      "step": 128400
    },
    {
      "epoch": 4.969640716247051,
      "grad_norm": 10.929756164550781,
      "learning_rate": 4.5858632736460795e-05,
      "loss": 2.3336,
      "step": 128500
    },
    {
      "epoch": 4.973508140928955,
      "grad_norm": 10.984946250915527,
      "learning_rate": 4.585540988255921e-05,
      "loss": 2.3493,
      "step": 128600
    },
    {
      "epoch": 4.97737556561086,
      "grad_norm": 13.407357215881348,
      "learning_rate": 4.585218702865762e-05,
      "loss": 2.3815,
      "step": 128700
    },
    {
      "epoch": 4.981242990292764,
      "grad_norm": 14.112512588500977,
      "learning_rate": 4.584896417475603e-05,
      "loss": 2.3658,
      "step": 128800
    },
    {
      "epoch": 4.985110414974669,
      "grad_norm": 11.438457489013672,
      "learning_rate": 4.584574132085445e-05,
      "loss": 2.4386,
      "step": 128900
    },
    {
      "epoch": 4.988977839656573,
      "grad_norm": 11.406499862670898,
      "learning_rate": 4.584251846695286e-05,
      "loss": 2.4188,
      "step": 129000
    },
    {
      "epoch": 4.992845264338477,
      "grad_norm": 10.341278076171875,
      "learning_rate": 4.583929561305127e-05,
      "loss": 2.3893,
      "step": 129100
    },
    {
      "epoch": 4.996712689020382,
      "grad_norm": 11.235922813415527,
      "learning_rate": 4.5836072759149684e-05,
      "loss": 2.4876,
      "step": 129200
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.2383902072906494,
      "eval_runtime": 5.8614,
      "eval_samples_per_second": 232.197,
      "eval_steps_per_second": 232.197,
      "step": 129285
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.230309009552002,
      "eval_runtime": 110.5888,
      "eval_samples_per_second": 233.812,
      "eval_steps_per_second": 233.812,
      "step": 129285
    },
    {
      "epoch": 5.000580113702286,
      "grad_norm": 10.945015907287598,
      "learning_rate": 4.58328499052481e-05,
      "loss": 2.2528,
      "step": 129300
    },
    {
      "epoch": 5.0044475383841895,
      "grad_norm": 12.397171020507812,
      "learning_rate": 4.5829627051346513e-05,
      "loss": 2.4622,
      "step": 129400
    },
    {
      "epoch": 5.008314963066094,
      "grad_norm": 14.279363632202148,
      "learning_rate": 4.582640419744492e-05,
      "loss": 2.2805,
      "step": 129500
    },
    {
      "epoch": 5.012182387747998,
      "grad_norm": 14.281538009643555,
      "learning_rate": 4.5823181343543336e-05,
      "loss": 2.3566,
      "step": 129600
    },
    {
      "epoch": 5.016049812429903,
      "grad_norm": 11.448527336120605,
      "learning_rate": 4.581995848964175e-05,
      "loss": 2.403,
      "step": 129700
    },
    {
      "epoch": 5.019917237111807,
      "grad_norm": 13.746163368225098,
      "learning_rate": 4.5816735635740166e-05,
      "loss": 2.3678,
      "step": 129800
    },
    {
      "epoch": 5.023784661793711,
      "grad_norm": 10.658577919006348,
      "learning_rate": 4.5813512781838574e-05,
      "loss": 2.4202,
      "step": 129900
    },
    {
      "epoch": 5.027652086475616,
      "grad_norm": 13.223814964294434,
      "learning_rate": 4.581028992793699e-05,
      "loss": 2.3608,
      "step": 130000
    },
    {
      "epoch": 5.03151951115752,
      "grad_norm": 9.246085166931152,
      "learning_rate": 4.58070670740354e-05,
      "loss": 2.3277,
      "step": 130100
    },
    {
      "epoch": 5.0353869358394245,
      "grad_norm": 14.991482734680176,
      "learning_rate": 4.580384422013381e-05,
      "loss": 2.3188,
      "step": 130200
    },
    {
      "epoch": 5.039254360521329,
      "grad_norm": 14.622990608215332,
      "learning_rate": 4.5800621366232226e-05,
      "loss": 2.3431,
      "step": 130300
    },
    {
      "epoch": 5.043121785203233,
      "grad_norm": 10.509198188781738,
      "learning_rate": 4.579739851233064e-05,
      "loss": 2.3741,
      "step": 130400
    },
    {
      "epoch": 5.046989209885138,
      "grad_norm": 13.615644454956055,
      "learning_rate": 4.5794175658429055e-05,
      "loss": 2.4073,
      "step": 130500
    },
    {
      "epoch": 5.050856634567042,
      "grad_norm": 12.555498123168945,
      "learning_rate": 4.579095280452746e-05,
      "loss": 2.388,
      "step": 130600
    },
    {
      "epoch": 5.054724059248946,
      "grad_norm": 13.18122386932373,
      "learning_rate": 4.578772995062588e-05,
      "loss": 2.3795,
      "step": 130700
    },
    {
      "epoch": 5.058591483930851,
      "grad_norm": 14.706437110900879,
      "learning_rate": 4.578450709672429e-05,
      "loss": 2.3423,
      "step": 130800
    },
    {
      "epoch": 5.062458908612755,
      "grad_norm": 11.283004760742188,
      "learning_rate": 4.578128424282271e-05,
      "loss": 2.2815,
      "step": 130900
    },
    {
      "epoch": 5.0663263332946595,
      "grad_norm": 13.179579734802246,
      "learning_rate": 4.5778061388921115e-05,
      "loss": 2.3769,
      "step": 131000
    },
    {
      "epoch": 5.070193757976563,
      "grad_norm": 13.679162979125977,
      "learning_rate": 4.577483853501953e-05,
      "loss": 2.395,
      "step": 131100
    },
    {
      "epoch": 5.074061182658467,
      "grad_norm": 13.490979194641113,
      "learning_rate": 4.5771615681117945e-05,
      "loss": 2.2959,
      "step": 131200
    },
    {
      "epoch": 5.077928607340372,
      "grad_norm": 9.009600639343262,
      "learning_rate": 4.576839282721636e-05,
      "loss": 2.3132,
      "step": 131300
    },
    {
      "epoch": 5.081796032022276,
      "grad_norm": 10.051673889160156,
      "learning_rate": 4.576516997331477e-05,
      "loss": 2.3608,
      "step": 131400
    },
    {
      "epoch": 5.0856634567041805,
      "grad_norm": 9.943852424621582,
      "learning_rate": 4.576194711941318e-05,
      "loss": 2.3765,
      "step": 131500
    },
    {
      "epoch": 5.089530881386085,
      "grad_norm": 9.783368110656738,
      "learning_rate": 4.57587242655116e-05,
      "loss": 2.3288,
      "step": 131600
    },
    {
      "epoch": 5.093398306067989,
      "grad_norm": 12.733195304870605,
      "learning_rate": 4.575550141161001e-05,
      "loss": 2.4335,
      "step": 131700
    },
    {
      "epoch": 5.097265730749894,
      "grad_norm": 13.068192481994629,
      "learning_rate": 4.575227855770842e-05,
      "loss": 2.3712,
      "step": 131800
    },
    {
      "epoch": 5.101133155431798,
      "grad_norm": 10.058995246887207,
      "learning_rate": 4.5749055703806834e-05,
      "loss": 2.4576,
      "step": 131900
    },
    {
      "epoch": 5.105000580113702,
      "grad_norm": 11.302464485168457,
      "learning_rate": 4.574583284990525e-05,
      "loss": 2.3546,
      "step": 132000
    },
    {
      "epoch": 5.108868004795607,
      "grad_norm": 15.630532264709473,
      "learning_rate": 4.5742609996003663e-05,
      "loss": 2.4113,
      "step": 132100
    },
    {
      "epoch": 5.112735429477511,
      "grad_norm": 10.82150936126709,
      "learning_rate": 4.573938714210207e-05,
      "loss": 2.4113,
      "step": 132200
    },
    {
      "epoch": 5.1166028541594155,
      "grad_norm": 13.69981861114502,
      "learning_rate": 4.5736164288200486e-05,
      "loss": 2.345,
      "step": 132300
    },
    {
      "epoch": 5.12047027884132,
      "grad_norm": 11.889789581298828,
      "learning_rate": 4.57329414342989e-05,
      "loss": 2.3125,
      "step": 132400
    },
    {
      "epoch": 5.124337703523224,
      "grad_norm": 9.463424682617188,
      "learning_rate": 4.5729718580397316e-05,
      "loss": 2.3397,
      "step": 132500
    },
    {
      "epoch": 5.128205128205128,
      "grad_norm": 13.624143600463867,
      "learning_rate": 4.572649572649573e-05,
      "loss": 2.4233,
      "step": 132600
    },
    {
      "epoch": 5.132072552887032,
      "grad_norm": 12.40701961517334,
      "learning_rate": 4.5723272872594145e-05,
      "loss": 2.3282,
      "step": 132700
    },
    {
      "epoch": 5.1359399775689365,
      "grad_norm": 10.174497604370117,
      "learning_rate": 4.572005001869255e-05,
      "loss": 2.4287,
      "step": 132800
    },
    {
      "epoch": 5.139807402250841,
      "grad_norm": 13.44401741027832,
      "learning_rate": 4.571682716479097e-05,
      "loss": 2.3616,
      "step": 132900
    },
    {
      "epoch": 5.143674826932745,
      "grad_norm": 13.86517333984375,
      "learning_rate": 4.571360431088938e-05,
      "loss": 2.3757,
      "step": 133000
    },
    {
      "epoch": 5.14754225161465,
      "grad_norm": 12.074816703796387,
      "learning_rate": 4.57103814569878e-05,
      "loss": 2.4495,
      "step": 133100
    },
    {
      "epoch": 5.151409676296554,
      "grad_norm": 12.784018516540527,
      "learning_rate": 4.570715860308621e-05,
      "loss": 2.3175,
      "step": 133200
    },
    {
      "epoch": 5.155277100978458,
      "grad_norm": 12.710245132446289,
      "learning_rate": 4.570393574918462e-05,
      "loss": 2.3807,
      "step": 133300
    },
    {
      "epoch": 5.159144525660363,
      "grad_norm": 12.079216003417969,
      "learning_rate": 4.5700712895283034e-05,
      "loss": 2.3717,
      "step": 133400
    },
    {
      "epoch": 5.163011950342267,
      "grad_norm": 12.914828300476074,
      "learning_rate": 4.569749004138145e-05,
      "loss": 2.4135,
      "step": 133500
    },
    {
      "epoch": 5.1668793750241715,
      "grad_norm": 14.468396186828613,
      "learning_rate": 4.5694267187479864e-05,
      "loss": 2.3696,
      "step": 133600
    },
    {
      "epoch": 5.170746799706076,
      "grad_norm": 13.857297897338867,
      "learning_rate": 4.569104433357827e-05,
      "loss": 2.3588,
      "step": 133700
    },
    {
      "epoch": 5.17461422438798,
      "grad_norm": 11.16067123413086,
      "learning_rate": 4.5687821479676687e-05,
      "loss": 2.3994,
      "step": 133800
    },
    {
      "epoch": 5.178481649069885,
      "grad_norm": 12.486268997192383,
      "learning_rate": 4.56845986257751e-05,
      "loss": 2.395,
      "step": 133900
    },
    {
      "epoch": 5.182349073751789,
      "grad_norm": 12.20486831665039,
      "learning_rate": 4.5681375771873516e-05,
      "loss": 2.3424,
      "step": 134000
    },
    {
      "epoch": 5.186216498433693,
      "grad_norm": 19.227304458618164,
      "learning_rate": 4.5678152917971924e-05,
      "loss": 2.3752,
      "step": 134100
    },
    {
      "epoch": 5.190083923115598,
      "grad_norm": 13.15311336517334,
      "learning_rate": 4.567493006407034e-05,
      "loss": 2.4089,
      "step": 134200
    },
    {
      "epoch": 5.193951347797501,
      "grad_norm": 12.24612808227539,
      "learning_rate": 4.567170721016875e-05,
      "loss": 2.3083,
      "step": 134300
    },
    {
      "epoch": 5.197818772479406,
      "grad_norm": 11.73878002166748,
      "learning_rate": 4.566848435626717e-05,
      "loss": 2.3719,
      "step": 134400
    },
    {
      "epoch": 5.20168619716131,
      "grad_norm": 14.745508193969727,
      "learning_rate": 4.5665261502365576e-05,
      "loss": 2.4013,
      "step": 134500
    },
    {
      "epoch": 5.205553621843214,
      "grad_norm": 14.844449043273926,
      "learning_rate": 4.566203864846399e-05,
      "loss": 2.381,
      "step": 134600
    },
    {
      "epoch": 5.209421046525119,
      "grad_norm": 9.912881851196289,
      "learning_rate": 4.5658815794562405e-05,
      "loss": 2.3727,
      "step": 134700
    },
    {
      "epoch": 5.213288471207023,
      "grad_norm": 13.610949516296387,
      "learning_rate": 4.565559294066082e-05,
      "loss": 2.3768,
      "step": 134800
    },
    {
      "epoch": 5.2171558958889275,
      "grad_norm": 11.966053009033203,
      "learning_rate": 4.565237008675923e-05,
      "loss": 2.3719,
      "step": 134900
    },
    {
      "epoch": 5.221023320570832,
      "grad_norm": 13.913896560668945,
      "learning_rate": 4.564914723285764e-05,
      "loss": 2.2726,
      "step": 135000
    },
    {
      "epoch": 5.224890745252736,
      "grad_norm": 13.978840827941895,
      "learning_rate": 4.564592437895606e-05,
      "loss": 2.327,
      "step": 135100
    },
    {
      "epoch": 5.228758169934641,
      "grad_norm": 11.388843536376953,
      "learning_rate": 4.564270152505447e-05,
      "loss": 2.4195,
      "step": 135200
    },
    {
      "epoch": 5.232625594616545,
      "grad_norm": 12.885726928710938,
      "learning_rate": 4.563947867115288e-05,
      "loss": 2.3555,
      "step": 135300
    },
    {
      "epoch": 5.236493019298449,
      "grad_norm": 11.10180950164795,
      "learning_rate": 4.5636255817251295e-05,
      "loss": 2.3057,
      "step": 135400
    },
    {
      "epoch": 5.240360443980354,
      "grad_norm": 14.1976900100708,
      "learning_rate": 4.563303296334971e-05,
      "loss": 2.3507,
      "step": 135500
    },
    {
      "epoch": 5.244227868662258,
      "grad_norm": 11.604106903076172,
      "learning_rate": 4.5629810109448124e-05,
      "loss": 2.3884,
      "step": 135600
    },
    {
      "epoch": 5.2480952933441625,
      "grad_norm": 10.56655502319336,
      "learning_rate": 4.562658725554653e-05,
      "loss": 2.2915,
      "step": 135700
    },
    {
      "epoch": 5.251962718026067,
      "grad_norm": 13.174293518066406,
      "learning_rate": 4.562336440164495e-05,
      "loss": 2.3778,
      "step": 135800
    },
    {
      "epoch": 5.255830142707971,
      "grad_norm": 9.858716011047363,
      "learning_rate": 4.562014154774336e-05,
      "loss": 2.3652,
      "step": 135900
    },
    {
      "epoch": 5.259697567389875,
      "grad_norm": 13.936670303344727,
      "learning_rate": 4.561691869384177e-05,
      "loss": 2.4682,
      "step": 136000
    },
    {
      "epoch": 5.263564992071779,
      "grad_norm": 11.544763565063477,
      "learning_rate": 4.5613695839940184e-05,
      "loss": 2.3457,
      "step": 136100
    },
    {
      "epoch": 5.2674324167536835,
      "grad_norm": 10.524423599243164,
      "learning_rate": 4.56104729860386e-05,
      "loss": 2.3065,
      "step": 136200
    },
    {
      "epoch": 5.271299841435588,
      "grad_norm": 10.38353443145752,
      "learning_rate": 4.5607250132137014e-05,
      "loss": 2.374,
      "step": 136300
    },
    {
      "epoch": 5.275167266117492,
      "grad_norm": 10.200984954833984,
      "learning_rate": 4.560402727823542e-05,
      "loss": 2.3522,
      "step": 136400
    },
    {
      "epoch": 5.279034690799397,
      "grad_norm": 13.41512393951416,
      "learning_rate": 4.5600804424333837e-05,
      "loss": 2.3884,
      "step": 136500
    },
    {
      "epoch": 5.282902115481301,
      "grad_norm": 12.06322956085205,
      "learning_rate": 4.559758157043225e-05,
      "loss": 2.3664,
      "step": 136600
    },
    {
      "epoch": 5.286769540163205,
      "grad_norm": 11.799959182739258,
      "learning_rate": 4.5594358716530666e-05,
      "loss": 2.4275,
      "step": 136700
    },
    {
      "epoch": 5.29063696484511,
      "grad_norm": 12.073827743530273,
      "learning_rate": 4.5591135862629074e-05,
      "loss": 2.4635,
      "step": 136800
    },
    {
      "epoch": 5.294504389527014,
      "grad_norm": 10.13961124420166,
      "learning_rate": 4.558791300872749e-05,
      "loss": 2.2681,
      "step": 136900
    },
    {
      "epoch": 5.2983718142089185,
      "grad_norm": 11.77895736694336,
      "learning_rate": 4.55846901548259e-05,
      "loss": 2.3143,
      "step": 137000
    },
    {
      "epoch": 5.302239238890823,
      "grad_norm": 10.067585945129395,
      "learning_rate": 4.558146730092432e-05,
      "loss": 2.4265,
      "step": 137100
    },
    {
      "epoch": 5.306106663572727,
      "grad_norm": 9.657787322998047,
      "learning_rate": 4.5578244447022726e-05,
      "loss": 2.3155,
      "step": 137200
    },
    {
      "epoch": 5.309974088254632,
      "grad_norm": 12.69342041015625,
      "learning_rate": 4.557502159312114e-05,
      "loss": 2.3228,
      "step": 137300
    },
    {
      "epoch": 5.313841512936536,
      "grad_norm": 11.738700866699219,
      "learning_rate": 4.5571798739219555e-05,
      "loss": 2.2606,
      "step": 137400
    },
    {
      "epoch": 5.317708937618439,
      "grad_norm": 14.64936637878418,
      "learning_rate": 4.556857588531797e-05,
      "loss": 2.2958,
      "step": 137500
    },
    {
      "epoch": 5.321576362300344,
      "grad_norm": 12.218667984008789,
      "learning_rate": 4.556535303141638e-05,
      "loss": 2.3792,
      "step": 137600
    },
    {
      "epoch": 5.325443786982248,
      "grad_norm": 13.57571792602539,
      "learning_rate": 4.556213017751479e-05,
      "loss": 2.3088,
      "step": 137700
    },
    {
      "epoch": 5.329311211664153,
      "grad_norm": 11.600296020507812,
      "learning_rate": 4.555890732361321e-05,
      "loss": 2.3838,
      "step": 137800
    },
    {
      "epoch": 5.333178636346057,
      "grad_norm": 9.917498588562012,
      "learning_rate": 4.555568446971162e-05,
      "loss": 2.2682,
      "step": 137900
    },
    {
      "epoch": 5.337046061027961,
      "grad_norm": 11.871506690979004,
      "learning_rate": 4.555246161581003e-05,
      "loss": 2.3717,
      "step": 138000
    },
    {
      "epoch": 5.340913485709866,
      "grad_norm": 15.569343566894531,
      "learning_rate": 4.5549238761908445e-05,
      "loss": 2.3679,
      "step": 138100
    },
    {
      "epoch": 5.34478091039177,
      "grad_norm": 10.764677047729492,
      "learning_rate": 4.554601590800686e-05,
      "loss": 2.3893,
      "step": 138200
    },
    {
      "epoch": 5.348648335073674,
      "grad_norm": 15.503010749816895,
      "learning_rate": 4.5542793054105274e-05,
      "loss": 2.3135,
      "step": 138300
    },
    {
      "epoch": 5.352515759755579,
      "grad_norm": 12.812050819396973,
      "learning_rate": 4.553957020020368e-05,
      "loss": 2.3711,
      "step": 138400
    },
    {
      "epoch": 5.356383184437483,
      "grad_norm": 11.332586288452148,
      "learning_rate": 4.55363473463021e-05,
      "loss": 2.3132,
      "step": 138500
    },
    {
      "epoch": 5.3602506091193876,
      "grad_norm": 13.494573593139648,
      "learning_rate": 4.553312449240051e-05,
      "loss": 2.3322,
      "step": 138600
    },
    {
      "epoch": 5.364118033801292,
      "grad_norm": 12.418649673461914,
      "learning_rate": 4.552990163849892e-05,
      "loss": 2.3101,
      "step": 138700
    },
    {
      "epoch": 5.367985458483196,
      "grad_norm": 11.078238487243652,
      "learning_rate": 4.5526678784597334e-05,
      "loss": 2.3227,
      "step": 138800
    },
    {
      "epoch": 5.371852883165101,
      "grad_norm": 12.465852737426758,
      "learning_rate": 4.552345593069575e-05,
      "loss": 2.3445,
      "step": 138900
    },
    {
      "epoch": 5.375720307847005,
      "grad_norm": 11.78266429901123,
      "learning_rate": 4.5520233076794164e-05,
      "loss": 2.3356,
      "step": 139000
    },
    {
      "epoch": 5.379587732528909,
      "grad_norm": 13.626274108886719,
      "learning_rate": 4.551701022289258e-05,
      "loss": 2.3902,
      "step": 139100
    },
    {
      "epoch": 5.383455157210813,
      "grad_norm": 9.069021224975586,
      "learning_rate": 4.551378736899099e-05,
      "loss": 2.3912,
      "step": 139200
    },
    {
      "epoch": 5.387322581892717,
      "grad_norm": 12.690099716186523,
      "learning_rate": 4.55105645150894e-05,
      "loss": 2.303,
      "step": 139300
    },
    {
      "epoch": 5.391190006574622,
      "grad_norm": 10.305420875549316,
      "learning_rate": 4.5507341661187816e-05,
      "loss": 2.283,
      "step": 139400
    },
    {
      "epoch": 5.395057431256526,
      "grad_norm": 14.602618217468262,
      "learning_rate": 4.550411880728623e-05,
      "loss": 2.3561,
      "step": 139500
    },
    {
      "epoch": 5.39892485593843,
      "grad_norm": 19.535036087036133,
      "learning_rate": 4.5500895953384645e-05,
      "loss": 2.3015,
      "step": 139600
    },
    {
      "epoch": 5.402792280620335,
      "grad_norm": 12.398127555847168,
      "learning_rate": 4.549767309948306e-05,
      "loss": 2.3283,
      "step": 139700
    },
    {
      "epoch": 5.406659705302239,
      "grad_norm": 10.950878143310547,
      "learning_rate": 4.5494450245581475e-05,
      "loss": 2.3409,
      "step": 139800
    },
    {
      "epoch": 5.4105271299841435,
      "grad_norm": 11.264347076416016,
      "learning_rate": 4.549122739167988e-05,
      "loss": 2.3217,
      "step": 139900
    },
    {
      "epoch": 5.414394554666048,
      "grad_norm": 9.944347381591797,
      "learning_rate": 4.54880045377783e-05,
      "loss": 2.3483,
      "step": 140000
    },
    {
      "epoch": 5.418261979347952,
      "grad_norm": 13.788101196289062,
      "learning_rate": 4.548478168387671e-05,
      "loss": 2.3111,
      "step": 140100
    },
    {
      "epoch": 5.422129404029857,
      "grad_norm": 18.964094161987305,
      "learning_rate": 4.548155882997513e-05,
      "loss": 2.3565,
      "step": 140200
    },
    {
      "epoch": 5.425996828711761,
      "grad_norm": 11.291553497314453,
      "learning_rate": 4.5478335976073535e-05,
      "loss": 2.3265,
      "step": 140300
    },
    {
      "epoch": 5.429864253393665,
      "grad_norm": 12.600942611694336,
      "learning_rate": 4.547511312217195e-05,
      "loss": 2.3716,
      "step": 140400
    },
    {
      "epoch": 5.43373167807557,
      "grad_norm": 13.579487800598145,
      "learning_rate": 4.5471890268270364e-05,
      "loss": 2.3671,
      "step": 140500
    },
    {
      "epoch": 5.437599102757474,
      "grad_norm": 11.883614540100098,
      "learning_rate": 4.546866741436878e-05,
      "loss": 2.345,
      "step": 140600
    },
    {
      "epoch": 5.4414665274393785,
      "grad_norm": 12.420919418334961,
      "learning_rate": 4.546544456046719e-05,
      "loss": 2.3009,
      "step": 140700
    },
    {
      "epoch": 5.445333952121283,
      "grad_norm": 12.027763366699219,
      "learning_rate": 4.54622217065656e-05,
      "loss": 2.3582,
      "step": 140800
    },
    {
      "epoch": 5.449201376803186,
      "grad_norm": 9.668499946594238,
      "learning_rate": 4.5458998852664016e-05,
      "loss": 2.3247,
      "step": 140900
    },
    {
      "epoch": 5.453068801485091,
      "grad_norm": 10.809920310974121,
      "learning_rate": 4.545577599876243e-05,
      "loss": 2.3921,
      "step": 141000
    },
    {
      "epoch": 5.456936226166995,
      "grad_norm": 14.903241157531738,
      "learning_rate": 4.545255314486084e-05,
      "loss": 2.3128,
      "step": 141100
    },
    {
      "epoch": 5.4608036508488995,
      "grad_norm": 12.607414245605469,
      "learning_rate": 4.5449330290959254e-05,
      "loss": 2.3974,
      "step": 141200
    },
    {
      "epoch": 5.464671075530804,
      "grad_norm": 8.867361068725586,
      "learning_rate": 4.544610743705767e-05,
      "loss": 2.4217,
      "step": 141300
    },
    {
      "epoch": 5.468538500212708,
      "grad_norm": 10.79995346069336,
      "learning_rate": 4.544288458315608e-05,
      "loss": 2.3292,
      "step": 141400
    },
    {
      "epoch": 5.472405924894613,
      "grad_norm": 11.790681838989258,
      "learning_rate": 4.543966172925449e-05,
      "loss": 2.3907,
      "step": 141500
    },
    {
      "epoch": 5.476273349576517,
      "grad_norm": 12.880337715148926,
      "learning_rate": 4.5436438875352906e-05,
      "loss": 2.2793,
      "step": 141600
    },
    {
      "epoch": 5.480140774258421,
      "grad_norm": 12.844505310058594,
      "learning_rate": 4.543321602145132e-05,
      "loss": 2.3638,
      "step": 141700
    },
    {
      "epoch": 5.484008198940326,
      "grad_norm": 14.370574951171875,
      "learning_rate": 4.542999316754973e-05,
      "loss": 2.2912,
      "step": 141800
    },
    {
      "epoch": 5.48787562362223,
      "grad_norm": 13.12688159942627,
      "learning_rate": 4.542677031364814e-05,
      "loss": 2.3858,
      "step": 141900
    },
    {
      "epoch": 5.4917430483041345,
      "grad_norm": 13.68286418914795,
      "learning_rate": 4.542354745974656e-05,
      "loss": 2.3477,
      "step": 142000
    },
    {
      "epoch": 5.495610472986039,
      "grad_norm": 12.630529403686523,
      "learning_rate": 4.542032460584497e-05,
      "loss": 2.4169,
      "step": 142100
    },
    {
      "epoch": 5.499477897667943,
      "grad_norm": 11.771166801452637,
      "learning_rate": 4.541710175194338e-05,
      "loss": 2.4568,
      "step": 142200
    },
    {
      "epoch": 5.503345322349848,
      "grad_norm": 10.578421592712402,
      "learning_rate": 4.5413878898041795e-05,
      "loss": 2.3068,
      "step": 142300
    },
    {
      "epoch": 5.507212747031751,
      "grad_norm": 13.291845321655273,
      "learning_rate": 4.541065604414021e-05,
      "loss": 2.3396,
      "step": 142400
    },
    {
      "epoch": 5.5110801717136555,
      "grad_norm": 11.370368003845215,
      "learning_rate": 4.5407433190238625e-05,
      "loss": 2.3995,
      "step": 142500
    },
    {
      "epoch": 5.51494759639556,
      "grad_norm": 10.943228721618652,
      "learning_rate": 4.540421033633703e-05,
      "loss": 2.2997,
      "step": 142600
    },
    {
      "epoch": 5.518815021077464,
      "grad_norm": 9.830220222473145,
      "learning_rate": 4.540098748243545e-05,
      "loss": 2.3116,
      "step": 142700
    },
    {
      "epoch": 5.522682445759369,
      "grad_norm": 11.874553680419922,
      "learning_rate": 4.539776462853386e-05,
      "loss": 2.3622,
      "step": 142800
    },
    {
      "epoch": 5.526549870441273,
      "grad_norm": 14.084383010864258,
      "learning_rate": 4.539454177463228e-05,
      "loss": 2.4038,
      "step": 142900
    },
    {
      "epoch": 5.530417295123177,
      "grad_norm": 12.33244514465332,
      "learning_rate": 4.5391318920730685e-05,
      "loss": 2.276,
      "step": 143000
    },
    {
      "epoch": 5.534284719805082,
      "grad_norm": 11.424076080322266,
      "learning_rate": 4.53880960668291e-05,
      "loss": 2.3725,
      "step": 143100
    },
    {
      "epoch": 5.538152144486986,
      "grad_norm": 11.757927894592285,
      "learning_rate": 4.5384873212927514e-05,
      "loss": 2.3402,
      "step": 143200
    },
    {
      "epoch": 5.5420195691688905,
      "grad_norm": 14.36474323272705,
      "learning_rate": 4.538165035902593e-05,
      "loss": 2.3115,
      "step": 143300
    },
    {
      "epoch": 5.545886993850795,
      "grad_norm": 13.250811576843262,
      "learning_rate": 4.537842750512434e-05,
      "loss": 2.3531,
      "step": 143400
    },
    {
      "epoch": 5.549754418532699,
      "grad_norm": 11.653021812438965,
      "learning_rate": 4.537520465122275e-05,
      "loss": 2.3995,
      "step": 143500
    },
    {
      "epoch": 5.553621843214604,
      "grad_norm": 10.205862998962402,
      "learning_rate": 4.5371981797321166e-05,
      "loss": 2.2952,
      "step": 143600
    },
    {
      "epoch": 5.557489267896508,
      "grad_norm": 10.664948463439941,
      "learning_rate": 4.536875894341958e-05,
      "loss": 2.2996,
      "step": 143700
    },
    {
      "epoch": 5.561356692578412,
      "grad_norm": 13.532986640930176,
      "learning_rate": 4.536553608951799e-05,
      "loss": 2.3607,
      "step": 143800
    },
    {
      "epoch": 5.565224117260317,
      "grad_norm": 11.17636775970459,
      "learning_rate": 4.5362313235616404e-05,
      "loss": 2.3158,
      "step": 143900
    },
    {
      "epoch": 5.569091541942221,
      "grad_norm": 12.491456031799316,
      "learning_rate": 4.535909038171482e-05,
      "loss": 2.4463,
      "step": 144000
    },
    {
      "epoch": 5.572958966624125,
      "grad_norm": 11.028969764709473,
      "learning_rate": 4.535586752781323e-05,
      "loss": 2.3443,
      "step": 144100
    },
    {
      "epoch": 5.576826391306029,
      "grad_norm": 11.715405464172363,
      "learning_rate": 4.535264467391164e-05,
      "loss": 2.4345,
      "step": 144200
    },
    {
      "epoch": 5.580693815987933,
      "grad_norm": 9.80809211730957,
      "learning_rate": 4.5349421820010056e-05,
      "loss": 2.3057,
      "step": 144300
    },
    {
      "epoch": 5.584561240669838,
      "grad_norm": 12.30965805053711,
      "learning_rate": 4.534619896610847e-05,
      "loss": 2.4168,
      "step": 144400
    },
    {
      "epoch": 5.588428665351742,
      "grad_norm": 10.481196403503418,
      "learning_rate": 4.5342976112206885e-05,
      "loss": 2.3654,
      "step": 144500
    },
    {
      "epoch": 5.5922960900336465,
      "grad_norm": 12.4320068359375,
      "learning_rate": 4.533975325830529e-05,
      "loss": 2.3638,
      "step": 144600
    },
    {
      "epoch": 5.596163514715551,
      "grad_norm": 10.170736312866211,
      "learning_rate": 4.533653040440371e-05,
      "loss": 2.3503,
      "step": 144700
    },
    {
      "epoch": 5.600030939397455,
      "grad_norm": 19.908958435058594,
      "learning_rate": 4.533330755050212e-05,
      "loss": 2.3659,
      "step": 144800
    },
    {
      "epoch": 5.60389836407936,
      "grad_norm": 13.517443656921387,
      "learning_rate": 4.533008469660053e-05,
      "loss": 2.2592,
      "step": 144900
    },
    {
      "epoch": 5.607765788761264,
      "grad_norm": 12.20412826538086,
      "learning_rate": 4.5326861842698945e-05,
      "loss": 2.3304,
      "step": 145000
    },
    {
      "epoch": 5.611633213443168,
      "grad_norm": 13.639969825744629,
      "learning_rate": 4.532363898879736e-05,
      "loss": 2.279,
      "step": 145100
    },
    {
      "epoch": 5.615500638125073,
      "grad_norm": 10.99632453918457,
      "learning_rate": 4.5320416134895775e-05,
      "loss": 2.3457,
      "step": 145200
    },
    {
      "epoch": 5.619368062806977,
      "grad_norm": 14.275345802307129,
      "learning_rate": 4.531719328099418e-05,
      "loss": 2.4012,
      "step": 145300
    },
    {
      "epoch": 5.6232354874888815,
      "grad_norm": 14.703791618347168,
      "learning_rate": 4.53139704270926e-05,
      "loss": 2.2863,
      "step": 145400
    },
    {
      "epoch": 5.627102912170786,
      "grad_norm": 10.332134246826172,
      "learning_rate": 4.531074757319101e-05,
      "loss": 2.3337,
      "step": 145500
    },
    {
      "epoch": 5.630970336852689,
      "grad_norm": 16.96687126159668,
      "learning_rate": 4.530752471928943e-05,
      "loss": 2.3522,
      "step": 145600
    },
    {
      "epoch": 5.634837761534595,
      "grad_norm": 11.508260726928711,
      "learning_rate": 4.530430186538784e-05,
      "loss": 2.4685,
      "step": 145700
    },
    {
      "epoch": 5.638705186216498,
      "grad_norm": 11.004852294921875,
      "learning_rate": 4.530107901148625e-05,
      "loss": 2.3641,
      "step": 145800
    },
    {
      "epoch": 5.6425726108984025,
      "grad_norm": 9.234580993652344,
      "learning_rate": 4.5297856157584664e-05,
      "loss": 2.2725,
      "step": 145900
    },
    {
      "epoch": 5.646440035580307,
      "grad_norm": 10.55862808227539,
      "learning_rate": 4.529463330368308e-05,
      "loss": 2.3284,
      "step": 146000
    },
    {
      "epoch": 5.650307460262211,
      "grad_norm": 14.339842796325684,
      "learning_rate": 4.5291410449781494e-05,
      "loss": 2.343,
      "step": 146100
    },
    {
      "epoch": 5.654174884944116,
      "grad_norm": 11.248373985290527,
      "learning_rate": 4.528818759587991e-05,
      "loss": 2.2609,
      "step": 146200
    },
    {
      "epoch": 5.65804230962602,
      "grad_norm": 10.936239242553711,
      "learning_rate": 4.5284964741978316e-05,
      "loss": 2.3525,
      "step": 146300
    },
    {
      "epoch": 5.661909734307924,
      "grad_norm": 13.619312286376953,
      "learning_rate": 4.528174188807673e-05,
      "loss": 2.2915,
      "step": 146400
    },
    {
      "epoch": 5.665777158989829,
      "grad_norm": 9.287217140197754,
      "learning_rate": 4.5278519034175146e-05,
      "loss": 2.3065,
      "step": 146500
    },
    {
      "epoch": 5.669644583671733,
      "grad_norm": 8.770960807800293,
      "learning_rate": 4.527529618027356e-05,
      "loss": 2.4098,
      "step": 146600
    },
    {
      "epoch": 5.6735120083536374,
      "grad_norm": 10.687017440795898,
      "learning_rate": 4.5272073326371975e-05,
      "loss": 2.347,
      "step": 146700
    },
    {
      "epoch": 5.677379433035542,
      "grad_norm": 12.747092247009277,
      "learning_rate": 4.526885047247039e-05,
      "loss": 2.2964,
      "step": 146800
    },
    {
      "epoch": 5.681246857717446,
      "grad_norm": 13.287671089172363,
      "learning_rate": 4.52656276185688e-05,
      "loss": 2.3849,
      "step": 146900
    },
    {
      "epoch": 5.685114282399351,
      "grad_norm": 11.12114143371582,
      "learning_rate": 4.526240476466721e-05,
      "loss": 2.3936,
      "step": 147000
    },
    {
      "epoch": 5.688981707081255,
      "grad_norm": 11.522353172302246,
      "learning_rate": 4.525918191076563e-05,
      "loss": 2.3474,
      "step": 147100
    },
    {
      "epoch": 5.692849131763159,
      "grad_norm": 9.958056449890137,
      "learning_rate": 4.525595905686404e-05,
      "loss": 2.3757,
      "step": 147200
    },
    {
      "epoch": 5.696716556445063,
      "grad_norm": 10.535476684570312,
      "learning_rate": 4.525273620296245e-05,
      "loss": 2.2354,
      "step": 147300
    },
    {
      "epoch": 5.700583981126967,
      "grad_norm": 10.485671043395996,
      "learning_rate": 4.5249513349060865e-05,
      "loss": 2.3055,
      "step": 147400
    },
    {
      "epoch": 5.7044514058088716,
      "grad_norm": 12.00709056854248,
      "learning_rate": 4.524629049515928e-05,
      "loss": 2.3125,
      "step": 147500
    },
    {
      "epoch": 5.708318830490776,
      "grad_norm": 11.519981384277344,
      "learning_rate": 4.524306764125769e-05,
      "loss": 2.2789,
      "step": 147600
    },
    {
      "epoch": 5.71218625517268,
      "grad_norm": 12.11697006225586,
      "learning_rate": 4.52398447873561e-05,
      "loss": 2.3249,
      "step": 147700
    },
    {
      "epoch": 5.716053679854585,
      "grad_norm": 12.471550941467285,
      "learning_rate": 4.523662193345452e-05,
      "loss": 2.3572,
      "step": 147800
    },
    {
      "epoch": 5.719921104536489,
      "grad_norm": 13.867758750915527,
      "learning_rate": 4.523339907955293e-05,
      "loss": 2.3804,
      "step": 147900
    },
    {
      "epoch": 5.723788529218393,
      "grad_norm": 12.096198081970215,
      "learning_rate": 4.523017622565134e-05,
      "loss": 2.383,
      "step": 148000
    },
    {
      "epoch": 5.727655953900298,
      "grad_norm": 11.570363998413086,
      "learning_rate": 4.5226953371749754e-05,
      "loss": 2.3002,
      "step": 148100
    },
    {
      "epoch": 5.731523378582202,
      "grad_norm": 11.008401870727539,
      "learning_rate": 4.522373051784817e-05,
      "loss": 2.2743,
      "step": 148200
    },
    {
      "epoch": 5.7353908032641066,
      "grad_norm": 13.047846794128418,
      "learning_rate": 4.5220507663946584e-05,
      "loss": 2.3108,
      "step": 148300
    },
    {
      "epoch": 5.739258227946011,
      "grad_norm": 11.688179969787598,
      "learning_rate": 4.521728481004499e-05,
      "loss": 2.286,
      "step": 148400
    },
    {
      "epoch": 5.743125652627915,
      "grad_norm": 12.81490707397461,
      "learning_rate": 4.5214061956143406e-05,
      "loss": 2.3735,
      "step": 148500
    },
    {
      "epoch": 5.74699307730982,
      "grad_norm": 10.323755264282227,
      "learning_rate": 4.521083910224182e-05,
      "loss": 2.3615,
      "step": 148600
    },
    {
      "epoch": 5.750860501991724,
      "grad_norm": 12.909690856933594,
      "learning_rate": 4.5207616248340236e-05,
      "loss": 2.4018,
      "step": 148700
    },
    {
      "epoch": 5.7547279266736275,
      "grad_norm": 10.000785827636719,
      "learning_rate": 4.5204393394438644e-05,
      "loss": 2.3009,
      "step": 148800
    },
    {
      "epoch": 5.758595351355533,
      "grad_norm": 17.548032760620117,
      "learning_rate": 4.520117054053706e-05,
      "loss": 2.3117,
      "step": 148900
    },
    {
      "epoch": 5.762462776037436,
      "grad_norm": 11.10816478729248,
      "learning_rate": 4.519794768663547e-05,
      "loss": 2.2772,
      "step": 149000
    },
    {
      "epoch": 5.766330200719341,
      "grad_norm": 10.924492835998535,
      "learning_rate": 4.519472483273389e-05,
      "loss": 2.3062,
      "step": 149100
    },
    {
      "epoch": 5.770197625401245,
      "grad_norm": 12.34625244140625,
      "learning_rate": 4.5191501978832296e-05,
      "loss": 2.3844,
      "step": 149200
    },
    {
      "epoch": 5.774065050083149,
      "grad_norm": 11.463930130004883,
      "learning_rate": 4.518827912493071e-05,
      "loss": 2.35,
      "step": 149300
    },
    {
      "epoch": 5.777932474765054,
      "grad_norm": 10.233537673950195,
      "learning_rate": 4.5185056271029125e-05,
      "loss": 2.339,
      "step": 149400
    },
    {
      "epoch": 5.781799899446958,
      "grad_norm": 11.378233909606934,
      "learning_rate": 4.518183341712754e-05,
      "loss": 2.2921,
      "step": 149500
    },
    {
      "epoch": 5.7856673241288625,
      "grad_norm": 15.193289756774902,
      "learning_rate": 4.517861056322595e-05,
      "loss": 2.3041,
      "step": 149600
    },
    {
      "epoch": 5.789534748810767,
      "grad_norm": 9.837244033813477,
      "learning_rate": 4.517538770932436e-05,
      "loss": 2.3509,
      "step": 149700
    },
    {
      "epoch": 5.793402173492671,
      "grad_norm": 10.291934967041016,
      "learning_rate": 4.517216485542278e-05,
      "loss": 2.2469,
      "step": 149800
    },
    {
      "epoch": 5.797269598174576,
      "grad_norm": 12.52830696105957,
      "learning_rate": 4.516894200152119e-05,
      "loss": 2.3808,
      "step": 149900
    },
    {
      "epoch": 5.80113702285648,
      "grad_norm": 10.181922912597656,
      "learning_rate": 4.51657191476196e-05,
      "loss": 2.2665,
      "step": 150000
    },
    {
      "epoch": 5.805004447538384,
      "grad_norm": 12.085406303405762,
      "learning_rate": 4.5162496293718015e-05,
      "loss": 2.4346,
      "step": 150100
    },
    {
      "epoch": 5.808871872220289,
      "grad_norm": 11.593052864074707,
      "learning_rate": 4.515927343981643e-05,
      "loss": 2.2993,
      "step": 150200
    },
    {
      "epoch": 5.812739296902193,
      "grad_norm": 10.117834091186523,
      "learning_rate": 4.5156050585914844e-05,
      "loss": 2.235,
      "step": 150300
    },
    {
      "epoch": 5.8166067215840975,
      "grad_norm": 11.56397819519043,
      "learning_rate": 4.515282773201325e-05,
      "loss": 2.2906,
      "step": 150400
    },
    {
      "epoch": 5.820474146266001,
      "grad_norm": 12.691473960876465,
      "learning_rate": 4.514960487811167e-05,
      "loss": 2.3945,
      "step": 150500
    },
    {
      "epoch": 5.824341570947906,
      "grad_norm": 14.512802124023438,
      "learning_rate": 4.514638202421008e-05,
      "loss": 2.3181,
      "step": 150600
    },
    {
      "epoch": 5.82820899562981,
      "grad_norm": 8.915377616882324,
      "learning_rate": 4.514315917030849e-05,
      "loss": 2.2719,
      "step": 150700
    },
    {
      "epoch": 5.832076420311714,
      "grad_norm": 10.447049140930176,
      "learning_rate": 4.5139936316406904e-05,
      "loss": 2.3602,
      "step": 150800
    },
    {
      "epoch": 5.8359438449936185,
      "grad_norm": 12.780564308166504,
      "learning_rate": 4.513671346250532e-05,
      "loss": 2.2846,
      "step": 150900
    },
    {
      "epoch": 5.839811269675523,
      "grad_norm": 10.995190620422363,
      "learning_rate": 4.5133490608603734e-05,
      "loss": 2.3487,
      "step": 151000
    },
    {
      "epoch": 5.843678694357427,
      "grad_norm": 12.588932037353516,
      "learning_rate": 4.513026775470214e-05,
      "loss": 2.367,
      "step": 151100
    },
    {
      "epoch": 5.847546119039332,
      "grad_norm": 10.388062477111816,
      "learning_rate": 4.5127044900800556e-05,
      "loss": 2.3153,
      "step": 151200
    },
    {
      "epoch": 5.851413543721236,
      "grad_norm": 11.511999130249023,
      "learning_rate": 4.512382204689897e-05,
      "loss": 2.3679,
      "step": 151300
    },
    {
      "epoch": 5.85528096840314,
      "grad_norm": 11.23633861541748,
      "learning_rate": 4.5120599192997386e-05,
      "loss": 2.4394,
      "step": 151400
    },
    {
      "epoch": 5.859148393085045,
      "grad_norm": 14.4005126953125,
      "learning_rate": 4.5117376339095794e-05,
      "loss": 2.3087,
      "step": 151500
    },
    {
      "epoch": 5.863015817766949,
      "grad_norm": 12.426106452941895,
      "learning_rate": 4.511415348519421e-05,
      "loss": 2.3281,
      "step": 151600
    },
    {
      "epoch": 5.8668832424488535,
      "grad_norm": 12.764134407043457,
      "learning_rate": 4.511093063129262e-05,
      "loss": 2.3606,
      "step": 151700
    },
    {
      "epoch": 5.870750667130758,
      "grad_norm": 9.332873344421387,
      "learning_rate": 4.510770777739104e-05,
      "loss": 2.3588,
      "step": 151800
    },
    {
      "epoch": 5.874618091812662,
      "grad_norm": 12.894227981567383,
      "learning_rate": 4.5104484923489446e-05,
      "loss": 2.2915,
      "step": 151900
    },
    {
      "epoch": 5.878485516494567,
      "grad_norm": 9.414896965026855,
      "learning_rate": 4.510126206958786e-05,
      "loss": 2.3945,
      "step": 152000
    },
    {
      "epoch": 5.882352941176471,
      "grad_norm": 13.108661651611328,
      "learning_rate": 4.5098039215686275e-05,
      "loss": 2.3655,
      "step": 152100
    },
    {
      "epoch": 5.8862203658583745,
      "grad_norm": 13.836821556091309,
      "learning_rate": 4.509481636178469e-05,
      "loss": 2.4011,
      "step": 152200
    },
    {
      "epoch": 5.890087790540279,
      "grad_norm": 13.605450630187988,
      "learning_rate": 4.50915935078831e-05,
      "loss": 2.2606,
      "step": 152300
    },
    {
      "epoch": 5.893955215222183,
      "grad_norm": 10.254502296447754,
      "learning_rate": 4.508837065398151e-05,
      "loss": 2.4685,
      "step": 152400
    },
    {
      "epoch": 5.897822639904088,
      "grad_norm": 14.079923629760742,
      "learning_rate": 4.508514780007993e-05,
      "loss": 2.3086,
      "step": 152500
    },
    {
      "epoch": 5.901690064585992,
      "grad_norm": 12.773409843444824,
      "learning_rate": 4.508192494617834e-05,
      "loss": 2.297,
      "step": 152600
    },
    {
      "epoch": 5.905557489267896,
      "grad_norm": 13.571589469909668,
      "learning_rate": 4.5078702092276757e-05,
      "loss": 2.3422,
      "step": 152700
    },
    {
      "epoch": 5.909424913949801,
      "grad_norm": 16.927852630615234,
      "learning_rate": 4.5075479238375165e-05,
      "loss": 2.3527,
      "step": 152800
    },
    {
      "epoch": 5.913292338631705,
      "grad_norm": 14.06855583190918,
      "learning_rate": 4.507225638447358e-05,
      "loss": 2.2509,
      "step": 152900
    },
    {
      "epoch": 5.9171597633136095,
      "grad_norm": 14.330780029296875,
      "learning_rate": 4.5069033530571994e-05,
      "loss": 2.3069,
      "step": 153000
    },
    {
      "epoch": 5.921027187995514,
      "grad_norm": 12.088488578796387,
      "learning_rate": 4.506581067667041e-05,
      "loss": 2.3805,
      "step": 153100
    },
    {
      "epoch": 5.924894612677418,
      "grad_norm": 12.716817855834961,
      "learning_rate": 4.5062587822768823e-05,
      "loss": 2.3806,
      "step": 153200
    },
    {
      "epoch": 5.928762037359323,
      "grad_norm": 12.171579360961914,
      "learning_rate": 4.505936496886724e-05,
      "loss": 2.296,
      "step": 153300
    },
    {
      "epoch": 5.932629462041227,
      "grad_norm": 14.885003089904785,
      "learning_rate": 4.5056142114965646e-05,
      "loss": 2.2302,
      "step": 153400
    },
    {
      "epoch": 5.936496886723131,
      "grad_norm": 14.378519058227539,
      "learning_rate": 4.505291926106406e-05,
      "loss": 2.3284,
      "step": 153500
    },
    {
      "epoch": 5.940364311405036,
      "grad_norm": 12.79924488067627,
      "learning_rate": 4.5049696407162476e-05,
      "loss": 2.2932,
      "step": 153600
    },
    {
      "epoch": 5.944231736086939,
      "grad_norm": 14.074980735778809,
      "learning_rate": 4.504647355326089e-05,
      "loss": 2.3118,
      "step": 153700
    },
    {
      "epoch": 5.9480991607688445,
      "grad_norm": 9.230062484741211,
      "learning_rate": 4.50432506993593e-05,
      "loss": 2.3216,
      "step": 153800
    },
    {
      "epoch": 5.951966585450748,
      "grad_norm": 13.557811737060547,
      "learning_rate": 4.504002784545771e-05,
      "loss": 2.2569,
      "step": 153900
    },
    {
      "epoch": 5.955834010132652,
      "grad_norm": 12.666983604431152,
      "learning_rate": 4.503680499155613e-05,
      "loss": 2.4417,
      "step": 154000
    },
    {
      "epoch": 5.959701434814557,
      "grad_norm": 15.187362670898438,
      "learning_rate": 4.503358213765454e-05,
      "loss": 2.1826,
      "step": 154100
    },
    {
      "epoch": 5.963568859496461,
      "grad_norm": 14.39295482635498,
      "learning_rate": 4.503035928375295e-05,
      "loss": 2.386,
      "step": 154200
    },
    {
      "epoch": 5.9674362841783655,
      "grad_norm": 11.127165794372559,
      "learning_rate": 4.5027136429851365e-05,
      "loss": 2.326,
      "step": 154300
    },
    {
      "epoch": 5.97130370886027,
      "grad_norm": 10.600661277770996,
      "learning_rate": 4.502391357594978e-05,
      "loss": 2.3775,
      "step": 154400
    },
    {
      "epoch": 5.975171133542174,
      "grad_norm": 12.950332641601562,
      "learning_rate": 4.5020690722048194e-05,
      "loss": 2.3329,
      "step": 154500
    },
    {
      "epoch": 5.979038558224079,
      "grad_norm": 12.012478828430176,
      "learning_rate": 4.50174678681466e-05,
      "loss": 2.382,
      "step": 154600
    },
    {
      "epoch": 5.982905982905983,
      "grad_norm": 13.961502075195312,
      "learning_rate": 4.501424501424502e-05,
      "loss": 2.3131,
      "step": 154700
    },
    {
      "epoch": 5.986773407587887,
      "grad_norm": 13.739663124084473,
      "learning_rate": 4.501102216034343e-05,
      "loss": 2.2537,
      "step": 154800
    },
    {
      "epoch": 5.990640832269792,
      "grad_norm": 10.336491584777832,
      "learning_rate": 4.5007799306441847e-05,
      "loss": 2.3871,
      "step": 154900
    },
    {
      "epoch": 5.994508256951696,
      "grad_norm": 12.274712562561035,
      "learning_rate": 4.5004576452540254e-05,
      "loss": 2.2883,
      "step": 155000
    },
    {
      "epoch": 5.9983756816336005,
      "grad_norm": 13.121318817138672,
      "learning_rate": 4.500135359863867e-05,
      "loss": 2.3273,
      "step": 155100
    },
    {
      "epoch": 6.0,
      "eval_loss": 2.175354242324829,
      "eval_runtime": 6.0737,
      "eval_samples_per_second": 224.08,
      "eval_steps_per_second": 224.08,
      "step": 155142
    },
    {
      "epoch": 6.0,
      "eval_loss": 2.158094644546509,
      "eval_runtime": 114.0365,
      "eval_samples_per_second": 226.743,
      "eval_steps_per_second": 226.743,
      "step": 155142
    },
    {
      "epoch": 6.002243106315505,
      "grad_norm": 10.273045539855957,
      "learning_rate": 4.4998130744737084e-05,
      "loss": 2.3929,
      "step": 155200
    },
    {
      "epoch": 6.006110530997409,
      "grad_norm": 11.721296310424805,
      "learning_rate": 4.49949078908355e-05,
      "loss": 2.226,
      "step": 155300
    },
    {
      "epoch": 6.009977955679314,
      "grad_norm": 11.958680152893066,
      "learning_rate": 4.4991685036933907e-05,
      "loss": 2.3157,
      "step": 155400
    },
    {
      "epoch": 6.013845380361217,
      "grad_norm": 14.147890090942383,
      "learning_rate": 4.498846218303232e-05,
      "loss": 2.2526,
      "step": 155500
    },
    {
      "epoch": 6.0177128050431214,
      "grad_norm": 7.14200496673584,
      "learning_rate": 4.4985239329130736e-05,
      "loss": 2.3346,
      "step": 155600
    },
    {
      "epoch": 6.021580229725026,
      "grad_norm": 12.62277603149414,
      "learning_rate": 4.498201647522915e-05,
      "loss": 2.3242,
      "step": 155700
    },
    {
      "epoch": 6.02544765440693,
      "grad_norm": 15.674485206604004,
      "learning_rate": 4.497879362132756e-05,
      "loss": 2.2801,
      "step": 155800
    },
    {
      "epoch": 6.029315079088835,
      "grad_norm": 12.850018501281738,
      "learning_rate": 4.4975570767425973e-05,
      "loss": 2.2693,
      "step": 155900
    },
    {
      "epoch": 6.033182503770739,
      "grad_norm": 10.517725944519043,
      "learning_rate": 4.497234791352439e-05,
      "loss": 2.3299,
      "step": 156000
    },
    {
      "epoch": 6.037049928452643,
      "grad_norm": 11.406094551086426,
      "learning_rate": 4.49691250596228e-05,
      "loss": 2.2382,
      "step": 156100
    },
    {
      "epoch": 6.040917353134548,
      "grad_norm": 12.925272941589355,
      "learning_rate": 4.496590220572121e-05,
      "loss": 2.2493,
      "step": 156200
    },
    {
      "epoch": 6.044784777816452,
      "grad_norm": 10.566222190856934,
      "learning_rate": 4.4962679351819625e-05,
      "loss": 2.2341,
      "step": 156300
    },
    {
      "epoch": 6.0486522024983564,
      "grad_norm": 11.576814651489258,
      "learning_rate": 4.495945649791804e-05,
      "loss": 2.2361,
      "step": 156400
    },
    {
      "epoch": 6.052519627180261,
      "grad_norm": 11.977063179016113,
      "learning_rate": 4.495623364401645e-05,
      "loss": 2.2465,
      "step": 156500
    },
    {
      "epoch": 6.056387051862165,
      "grad_norm": 14.913019180297852,
      "learning_rate": 4.495301079011486e-05,
      "loss": 2.3287,
      "step": 156600
    },
    {
      "epoch": 6.06025447654407,
      "grad_norm": 14.364986419677734,
      "learning_rate": 4.494978793621328e-05,
      "loss": 2.3702,
      "step": 156700
    },
    {
      "epoch": 6.064121901225974,
      "grad_norm": 10.564715385437012,
      "learning_rate": 4.494656508231169e-05,
      "loss": 2.2621,
      "step": 156800
    },
    {
      "epoch": 6.067989325907878,
      "grad_norm": 10.655891418457031,
      "learning_rate": 4.49433422284101e-05,
      "loss": 2.2886,
      "step": 156900
    },
    {
      "epoch": 6.071856750589783,
      "grad_norm": 8.471016883850098,
      "learning_rate": 4.4940119374508515e-05,
      "loss": 2.2478,
      "step": 157000
    },
    {
      "epoch": 6.075724175271686,
      "grad_norm": 11.195882797241211,
      "learning_rate": 4.493689652060693e-05,
      "loss": 2.201,
      "step": 157100
    },
    {
      "epoch": 6.0795915999535906,
      "grad_norm": 10.295583724975586,
      "learning_rate": 4.4933673666705344e-05,
      "loss": 2.2628,
      "step": 157200
    },
    {
      "epoch": 6.083459024635495,
      "grad_norm": 12.801512718200684,
      "learning_rate": 4.493045081280375e-05,
      "loss": 2.3714,
      "step": 157300
    },
    {
      "epoch": 6.087326449317399,
      "grad_norm": 12.313569068908691,
      "learning_rate": 4.492722795890217e-05,
      "loss": 2.2462,
      "step": 157400
    },
    {
      "epoch": 6.091193873999304,
      "grad_norm": 14.124367713928223,
      "learning_rate": 4.492400510500058e-05,
      "loss": 2.3011,
      "step": 157500
    },
    {
      "epoch": 6.095061298681208,
      "grad_norm": 16.080055236816406,
      "learning_rate": 4.4920782251098996e-05,
      "loss": 2.2502,
      "step": 157600
    },
    {
      "epoch": 6.098928723363112,
      "grad_norm": 10.035037994384766,
      "learning_rate": 4.4917559397197404e-05,
      "loss": 2.2983,
      "step": 157700
    },
    {
      "epoch": 6.102796148045017,
      "grad_norm": 12.619180679321289,
      "learning_rate": 4.491433654329582e-05,
      "loss": 2.2655,
      "step": 157800
    },
    {
      "epoch": 6.106663572726921,
      "grad_norm": 9.27881908416748,
      "learning_rate": 4.4911113689394234e-05,
      "loss": 2.3318,
      "step": 157900
    },
    {
      "epoch": 6.1105309974088255,
      "grad_norm": 13.068303108215332,
      "learning_rate": 4.490789083549265e-05,
      "loss": 2.2813,
      "step": 158000
    },
    {
      "epoch": 6.11439842209073,
      "grad_norm": 13.287738800048828,
      "learning_rate": 4.4904667981591057e-05,
      "loss": 2.3331,
      "step": 158100
    },
    {
      "epoch": 6.118265846772634,
      "grad_norm": 12.101085662841797,
      "learning_rate": 4.490144512768947e-05,
      "loss": 2.3546,
      "step": 158200
    },
    {
      "epoch": 6.122133271454539,
      "grad_norm": 11.89338207244873,
      "learning_rate": 4.4898222273787886e-05,
      "loss": 2.3477,
      "step": 158300
    },
    {
      "epoch": 6.126000696136443,
      "grad_norm": 10.389058113098145,
      "learning_rate": 4.48949994198863e-05,
      "loss": 2.3636,
      "step": 158400
    },
    {
      "epoch": 6.129868120818347,
      "grad_norm": 14.717849731445312,
      "learning_rate": 4.489177656598471e-05,
      "loss": 2.3381,
      "step": 158500
    },
    {
      "epoch": 6.133735545500252,
      "grad_norm": 10.546929359436035,
      "learning_rate": 4.488855371208312e-05,
      "loss": 2.3316,
      "step": 158600
    },
    {
      "epoch": 6.137602970182155,
      "grad_norm": 11.12467098236084,
      "learning_rate": 4.488533085818154e-05,
      "loss": 2.2483,
      "step": 158700
    },
    {
      "epoch": 6.14147039486406,
      "grad_norm": 10.730706214904785,
      "learning_rate": 4.488210800427995e-05,
      "loss": 2.3667,
      "step": 158800
    },
    {
      "epoch": 6.145337819545964,
      "grad_norm": 10.848638534545898,
      "learning_rate": 4.487888515037836e-05,
      "loss": 2.3163,
      "step": 158900
    },
    {
      "epoch": 6.149205244227868,
      "grad_norm": 14.05029010772705,
      "learning_rate": 4.4875662296476775e-05,
      "loss": 2.3039,
      "step": 159000
    },
    {
      "epoch": 6.153072668909773,
      "grad_norm": 11.970884323120117,
      "learning_rate": 4.487243944257519e-05,
      "loss": 2.1446,
      "step": 159100
    },
    {
      "epoch": 6.156940093591677,
      "grad_norm": 12.3070707321167,
      "learning_rate": 4.4869216588673605e-05,
      "loss": 2.3782,
      "step": 159200
    },
    {
      "epoch": 6.1608075182735815,
      "grad_norm": 13.472516059875488,
      "learning_rate": 4.486599373477201e-05,
      "loss": 2.2941,
      "step": 159300
    },
    {
      "epoch": 6.164674942955486,
      "grad_norm": 10.081846237182617,
      "learning_rate": 4.486277088087043e-05,
      "loss": 2.3656,
      "step": 159400
    },
    {
      "epoch": 6.16854236763739,
      "grad_norm": 11.529671669006348,
      "learning_rate": 4.485954802696884e-05,
      "loss": 2.2702,
      "step": 159500
    },
    {
      "epoch": 6.172409792319295,
      "grad_norm": 14.852384567260742,
      "learning_rate": 4.485632517306726e-05,
      "loss": 2.3267,
      "step": 159600
    },
    {
      "epoch": 6.176277217001199,
      "grad_norm": 10.203752517700195,
      "learning_rate": 4.485310231916567e-05,
      "loss": 2.3273,
      "step": 159700
    },
    {
      "epoch": 6.180144641683103,
      "grad_norm": 14.050339698791504,
      "learning_rate": 4.484987946526408e-05,
      "loss": 2.2287,
      "step": 159800
    },
    {
      "epoch": 6.184012066365008,
      "grad_norm": 12.387970924377441,
      "learning_rate": 4.4846656611362494e-05,
      "loss": 2.3656,
      "step": 159900
    },
    {
      "epoch": 6.187879491046912,
      "grad_norm": 13.093062400817871,
      "learning_rate": 4.484343375746091e-05,
      "loss": 2.3141,
      "step": 160000
    },
    {
      "epoch": 6.1917469157288165,
      "grad_norm": 13.04067611694336,
      "learning_rate": 4.4840210903559324e-05,
      "loss": 2.3567,
      "step": 160100
    },
    {
      "epoch": 6.195614340410721,
      "grad_norm": 10.971131324768066,
      "learning_rate": 4.483698804965774e-05,
      "loss": 2.2816,
      "step": 160200
    },
    {
      "epoch": 6.199481765092624,
      "grad_norm": 10.72702407836914,
      "learning_rate": 4.483376519575615e-05,
      "loss": 2.2498,
      "step": 160300
    },
    {
      "epoch": 6.203349189774529,
      "grad_norm": 12.886380195617676,
      "learning_rate": 4.483054234185456e-05,
      "loss": 2.2565,
      "step": 160400
    },
    {
      "epoch": 6.207216614456433,
      "grad_norm": 13.236305236816406,
      "learning_rate": 4.4827319487952976e-05,
      "loss": 2.3165,
      "step": 160500
    },
    {
      "epoch": 6.2110840391383375,
      "grad_norm": 12.124499320983887,
      "learning_rate": 4.482409663405139e-05,
      "loss": 2.2249,
      "step": 160600
    },
    {
      "epoch": 6.214951463820242,
      "grad_norm": 15.398656845092773,
      "learning_rate": 4.4820873780149805e-05,
      "loss": 2.2038,
      "step": 160700
    },
    {
      "epoch": 6.218818888502146,
      "grad_norm": 13.971338272094727,
      "learning_rate": 4.481765092624821e-05,
      "loss": 2.2603,
      "step": 160800
    },
    {
      "epoch": 6.222686313184051,
      "grad_norm": 13.173385620117188,
      "learning_rate": 4.481442807234663e-05,
      "loss": 2.318,
      "step": 160900
    },
    {
      "epoch": 6.226553737865955,
      "grad_norm": 13.663077354431152,
      "learning_rate": 4.481120521844504e-05,
      "loss": 2.4104,
      "step": 161000
    },
    {
      "epoch": 6.230421162547859,
      "grad_norm": 12.359652519226074,
      "learning_rate": 4.480798236454346e-05,
      "loss": 2.3066,
      "step": 161100
    },
    {
      "epoch": 6.234288587229764,
      "grad_norm": 11.544181823730469,
      "learning_rate": 4.4804759510641865e-05,
      "loss": 2.2887,
      "step": 161200
    },
    {
      "epoch": 6.238156011911668,
      "grad_norm": 12.286558151245117,
      "learning_rate": 4.480153665674028e-05,
      "loss": 2.2996,
      "step": 161300
    },
    {
      "epoch": 6.2420234365935725,
      "grad_norm": 10.923833847045898,
      "learning_rate": 4.4798313802838695e-05,
      "loss": 2.2346,
      "step": 161400
    },
    {
      "epoch": 6.245890861275477,
      "grad_norm": 12.092007637023926,
      "learning_rate": 4.479509094893711e-05,
      "loss": 2.2887,
      "step": 161500
    },
    {
      "epoch": 6.249758285957381,
      "grad_norm": 12.849008560180664,
      "learning_rate": 4.479186809503552e-05,
      "loss": 2.3057,
      "step": 161600
    },
    {
      "epoch": 6.253625710639286,
      "grad_norm": 14.82111644744873,
      "learning_rate": 4.478864524113393e-05,
      "loss": 2.3143,
      "step": 161700
    },
    {
      "epoch": 6.25749313532119,
      "grad_norm": 10.764193534851074,
      "learning_rate": 4.478542238723235e-05,
      "loss": 2.2404,
      "step": 161800
    },
    {
      "epoch": 6.261360560003094,
      "grad_norm": 11.349038124084473,
      "learning_rate": 4.478219953333076e-05,
      "loss": 2.3026,
      "step": 161900
    },
    {
      "epoch": 6.265227984684998,
      "grad_norm": 11.029356956481934,
      "learning_rate": 4.477897667942917e-05,
      "loss": 2.2862,
      "step": 162000
    },
    {
      "epoch": 6.269095409366902,
      "grad_norm": 11.325286865234375,
      "learning_rate": 4.4775753825527584e-05,
      "loss": 2.2789,
      "step": 162100
    },
    {
      "epoch": 6.272962834048807,
      "grad_norm": 14.126323699951172,
      "learning_rate": 4.4772530971626e-05,
      "loss": 2.3229,
      "step": 162200
    },
    {
      "epoch": 6.276830258730711,
      "grad_norm": 11.326254844665527,
      "learning_rate": 4.476930811772441e-05,
      "loss": 2.2539,
      "step": 162300
    },
    {
      "epoch": 6.280697683412615,
      "grad_norm": 12.628454208374023,
      "learning_rate": 4.476608526382282e-05,
      "loss": 2.2689,
      "step": 162400
    },
    {
      "epoch": 6.28456510809452,
      "grad_norm": 13.52325439453125,
      "learning_rate": 4.4762862409921236e-05,
      "loss": 2.3749,
      "step": 162500
    },
    {
      "epoch": 6.288432532776424,
      "grad_norm": 11.570672988891602,
      "learning_rate": 4.475963955601965e-05,
      "loss": 2.3173,
      "step": 162600
    },
    {
      "epoch": 6.2922999574583285,
      "grad_norm": 12.432345390319824,
      "learning_rate": 4.475641670211806e-05,
      "loss": 2.2854,
      "step": 162700
    },
    {
      "epoch": 6.296167382140233,
      "grad_norm": 11.962895393371582,
      "learning_rate": 4.4753193848216474e-05,
      "loss": 2.2947,
      "step": 162800
    },
    {
      "epoch": 6.300034806822137,
      "grad_norm": 12.463229179382324,
      "learning_rate": 4.474997099431489e-05,
      "loss": 2.2911,
      "step": 162900
    },
    {
      "epoch": 6.303902231504042,
      "grad_norm": 15.709867477416992,
      "learning_rate": 4.47467481404133e-05,
      "loss": 2.3816,
      "step": 163000
    },
    {
      "epoch": 6.307769656185946,
      "grad_norm": 11.045437812805176,
      "learning_rate": 4.474352528651171e-05,
      "loss": 2.2561,
      "step": 163100
    },
    {
      "epoch": 6.31163708086785,
      "grad_norm": 12.22720718383789,
      "learning_rate": 4.4740302432610126e-05,
      "loss": 2.265,
      "step": 163200
    },
    {
      "epoch": 6.315504505549755,
      "grad_norm": 12.417768478393555,
      "learning_rate": 4.473707957870854e-05,
      "loss": 2.2907,
      "step": 163300
    },
    {
      "epoch": 6.319371930231659,
      "grad_norm": 11.529210090637207,
      "learning_rate": 4.4733856724806955e-05,
      "loss": 2.282,
      "step": 163400
    },
    {
      "epoch": 6.323239354913563,
      "grad_norm": 9.682816505432129,
      "learning_rate": 4.473063387090536e-05,
      "loss": 2.3352,
      "step": 163500
    },
    {
      "epoch": 6.327106779595467,
      "grad_norm": 13.678802490234375,
      "learning_rate": 4.472741101700378e-05,
      "loss": 2.3261,
      "step": 163600
    },
    {
      "epoch": 6.330974204277371,
      "grad_norm": 10.63836669921875,
      "learning_rate": 4.472418816310219e-05,
      "loss": 2.3372,
      "step": 163700
    },
    {
      "epoch": 6.334841628959276,
      "grad_norm": 14.643939971923828,
      "learning_rate": 4.472096530920061e-05,
      "loss": 2.3047,
      "step": 163800
    },
    {
      "epoch": 6.33870905364118,
      "grad_norm": 10.10888385772705,
      "learning_rate": 4.4717742455299015e-05,
      "loss": 2.2258,
      "step": 163900
    },
    {
      "epoch": 6.3425764783230845,
      "grad_norm": 11.246588706970215,
      "learning_rate": 4.471451960139743e-05,
      "loss": 2.3442,
      "step": 164000
    },
    {
      "epoch": 6.346443903004989,
      "grad_norm": 13.584208488464355,
      "learning_rate": 4.4711296747495845e-05,
      "loss": 2.2029,
      "step": 164100
    },
    {
      "epoch": 6.350311327686893,
      "grad_norm": 11.4193115234375,
      "learning_rate": 4.470807389359426e-05,
      "loss": 2.2393,
      "step": 164200
    },
    {
      "epoch": 6.354178752368798,
      "grad_norm": 11.03315544128418,
      "learning_rate": 4.470485103969267e-05,
      "loss": 2.311,
      "step": 164300
    },
    {
      "epoch": 6.358046177050702,
      "grad_norm": 10.274169921875,
      "learning_rate": 4.470162818579108e-05,
      "loss": 2.2495,
      "step": 164400
    },
    {
      "epoch": 6.361913601732606,
      "grad_norm": 11.967653274536133,
      "learning_rate": 4.46984053318895e-05,
      "loss": 2.2952,
      "step": 164500
    },
    {
      "epoch": 6.365781026414511,
      "grad_norm": 11.370384216308594,
      "learning_rate": 4.469518247798791e-05,
      "loss": 2.3335,
      "step": 164600
    },
    {
      "epoch": 6.369648451096415,
      "grad_norm": 11.68369197845459,
      "learning_rate": 4.469195962408632e-05,
      "loss": 2.2162,
      "step": 164700
    },
    {
      "epoch": 6.3735158757783195,
      "grad_norm": 12.435985565185547,
      "learning_rate": 4.4688736770184734e-05,
      "loss": 2.3224,
      "step": 164800
    },
    {
      "epoch": 6.377383300460224,
      "grad_norm": 12.739595413208008,
      "learning_rate": 4.468551391628315e-05,
      "loss": 2.281,
      "step": 164900
    },
    {
      "epoch": 6.381250725142128,
      "grad_norm": 9.890226364135742,
      "learning_rate": 4.4682291062381564e-05,
      "loss": 2.3309,
      "step": 165000
    },
    {
      "epoch": 6.385118149824033,
      "grad_norm": 14.782671928405762,
      "learning_rate": 4.467906820847997e-05,
      "loss": 2.3513,
      "step": 165100
    },
    {
      "epoch": 6.388985574505936,
      "grad_norm": 11.567422866821289,
      "learning_rate": 4.4675845354578386e-05,
      "loss": 2.3123,
      "step": 165200
    },
    {
      "epoch": 6.3928529991878404,
      "grad_norm": 14.592717170715332,
      "learning_rate": 4.46726225006768e-05,
      "loss": 2.2318,
      "step": 165300
    },
    {
      "epoch": 6.396720423869745,
      "grad_norm": 10.961664199829102,
      "learning_rate": 4.466939964677521e-05,
      "loss": 2.3516,
      "step": 165400
    },
    {
      "epoch": 6.400587848551649,
      "grad_norm": 11.434188842773438,
      "learning_rate": 4.4666176792873624e-05,
      "loss": 2.179,
      "step": 165500
    },
    {
      "epoch": 6.404455273233554,
      "grad_norm": 12.569928169250488,
      "learning_rate": 4.466295393897204e-05,
      "loss": 2.3185,
      "step": 165600
    },
    {
      "epoch": 6.408322697915458,
      "grad_norm": 11.0455904006958,
      "learning_rate": 4.465973108507045e-05,
      "loss": 2.2279,
      "step": 165700
    },
    {
      "epoch": 6.412190122597362,
      "grad_norm": 16.268224716186523,
      "learning_rate": 4.465650823116886e-05,
      "loss": 2.1918,
      "step": 165800
    },
    {
      "epoch": 6.416057547279267,
      "grad_norm": 14.54850959777832,
      "learning_rate": 4.4653285377267276e-05,
      "loss": 2.2471,
      "step": 165900
    },
    {
      "epoch": 6.419924971961171,
      "grad_norm": 13.347646713256836,
      "learning_rate": 4.465006252336569e-05,
      "loss": 2.2383,
      "step": 166000
    },
    {
      "epoch": 6.423792396643075,
      "grad_norm": 10.729731559753418,
      "learning_rate": 4.4646839669464105e-05,
      "loss": 2.2488,
      "step": 166100
    },
    {
      "epoch": 6.42765982132498,
      "grad_norm": 12.694572448730469,
      "learning_rate": 4.464361681556252e-05,
      "loss": 2.3669,
      "step": 166200
    },
    {
      "epoch": 6.431527246006884,
      "grad_norm": 13.352538108825684,
      "learning_rate": 4.464039396166093e-05,
      "loss": 2.3308,
      "step": 166300
    },
    {
      "epoch": 6.435394670688789,
      "grad_norm": 11.21689224243164,
      "learning_rate": 4.463717110775934e-05,
      "loss": 2.3035,
      "step": 166400
    },
    {
      "epoch": 6.439262095370693,
      "grad_norm": 11.040016174316406,
      "learning_rate": 4.463394825385776e-05,
      "loss": 2.2835,
      "step": 166500
    },
    {
      "epoch": 6.443129520052597,
      "grad_norm": 10.48513126373291,
      "learning_rate": 4.463072539995617e-05,
      "loss": 2.227,
      "step": 166600
    },
    {
      "epoch": 6.446996944734502,
      "grad_norm": 15.883793830871582,
      "learning_rate": 4.462750254605459e-05,
      "loss": 2.3474,
      "step": 166700
    },
    {
      "epoch": 6.450864369416406,
      "grad_norm": 11.783021926879883,
      "learning_rate": 4.4624279692153e-05,
      "loss": 2.3151,
      "step": 166800
    },
    {
      "epoch": 6.4547317940983095,
      "grad_norm": 10.241401672363281,
      "learning_rate": 4.462105683825141e-05,
      "loss": 2.2924,
      "step": 166900
    },
    {
      "epoch": 6.458599218780214,
      "grad_norm": 14.729707717895508,
      "learning_rate": 4.4617833984349824e-05,
      "loss": 2.345,
      "step": 167000
    },
    {
      "epoch": 6.462466643462118,
      "grad_norm": 13.954041481018066,
      "learning_rate": 4.461461113044824e-05,
      "loss": 2.3612,
      "step": 167100
    },
    {
      "epoch": 6.466334068144023,
      "grad_norm": 12.489351272583008,
      "learning_rate": 4.4611388276546654e-05,
      "loss": 2.316,
      "step": 167200
    },
    {
      "epoch": 6.470201492825927,
      "grad_norm": 11.281707763671875,
      "learning_rate": 4.460816542264507e-05,
      "loss": 2.2773,
      "step": 167300
    },
    {
      "epoch": 6.474068917507831,
      "grad_norm": 13.191518783569336,
      "learning_rate": 4.4604942568743476e-05,
      "loss": 2.2758,
      "step": 167400
    },
    {
      "epoch": 6.477936342189736,
      "grad_norm": 10.07768726348877,
      "learning_rate": 4.460171971484189e-05,
      "loss": 2.2759,
      "step": 167500
    },
    {
      "epoch": 6.48180376687164,
      "grad_norm": 13.093249320983887,
      "learning_rate": 4.4598496860940306e-05,
      "loss": 2.2587,
      "step": 167600
    },
    {
      "epoch": 6.4856711915535445,
      "grad_norm": 12.95169448852539,
      "learning_rate": 4.459527400703872e-05,
      "loss": 2.3024,
      "step": 167700
    },
    {
      "epoch": 6.489538616235449,
      "grad_norm": 11.45022964477539,
      "learning_rate": 4.459205115313713e-05,
      "loss": 2.3445,
      "step": 167800
    },
    {
      "epoch": 6.493406040917353,
      "grad_norm": 14.600977897644043,
      "learning_rate": 4.458882829923554e-05,
      "loss": 2.2798,
      "step": 167900
    },
    {
      "epoch": 6.497273465599258,
      "grad_norm": 14.033334732055664,
      "learning_rate": 4.458560544533396e-05,
      "loss": 2.3507,
      "step": 168000
    },
    {
      "epoch": 6.501140890281162,
      "grad_norm": 14.803410530090332,
      "learning_rate": 4.458238259143237e-05,
      "loss": 2.27,
      "step": 168100
    },
    {
      "epoch": 6.505008314963066,
      "grad_norm": 10.639187812805176,
      "learning_rate": 4.457915973753078e-05,
      "loss": 2.2732,
      "step": 168200
    },
    {
      "epoch": 6.508875739644971,
      "grad_norm": 10.20059585571289,
      "learning_rate": 4.4575936883629195e-05,
      "loss": 2.332,
      "step": 168300
    },
    {
      "epoch": 6.512743164326874,
      "grad_norm": 9.881566047668457,
      "learning_rate": 4.457271402972761e-05,
      "loss": 2.2235,
      "step": 168400
    },
    {
      "epoch": 6.5166105890087795,
      "grad_norm": 11.692581176757812,
      "learning_rate": 4.456949117582602e-05,
      "loss": 2.3366,
      "step": 168500
    },
    {
      "epoch": 6.520478013690683,
      "grad_norm": 11.291296005249023,
      "learning_rate": 4.456626832192443e-05,
      "loss": 2.2999,
      "step": 168600
    },
    {
      "epoch": 6.524345438372587,
      "grad_norm": 13.266952514648438,
      "learning_rate": 4.456304546802285e-05,
      "loss": 2.3062,
      "step": 168700
    },
    {
      "epoch": 6.528212863054492,
      "grad_norm": 11.8084716796875,
      "learning_rate": 4.455982261412126e-05,
      "loss": 2.3345,
      "step": 168800
    },
    {
      "epoch": 6.532080287736396,
      "grad_norm": 10.892831802368164,
      "learning_rate": 4.455659976021967e-05,
      "loss": 2.3719,
      "step": 168900
    },
    {
      "epoch": 6.5359477124183005,
      "grad_norm": 10.7061767578125,
      "learning_rate": 4.4553376906318085e-05,
      "loss": 2.3573,
      "step": 169000
    },
    {
      "epoch": 6.539815137100205,
      "grad_norm": 11.811552047729492,
      "learning_rate": 4.45501540524165e-05,
      "loss": 2.3147,
      "step": 169100
    },
    {
      "epoch": 6.543682561782109,
      "grad_norm": 11.121356010437012,
      "learning_rate": 4.4546931198514914e-05,
      "loss": 2.2975,
      "step": 169200
    },
    {
      "epoch": 6.547549986464014,
      "grad_norm": 13.574260711669922,
      "learning_rate": 4.454370834461332e-05,
      "loss": 2.2892,
      "step": 169300
    },
    {
      "epoch": 6.551417411145918,
      "grad_norm": 14.587419509887695,
      "learning_rate": 4.454048549071174e-05,
      "loss": 2.2848,
      "step": 169400
    },
    {
      "epoch": 6.555284835827822,
      "grad_norm": 15.176835060119629,
      "learning_rate": 4.453726263681015e-05,
      "loss": 2.3364,
      "step": 169500
    },
    {
      "epoch": 6.559152260509727,
      "grad_norm": 11.080353736877441,
      "learning_rate": 4.4534039782908566e-05,
      "loss": 2.1375,
      "step": 169600
    },
    {
      "epoch": 6.563019685191631,
      "grad_norm": 11.46362018585205,
      "learning_rate": 4.4530816929006974e-05,
      "loss": 2.2432,
      "step": 169700
    },
    {
      "epoch": 6.5668871098735355,
      "grad_norm": 12.221271514892578,
      "learning_rate": 4.452759407510539e-05,
      "loss": 2.282,
      "step": 169800
    },
    {
      "epoch": 6.57075453455544,
      "grad_norm": 10.446866035461426,
      "learning_rate": 4.4524371221203804e-05,
      "loss": 2.2447,
      "step": 169900
    },
    {
      "epoch": 6.574621959237344,
      "grad_norm": 13.142581939697266,
      "learning_rate": 4.452114836730222e-05,
      "loss": 2.3009,
      "step": 170000
    },
    {
      "epoch": 6.578489383919248,
      "grad_norm": 10.567658424377441,
      "learning_rate": 4.4517925513400626e-05,
      "loss": 2.188,
      "step": 170100
    },
    {
      "epoch": 6.582356808601152,
      "grad_norm": 11.425747871398926,
      "learning_rate": 4.451470265949904e-05,
      "loss": 2.2575,
      "step": 170200
    },
    {
      "epoch": 6.5862242332830565,
      "grad_norm": 11.843059539794922,
      "learning_rate": 4.4511479805597456e-05,
      "loss": 2.1766,
      "step": 170300
    },
    {
      "epoch": 6.590091657964961,
      "grad_norm": 11.620388984680176,
      "learning_rate": 4.450825695169587e-05,
      "loss": 2.3031,
      "step": 170400
    },
    {
      "epoch": 6.593959082646865,
      "grad_norm": 10.52685546875,
      "learning_rate": 4.450503409779428e-05,
      "loss": 2.2449,
      "step": 170500
    },
    {
      "epoch": 6.59782650732877,
      "grad_norm": 11.926066398620605,
      "learning_rate": 4.450181124389269e-05,
      "loss": 2.2593,
      "step": 170600
    },
    {
      "epoch": 6.601693932010674,
      "grad_norm": 10.318955421447754,
      "learning_rate": 4.449858838999111e-05,
      "loss": 2.3289,
      "step": 170700
    },
    {
      "epoch": 6.605561356692578,
      "grad_norm": 13.618081092834473,
      "learning_rate": 4.449536553608952e-05,
      "loss": 2.2662,
      "step": 170800
    },
    {
      "epoch": 6.609428781374483,
      "grad_norm": 13.396992683410645,
      "learning_rate": 4.449214268218793e-05,
      "loss": 2.2957,
      "step": 170900
    },
    {
      "epoch": 6.613296206056387,
      "grad_norm": 10.981281280517578,
      "learning_rate": 4.4488919828286345e-05,
      "loss": 2.3971,
      "step": 171000
    },
    {
      "epoch": 6.6171636307382915,
      "grad_norm": 15.36192798614502,
      "learning_rate": 4.448569697438476e-05,
      "loss": 2.3497,
      "step": 171100
    },
    {
      "epoch": 6.621031055420196,
      "grad_norm": 10.041831016540527,
      "learning_rate": 4.448247412048317e-05,
      "loss": 2.2955,
      "step": 171200
    },
    {
      "epoch": 6.6248984801021,
      "grad_norm": 12.422196388244629,
      "learning_rate": 4.447925126658158e-05,
      "loss": 2.2139,
      "step": 171300
    },
    {
      "epoch": 6.628765904784005,
      "grad_norm": 12.360346794128418,
      "learning_rate": 4.447602841268e-05,
      "loss": 2.291,
      "step": 171400
    },
    {
      "epoch": 6.632633329465909,
      "grad_norm": 10.878562927246094,
      "learning_rate": 4.447280555877841e-05,
      "loss": 2.3211,
      "step": 171500
    },
    {
      "epoch": 6.6365007541478125,
      "grad_norm": 10.762721061706543,
      "learning_rate": 4.446958270487682e-05,
      "loss": 2.3452,
      "step": 171600
    },
    {
      "epoch": 6.640368178829718,
      "grad_norm": 15.205671310424805,
      "learning_rate": 4.4466359850975235e-05,
      "loss": 2.3065,
      "step": 171700
    },
    {
      "epoch": 6.644235603511621,
      "grad_norm": 10.206686973571777,
      "learning_rate": 4.446313699707365e-05,
      "loss": 2.2236,
      "step": 171800
    },
    {
      "epoch": 6.648103028193526,
      "grad_norm": 10.37610149383545,
      "learning_rate": 4.4459914143172064e-05,
      "loss": 2.3694,
      "step": 171900
    },
    {
      "epoch": 6.65197045287543,
      "grad_norm": 12.959606170654297,
      "learning_rate": 4.445669128927047e-05,
      "loss": 2.251,
      "step": 172000
    },
    {
      "epoch": 6.655837877557334,
      "grad_norm": 8.814438819885254,
      "learning_rate": 4.445346843536889e-05,
      "loss": 2.4035,
      "step": 172100
    },
    {
      "epoch": 6.659705302239239,
      "grad_norm": 12.166178703308105,
      "learning_rate": 4.44502455814673e-05,
      "loss": 2.3334,
      "step": 172200
    },
    {
      "epoch": 6.663572726921143,
      "grad_norm": 13.878923416137695,
      "learning_rate": 4.4447022727565716e-05,
      "loss": 2.3304,
      "step": 172300
    },
    {
      "epoch": 6.6674401516030475,
      "grad_norm": 11.344419479370117,
      "learning_rate": 4.4443799873664124e-05,
      "loss": 2.2936,
      "step": 172400
    },
    {
      "epoch": 6.671307576284952,
      "grad_norm": 13.302605628967285,
      "learning_rate": 4.444057701976254e-05,
      "loss": 2.31,
      "step": 172500
    },
    {
      "epoch": 6.675175000966856,
      "grad_norm": 12.280543327331543,
      "learning_rate": 4.4437354165860954e-05,
      "loss": 2.2909,
      "step": 172600
    },
    {
      "epoch": 6.679042425648761,
      "grad_norm": 10.552444458007812,
      "learning_rate": 4.443413131195937e-05,
      "loss": 2.2297,
      "step": 172700
    },
    {
      "epoch": 6.682909850330665,
      "grad_norm": 9.722687721252441,
      "learning_rate": 4.4430908458057776e-05,
      "loss": 2.2709,
      "step": 172800
    },
    {
      "epoch": 6.686777275012569,
      "grad_norm": 10.611647605895996,
      "learning_rate": 4.442768560415619e-05,
      "loss": 2.3025,
      "step": 172900
    },
    {
      "epoch": 6.690644699694474,
      "grad_norm": 11.019803047180176,
      "learning_rate": 4.4424462750254606e-05,
      "loss": 2.2298,
      "step": 173000
    },
    {
      "epoch": 6.694512124376378,
      "grad_norm": 17.043840408325195,
      "learning_rate": 4.442123989635302e-05,
      "loss": 2.3519,
      "step": 173100
    },
    {
      "epoch": 6.6983795490582825,
      "grad_norm": 16.55328941345215,
      "learning_rate": 4.4418017042451435e-05,
      "loss": 2.3313,
      "step": 173200
    },
    {
      "epoch": 6.702246973740186,
      "grad_norm": 11.343931198120117,
      "learning_rate": 4.441479418854985e-05,
      "loss": 2.2656,
      "step": 173300
    },
    {
      "epoch": 6.706114398422091,
      "grad_norm": 12.155753135681152,
      "learning_rate": 4.441157133464826e-05,
      "loss": 2.318,
      "step": 173400
    },
    {
      "epoch": 6.709981823103995,
      "grad_norm": 10.442686080932617,
      "learning_rate": 4.440834848074667e-05,
      "loss": 2.2828,
      "step": 173500
    },
    {
      "epoch": 6.713849247785899,
      "grad_norm": 10.58623218536377,
      "learning_rate": 4.440512562684509e-05,
      "loss": 2.1955,
      "step": 173600
    },
    {
      "epoch": 6.7177166724678035,
      "grad_norm": 13.658815383911133,
      "learning_rate": 4.44019027729435e-05,
      "loss": 2.2029,
      "step": 173700
    },
    {
      "epoch": 6.721584097149708,
      "grad_norm": 13.48212718963623,
      "learning_rate": 4.4398679919041917e-05,
      "loss": 2.3062,
      "step": 173800
    },
    {
      "epoch": 6.725451521831612,
      "grad_norm": 14.284507751464844,
      "learning_rate": 4.4395457065140325e-05,
      "loss": 2.2639,
      "step": 173900
    },
    {
      "epoch": 6.729318946513517,
      "grad_norm": 12.111263275146484,
      "learning_rate": 4.439223421123874e-05,
      "loss": 2.3273,
      "step": 174000
    },
    {
      "epoch": 6.733186371195421,
      "grad_norm": 12.876860618591309,
      "learning_rate": 4.4389011357337154e-05,
      "loss": 2.3537,
      "step": 174100
    },
    {
      "epoch": 6.737053795877325,
      "grad_norm": 13.200705528259277,
      "learning_rate": 4.438578850343557e-05,
      "loss": 2.2747,
      "step": 174200
    },
    {
      "epoch": 6.74092122055923,
      "grad_norm": 10.11650562286377,
      "learning_rate": 4.438256564953398e-05,
      "loss": 2.2565,
      "step": 174300
    },
    {
      "epoch": 6.744788645241134,
      "grad_norm": 13.513256072998047,
      "learning_rate": 4.437934279563239e-05,
      "loss": 2.2373,
      "step": 174400
    },
    {
      "epoch": 6.7486560699230385,
      "grad_norm": 13.386208534240723,
      "learning_rate": 4.4376119941730806e-05,
      "loss": 2.3414,
      "step": 174500
    },
    {
      "epoch": 6.752523494604943,
      "grad_norm": 10.824552536010742,
      "learning_rate": 4.437289708782922e-05,
      "loss": 2.2322,
      "step": 174600
    },
    {
      "epoch": 6.756390919286847,
      "grad_norm": 11.72589111328125,
      "learning_rate": 4.436967423392763e-05,
      "loss": 2.3263,
      "step": 174700
    },
    {
      "epoch": 6.760258343968752,
      "grad_norm": 11.190205574035645,
      "learning_rate": 4.4366451380026043e-05,
      "loss": 2.0799,
      "step": 174800
    },
    {
      "epoch": 6.764125768650656,
      "grad_norm": 13.298381805419922,
      "learning_rate": 4.436322852612446e-05,
      "loss": 2.3241,
      "step": 174900
    },
    {
      "epoch": 6.767993193332559,
      "grad_norm": 12.197019577026367,
      "learning_rate": 4.436000567222287e-05,
      "loss": 2.2967,
      "step": 175000
    },
    {
      "epoch": 6.771860618014464,
      "grad_norm": 20.523866653442383,
      "learning_rate": 4.435678281832128e-05,
      "loss": 2.4125,
      "step": 175100
    },
    {
      "epoch": 6.775728042696368,
      "grad_norm": 11.772154808044434,
      "learning_rate": 4.4353559964419696e-05,
      "loss": 2.3027,
      "step": 175200
    },
    {
      "epoch": 6.779595467378273,
      "grad_norm": 12.66750717163086,
      "learning_rate": 4.435033711051811e-05,
      "loss": 2.2596,
      "step": 175300
    },
    {
      "epoch": 6.783462892060177,
      "grad_norm": 13.072973251342773,
      "learning_rate": 4.4347114256616525e-05,
      "loss": 2.2983,
      "step": 175400
    },
    {
      "epoch": 6.787330316742081,
      "grad_norm": 12.584729194641113,
      "learning_rate": 4.434389140271493e-05,
      "loss": 2.3533,
      "step": 175500
    },
    {
      "epoch": 6.791197741423986,
      "grad_norm": 11.20864486694336,
      "learning_rate": 4.434066854881335e-05,
      "loss": 2.2993,
      "step": 175600
    },
    {
      "epoch": 6.79506516610589,
      "grad_norm": 10.1849365234375,
      "learning_rate": 4.433744569491176e-05,
      "loss": 2.2097,
      "step": 175700
    },
    {
      "epoch": 6.798932590787794,
      "grad_norm": 10.732061386108398,
      "learning_rate": 4.433422284101018e-05,
      "loss": 2.2225,
      "step": 175800
    },
    {
      "epoch": 6.802800015469699,
      "grad_norm": 17.412269592285156,
      "learning_rate": 4.4330999987108585e-05,
      "loss": 2.29,
      "step": 175900
    },
    {
      "epoch": 6.806667440151603,
      "grad_norm": 11.433286666870117,
      "learning_rate": 4.4327777133207e-05,
      "loss": 2.2235,
      "step": 176000
    },
    {
      "epoch": 6.810534864833508,
      "grad_norm": 10.584070205688477,
      "learning_rate": 4.4324554279305414e-05,
      "loss": 2.3194,
      "step": 176100
    },
    {
      "epoch": 6.814402289515412,
      "grad_norm": 13.65460205078125,
      "learning_rate": 4.432133142540383e-05,
      "loss": 2.2939,
      "step": 176200
    },
    {
      "epoch": 6.818269714197316,
      "grad_norm": 13.000483512878418,
      "learning_rate": 4.431810857150224e-05,
      "loss": 2.3511,
      "step": 176300
    },
    {
      "epoch": 6.822137138879221,
      "grad_norm": 11.260722160339355,
      "learning_rate": 4.431488571760065e-05,
      "loss": 2.3122,
      "step": 176400
    },
    {
      "epoch": 6.826004563561124,
      "grad_norm": 12.940215110778809,
      "learning_rate": 4.4311662863699067e-05,
      "loss": 2.1468,
      "step": 176500
    },
    {
      "epoch": 6.829871988243029,
      "grad_norm": 15.35579776763916,
      "learning_rate": 4.430844000979748e-05,
      "loss": 2.4029,
      "step": 176600
    },
    {
      "epoch": 6.833739412924933,
      "grad_norm": 13.277205467224121,
      "learning_rate": 4.430521715589589e-05,
      "loss": 2.3059,
      "step": 176700
    },
    {
      "epoch": 6.837606837606837,
      "grad_norm": 12.904909133911133,
      "learning_rate": 4.4301994301994304e-05,
      "loss": 2.3481,
      "step": 176800
    },
    {
      "epoch": 6.841474262288742,
      "grad_norm": 10.774064064025879,
      "learning_rate": 4.429877144809272e-05,
      "loss": 2.2635,
      "step": 176900
    },
    {
      "epoch": 6.845341686970646,
      "grad_norm": 7.854698657989502,
      "learning_rate": 4.429554859419113e-05,
      "loss": 2.3027,
      "step": 177000
    },
    {
      "epoch": 6.84920911165255,
      "grad_norm": 12.817441940307617,
      "learning_rate": 4.429232574028954e-05,
      "loss": 2.2861,
      "step": 177100
    },
    {
      "epoch": 6.853076536334455,
      "grad_norm": 10.525808334350586,
      "learning_rate": 4.4289102886387956e-05,
      "loss": 2.353,
      "step": 177200
    },
    {
      "epoch": 6.856943961016359,
      "grad_norm": 11.994364738464355,
      "learning_rate": 4.428588003248637e-05,
      "loss": 2.2154,
      "step": 177300
    },
    {
      "epoch": 6.8608113856982635,
      "grad_norm": 14.256208419799805,
      "learning_rate": 4.428265717858478e-05,
      "loss": 2.2579,
      "step": 177400
    },
    {
      "epoch": 6.864678810380168,
      "grad_norm": 12.627713203430176,
      "learning_rate": 4.4279434324683193e-05,
      "loss": 2.3088,
      "step": 177500
    },
    {
      "epoch": 6.868546235062072,
      "grad_norm": 9.70182991027832,
      "learning_rate": 4.427621147078161e-05,
      "loss": 2.3293,
      "step": 177600
    },
    {
      "epoch": 6.872413659743977,
      "grad_norm": 10.71340274810791,
      "learning_rate": 4.427298861688002e-05,
      "loss": 2.2505,
      "step": 177700
    },
    {
      "epoch": 6.876281084425881,
      "grad_norm": 11.346648216247559,
      "learning_rate": 4.426976576297843e-05,
      "loss": 2.2897,
      "step": 177800
    },
    {
      "epoch": 6.880148509107785,
      "grad_norm": 11.68632984161377,
      "learning_rate": 4.4266542909076846e-05,
      "loss": 2.2726,
      "step": 177900
    },
    {
      "epoch": 6.88401593378969,
      "grad_norm": 12.162912368774414,
      "learning_rate": 4.426332005517526e-05,
      "loss": 2.3801,
      "step": 178000
    },
    {
      "epoch": 6.887883358471594,
      "grad_norm": 13.435456275939941,
      "learning_rate": 4.4260097201273675e-05,
      "loss": 2.1827,
      "step": 178100
    },
    {
      "epoch": 6.891750783153498,
      "grad_norm": 11.568901062011719,
      "learning_rate": 4.425687434737208e-05,
      "loss": 2.3575,
      "step": 178200
    },
    {
      "epoch": 6.895618207835402,
      "grad_norm": 10.942512512207031,
      "learning_rate": 4.42536514934705e-05,
      "loss": 2.384,
      "step": 178300
    },
    {
      "epoch": 6.899485632517306,
      "grad_norm": 16.46786117553711,
      "learning_rate": 4.425042863956891e-05,
      "loss": 2.3459,
      "step": 178400
    },
    {
      "epoch": 6.903353057199211,
      "grad_norm": 11.520557403564453,
      "learning_rate": 4.424720578566733e-05,
      "loss": 2.2706,
      "step": 178500
    },
    {
      "epoch": 6.907220481881115,
      "grad_norm": 11.0692138671875,
      "learning_rate": 4.4243982931765735e-05,
      "loss": 2.3301,
      "step": 178600
    },
    {
      "epoch": 6.9110879065630195,
      "grad_norm": 11.871074676513672,
      "learning_rate": 4.424076007786415e-05,
      "loss": 2.3553,
      "step": 178700
    },
    {
      "epoch": 6.914955331244924,
      "grad_norm": 10.901378631591797,
      "learning_rate": 4.4237537223962564e-05,
      "loss": 2.2052,
      "step": 178800
    },
    {
      "epoch": 6.918822755926828,
      "grad_norm": 10.619532585144043,
      "learning_rate": 4.423431437006098e-05,
      "loss": 2.3166,
      "step": 178900
    },
    {
      "epoch": 6.922690180608733,
      "grad_norm": 11.860443115234375,
      "learning_rate": 4.423109151615939e-05,
      "loss": 2.2957,
      "step": 179000
    },
    {
      "epoch": 6.926557605290637,
      "grad_norm": 10.363069534301758,
      "learning_rate": 4.42278686622578e-05,
      "loss": 2.2184,
      "step": 179100
    },
    {
      "epoch": 6.930425029972541,
      "grad_norm": 12.941669464111328,
      "learning_rate": 4.4224645808356217e-05,
      "loss": 2.224,
      "step": 179200
    },
    {
      "epoch": 6.934292454654446,
      "grad_norm": 13.592395782470703,
      "learning_rate": 4.422142295445463e-05,
      "loss": 2.3684,
      "step": 179300
    },
    {
      "epoch": 6.93815987933635,
      "grad_norm": 12.152541160583496,
      "learning_rate": 4.421820010055304e-05,
      "loss": 2.2659,
      "step": 179400
    },
    {
      "epoch": 6.9420273040182545,
      "grad_norm": 11.209243774414062,
      "learning_rate": 4.4214977246651454e-05,
      "loss": 2.2379,
      "step": 179500
    },
    {
      "epoch": 6.945894728700159,
      "grad_norm": 15.887845993041992,
      "learning_rate": 4.421175439274987e-05,
      "loss": 2.2232,
      "step": 179600
    },
    {
      "epoch": 6.949762153382063,
      "grad_norm": 12.5089111328125,
      "learning_rate": 4.420853153884828e-05,
      "loss": 2.2985,
      "step": 179700
    },
    {
      "epoch": 6.953629578063968,
      "grad_norm": 10.31580638885498,
      "learning_rate": 4.420530868494669e-05,
      "loss": 2.2072,
      "step": 179800
    },
    {
      "epoch": 6.957497002745871,
      "grad_norm": 9.292449951171875,
      "learning_rate": 4.4202085831045106e-05,
      "loss": 2.2894,
      "step": 179900
    },
    {
      "epoch": 6.9613644274277755,
      "grad_norm": 11.892962455749512,
      "learning_rate": 4.419886297714352e-05,
      "loss": 2.3067,
      "step": 180000
    },
    {
      "epoch": 6.96523185210968,
      "grad_norm": 11.346855163574219,
      "learning_rate": 4.4195640123241935e-05,
      "loss": 2.2717,
      "step": 180100
    },
    {
      "epoch": 6.969099276791584,
      "grad_norm": 11.265694618225098,
      "learning_rate": 4.419241726934035e-05,
      "loss": 2.1939,
      "step": 180200
    },
    {
      "epoch": 6.972966701473489,
      "grad_norm": 10.285961151123047,
      "learning_rate": 4.4189194415438765e-05,
      "loss": 2.2402,
      "step": 180300
    },
    {
      "epoch": 6.976834126155393,
      "grad_norm": 17.137489318847656,
      "learning_rate": 4.418597156153717e-05,
      "loss": 2.2243,
      "step": 180400
    },
    {
      "epoch": 6.980701550837297,
      "grad_norm": 11.394394874572754,
      "learning_rate": 4.418274870763559e-05,
      "loss": 2.2712,
      "step": 180500
    },
    {
      "epoch": 6.984568975519202,
      "grad_norm": 12.098840713500977,
      "learning_rate": 4.4179525853734e-05,
      "loss": 2.2766,
      "step": 180600
    },
    {
      "epoch": 6.988436400201106,
      "grad_norm": 11.318239212036133,
      "learning_rate": 4.417630299983242e-05,
      "loss": 2.2715,
      "step": 180700
    },
    {
      "epoch": 6.9923038248830105,
      "grad_norm": 11.893146514892578,
      "learning_rate": 4.417308014593083e-05,
      "loss": 2.2002,
      "step": 180800
    },
    {
      "epoch": 6.996171249564915,
      "grad_norm": 11.938806533813477,
      "learning_rate": 4.416985729202924e-05,
      "loss": 2.1811,
      "step": 180900
    },
    {
      "epoch": 7.0,
      "eval_loss": 2.132535696029663,
      "eval_runtime": 6.0996,
      "eval_samples_per_second": 223.131,
      "eval_steps_per_second": 223.131,
      "step": 180999
    },
    {
      "epoch": 7.0,
      "eval_loss": 2.107903480529785,
      "eval_runtime": 114.7392,
      "eval_samples_per_second": 225.354,
      "eval_steps_per_second": 225.354,
      "step": 180999
    },
    {
      "epoch": 7.000038674246819,
      "grad_norm": 13.335861206054688,
      "learning_rate": 4.4166634438127654e-05,
      "loss": 2.2631,
      "step": 181000
    },
    {
      "epoch": 7.003906098928724,
      "grad_norm": 12.632758140563965,
      "learning_rate": 4.416341158422607e-05,
      "loss": 2.2395,
      "step": 181100
    },
    {
      "epoch": 7.007773523610628,
      "grad_norm": 11.26323413848877,
      "learning_rate": 4.4160188730324484e-05,
      "loss": 2.2201,
      "step": 181200
    },
    {
      "epoch": 7.011640948292532,
      "grad_norm": 14.1102933883667,
      "learning_rate": 4.415696587642289e-05,
      "loss": 2.2333,
      "step": 181300
    },
    {
      "epoch": 7.015508372974437,
      "grad_norm": 11.401697158813477,
      "learning_rate": 4.4153743022521306e-05,
      "loss": 2.2046,
      "step": 181400
    },
    {
      "epoch": 7.01937579765634,
      "grad_norm": 10.901248931884766,
      "learning_rate": 4.415052016861972e-05,
      "loss": 2.2532,
      "step": 181500
    },
    {
      "epoch": 7.023243222338245,
      "grad_norm": 10.708808898925781,
      "learning_rate": 4.4147297314718136e-05,
      "loss": 2.2816,
      "step": 181600
    },
    {
      "epoch": 7.027110647020149,
      "grad_norm": 12.146286010742188,
      "learning_rate": 4.4144074460816544e-05,
      "loss": 2.2486,
      "step": 181700
    },
    {
      "epoch": 7.030978071702053,
      "grad_norm": 11.009759902954102,
      "learning_rate": 4.414085160691496e-05,
      "loss": 2.1837,
      "step": 181800
    },
    {
      "epoch": 7.034845496383958,
      "grad_norm": 12.129483222961426,
      "learning_rate": 4.413762875301337e-05,
      "loss": 2.2196,
      "step": 181900
    },
    {
      "epoch": 7.038712921065862,
      "grad_norm": 10.546443939208984,
      "learning_rate": 4.413440589911179e-05,
      "loss": 2.2198,
      "step": 182000
    },
    {
      "epoch": 7.0425803457477665,
      "grad_norm": 13.042956352233887,
      "learning_rate": 4.4131183045210196e-05,
      "loss": 2.3866,
      "step": 182100
    },
    {
      "epoch": 7.046447770429671,
      "grad_norm": 13.722989082336426,
      "learning_rate": 4.412796019130861e-05,
      "loss": 2.1675,
      "step": 182200
    },
    {
      "epoch": 7.050315195111575,
      "grad_norm": 13.494808197021484,
      "learning_rate": 4.4124737337407025e-05,
      "loss": 2.2696,
      "step": 182300
    },
    {
      "epoch": 7.05418261979348,
      "grad_norm": 12.495169639587402,
      "learning_rate": 4.412151448350544e-05,
      "loss": 2.2801,
      "step": 182400
    },
    {
      "epoch": 7.058050044475384,
      "grad_norm": 11.617061614990234,
      "learning_rate": 4.411829162960385e-05,
      "loss": 2.2688,
      "step": 182500
    },
    {
      "epoch": 7.061917469157288,
      "grad_norm": 11.529026985168457,
      "learning_rate": 4.411506877570226e-05,
      "loss": 2.2612,
      "step": 182600
    },
    {
      "epoch": 7.065784893839193,
      "grad_norm": 14.90998363494873,
      "learning_rate": 4.411184592180068e-05,
      "loss": 2.2663,
      "step": 182700
    },
    {
      "epoch": 7.069652318521097,
      "grad_norm": 13.878792762756348,
      "learning_rate": 4.410862306789909e-05,
      "loss": 2.2239,
      "step": 182800
    },
    {
      "epoch": 7.0735197432030015,
      "grad_norm": 12.258933067321777,
      "learning_rate": 4.41054002139975e-05,
      "loss": 2.2451,
      "step": 182900
    },
    {
      "epoch": 7.077387167884906,
      "grad_norm": 9.835273742675781,
      "learning_rate": 4.4102177360095915e-05,
      "loss": 2.2219,
      "step": 183000
    },
    {
      "epoch": 7.081254592566809,
      "grad_norm": 8.765593528747559,
      "learning_rate": 4.409895450619433e-05,
      "loss": 2.246,
      "step": 183100
    },
    {
      "epoch": 7.085122017248714,
      "grad_norm": 13.307403564453125,
      "learning_rate": 4.409573165229274e-05,
      "loss": 2.2075,
      "step": 183200
    },
    {
      "epoch": 7.088989441930618,
      "grad_norm": 12.712532997131348,
      "learning_rate": 4.409250879839115e-05,
      "loss": 2.1574,
      "step": 183300
    },
    {
      "epoch": 7.0928568666125225,
      "grad_norm": 22.132465362548828,
      "learning_rate": 4.408928594448957e-05,
      "loss": 2.1817,
      "step": 183400
    },
    {
      "epoch": 7.096724291294427,
      "grad_norm": 11.29442024230957,
      "learning_rate": 4.408606309058798e-05,
      "loss": 2.292,
      "step": 183500
    },
    {
      "epoch": 7.100591715976331,
      "grad_norm": 9.418594360351562,
      "learning_rate": 4.408284023668639e-05,
      "loss": 2.1774,
      "step": 183600
    },
    {
      "epoch": 7.104459140658236,
      "grad_norm": 15.061014175415039,
      "learning_rate": 4.4079617382784804e-05,
      "loss": 2.2663,
      "step": 183700
    },
    {
      "epoch": 7.10832656534014,
      "grad_norm": 15.399136543273926,
      "learning_rate": 4.407639452888322e-05,
      "loss": 2.2282,
      "step": 183800
    },
    {
      "epoch": 7.112193990022044,
      "grad_norm": 11.964653968811035,
      "learning_rate": 4.4073171674981634e-05,
      "loss": 2.2436,
      "step": 183900
    },
    {
      "epoch": 7.116061414703949,
      "grad_norm": 14.872809410095215,
      "learning_rate": 4.406994882108004e-05,
      "loss": 2.2339,
      "step": 184000
    },
    {
      "epoch": 7.119928839385853,
      "grad_norm": 11.448590278625488,
      "learning_rate": 4.4066725967178456e-05,
      "loss": 2.2194,
      "step": 184100
    },
    {
      "epoch": 7.1237962640677575,
      "grad_norm": 11.748808860778809,
      "learning_rate": 4.406350311327687e-05,
      "loss": 2.224,
      "step": 184200
    },
    {
      "epoch": 7.127663688749662,
      "grad_norm": 12.686187744140625,
      "learning_rate": 4.4060280259375286e-05,
      "loss": 2.2062,
      "step": 184300
    },
    {
      "epoch": 7.131531113431566,
      "grad_norm": 13.496487617492676,
      "learning_rate": 4.4057057405473694e-05,
      "loss": 2.262,
      "step": 184400
    },
    {
      "epoch": 7.135398538113471,
      "grad_norm": 11.483918190002441,
      "learning_rate": 4.405383455157211e-05,
      "loss": 2.3157,
      "step": 184500
    },
    {
      "epoch": 7.139265962795375,
      "grad_norm": 12.08746337890625,
      "learning_rate": 4.405061169767052e-05,
      "loss": 2.2142,
      "step": 184600
    },
    {
      "epoch": 7.143133387477279,
      "grad_norm": 9.976991653442383,
      "learning_rate": 4.404738884376894e-05,
      "loss": 2.2289,
      "step": 184700
    },
    {
      "epoch": 7.147000812159183,
      "grad_norm": 13.156947135925293,
      "learning_rate": 4.4044165989867346e-05,
      "loss": 2.2598,
      "step": 184800
    },
    {
      "epoch": 7.150868236841087,
      "grad_norm": 13.58476448059082,
      "learning_rate": 4.404094313596576e-05,
      "loss": 2.2116,
      "step": 184900
    },
    {
      "epoch": 7.154735661522992,
      "grad_norm": 11.548665046691895,
      "learning_rate": 4.4037720282064175e-05,
      "loss": 2.1948,
      "step": 185000
    },
    {
      "epoch": 7.158603086204896,
      "grad_norm": 11.987388610839844,
      "learning_rate": 4.403449742816259e-05,
      "loss": 2.2579,
      "step": 185100
    },
    {
      "epoch": 7.1624705108868,
      "grad_norm": 7.999796390533447,
      "learning_rate": 4.4031274574261e-05,
      "loss": 2.2182,
      "step": 185200
    },
    {
      "epoch": 7.166337935568705,
      "grad_norm": 13.007307052612305,
      "learning_rate": 4.402805172035941e-05,
      "loss": 2.3372,
      "step": 185300
    },
    {
      "epoch": 7.170205360250609,
      "grad_norm": 11.842785835266113,
      "learning_rate": 4.402482886645783e-05,
      "loss": 2.2928,
      "step": 185400
    },
    {
      "epoch": 7.174072784932513,
      "grad_norm": 14.885726928710938,
      "learning_rate": 4.402160601255624e-05,
      "loss": 2.1867,
      "step": 185500
    },
    {
      "epoch": 7.177940209614418,
      "grad_norm": 14.184608459472656,
      "learning_rate": 4.401838315865465e-05,
      "loss": 2.2482,
      "step": 185600
    },
    {
      "epoch": 7.181807634296322,
      "grad_norm": 13.016336441040039,
      "learning_rate": 4.4015160304753065e-05,
      "loss": 2.2249,
      "step": 185700
    },
    {
      "epoch": 7.185675058978227,
      "grad_norm": 12.377042770385742,
      "learning_rate": 4.401193745085148e-05,
      "loss": 2.2273,
      "step": 185800
    },
    {
      "epoch": 7.189542483660131,
      "grad_norm": 12.084300994873047,
      "learning_rate": 4.400871459694989e-05,
      "loss": 2.284,
      "step": 185900
    },
    {
      "epoch": 7.193409908342035,
      "grad_norm": 13.026610374450684,
      "learning_rate": 4.40054917430483e-05,
      "loss": 2.2431,
      "step": 186000
    },
    {
      "epoch": 7.19727733302394,
      "grad_norm": 15.552038192749023,
      "learning_rate": 4.400226888914672e-05,
      "loss": 2.3075,
      "step": 186100
    },
    {
      "epoch": 7.201144757705844,
      "grad_norm": 13.761021614074707,
      "learning_rate": 4.399904603524513e-05,
      "loss": 2.3368,
      "step": 186200
    },
    {
      "epoch": 7.2050121823877475,
      "grad_norm": 12.765851974487305,
      "learning_rate": 4.399582318134354e-05,
      "loss": 2.3225,
      "step": 186300
    },
    {
      "epoch": 7.208879607069652,
      "grad_norm": 12.10665512084961,
      "learning_rate": 4.3992600327441954e-05,
      "loss": 2.2648,
      "step": 186400
    },
    {
      "epoch": 7.212747031751556,
      "grad_norm": 11.76323413848877,
      "learning_rate": 4.398937747354037e-05,
      "loss": 2.2462,
      "step": 186500
    },
    {
      "epoch": 7.216614456433461,
      "grad_norm": 16.230443954467773,
      "learning_rate": 4.3986154619638784e-05,
      "loss": 2.2373,
      "step": 186600
    },
    {
      "epoch": 7.220481881115365,
      "grad_norm": 13.767989158630371,
      "learning_rate": 4.39829317657372e-05,
      "loss": 2.2484,
      "step": 186700
    },
    {
      "epoch": 7.224349305797269,
      "grad_norm": 10.651396751403809,
      "learning_rate": 4.397970891183561e-05,
      "loss": 2.2628,
      "step": 186800
    },
    {
      "epoch": 7.228216730479174,
      "grad_norm": 11.882153511047363,
      "learning_rate": 4.397648605793402e-05,
      "loss": 2.3022,
      "step": 186900
    },
    {
      "epoch": 7.232084155161078,
      "grad_norm": 11.78177547454834,
      "learning_rate": 4.3973263204032436e-05,
      "loss": 2.2544,
      "step": 187000
    },
    {
      "epoch": 7.2359515798429825,
      "grad_norm": 12.030635833740234,
      "learning_rate": 4.397004035013085e-05,
      "loss": 2.318,
      "step": 187100
    },
    {
      "epoch": 7.239819004524887,
      "grad_norm": 12.047574043273926,
      "learning_rate": 4.3966817496229265e-05,
      "loss": 2.2237,
      "step": 187200
    },
    {
      "epoch": 7.243686429206791,
      "grad_norm": 12.799459457397461,
      "learning_rate": 4.396359464232768e-05,
      "loss": 2.3755,
      "step": 187300
    },
    {
      "epoch": 7.247553853888696,
      "grad_norm": 11.324006080627441,
      "learning_rate": 4.396037178842609e-05,
      "loss": 2.2178,
      "step": 187400
    },
    {
      "epoch": 7.2514212785706,
      "grad_norm": 13.352319717407227,
      "learning_rate": 4.39571489345245e-05,
      "loss": 2.2537,
      "step": 187500
    },
    {
      "epoch": 7.255288703252504,
      "grad_norm": 10.63156795501709,
      "learning_rate": 4.395392608062292e-05,
      "loss": 2.2142,
      "step": 187600
    },
    {
      "epoch": 7.259156127934409,
      "grad_norm": 10.743565559387207,
      "learning_rate": 4.395070322672133e-05,
      "loss": 2.1838,
      "step": 187700
    },
    {
      "epoch": 7.263023552616313,
      "grad_norm": 10.904258728027344,
      "learning_rate": 4.394748037281975e-05,
      "loss": 2.3211,
      "step": 187800
    },
    {
      "epoch": 7.2668909772982175,
      "grad_norm": 12.532048225402832,
      "learning_rate": 4.3944257518918155e-05,
      "loss": 2.2126,
      "step": 187900
    },
    {
      "epoch": 7.270758401980121,
      "grad_norm": 14.453232765197754,
      "learning_rate": 4.394103466501657e-05,
      "loss": 2.2452,
      "step": 188000
    },
    {
      "epoch": 7.274625826662025,
      "grad_norm": 10.790618896484375,
      "learning_rate": 4.3937811811114984e-05,
      "loss": 2.2363,
      "step": 188100
    },
    {
      "epoch": 7.27849325134393,
      "grad_norm": 13.72252082824707,
      "learning_rate": 4.39345889572134e-05,
      "loss": 2.2139,
      "step": 188200
    },
    {
      "epoch": 7.282360676025834,
      "grad_norm": 15.535345077514648,
      "learning_rate": 4.393136610331181e-05,
      "loss": 2.2819,
      "step": 188300
    },
    {
      "epoch": 7.2862281007077385,
      "grad_norm": 13.82123851776123,
      "learning_rate": 4.392814324941022e-05,
      "loss": 2.2425,
      "step": 188400
    },
    {
      "epoch": 7.290095525389643,
      "grad_norm": 11.698164939880371,
      "learning_rate": 4.3924920395508636e-05,
      "loss": 2.2271,
      "step": 188500
    },
    {
      "epoch": 7.293962950071547,
      "grad_norm": 10.862675666809082,
      "learning_rate": 4.392169754160705e-05,
      "loss": 2.2481,
      "step": 188600
    },
    {
      "epoch": 7.297830374753452,
      "grad_norm": 12.08227825164795,
      "learning_rate": 4.391847468770546e-05,
      "loss": 2.206,
      "step": 188700
    },
    {
      "epoch": 7.301697799435356,
      "grad_norm": 13.627402305603027,
      "learning_rate": 4.3915251833803874e-05,
      "loss": 2.2637,
      "step": 188800
    },
    {
      "epoch": 7.30556522411726,
      "grad_norm": 11.514573097229004,
      "learning_rate": 4.391202897990229e-05,
      "loss": 2.2721,
      "step": 188900
    },
    {
      "epoch": 7.309432648799165,
      "grad_norm": 14.2695894241333,
      "learning_rate": 4.3908806126000696e-05,
      "loss": 2.2542,
      "step": 189000
    },
    {
      "epoch": 7.313300073481069,
      "grad_norm": 14.662792205810547,
      "learning_rate": 4.390558327209911e-05,
      "loss": 2.1938,
      "step": 189100
    },
    {
      "epoch": 7.3171674981629735,
      "grad_norm": 12.892096519470215,
      "learning_rate": 4.3902360418197526e-05,
      "loss": 2.2997,
      "step": 189200
    },
    {
      "epoch": 7.321034922844878,
      "grad_norm": 9.312294960021973,
      "learning_rate": 4.389913756429594e-05,
      "loss": 2.289,
      "step": 189300
    },
    {
      "epoch": 7.324902347526782,
      "grad_norm": 10.685060501098633,
      "learning_rate": 4.389591471039435e-05,
      "loss": 2.2148,
      "step": 189400
    },
    {
      "epoch": 7.328769772208687,
      "grad_norm": 12.229034423828125,
      "learning_rate": 4.389269185649276e-05,
      "loss": 2.2391,
      "step": 189500
    },
    {
      "epoch": 7.332637196890591,
      "grad_norm": 9.578125953674316,
      "learning_rate": 4.388946900259118e-05,
      "loss": 2.1883,
      "step": 189600
    },
    {
      "epoch": 7.3365046215724945,
      "grad_norm": 12.872321128845215,
      "learning_rate": 4.388624614868959e-05,
      "loss": 2.2195,
      "step": 189700
    },
    {
      "epoch": 7.340372046254399,
      "grad_norm": 12.055427551269531,
      "learning_rate": 4.3883023294788e-05,
      "loss": 2.2689,
      "step": 189800
    },
    {
      "epoch": 7.344239470936303,
      "grad_norm": 9.453484535217285,
      "learning_rate": 4.3879800440886415e-05,
      "loss": 2.2897,
      "step": 189900
    },
    {
      "epoch": 7.348106895618208,
      "grad_norm": 12.386994361877441,
      "learning_rate": 4.387657758698483e-05,
      "loss": 2.2453,
      "step": 190000
    },
    {
      "epoch": 7.351974320300112,
      "grad_norm": 11.564349174499512,
      "learning_rate": 4.3873354733083245e-05,
      "loss": 2.2742,
      "step": 190100
    },
    {
      "epoch": 7.355841744982016,
      "grad_norm": 13.135738372802734,
      "learning_rate": 4.387013187918165e-05,
      "loss": 2.2491,
      "step": 190200
    },
    {
      "epoch": 7.359709169663921,
      "grad_norm": 10.673206329345703,
      "learning_rate": 4.386690902528007e-05,
      "loss": 2.2928,
      "step": 190300
    },
    {
      "epoch": 7.363576594345825,
      "grad_norm": 13.49967098236084,
      "learning_rate": 4.386368617137848e-05,
      "loss": 2.1996,
      "step": 190400
    },
    {
      "epoch": 7.3674440190277295,
      "grad_norm": 11.092491149902344,
      "learning_rate": 4.38604633174769e-05,
      "loss": 2.3223,
      "step": 190500
    },
    {
      "epoch": 7.371311443709634,
      "grad_norm": 13.155824661254883,
      "learning_rate": 4.3857240463575305e-05,
      "loss": 2.2035,
      "step": 190600
    },
    {
      "epoch": 7.375178868391538,
      "grad_norm": 12.422030448913574,
      "learning_rate": 4.385401760967372e-05,
      "loss": 2.1899,
      "step": 190700
    },
    {
      "epoch": 7.379046293073443,
      "grad_norm": 12.063658714294434,
      "learning_rate": 4.3850794755772134e-05,
      "loss": 2.1299,
      "step": 190800
    },
    {
      "epoch": 7.382913717755347,
      "grad_norm": 16.39330291748047,
      "learning_rate": 4.384757190187055e-05,
      "loss": 2.2606,
      "step": 190900
    },
    {
      "epoch": 7.386781142437251,
      "grad_norm": 10.044718742370605,
      "learning_rate": 4.384434904796896e-05,
      "loss": 2.2572,
      "step": 191000
    },
    {
      "epoch": 7.390648567119156,
      "grad_norm": 13.262288093566895,
      "learning_rate": 4.384112619406737e-05,
      "loss": 2.2144,
      "step": 191100
    },
    {
      "epoch": 7.394515991801059,
      "grad_norm": 14.787455558776855,
      "learning_rate": 4.3837903340165786e-05,
      "loss": 2.2662,
      "step": 191200
    },
    {
      "epoch": 7.398383416482964,
      "grad_norm": 12.832663536071777,
      "learning_rate": 4.38346804862642e-05,
      "loss": 2.2737,
      "step": 191300
    },
    {
      "epoch": 7.402250841164868,
      "grad_norm": 11.414704322814941,
      "learning_rate": 4.383145763236261e-05,
      "loss": 2.2263,
      "step": 191400
    },
    {
      "epoch": 7.406118265846772,
      "grad_norm": 14.497854232788086,
      "learning_rate": 4.3828234778461024e-05,
      "loss": 2.1862,
      "step": 191500
    },
    {
      "epoch": 7.409985690528677,
      "grad_norm": 11.10820198059082,
      "learning_rate": 4.382501192455944e-05,
      "loss": 2.259,
      "step": 191600
    },
    {
      "epoch": 7.413853115210581,
      "grad_norm": 13.724746704101562,
      "learning_rate": 4.382178907065785e-05,
      "loss": 2.244,
      "step": 191700
    },
    {
      "epoch": 7.4177205398924855,
      "grad_norm": 12.40701675415039,
      "learning_rate": 4.381856621675626e-05,
      "loss": 2.1402,
      "step": 191800
    },
    {
      "epoch": 7.42158796457439,
      "grad_norm": 15.596485137939453,
      "learning_rate": 4.3815343362854676e-05,
      "loss": 2.3341,
      "step": 191900
    },
    {
      "epoch": 7.425455389256294,
      "grad_norm": 10.444303512573242,
      "learning_rate": 4.381212050895309e-05,
      "loss": 2.2592,
      "step": 192000
    },
    {
      "epoch": 7.429322813938199,
      "grad_norm": 13.903883934020996,
      "learning_rate": 4.38088976550515e-05,
      "loss": 2.251,
      "step": 192100
    },
    {
      "epoch": 7.433190238620103,
      "grad_norm": 15.871060371398926,
      "learning_rate": 4.380567480114991e-05,
      "loss": 2.3329,
      "step": 192200
    },
    {
      "epoch": 7.437057663302007,
      "grad_norm": 12.403626441955566,
      "learning_rate": 4.380245194724833e-05,
      "loss": 2.2635,
      "step": 192300
    },
    {
      "epoch": 7.440925087983912,
      "grad_norm": 9.388932228088379,
      "learning_rate": 4.379922909334674e-05,
      "loss": 2.2993,
      "step": 192400
    },
    {
      "epoch": 7.444792512665816,
      "grad_norm": 11.592880249023438,
      "learning_rate": 4.379600623944515e-05,
      "loss": 2.2699,
      "step": 192500
    },
    {
      "epoch": 7.4486599373477205,
      "grad_norm": 9.918617248535156,
      "learning_rate": 4.3792783385543565e-05,
      "loss": 2.2311,
      "step": 192600
    },
    {
      "epoch": 7.452527362029625,
      "grad_norm": 11.950231552124023,
      "learning_rate": 4.378956053164198e-05,
      "loss": 2.3062,
      "step": 192700
    },
    {
      "epoch": 7.456394786711529,
      "grad_norm": 11.859489440917969,
      "learning_rate": 4.3786337677740395e-05,
      "loss": 2.2278,
      "step": 192800
    },
    {
      "epoch": 7.460262211393433,
      "grad_norm": 12.715595245361328,
      "learning_rate": 4.37831148238388e-05,
      "loss": 2.2254,
      "step": 192900
    },
    {
      "epoch": 7.464129636075337,
      "grad_norm": 12.028947830200195,
      "learning_rate": 4.377989196993722e-05,
      "loss": 2.2556,
      "step": 193000
    },
    {
      "epoch": 7.4679970607572415,
      "grad_norm": 12.851701736450195,
      "learning_rate": 4.377666911603563e-05,
      "loss": 2.2659,
      "step": 193100
    },
    {
      "epoch": 7.471864485439146,
      "grad_norm": 15.565686225891113,
      "learning_rate": 4.377344626213405e-05,
      "loss": 2.2383,
      "step": 193200
    },
    {
      "epoch": 7.47573191012105,
      "grad_norm": 11.19985294342041,
      "learning_rate": 4.3770223408232455e-05,
      "loss": 2.3209,
      "step": 193300
    },
    {
      "epoch": 7.479599334802955,
      "grad_norm": 10.752013206481934,
      "learning_rate": 4.376700055433087e-05,
      "loss": 2.3119,
      "step": 193400
    },
    {
      "epoch": 7.483466759484859,
      "grad_norm": 11.849764823913574,
      "learning_rate": 4.3763777700429284e-05,
      "loss": 2.3196,
      "step": 193500
    },
    {
      "epoch": 7.487334184166763,
      "grad_norm": 13.017108917236328,
      "learning_rate": 4.37605548465277e-05,
      "loss": 2.369,
      "step": 193600
    },
    {
      "epoch": 7.491201608848668,
      "grad_norm": 13.91140365600586,
      "learning_rate": 4.3757331992626114e-05,
      "loss": 2.2476,
      "step": 193700
    },
    {
      "epoch": 7.495069033530572,
      "grad_norm": 13.100425720214844,
      "learning_rate": 4.375410913872453e-05,
      "loss": 2.3077,
      "step": 193800
    },
    {
      "epoch": 7.4989364582124765,
      "grad_norm": 10.856334686279297,
      "learning_rate": 4.3750886284822936e-05,
      "loss": 2.3008,
      "step": 193900
    },
    {
      "epoch": 7.502803882894381,
      "grad_norm": 11.23588752746582,
      "learning_rate": 4.374766343092135e-05,
      "loss": 2.2699,
      "step": 194000
    },
    {
      "epoch": 7.506671307576285,
      "grad_norm": 11.754834175109863,
      "learning_rate": 4.3744440577019766e-05,
      "loss": 2.2955,
      "step": 194100
    },
    {
      "epoch": 7.51053873225819,
      "grad_norm": 15.58961296081543,
      "learning_rate": 4.374121772311818e-05,
      "loss": 2.2176,
      "step": 194200
    },
    {
      "epoch": 7.514406156940094,
      "grad_norm": 12.088756561279297,
      "learning_rate": 4.3737994869216595e-05,
      "loss": 2.1806,
      "step": 194300
    },
    {
      "epoch": 7.518273581621997,
      "grad_norm": 11.345163345336914,
      "learning_rate": 4.373477201531501e-05,
      "loss": 2.131,
      "step": 194400
    },
    {
      "epoch": 7.522141006303903,
      "grad_norm": 12.817936897277832,
      "learning_rate": 4.373154916141342e-05,
      "loss": 2.2566,
      "step": 194500
    },
    {
      "epoch": 7.526008430985806,
      "grad_norm": 13.617117881774902,
      "learning_rate": 4.372832630751183e-05,
      "loss": 2.1939,
      "step": 194600
    },
    {
      "epoch": 7.529875855667711,
      "grad_norm": 11.18099308013916,
      "learning_rate": 4.372510345361025e-05,
      "loss": 2.2363,
      "step": 194700
    },
    {
      "epoch": 7.533743280349615,
      "grad_norm": 10.305830955505371,
      "learning_rate": 4.3721880599708655e-05,
      "loss": 2.1855,
      "step": 194800
    },
    {
      "epoch": 7.537610705031519,
      "grad_norm": 14.101547241210938,
      "learning_rate": 4.371865774580707e-05,
      "loss": 2.2939,
      "step": 194900
    },
    {
      "epoch": 7.541478129713424,
      "grad_norm": 13.198596000671387,
      "learning_rate": 4.3715434891905485e-05,
      "loss": 2.2611,
      "step": 195000
    },
    {
      "epoch": 7.545345554395328,
      "grad_norm": 12.723478317260742,
      "learning_rate": 4.37122120380039e-05,
      "loss": 2.2286,
      "step": 195100
    },
    {
      "epoch": 7.549212979077232,
      "grad_norm": 13.73617935180664,
      "learning_rate": 4.370898918410231e-05,
      "loss": 2.2912,
      "step": 195200
    },
    {
      "epoch": 7.553080403759137,
      "grad_norm": 10.67046070098877,
      "learning_rate": 4.370576633020072e-05,
      "loss": 2.2941,
      "step": 195300
    },
    {
      "epoch": 7.556947828441041,
      "grad_norm": 13.583348274230957,
      "learning_rate": 4.370254347629914e-05,
      "loss": 2.2472,
      "step": 195400
    },
    {
      "epoch": 7.560815253122946,
      "grad_norm": 11.196688652038574,
      "learning_rate": 4.369932062239755e-05,
      "loss": 2.1596,
      "step": 195500
    },
    {
      "epoch": 7.56468267780485,
      "grad_norm": 11.725541114807129,
      "learning_rate": 4.369609776849596e-05,
      "loss": 2.1728,
      "step": 195600
    },
    {
      "epoch": 7.568550102486754,
      "grad_norm": 10.425049781799316,
      "learning_rate": 4.3692874914594374e-05,
      "loss": 2.2327,
      "step": 195700
    },
    {
      "epoch": 7.572417527168659,
      "grad_norm": 15.254711151123047,
      "learning_rate": 4.368965206069279e-05,
      "loss": 2.2226,
      "step": 195800
    },
    {
      "epoch": 7.576284951850563,
      "grad_norm": 13.526618003845215,
      "learning_rate": 4.3686429206791203e-05,
      "loss": 2.257,
      "step": 195900
    },
    {
      "epoch": 7.580152376532467,
      "grad_norm": 11.176012992858887,
      "learning_rate": 4.368320635288961e-05,
      "loss": 2.2569,
      "step": 196000
    },
    {
      "epoch": 7.584019801214371,
      "grad_norm": 13.316061019897461,
      "learning_rate": 4.3679983498988026e-05,
      "loss": 2.2923,
      "step": 196100
    },
    {
      "epoch": 7.587887225896275,
      "grad_norm": 14.667302131652832,
      "learning_rate": 4.367676064508644e-05,
      "loss": 2.2552,
      "step": 196200
    },
    {
      "epoch": 7.59175465057818,
      "grad_norm": 17.79754066467285,
      "learning_rate": 4.3673537791184856e-05,
      "loss": 2.1159,
      "step": 196300
    },
    {
      "epoch": 7.595622075260084,
      "grad_norm": 9.390885353088379,
      "learning_rate": 4.3670314937283263e-05,
      "loss": 2.219,
      "step": 196400
    },
    {
      "epoch": 7.599489499941988,
      "grad_norm": 9.3258638381958,
      "learning_rate": 4.366709208338168e-05,
      "loss": 2.3226,
      "step": 196500
    },
    {
      "epoch": 7.603356924623893,
      "grad_norm": 14.050653457641602,
      "learning_rate": 4.366386922948009e-05,
      "loss": 2.206,
      "step": 196600
    },
    {
      "epoch": 7.607224349305797,
      "grad_norm": 11.860901832580566,
      "learning_rate": 4.366064637557851e-05,
      "loss": 2.21,
      "step": 196700
    },
    {
      "epoch": 7.6110917739877015,
      "grad_norm": 11.039584159851074,
      "learning_rate": 4.3657423521676916e-05,
      "loss": 2.1291,
      "step": 196800
    },
    {
      "epoch": 7.614959198669606,
      "grad_norm": 11.29167366027832,
      "learning_rate": 4.365420066777533e-05,
      "loss": 2.2358,
      "step": 196900
    },
    {
      "epoch": 7.61882662335151,
      "grad_norm": 12.69888687133789,
      "learning_rate": 4.3650977813873745e-05,
      "loss": 2.3523,
      "step": 197000
    },
    {
      "epoch": 7.622694048033415,
      "grad_norm": 12.9064359664917,
      "learning_rate": 4.364775495997216e-05,
      "loss": 2.3309,
      "step": 197100
    },
    {
      "epoch": 7.626561472715319,
      "grad_norm": 11.717667579650879,
      "learning_rate": 4.364453210607057e-05,
      "loss": 2.1838,
      "step": 197200
    },
    {
      "epoch": 7.630428897397223,
      "grad_norm": 14.314336776733398,
      "learning_rate": 4.364130925216898e-05,
      "loss": 2.3047,
      "step": 197300
    },
    {
      "epoch": 7.634296322079128,
      "grad_norm": 11.113804817199707,
      "learning_rate": 4.36380863982674e-05,
      "loss": 2.2824,
      "step": 197400
    },
    {
      "epoch": 7.638163746761032,
      "grad_norm": 11.19440746307373,
      "learning_rate": 4.363486354436581e-05,
      "loss": 2.2854,
      "step": 197500
    },
    {
      "epoch": 7.6420311714429365,
      "grad_norm": 13.370847702026367,
      "learning_rate": 4.363164069046422e-05,
      "loss": 2.1515,
      "step": 197600
    },
    {
      "epoch": 7.645898596124841,
      "grad_norm": 14.844043731689453,
      "learning_rate": 4.3628417836562634e-05,
      "loss": 2.2504,
      "step": 197700
    },
    {
      "epoch": 7.649766020806744,
      "grad_norm": 11.693036079406738,
      "learning_rate": 4.362519498266105e-05,
      "loss": 2.2486,
      "step": 197800
    },
    {
      "epoch": 7.653633445488649,
      "grad_norm": 9.48432445526123,
      "learning_rate": 4.362197212875946e-05,
      "loss": 2.34,
      "step": 197900
    },
    {
      "epoch": 7.657500870170553,
      "grad_norm": 9.628253936767578,
      "learning_rate": 4.361874927485787e-05,
      "loss": 2.2755,
      "step": 198000
    },
    {
      "epoch": 7.6613682948524575,
      "grad_norm": 15.026786804199219,
      "learning_rate": 4.3615526420956287e-05,
      "loss": 2.2385,
      "step": 198100
    },
    {
      "epoch": 7.665235719534362,
      "grad_norm": 13.383749961853027,
      "learning_rate": 4.36123035670547e-05,
      "loss": 2.1828,
      "step": 198200
    },
    {
      "epoch": 7.669103144216266,
      "grad_norm": 13.645383834838867,
      "learning_rate": 4.360908071315311e-05,
      "loss": 2.2045,
      "step": 198300
    },
    {
      "epoch": 7.672970568898171,
      "grad_norm": 13.108428955078125,
      "learning_rate": 4.3605857859251524e-05,
      "loss": 2.2653,
      "step": 198400
    },
    {
      "epoch": 7.676837993580075,
      "grad_norm": 11.373696327209473,
      "learning_rate": 4.360263500534994e-05,
      "loss": 2.2409,
      "step": 198500
    },
    {
      "epoch": 7.680705418261979,
      "grad_norm": 9.203890800476074,
      "learning_rate": 4.3599412151448353e-05,
      "loss": 2.2604,
      "step": 198600
    },
    {
      "epoch": 7.684572842943884,
      "grad_norm": 11.580358505249023,
      "learning_rate": 4.359618929754676e-05,
      "loss": 2.2139,
      "step": 198700
    },
    {
      "epoch": 7.688440267625788,
      "grad_norm": 11.799734115600586,
      "learning_rate": 4.3592966443645176e-05,
      "loss": 2.1941,
      "step": 198800
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 11.707391738891602,
      "learning_rate": 4.358974358974359e-05,
      "loss": 2.1284,
      "step": 198900
    },
    {
      "epoch": 7.696175116989597,
      "grad_norm": 10.967236518859863,
      "learning_rate": 4.3586520735842006e-05,
      "loss": 2.2368,
      "step": 199000
    },
    {
      "epoch": 7.700042541671501,
      "grad_norm": 11.223848342895508,
      "learning_rate": 4.3583297881940413e-05,
      "loss": 2.2384,
      "step": 199100
    },
    {
      "epoch": 7.703909966353406,
      "grad_norm": 10.46645736694336,
      "learning_rate": 4.358007502803883e-05,
      "loss": 2.2394,
      "step": 199200
    },
    {
      "epoch": 7.707777391035309,
      "grad_norm": 12.503602027893066,
      "learning_rate": 4.357685217413724e-05,
      "loss": 2.3608,
      "step": 199300
    },
    {
      "epoch": 7.711644815717214,
      "grad_norm": 12.665453910827637,
      "learning_rate": 4.357362932023566e-05,
      "loss": 2.2171,
      "step": 199400
    },
    {
      "epoch": 7.715512240399118,
      "grad_norm": 12.629659652709961,
      "learning_rate": 4.3570406466334066e-05,
      "loss": 2.2086,
      "step": 199500
    },
    {
      "epoch": 7.719379665081022,
      "grad_norm": 15.348384857177734,
      "learning_rate": 4.356718361243248e-05,
      "loss": 2.1995,
      "step": 199600
    },
    {
      "epoch": 7.723247089762927,
      "grad_norm": 13.668366432189941,
      "learning_rate": 4.3563960758530895e-05,
      "loss": 2.1621,
      "step": 199700
    },
    {
      "epoch": 7.727114514444831,
      "grad_norm": 10.774409294128418,
      "learning_rate": 4.356073790462931e-05,
      "loss": 2.2816,
      "step": 199800
    },
    {
      "epoch": 7.730981939126735,
      "grad_norm": 10.16054630279541,
      "learning_rate": 4.355751505072772e-05,
      "loss": 2.1627,
      "step": 199900
    },
    {
      "epoch": 7.73484936380864,
      "grad_norm": 11.474533081054688,
      "learning_rate": 4.355429219682613e-05,
      "loss": 2.2336,
      "step": 200000
    },
    {
      "epoch": 7.738716788490544,
      "grad_norm": 10.911283493041992,
      "learning_rate": 4.355106934292455e-05,
      "loss": 2.2105,
      "step": 200100
    },
    {
      "epoch": 7.7425842131724485,
      "grad_norm": 9.615225791931152,
      "learning_rate": 4.354784648902296e-05,
      "loss": 2.1856,
      "step": 200200
    },
    {
      "epoch": 7.746451637854353,
      "grad_norm": 9.5921049118042,
      "learning_rate": 4.3544623635121377e-05,
      "loss": 2.3334,
      "step": 200300
    },
    {
      "epoch": 7.750319062536257,
      "grad_norm": 12.589726448059082,
      "learning_rate": 4.3541400781219784e-05,
      "loss": 2.2478,
      "step": 200400
    },
    {
      "epoch": 7.754186487218162,
      "grad_norm": 11.580221176147461,
      "learning_rate": 4.35381779273182e-05,
      "loss": 2.2426,
      "step": 200500
    },
    {
      "epoch": 7.758053911900066,
      "grad_norm": 8.786606788635254,
      "learning_rate": 4.3534955073416614e-05,
      "loss": 2.2267,
      "step": 200600
    },
    {
      "epoch": 7.76192133658197,
      "grad_norm": 10.820287704467773,
      "learning_rate": 4.353173221951503e-05,
      "loss": 2.2158,
      "step": 200700
    },
    {
      "epoch": 7.765788761263875,
      "grad_norm": 9.758195877075195,
      "learning_rate": 4.352850936561344e-05,
      "loss": 2.2181,
      "step": 200800
    },
    {
      "epoch": 7.769656185945779,
      "grad_norm": 11.410199165344238,
      "learning_rate": 4.352528651171186e-05,
      "loss": 2.1951,
      "step": 200900
    },
    {
      "epoch": 7.773523610627683,
      "grad_norm": 15.504602432250977,
      "learning_rate": 4.3522063657810266e-05,
      "loss": 2.2102,
      "step": 201000
    },
    {
      "epoch": 7.777391035309587,
      "grad_norm": 15.262834548950195,
      "learning_rate": 4.351884080390868e-05,
      "loss": 2.1905,
      "step": 201100
    },
    {
      "epoch": 7.781258459991491,
      "grad_norm": 11.310507774353027,
      "learning_rate": 4.3515617950007095e-05,
      "loss": 2.2526,
      "step": 201200
    },
    {
      "epoch": 7.785125884673396,
      "grad_norm": 13.324625015258789,
      "learning_rate": 4.351239509610551e-05,
      "loss": 2.2774,
      "step": 201300
    },
    {
      "epoch": 7.7889933093553,
      "grad_norm": 12.408343315124512,
      "learning_rate": 4.350917224220392e-05,
      "loss": 2.2483,
      "step": 201400
    },
    {
      "epoch": 7.7928607340372045,
      "grad_norm": 11.205760955810547,
      "learning_rate": 4.350594938830233e-05,
      "loss": 2.2335,
      "step": 201500
    },
    {
      "epoch": 7.796728158719109,
      "grad_norm": 11.058420181274414,
      "learning_rate": 4.350272653440075e-05,
      "loss": 2.1954,
      "step": 201600
    },
    {
      "epoch": 7.800595583401013,
      "grad_norm": 10.8734712600708,
      "learning_rate": 4.349950368049916e-05,
      "loss": 2.1541,
      "step": 201700
    },
    {
      "epoch": 7.804463008082918,
      "grad_norm": 9.366097450256348,
      "learning_rate": 4.349628082659757e-05,
      "loss": 2.2185,
      "step": 201800
    },
    {
      "epoch": 7.808330432764822,
      "grad_norm": 12.898704528808594,
      "learning_rate": 4.3493057972695985e-05,
      "loss": 2.236,
      "step": 201900
    },
    {
      "epoch": 7.812197857446726,
      "grad_norm": 9.686200141906738,
      "learning_rate": 4.34898351187944e-05,
      "loss": 2.1898,
      "step": 202000
    },
    {
      "epoch": 7.816065282128631,
      "grad_norm": 10.931453704833984,
      "learning_rate": 4.3486612264892814e-05,
      "loss": 2.2899,
      "step": 202100
    },
    {
      "epoch": 7.819932706810535,
      "grad_norm": 7.5971527099609375,
      "learning_rate": 4.348338941099122e-05,
      "loss": 2.2784,
      "step": 202200
    },
    {
      "epoch": 7.8238001314924395,
      "grad_norm": 16.705408096313477,
      "learning_rate": 4.348016655708964e-05,
      "loss": 2.2206,
      "step": 202300
    },
    {
      "epoch": 7.827667556174344,
      "grad_norm": 13.988906860351562,
      "learning_rate": 4.347694370318805e-05,
      "loss": 2.2208,
      "step": 202400
    },
    {
      "epoch": 7.831534980856247,
      "grad_norm": 14.561445236206055,
      "learning_rate": 4.3473720849286466e-05,
      "loss": 2.1775,
      "step": 202500
    },
    {
      "epoch": 7.835402405538153,
      "grad_norm": 10.711820602416992,
      "learning_rate": 4.3470497995384874e-05,
      "loss": 2.1989,
      "step": 202600
    },
    {
      "epoch": 7.839269830220056,
      "grad_norm": 15.559953689575195,
      "learning_rate": 4.346727514148329e-05,
      "loss": 2.216,
      "step": 202700
    },
    {
      "epoch": 7.8431372549019605,
      "grad_norm": 9.900463104248047,
      "learning_rate": 4.3464052287581704e-05,
      "loss": 2.2072,
      "step": 202800
    },
    {
      "epoch": 7.847004679583865,
      "grad_norm": 11.483102798461914,
      "learning_rate": 4.346082943368012e-05,
      "loss": 2.229,
      "step": 202900
    },
    {
      "epoch": 7.850872104265769,
      "grad_norm": 14.900507926940918,
      "learning_rate": 4.3457606579778526e-05,
      "loss": 2.2635,
      "step": 203000
    },
    {
      "epoch": 7.854739528947674,
      "grad_norm": 10.14705753326416,
      "learning_rate": 4.345438372587694e-05,
      "loss": 2.1791,
      "step": 203100
    },
    {
      "epoch": 7.858606953629578,
      "grad_norm": 9.841728210449219,
      "learning_rate": 4.3451160871975356e-05,
      "loss": 2.2262,
      "step": 203200
    },
    {
      "epoch": 7.862474378311482,
      "grad_norm": 12.053021430969238,
      "learning_rate": 4.344793801807377e-05,
      "loss": 2.2295,
      "step": 203300
    },
    {
      "epoch": 7.866341802993387,
      "grad_norm": 12.52900218963623,
      "learning_rate": 4.344471516417218e-05,
      "loss": 2.2853,
      "step": 203400
    },
    {
      "epoch": 7.870209227675291,
      "grad_norm": 11.817680358886719,
      "learning_rate": 4.344149231027059e-05,
      "loss": 2.2597,
      "step": 203500
    },
    {
      "epoch": 7.8740766523571954,
      "grad_norm": 14.534713745117188,
      "learning_rate": 4.343826945636901e-05,
      "loss": 2.2097,
      "step": 203600
    },
    {
      "epoch": 7.8779440770391,
      "grad_norm": 13.123051643371582,
      "learning_rate": 4.3435046602467416e-05,
      "loss": 2.2209,
      "step": 203700
    },
    {
      "epoch": 7.881811501721004,
      "grad_norm": 12.298192024230957,
      "learning_rate": 4.343182374856583e-05,
      "loss": 2.2292,
      "step": 203800
    },
    {
      "epoch": 7.885678926402909,
      "grad_norm": 16.404417037963867,
      "learning_rate": 4.3428600894664245e-05,
      "loss": 2.2826,
      "step": 203900
    },
    {
      "epoch": 7.889546351084813,
      "grad_norm": 11.856385231018066,
      "learning_rate": 4.342537804076266e-05,
      "loss": 2.2853,
      "step": 204000
    },
    {
      "epoch": 7.893413775766717,
      "grad_norm": 11.74429702758789,
      "learning_rate": 4.342215518686107e-05,
      "loss": 2.1457,
      "step": 204100
    },
    {
      "epoch": 7.897281200448621,
      "grad_norm": 16.797359466552734,
      "learning_rate": 4.341893233295948e-05,
      "loss": 2.2703,
      "step": 204200
    },
    {
      "epoch": 7.901148625130526,
      "grad_norm": 12.227432250976562,
      "learning_rate": 4.34157094790579e-05,
      "loss": 2.1843,
      "step": 204300
    },
    {
      "epoch": 7.90501604981243,
      "grad_norm": 12.950413703918457,
      "learning_rate": 4.341248662515631e-05,
      "loss": 2.3204,
      "step": 204400
    },
    {
      "epoch": 7.908883474494334,
      "grad_norm": 12.705582618713379,
      "learning_rate": 4.340926377125472e-05,
      "loss": 2.1962,
      "step": 204500
    },
    {
      "epoch": 7.912750899176238,
      "grad_norm": 11.121380805969238,
      "learning_rate": 4.3406040917353135e-05,
      "loss": 2.2142,
      "step": 204600
    },
    {
      "epoch": 7.916618323858143,
      "grad_norm": 14.104405403137207,
      "learning_rate": 4.340281806345155e-05,
      "loss": 2.2437,
      "step": 204700
    },
    {
      "epoch": 7.920485748540047,
      "grad_norm": 12.20433521270752,
      "learning_rate": 4.3399595209549964e-05,
      "loss": 2.2485,
      "step": 204800
    },
    {
      "epoch": 7.924353173221951,
      "grad_norm": 11.377140998840332,
      "learning_rate": 4.339637235564837e-05,
      "loss": 2.2103,
      "step": 204900
    },
    {
      "epoch": 7.928220597903856,
      "grad_norm": 18.126007080078125,
      "learning_rate": 4.339314950174679e-05,
      "loss": 2.3328,
      "step": 205000
    },
    {
      "epoch": 7.93208802258576,
      "grad_norm": 10.0670166015625,
      "learning_rate": 4.33899266478452e-05,
      "loss": 2.2218,
      "step": 205100
    },
    {
      "epoch": 7.9359554472676646,
      "grad_norm": 13.441219329833984,
      "learning_rate": 4.3386703793943616e-05,
      "loss": 2.1815,
      "step": 205200
    },
    {
      "epoch": 7.939822871949569,
      "grad_norm": 12.293261528015137,
      "learning_rate": 4.3383480940042024e-05,
      "loss": 2.1934,
      "step": 205300
    },
    {
      "epoch": 7.943690296631473,
      "grad_norm": 10.38304615020752,
      "learning_rate": 4.338025808614044e-05,
      "loss": 2.2536,
      "step": 205400
    },
    {
      "epoch": 7.947557721313378,
      "grad_norm": 14.346477508544922,
      "learning_rate": 4.3377035232238854e-05,
      "loss": 2.2054,
      "step": 205500
    },
    {
      "epoch": 7.951425145995282,
      "grad_norm": 13.012307167053223,
      "learning_rate": 4.337381237833727e-05,
      "loss": 2.2728,
      "step": 205600
    },
    {
      "epoch": 7.955292570677186,
      "grad_norm": 10.532224655151367,
      "learning_rate": 4.3370589524435676e-05,
      "loss": 2.1356,
      "step": 205700
    },
    {
      "epoch": 7.959159995359091,
      "grad_norm": 13.502227783203125,
      "learning_rate": 4.336736667053409e-05,
      "loss": 2.2126,
      "step": 205800
    },
    {
      "epoch": 7.963027420040994,
      "grad_norm": 11.90266227722168,
      "learning_rate": 4.3364143816632506e-05,
      "loss": 2.2296,
      "step": 205900
    },
    {
      "epoch": 7.966894844722899,
      "grad_norm": 10.825054168701172,
      "learning_rate": 4.336092096273092e-05,
      "loss": 2.2832,
      "step": 206000
    },
    {
      "epoch": 7.970762269404803,
      "grad_norm": 11.069661140441895,
      "learning_rate": 4.335769810882933e-05,
      "loss": 2.2301,
      "step": 206100
    },
    {
      "epoch": 7.974629694086707,
      "grad_norm": 12.359528541564941,
      "learning_rate": 4.335447525492774e-05,
      "loss": 2.1822,
      "step": 206200
    },
    {
      "epoch": 7.978497118768612,
      "grad_norm": 12.780341148376465,
      "learning_rate": 4.335125240102616e-05,
      "loss": 2.2035,
      "step": 206300
    },
    {
      "epoch": 7.982364543450516,
      "grad_norm": 12.565465927124023,
      "learning_rate": 4.334802954712457e-05,
      "loss": 2.1747,
      "step": 206400
    },
    {
      "epoch": 7.9862319681324205,
      "grad_norm": 9.929458618164062,
      "learning_rate": 4.334480669322298e-05,
      "loss": 2.3566,
      "step": 206500
    },
    {
      "epoch": 7.990099392814325,
      "grad_norm": 9.935774803161621,
      "learning_rate": 4.3341583839321395e-05,
      "loss": 2.1647,
      "step": 206600
    },
    {
      "epoch": 7.993966817496229,
      "grad_norm": 12.37850570678711,
      "learning_rate": 4.333836098541981e-05,
      "loss": 2.2555,
      "step": 206700
    },
    {
      "epoch": 7.997834242178134,
      "grad_norm": 16.281267166137695,
      "learning_rate": 4.3335138131518225e-05,
      "loss": 2.2087,
      "step": 206800
    },
    {
      "epoch": 8.0,
      "eval_loss": 2.0887699127197266,
      "eval_runtime": 5.8978,
      "eval_samples_per_second": 230.763,
      "eval_steps_per_second": 230.763,
      "step": 206856
    },
    {
      "epoch": 8.0,
      "eval_loss": 2.0563902854919434,
      "eval_runtime": 113.038,
      "eval_samples_per_second": 228.746,
      "eval_steps_per_second": 228.746,
      "step": 206856
    },
    {
      "epoch": 8.001701666860038,
      "grad_norm": 12.670221328735352,
      "learning_rate": 4.333191527761663e-05,
      "loss": 2.3345,
      "step": 206900
    },
    {
      "epoch": 8.005569091541942,
      "grad_norm": 12.531571388244629,
      "learning_rate": 4.332869242371505e-05,
      "loss": 2.2357,
      "step": 207000
    },
    {
      "epoch": 8.009436516223847,
      "grad_norm": 8.926356315612793,
      "learning_rate": 4.332546956981346e-05,
      "loss": 2.2139,
      "step": 207100
    },
    {
      "epoch": 8.01330394090575,
      "grad_norm": 11.228642463684082,
      "learning_rate": 4.332224671591188e-05,
      "loss": 2.3011,
      "step": 207200
    },
    {
      "epoch": 8.017171365587656,
      "grad_norm": 13.394743919372559,
      "learning_rate": 4.331902386201029e-05,
      "loss": 2.2666,
      "step": 207300
    },
    {
      "epoch": 8.021038790269559,
      "grad_norm": 11.20492172241211,
      "learning_rate": 4.33158010081087e-05,
      "loss": 2.1944,
      "step": 207400
    },
    {
      "epoch": 8.024906214951464,
      "grad_norm": 9.573433876037598,
      "learning_rate": 4.3312578154207114e-05,
      "loss": 2.1303,
      "step": 207500
    },
    {
      "epoch": 8.028773639633368,
      "grad_norm": 13.782215118408203,
      "learning_rate": 4.330935530030553e-05,
      "loss": 2.1185,
      "step": 207600
    },
    {
      "epoch": 8.032641064315273,
      "grad_norm": 12.742391586303711,
      "learning_rate": 4.3306132446403944e-05,
      "loss": 2.2064,
      "step": 207700
    },
    {
      "epoch": 8.036508488997177,
      "grad_norm": 10.971728324890137,
      "learning_rate": 4.330290959250236e-05,
      "loss": 2.2229,
      "step": 207800
    },
    {
      "epoch": 8.040375913679082,
      "grad_norm": 11.678144454956055,
      "learning_rate": 4.329968673860077e-05,
      "loss": 2.186,
      "step": 207900
    },
    {
      "epoch": 8.044243338360985,
      "grad_norm": 17.35743522644043,
      "learning_rate": 4.329646388469918e-05,
      "loss": 2.1905,
      "step": 208000
    },
    {
      "epoch": 8.04811076304289,
      "grad_norm": 8.495697975158691,
      "learning_rate": 4.3293241030797596e-05,
      "loss": 2.2283,
      "step": 208100
    },
    {
      "epoch": 8.051978187724794,
      "grad_norm": 8.92617416381836,
      "learning_rate": 4.329001817689601e-05,
      "loss": 2.1728,
      "step": 208200
    },
    {
      "epoch": 8.055845612406697,
      "grad_norm": 11.166115760803223,
      "learning_rate": 4.3286795322994425e-05,
      "loss": 2.237,
      "step": 208300
    },
    {
      "epoch": 8.059713037088603,
      "grad_norm": 10.53809642791748,
      "learning_rate": 4.328357246909283e-05,
      "loss": 2.2407,
      "step": 208400
    },
    {
      "epoch": 8.063580461770506,
      "grad_norm": 10.92932415008545,
      "learning_rate": 4.328034961519125e-05,
      "loss": 2.2541,
      "step": 208500
    },
    {
      "epoch": 8.067447886452412,
      "grad_norm": 11.447908401489258,
      "learning_rate": 4.327712676128966e-05,
      "loss": 2.1766,
      "step": 208600
    },
    {
      "epoch": 8.071315311134315,
      "grad_norm": 10.203829765319824,
      "learning_rate": 4.327390390738808e-05,
      "loss": 2.2103,
      "step": 208700
    },
    {
      "epoch": 8.07518273581622,
      "grad_norm": 13.660066604614258,
      "learning_rate": 4.3270681053486485e-05,
      "loss": 2.2505,
      "step": 208800
    },
    {
      "epoch": 8.079050160498124,
      "grad_norm": 11.947287559509277,
      "learning_rate": 4.32674581995849e-05,
      "loss": 2.2581,
      "step": 208900
    },
    {
      "epoch": 8.082917585180029,
      "grad_norm": 10.587019920349121,
      "learning_rate": 4.3264235345683315e-05,
      "loss": 2.2068,
      "step": 209000
    },
    {
      "epoch": 8.086785009861932,
      "grad_norm": 9.886767387390137,
      "learning_rate": 4.326101249178173e-05,
      "loss": 2.1784,
      "step": 209100
    },
    {
      "epoch": 8.090652434543838,
      "grad_norm": 9.63099193572998,
      "learning_rate": 4.325778963788014e-05,
      "loss": 2.1816,
      "step": 209200
    },
    {
      "epoch": 8.094519859225741,
      "grad_norm": 11.492592811584473,
      "learning_rate": 4.325456678397855e-05,
      "loss": 2.1236,
      "step": 209300
    },
    {
      "epoch": 8.098387283907647,
      "grad_norm": 10.920870780944824,
      "learning_rate": 4.325134393007697e-05,
      "loss": 2.2366,
      "step": 209400
    },
    {
      "epoch": 8.10225470858955,
      "grad_norm": 16.82801628112793,
      "learning_rate": 4.3248121076175375e-05,
      "loss": 2.2182,
      "step": 209500
    },
    {
      "epoch": 8.106122133271455,
      "grad_norm": 14.28728199005127,
      "learning_rate": 4.324489822227379e-05,
      "loss": 2.2051,
      "step": 209600
    },
    {
      "epoch": 8.109989557953359,
      "grad_norm": 14.546550750732422,
      "learning_rate": 4.3241675368372204e-05,
      "loss": 2.2061,
      "step": 209700
    },
    {
      "epoch": 8.113856982635264,
      "grad_norm": 11.246026039123535,
      "learning_rate": 4.323845251447062e-05,
      "loss": 2.2295,
      "step": 209800
    },
    {
      "epoch": 8.117724407317167,
      "grad_norm": 13.559538841247559,
      "learning_rate": 4.323522966056903e-05,
      "loss": 2.2558,
      "step": 209900
    },
    {
      "epoch": 8.121591831999071,
      "grad_norm": 10.381031036376953,
      "learning_rate": 4.323200680666744e-05,
      "loss": 2.1901,
      "step": 210000
    },
    {
      "epoch": 8.125459256680976,
      "grad_norm": 10.747252464294434,
      "learning_rate": 4.3228783952765856e-05,
      "loss": 2.1562,
      "step": 210100
    },
    {
      "epoch": 8.12932668136288,
      "grad_norm": 11.834875106811523,
      "learning_rate": 4.322556109886427e-05,
      "loss": 2.2535,
      "step": 210200
    },
    {
      "epoch": 8.133194106044785,
      "grad_norm": 9.850801467895508,
      "learning_rate": 4.322233824496268e-05,
      "loss": 2.2323,
      "step": 210300
    },
    {
      "epoch": 8.137061530726688,
      "grad_norm": 11.675289154052734,
      "learning_rate": 4.3219115391061094e-05,
      "loss": 2.2003,
      "step": 210400
    },
    {
      "epoch": 8.140928955408594,
      "grad_norm": 10.317261695861816,
      "learning_rate": 4.321589253715951e-05,
      "loss": 2.1604,
      "step": 210500
    },
    {
      "epoch": 8.144796380090497,
      "grad_norm": 11.205028533935547,
      "learning_rate": 4.321266968325792e-05,
      "loss": 2.2351,
      "step": 210600
    },
    {
      "epoch": 8.148663804772402,
      "grad_norm": 15.602287292480469,
      "learning_rate": 4.320944682935633e-05,
      "loss": 2.2895,
      "step": 210700
    },
    {
      "epoch": 8.152531229454306,
      "grad_norm": 11.172900199890137,
      "learning_rate": 4.3206223975454746e-05,
      "loss": 2.2284,
      "step": 210800
    },
    {
      "epoch": 8.156398654136211,
      "grad_norm": 12.346579551696777,
      "learning_rate": 4.320300112155316e-05,
      "loss": 2.1474,
      "step": 210900
    },
    {
      "epoch": 8.160266078818115,
      "grad_norm": 9.384201049804688,
      "learning_rate": 4.3199778267651575e-05,
      "loss": 2.225,
      "step": 211000
    },
    {
      "epoch": 8.16413350350002,
      "grad_norm": 13.962628364562988,
      "learning_rate": 4.319655541374998e-05,
      "loss": 2.1561,
      "step": 211100
    },
    {
      "epoch": 8.168000928181923,
      "grad_norm": 12.86137580871582,
      "learning_rate": 4.31933325598484e-05,
      "loss": 2.2703,
      "step": 211200
    },
    {
      "epoch": 8.171868352863829,
      "grad_norm": 9.564162254333496,
      "learning_rate": 4.319010970594681e-05,
      "loss": 2.2607,
      "step": 211300
    },
    {
      "epoch": 8.175735777545732,
      "grad_norm": 12.482871055603027,
      "learning_rate": 4.318688685204523e-05,
      "loss": 2.3108,
      "step": 211400
    },
    {
      "epoch": 8.179603202227637,
      "grad_norm": 11.944853782653809,
      "learning_rate": 4.3183663998143635e-05,
      "loss": 2.215,
      "step": 211500
    },
    {
      "epoch": 8.183470626909541,
      "grad_norm": 13.299302101135254,
      "learning_rate": 4.318044114424205e-05,
      "loss": 2.1701,
      "step": 211600
    },
    {
      "epoch": 8.187338051591444,
      "grad_norm": 8.67965030670166,
      "learning_rate": 4.3177218290340465e-05,
      "loss": 2.1994,
      "step": 211700
    },
    {
      "epoch": 8.19120547627335,
      "grad_norm": 13.16946029663086,
      "learning_rate": 4.317399543643888e-05,
      "loss": 2.1726,
      "step": 211800
    },
    {
      "epoch": 8.195072900955253,
      "grad_norm": 10.601174354553223,
      "learning_rate": 4.317077258253729e-05,
      "loss": 2.2538,
      "step": 211900
    },
    {
      "epoch": 8.198940325637158,
      "grad_norm": 15.513527870178223,
      "learning_rate": 4.31675497286357e-05,
      "loss": 2.2618,
      "step": 212000
    },
    {
      "epoch": 8.202807750319062,
      "grad_norm": 10.785868644714355,
      "learning_rate": 4.316432687473412e-05,
      "loss": 2.184,
      "step": 212100
    },
    {
      "epoch": 8.206675175000967,
      "grad_norm": 12.557575225830078,
      "learning_rate": 4.316110402083253e-05,
      "loss": 2.1393,
      "step": 212200
    },
    {
      "epoch": 8.21054259968287,
      "grad_norm": 12.358692169189453,
      "learning_rate": 4.315788116693094e-05,
      "loss": 2.1954,
      "step": 212300
    },
    {
      "epoch": 8.214410024364776,
      "grad_norm": 12.141637802124023,
      "learning_rate": 4.3154658313029354e-05,
      "loss": 2.2526,
      "step": 212400
    },
    {
      "epoch": 8.21827744904668,
      "grad_norm": 13.102761268615723,
      "learning_rate": 4.315143545912777e-05,
      "loss": 2.2159,
      "step": 212500
    },
    {
      "epoch": 8.222144873728585,
      "grad_norm": 11.696256637573242,
      "learning_rate": 4.314821260522618e-05,
      "loss": 2.192,
      "step": 212600
    },
    {
      "epoch": 8.226012298410488,
      "grad_norm": 9.430859565734863,
      "learning_rate": 4.314498975132459e-05,
      "loss": 2.2487,
      "step": 212700
    },
    {
      "epoch": 8.229879723092393,
      "grad_norm": 11.786030769348145,
      "learning_rate": 4.3141766897423006e-05,
      "loss": 2.2022,
      "step": 212800
    },
    {
      "epoch": 8.233747147774297,
      "grad_norm": 10.891024589538574,
      "learning_rate": 4.313854404352142e-05,
      "loss": 2.1438,
      "step": 212900
    },
    {
      "epoch": 8.237614572456202,
      "grad_norm": 12.984312057495117,
      "learning_rate": 4.313532118961983e-05,
      "loss": 2.2145,
      "step": 213000
    },
    {
      "epoch": 8.241481997138106,
      "grad_norm": 12.762530326843262,
      "learning_rate": 4.3132098335718244e-05,
      "loss": 2.204,
      "step": 213100
    },
    {
      "epoch": 8.24534942182001,
      "grad_norm": 13.856260299682617,
      "learning_rate": 4.312887548181666e-05,
      "loss": 2.2134,
      "step": 213200
    },
    {
      "epoch": 8.249216846501914,
      "grad_norm": 12.936491966247559,
      "learning_rate": 4.312565262791507e-05,
      "loss": 2.255,
      "step": 213300
    },
    {
      "epoch": 8.253084271183818,
      "grad_norm": 11.751813888549805,
      "learning_rate": 4.312242977401348e-05,
      "loss": 2.2279,
      "step": 213400
    },
    {
      "epoch": 8.256951695865723,
      "grad_norm": 11.415206909179688,
      "learning_rate": 4.3119206920111896e-05,
      "loss": 2.0486,
      "step": 213500
    },
    {
      "epoch": 8.260819120547627,
      "grad_norm": 16.07219123840332,
      "learning_rate": 4.311598406621031e-05,
      "loss": 2.2225,
      "step": 213600
    },
    {
      "epoch": 8.264686545229532,
      "grad_norm": 7.8622660636901855,
      "learning_rate": 4.3112761212308725e-05,
      "loss": 2.1584,
      "step": 213700
    },
    {
      "epoch": 8.268553969911435,
      "grad_norm": 10.490405082702637,
      "learning_rate": 4.310953835840714e-05,
      "loss": 2.2028,
      "step": 213800
    },
    {
      "epoch": 8.27242139459334,
      "grad_norm": 15.816308975219727,
      "learning_rate": 4.310631550450555e-05,
      "loss": 2.2228,
      "step": 213900
    },
    {
      "epoch": 8.276288819275244,
      "grad_norm": 12.728180885314941,
      "learning_rate": 4.310309265060396e-05,
      "loss": 2.1965,
      "step": 214000
    },
    {
      "epoch": 8.28015624395715,
      "grad_norm": 9.742600440979004,
      "learning_rate": 4.309986979670238e-05,
      "loss": 2.3018,
      "step": 214100
    },
    {
      "epoch": 8.284023668639053,
      "grad_norm": 10.178526878356934,
      "learning_rate": 4.309664694280079e-05,
      "loss": 2.2425,
      "step": 214200
    },
    {
      "epoch": 8.287891093320958,
      "grad_norm": 10.465962409973145,
      "learning_rate": 4.309342408889921e-05,
      "loss": 2.1497,
      "step": 214300
    },
    {
      "epoch": 8.291758518002862,
      "grad_norm": 9.562175750732422,
      "learning_rate": 4.309020123499762e-05,
      "loss": 2.1974,
      "step": 214400
    },
    {
      "epoch": 8.295625942684767,
      "grad_norm": 12.949968338012695,
      "learning_rate": 4.308697838109603e-05,
      "loss": 2.168,
      "step": 214500
    },
    {
      "epoch": 8.29949336736667,
      "grad_norm": 10.137618064880371,
      "learning_rate": 4.3083755527194444e-05,
      "loss": 2.1641,
      "step": 214600
    },
    {
      "epoch": 8.303360792048576,
      "grad_norm": 11.74106502532959,
      "learning_rate": 4.308053267329286e-05,
      "loss": 2.1603,
      "step": 214700
    },
    {
      "epoch": 8.30722821673048,
      "grad_norm": 10.577446937561035,
      "learning_rate": 4.3077309819391274e-05,
      "loss": 2.2216,
      "step": 214800
    },
    {
      "epoch": 8.311095641412383,
      "grad_norm": 13.158750534057617,
      "learning_rate": 4.307408696548969e-05,
      "loss": 2.2409,
      "step": 214900
    },
    {
      "epoch": 8.314963066094288,
      "grad_norm": 11.615138053894043,
      "learning_rate": 4.3070864111588096e-05,
      "loss": 2.1335,
      "step": 215000
    },
    {
      "epoch": 8.318830490776191,
      "grad_norm": 11.968796730041504,
      "learning_rate": 4.306764125768651e-05,
      "loss": 2.2236,
      "step": 215100
    },
    {
      "epoch": 8.322697915458097,
      "grad_norm": 15.307527542114258,
      "learning_rate": 4.3064418403784926e-05,
      "loss": 2.244,
      "step": 215200
    },
    {
      "epoch": 8.32656534014,
      "grad_norm": 11.683287620544434,
      "learning_rate": 4.306119554988334e-05,
      "loss": 2.1851,
      "step": 215300
    },
    {
      "epoch": 8.330432764821905,
      "grad_norm": 14.352900505065918,
      "learning_rate": 4.305797269598175e-05,
      "loss": 2.2934,
      "step": 215400
    },
    {
      "epoch": 8.334300189503809,
      "grad_norm": 12.927101135253906,
      "learning_rate": 4.305474984208016e-05,
      "loss": 2.2575,
      "step": 215500
    },
    {
      "epoch": 8.338167614185714,
      "grad_norm": 13.371002197265625,
      "learning_rate": 4.305152698817858e-05,
      "loss": 2.1737,
      "step": 215600
    },
    {
      "epoch": 8.342035038867618,
      "grad_norm": 12.674132347106934,
      "learning_rate": 4.3048304134276986e-05,
      "loss": 2.2183,
      "step": 215700
    },
    {
      "epoch": 8.345902463549523,
      "grad_norm": 14.758345603942871,
      "learning_rate": 4.30450812803754e-05,
      "loss": 2.1735,
      "step": 215800
    },
    {
      "epoch": 8.349769888231426,
      "grad_norm": 13.514239311218262,
      "learning_rate": 4.3041858426473815e-05,
      "loss": 2.2027,
      "step": 215900
    },
    {
      "epoch": 8.353637312913332,
      "grad_norm": 13.338539123535156,
      "learning_rate": 4.303863557257223e-05,
      "loss": 2.1108,
      "step": 216000
    },
    {
      "epoch": 8.357504737595235,
      "grad_norm": 12.559454917907715,
      "learning_rate": 4.303541271867064e-05,
      "loss": 2.2581,
      "step": 216100
    },
    {
      "epoch": 8.36137216227714,
      "grad_norm": 12.39059066772461,
      "learning_rate": 4.303218986476905e-05,
      "loss": 2.1132,
      "step": 216200
    },
    {
      "epoch": 8.365239586959044,
      "grad_norm": 11.021930694580078,
      "learning_rate": 4.302896701086747e-05,
      "loss": 2.2319,
      "step": 216300
    },
    {
      "epoch": 8.36910701164095,
      "grad_norm": 8.592711448669434,
      "learning_rate": 4.302574415696588e-05,
      "loss": 2.1315,
      "step": 216400
    },
    {
      "epoch": 8.372974436322853,
      "grad_norm": 11.471406936645508,
      "learning_rate": 4.302252130306429e-05,
      "loss": 2.2608,
      "step": 216500
    },
    {
      "epoch": 8.376841861004756,
      "grad_norm": 11.004861831665039,
      "learning_rate": 4.3019298449162705e-05,
      "loss": 2.2682,
      "step": 216600
    },
    {
      "epoch": 8.380709285686661,
      "grad_norm": 10.512377738952637,
      "learning_rate": 4.301607559526112e-05,
      "loss": 2.2206,
      "step": 216700
    },
    {
      "epoch": 8.384576710368565,
      "grad_norm": 12.671363830566406,
      "learning_rate": 4.3012852741359534e-05,
      "loss": 2.1784,
      "step": 216800
    },
    {
      "epoch": 8.38844413505047,
      "grad_norm": 11.022843360900879,
      "learning_rate": 4.300962988745794e-05,
      "loss": 2.2009,
      "step": 216900
    },
    {
      "epoch": 8.392311559732374,
      "grad_norm": 15.267773628234863,
      "learning_rate": 4.300640703355636e-05,
      "loss": 2.2188,
      "step": 217000
    },
    {
      "epoch": 8.396178984414279,
      "grad_norm": 12.616789817810059,
      "learning_rate": 4.300318417965477e-05,
      "loss": 2.2578,
      "step": 217100
    },
    {
      "epoch": 8.400046409096182,
      "grad_norm": 14.935312271118164,
      "learning_rate": 4.2999961325753186e-05,
      "loss": 2.2021,
      "step": 217200
    },
    {
      "epoch": 8.403913833778088,
      "grad_norm": 9.563230514526367,
      "learning_rate": 4.2996738471851594e-05,
      "loss": 2.1841,
      "step": 217300
    },
    {
      "epoch": 8.407781258459991,
      "grad_norm": 11.786503791809082,
      "learning_rate": 4.299351561795001e-05,
      "loss": 2.2501,
      "step": 217400
    },
    {
      "epoch": 8.411648683141896,
      "grad_norm": 13.566619873046875,
      "learning_rate": 4.2990292764048423e-05,
      "loss": 2.1792,
      "step": 217500
    },
    {
      "epoch": 8.4155161078238,
      "grad_norm": 10.092617988586426,
      "learning_rate": 4.298706991014684e-05,
      "loss": 2.2842,
      "step": 217600
    },
    {
      "epoch": 8.419383532505705,
      "grad_norm": 11.726369857788086,
      "learning_rate": 4.2983847056245246e-05,
      "loss": 2.2064,
      "step": 217700
    },
    {
      "epoch": 8.423250957187609,
      "grad_norm": 9.240487098693848,
      "learning_rate": 4.298062420234366e-05,
      "loss": 2.1877,
      "step": 217800
    },
    {
      "epoch": 8.427118381869514,
      "grad_norm": 10.715977668762207,
      "learning_rate": 4.2977401348442076e-05,
      "loss": 2.2444,
      "step": 217900
    },
    {
      "epoch": 8.430985806551417,
      "grad_norm": 11.492835998535156,
      "learning_rate": 4.297417849454049e-05,
      "loss": 2.3005,
      "step": 218000
    },
    {
      "epoch": 8.434853231233323,
      "grad_norm": 9.29169750213623,
      "learning_rate": 4.29709556406389e-05,
      "loss": 2.1543,
      "step": 218100
    },
    {
      "epoch": 8.438720655915226,
      "grad_norm": 12.381664276123047,
      "learning_rate": 4.296773278673731e-05,
      "loss": 2.2231,
      "step": 218200
    },
    {
      "epoch": 8.44258808059713,
      "grad_norm": 9.655536651611328,
      "learning_rate": 4.296450993283573e-05,
      "loss": 2.2353,
      "step": 218300
    },
    {
      "epoch": 8.446455505279035,
      "grad_norm": 10.013707160949707,
      "learning_rate": 4.2961287078934136e-05,
      "loss": 2.1954,
      "step": 218400
    },
    {
      "epoch": 8.450322929960938,
      "grad_norm": 11.447029113769531,
      "learning_rate": 4.295806422503255e-05,
      "loss": 2.1938,
      "step": 218500
    },
    {
      "epoch": 8.454190354642844,
      "grad_norm": 15.984798431396484,
      "learning_rate": 4.2954841371130965e-05,
      "loss": 2.1472,
      "step": 218600
    },
    {
      "epoch": 8.458057779324747,
      "grad_norm": 9.760367393493652,
      "learning_rate": 4.295161851722938e-05,
      "loss": 2.1576,
      "step": 218700
    },
    {
      "epoch": 8.461925204006652,
      "grad_norm": 9.05170726776123,
      "learning_rate": 4.294839566332779e-05,
      "loss": 2.2429,
      "step": 218800
    },
    {
      "epoch": 8.465792628688556,
      "grad_norm": 11.450088500976562,
      "learning_rate": 4.29451728094262e-05,
      "loss": 2.1739,
      "step": 218900
    },
    {
      "epoch": 8.469660053370461,
      "grad_norm": 13.62014389038086,
      "learning_rate": 4.294194995552462e-05,
      "loss": 2.2586,
      "step": 219000
    },
    {
      "epoch": 8.473527478052365,
      "grad_norm": 11.730224609375,
      "learning_rate": 4.293872710162303e-05,
      "loss": 2.178,
      "step": 219100
    },
    {
      "epoch": 8.47739490273427,
      "grad_norm": 12.561121940612793,
      "learning_rate": 4.293550424772144e-05,
      "loss": 2.2323,
      "step": 219200
    },
    {
      "epoch": 8.481262327416173,
      "grad_norm": 9.57602596282959,
      "learning_rate": 4.2932281393819855e-05,
      "loss": 2.2636,
      "step": 219300
    },
    {
      "epoch": 8.485129752098079,
      "grad_norm": 11.402158737182617,
      "learning_rate": 4.292905853991827e-05,
      "loss": 2.1011,
      "step": 219400
    },
    {
      "epoch": 8.488997176779982,
      "grad_norm": 10.338915824890137,
      "learning_rate": 4.2925835686016684e-05,
      "loss": 2.1354,
      "step": 219500
    },
    {
      "epoch": 8.492864601461887,
      "grad_norm": 10.770048141479492,
      "learning_rate": 4.292261283211509e-05,
      "loss": 2.1543,
      "step": 219600
    },
    {
      "epoch": 8.49673202614379,
      "grad_norm": 8.951868057250977,
      "learning_rate": 4.291938997821351e-05,
      "loss": 2.235,
      "step": 219700
    },
    {
      "epoch": 8.500599450825694,
      "grad_norm": 10.50550365447998,
      "learning_rate": 4.291616712431192e-05,
      "loss": 2.1883,
      "step": 219800
    },
    {
      "epoch": 8.5044668755076,
      "grad_norm": 10.641298294067383,
      "learning_rate": 4.2912944270410336e-05,
      "loss": 2.2791,
      "step": 219900
    },
    {
      "epoch": 8.508334300189503,
      "grad_norm": 11.40611457824707,
      "learning_rate": 4.2909721416508744e-05,
      "loss": 2.1849,
      "step": 220000
    },
    {
      "epoch": 8.512201724871408,
      "grad_norm": 12.487443923950195,
      "learning_rate": 4.290649856260716e-05,
      "loss": 2.2538,
      "step": 220100
    },
    {
      "epoch": 8.516069149553312,
      "grad_norm": 11.553535461425781,
      "learning_rate": 4.2903275708705573e-05,
      "loss": 2.1776,
      "step": 220200
    },
    {
      "epoch": 8.519936574235217,
      "grad_norm": 10.97367000579834,
      "learning_rate": 4.290005285480399e-05,
      "loss": 2.2225,
      "step": 220300
    },
    {
      "epoch": 8.52380399891712,
      "grad_norm": 10.095280647277832,
      "learning_rate": 4.2896830000902396e-05,
      "loss": 2.2142,
      "step": 220400
    },
    {
      "epoch": 8.527671423599026,
      "grad_norm": 11.333436012268066,
      "learning_rate": 4.289360714700081e-05,
      "loss": 2.2486,
      "step": 220500
    },
    {
      "epoch": 8.53153884828093,
      "grad_norm": 11.57818603515625,
      "learning_rate": 4.2890384293099226e-05,
      "loss": 2.2686,
      "step": 220600
    },
    {
      "epoch": 8.535406272962835,
      "grad_norm": 11.152509689331055,
      "learning_rate": 4.288716143919764e-05,
      "loss": 2.2238,
      "step": 220700
    },
    {
      "epoch": 8.539273697644738,
      "grad_norm": 11.312867164611816,
      "learning_rate": 4.2883938585296055e-05,
      "loss": 2.1918,
      "step": 220800
    },
    {
      "epoch": 8.543141122326643,
      "grad_norm": 11.144046783447266,
      "learning_rate": 4.288071573139446e-05,
      "loss": 2.1144,
      "step": 220900
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 12.636210441589355,
      "learning_rate": 4.287749287749288e-05,
      "loss": 2.2268,
      "step": 221000
    },
    {
      "epoch": 8.550875971690452,
      "grad_norm": 11.686417579650879,
      "learning_rate": 4.287427002359129e-05,
      "loss": 2.168,
      "step": 221100
    },
    {
      "epoch": 8.554743396372356,
      "grad_norm": 11.359848022460938,
      "learning_rate": 4.287104716968971e-05,
      "loss": 2.1941,
      "step": 221200
    },
    {
      "epoch": 8.558610821054259,
      "grad_norm": 10.633407592773438,
      "learning_rate": 4.286782431578812e-05,
      "loss": 2.1596,
      "step": 221300
    },
    {
      "epoch": 8.562478245736164,
      "grad_norm": 11.3575439453125,
      "learning_rate": 4.2864601461886537e-05,
      "loss": 2.2293,
      "step": 221400
    },
    {
      "epoch": 8.566345670418068,
      "grad_norm": 10.155577659606934,
      "learning_rate": 4.2861378607984944e-05,
      "loss": 2.2181,
      "step": 221500
    },
    {
      "epoch": 8.570213095099973,
      "grad_norm": 9.507963180541992,
      "learning_rate": 4.285815575408336e-05,
      "loss": 2.2065,
      "step": 221600
    },
    {
      "epoch": 8.574080519781877,
      "grad_norm": 11.22316837310791,
      "learning_rate": 4.2854932900181774e-05,
      "loss": 2.2152,
      "step": 221700
    },
    {
      "epoch": 8.577947944463782,
      "grad_norm": 8.505548477172852,
      "learning_rate": 4.285171004628019e-05,
      "loss": 2.2622,
      "step": 221800
    },
    {
      "epoch": 8.581815369145685,
      "grad_norm": 9.712578773498535,
      "learning_rate": 4.2848487192378597e-05,
      "loss": 2.1996,
      "step": 221900
    },
    {
      "epoch": 8.58568279382759,
      "grad_norm": 11.723159790039062,
      "learning_rate": 4.284526433847701e-05,
      "loss": 2.1641,
      "step": 222000
    },
    {
      "epoch": 8.589550218509494,
      "grad_norm": 13.26596736907959,
      "learning_rate": 4.2842041484575426e-05,
      "loss": 2.2303,
      "step": 222100
    },
    {
      "epoch": 8.5934176431914,
      "grad_norm": 12.34197998046875,
      "learning_rate": 4.283881863067384e-05,
      "loss": 2.1778,
      "step": 222200
    },
    {
      "epoch": 8.597285067873303,
      "grad_norm": 9.9983549118042,
      "learning_rate": 4.283559577677225e-05,
      "loss": 2.2867,
      "step": 222300
    },
    {
      "epoch": 8.601152492555208,
      "grad_norm": 9.74915599822998,
      "learning_rate": 4.283237292287066e-05,
      "loss": 2.2428,
      "step": 222400
    },
    {
      "epoch": 8.605019917237112,
      "grad_norm": 14.153769493103027,
      "learning_rate": 4.282915006896908e-05,
      "loss": 2.3088,
      "step": 222500
    },
    {
      "epoch": 8.608887341919017,
      "grad_norm": 10.793010711669922,
      "learning_rate": 4.282592721506749e-05,
      "loss": 2.1135,
      "step": 222600
    },
    {
      "epoch": 8.61275476660092,
      "grad_norm": 10.634591102600098,
      "learning_rate": 4.28227043611659e-05,
      "loss": 2.2548,
      "step": 222700
    },
    {
      "epoch": 8.616622191282826,
      "grad_norm": 11.884855270385742,
      "learning_rate": 4.2819481507264315e-05,
      "loss": 2.18,
      "step": 222800
    },
    {
      "epoch": 8.620489615964729,
      "grad_norm": 12.968990325927734,
      "learning_rate": 4.281625865336273e-05,
      "loss": 2.2793,
      "step": 222900
    },
    {
      "epoch": 8.624357040646633,
      "grad_norm": 10.642663955688477,
      "learning_rate": 4.2813035799461145e-05,
      "loss": 2.1966,
      "step": 223000
    },
    {
      "epoch": 8.628224465328538,
      "grad_norm": 11.908950805664062,
      "learning_rate": 4.280981294555955e-05,
      "loss": 2.1492,
      "step": 223100
    },
    {
      "epoch": 8.632091890010441,
      "grad_norm": 16.68023681640625,
      "learning_rate": 4.280659009165797e-05,
      "loss": 2.2205,
      "step": 223200
    },
    {
      "epoch": 8.635959314692347,
      "grad_norm": 10.859761238098145,
      "learning_rate": 4.280336723775638e-05,
      "loss": 2.2017,
      "step": 223300
    },
    {
      "epoch": 8.63982673937425,
      "grad_norm": 9.728386878967285,
      "learning_rate": 4.28001443838548e-05,
      "loss": 2.2192,
      "step": 223400
    },
    {
      "epoch": 8.643694164056155,
      "grad_norm": 9.07637882232666,
      "learning_rate": 4.2796921529953205e-05,
      "loss": 2.2019,
      "step": 223500
    },
    {
      "epoch": 8.647561588738059,
      "grad_norm": 13.266793251037598,
      "learning_rate": 4.279369867605162e-05,
      "loss": 2.2375,
      "step": 223600
    },
    {
      "epoch": 8.651429013419964,
      "grad_norm": 11.538213729858398,
      "learning_rate": 4.2790475822150034e-05,
      "loss": 2.1408,
      "step": 223700
    },
    {
      "epoch": 8.655296438101868,
      "grad_norm": 8.64452075958252,
      "learning_rate": 4.278725296824845e-05,
      "loss": 2.1026,
      "step": 223800
    },
    {
      "epoch": 8.659163862783773,
      "grad_norm": 11.284327507019043,
      "learning_rate": 4.278403011434686e-05,
      "loss": 2.1708,
      "step": 223900
    },
    {
      "epoch": 8.663031287465676,
      "grad_norm": 13.23456859588623,
      "learning_rate": 4.278080726044527e-05,
      "loss": 2.2406,
      "step": 224000
    },
    {
      "epoch": 8.666898712147582,
      "grad_norm": 11.907127380371094,
      "learning_rate": 4.2777584406543686e-05,
      "loss": 2.2199,
      "step": 224100
    },
    {
      "epoch": 8.670766136829485,
      "grad_norm": 11.935140609741211,
      "learning_rate": 4.27743615526421e-05,
      "loss": 2.1637,
      "step": 224200
    },
    {
      "epoch": 8.67463356151139,
      "grad_norm": 11.402158737182617,
      "learning_rate": 4.277113869874051e-05,
      "loss": 2.1324,
      "step": 224300
    },
    {
      "epoch": 8.678500986193294,
      "grad_norm": 12.301904678344727,
      "learning_rate": 4.2767915844838924e-05,
      "loss": 2.169,
      "step": 224400
    },
    {
      "epoch": 8.682368410875199,
      "grad_norm": 11.333333969116211,
      "learning_rate": 4.276469299093734e-05,
      "loss": 2.2038,
      "step": 224500
    },
    {
      "epoch": 8.686235835557103,
      "grad_norm": 12.29476261138916,
      "learning_rate": 4.2761470137035747e-05,
      "loss": 2.2319,
      "step": 224600
    },
    {
      "epoch": 8.690103260239006,
      "grad_norm": 8.859933853149414,
      "learning_rate": 4.275824728313416e-05,
      "loss": 2.2274,
      "step": 224700
    },
    {
      "epoch": 8.693970684920911,
      "grad_norm": 11.584697723388672,
      "learning_rate": 4.2755024429232576e-05,
      "loss": 2.2869,
      "step": 224800
    },
    {
      "epoch": 8.697838109602815,
      "grad_norm": 10.208250045776367,
      "learning_rate": 4.275180157533099e-05,
      "loss": 2.2053,
      "step": 224900
    },
    {
      "epoch": 8.70170553428472,
      "grad_norm": 8.219146728515625,
      "learning_rate": 4.27485787214294e-05,
      "loss": 2.1741,
      "step": 225000
    },
    {
      "epoch": 8.705572958966624,
      "grad_norm": 13.275525093078613,
      "learning_rate": 4.274535586752781e-05,
      "loss": 2.155,
      "step": 225100
    },
    {
      "epoch": 8.709440383648529,
      "grad_norm": 10.01063060760498,
      "learning_rate": 4.274213301362623e-05,
      "loss": 2.246,
      "step": 225200
    },
    {
      "epoch": 8.713307808330432,
      "grad_norm": 12.446361541748047,
      "learning_rate": 4.273891015972464e-05,
      "loss": 2.1745,
      "step": 225300
    },
    {
      "epoch": 8.717175233012338,
      "grad_norm": 13.524484634399414,
      "learning_rate": 4.273568730582305e-05,
      "loss": 2.272,
      "step": 225400
    },
    {
      "epoch": 8.721042657694241,
      "grad_norm": 11.770200729370117,
      "learning_rate": 4.2732464451921465e-05,
      "loss": 2.2546,
      "step": 225500
    },
    {
      "epoch": 8.724910082376146,
      "grad_norm": 15.802268981933594,
      "learning_rate": 4.272924159801988e-05,
      "loss": 2.2571,
      "step": 225600
    },
    {
      "epoch": 8.72877750705805,
      "grad_norm": 10.844188690185547,
      "learning_rate": 4.2726018744118295e-05,
      "loss": 2.0893,
      "step": 225700
    },
    {
      "epoch": 8.732644931739955,
      "grad_norm": 11.259300231933594,
      "learning_rate": 4.27227958902167e-05,
      "loss": 2.1478,
      "step": 225800
    },
    {
      "epoch": 8.736512356421859,
      "grad_norm": 12.249700546264648,
      "learning_rate": 4.271957303631512e-05,
      "loss": 2.1187,
      "step": 225900
    },
    {
      "epoch": 8.740379781103764,
      "grad_norm": 9.079833984375,
      "learning_rate": 4.271635018241353e-05,
      "loss": 2.2291,
      "step": 226000
    },
    {
      "epoch": 8.744247205785667,
      "grad_norm": 12.062575340270996,
      "learning_rate": 4.271312732851195e-05,
      "loss": 2.228,
      "step": 226100
    },
    {
      "epoch": 8.748114630467573,
      "grad_norm": 15.962491989135742,
      "learning_rate": 4.2709904474610355e-05,
      "loss": 2.1626,
      "step": 226200
    },
    {
      "epoch": 8.751982055149476,
      "grad_norm": 10.55636215209961,
      "learning_rate": 4.270668162070877e-05,
      "loss": 2.1781,
      "step": 226300
    },
    {
      "epoch": 8.75584947983138,
      "grad_norm": 14.367155075073242,
      "learning_rate": 4.2703458766807184e-05,
      "loss": 2.2217,
      "step": 226400
    },
    {
      "epoch": 8.759716904513285,
      "grad_norm": 12.429353713989258,
      "learning_rate": 4.27002359129056e-05,
      "loss": 2.2508,
      "step": 226500
    },
    {
      "epoch": 8.763584329195188,
      "grad_norm": 9.420506477355957,
      "learning_rate": 4.269701305900401e-05,
      "loss": 2.1883,
      "step": 226600
    },
    {
      "epoch": 8.767451753877094,
      "grad_norm": 11.6668119430542,
      "learning_rate": 4.269379020510242e-05,
      "loss": 2.2279,
      "step": 226700
    },
    {
      "epoch": 8.771319178558997,
      "grad_norm": 12.819132804870605,
      "learning_rate": 4.2690567351200836e-05,
      "loss": 2.0842,
      "step": 226800
    },
    {
      "epoch": 8.775186603240902,
      "grad_norm": 12.512362480163574,
      "learning_rate": 4.268734449729925e-05,
      "loss": 2.2025,
      "step": 226900
    },
    {
      "epoch": 8.779054027922806,
      "grad_norm": 9.943926811218262,
      "learning_rate": 4.268412164339766e-05,
      "loss": 2.2155,
      "step": 227000
    },
    {
      "epoch": 8.782921452604711,
      "grad_norm": 10.191516876220703,
      "learning_rate": 4.2680898789496074e-05,
      "loss": 2.2073,
      "step": 227100
    },
    {
      "epoch": 8.786788877286615,
      "grad_norm": 11.535150527954102,
      "learning_rate": 4.267767593559449e-05,
      "loss": 2.1798,
      "step": 227200
    },
    {
      "epoch": 8.79065630196852,
      "grad_norm": 12.471918106079102,
      "learning_rate": 4.26744530816929e-05,
      "loss": 2.1808,
      "step": 227300
    },
    {
      "epoch": 8.794523726650423,
      "grad_norm": 10.114471435546875,
      "learning_rate": 4.267123022779131e-05,
      "loss": 2.1562,
      "step": 227400
    },
    {
      "epoch": 8.798391151332329,
      "grad_norm": 12.3662748336792,
      "learning_rate": 4.2668007373889726e-05,
      "loss": 2.1722,
      "step": 227500
    },
    {
      "epoch": 8.802258576014232,
      "grad_norm": 13.204353332519531,
      "learning_rate": 4.266478451998814e-05,
      "loss": 2.1601,
      "step": 227600
    },
    {
      "epoch": 8.806126000696137,
      "grad_norm": 8.485102653503418,
      "learning_rate": 4.2661561666086555e-05,
      "loss": 2.1948,
      "step": 227700
    },
    {
      "epoch": 8.80999342537804,
      "grad_norm": 12.618372917175293,
      "learning_rate": 4.265833881218497e-05,
      "loss": 2.1577,
      "step": 227800
    },
    {
      "epoch": 8.813860850059946,
      "grad_norm": 12.340635299682617,
      "learning_rate": 4.2655115958283385e-05,
      "loss": 2.1848,
      "step": 227900
    },
    {
      "epoch": 8.81772827474185,
      "grad_norm": 8.49302864074707,
      "learning_rate": 4.265189310438179e-05,
      "loss": 2.3033,
      "step": 228000
    },
    {
      "epoch": 8.821595699423753,
      "grad_norm": 10.092974662780762,
      "learning_rate": 4.264867025048021e-05,
      "loss": 2.1683,
      "step": 228100
    },
    {
      "epoch": 8.825463124105658,
      "grad_norm": 11.319445610046387,
      "learning_rate": 4.264544739657862e-05,
      "loss": 2.3066,
      "step": 228200
    },
    {
      "epoch": 8.829330548787562,
      "grad_norm": 14.197212219238281,
      "learning_rate": 4.264222454267704e-05,
      "loss": 2.1742,
      "step": 228300
    },
    {
      "epoch": 8.833197973469467,
      "grad_norm": 13.201326370239258,
      "learning_rate": 4.263900168877545e-05,
      "loss": 2.2794,
      "step": 228400
    },
    {
      "epoch": 8.83706539815137,
      "grad_norm": 15.119247436523438,
      "learning_rate": 4.263577883487386e-05,
      "loss": 2.348,
      "step": 228500
    },
    {
      "epoch": 8.840932822833276,
      "grad_norm": 12.099349021911621,
      "learning_rate": 4.2632555980972274e-05,
      "loss": 2.1813,
      "step": 228600
    },
    {
      "epoch": 8.84480024751518,
      "grad_norm": 11.235062599182129,
      "learning_rate": 4.262933312707069e-05,
      "loss": 2.1733,
      "step": 228700
    },
    {
      "epoch": 8.848667672197084,
      "grad_norm": 9.188131332397461,
      "learning_rate": 4.2626110273169104e-05,
      "loss": 2.197,
      "step": 228800
    },
    {
      "epoch": 8.852535096878988,
      "grad_norm": 12.16311264038086,
      "learning_rate": 4.262288741926751e-05,
      "loss": 2.1562,
      "step": 228900
    },
    {
      "epoch": 8.856402521560893,
      "grad_norm": 11.296134948730469,
      "learning_rate": 4.2619664565365926e-05,
      "loss": 2.163,
      "step": 229000
    },
    {
      "epoch": 8.860269946242797,
      "grad_norm": 13.97680377960205,
      "learning_rate": 4.261644171146434e-05,
      "loss": 2.1404,
      "step": 229100
    },
    {
      "epoch": 8.864137370924702,
      "grad_norm": 9.797869682312012,
      "learning_rate": 4.2613218857562756e-05,
      "loss": 2.1246,
      "step": 229200
    },
    {
      "epoch": 8.868004795606605,
      "grad_norm": 12.899264335632324,
      "learning_rate": 4.2609996003661164e-05,
      "loss": 2.2405,
      "step": 229300
    },
    {
      "epoch": 8.871872220288509,
      "grad_norm": 15.289679527282715,
      "learning_rate": 4.260677314975958e-05,
      "loss": 2.186,
      "step": 229400
    },
    {
      "epoch": 8.875739644970414,
      "grad_norm": 12.559998512268066,
      "learning_rate": 4.260355029585799e-05,
      "loss": 2.2184,
      "step": 229500
    },
    {
      "epoch": 8.879607069652318,
      "grad_norm": 9.856900215148926,
      "learning_rate": 4.260032744195641e-05,
      "loss": 2.2668,
      "step": 229600
    },
    {
      "epoch": 8.883474494334223,
      "grad_norm": 9.516974449157715,
      "learning_rate": 4.2597104588054816e-05,
      "loss": 2.1924,
      "step": 229700
    },
    {
      "epoch": 8.887341919016126,
      "grad_norm": 11.81993293762207,
      "learning_rate": 4.259388173415323e-05,
      "loss": 2.1434,
      "step": 229800
    },
    {
      "epoch": 8.891209343698032,
      "grad_norm": 9.579065322875977,
      "learning_rate": 4.2590658880251645e-05,
      "loss": 2.0741,
      "step": 229900
    },
    {
      "epoch": 8.895076768379935,
      "grad_norm": 15.599115371704102,
      "learning_rate": 4.258743602635006e-05,
      "loss": 2.1438,
      "step": 230000
    },
    {
      "epoch": 8.89894419306184,
      "grad_norm": 12.461519241333008,
      "learning_rate": 4.258421317244847e-05,
      "loss": 2.1745,
      "step": 230100
    },
    {
      "epoch": 8.902811617743744,
      "grad_norm": 10.799053192138672,
      "learning_rate": 4.258099031854688e-05,
      "loss": 2.1829,
      "step": 230200
    },
    {
      "epoch": 8.90667904242565,
      "grad_norm": 11.42292308807373,
      "learning_rate": 4.25777674646453e-05,
      "loss": 2.2079,
      "step": 230300
    },
    {
      "epoch": 8.910546467107553,
      "grad_norm": 12.501721382141113,
      "learning_rate": 4.2574544610743705e-05,
      "loss": 2.1937,
      "step": 230400
    },
    {
      "epoch": 8.914413891789458,
      "grad_norm": 10.692347526550293,
      "learning_rate": 4.257132175684212e-05,
      "loss": 2.2148,
      "step": 230500
    },
    {
      "epoch": 8.918281316471361,
      "grad_norm": 10.530803680419922,
      "learning_rate": 4.2568098902940535e-05,
      "loss": 2.1146,
      "step": 230600
    },
    {
      "epoch": 8.922148741153267,
      "grad_norm": 11.892982482910156,
      "learning_rate": 4.256487604903895e-05,
      "loss": 2.2798,
      "step": 230700
    },
    {
      "epoch": 8.92601616583517,
      "grad_norm": 13.427384376525879,
      "learning_rate": 4.256165319513736e-05,
      "loss": 2.2018,
      "step": 230800
    },
    {
      "epoch": 8.929883590517075,
      "grad_norm": 11.459000587463379,
      "learning_rate": 4.255843034123577e-05,
      "loss": 2.2091,
      "step": 230900
    },
    {
      "epoch": 8.933751015198979,
      "grad_norm": 7.124354362487793,
      "learning_rate": 4.255520748733419e-05,
      "loss": 2.245,
      "step": 231000
    },
    {
      "epoch": 8.937618439880882,
      "grad_norm": 11.61506175994873,
      "learning_rate": 4.25519846334326e-05,
      "loss": 2.1694,
      "step": 231100
    },
    {
      "epoch": 8.941485864562788,
      "grad_norm": 13.402470588684082,
      "learning_rate": 4.254876177953101e-05,
      "loss": 2.2504,
      "step": 231200
    },
    {
      "epoch": 8.945353289244691,
      "grad_norm": 11.087815284729004,
      "learning_rate": 4.2545538925629424e-05,
      "loss": 2.0938,
      "step": 231300
    },
    {
      "epoch": 8.949220713926596,
      "grad_norm": 14.170080184936523,
      "learning_rate": 4.254231607172784e-05,
      "loss": 2.1362,
      "step": 231400
    },
    {
      "epoch": 8.9530881386085,
      "grad_norm": 12.35180377960205,
      "learning_rate": 4.2539093217826254e-05,
      "loss": 2.1937,
      "step": 231500
    },
    {
      "epoch": 8.956955563290405,
      "grad_norm": 10.161900520324707,
      "learning_rate": 4.253587036392466e-05,
      "loss": 2.2061,
      "step": 231600
    },
    {
      "epoch": 8.960822987972309,
      "grad_norm": 10.800456047058105,
      "learning_rate": 4.2532647510023076e-05,
      "loss": 2.1186,
      "step": 231700
    },
    {
      "epoch": 8.964690412654214,
      "grad_norm": 13.56710147857666,
      "learning_rate": 4.252942465612149e-05,
      "loss": 2.1348,
      "step": 231800
    },
    {
      "epoch": 8.968557837336117,
      "grad_norm": 12.657103538513184,
      "learning_rate": 4.2526201802219906e-05,
      "loss": 2.2221,
      "step": 231900
    },
    {
      "epoch": 8.972425262018023,
      "grad_norm": 13.339235305786133,
      "learning_rate": 4.2522978948318314e-05,
      "loss": 2.1939,
      "step": 232000
    },
    {
      "epoch": 8.976292686699926,
      "grad_norm": 11.22075080871582,
      "learning_rate": 4.251975609441673e-05,
      "loss": 2.0896,
      "step": 232100
    },
    {
      "epoch": 8.980160111381831,
      "grad_norm": 15.164238929748535,
      "learning_rate": 4.251653324051514e-05,
      "loss": 2.1981,
      "step": 232200
    },
    {
      "epoch": 8.984027536063735,
      "grad_norm": 10.90821647644043,
      "learning_rate": 4.251331038661356e-05,
      "loss": 2.2648,
      "step": 232300
    },
    {
      "epoch": 8.98789496074564,
      "grad_norm": 10.713295936584473,
      "learning_rate": 4.2510087532711966e-05,
      "loss": 2.2805,
      "step": 232400
    },
    {
      "epoch": 8.991762385427544,
      "grad_norm": 11.575695991516113,
      "learning_rate": 4.250686467881038e-05,
      "loss": 2.1077,
      "step": 232500
    },
    {
      "epoch": 8.995629810109449,
      "grad_norm": 10.468475341796875,
      "learning_rate": 4.2503641824908795e-05,
      "loss": 2.083,
      "step": 232600
    },
    {
      "epoch": 8.999497234791352,
      "grad_norm": 10.776212692260742,
      "learning_rate": 4.250041897100721e-05,
      "loss": 2.1164,
      "step": 232700
    },
    {
      "epoch": 9.0,
      "eval_loss": 2.055288791656494,
      "eval_runtime": 3.0369,
      "eval_samples_per_second": 448.16,
      "eval_steps_per_second": 448.16,
      "step": 232713
    },
    {
      "epoch": 9.0,
      "eval_loss": 2.0161006450653076,
      "eval_runtime": 55.3622,
      "eval_samples_per_second": 467.051,
      "eval_steps_per_second": 467.051,
      "step": 232713
    },
    {
      "epoch": 9.003364659473256,
      "grad_norm": 11.540826797485352,
      "learning_rate": 4.249719611710562e-05,
      "loss": 2.0748,
      "step": 232800
    },
    {
      "epoch": 9.007232084155161,
      "grad_norm": 11.380171775817871,
      "learning_rate": 4.249397326320403e-05,
      "loss": 2.2068,
      "step": 232900
    },
    {
      "epoch": 9.011099508837065,
      "grad_norm": 11.388605117797852,
      "learning_rate": 4.249075040930245e-05,
      "loss": 2.1898,
      "step": 233000
    },
    {
      "epoch": 9.01496693351897,
      "grad_norm": 13.125096321105957,
      "learning_rate": 4.2487527555400855e-05,
      "loss": 2.1586,
      "step": 233100
    },
    {
      "epoch": 9.018834358200873,
      "grad_norm": 9.978447914123535,
      "learning_rate": 4.248430470149927e-05,
      "loss": 2.1692,
      "step": 233200
    },
    {
      "epoch": 9.022701782882779,
      "grad_norm": 10.30325698852539,
      "learning_rate": 4.2481081847597685e-05,
      "loss": 2.2191,
      "step": 233300
    },
    {
      "epoch": 9.026569207564682,
      "grad_norm": 10.110248565673828,
      "learning_rate": 4.24778589936961e-05,
      "loss": 2.1564,
      "step": 233400
    },
    {
      "epoch": 9.030436632246587,
      "grad_norm": 6.3167548179626465,
      "learning_rate": 4.247463613979451e-05,
      "loss": 2.1886,
      "step": 233500
    },
    {
      "epoch": 9.034304056928491,
      "grad_norm": 10.207708358764648,
      "learning_rate": 4.247141328589292e-05,
      "loss": 2.1203,
      "step": 233600
    },
    {
      "epoch": 9.038171481610396,
      "grad_norm": 11.198580741882324,
      "learning_rate": 4.246819043199134e-05,
      "loss": 2.2227,
      "step": 233700
    },
    {
      "epoch": 9.0420389062923,
      "grad_norm": 12.536664009094238,
      "learning_rate": 4.246496757808975e-05,
      "loss": 2.1095,
      "step": 233800
    },
    {
      "epoch": 9.045906330974205,
      "grad_norm": 14.641580581665039,
      "learning_rate": 4.246174472418816e-05,
      "loss": 2.1726,
      "step": 233900
    },
    {
      "epoch": 9.049773755656108,
      "grad_norm": 12.035415649414062,
      "learning_rate": 4.2458521870286574e-05,
      "loss": 2.1325,
      "step": 234000
    },
    {
      "epoch": 9.053641180338014,
      "grad_norm": 11.91506290435791,
      "learning_rate": 4.245529901638499e-05,
      "loss": 2.1714,
      "step": 234100
    },
    {
      "epoch": 9.057508605019917,
      "grad_norm": 13.883377075195312,
      "learning_rate": 4.2452076162483404e-05,
      "loss": 2.1763,
      "step": 234200
    },
    {
      "epoch": 9.061376029701822,
      "grad_norm": 12.97231674194336,
      "learning_rate": 4.244885330858182e-05,
      "loss": 2.1635,
      "step": 234300
    },
    {
      "epoch": 9.065243454383726,
      "grad_norm": 10.16178035736084,
      "learning_rate": 4.244563045468023e-05,
      "loss": 2.2249,
      "step": 234400
    },
    {
      "epoch": 9.06911087906563,
      "grad_norm": 11.951560020446777,
      "learning_rate": 4.244240760077864e-05,
      "loss": 2.1619,
      "step": 234500
    },
    {
      "epoch": 9.072978303747535,
      "grad_norm": 10.182988166809082,
      "learning_rate": 4.2439184746877056e-05,
      "loss": 2.2207,
      "step": 234600
    },
    {
      "epoch": 9.076845728429438,
      "grad_norm": 9.722732543945312,
      "learning_rate": 4.243596189297547e-05,
      "loss": 2.2654,
      "step": 234700
    },
    {
      "epoch": 9.080713153111343,
      "grad_norm": 12.16311264038086,
      "learning_rate": 4.2432739039073885e-05,
      "loss": 2.0482,
      "step": 234800
    },
    {
      "epoch": 9.084580577793247,
      "grad_norm": 15.509821891784668,
      "learning_rate": 4.24295161851723e-05,
      "loss": 2.1584,
      "step": 234900
    },
    {
      "epoch": 9.088448002475152,
      "grad_norm": 12.798998832702637,
      "learning_rate": 4.242629333127071e-05,
      "loss": 2.1823,
      "step": 235000
    },
    {
      "epoch": 9.092315427157056,
      "grad_norm": 11.723204612731934,
      "learning_rate": 4.242307047736912e-05,
      "loss": 2.1489,
      "step": 235100
    },
    {
      "epoch": 9.096182851838961,
      "grad_norm": 10.082104682922363,
      "learning_rate": 4.241984762346754e-05,
      "loss": 2.195,
      "step": 235200
    },
    {
      "epoch": 9.100050276520864,
      "grad_norm": 12.79655933380127,
      "learning_rate": 4.241662476956595e-05,
      "loss": 2.1593,
      "step": 235300
    },
    {
      "epoch": 9.10391770120277,
      "grad_norm": 11.768580436706543,
      "learning_rate": 4.241340191566437e-05,
      "loss": 2.2374,
      "step": 235400
    },
    {
      "epoch": 9.107785125884673,
      "grad_norm": 12.367969512939453,
      "learning_rate": 4.2410179061762775e-05,
      "loss": 2.1979,
      "step": 235500
    },
    {
      "epoch": 9.111652550566578,
      "grad_norm": 10.861468315124512,
      "learning_rate": 4.240695620786119e-05,
      "loss": 2.1455,
      "step": 235600
    },
    {
      "epoch": 9.115519975248482,
      "grad_norm": 12.15113639831543,
      "learning_rate": 4.2403733353959604e-05,
      "loss": 2.1884,
      "step": 235700
    },
    {
      "epoch": 9.119387399930387,
      "grad_norm": 20.22584342956543,
      "learning_rate": 4.240051050005802e-05,
      "loss": 2.0347,
      "step": 235800
    },
    {
      "epoch": 9.12325482461229,
      "grad_norm": 10.780393600463867,
      "learning_rate": 4.239728764615643e-05,
      "loss": 2.2522,
      "step": 235900
    },
    {
      "epoch": 9.127122249294194,
      "grad_norm": 13.150086402893066,
      "learning_rate": 4.239406479225484e-05,
      "loss": 2.1231,
      "step": 236000
    },
    {
      "epoch": 9.1309896739761,
      "grad_norm": 11.643109321594238,
      "learning_rate": 4.2390841938353256e-05,
      "loss": 2.221,
      "step": 236100
    },
    {
      "epoch": 9.134857098658003,
      "grad_norm": 12.98039722442627,
      "learning_rate": 4.2387619084451664e-05,
      "loss": 2.1193,
      "step": 236200
    },
    {
      "epoch": 9.138724523339908,
      "grad_norm": 9.573159217834473,
      "learning_rate": 4.238439623055008e-05,
      "loss": 2.0975,
      "step": 236300
    },
    {
      "epoch": 9.142591948021812,
      "grad_norm": 10.702608108520508,
      "learning_rate": 4.2381173376648494e-05,
      "loss": 2.1792,
      "step": 236400
    },
    {
      "epoch": 9.146459372703717,
      "grad_norm": 11.788990020751953,
      "learning_rate": 4.237795052274691e-05,
      "loss": 2.2027,
      "step": 236500
    },
    {
      "epoch": 9.15032679738562,
      "grad_norm": 12.496159553527832,
      "learning_rate": 4.2374727668845316e-05,
      "loss": 2.1081,
      "step": 236600
    },
    {
      "epoch": 9.154194222067526,
      "grad_norm": 14.05435562133789,
      "learning_rate": 4.237150481494373e-05,
      "loss": 2.1932,
      "step": 236700
    },
    {
      "epoch": 9.15806164674943,
      "grad_norm": 10.886852264404297,
      "learning_rate": 4.2368281961042146e-05,
      "loss": 2.2182,
      "step": 236800
    },
    {
      "epoch": 9.161929071431334,
      "grad_norm": 10.6500883102417,
      "learning_rate": 4.236505910714056e-05,
      "loss": 2.174,
      "step": 236900
    },
    {
      "epoch": 9.165796496113238,
      "grad_norm": 15.141228675842285,
      "learning_rate": 4.236183625323897e-05,
      "loss": 2.1494,
      "step": 237000
    },
    {
      "epoch": 9.169663920795143,
      "grad_norm": 12.821769714355469,
      "learning_rate": 4.235861339933738e-05,
      "loss": 2.1811,
      "step": 237100
    },
    {
      "epoch": 9.173531345477047,
      "grad_norm": 12.608488082885742,
      "learning_rate": 4.23553905454358e-05,
      "loss": 2.1995,
      "step": 237200
    },
    {
      "epoch": 9.177398770158952,
      "grad_norm": 13.980506896972656,
      "learning_rate": 4.235216769153421e-05,
      "loss": 2.1541,
      "step": 237300
    },
    {
      "epoch": 9.181266194840855,
      "grad_norm": 15.20203971862793,
      "learning_rate": 4.234894483763262e-05,
      "loss": 2.1482,
      "step": 237400
    },
    {
      "epoch": 9.18513361952276,
      "grad_norm": 10.692249298095703,
      "learning_rate": 4.2345721983731035e-05,
      "loss": 2.0626,
      "step": 237500
    },
    {
      "epoch": 9.189001044204664,
      "grad_norm": 10.462318420410156,
      "learning_rate": 4.234249912982945e-05,
      "loss": 2.1294,
      "step": 237600
    },
    {
      "epoch": 9.192868468886568,
      "grad_norm": 11.746265411376953,
      "learning_rate": 4.2339276275927865e-05,
      "loss": 2.2417,
      "step": 237700
    },
    {
      "epoch": 9.196735893568473,
      "grad_norm": 13.924200057983398,
      "learning_rate": 4.233605342202627e-05,
      "loss": 2.1596,
      "step": 237800
    },
    {
      "epoch": 9.200603318250376,
      "grad_norm": 10.718231201171875,
      "learning_rate": 4.233283056812469e-05,
      "loss": 2.1464,
      "step": 237900
    },
    {
      "epoch": 9.204470742932282,
      "grad_norm": 13.331557273864746,
      "learning_rate": 4.23296077142231e-05,
      "loss": 2.174,
      "step": 238000
    },
    {
      "epoch": 9.208338167614185,
      "grad_norm": 12.210372924804688,
      "learning_rate": 4.232638486032152e-05,
      "loss": 2.0468,
      "step": 238100
    },
    {
      "epoch": 9.21220559229609,
      "grad_norm": 10.533883094787598,
      "learning_rate": 4.2323162006419925e-05,
      "loss": 2.178,
      "step": 238200
    },
    {
      "epoch": 9.216073016977994,
      "grad_norm": 10.823905944824219,
      "learning_rate": 4.231993915251834e-05,
      "loss": 2.2491,
      "step": 238300
    },
    {
      "epoch": 9.2199404416599,
      "grad_norm": 11.831421852111816,
      "learning_rate": 4.2316716298616754e-05,
      "loss": 2.1863,
      "step": 238400
    },
    {
      "epoch": 9.223807866341803,
      "grad_norm": 12.8300199508667,
      "learning_rate": 4.231349344471517e-05,
      "loss": 2.1828,
      "step": 238500
    },
    {
      "epoch": 9.227675291023708,
      "grad_norm": 8.715254783630371,
      "learning_rate": 4.231027059081358e-05,
      "loss": 2.3069,
      "step": 238600
    },
    {
      "epoch": 9.231542715705611,
      "grad_norm": 10.016597747802734,
      "learning_rate": 4.230704773691199e-05,
      "loss": 2.1782,
      "step": 238700
    },
    {
      "epoch": 9.235410140387517,
      "grad_norm": 13.69211196899414,
      "learning_rate": 4.2303824883010406e-05,
      "loss": 2.16,
      "step": 238800
    },
    {
      "epoch": 9.23927756506942,
      "grad_norm": 10.457914352416992,
      "learning_rate": 4.230060202910882e-05,
      "loss": 2.1623,
      "step": 238900
    },
    {
      "epoch": 9.243144989751325,
      "grad_norm": 12.146238327026367,
      "learning_rate": 4.229737917520723e-05,
      "loss": 2.218,
      "step": 239000
    },
    {
      "epoch": 9.247012414433229,
      "grad_norm": 12.719615936279297,
      "learning_rate": 4.2294156321305644e-05,
      "loss": 2.1606,
      "step": 239100
    },
    {
      "epoch": 9.250879839115132,
      "grad_norm": 10.563313484191895,
      "learning_rate": 4.229093346740406e-05,
      "loss": 2.11,
      "step": 239200
    },
    {
      "epoch": 9.254747263797038,
      "grad_norm": 9.163715362548828,
      "learning_rate": 4.2287710613502466e-05,
      "loss": 2.1745,
      "step": 239300
    },
    {
      "epoch": 9.258614688478941,
      "grad_norm": 13.25971508026123,
      "learning_rate": 4.228448775960088e-05,
      "loss": 2.1954,
      "step": 239400
    },
    {
      "epoch": 9.262482113160846,
      "grad_norm": 14.32597541809082,
      "learning_rate": 4.2281264905699296e-05,
      "loss": 2.1036,
      "step": 239500
    },
    {
      "epoch": 9.26634953784275,
      "grad_norm": 13.135358810424805,
      "learning_rate": 4.227804205179771e-05,
      "loss": 2.1687,
      "step": 239600
    },
    {
      "epoch": 9.270216962524655,
      "grad_norm": 10.122623443603516,
      "learning_rate": 4.227481919789612e-05,
      "loss": 2.2007,
      "step": 239700
    },
    {
      "epoch": 9.274084387206559,
      "grad_norm": 11.290445327758789,
      "learning_rate": 4.227159634399453e-05,
      "loss": 2.1624,
      "step": 239800
    },
    {
      "epoch": 9.277951811888464,
      "grad_norm": 10.238513946533203,
      "learning_rate": 4.226837349009295e-05,
      "loss": 2.2316,
      "step": 239900
    },
    {
      "epoch": 9.281819236570367,
      "grad_norm": 9.160783767700195,
      "learning_rate": 4.226515063619136e-05,
      "loss": 2.1356,
      "step": 240000
    },
    {
      "epoch": 9.285686661252273,
      "grad_norm": 10.375486373901367,
      "learning_rate": 4.226192778228977e-05,
      "loss": 2.216,
      "step": 240100
    },
    {
      "epoch": 9.289554085934176,
      "grad_norm": 12.050199508666992,
      "learning_rate": 4.2258704928388185e-05,
      "loss": 2.1843,
      "step": 240200
    },
    {
      "epoch": 9.293421510616081,
      "grad_norm": 10.638970375061035,
      "learning_rate": 4.22554820744866e-05,
      "loss": 2.1542,
      "step": 240300
    },
    {
      "epoch": 9.297288935297985,
      "grad_norm": 11.254748344421387,
      "learning_rate": 4.2252259220585015e-05,
      "loss": 2.1094,
      "step": 240400
    },
    {
      "epoch": 9.30115635997989,
      "grad_norm": 12.058406829833984,
      "learning_rate": 4.224903636668342e-05,
      "loss": 2.1233,
      "step": 240500
    },
    {
      "epoch": 9.305023784661794,
      "grad_norm": 12.292142868041992,
      "learning_rate": 4.224581351278184e-05,
      "loss": 2.1845,
      "step": 240600
    },
    {
      "epoch": 9.308891209343699,
      "grad_norm": 12.61225414276123,
      "learning_rate": 4.224259065888025e-05,
      "loss": 2.2836,
      "step": 240700
    },
    {
      "epoch": 9.312758634025602,
      "grad_norm": 12.736886978149414,
      "learning_rate": 4.223936780497867e-05,
      "loss": 2.1672,
      "step": 240800
    },
    {
      "epoch": 9.316626058707506,
      "grad_norm": 12.003595352172852,
      "learning_rate": 4.2236144951077075e-05,
      "loss": 2.152,
      "step": 240900
    },
    {
      "epoch": 9.320493483389411,
      "grad_norm": 13.217649459838867,
      "learning_rate": 4.223292209717549e-05,
      "loss": 2.1384,
      "step": 241000
    },
    {
      "epoch": 9.324360908071315,
      "grad_norm": 11.106693267822266,
      "learning_rate": 4.2229699243273904e-05,
      "loss": 2.163,
      "step": 241100
    },
    {
      "epoch": 9.32822833275322,
      "grad_norm": 10.672511100769043,
      "learning_rate": 4.222647638937232e-05,
      "loss": 2.2318,
      "step": 241200
    },
    {
      "epoch": 9.332095757435123,
      "grad_norm": 12.099444389343262,
      "learning_rate": 4.2223253535470733e-05,
      "loss": 2.0623,
      "step": 241300
    },
    {
      "epoch": 9.335963182117029,
      "grad_norm": 11.480680465698242,
      "learning_rate": 4.222003068156915e-05,
      "loss": 2.0666,
      "step": 241400
    },
    {
      "epoch": 9.339830606798932,
      "grad_norm": 12.954084396362305,
      "learning_rate": 4.2216807827667556e-05,
      "loss": 2.1716,
      "step": 241500
    },
    {
      "epoch": 9.343698031480837,
      "grad_norm": 9.867435455322266,
      "learning_rate": 4.221358497376597e-05,
      "loss": 2.1709,
      "step": 241600
    },
    {
      "epoch": 9.34756545616274,
      "grad_norm": 12.671025276184082,
      "learning_rate": 4.2210362119864386e-05,
      "loss": 2.1879,
      "step": 241700
    },
    {
      "epoch": 9.351432880844646,
      "grad_norm": 10.668850898742676,
      "learning_rate": 4.22071392659628e-05,
      "loss": 2.1883,
      "step": 241800
    },
    {
      "epoch": 9.35530030552655,
      "grad_norm": 11.75444221496582,
      "learning_rate": 4.2203916412061215e-05,
      "loss": 2.153,
      "step": 241900
    },
    {
      "epoch": 9.359167730208455,
      "grad_norm": 10.919807434082031,
      "learning_rate": 4.220069355815962e-05,
      "loss": 2.1863,
      "step": 242000
    },
    {
      "epoch": 9.363035154890358,
      "grad_norm": 10.383552551269531,
      "learning_rate": 4.219747070425804e-05,
      "loss": 2.215,
      "step": 242100
    },
    {
      "epoch": 9.366902579572264,
      "grad_norm": 10.158937454223633,
      "learning_rate": 4.219424785035645e-05,
      "loss": 2.1951,
      "step": 242200
    },
    {
      "epoch": 9.370770004254167,
      "grad_norm": 10.992897033691406,
      "learning_rate": 4.219102499645487e-05,
      "loss": 2.1586,
      "step": 242300
    },
    {
      "epoch": 9.374637428936072,
      "grad_norm": 12.019636154174805,
      "learning_rate": 4.2187802142553275e-05,
      "loss": 2.1112,
      "step": 242400
    },
    {
      "epoch": 9.378504853617976,
      "grad_norm": 13.248332023620605,
      "learning_rate": 4.218457928865169e-05,
      "loss": 2.2643,
      "step": 242500
    },
    {
      "epoch": 9.38237227829988,
      "grad_norm": 14.359146118164062,
      "learning_rate": 4.2181356434750104e-05,
      "loss": 2.1574,
      "step": 242600
    },
    {
      "epoch": 9.386239702981785,
      "grad_norm": 12.874811172485352,
      "learning_rate": 4.217813358084852e-05,
      "loss": 2.1755,
      "step": 242700
    },
    {
      "epoch": 9.390107127663688,
      "grad_norm": 12.815939903259277,
      "learning_rate": 4.217491072694693e-05,
      "loss": 2.1869,
      "step": 242800
    },
    {
      "epoch": 9.393974552345593,
      "grad_norm": 12.535894393920898,
      "learning_rate": 4.217168787304534e-05,
      "loss": 2.1274,
      "step": 242900
    },
    {
      "epoch": 9.397841977027497,
      "grad_norm": 11.652700424194336,
      "learning_rate": 4.2168465019143757e-05,
      "loss": 2.1933,
      "step": 243000
    },
    {
      "epoch": 9.401709401709402,
      "grad_norm": 11.427800178527832,
      "learning_rate": 4.216524216524217e-05,
      "loss": 2.1344,
      "step": 243100
    },
    {
      "epoch": 9.405576826391306,
      "grad_norm": 12.431473731994629,
      "learning_rate": 4.216201931134058e-05,
      "loss": 2.1181,
      "step": 243200
    },
    {
      "epoch": 9.40944425107321,
      "grad_norm": 10.006606101989746,
      "learning_rate": 4.2158796457438994e-05,
      "loss": 2.2133,
      "step": 243300
    },
    {
      "epoch": 9.413311675755114,
      "grad_norm": 10.644702911376953,
      "learning_rate": 4.215557360353741e-05,
      "loss": 2.1217,
      "step": 243400
    },
    {
      "epoch": 9.41717910043702,
      "grad_norm": 12.31657886505127,
      "learning_rate": 4.215235074963582e-05,
      "loss": 2.185,
      "step": 243500
    },
    {
      "epoch": 9.421046525118923,
      "grad_norm": 9.64072322845459,
      "learning_rate": 4.214912789573423e-05,
      "loss": 2.1437,
      "step": 243600
    },
    {
      "epoch": 9.424913949800828,
      "grad_norm": 12.109087944030762,
      "learning_rate": 4.2145905041832646e-05,
      "loss": 2.0924,
      "step": 243700
    },
    {
      "epoch": 9.428781374482732,
      "grad_norm": 12.644538879394531,
      "learning_rate": 4.214268218793106e-05,
      "loss": 2.1156,
      "step": 243800
    },
    {
      "epoch": 9.432648799164637,
      "grad_norm": 16.24596405029297,
      "learning_rate": 4.2139459334029475e-05,
      "loss": 2.19,
      "step": 243900
    },
    {
      "epoch": 9.43651622384654,
      "grad_norm": 11.615209579467773,
      "learning_rate": 4.2136236480127883e-05,
      "loss": 2.245,
      "step": 244000
    },
    {
      "epoch": 9.440383648528446,
      "grad_norm": 12.537137031555176,
      "learning_rate": 4.21330136262263e-05,
      "loss": 2.2019,
      "step": 244100
    },
    {
      "epoch": 9.44425107321035,
      "grad_norm": 11.840322494506836,
      "learning_rate": 4.212979077232471e-05,
      "loss": 2.1605,
      "step": 244200
    },
    {
      "epoch": 9.448118497892253,
      "grad_norm": 8.935558319091797,
      "learning_rate": 4.212656791842313e-05,
      "loss": 2.1999,
      "step": 244300
    },
    {
      "epoch": 9.451985922574158,
      "grad_norm": 15.156214714050293,
      "learning_rate": 4.2123345064521535e-05,
      "loss": 2.1526,
      "step": 244400
    },
    {
      "epoch": 9.455853347256062,
      "grad_norm": 12.021750450134277,
      "learning_rate": 4.212012221061995e-05,
      "loss": 2.1287,
      "step": 244500
    },
    {
      "epoch": 9.459720771937967,
      "grad_norm": 11.244779586791992,
      "learning_rate": 4.2116899356718365e-05,
      "loss": 2.1326,
      "step": 244600
    },
    {
      "epoch": 9.46358819661987,
      "grad_norm": 12.264408111572266,
      "learning_rate": 4.211367650281678e-05,
      "loss": 2.1374,
      "step": 244700
    },
    {
      "epoch": 9.467455621301776,
      "grad_norm": 12.66589069366455,
      "learning_rate": 4.211045364891519e-05,
      "loss": 2.1525,
      "step": 244800
    },
    {
      "epoch": 9.471323045983679,
      "grad_norm": 11.384862899780273,
      "learning_rate": 4.21072307950136e-05,
      "loss": 2.2381,
      "step": 244900
    },
    {
      "epoch": 9.475190470665584,
      "grad_norm": 9.421963691711426,
      "learning_rate": 4.210400794111202e-05,
      "loss": 2.2325,
      "step": 245000
    },
    {
      "epoch": 9.479057895347488,
      "grad_norm": 12.301215171813965,
      "learning_rate": 4.2100785087210425e-05,
      "loss": 2.1413,
      "step": 245100
    },
    {
      "epoch": 9.482925320029393,
      "grad_norm": 11.072748184204102,
      "learning_rate": 4.209756223330884e-05,
      "loss": 2.1831,
      "step": 245200
    },
    {
      "epoch": 9.486792744711297,
      "grad_norm": 10.872638702392578,
      "learning_rate": 4.2094339379407254e-05,
      "loss": 2.1708,
      "step": 245300
    },
    {
      "epoch": 9.490660169393202,
      "grad_norm": 13.470233917236328,
      "learning_rate": 4.209111652550567e-05,
      "loss": 2.1867,
      "step": 245400
    },
    {
      "epoch": 9.494527594075105,
      "grad_norm": 10.553378105163574,
      "learning_rate": 4.208789367160408e-05,
      "loss": 2.128,
      "step": 245500
    },
    {
      "epoch": 9.49839501875701,
      "grad_norm": 8.254568099975586,
      "learning_rate": 4.208467081770249e-05,
      "loss": 2.1322,
      "step": 245600
    },
    {
      "epoch": 9.502262443438914,
      "grad_norm": 11.897534370422363,
      "learning_rate": 4.2081447963800907e-05,
      "loss": 2.0872,
      "step": 245700
    },
    {
      "epoch": 9.50612986812082,
      "grad_norm": 10.759927749633789,
      "learning_rate": 4.207822510989932e-05,
      "loss": 2.1205,
      "step": 245800
    },
    {
      "epoch": 9.509997292802723,
      "grad_norm": 10.06356143951416,
      "learning_rate": 4.207500225599773e-05,
      "loss": 2.121,
      "step": 245900
    },
    {
      "epoch": 9.513864717484626,
      "grad_norm": 12.453007698059082,
      "learning_rate": 4.2071779402096144e-05,
      "loss": 2.1462,
      "step": 246000
    },
    {
      "epoch": 9.517732142166532,
      "grad_norm": 8.8963623046875,
      "learning_rate": 4.206855654819456e-05,
      "loss": 2.2082,
      "step": 246100
    },
    {
      "epoch": 9.521599566848435,
      "grad_norm": 6.663743495941162,
      "learning_rate": 4.206533369429297e-05,
      "loss": 2.2127,
      "step": 246200
    },
    {
      "epoch": 9.52546699153034,
      "grad_norm": 13.44086742401123,
      "learning_rate": 4.206211084039138e-05,
      "loss": 2.16,
      "step": 246300
    },
    {
      "epoch": 9.529334416212244,
      "grad_norm": 10.168075561523438,
      "learning_rate": 4.2058887986489796e-05,
      "loss": 2.1506,
      "step": 246400
    },
    {
      "epoch": 9.533201840894149,
      "grad_norm": 10.84335994720459,
      "learning_rate": 4.205566513258821e-05,
      "loss": 2.2242,
      "step": 246500
    },
    {
      "epoch": 9.537069265576053,
      "grad_norm": 14.45814323425293,
      "learning_rate": 4.2052442278686625e-05,
      "loss": 2.2542,
      "step": 246600
    },
    {
      "epoch": 9.540936690257958,
      "grad_norm": 11.6263427734375,
      "learning_rate": 4.204921942478503e-05,
      "loss": 2.179,
      "step": 246700
    },
    {
      "epoch": 9.544804114939861,
      "grad_norm": 9.088186264038086,
      "learning_rate": 4.204599657088345e-05,
      "loss": 2.2441,
      "step": 246800
    },
    {
      "epoch": 9.548671539621767,
      "grad_norm": 14.365204811096191,
      "learning_rate": 4.204277371698186e-05,
      "loss": 2.0882,
      "step": 246900
    },
    {
      "epoch": 9.55253896430367,
      "grad_norm": 9.318012237548828,
      "learning_rate": 4.203955086308028e-05,
      "loss": 2.2302,
      "step": 247000
    },
    {
      "epoch": 9.556406388985575,
      "grad_norm": 12.031501770019531,
      "learning_rate": 4.2036328009178685e-05,
      "loss": 2.149,
      "step": 247100
    },
    {
      "epoch": 9.560273813667479,
      "grad_norm": 12.1376371383667,
      "learning_rate": 4.20331051552771e-05,
      "loss": 2.182,
      "step": 247200
    },
    {
      "epoch": 9.564141238349382,
      "grad_norm": 8.860101699829102,
      "learning_rate": 4.2029882301375515e-05,
      "loss": 2.1525,
      "step": 247300
    },
    {
      "epoch": 9.568008663031287,
      "grad_norm": 14.064003944396973,
      "learning_rate": 4.202665944747393e-05,
      "loss": 2.0677,
      "step": 247400
    },
    {
      "epoch": 9.571876087713191,
      "grad_norm": 14.42984390258789,
      "learning_rate": 4.202343659357234e-05,
      "loss": 2.2696,
      "step": 247500
    },
    {
      "epoch": 9.575743512395096,
      "grad_norm": 10.307083129882812,
      "learning_rate": 4.202021373967075e-05,
      "loss": 2.1553,
      "step": 247600
    },
    {
      "epoch": 9.579610937077,
      "grad_norm": 13.605899810791016,
      "learning_rate": 4.201699088576917e-05,
      "loss": 2.2683,
      "step": 247700
    },
    {
      "epoch": 9.583478361758905,
      "grad_norm": 10.938706398010254,
      "learning_rate": 4.201376803186758e-05,
      "loss": 2.2221,
      "step": 247800
    },
    {
      "epoch": 9.587345786440808,
      "grad_norm": 14.458353042602539,
      "learning_rate": 4.2010545177965996e-05,
      "loss": 2.1418,
      "step": 247900
    },
    {
      "epoch": 9.591213211122714,
      "grad_norm": 12.903502464294434,
      "learning_rate": 4.2007322324064404e-05,
      "loss": 2.1862,
      "step": 248000
    },
    {
      "epoch": 9.595080635804617,
      "grad_norm": 11.245341300964355,
      "learning_rate": 4.200409947016282e-05,
      "loss": 2.1449,
      "step": 248100
    },
    {
      "epoch": 9.598948060486522,
      "grad_norm": 12.572614669799805,
      "learning_rate": 4.2000876616261234e-05,
      "loss": 2.1703,
      "step": 248200
    },
    {
      "epoch": 9.602815485168426,
      "grad_norm": 10.55496883392334,
      "learning_rate": 4.199765376235965e-05,
      "loss": 2.2542,
      "step": 248300
    },
    {
      "epoch": 9.606682909850331,
      "grad_norm": 13.28669261932373,
      "learning_rate": 4.199443090845806e-05,
      "loss": 2.0839,
      "step": 248400
    },
    {
      "epoch": 9.610550334532235,
      "grad_norm": 13.864317893981934,
      "learning_rate": 4.199120805455647e-05,
      "loss": 2.2337,
      "step": 248500
    },
    {
      "epoch": 9.61441775921414,
      "grad_norm": 12.481078147888184,
      "learning_rate": 4.1987985200654886e-05,
      "loss": 2.1621,
      "step": 248600
    },
    {
      "epoch": 9.618285183896043,
      "grad_norm": 15.183008193969727,
      "learning_rate": 4.19847623467533e-05,
      "loss": 2.0923,
      "step": 248700
    },
    {
      "epoch": 9.622152608577949,
      "grad_norm": 9.926586151123047,
      "learning_rate": 4.1981539492851715e-05,
      "loss": 2.0993,
      "step": 248800
    },
    {
      "epoch": 9.626020033259852,
      "grad_norm": 12.351170539855957,
      "learning_rate": 4.197831663895013e-05,
      "loss": 2.2075,
      "step": 248900
    },
    {
      "epoch": 9.629887457941756,
      "grad_norm": 12.569039344787598,
      "learning_rate": 4.197509378504854e-05,
      "loss": 2.1173,
      "step": 249000
    },
    {
      "epoch": 9.633754882623661,
      "grad_norm": 10.37318229675293,
      "learning_rate": 4.197187093114695e-05,
      "loss": 2.1833,
      "step": 249100
    },
    {
      "epoch": 9.637622307305564,
      "grad_norm": 14.015089988708496,
      "learning_rate": 4.196864807724537e-05,
      "loss": 2.1724,
      "step": 249200
    },
    {
      "epoch": 9.64148973198747,
      "grad_norm": 11.858668327331543,
      "learning_rate": 4.196542522334378e-05,
      "loss": 2.1833,
      "step": 249300
    },
    {
      "epoch": 9.645357156669373,
      "grad_norm": 12.944164276123047,
      "learning_rate": 4.196220236944219e-05,
      "loss": 2.2255,
      "step": 249400
    },
    {
      "epoch": 9.649224581351278,
      "grad_norm": 13.977860450744629,
      "learning_rate": 4.1958979515540605e-05,
      "loss": 2.1695,
      "step": 249500
    },
    {
      "epoch": 9.653092006033182,
      "grad_norm": 13.316052436828613,
      "learning_rate": 4.195575666163902e-05,
      "loss": 2.1359,
      "step": 249600
    },
    {
      "epoch": 9.656959430715087,
      "grad_norm": 14.509242057800293,
      "learning_rate": 4.1952533807737434e-05,
      "loss": 2.1379,
      "step": 249700
    },
    {
      "epoch": 9.66082685539699,
      "grad_norm": 10.917590141296387,
      "learning_rate": 4.194931095383584e-05,
      "loss": 2.201,
      "step": 249800
    },
    {
      "epoch": 9.664694280078896,
      "grad_norm": 15.876693725585938,
      "learning_rate": 4.194608809993426e-05,
      "loss": 2.1072,
      "step": 249900
    },
    {
      "epoch": 9.6685617047608,
      "grad_norm": 13.46124267578125,
      "learning_rate": 4.194286524603267e-05,
      "loss": 2.096,
      "step": 250000
    },
    {
      "epoch": 9.672429129442705,
      "grad_norm": 14.647951126098633,
      "learning_rate": 4.1939642392131086e-05,
      "loss": 2.2067,
      "step": 250100
    },
    {
      "epoch": 9.676296554124608,
      "grad_norm": 10.514871597290039,
      "learning_rate": 4.1936419538229494e-05,
      "loss": 2.179,
      "step": 250200
    },
    {
      "epoch": 9.680163978806513,
      "grad_norm": 12.40882396697998,
      "learning_rate": 4.193319668432791e-05,
      "loss": 2.2253,
      "step": 250300
    },
    {
      "epoch": 9.684031403488417,
      "grad_norm": 15.94701099395752,
      "learning_rate": 4.1929973830426324e-05,
      "loss": 2.1323,
      "step": 250400
    },
    {
      "epoch": 9.687898828170322,
      "grad_norm": 9.321755409240723,
      "learning_rate": 4.192675097652474e-05,
      "loss": 2.1753,
      "step": 250500
    },
    {
      "epoch": 9.691766252852226,
      "grad_norm": 10.579745292663574,
      "learning_rate": 4.1923528122623146e-05,
      "loss": 2.095,
      "step": 250600
    },
    {
      "epoch": 9.69563367753413,
      "grad_norm": 12.062246322631836,
      "learning_rate": 4.192030526872156e-05,
      "loss": 2.1789,
      "step": 250700
    },
    {
      "epoch": 9.699501102216034,
      "grad_norm": 10.886141777038574,
      "learning_rate": 4.1917082414819976e-05,
      "loss": 2.3158,
      "step": 250800
    },
    {
      "epoch": 9.703368526897938,
      "grad_norm": 10.75299072265625,
      "learning_rate": 4.1913859560918384e-05,
      "loss": 2.1508,
      "step": 250900
    },
    {
      "epoch": 9.707235951579843,
      "grad_norm": 13.302266120910645,
      "learning_rate": 4.19106367070168e-05,
      "loss": 2.1476,
      "step": 251000
    },
    {
      "epoch": 9.711103376261747,
      "grad_norm": 11.849349975585938,
      "learning_rate": 4.190741385311521e-05,
      "loss": 2.1474,
      "step": 251100
    },
    {
      "epoch": 9.714970800943652,
      "grad_norm": 11.98757553100586,
      "learning_rate": 4.190419099921363e-05,
      "loss": 2.1652,
      "step": 251200
    },
    {
      "epoch": 9.718838225625555,
      "grad_norm": 13.433938980102539,
      "learning_rate": 4.1900968145312036e-05,
      "loss": 2.1467,
      "step": 251300
    },
    {
      "epoch": 9.72270565030746,
      "grad_norm": 11.979338645935059,
      "learning_rate": 4.189774529141045e-05,
      "loss": 2.0929,
      "step": 251400
    },
    {
      "epoch": 9.726573074989364,
      "grad_norm": 10.841794967651367,
      "learning_rate": 4.1894522437508865e-05,
      "loss": 2.1862,
      "step": 251500
    },
    {
      "epoch": 9.73044049967127,
      "grad_norm": 13.707817077636719,
      "learning_rate": 4.189129958360728e-05,
      "loss": 2.1327,
      "step": 251600
    },
    {
      "epoch": 9.734307924353173,
      "grad_norm": 10.493213653564453,
      "learning_rate": 4.188807672970569e-05,
      "loss": 2.0525,
      "step": 251700
    },
    {
      "epoch": 9.738175349035078,
      "grad_norm": 9.172791481018066,
      "learning_rate": 4.18848538758041e-05,
      "loss": 2.1572,
      "step": 251800
    },
    {
      "epoch": 9.742042773716982,
      "grad_norm": 12.479819297790527,
      "learning_rate": 4.188163102190252e-05,
      "loss": 2.226,
      "step": 251900
    },
    {
      "epoch": 9.745910198398887,
      "grad_norm": 11.820242881774902,
      "learning_rate": 4.187840816800093e-05,
      "loss": 2.1795,
      "step": 252000
    },
    {
      "epoch": 9.74977762308079,
      "grad_norm": 10.056798934936523,
      "learning_rate": 4.187518531409934e-05,
      "loss": 2.1108,
      "step": 252100
    },
    {
      "epoch": 9.753645047762696,
      "grad_norm": 12.064864158630371,
      "learning_rate": 4.1871962460197755e-05,
      "loss": 2.1862,
      "step": 252200
    },
    {
      "epoch": 9.7575124724446,
      "grad_norm": 13.314969062805176,
      "learning_rate": 4.186873960629617e-05,
      "loss": 2.2326,
      "step": 252300
    },
    {
      "epoch": 9.761379897126503,
      "grad_norm": 11.02053451538086,
      "learning_rate": 4.1865516752394584e-05,
      "loss": 2.242,
      "step": 252400
    },
    {
      "epoch": 9.765247321808408,
      "grad_norm": 12.169541358947754,
      "learning_rate": 4.186229389849299e-05,
      "loss": 2.2576,
      "step": 252500
    },
    {
      "epoch": 9.769114746490311,
      "grad_norm": 14.797196388244629,
      "learning_rate": 4.185907104459141e-05,
      "loss": 2.2227,
      "step": 252600
    },
    {
      "epoch": 9.772982171172217,
      "grad_norm": 12.237421989440918,
      "learning_rate": 4.185584819068982e-05,
      "loss": 2.1293,
      "step": 252700
    },
    {
      "epoch": 9.77684959585412,
      "grad_norm": 11.530632019042969,
      "learning_rate": 4.1852625336788236e-05,
      "loss": 2.2293,
      "step": 252800
    },
    {
      "epoch": 9.780717020536025,
      "grad_norm": 13.96288776397705,
      "learning_rate": 4.1849402482886644e-05,
      "loss": 2.185,
      "step": 252900
    },
    {
      "epoch": 9.784584445217929,
      "grad_norm": 14.516550064086914,
      "learning_rate": 4.184617962898506e-05,
      "loss": 2.1738,
      "step": 253000
    },
    {
      "epoch": 9.788451869899834,
      "grad_norm": 11.867517471313477,
      "learning_rate": 4.1842956775083474e-05,
      "loss": 2.1523,
      "step": 253100
    },
    {
      "epoch": 9.792319294581738,
      "grad_norm": 9.811789512634277,
      "learning_rate": 4.183973392118189e-05,
      "loss": 2.2446,
      "step": 253200
    },
    {
      "epoch": 9.796186719263643,
      "grad_norm": 10.694791793823242,
      "learning_rate": 4.1836511067280296e-05,
      "loss": 2.1966,
      "step": 253300
    },
    {
      "epoch": 9.800054143945546,
      "grad_norm": 9.532353401184082,
      "learning_rate": 4.183328821337871e-05,
      "loss": 2.1408,
      "step": 253400
    },
    {
      "epoch": 9.803921568627452,
      "grad_norm": 14.903327941894531,
      "learning_rate": 4.1830065359477126e-05,
      "loss": 2.0769,
      "step": 253500
    },
    {
      "epoch": 9.807788993309355,
      "grad_norm": 14.923395156860352,
      "learning_rate": 4.182684250557554e-05,
      "loss": 2.2209,
      "step": 253600
    },
    {
      "epoch": 9.81165641799126,
      "grad_norm": 10.766118049621582,
      "learning_rate": 4.182361965167395e-05,
      "loss": 2.2016,
      "step": 253700
    },
    {
      "epoch": 9.815523842673164,
      "grad_norm": 11.402070999145508,
      "learning_rate": 4.182039679777236e-05,
      "loss": 2.2229,
      "step": 253800
    },
    {
      "epoch": 9.81939126735507,
      "grad_norm": 10.724069595336914,
      "learning_rate": 4.181717394387078e-05,
      "loss": 2.1554,
      "step": 253900
    },
    {
      "epoch": 9.823258692036973,
      "grad_norm": 11.054059028625488,
      "learning_rate": 4.1813951089969186e-05,
      "loss": 2.1502,
      "step": 254000
    },
    {
      "epoch": 9.827126116718876,
      "grad_norm": 11.91269302368164,
      "learning_rate": 4.18107282360676e-05,
      "loss": 2.1984,
      "step": 254100
    },
    {
      "epoch": 9.830993541400781,
      "grad_norm": 11.443973541259766,
      "learning_rate": 4.1807505382166015e-05,
      "loss": 2.1211,
      "step": 254200
    },
    {
      "epoch": 9.834860966082685,
      "grad_norm": 11.641178131103516,
      "learning_rate": 4.180428252826443e-05,
      "loss": 2.2498,
      "step": 254300
    },
    {
      "epoch": 9.83872839076459,
      "grad_norm": 13.410513877868652,
      "learning_rate": 4.1801059674362845e-05,
      "loss": 2.2955,
      "step": 254400
    },
    {
      "epoch": 9.842595815446494,
      "grad_norm": 13.282660484313965,
      "learning_rate": 4.179783682046125e-05,
      "loss": 2.0304,
      "step": 254500
    },
    {
      "epoch": 9.846463240128399,
      "grad_norm": 12.848708152770996,
      "learning_rate": 4.179461396655967e-05,
      "loss": 2.1973,
      "step": 254600
    },
    {
      "epoch": 9.850330664810302,
      "grad_norm": 11.430763244628906,
      "learning_rate": 4.179139111265808e-05,
      "loss": 2.1276,
      "step": 254700
    },
    {
      "epoch": 9.854198089492208,
      "grad_norm": 10.60421085357666,
      "learning_rate": 4.17881682587565e-05,
      "loss": 2.2017,
      "step": 254800
    },
    {
      "epoch": 9.858065514174111,
      "grad_norm": 10.98563289642334,
      "learning_rate": 4.178494540485491e-05,
      "loss": 2.1274,
      "step": 254900
    },
    {
      "epoch": 9.861932938856016,
      "grad_norm": 14.124847412109375,
      "learning_rate": 4.178172255095332e-05,
      "loss": 2.1735,
      "step": 255000
    },
    {
      "epoch": 9.86580036353792,
      "grad_norm": 12.63821792602539,
      "learning_rate": 4.1778499697051734e-05,
      "loss": 2.0981,
      "step": 255100
    },
    {
      "epoch": 9.869667788219825,
      "grad_norm": 14.792263984680176,
      "learning_rate": 4.177527684315015e-05,
      "loss": 2.0635,
      "step": 255200
    },
    {
      "epoch": 9.873535212901729,
      "grad_norm": 15.006789207458496,
      "learning_rate": 4.1772053989248564e-05,
      "loss": 2.1488,
      "step": 255300
    },
    {
      "epoch": 9.877402637583632,
      "grad_norm": 11.682007789611816,
      "learning_rate": 4.176883113534698e-05,
      "loss": 2.094,
      "step": 255400
    },
    {
      "epoch": 9.881270062265537,
      "grad_norm": 14.288985252380371,
      "learning_rate": 4.176560828144539e-05,
      "loss": 2.2235,
      "step": 255500
    },
    {
      "epoch": 9.88513748694744,
      "grad_norm": 11.998867988586426,
      "learning_rate": 4.17623854275438e-05,
      "loss": 2.1533,
      "step": 255600
    },
    {
      "epoch": 9.889004911629346,
      "grad_norm": 11.67657470703125,
      "learning_rate": 4.1759162573642216e-05,
      "loss": 2.1868,
      "step": 255700
    },
    {
      "epoch": 9.89287233631125,
      "grad_norm": 14.236157417297363,
      "learning_rate": 4.175593971974063e-05,
      "loss": 2.1071,
      "step": 255800
    },
    {
      "epoch": 9.896739760993155,
      "grad_norm": 10.159002304077148,
      "learning_rate": 4.1752716865839045e-05,
      "loss": 2.1724,
      "step": 255900
    },
    {
      "epoch": 9.900607185675058,
      "grad_norm": 11.8694486618042,
      "learning_rate": 4.174949401193745e-05,
      "loss": 2.1712,
      "step": 256000
    },
    {
      "epoch": 9.904474610356964,
      "grad_norm": 13.904735565185547,
      "learning_rate": 4.174627115803587e-05,
      "loss": 2.118,
      "step": 256100
    },
    {
      "epoch": 9.908342035038867,
      "grad_norm": 11.967449188232422,
      "learning_rate": 4.174304830413428e-05,
      "loss": 2.1657,
      "step": 256200
    },
    {
      "epoch": 9.912209459720772,
      "grad_norm": 13.525626182556152,
      "learning_rate": 4.17398254502327e-05,
      "loss": 2.1924,
      "step": 256300
    },
    {
      "epoch": 9.916076884402676,
      "grad_norm": 13.294194221496582,
      "learning_rate": 4.1736602596331105e-05,
      "loss": 2.2076,
      "step": 256400
    },
    {
      "epoch": 9.919944309084581,
      "grad_norm": 11.570420265197754,
      "learning_rate": 4.173337974242952e-05,
      "loss": 2.1794,
      "step": 256500
    },
    {
      "epoch": 9.923811733766485,
      "grad_norm": 12.234078407287598,
      "learning_rate": 4.1730156888527935e-05,
      "loss": 2.1689,
      "step": 256600
    },
    {
      "epoch": 9.92767915844839,
      "grad_norm": 12.091064453125,
      "learning_rate": 4.172693403462635e-05,
      "loss": 2.1458,
      "step": 256700
    },
    {
      "epoch": 9.931546583130293,
      "grad_norm": 11.227983474731445,
      "learning_rate": 4.172371118072476e-05,
      "loss": 2.1483,
      "step": 256800
    },
    {
      "epoch": 9.935414007812199,
      "grad_norm": 11.673196792602539,
      "learning_rate": 4.172048832682317e-05,
      "loss": 2.1301,
      "step": 256900
    },
    {
      "epoch": 9.939281432494102,
      "grad_norm": 9.518553733825684,
      "learning_rate": 4.171726547292159e-05,
      "loss": 2.0246,
      "step": 257000
    },
    {
      "epoch": 9.943148857176006,
      "grad_norm": 12.519980430603027,
      "learning_rate": 4.1714042619019995e-05,
      "loss": 2.1893,
      "step": 257100
    },
    {
      "epoch": 9.94701628185791,
      "grad_norm": 9.649070739746094,
      "learning_rate": 4.171081976511841e-05,
      "loss": 2.0641,
      "step": 257200
    },
    {
      "epoch": 9.950883706539814,
      "grad_norm": 11.937344551086426,
      "learning_rate": 4.1707596911216824e-05,
      "loss": 2.1265,
      "step": 257300
    },
    {
      "epoch": 9.95475113122172,
      "grad_norm": 11.289721488952637,
      "learning_rate": 4.170437405731524e-05,
      "loss": 2.2373,
      "step": 257400
    },
    {
      "epoch": 9.958618555903623,
      "grad_norm": 10.132372856140137,
      "learning_rate": 4.170115120341365e-05,
      "loss": 2.1746,
      "step": 257500
    },
    {
      "epoch": 9.962485980585528,
      "grad_norm": 9.712055206298828,
      "learning_rate": 4.169792834951206e-05,
      "loss": 2.1845,
      "step": 257600
    },
    {
      "epoch": 9.966353405267432,
      "grad_norm": 13.600191116333008,
      "learning_rate": 4.1694705495610476e-05,
      "loss": 2.1915,
      "step": 257700
    },
    {
      "epoch": 9.970220829949337,
      "grad_norm": 10.238422393798828,
      "learning_rate": 4.169148264170889e-05,
      "loss": 2.1456,
      "step": 257800
    },
    {
      "epoch": 9.97408825463124,
      "grad_norm": 11.89077091217041,
      "learning_rate": 4.16882597878073e-05,
      "loss": 2.0891,
      "step": 257900
    },
    {
      "epoch": 9.977955679313146,
      "grad_norm": 11.363920211791992,
      "learning_rate": 4.1685036933905714e-05,
      "loss": 2.1082,
      "step": 258000
    },
    {
      "epoch": 9.98182310399505,
      "grad_norm": 12.54404067993164,
      "learning_rate": 4.168181408000413e-05,
      "loss": 2.1681,
      "step": 258100
    },
    {
      "epoch": 9.985690528676955,
      "grad_norm": 10.610776901245117,
      "learning_rate": 4.167859122610254e-05,
      "loss": 2.1542,
      "step": 258200
    },
    {
      "epoch": 9.989557953358858,
      "grad_norm": 12.530509948730469,
      "learning_rate": 4.167536837220095e-05,
      "loss": 2.219,
      "step": 258300
    },
    {
      "epoch": 9.993425378040763,
      "grad_norm": 14.346247673034668,
      "learning_rate": 4.1672145518299366e-05,
      "loss": 2.0972,
      "step": 258400
    },
    {
      "epoch": 9.997292802722667,
      "grad_norm": 11.006471633911133,
      "learning_rate": 4.166892266439778e-05,
      "loss": 2.174,
      "step": 258500
    },
    {
      "epoch": 10.0,
      "eval_loss": 2.025038242340088,
      "eval_runtime": 3.4699,
      "eval_samples_per_second": 392.23,
      "eval_steps_per_second": 392.23,
      "step": 258570
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.9772415161132812,
      "eval_runtime": 55.7675,
      "eval_samples_per_second": 463.657,
      "eval_steps_per_second": 463.657,
      "step": 258570
    },
    {
      "epoch": 10.001160227404572,
      "grad_norm": 11.450145721435547,
      "learning_rate": 4.1665699810496195e-05,
      "loss": 2.194,
      "step": 258600
    },
    {
      "epoch": 10.005027652086476,
      "grad_norm": 10.875626564025879,
      "learning_rate": 4.16624769565946e-05,
      "loss": 2.2251,
      "step": 258700
    },
    {
      "epoch": 10.008895076768379,
      "grad_norm": 13.241058349609375,
      "learning_rate": 4.165925410269302e-05,
      "loss": 2.1769,
      "step": 258800
    },
    {
      "epoch": 10.012762501450284,
      "grad_norm": 11.042940139770508,
      "learning_rate": 4.165603124879143e-05,
      "loss": 2.0413,
      "step": 258900
    },
    {
      "epoch": 10.016629926132188,
      "grad_norm": 15.021622657775879,
      "learning_rate": 4.165280839488985e-05,
      "loss": 2.1017,
      "step": 259000
    },
    {
      "epoch": 10.020497350814093,
      "grad_norm": 9.587682723999023,
      "learning_rate": 4.1649585540988255e-05,
      "loss": 2.0661,
      "step": 259100
    },
    {
      "epoch": 10.024364775495997,
      "grad_norm": 19.967161178588867,
      "learning_rate": 4.164636268708667e-05,
      "loss": 2.1333,
      "step": 259200
    },
    {
      "epoch": 10.028232200177902,
      "grad_norm": 14.333660125732422,
      "learning_rate": 4.1643139833185085e-05,
      "loss": 2.1395,
      "step": 259300
    },
    {
      "epoch": 10.032099624859805,
      "grad_norm": 13.790858268737793,
      "learning_rate": 4.16399169792835e-05,
      "loss": 2.1753,
      "step": 259400
    },
    {
      "epoch": 10.03596704954171,
      "grad_norm": 10.25589370727539,
      "learning_rate": 4.163669412538191e-05,
      "loss": 2.2281,
      "step": 259500
    },
    {
      "epoch": 10.039834474223614,
      "grad_norm": 10.472558975219727,
      "learning_rate": 4.163347127148032e-05,
      "loss": 2.0699,
      "step": 259600
    },
    {
      "epoch": 10.04370189890552,
      "grad_norm": 13.926657676696777,
      "learning_rate": 4.163024841757874e-05,
      "loss": 2.1822,
      "step": 259700
    },
    {
      "epoch": 10.047569323587423,
      "grad_norm": 11.802775382995605,
      "learning_rate": 4.1627025563677145e-05,
      "loss": 2.1977,
      "step": 259800
    },
    {
      "epoch": 10.051436748269328,
      "grad_norm": 11.915445327758789,
      "learning_rate": 4.162380270977556e-05,
      "loss": 2.1388,
      "step": 259900
    },
    {
      "epoch": 10.055304172951232,
      "grad_norm": 11.872323989868164,
      "learning_rate": 4.1620579855873974e-05,
      "loss": 2.2492,
      "step": 260000
    },
    {
      "epoch": 10.059171597633137,
      "grad_norm": 13.24018383026123,
      "learning_rate": 4.161735700197239e-05,
      "loss": 2.1049,
      "step": 260100
    },
    {
      "epoch": 10.06303902231504,
      "grad_norm": 14.448805809020996,
      "learning_rate": 4.16141341480708e-05,
      "loss": 2.0735,
      "step": 260200
    },
    {
      "epoch": 10.066906446996946,
      "grad_norm": 11.707460403442383,
      "learning_rate": 4.161091129416921e-05,
      "loss": 2.0913,
      "step": 260300
    },
    {
      "epoch": 10.070773871678849,
      "grad_norm": 11.59472942352295,
      "learning_rate": 4.1607688440267626e-05,
      "loss": 2.1516,
      "step": 260400
    },
    {
      "epoch": 10.074641296360753,
      "grad_norm": 10.3406343460083,
      "learning_rate": 4.160446558636604e-05,
      "loss": 2.1594,
      "step": 260500
    },
    {
      "epoch": 10.078508721042658,
      "grad_norm": 11.153170585632324,
      "learning_rate": 4.160124273246445e-05,
      "loss": 2.1653,
      "step": 260600
    },
    {
      "epoch": 10.082376145724561,
      "grad_norm": 12.400754928588867,
      "learning_rate": 4.1598019878562864e-05,
      "loss": 2.123,
      "step": 260700
    },
    {
      "epoch": 10.086243570406467,
      "grad_norm": 10.938355445861816,
      "learning_rate": 4.159479702466128e-05,
      "loss": 2.0861,
      "step": 260800
    },
    {
      "epoch": 10.09011099508837,
      "grad_norm": 12.667623519897461,
      "learning_rate": 4.159157417075969e-05,
      "loss": 2.1206,
      "step": 260900
    },
    {
      "epoch": 10.093978419770275,
      "grad_norm": 11.535059928894043,
      "learning_rate": 4.15883513168581e-05,
      "loss": 2.1198,
      "step": 261000
    },
    {
      "epoch": 10.097845844452179,
      "grad_norm": 11.833746910095215,
      "learning_rate": 4.1585128462956516e-05,
      "loss": 2.1132,
      "step": 261100
    },
    {
      "epoch": 10.101713269134084,
      "grad_norm": 12.179443359375,
      "learning_rate": 4.158190560905493e-05,
      "loss": 2.1348,
      "step": 261200
    },
    {
      "epoch": 10.105580693815988,
      "grad_norm": 13.994494438171387,
      "learning_rate": 4.1578682755153345e-05,
      "loss": 2.1053,
      "step": 261300
    },
    {
      "epoch": 10.109448118497893,
      "grad_norm": 11.350306510925293,
      "learning_rate": 4.157545990125176e-05,
      "loss": 2.1499,
      "step": 261400
    },
    {
      "epoch": 10.113315543179796,
      "grad_norm": 12.81255054473877,
      "learning_rate": 4.157223704735017e-05,
      "loss": 2.1523,
      "step": 261500
    },
    {
      "epoch": 10.117182967861702,
      "grad_norm": 10.970109939575195,
      "learning_rate": 4.156901419344858e-05,
      "loss": 2.16,
      "step": 261600
    },
    {
      "epoch": 10.121050392543605,
      "grad_norm": 14.470446586608887,
      "learning_rate": 4.1565791339547e-05,
      "loss": 2.1277,
      "step": 261700
    },
    {
      "epoch": 10.12491781722551,
      "grad_norm": 12.064637184143066,
      "learning_rate": 4.156256848564541e-05,
      "loss": 2.2447,
      "step": 261800
    },
    {
      "epoch": 10.128785241907414,
      "grad_norm": 17.726604461669922,
      "learning_rate": 4.1559345631743827e-05,
      "loss": 2.1455,
      "step": 261900
    },
    {
      "epoch": 10.132652666589319,
      "grad_norm": 13.9408597946167,
      "learning_rate": 4.155612277784224e-05,
      "loss": 2.2294,
      "step": 262000
    },
    {
      "epoch": 10.136520091271223,
      "grad_norm": 12.826127052307129,
      "learning_rate": 4.155289992394065e-05,
      "loss": 2.1321,
      "step": 262100
    },
    {
      "epoch": 10.140387515953126,
      "grad_norm": 9.985930442810059,
      "learning_rate": 4.1549677070039064e-05,
      "loss": 2.1193,
      "step": 262200
    },
    {
      "epoch": 10.144254940635031,
      "grad_norm": 17.813804626464844,
      "learning_rate": 4.154645421613748e-05,
      "loss": 2.1308,
      "step": 262300
    },
    {
      "epoch": 10.148122365316935,
      "grad_norm": 13.839405059814453,
      "learning_rate": 4.1543231362235893e-05,
      "loss": 2.2441,
      "step": 262400
    },
    {
      "epoch": 10.15198978999884,
      "grad_norm": 11.515584945678711,
      "learning_rate": 4.154000850833431e-05,
      "loss": 2.1645,
      "step": 262500
    },
    {
      "epoch": 10.155857214680744,
      "grad_norm": 11.938607215881348,
      "learning_rate": 4.1536785654432716e-05,
      "loss": 2.1517,
      "step": 262600
    },
    {
      "epoch": 10.159724639362649,
      "grad_norm": 13.872063636779785,
      "learning_rate": 4.153356280053113e-05,
      "loss": 2.0858,
      "step": 262700
    },
    {
      "epoch": 10.163592064044552,
      "grad_norm": 13.069585800170898,
      "learning_rate": 4.1530339946629546e-05,
      "loss": 2.0826,
      "step": 262800
    },
    {
      "epoch": 10.167459488726458,
      "grad_norm": 12.726425170898438,
      "learning_rate": 4.1527117092727953e-05,
      "loss": 2.0898,
      "step": 262900
    },
    {
      "epoch": 10.171326913408361,
      "grad_norm": 13.783754348754883,
      "learning_rate": 4.152389423882637e-05,
      "loss": 2.101,
      "step": 263000
    },
    {
      "epoch": 10.175194338090266,
      "grad_norm": 11.033123970031738,
      "learning_rate": 4.152067138492478e-05,
      "loss": 2.2148,
      "step": 263100
    },
    {
      "epoch": 10.17906176277217,
      "grad_norm": 12.165374755859375,
      "learning_rate": 4.15174485310232e-05,
      "loss": 2.2221,
      "step": 263200
    },
    {
      "epoch": 10.182929187454075,
      "grad_norm": 11.852714538574219,
      "learning_rate": 4.1514225677121606e-05,
      "loss": 2.0824,
      "step": 263300
    },
    {
      "epoch": 10.186796612135979,
      "grad_norm": 15.583998680114746,
      "learning_rate": 4.151100282322002e-05,
      "loss": 2.164,
      "step": 263400
    },
    {
      "epoch": 10.190664036817884,
      "grad_norm": 13.427803993225098,
      "learning_rate": 4.1507779969318435e-05,
      "loss": 2.1082,
      "step": 263500
    },
    {
      "epoch": 10.194531461499787,
      "grad_norm": 9.773388862609863,
      "learning_rate": 4.150455711541685e-05,
      "loss": 2.2114,
      "step": 263600
    },
    {
      "epoch": 10.19839888618169,
      "grad_norm": 12.73758602142334,
      "learning_rate": 4.150133426151526e-05,
      "loss": 2.1418,
      "step": 263700
    },
    {
      "epoch": 10.202266310863596,
      "grad_norm": 10.246258735656738,
      "learning_rate": 4.149811140761367e-05,
      "loss": 2.0795,
      "step": 263800
    },
    {
      "epoch": 10.2061337355455,
      "grad_norm": 10.857405662536621,
      "learning_rate": 4.149488855371209e-05,
      "loss": 2.1683,
      "step": 263900
    },
    {
      "epoch": 10.210001160227405,
      "grad_norm": 10.353638648986816,
      "learning_rate": 4.14916656998105e-05,
      "loss": 2.1901,
      "step": 264000
    },
    {
      "epoch": 10.213868584909308,
      "grad_norm": 9.415093421936035,
      "learning_rate": 4.148844284590891e-05,
      "loss": 2.0854,
      "step": 264100
    },
    {
      "epoch": 10.217736009591214,
      "grad_norm": 14.204602241516113,
      "learning_rate": 4.1485219992007324e-05,
      "loss": 2.1593,
      "step": 264200
    },
    {
      "epoch": 10.221603434273117,
      "grad_norm": 16.029911041259766,
      "learning_rate": 4.148199713810574e-05,
      "loss": 2.1504,
      "step": 264300
    },
    {
      "epoch": 10.225470858955022,
      "grad_norm": 14.201508522033691,
      "learning_rate": 4.1478774284204154e-05,
      "loss": 2.1423,
      "step": 264400
    },
    {
      "epoch": 10.229338283636926,
      "grad_norm": 13.410253524780273,
      "learning_rate": 4.147555143030256e-05,
      "loss": 2.1944,
      "step": 264500
    },
    {
      "epoch": 10.233205708318831,
      "grad_norm": 9.711664199829102,
      "learning_rate": 4.1472328576400977e-05,
      "loss": 2.1713,
      "step": 264600
    },
    {
      "epoch": 10.237073133000735,
      "grad_norm": 11.536652565002441,
      "learning_rate": 4.146910572249939e-05,
      "loss": 2.134,
      "step": 264700
    },
    {
      "epoch": 10.24094055768264,
      "grad_norm": 12.810596466064453,
      "learning_rate": 4.1465882868597806e-05,
      "loss": 2.1297,
      "step": 264800
    },
    {
      "epoch": 10.244807982364543,
      "grad_norm": 13.054097175598145,
      "learning_rate": 4.1462660014696214e-05,
      "loss": 2.2442,
      "step": 264900
    },
    {
      "epoch": 10.248675407046449,
      "grad_norm": 15.160638809204102,
      "learning_rate": 4.145943716079463e-05,
      "loss": 2.2085,
      "step": 265000
    },
    {
      "epoch": 10.252542831728352,
      "grad_norm": 13.79695987701416,
      "learning_rate": 4.145621430689304e-05,
      "loss": 2.1149,
      "step": 265100
    },
    {
      "epoch": 10.256410256410255,
      "grad_norm": 11.046650886535645,
      "learning_rate": 4.145299145299146e-05,
      "loss": 2.1452,
      "step": 265200
    },
    {
      "epoch": 10.26027768109216,
      "grad_norm": 12.248886108398438,
      "learning_rate": 4.1449768599089866e-05,
      "loss": 2.196,
      "step": 265300
    },
    {
      "epoch": 10.264145105774064,
      "grad_norm": 10.091201782226562,
      "learning_rate": 4.144654574518828e-05,
      "loss": 2.0526,
      "step": 265400
    },
    {
      "epoch": 10.26801253045597,
      "grad_norm": 12.291923522949219,
      "learning_rate": 4.1443322891286695e-05,
      "loss": 2.1347,
      "step": 265500
    },
    {
      "epoch": 10.271879955137873,
      "grad_norm": 11.080307960510254,
      "learning_rate": 4.1440100037385103e-05,
      "loss": 2.1076,
      "step": 265600
    },
    {
      "epoch": 10.275747379819778,
      "grad_norm": 12.256486892700195,
      "learning_rate": 4.143687718348352e-05,
      "loss": 2.1934,
      "step": 265700
    },
    {
      "epoch": 10.279614804501682,
      "grad_norm": 9.352410316467285,
      "learning_rate": 4.143365432958193e-05,
      "loss": 2.1316,
      "step": 265800
    },
    {
      "epoch": 10.283482229183587,
      "grad_norm": 12.803875923156738,
      "learning_rate": 4.143043147568035e-05,
      "loss": 2.1401,
      "step": 265900
    },
    {
      "epoch": 10.28734965386549,
      "grad_norm": 12.50211238861084,
      "learning_rate": 4.1427208621778756e-05,
      "loss": 2.0533,
      "step": 266000
    },
    {
      "epoch": 10.291217078547396,
      "grad_norm": 11.038287162780762,
      "learning_rate": 4.142398576787717e-05,
      "loss": 2.1921,
      "step": 266100
    },
    {
      "epoch": 10.2950845032293,
      "grad_norm": 12.98604679107666,
      "learning_rate": 4.1420762913975585e-05,
      "loss": 2.1585,
      "step": 266200
    },
    {
      "epoch": 10.298951927911205,
      "grad_norm": 13.24588394165039,
      "learning_rate": 4.1417540060074e-05,
      "loss": 2.1732,
      "step": 266300
    },
    {
      "epoch": 10.302819352593108,
      "grad_norm": 12.519797325134277,
      "learning_rate": 4.141431720617241e-05,
      "loss": 2.1341,
      "step": 266400
    },
    {
      "epoch": 10.306686777275013,
      "grad_norm": 12.133149147033691,
      "learning_rate": 4.141109435227082e-05,
      "loss": 2.1255,
      "step": 266500
    },
    {
      "epoch": 10.310554201956917,
      "grad_norm": 12.063129425048828,
      "learning_rate": 4.140787149836924e-05,
      "loss": 2.2394,
      "step": 266600
    },
    {
      "epoch": 10.314421626638822,
      "grad_norm": 11.69139289855957,
      "learning_rate": 4.140464864446765e-05,
      "loss": 2.1491,
      "step": 266700
    },
    {
      "epoch": 10.318289051320725,
      "grad_norm": 11.720739364624023,
      "learning_rate": 4.140142579056606e-05,
      "loss": 2.0617,
      "step": 266800
    },
    {
      "epoch": 10.322156476002629,
      "grad_norm": 10.624529838562012,
      "learning_rate": 4.1398202936664474e-05,
      "loss": 2.1807,
      "step": 266900
    },
    {
      "epoch": 10.326023900684534,
      "grad_norm": 9.642662048339844,
      "learning_rate": 4.139498008276289e-05,
      "loss": 2.174,
      "step": 267000
    },
    {
      "epoch": 10.329891325366438,
      "grad_norm": 11.206972122192383,
      "learning_rate": 4.1391757228861304e-05,
      "loss": 2.0464,
      "step": 267100
    },
    {
      "epoch": 10.333758750048343,
      "grad_norm": 11.993326187133789,
      "learning_rate": 4.138853437495971e-05,
      "loss": 2.0772,
      "step": 267200
    },
    {
      "epoch": 10.337626174730246,
      "grad_norm": 12.026073455810547,
      "learning_rate": 4.1385311521058127e-05,
      "loss": 2.0531,
      "step": 267300
    },
    {
      "epoch": 10.341493599412152,
      "grad_norm": 10.74563217163086,
      "learning_rate": 4.138208866715654e-05,
      "loss": 2.1351,
      "step": 267400
    },
    {
      "epoch": 10.345361024094055,
      "grad_norm": 12.214524269104004,
      "learning_rate": 4.1378865813254956e-05,
      "loss": 2.1833,
      "step": 267500
    },
    {
      "epoch": 10.34922844877596,
      "grad_norm": 8.980790138244629,
      "learning_rate": 4.1375642959353364e-05,
      "loss": 2.1693,
      "step": 267600
    },
    {
      "epoch": 10.353095873457864,
      "grad_norm": 7.844147205352783,
      "learning_rate": 4.137242010545178e-05,
      "loss": 2.1818,
      "step": 267700
    },
    {
      "epoch": 10.35696329813977,
      "grad_norm": 12.4769287109375,
      "learning_rate": 4.136919725155019e-05,
      "loss": 2.127,
      "step": 267800
    },
    {
      "epoch": 10.360830722821673,
      "grad_norm": 11.002939224243164,
      "learning_rate": 4.136597439764861e-05,
      "loss": 2.1598,
      "step": 267900
    },
    {
      "epoch": 10.364698147503578,
      "grad_norm": 13.233006477355957,
      "learning_rate": 4.1362751543747016e-05,
      "loss": 2.165,
      "step": 268000
    },
    {
      "epoch": 10.368565572185481,
      "grad_norm": 11.38674545288086,
      "learning_rate": 4.135952868984543e-05,
      "loss": 2.0077,
      "step": 268100
    },
    {
      "epoch": 10.372432996867387,
      "grad_norm": 9.222884178161621,
      "learning_rate": 4.1356305835943845e-05,
      "loss": 2.0786,
      "step": 268200
    },
    {
      "epoch": 10.37630042154929,
      "grad_norm": 12.525362968444824,
      "learning_rate": 4.135308298204226e-05,
      "loss": 2.1467,
      "step": 268300
    },
    {
      "epoch": 10.380167846231195,
      "grad_norm": 11.101079940795898,
      "learning_rate": 4.1349860128140675e-05,
      "loss": 2.1683,
      "step": 268400
    },
    {
      "epoch": 10.384035270913099,
      "grad_norm": 15.322345733642578,
      "learning_rate": 4.134663727423908e-05,
      "loss": 2.0813,
      "step": 268500
    },
    {
      "epoch": 10.387902695595002,
      "grad_norm": 12.720629692077637,
      "learning_rate": 4.13434144203375e-05,
      "loss": 2.1534,
      "step": 268600
    },
    {
      "epoch": 10.391770120276908,
      "grad_norm": 12.450976371765137,
      "learning_rate": 4.134019156643591e-05,
      "loss": 2.073,
      "step": 268700
    },
    {
      "epoch": 10.395637544958811,
      "grad_norm": 12.18795394897461,
      "learning_rate": 4.133696871253433e-05,
      "loss": 2.2718,
      "step": 268800
    },
    {
      "epoch": 10.399504969640716,
      "grad_norm": 14.965231895446777,
      "learning_rate": 4.133374585863274e-05,
      "loss": 2.1469,
      "step": 268900
    },
    {
      "epoch": 10.40337239432262,
      "grad_norm": 14.881417274475098,
      "learning_rate": 4.1330523004731156e-05,
      "loss": 2.1252,
      "step": 269000
    },
    {
      "epoch": 10.407239819004525,
      "grad_norm": 12.367789268493652,
      "learning_rate": 4.1327300150829564e-05,
      "loss": 2.0907,
      "step": 269100
    },
    {
      "epoch": 10.411107243686429,
      "grad_norm": 11.342734336853027,
      "learning_rate": 4.132407729692798e-05,
      "loss": 2.2058,
      "step": 269200
    },
    {
      "epoch": 10.414974668368334,
      "grad_norm": 11.978657722473145,
      "learning_rate": 4.1320854443026394e-05,
      "loss": 2.1692,
      "step": 269300
    },
    {
      "epoch": 10.418842093050237,
      "grad_norm": 11.65017032623291,
      "learning_rate": 4.131763158912481e-05,
      "loss": 2.1371,
      "step": 269400
    },
    {
      "epoch": 10.422709517732143,
      "grad_norm": 6.833559513092041,
      "learning_rate": 4.1314408735223216e-05,
      "loss": 2.1377,
      "step": 269500
    },
    {
      "epoch": 10.426576942414046,
      "grad_norm": 7.837721347808838,
      "learning_rate": 4.131118588132163e-05,
      "loss": 2.1287,
      "step": 269600
    },
    {
      "epoch": 10.430444367095951,
      "grad_norm": 12.387167930603027,
      "learning_rate": 4.1307963027420046e-05,
      "loss": 2.1315,
      "step": 269700
    },
    {
      "epoch": 10.434311791777855,
      "grad_norm": 12.652897834777832,
      "learning_rate": 4.130474017351846e-05,
      "loss": 2.1475,
      "step": 269800
    },
    {
      "epoch": 10.43817921645976,
      "grad_norm": 11.03197956085205,
      "learning_rate": 4.130151731961687e-05,
      "loss": 2.1623,
      "step": 269900
    },
    {
      "epoch": 10.442046641141664,
      "grad_norm": 10.408308029174805,
      "learning_rate": 4.129829446571528e-05,
      "loss": 2.1386,
      "step": 270000
    },
    {
      "epoch": 10.445914065823569,
      "grad_norm": 9.963687896728516,
      "learning_rate": 4.12950716118137e-05,
      "loss": 2.1196,
      "step": 270100
    },
    {
      "epoch": 10.449781490505472,
      "grad_norm": 10.653496742248535,
      "learning_rate": 4.129184875791211e-05,
      "loss": 2.2109,
      "step": 270200
    },
    {
      "epoch": 10.453648915187376,
      "grad_norm": 11.836185455322266,
      "learning_rate": 4.128862590401052e-05,
      "loss": 2.1716,
      "step": 270300
    },
    {
      "epoch": 10.457516339869281,
      "grad_norm": 15.047355651855469,
      "learning_rate": 4.1285403050108935e-05,
      "loss": 2.1576,
      "step": 270400
    },
    {
      "epoch": 10.461383764551185,
      "grad_norm": 9.981560707092285,
      "learning_rate": 4.128218019620735e-05,
      "loss": 2.1516,
      "step": 270500
    },
    {
      "epoch": 10.46525118923309,
      "grad_norm": 11.87415599822998,
      "learning_rate": 4.1278957342305765e-05,
      "loss": 2.1265,
      "step": 270600
    },
    {
      "epoch": 10.469118613914993,
      "grad_norm": 13.534071922302246,
      "learning_rate": 4.127573448840417e-05,
      "loss": 2.144,
      "step": 270700
    },
    {
      "epoch": 10.472986038596899,
      "grad_norm": 14.493513107299805,
      "learning_rate": 4.127251163450259e-05,
      "loss": 2.2046,
      "step": 270800
    },
    {
      "epoch": 10.476853463278802,
      "grad_norm": 12.857516288757324,
      "learning_rate": 4.1269288780601e-05,
      "loss": 2.2099,
      "step": 270900
    },
    {
      "epoch": 10.480720887960707,
      "grad_norm": 10.99538516998291,
      "learning_rate": 4.126606592669942e-05,
      "loss": 2.165,
      "step": 271000
    },
    {
      "epoch": 10.484588312642611,
      "grad_norm": 10.611845970153809,
      "learning_rate": 4.1262843072797825e-05,
      "loss": 2.1027,
      "step": 271100
    },
    {
      "epoch": 10.488455737324516,
      "grad_norm": 12.349326133728027,
      "learning_rate": 4.125962021889624e-05,
      "loss": 2.1784,
      "step": 271200
    },
    {
      "epoch": 10.49232316200642,
      "grad_norm": 9.986934661865234,
      "learning_rate": 4.1256397364994654e-05,
      "loss": 2.1631,
      "step": 271300
    },
    {
      "epoch": 10.496190586688325,
      "grad_norm": 12.550963401794434,
      "learning_rate": 4.125317451109307e-05,
      "loss": 2.1244,
      "step": 271400
    },
    {
      "epoch": 10.500058011370228,
      "grad_norm": 12.070100784301758,
      "learning_rate": 4.124995165719148e-05,
      "loss": 2.1084,
      "step": 271500
    },
    {
      "epoch": 10.503925436052134,
      "grad_norm": 11.990163803100586,
      "learning_rate": 4.124672880328989e-05,
      "loss": 2.2562,
      "step": 271600
    },
    {
      "epoch": 10.507792860734037,
      "grad_norm": 12.56031322479248,
      "learning_rate": 4.1243505949388306e-05,
      "loss": 2.1314,
      "step": 271700
    },
    {
      "epoch": 10.511660285415942,
      "grad_norm": 16.79570198059082,
      "learning_rate": 4.1240283095486714e-05,
      "loss": 2.1571,
      "step": 271800
    },
    {
      "epoch": 10.515527710097846,
      "grad_norm": 11.441813468933105,
      "learning_rate": 4.123706024158513e-05,
      "loss": 2.1849,
      "step": 271900
    },
    {
      "epoch": 10.51939513477975,
      "grad_norm": 11.56906795501709,
      "learning_rate": 4.1233837387683544e-05,
      "loss": 2.1184,
      "step": 272000
    },
    {
      "epoch": 10.523262559461655,
      "grad_norm": 10.629402160644531,
      "learning_rate": 4.123061453378196e-05,
      "loss": 2.1232,
      "step": 272100
    },
    {
      "epoch": 10.527129984143558,
      "grad_norm": 13.726744651794434,
      "learning_rate": 4.1227391679880366e-05,
      "loss": 2.1341,
      "step": 272200
    },
    {
      "epoch": 10.530997408825463,
      "grad_norm": 12.589381217956543,
      "learning_rate": 4.122416882597878e-05,
      "loss": 2.1272,
      "step": 272300
    },
    {
      "epoch": 10.534864833507367,
      "grad_norm": 11.646992683410645,
      "learning_rate": 4.1220945972077196e-05,
      "loss": 2.1674,
      "step": 272400
    },
    {
      "epoch": 10.538732258189272,
      "grad_norm": 12.07614517211914,
      "learning_rate": 4.121772311817561e-05,
      "loss": 2.0892,
      "step": 272500
    },
    {
      "epoch": 10.542599682871176,
      "grad_norm": 12.426161766052246,
      "learning_rate": 4.121450026427402e-05,
      "loss": 2.1667,
      "step": 272600
    },
    {
      "epoch": 10.546467107553081,
      "grad_norm": 10.528546333312988,
      "learning_rate": 4.121127741037243e-05,
      "loss": 2.2233,
      "step": 272700
    },
    {
      "epoch": 10.550334532234984,
      "grad_norm": 10.925167083740234,
      "learning_rate": 4.120805455647085e-05,
      "loss": 2.098,
      "step": 272800
    },
    {
      "epoch": 10.55420195691689,
      "grad_norm": 11.430376052856445,
      "learning_rate": 4.120483170256926e-05,
      "loss": 2.0189,
      "step": 272900
    },
    {
      "epoch": 10.558069381598793,
      "grad_norm": 9.84465503692627,
      "learning_rate": 4.120160884866767e-05,
      "loss": 2.1481,
      "step": 273000
    },
    {
      "epoch": 10.561936806280698,
      "grad_norm": 11.79317569732666,
      "learning_rate": 4.1198385994766085e-05,
      "loss": 2.1303,
      "step": 273100
    },
    {
      "epoch": 10.565804230962602,
      "grad_norm": 10.549697875976562,
      "learning_rate": 4.11951631408645e-05,
      "loss": 2.0548,
      "step": 273200
    },
    {
      "epoch": 10.569671655644505,
      "grad_norm": 10.852391242980957,
      "learning_rate": 4.1191940286962915e-05,
      "loss": 2.1917,
      "step": 273300
    },
    {
      "epoch": 10.57353908032641,
      "grad_norm": 9.435365676879883,
      "learning_rate": 4.118871743306132e-05,
      "loss": 2.0868,
      "step": 273400
    },
    {
      "epoch": 10.577406505008314,
      "grad_norm": 14.457067489624023,
      "learning_rate": 4.118549457915974e-05,
      "loss": 2.1243,
      "step": 273500
    },
    {
      "epoch": 10.58127392969022,
      "grad_norm": 12.739352226257324,
      "learning_rate": 4.118227172525815e-05,
      "loss": 2.1757,
      "step": 273600
    },
    {
      "epoch": 10.585141354372123,
      "grad_norm": 11.89754581451416,
      "learning_rate": 4.117904887135657e-05,
      "loss": 2.1127,
      "step": 273700
    },
    {
      "epoch": 10.589008779054028,
      "grad_norm": 11.286125183105469,
      "learning_rate": 4.1175826017454975e-05,
      "loss": 2.1497,
      "step": 273800
    },
    {
      "epoch": 10.592876203735932,
      "grad_norm": 13.687834739685059,
      "learning_rate": 4.117260316355339e-05,
      "loss": 2.1041,
      "step": 273900
    },
    {
      "epoch": 10.596743628417837,
      "grad_norm": 10.10744571685791,
      "learning_rate": 4.1169380309651804e-05,
      "loss": 2.1304,
      "step": 274000
    },
    {
      "epoch": 10.60061105309974,
      "grad_norm": 12.153221130371094,
      "learning_rate": 4.116615745575022e-05,
      "loss": 2.1373,
      "step": 274100
    },
    {
      "epoch": 10.604478477781646,
      "grad_norm": 14.713949203491211,
      "learning_rate": 4.116293460184863e-05,
      "loss": 2.0614,
      "step": 274200
    },
    {
      "epoch": 10.60834590246355,
      "grad_norm": 10.717267036437988,
      "learning_rate": 4.115971174794704e-05,
      "loss": 2.1228,
      "step": 274300
    },
    {
      "epoch": 10.612213327145454,
      "grad_norm": 13.097726821899414,
      "learning_rate": 4.1156488894045456e-05,
      "loss": 2.0742,
      "step": 274400
    },
    {
      "epoch": 10.616080751827358,
      "grad_norm": 12.657296180725098,
      "learning_rate": 4.1153266040143864e-05,
      "loss": 2.0945,
      "step": 274500
    },
    {
      "epoch": 10.619948176509263,
      "grad_norm": 10.917182922363281,
      "learning_rate": 4.115004318624228e-05,
      "loss": 2.2144,
      "step": 274600
    },
    {
      "epoch": 10.623815601191167,
      "grad_norm": 11.922611236572266,
      "learning_rate": 4.1146820332340694e-05,
      "loss": 2.1202,
      "step": 274700
    },
    {
      "epoch": 10.627683025873072,
      "grad_norm": 13.089312553405762,
      "learning_rate": 4.114359747843911e-05,
      "loss": 2.1845,
      "step": 274800
    },
    {
      "epoch": 10.631550450554975,
      "grad_norm": 12.805532455444336,
      "learning_rate": 4.114037462453752e-05,
      "loss": 2.1638,
      "step": 274900
    },
    {
      "epoch": 10.635417875236879,
      "grad_norm": 11.008035659790039,
      "learning_rate": 4.113715177063593e-05,
      "loss": 2.1562,
      "step": 275000
    },
    {
      "epoch": 10.639285299918784,
      "grad_norm": 13.70223617553711,
      "learning_rate": 4.1133928916734346e-05,
      "loss": 2.1598,
      "step": 275100
    },
    {
      "epoch": 10.643152724600688,
      "grad_norm": 11.43398380279541,
      "learning_rate": 4.113070606283276e-05,
      "loss": 2.1471,
      "step": 275200
    },
    {
      "epoch": 10.647020149282593,
      "grad_norm": 12.085829734802246,
      "learning_rate": 4.1127483208931175e-05,
      "loss": 2.2107,
      "step": 275300
    },
    {
      "epoch": 10.650887573964496,
      "grad_norm": 11.20397663116455,
      "learning_rate": 4.112426035502959e-05,
      "loss": 2.1331,
      "step": 275400
    },
    {
      "epoch": 10.654754998646402,
      "grad_norm": 12.037760734558105,
      "learning_rate": 4.1121037501128005e-05,
      "loss": 2.1616,
      "step": 275500
    },
    {
      "epoch": 10.658622423328305,
      "grad_norm": 9.532127380371094,
      "learning_rate": 4.111781464722641e-05,
      "loss": 2.123,
      "step": 275600
    },
    {
      "epoch": 10.66248984801021,
      "grad_norm": 9.672449111938477,
      "learning_rate": 4.111459179332483e-05,
      "loss": 2.1374,
      "step": 275700
    },
    {
      "epoch": 10.666357272692114,
      "grad_norm": 14.201984405517578,
      "learning_rate": 4.111136893942324e-05,
      "loss": 2.1275,
      "step": 275800
    },
    {
      "epoch": 10.67022469737402,
      "grad_norm": 12.064573287963867,
      "learning_rate": 4.110814608552166e-05,
      "loss": 2.1189,
      "step": 275900
    },
    {
      "epoch": 10.674092122055923,
      "grad_norm": 11.868042945861816,
      "learning_rate": 4.110492323162007e-05,
      "loss": 2.1409,
      "step": 276000
    },
    {
      "epoch": 10.677959546737828,
      "grad_norm": 13.059488296508789,
      "learning_rate": 4.110170037771848e-05,
      "loss": 2.1216,
      "step": 276100
    },
    {
      "epoch": 10.681826971419731,
      "grad_norm": 11.969121932983398,
      "learning_rate": 4.1098477523816894e-05,
      "loss": 2.1946,
      "step": 276200
    },
    {
      "epoch": 10.685694396101637,
      "grad_norm": 11.188666343688965,
      "learning_rate": 4.109525466991531e-05,
      "loss": 2.1501,
      "step": 276300
    },
    {
      "epoch": 10.68956182078354,
      "grad_norm": 13.823323249816895,
      "learning_rate": 4.1092031816013724e-05,
      "loss": 2.1739,
      "step": 276400
    },
    {
      "epoch": 10.693429245465445,
      "grad_norm": 13.205887794494629,
      "learning_rate": 4.108880896211213e-05,
      "loss": 2.1373,
      "step": 276500
    },
    {
      "epoch": 10.697296670147349,
      "grad_norm": 11.020442008972168,
      "learning_rate": 4.1085586108210546e-05,
      "loss": 2.1817,
      "step": 276600
    },
    {
      "epoch": 10.701164094829252,
      "grad_norm": 14.064948081970215,
      "learning_rate": 4.108236325430896e-05,
      "loss": 2.0384,
      "step": 276700
    },
    {
      "epoch": 10.705031519511158,
      "grad_norm": 14.315003395080566,
      "learning_rate": 4.1079140400407376e-05,
      "loss": 2.1963,
      "step": 276800
    },
    {
      "epoch": 10.708898944193061,
      "grad_norm": 11.276573181152344,
      "learning_rate": 4.1075917546505784e-05,
      "loss": 2.0938,
      "step": 276900
    },
    {
      "epoch": 10.712766368874966,
      "grad_norm": 9.258590698242188,
      "learning_rate": 4.10726946926042e-05,
      "loss": 2.1198,
      "step": 277000
    },
    {
      "epoch": 10.71663379355687,
      "grad_norm": 10.92993450164795,
      "learning_rate": 4.106947183870261e-05,
      "loss": 2.0627,
      "step": 277100
    },
    {
      "epoch": 10.720501218238775,
      "grad_norm": 10.689811706542969,
      "learning_rate": 4.106624898480103e-05,
      "loss": 2.104,
      "step": 277200
    },
    {
      "epoch": 10.724368642920679,
      "grad_norm": 12.482308387756348,
      "learning_rate": 4.1063026130899436e-05,
      "loss": 2.1491,
      "step": 277300
    },
    {
      "epoch": 10.728236067602584,
      "grad_norm": 9.013486862182617,
      "learning_rate": 4.105980327699785e-05,
      "loss": 2.2101,
      "step": 277400
    },
    {
      "epoch": 10.732103492284487,
      "grad_norm": 10.655913352966309,
      "learning_rate": 4.1056580423096265e-05,
      "loss": 2.0208,
      "step": 277500
    },
    {
      "epoch": 10.735970916966393,
      "grad_norm": 10.765205383300781,
      "learning_rate": 4.105335756919467e-05,
      "loss": 2.1178,
      "step": 277600
    },
    {
      "epoch": 10.739838341648296,
      "grad_norm": 12.922052383422852,
      "learning_rate": 4.105013471529309e-05,
      "loss": 2.1885,
      "step": 277700
    },
    {
      "epoch": 10.743705766330201,
      "grad_norm": 13.356176376342773,
      "learning_rate": 4.10469118613915e-05,
      "loss": 2.1361,
      "step": 277800
    },
    {
      "epoch": 10.747573191012105,
      "grad_norm": 14.01701831817627,
      "learning_rate": 4.104368900748992e-05,
      "loss": 2.1106,
      "step": 277900
    },
    {
      "epoch": 10.75144061569401,
      "grad_norm": 10.248202323913574,
      "learning_rate": 4.1040466153588325e-05,
      "loss": 2.141,
      "step": 278000
    },
    {
      "epoch": 10.755308040375914,
      "grad_norm": 9.893004417419434,
      "learning_rate": 4.103724329968674e-05,
      "loss": 2.1531,
      "step": 278100
    },
    {
      "epoch": 10.759175465057819,
      "grad_norm": 14.768516540527344,
      "learning_rate": 4.1034020445785155e-05,
      "loss": 2.101,
      "step": 278200
    },
    {
      "epoch": 10.763042889739722,
      "grad_norm": 15.3595552444458,
      "learning_rate": 4.103079759188357e-05,
      "loss": 2.0638,
      "step": 278300
    },
    {
      "epoch": 10.766910314421626,
      "grad_norm": 13.532614707946777,
      "learning_rate": 4.102757473798198e-05,
      "loss": 2.2489,
      "step": 278400
    },
    {
      "epoch": 10.770777739103531,
      "grad_norm": 9.097392082214355,
      "learning_rate": 4.102435188408039e-05,
      "loss": 2.1511,
      "step": 278500
    },
    {
      "epoch": 10.774645163785435,
      "grad_norm": 11.623008728027344,
      "learning_rate": 4.102112903017881e-05,
      "loss": 2.1868,
      "step": 278600
    },
    {
      "epoch": 10.77851258846734,
      "grad_norm": 9.300901412963867,
      "learning_rate": 4.101790617627722e-05,
      "loss": 2.0834,
      "step": 278700
    },
    {
      "epoch": 10.782380013149243,
      "grad_norm": 11.066291809082031,
      "learning_rate": 4.101468332237563e-05,
      "loss": 2.1018,
      "step": 278800
    },
    {
      "epoch": 10.786247437831149,
      "grad_norm": 11.750395774841309,
      "learning_rate": 4.1011460468474044e-05,
      "loss": 2.1701,
      "step": 278900
    },
    {
      "epoch": 10.790114862513052,
      "grad_norm": 13.414769172668457,
      "learning_rate": 4.100823761457246e-05,
      "loss": 2.2106,
      "step": 279000
    },
    {
      "epoch": 10.793982287194957,
      "grad_norm": 13.838963508605957,
      "learning_rate": 4.1005014760670874e-05,
      "loss": 2.2292,
      "step": 279100
    },
    {
      "epoch": 10.79784971187686,
      "grad_norm": 8.842605590820312,
      "learning_rate": 4.100179190676928e-05,
      "loss": 2.0823,
      "step": 279200
    },
    {
      "epoch": 10.801717136558766,
      "grad_norm": 10.468705177307129,
      "learning_rate": 4.0998569052867696e-05,
      "loss": 2.0199,
      "step": 279300
    },
    {
      "epoch": 10.80558456124067,
      "grad_norm": 10.568656921386719,
      "learning_rate": 4.099534619896611e-05,
      "loss": 2.128,
      "step": 279400
    },
    {
      "epoch": 10.809451985922575,
      "grad_norm": 15.770081520080566,
      "learning_rate": 4.0992123345064526e-05,
      "loss": 2.2006,
      "step": 279500
    },
    {
      "epoch": 10.813319410604478,
      "grad_norm": 11.798111915588379,
      "learning_rate": 4.0988900491162934e-05,
      "loss": 2.1409,
      "step": 279600
    },
    {
      "epoch": 10.817186835286384,
      "grad_norm": 8.903541564941406,
      "learning_rate": 4.098567763726135e-05,
      "loss": 2.1986,
      "step": 279700
    },
    {
      "epoch": 10.821054259968287,
      "grad_norm": 12.728395462036133,
      "learning_rate": 4.098245478335976e-05,
      "loss": 2.2169,
      "step": 279800
    },
    {
      "epoch": 10.824921684650192,
      "grad_norm": 10.222197532653809,
      "learning_rate": 4.097923192945818e-05,
      "loss": 2.1349,
      "step": 279900
    },
    {
      "epoch": 10.828789109332096,
      "grad_norm": 11.753143310546875,
      "learning_rate": 4.0976009075556586e-05,
      "loss": 2.0079,
      "step": 280000
    },
    {
      "epoch": 10.832656534014,
      "grad_norm": 14.936561584472656,
      "learning_rate": 4.0972786221655e-05,
      "loss": 2.1057,
      "step": 280100
    },
    {
      "epoch": 10.836523958695905,
      "grad_norm": 9.470643043518066,
      "learning_rate": 4.0969563367753415e-05,
      "loss": 2.0205,
      "step": 280200
    },
    {
      "epoch": 10.840391383377808,
      "grad_norm": 11.778162956237793,
      "learning_rate": 4.096634051385183e-05,
      "loss": 2.1559,
      "step": 280300
    },
    {
      "epoch": 10.844258808059713,
      "grad_norm": 14.220440864562988,
      "learning_rate": 4.096311765995024e-05,
      "loss": 2.0934,
      "step": 280400
    },
    {
      "epoch": 10.848126232741617,
      "grad_norm": 13.462739944458008,
      "learning_rate": 4.095989480604865e-05,
      "loss": 2.1755,
      "step": 280500
    },
    {
      "epoch": 10.851993657423522,
      "grad_norm": 12.561014175415039,
      "learning_rate": 4.095667195214707e-05,
      "loss": 2.1647,
      "step": 280600
    },
    {
      "epoch": 10.855861082105426,
      "grad_norm": 13.361342430114746,
      "learning_rate": 4.0953449098245475e-05,
      "loss": 2.0815,
      "step": 280700
    },
    {
      "epoch": 10.85972850678733,
      "grad_norm": 10.18126392364502,
      "learning_rate": 4.095022624434389e-05,
      "loss": 2.1357,
      "step": 280800
    },
    {
      "epoch": 10.863595931469234,
      "grad_norm": 9.944458961486816,
      "learning_rate": 4.0947003390442305e-05,
      "loss": 2.1649,
      "step": 280900
    },
    {
      "epoch": 10.86746335615114,
      "grad_norm": 18.50790786743164,
      "learning_rate": 4.094378053654072e-05,
      "loss": 2.0453,
      "step": 281000
    },
    {
      "epoch": 10.871330780833043,
      "grad_norm": 13.679367065429688,
      "learning_rate": 4.094055768263913e-05,
      "loss": 2.0987,
      "step": 281100
    },
    {
      "epoch": 10.875198205514948,
      "grad_norm": 9.65885066986084,
      "learning_rate": 4.093733482873754e-05,
      "loss": 2.103,
      "step": 281200
    },
    {
      "epoch": 10.879065630196852,
      "grad_norm": 14.32805347442627,
      "learning_rate": 4.093411197483596e-05,
      "loss": 2.12,
      "step": 281300
    },
    {
      "epoch": 10.882933054878757,
      "grad_norm": 11.502496719360352,
      "learning_rate": 4.093088912093437e-05,
      "loss": 2.1523,
      "step": 281400
    },
    {
      "epoch": 10.88680047956066,
      "grad_norm": 11.771101951599121,
      "learning_rate": 4.092766626703278e-05,
      "loss": 2.0859,
      "step": 281500
    },
    {
      "epoch": 10.890667904242566,
      "grad_norm": 11.171585083007812,
      "learning_rate": 4.0924443413131194e-05,
      "loss": 2.152,
      "step": 281600
    },
    {
      "epoch": 10.89453532892447,
      "grad_norm": 12.121732711791992,
      "learning_rate": 4.092122055922961e-05,
      "loss": 2.0913,
      "step": 281700
    },
    {
      "epoch": 10.898402753606373,
      "grad_norm": 13.40475845336914,
      "learning_rate": 4.0917997705328024e-05,
      "loss": 2.0411,
      "step": 281800
    },
    {
      "epoch": 10.902270178288278,
      "grad_norm": 10.766427040100098,
      "learning_rate": 4.091477485142644e-05,
      "loss": 2.1222,
      "step": 281900
    },
    {
      "epoch": 10.906137602970182,
      "grad_norm": 13.147322654724121,
      "learning_rate": 4.0911551997524846e-05,
      "loss": 2.1474,
      "step": 282000
    },
    {
      "epoch": 10.910005027652087,
      "grad_norm": 15.354279518127441,
      "learning_rate": 4.090832914362326e-05,
      "loss": 2.0635,
      "step": 282100
    },
    {
      "epoch": 10.91387245233399,
      "grad_norm": 10.371943473815918,
      "learning_rate": 4.0905106289721676e-05,
      "loss": 2.1266,
      "step": 282200
    },
    {
      "epoch": 10.917739877015896,
      "grad_norm": 8.964998245239258,
      "learning_rate": 4.090188343582009e-05,
      "loss": 2.0325,
      "step": 282300
    },
    {
      "epoch": 10.921607301697799,
      "grad_norm": 10.208242416381836,
      "learning_rate": 4.0898660581918505e-05,
      "loss": 2.0977,
      "step": 282400
    },
    {
      "epoch": 10.925474726379704,
      "grad_norm": 14.486711502075195,
      "learning_rate": 4.089543772801692e-05,
      "loss": 2.1252,
      "step": 282500
    },
    {
      "epoch": 10.929342151061608,
      "grad_norm": 10.387423515319824,
      "learning_rate": 4.089221487411533e-05,
      "loss": 2.1246,
      "step": 282600
    },
    {
      "epoch": 10.933209575743513,
      "grad_norm": 9.778450965881348,
      "learning_rate": 4.088899202021374e-05,
      "loss": 2.0644,
      "step": 282700
    },
    {
      "epoch": 10.937077000425417,
      "grad_norm": 15.044574737548828,
      "learning_rate": 4.088576916631216e-05,
      "loss": 1.9812,
      "step": 282800
    },
    {
      "epoch": 10.940944425107322,
      "grad_norm": 11.096649169921875,
      "learning_rate": 4.088254631241057e-05,
      "loss": 2.0935,
      "step": 282900
    },
    {
      "epoch": 10.944811849789225,
      "grad_norm": 16.455846786499023,
      "learning_rate": 4.0879323458508987e-05,
      "loss": 2.1704,
      "step": 283000
    },
    {
      "epoch": 10.948679274471129,
      "grad_norm": 11.116930961608887,
      "learning_rate": 4.0876100604607395e-05,
      "loss": 2.0556,
      "step": 283100
    },
    {
      "epoch": 10.952546699153034,
      "grad_norm": 11.59019947052002,
      "learning_rate": 4.087287775070581e-05,
      "loss": 2.1071,
      "step": 283200
    },
    {
      "epoch": 10.956414123834938,
      "grad_norm": 8.998749732971191,
      "learning_rate": 4.0869654896804224e-05,
      "loss": 2.213,
      "step": 283300
    },
    {
      "epoch": 10.960281548516843,
      "grad_norm": 9.850077629089355,
      "learning_rate": 4.086643204290263e-05,
      "loss": 2.115,
      "step": 283400
    },
    {
      "epoch": 10.964148973198746,
      "grad_norm": 10.481839179992676,
      "learning_rate": 4.086320918900105e-05,
      "loss": 2.1893,
      "step": 283500
    },
    {
      "epoch": 10.968016397880652,
      "grad_norm": 12.647315979003906,
      "learning_rate": 4.085998633509946e-05,
      "loss": 2.158,
      "step": 283600
    },
    {
      "epoch": 10.971883822562555,
      "grad_norm": 14.876317977905273,
      "learning_rate": 4.0856763481197876e-05,
      "loss": 2.0916,
      "step": 283700
    },
    {
      "epoch": 10.97575124724446,
      "grad_norm": 9.992533683776855,
      "learning_rate": 4.0853540627296284e-05,
      "loss": 2.1934,
      "step": 283800
    },
    {
      "epoch": 10.979618671926364,
      "grad_norm": 12.109212875366211,
      "learning_rate": 4.08503177733947e-05,
      "loss": 2.029,
      "step": 283900
    },
    {
      "epoch": 10.983486096608269,
      "grad_norm": 11.834365844726562,
      "learning_rate": 4.0847094919493113e-05,
      "loss": 2.1763,
      "step": 284000
    },
    {
      "epoch": 10.987353521290173,
      "grad_norm": 15.187216758728027,
      "learning_rate": 4.084387206559153e-05,
      "loss": 2.1778,
      "step": 284100
    },
    {
      "epoch": 10.991220945972078,
      "grad_norm": 14.056763648986816,
      "learning_rate": 4.0840649211689936e-05,
      "loss": 2.1603,
      "step": 284200
    },
    {
      "epoch": 10.995088370653981,
      "grad_norm": 17.361154556274414,
      "learning_rate": 4.083742635778835e-05,
      "loss": 2.0427,
      "step": 284300
    },
    {
      "epoch": 10.998955795335887,
      "grad_norm": 11.428977012634277,
      "learning_rate": 4.0834203503886766e-05,
      "loss": 2.1745,
      "step": 284400
    },
    {
      "epoch": 11.0,
      "eval_loss": 2.0108189582824707,
      "eval_runtime": 2.9223,
      "eval_samples_per_second": 465.732,
      "eval_steps_per_second": 465.732,
      "step": 284427
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.9570839405059814,
      "eval_runtime": 55.3796,
      "eval_samples_per_second": 466.905,
      "eval_steps_per_second": 466.905,
      "step": 284427
    },
    {
      "epoch": 11.00282322001779,
      "grad_norm": 10.499086380004883,
      "learning_rate": 4.083098064998518e-05,
      "loss": 2.1621,
      "step": 284500
    },
    {
      "epoch": 11.006690644699695,
      "grad_norm": 9.935378074645996,
      "learning_rate": 4.082775779608359e-05,
      "loss": 2.0681,
      "step": 284600
    },
    {
      "epoch": 11.010558069381599,
      "grad_norm": 10.719694137573242,
      "learning_rate": 4.0824534942182e-05,
      "loss": 2.1246,
      "step": 284700
    },
    {
      "epoch": 11.014425494063502,
      "grad_norm": 11.046989440917969,
      "learning_rate": 4.082131208828042e-05,
      "loss": 2.0487,
      "step": 284800
    },
    {
      "epoch": 11.018292918745408,
      "grad_norm": 12.005585670471191,
      "learning_rate": 4.081808923437883e-05,
      "loss": 2.139,
      "step": 284900
    },
    {
      "epoch": 11.022160343427311,
      "grad_norm": 10.387231826782227,
      "learning_rate": 4.081486638047724e-05,
      "loss": 2.1018,
      "step": 285000
    },
    {
      "epoch": 11.026027768109216,
      "grad_norm": 12.31597900390625,
      "learning_rate": 4.0811643526575655e-05,
      "loss": 2.1112,
      "step": 285100
    },
    {
      "epoch": 11.02989519279112,
      "grad_norm": 12.803871154785156,
      "learning_rate": 4.080842067267407e-05,
      "loss": 2.0745,
      "step": 285200
    },
    {
      "epoch": 11.033762617473025,
      "grad_norm": 9.177252769470215,
      "learning_rate": 4.0805197818772484e-05,
      "loss": 2.0904,
      "step": 285300
    },
    {
      "epoch": 11.037630042154928,
      "grad_norm": 14.555541038513184,
      "learning_rate": 4.080197496487089e-05,
      "loss": 2.1678,
      "step": 285400
    },
    {
      "epoch": 11.041497466836834,
      "grad_norm": 13.796149253845215,
      "learning_rate": 4.079875211096931e-05,
      "loss": 2.2265,
      "step": 285500
    },
    {
      "epoch": 11.045364891518737,
      "grad_norm": 12.342231750488281,
      "learning_rate": 4.079552925706772e-05,
      "loss": 2.169,
      "step": 285600
    },
    {
      "epoch": 11.049232316200643,
      "grad_norm": 13.208927154541016,
      "learning_rate": 4.0792306403166137e-05,
      "loss": 2.0606,
      "step": 285700
    },
    {
      "epoch": 11.053099740882546,
      "grad_norm": 11.65870189666748,
      "learning_rate": 4.0789083549264545e-05,
      "loss": 2.1548,
      "step": 285800
    },
    {
      "epoch": 11.056967165564451,
      "grad_norm": 11.51318359375,
      "learning_rate": 4.078586069536296e-05,
      "loss": 2.1588,
      "step": 285900
    },
    {
      "epoch": 11.060834590246355,
      "grad_norm": 8.941978454589844,
      "learning_rate": 4.0782637841461374e-05,
      "loss": 2.0914,
      "step": 286000
    },
    {
      "epoch": 11.06470201492826,
      "grad_norm": 11.342461585998535,
      "learning_rate": 4.077941498755979e-05,
      "loss": 2.1477,
      "step": 286100
    },
    {
      "epoch": 11.068569439610163,
      "grad_norm": 10.551506042480469,
      "learning_rate": 4.0776192133658197e-05,
      "loss": 2.1056,
      "step": 286200
    },
    {
      "epoch": 11.072436864292069,
      "grad_norm": 11.725444793701172,
      "learning_rate": 4.077296927975661e-05,
      "loss": 2.0853,
      "step": 286300
    },
    {
      "epoch": 11.076304288973972,
      "grad_norm": 13.235605239868164,
      "learning_rate": 4.0769746425855026e-05,
      "loss": 2.0604,
      "step": 286400
    },
    {
      "epoch": 11.080171713655876,
      "grad_norm": 10.855239868164062,
      "learning_rate": 4.0766523571953434e-05,
      "loss": 2.1292,
      "step": 286500
    },
    {
      "epoch": 11.084039138337781,
      "grad_norm": 12.134340286254883,
      "learning_rate": 4.076330071805185e-05,
      "loss": 2.0788,
      "step": 286600
    },
    {
      "epoch": 11.087906563019684,
      "grad_norm": 12.18571662902832,
      "learning_rate": 4.0760077864150263e-05,
      "loss": 2.0549,
      "step": 286700
    },
    {
      "epoch": 11.09177398770159,
      "grad_norm": 10.888751029968262,
      "learning_rate": 4.075685501024868e-05,
      "loss": 2.0912,
      "step": 286800
    },
    {
      "epoch": 11.095641412383493,
      "grad_norm": 12.915510177612305,
      "learning_rate": 4.0753632156347086e-05,
      "loss": 2.0906,
      "step": 286900
    },
    {
      "epoch": 11.099508837065398,
      "grad_norm": 12.273902893066406,
      "learning_rate": 4.07504093024455e-05,
      "loss": 2.1297,
      "step": 287000
    },
    {
      "epoch": 11.103376261747302,
      "grad_norm": 10.737882614135742,
      "learning_rate": 4.0747186448543916e-05,
      "loss": 2.0515,
      "step": 287100
    },
    {
      "epoch": 11.107243686429207,
      "grad_norm": 12.15967845916748,
      "learning_rate": 4.074396359464233e-05,
      "loss": 2.0492,
      "step": 287200
    },
    {
      "epoch": 11.11111111111111,
      "grad_norm": 10.938222885131836,
      "learning_rate": 4.074074074074074e-05,
      "loss": 2.0706,
      "step": 287300
    },
    {
      "epoch": 11.114978535793016,
      "grad_norm": 9.374714851379395,
      "learning_rate": 4.073751788683915e-05,
      "loss": 2.087,
      "step": 287400
    },
    {
      "epoch": 11.11884596047492,
      "grad_norm": 11.575769424438477,
      "learning_rate": 4.073429503293757e-05,
      "loss": 2.0505,
      "step": 287500
    },
    {
      "epoch": 11.122713385156825,
      "grad_norm": 11.546419143676758,
      "learning_rate": 4.073107217903598e-05,
      "loss": 2.0874,
      "step": 287600
    },
    {
      "epoch": 11.126580809838728,
      "grad_norm": 15.669450759887695,
      "learning_rate": 4.072784932513439e-05,
      "loss": 2.0448,
      "step": 287700
    },
    {
      "epoch": 11.130448234520633,
      "grad_norm": 11.220057487487793,
      "learning_rate": 4.0724626471232805e-05,
      "loss": 2.1499,
      "step": 287800
    },
    {
      "epoch": 11.134315659202537,
      "grad_norm": 13.41339111328125,
      "learning_rate": 4.072140361733122e-05,
      "loss": 2.1473,
      "step": 287900
    },
    {
      "epoch": 11.138183083884442,
      "grad_norm": 16.04039764404297,
      "learning_rate": 4.0718180763429634e-05,
      "loss": 1.9932,
      "step": 288000
    },
    {
      "epoch": 11.142050508566346,
      "grad_norm": 12.317010879516602,
      "learning_rate": 4.071495790952804e-05,
      "loss": 2.1279,
      "step": 288100
    },
    {
      "epoch": 11.14591793324825,
      "grad_norm": 9.902517318725586,
      "learning_rate": 4.071173505562646e-05,
      "loss": 2.1345,
      "step": 288200
    },
    {
      "epoch": 11.149785357930154,
      "grad_norm": 12.441511154174805,
      "learning_rate": 4.070851220172487e-05,
      "loss": 2.0972,
      "step": 288300
    },
    {
      "epoch": 11.153652782612058,
      "grad_norm": 11.6202392578125,
      "learning_rate": 4.0705289347823287e-05,
      "loss": 2.0915,
      "step": 288400
    },
    {
      "epoch": 11.157520207293963,
      "grad_norm": 11.736432075500488,
      "learning_rate": 4.0702066493921694e-05,
      "loss": 2.1266,
      "step": 288500
    },
    {
      "epoch": 11.161387631975867,
      "grad_norm": 13.59358024597168,
      "learning_rate": 4.069884364002011e-05,
      "loss": 2.1795,
      "step": 288600
    },
    {
      "epoch": 11.165255056657772,
      "grad_norm": 11.702909469604492,
      "learning_rate": 4.0695620786118524e-05,
      "loss": 2.1875,
      "step": 288700
    },
    {
      "epoch": 11.169122481339675,
      "grad_norm": 12.551587104797363,
      "learning_rate": 4.069239793221694e-05,
      "loss": 2.0927,
      "step": 288800
    },
    {
      "epoch": 11.17298990602158,
      "grad_norm": 16.462448120117188,
      "learning_rate": 4.068917507831535e-05,
      "loss": 2.0017,
      "step": 288900
    },
    {
      "epoch": 11.176857330703484,
      "grad_norm": 11.28924560546875,
      "learning_rate": 4.068595222441377e-05,
      "loss": 2.0723,
      "step": 289000
    },
    {
      "epoch": 11.18072475538539,
      "grad_norm": 10.62346363067627,
      "learning_rate": 4.0682729370512176e-05,
      "loss": 2.1811,
      "step": 289100
    },
    {
      "epoch": 11.184592180067293,
      "grad_norm": 11.903728485107422,
      "learning_rate": 4.067950651661059e-05,
      "loss": 2.2206,
      "step": 289200
    },
    {
      "epoch": 11.188459604749198,
      "grad_norm": 14.162673950195312,
      "learning_rate": 4.0676283662709005e-05,
      "loss": 2.1851,
      "step": 289300
    },
    {
      "epoch": 11.192327029431102,
      "grad_norm": 12.079734802246094,
      "learning_rate": 4.067306080880742e-05,
      "loss": 2.1058,
      "step": 289400
    },
    {
      "epoch": 11.196194454113007,
      "grad_norm": 13.930520057678223,
      "learning_rate": 4.0669837954905835e-05,
      "loss": 2.0588,
      "step": 289500
    },
    {
      "epoch": 11.20006187879491,
      "grad_norm": 13.015356063842773,
      "learning_rate": 4.066661510100424e-05,
      "loss": 2.1416,
      "step": 289600
    },
    {
      "epoch": 11.203929303476814,
      "grad_norm": 10.014717102050781,
      "learning_rate": 4.066339224710266e-05,
      "loss": 2.104,
      "step": 289700
    },
    {
      "epoch": 11.20779672815872,
      "grad_norm": 12.43544864654541,
      "learning_rate": 4.066016939320107e-05,
      "loss": 2.0604,
      "step": 289800
    },
    {
      "epoch": 11.211664152840623,
      "grad_norm": 10.848616600036621,
      "learning_rate": 4.065694653929949e-05,
      "loss": 2.1895,
      "step": 289900
    },
    {
      "epoch": 11.215531577522528,
      "grad_norm": 10.60894775390625,
      "learning_rate": 4.0653723685397895e-05,
      "loss": 2.0812,
      "step": 290000
    },
    {
      "epoch": 11.219399002204431,
      "grad_norm": 11.906438827514648,
      "learning_rate": 4.065050083149631e-05,
      "loss": 2.2276,
      "step": 290100
    },
    {
      "epoch": 11.223266426886337,
      "grad_norm": 11.647512435913086,
      "learning_rate": 4.0647277977594724e-05,
      "loss": 2.0121,
      "step": 290200
    },
    {
      "epoch": 11.22713385156824,
      "grad_norm": 11.59810733795166,
      "learning_rate": 4.064405512369314e-05,
      "loss": 2.142,
      "step": 290300
    },
    {
      "epoch": 11.231001276250145,
      "grad_norm": 9.370848655700684,
      "learning_rate": 4.064083226979155e-05,
      "loss": 2.2602,
      "step": 290400
    },
    {
      "epoch": 11.234868700932049,
      "grad_norm": 9.19599437713623,
      "learning_rate": 4.063760941588996e-05,
      "loss": 2.1335,
      "step": 290500
    },
    {
      "epoch": 11.238736125613954,
      "grad_norm": 12.633942604064941,
      "learning_rate": 4.0634386561988376e-05,
      "loss": 2.0158,
      "step": 290600
    },
    {
      "epoch": 11.242603550295858,
      "grad_norm": 8.479267120361328,
      "learning_rate": 4.063116370808679e-05,
      "loss": 2.2279,
      "step": 290700
    },
    {
      "epoch": 11.246470974977763,
      "grad_norm": 12.756328582763672,
      "learning_rate": 4.06279408541852e-05,
      "loss": 2.0856,
      "step": 290800
    },
    {
      "epoch": 11.250338399659666,
      "grad_norm": 11.74864673614502,
      "learning_rate": 4.0624718000283614e-05,
      "loss": 2.1854,
      "step": 290900
    },
    {
      "epoch": 11.254205824341572,
      "grad_norm": 11.144379615783691,
      "learning_rate": 4.062149514638203e-05,
      "loss": 2.0871,
      "step": 291000
    },
    {
      "epoch": 11.258073249023475,
      "grad_norm": 8.148275375366211,
      "learning_rate": 4.061827229248044e-05,
      "loss": 1.9435,
      "step": 291100
    },
    {
      "epoch": 11.26194067370538,
      "grad_norm": 10.74868106842041,
      "learning_rate": 4.061504943857885e-05,
      "loss": 2.0848,
      "step": 291200
    },
    {
      "epoch": 11.265808098387284,
      "grad_norm": 14.969893455505371,
      "learning_rate": 4.0611826584677266e-05,
      "loss": 2.1455,
      "step": 291300
    },
    {
      "epoch": 11.269675523069187,
      "grad_norm": 14.125213623046875,
      "learning_rate": 4.060860373077568e-05,
      "loss": 2.089,
      "step": 291400
    },
    {
      "epoch": 11.273542947751093,
      "grad_norm": 13.436948776245117,
      "learning_rate": 4.0605380876874095e-05,
      "loss": 2.1107,
      "step": 291500
    },
    {
      "epoch": 11.277410372432996,
      "grad_norm": 8.79018497467041,
      "learning_rate": 4.06021580229725e-05,
      "loss": 2.0806,
      "step": 291600
    },
    {
      "epoch": 11.281277797114901,
      "grad_norm": 15.016900062561035,
      "learning_rate": 4.059893516907092e-05,
      "loss": 2.1883,
      "step": 291700
    },
    {
      "epoch": 11.285145221796805,
      "grad_norm": 8.806093215942383,
      "learning_rate": 4.059571231516933e-05,
      "loss": 2.1288,
      "step": 291800
    },
    {
      "epoch": 11.28901264647871,
      "grad_norm": 12.616961479187012,
      "learning_rate": 4.059248946126775e-05,
      "loss": 2.0797,
      "step": 291900
    },
    {
      "epoch": 11.292880071160614,
      "grad_norm": 14.394430160522461,
      "learning_rate": 4.0589266607366155e-05,
      "loss": 2.2218,
      "step": 292000
    },
    {
      "epoch": 11.296747495842519,
      "grad_norm": 12.511679649353027,
      "learning_rate": 4.058604375346457e-05,
      "loss": 2.1408,
      "step": 292100
    },
    {
      "epoch": 11.300614920524422,
      "grad_norm": 15.759597778320312,
      "learning_rate": 4.0582820899562985e-05,
      "loss": 2.1136,
      "step": 292200
    },
    {
      "epoch": 11.304482345206328,
      "grad_norm": 14.952218055725098,
      "learning_rate": 4.057959804566139e-05,
      "loss": 2.0785,
      "step": 292300
    },
    {
      "epoch": 11.308349769888231,
      "grad_norm": 14.749871253967285,
      "learning_rate": 4.057637519175981e-05,
      "loss": 2.1521,
      "step": 292400
    },
    {
      "epoch": 11.312217194570136,
      "grad_norm": 13.679956436157227,
      "learning_rate": 4.057315233785822e-05,
      "loss": 2.1269,
      "step": 292500
    },
    {
      "epoch": 11.31608461925204,
      "grad_norm": 8.406454086303711,
      "learning_rate": 4.056992948395664e-05,
      "loss": 2.1131,
      "step": 292600
    },
    {
      "epoch": 11.319952043933945,
      "grad_norm": 18.723230361938477,
      "learning_rate": 4.0566706630055045e-05,
      "loss": 2.1299,
      "step": 292700
    },
    {
      "epoch": 11.323819468615849,
      "grad_norm": 13.007562637329102,
      "learning_rate": 4.056348377615346e-05,
      "loss": 2.1423,
      "step": 292800
    },
    {
      "epoch": 11.327686893297752,
      "grad_norm": 10.952630043029785,
      "learning_rate": 4.0560260922251874e-05,
      "loss": 2.1294,
      "step": 292900
    },
    {
      "epoch": 11.331554317979657,
      "grad_norm": 13.134221076965332,
      "learning_rate": 4.055703806835029e-05,
      "loss": 1.9747,
      "step": 293000
    },
    {
      "epoch": 11.33542174266156,
      "grad_norm": 12.100725173950195,
      "learning_rate": 4.05538152144487e-05,
      "loss": 2.1403,
      "step": 293100
    },
    {
      "epoch": 11.339289167343466,
      "grad_norm": 10.859110832214355,
      "learning_rate": 4.055059236054711e-05,
      "loss": 2.1623,
      "step": 293200
    },
    {
      "epoch": 11.34315659202537,
      "grad_norm": 9.74417781829834,
      "learning_rate": 4.0547369506645526e-05,
      "loss": 2.0461,
      "step": 293300
    },
    {
      "epoch": 11.347024016707275,
      "grad_norm": 15.57680606842041,
      "learning_rate": 4.054414665274394e-05,
      "loss": 2.1151,
      "step": 293400
    },
    {
      "epoch": 11.350891441389178,
      "grad_norm": 8.35233211517334,
      "learning_rate": 4.054092379884235e-05,
      "loss": 2.008,
      "step": 293500
    },
    {
      "epoch": 11.354758866071084,
      "grad_norm": 12.464457511901855,
      "learning_rate": 4.0537700944940764e-05,
      "loss": 2.0882,
      "step": 293600
    },
    {
      "epoch": 11.358626290752987,
      "grad_norm": 13.140220642089844,
      "learning_rate": 4.053447809103918e-05,
      "loss": 2.2107,
      "step": 293700
    },
    {
      "epoch": 11.362493715434892,
      "grad_norm": 10.591526985168457,
      "learning_rate": 4.053125523713759e-05,
      "loss": 2.1595,
      "step": 293800
    },
    {
      "epoch": 11.366361140116796,
      "grad_norm": 13.083678245544434,
      "learning_rate": 4.0528032383236e-05,
      "loss": 2.067,
      "step": 293900
    },
    {
      "epoch": 11.370228564798701,
      "grad_norm": 14.876919746398926,
      "learning_rate": 4.0524809529334416e-05,
      "loss": 1.9829,
      "step": 294000
    },
    {
      "epoch": 11.374095989480605,
      "grad_norm": 15.052709579467773,
      "learning_rate": 4.052158667543283e-05,
      "loss": 2.0664,
      "step": 294100
    },
    {
      "epoch": 11.37796341416251,
      "grad_norm": 10.173781394958496,
      "learning_rate": 4.0518363821531245e-05,
      "loss": 2.0896,
      "step": 294200
    },
    {
      "epoch": 11.381830838844413,
      "grad_norm": 10.695958137512207,
      "learning_rate": 4.051514096762965e-05,
      "loss": 2.1043,
      "step": 294300
    },
    {
      "epoch": 11.385698263526319,
      "grad_norm": 11.109967231750488,
      "learning_rate": 4.051191811372807e-05,
      "loss": 2.0164,
      "step": 294400
    },
    {
      "epoch": 11.389565688208222,
      "grad_norm": 11.657693862915039,
      "learning_rate": 4.050869525982648e-05,
      "loss": 2.1263,
      "step": 294500
    },
    {
      "epoch": 11.393433112890126,
      "grad_norm": 10.896066665649414,
      "learning_rate": 4.05054724059249e-05,
      "loss": 2.1667,
      "step": 294600
    },
    {
      "epoch": 11.39730053757203,
      "grad_norm": 11.62204360961914,
      "learning_rate": 4.0502249552023305e-05,
      "loss": 2.1073,
      "step": 294700
    },
    {
      "epoch": 11.401167962253934,
      "grad_norm": 12.442508697509766,
      "learning_rate": 4.049902669812172e-05,
      "loss": 2.1819,
      "step": 294800
    },
    {
      "epoch": 11.40503538693584,
      "grad_norm": 10.528362274169922,
      "learning_rate": 4.0495803844220135e-05,
      "loss": 2.1027,
      "step": 294900
    },
    {
      "epoch": 11.408902811617743,
      "grad_norm": 11.13593864440918,
      "learning_rate": 4.049258099031855e-05,
      "loss": 2.0368,
      "step": 295000
    },
    {
      "epoch": 11.412770236299648,
      "grad_norm": 11.783008575439453,
      "learning_rate": 4.048935813641696e-05,
      "loss": 2.1163,
      "step": 295100
    },
    {
      "epoch": 11.416637660981552,
      "grad_norm": 14.038322448730469,
      "learning_rate": 4.048613528251537e-05,
      "loss": 2.1433,
      "step": 295200
    },
    {
      "epoch": 11.420505085663457,
      "grad_norm": 9.454571723937988,
      "learning_rate": 4.048291242861379e-05,
      "loss": 2.1884,
      "step": 295300
    },
    {
      "epoch": 11.42437251034536,
      "grad_norm": 8.382988929748535,
      "learning_rate": 4.04796895747122e-05,
      "loss": 2.127,
      "step": 295400
    },
    {
      "epoch": 11.428239935027266,
      "grad_norm": 18.203306198120117,
      "learning_rate": 4.0476466720810616e-05,
      "loss": 2.1255,
      "step": 295500
    },
    {
      "epoch": 11.43210735970917,
      "grad_norm": 14.500500679016113,
      "learning_rate": 4.0473243866909024e-05,
      "loss": 2.1395,
      "step": 295600
    },
    {
      "epoch": 11.435974784391075,
      "grad_norm": 13.219321250915527,
      "learning_rate": 4.047002101300744e-05,
      "loss": 2.1388,
      "step": 295700
    },
    {
      "epoch": 11.439842209072978,
      "grad_norm": 10.715810775756836,
      "learning_rate": 4.0466798159105854e-05,
      "loss": 2.0759,
      "step": 295800
    },
    {
      "epoch": 11.443709633754883,
      "grad_norm": 12.207011222839355,
      "learning_rate": 4.046357530520427e-05,
      "loss": 2.1665,
      "step": 295900
    },
    {
      "epoch": 11.447577058436787,
      "grad_norm": 15.299969673156738,
      "learning_rate": 4.046035245130268e-05,
      "loss": 2.0525,
      "step": 296000
    },
    {
      "epoch": 11.451444483118692,
      "grad_norm": 12.364799499511719,
      "learning_rate": 4.045712959740109e-05,
      "loss": 2.1422,
      "step": 296100
    },
    {
      "epoch": 11.455311907800596,
      "grad_norm": 11.754706382751465,
      "learning_rate": 4.0453906743499506e-05,
      "loss": 2.2166,
      "step": 296200
    },
    {
      "epoch": 11.459179332482499,
      "grad_norm": 12.337715148925781,
      "learning_rate": 4.045068388959792e-05,
      "loss": 2.0864,
      "step": 296300
    },
    {
      "epoch": 11.463046757164404,
      "grad_norm": 13.727683067321777,
      "learning_rate": 4.0447461035696335e-05,
      "loss": 2.1504,
      "step": 296400
    },
    {
      "epoch": 11.466914181846308,
      "grad_norm": 10.79127311706543,
      "learning_rate": 4.044423818179475e-05,
      "loss": 2.0723,
      "step": 296500
    },
    {
      "epoch": 11.470781606528213,
      "grad_norm": 11.138289451599121,
      "learning_rate": 4.044101532789316e-05,
      "loss": 2.1436,
      "step": 296600
    },
    {
      "epoch": 11.474649031210117,
      "grad_norm": 9.611061096191406,
      "learning_rate": 4.043779247399157e-05,
      "loss": 2.0794,
      "step": 296700
    },
    {
      "epoch": 11.478516455892022,
      "grad_norm": 11.651800155639648,
      "learning_rate": 4.043456962008999e-05,
      "loss": 2.1343,
      "step": 296800
    },
    {
      "epoch": 11.482383880573925,
      "grad_norm": 11.128196716308594,
      "learning_rate": 4.04313467661884e-05,
      "loss": 2.1932,
      "step": 296900
    },
    {
      "epoch": 11.48625130525583,
      "grad_norm": 10.232702255249023,
      "learning_rate": 4.042812391228681e-05,
      "loss": 2.1052,
      "step": 297000
    },
    {
      "epoch": 11.490118729937734,
      "grad_norm": 10.851995468139648,
      "learning_rate": 4.0424901058385225e-05,
      "loss": 2.1193,
      "step": 297100
    },
    {
      "epoch": 11.49398615461964,
      "grad_norm": 11.916803359985352,
      "learning_rate": 4.042167820448364e-05,
      "loss": 2.0796,
      "step": 297200
    },
    {
      "epoch": 11.497853579301543,
      "grad_norm": 9.897552490234375,
      "learning_rate": 4.0418455350582054e-05,
      "loss": 2.0847,
      "step": 297300
    },
    {
      "epoch": 11.501721003983448,
      "grad_norm": 11.81787109375,
      "learning_rate": 4.041523249668046e-05,
      "loss": 2.0749,
      "step": 297400
    },
    {
      "epoch": 11.505588428665352,
      "grad_norm": 14.515625,
      "learning_rate": 4.041200964277888e-05,
      "loss": 2.1236,
      "step": 297500
    },
    {
      "epoch": 11.509455853347257,
      "grad_norm": 11.465917587280273,
      "learning_rate": 4.040878678887729e-05,
      "loss": 2.0984,
      "step": 297600
    },
    {
      "epoch": 11.51332327802916,
      "grad_norm": 13.720829010009766,
      "learning_rate": 4.0405563934975706e-05,
      "loss": 2.0983,
      "step": 297700
    },
    {
      "epoch": 11.517190702711066,
      "grad_norm": 13.359101295471191,
      "learning_rate": 4.0402341081074114e-05,
      "loss": 2.0994,
      "step": 297800
    },
    {
      "epoch": 11.521058127392969,
      "grad_norm": 13.35849666595459,
      "learning_rate": 4.039911822717253e-05,
      "loss": 2.1413,
      "step": 297900
    },
    {
      "epoch": 11.524925552074873,
      "grad_norm": 12.349973678588867,
      "learning_rate": 4.0395895373270944e-05,
      "loss": 2.1438,
      "step": 298000
    },
    {
      "epoch": 11.528792976756778,
      "grad_norm": 12.93005084991455,
      "learning_rate": 4.039267251936935e-05,
      "loss": 1.9666,
      "step": 298100
    },
    {
      "epoch": 11.532660401438681,
      "grad_norm": 11.191129684448242,
      "learning_rate": 4.0389449665467766e-05,
      "loss": 2.1941,
      "step": 298200
    },
    {
      "epoch": 11.536527826120587,
      "grad_norm": 12.520140647888184,
      "learning_rate": 4.038622681156618e-05,
      "loss": 2.0737,
      "step": 298300
    },
    {
      "epoch": 11.54039525080249,
      "grad_norm": 12.225163459777832,
      "learning_rate": 4.0383003957664596e-05,
      "loss": 2.0219,
      "step": 298400
    },
    {
      "epoch": 11.544262675484395,
      "grad_norm": 13.502944946289062,
      "learning_rate": 4.0379781103763004e-05,
      "loss": 2.0624,
      "step": 298500
    },
    {
      "epoch": 11.548130100166299,
      "grad_norm": 13.022411346435547,
      "learning_rate": 4.037655824986142e-05,
      "loss": 2.1119,
      "step": 298600
    },
    {
      "epoch": 11.551997524848204,
      "grad_norm": 11.362439155578613,
      "learning_rate": 4.037333539595983e-05,
      "loss": 2.112,
      "step": 298700
    },
    {
      "epoch": 11.555864949530108,
      "grad_norm": 12.872383117675781,
      "learning_rate": 4.037011254205825e-05,
      "loss": 2.0929,
      "step": 298800
    },
    {
      "epoch": 11.559732374212013,
      "grad_norm": 20.750898361206055,
      "learning_rate": 4.0366889688156656e-05,
      "loss": 2.0638,
      "step": 298900
    },
    {
      "epoch": 11.563599798893916,
      "grad_norm": 9.658251762390137,
      "learning_rate": 4.036366683425507e-05,
      "loss": 2.1257,
      "step": 299000
    },
    {
      "epoch": 11.567467223575822,
      "grad_norm": 13.578656196594238,
      "learning_rate": 4.0360443980353485e-05,
      "loss": 2.0449,
      "step": 299100
    },
    {
      "epoch": 11.571334648257725,
      "grad_norm": 16.84461212158203,
      "learning_rate": 4.03572211264519e-05,
      "loss": 2.0552,
      "step": 299200
    },
    {
      "epoch": 11.57520207293963,
      "grad_norm": 10.860722541809082,
      "learning_rate": 4.035399827255031e-05,
      "loss": 2.1604,
      "step": 299300
    },
    {
      "epoch": 11.579069497621534,
      "grad_norm": 13.64693832397461,
      "learning_rate": 4.035077541864872e-05,
      "loss": 2.1279,
      "step": 299400
    },
    {
      "epoch": 11.582936922303439,
      "grad_norm": 17.294618606567383,
      "learning_rate": 4.034755256474714e-05,
      "loss": 2.1744,
      "step": 299500
    },
    {
      "epoch": 11.586804346985343,
      "grad_norm": 10.234159469604492,
      "learning_rate": 4.034432971084555e-05,
      "loss": 2.2834,
      "step": 299600
    },
    {
      "epoch": 11.590671771667246,
      "grad_norm": 13.69837760925293,
      "learning_rate": 4.034110685694396e-05,
      "loss": 2.0162,
      "step": 299700
    },
    {
      "epoch": 11.594539196349151,
      "grad_norm": 8.654034614562988,
      "learning_rate": 4.0337884003042375e-05,
      "loss": 2.1547,
      "step": 299800
    },
    {
      "epoch": 11.598406621031055,
      "grad_norm": 10.482900619506836,
      "learning_rate": 4.033466114914079e-05,
      "loss": 2.1856,
      "step": 299900
    },
    {
      "epoch": 11.60227404571296,
      "grad_norm": 12.116975784301758,
      "learning_rate": 4.0331438295239204e-05,
      "loss": 2.0636,
      "step": 300000
    },
    {
      "epoch": 11.606141470394864,
      "grad_norm": 10.691466331481934,
      "learning_rate": 4.032821544133761e-05,
      "loss": 2.1388,
      "step": 300100
    },
    {
      "epoch": 11.610008895076769,
      "grad_norm": 13.793075561523438,
      "learning_rate": 4.032499258743603e-05,
      "loss": 2.1027,
      "step": 300200
    },
    {
      "epoch": 11.613876319758672,
      "grad_norm": 10.777359962463379,
      "learning_rate": 4.032176973353444e-05,
      "loss": 2.0842,
      "step": 300300
    },
    {
      "epoch": 11.617743744440578,
      "grad_norm": 9.490922927856445,
      "learning_rate": 4.0318546879632856e-05,
      "loss": 2.152,
      "step": 300400
    },
    {
      "epoch": 11.621611169122481,
      "grad_norm": 11.156482696533203,
      "learning_rate": 4.0315324025731264e-05,
      "loss": 2.1301,
      "step": 300500
    },
    {
      "epoch": 11.625478593804386,
      "grad_norm": 11.193440437316895,
      "learning_rate": 4.031210117182968e-05,
      "loss": 2.128,
      "step": 300600
    },
    {
      "epoch": 11.62934601848629,
      "grad_norm": 13.167259216308594,
      "learning_rate": 4.0308878317928094e-05,
      "loss": 2.1156,
      "step": 300700
    },
    {
      "epoch": 11.633213443168195,
      "grad_norm": 7.691091537475586,
      "learning_rate": 4.030565546402651e-05,
      "loss": 2.1071,
      "step": 300800
    },
    {
      "epoch": 11.637080867850099,
      "grad_norm": 13.234272003173828,
      "learning_rate": 4.0302432610124916e-05,
      "loss": 2.0121,
      "step": 300900
    },
    {
      "epoch": 11.640948292532002,
      "grad_norm": 11.447659492492676,
      "learning_rate": 4.029920975622333e-05,
      "loss": 2.1574,
      "step": 301000
    },
    {
      "epoch": 11.644815717213907,
      "grad_norm": 12.306154251098633,
      "learning_rate": 4.0295986902321746e-05,
      "loss": 2.0588,
      "step": 301100
    },
    {
      "epoch": 11.64868314189581,
      "grad_norm": 8.36308765411377,
      "learning_rate": 4.0292764048420154e-05,
      "loss": 2.0515,
      "step": 301200
    },
    {
      "epoch": 11.652550566577716,
      "grad_norm": 9.797197341918945,
      "learning_rate": 4.028954119451857e-05,
      "loss": 2.0867,
      "step": 301300
    },
    {
      "epoch": 11.65641799125962,
      "grad_norm": 10.058897972106934,
      "learning_rate": 4.028631834061698e-05,
      "loss": 2.0535,
      "step": 301400
    },
    {
      "epoch": 11.660285415941525,
      "grad_norm": 10.144084930419922,
      "learning_rate": 4.02830954867154e-05,
      "loss": 2.0459,
      "step": 301500
    },
    {
      "epoch": 11.664152840623428,
      "grad_norm": 10.845746994018555,
      "learning_rate": 4.0279872632813806e-05,
      "loss": 2.1349,
      "step": 301600
    },
    {
      "epoch": 11.668020265305334,
      "grad_norm": 14.864258766174316,
      "learning_rate": 4.027664977891222e-05,
      "loss": 2.1951,
      "step": 301700
    },
    {
      "epoch": 11.671887689987237,
      "grad_norm": 9.676774978637695,
      "learning_rate": 4.0273426925010635e-05,
      "loss": 2.0761,
      "step": 301800
    },
    {
      "epoch": 11.675755114669142,
      "grad_norm": 9.749902725219727,
      "learning_rate": 4.027020407110905e-05,
      "loss": 2.0929,
      "step": 301900
    },
    {
      "epoch": 11.679622539351046,
      "grad_norm": 12.096623420715332,
      "learning_rate": 4.026698121720746e-05,
      "loss": 2.0543,
      "step": 302000
    },
    {
      "epoch": 11.683489964032951,
      "grad_norm": 9.956660270690918,
      "learning_rate": 4.026375836330587e-05,
      "loss": 2.2061,
      "step": 302100
    },
    {
      "epoch": 11.687357388714855,
      "grad_norm": 10.820995330810547,
      "learning_rate": 4.026053550940429e-05,
      "loss": 2.0591,
      "step": 302200
    },
    {
      "epoch": 11.69122481339676,
      "grad_norm": 13.155174255371094,
      "learning_rate": 4.02573126555027e-05,
      "loss": 2.1529,
      "step": 302300
    },
    {
      "epoch": 11.695092238078663,
      "grad_norm": 13.865230560302734,
      "learning_rate": 4.025408980160112e-05,
      "loss": 2.0919,
      "step": 302400
    },
    {
      "epoch": 11.698959662760569,
      "grad_norm": 12.342445373535156,
      "learning_rate": 4.025086694769953e-05,
      "loss": 2.1408,
      "step": 302500
    },
    {
      "epoch": 11.702827087442472,
      "grad_norm": 10.950785636901855,
      "learning_rate": 4.024764409379794e-05,
      "loss": 2.0604,
      "step": 302600
    },
    {
      "epoch": 11.706694512124376,
      "grad_norm": 12.563053131103516,
      "learning_rate": 4.0244421239896354e-05,
      "loss": 2.0717,
      "step": 302700
    },
    {
      "epoch": 11.71056193680628,
      "grad_norm": 12.462141990661621,
      "learning_rate": 4.024119838599477e-05,
      "loss": 2.2322,
      "step": 302800
    },
    {
      "epoch": 11.714429361488184,
      "grad_norm": 10.9345064163208,
      "learning_rate": 4.0237975532093184e-05,
      "loss": 2.1132,
      "step": 302900
    },
    {
      "epoch": 11.71829678617009,
      "grad_norm": 9.062417030334473,
      "learning_rate": 4.02347526781916e-05,
      "loss": 2.1091,
      "step": 303000
    },
    {
      "epoch": 11.722164210851993,
      "grad_norm": 8.217700004577637,
      "learning_rate": 4.023152982429001e-05,
      "loss": 2.0896,
      "step": 303100
    },
    {
      "epoch": 11.726031635533898,
      "grad_norm": 9.961796760559082,
      "learning_rate": 4.022830697038842e-05,
      "loss": 2.1198,
      "step": 303200
    },
    {
      "epoch": 11.729899060215802,
      "grad_norm": 11.63864517211914,
      "learning_rate": 4.0225084116486836e-05,
      "loss": 2.1533,
      "step": 303300
    },
    {
      "epoch": 11.733766484897707,
      "grad_norm": 10.688409805297852,
      "learning_rate": 4.022186126258525e-05,
      "loss": 2.1608,
      "step": 303400
    },
    {
      "epoch": 11.73763390957961,
      "grad_norm": 14.867554664611816,
      "learning_rate": 4.0218638408683665e-05,
      "loss": 2.1018,
      "step": 303500
    },
    {
      "epoch": 11.741501334261516,
      "grad_norm": 14.679511070251465,
      "learning_rate": 4.021541555478207e-05,
      "loss": 2.121,
      "step": 303600
    },
    {
      "epoch": 11.74536875894342,
      "grad_norm": 11.913061141967773,
      "learning_rate": 4.021219270088049e-05,
      "loss": 2.0268,
      "step": 303700
    },
    {
      "epoch": 11.749236183625325,
      "grad_norm": 16.206375122070312,
      "learning_rate": 4.02089698469789e-05,
      "loss": 2.1301,
      "step": 303800
    },
    {
      "epoch": 11.753103608307228,
      "grad_norm": 16.395172119140625,
      "learning_rate": 4.020574699307732e-05,
      "loss": 2.0743,
      "step": 303900
    },
    {
      "epoch": 11.756971032989133,
      "grad_norm": 9.781134605407715,
      "learning_rate": 4.0202524139175725e-05,
      "loss": 2.1556,
      "step": 304000
    },
    {
      "epoch": 11.760838457671037,
      "grad_norm": 12.449857711791992,
      "learning_rate": 4.019930128527414e-05,
      "loss": 2.0806,
      "step": 304100
    },
    {
      "epoch": 11.764705882352942,
      "grad_norm": 10.090278625488281,
      "learning_rate": 4.0196078431372555e-05,
      "loss": 2.0286,
      "step": 304200
    },
    {
      "epoch": 11.768573307034845,
      "grad_norm": 11.087419509887695,
      "learning_rate": 4.019285557747096e-05,
      "loss": 2.1728,
      "step": 304300
    },
    {
      "epoch": 11.772440731716749,
      "grad_norm": 13.084665298461914,
      "learning_rate": 4.018963272356938e-05,
      "loss": 2.0353,
      "step": 304400
    },
    {
      "epoch": 11.776308156398654,
      "grad_norm": 12.872284889221191,
      "learning_rate": 4.018640986966779e-05,
      "loss": 2.172,
      "step": 304500
    },
    {
      "epoch": 11.780175581080558,
      "grad_norm": 10.857398986816406,
      "learning_rate": 4.018318701576621e-05,
      "loss": 2.1273,
      "step": 304600
    },
    {
      "epoch": 11.784043005762463,
      "grad_norm": 13.93586254119873,
      "learning_rate": 4.0179964161864615e-05,
      "loss": 2.0627,
      "step": 304700
    },
    {
      "epoch": 11.787910430444366,
      "grad_norm": 12.677023887634277,
      "learning_rate": 4.017674130796303e-05,
      "loss": 2.1367,
      "step": 304800
    },
    {
      "epoch": 11.791777855126272,
      "grad_norm": 13.96011734008789,
      "learning_rate": 4.0173518454061444e-05,
      "loss": 2.1362,
      "step": 304900
    },
    {
      "epoch": 11.795645279808175,
      "grad_norm": 15.544694900512695,
      "learning_rate": 4.017029560015986e-05,
      "loss": 2.0758,
      "step": 305000
    },
    {
      "epoch": 11.79951270449008,
      "grad_norm": 11.902262687683105,
      "learning_rate": 4.016707274625827e-05,
      "loss": 2.1047,
      "step": 305100
    },
    {
      "epoch": 11.803380129171984,
      "grad_norm": 13.399958610534668,
      "learning_rate": 4.016384989235668e-05,
      "loss": 2.1088,
      "step": 305200
    },
    {
      "epoch": 11.80724755385389,
      "grad_norm": 12.625367164611816,
      "learning_rate": 4.0160627038455096e-05,
      "loss": 2.1289,
      "step": 305300
    },
    {
      "epoch": 11.811114978535793,
      "grad_norm": 12.216635704040527,
      "learning_rate": 4.015740418455351e-05,
      "loss": 2.0359,
      "step": 305400
    },
    {
      "epoch": 11.814982403217698,
      "grad_norm": 13.730365753173828,
      "learning_rate": 4.015418133065192e-05,
      "loss": 2.1777,
      "step": 305500
    },
    {
      "epoch": 11.818849827899601,
      "grad_norm": 11.962550163269043,
      "learning_rate": 4.0150958476750333e-05,
      "loss": 2.1374,
      "step": 305600
    },
    {
      "epoch": 11.822717252581507,
      "grad_norm": 9.749093055725098,
      "learning_rate": 4.014773562284875e-05,
      "loss": 1.9991,
      "step": 305700
    },
    {
      "epoch": 11.82658467726341,
      "grad_norm": 12.962641716003418,
      "learning_rate": 4.014451276894716e-05,
      "loss": 2.0949,
      "step": 305800
    },
    {
      "epoch": 11.830452101945315,
      "grad_norm": 12.135199546813965,
      "learning_rate": 4.014128991504557e-05,
      "loss": 2.0711,
      "step": 305900
    },
    {
      "epoch": 11.834319526627219,
      "grad_norm": 11.300750732421875,
      "learning_rate": 4.0138067061143986e-05,
      "loss": 2.1777,
      "step": 306000
    },
    {
      "epoch": 11.838186951309122,
      "grad_norm": 10.515043258666992,
      "learning_rate": 4.01348442072424e-05,
      "loss": 2.1578,
      "step": 306100
    },
    {
      "epoch": 11.842054375991028,
      "grad_norm": 12.40146255493164,
      "learning_rate": 4.0131621353340815e-05,
      "loss": 2.0598,
      "step": 306200
    },
    {
      "epoch": 11.845921800672931,
      "grad_norm": 13.036582946777344,
      "learning_rate": 4.012839849943922e-05,
      "loss": 2.0616,
      "step": 306300
    },
    {
      "epoch": 11.849789225354836,
      "grad_norm": 12.352746963500977,
      "learning_rate": 4.012517564553764e-05,
      "loss": 2.1121,
      "step": 306400
    },
    {
      "epoch": 11.85365665003674,
      "grad_norm": 13.108555793762207,
      "learning_rate": 4.012195279163605e-05,
      "loss": 2.2043,
      "step": 306500
    },
    {
      "epoch": 11.857524074718645,
      "grad_norm": 15.058449745178223,
      "learning_rate": 4.011872993773447e-05,
      "loss": 2.2584,
      "step": 306600
    },
    {
      "epoch": 11.861391499400549,
      "grad_norm": 11.731674194335938,
      "learning_rate": 4.0115507083832875e-05,
      "loss": 2.1055,
      "step": 306700
    },
    {
      "epoch": 11.865258924082454,
      "grad_norm": 11.845688819885254,
      "learning_rate": 4.011228422993129e-05,
      "loss": 2.0238,
      "step": 306800
    },
    {
      "epoch": 11.869126348764357,
      "grad_norm": 9.473867416381836,
      "learning_rate": 4.0109061376029704e-05,
      "loss": 2.0552,
      "step": 306900
    },
    {
      "epoch": 11.872993773446263,
      "grad_norm": 12.176071166992188,
      "learning_rate": 4.010583852212811e-05,
      "loss": 2.0933,
      "step": 307000
    },
    {
      "epoch": 11.876861198128166,
      "grad_norm": 12.684091567993164,
      "learning_rate": 4.010261566822653e-05,
      "loss": 2.087,
      "step": 307100
    },
    {
      "epoch": 11.880728622810071,
      "grad_norm": 12.129037857055664,
      "learning_rate": 4.009939281432494e-05,
      "loss": 2.1465,
      "step": 307200
    },
    {
      "epoch": 11.884596047491975,
      "grad_norm": 11.094891548156738,
      "learning_rate": 4.0096169960423357e-05,
      "loss": 2.1213,
      "step": 307300
    },
    {
      "epoch": 11.88846347217388,
      "grad_norm": 10.908685684204102,
      "learning_rate": 4.0092947106521765e-05,
      "loss": 2.1107,
      "step": 307400
    },
    {
      "epoch": 11.892330896855784,
      "grad_norm": 16.80485725402832,
      "learning_rate": 4.008972425262018e-05,
      "loss": 2.1199,
      "step": 307500
    },
    {
      "epoch": 11.896198321537689,
      "grad_norm": 13.63170051574707,
      "learning_rate": 4.0086501398718594e-05,
      "loss": 2.0984,
      "step": 307600
    },
    {
      "epoch": 11.900065746219592,
      "grad_norm": 9.745594024658203,
      "learning_rate": 4.008327854481701e-05,
      "loss": 2.1301,
      "step": 307700
    },
    {
      "epoch": 11.903933170901496,
      "grad_norm": 13.616414070129395,
      "learning_rate": 4.008005569091542e-05,
      "loss": 2.0775,
      "step": 307800
    },
    {
      "epoch": 11.907800595583401,
      "grad_norm": 9.648736953735352,
      "learning_rate": 4.007683283701383e-05,
      "loss": 2.0867,
      "step": 307900
    },
    {
      "epoch": 11.911668020265305,
      "grad_norm": 11.688173294067383,
      "learning_rate": 4.0073609983112246e-05,
      "loss": 2.1718,
      "step": 308000
    },
    {
      "epoch": 11.91553544494721,
      "grad_norm": 11.00422477722168,
      "learning_rate": 4.007038712921066e-05,
      "loss": 2.1167,
      "step": 308100
    },
    {
      "epoch": 11.919402869629113,
      "grad_norm": 11.143238067626953,
      "learning_rate": 4.006716427530907e-05,
      "loss": 2.1199,
      "step": 308200
    },
    {
      "epoch": 11.923270294311019,
      "grad_norm": 12.847315788269043,
      "learning_rate": 4.0063941421407483e-05,
      "loss": 2.1714,
      "step": 308300
    },
    {
      "epoch": 11.927137718992922,
      "grad_norm": 21.51789665222168,
      "learning_rate": 4.00607185675059e-05,
      "loss": 2.1311,
      "step": 308400
    },
    {
      "epoch": 11.931005143674827,
      "grad_norm": 13.06116008758545,
      "learning_rate": 4.005749571360431e-05,
      "loss": 2.0632,
      "step": 308500
    },
    {
      "epoch": 11.934872568356731,
      "grad_norm": 12.230607032775879,
      "learning_rate": 4.005427285970272e-05,
      "loss": 2.1397,
      "step": 308600
    },
    {
      "epoch": 11.938739993038636,
      "grad_norm": 9.581917762756348,
      "learning_rate": 4.0051050005801136e-05,
      "loss": 2.1511,
      "step": 308700
    },
    {
      "epoch": 11.94260741772054,
      "grad_norm": 9.67674446105957,
      "learning_rate": 4.004782715189955e-05,
      "loss": 2.2463,
      "step": 308800
    },
    {
      "epoch": 11.946474842402445,
      "grad_norm": 10.950787544250488,
      "learning_rate": 4.0044604297997965e-05,
      "loss": 2.1948,
      "step": 308900
    },
    {
      "epoch": 11.950342267084348,
      "grad_norm": 13.419204711914062,
      "learning_rate": 4.004138144409638e-05,
      "loss": 2.1226,
      "step": 309000
    },
    {
      "epoch": 11.954209691766252,
      "grad_norm": 13.239428520202637,
      "learning_rate": 4.003815859019479e-05,
      "loss": 2.0976,
      "step": 309100
    },
    {
      "epoch": 11.958077116448157,
      "grad_norm": 11.76866340637207,
      "learning_rate": 4.00349357362932e-05,
      "loss": 2.1108,
      "step": 309200
    },
    {
      "epoch": 11.96194454113006,
      "grad_norm": 10.39997386932373,
      "learning_rate": 4.003171288239162e-05,
      "loss": 2.0741,
      "step": 309300
    },
    {
      "epoch": 11.965811965811966,
      "grad_norm": 11.50974178314209,
      "learning_rate": 4.002849002849003e-05,
      "loss": 2.0952,
      "step": 309400
    },
    {
      "epoch": 11.96967939049387,
      "grad_norm": 12.202374458312988,
      "learning_rate": 4.0025267174588447e-05,
      "loss": 2.1065,
      "step": 309500
    },
    {
      "epoch": 11.973546815175775,
      "grad_norm": 10.80517864227295,
      "learning_rate": 4.0022044320686854e-05,
      "loss": 2.1619,
      "step": 309600
    },
    {
      "epoch": 11.977414239857678,
      "grad_norm": 12.966946601867676,
      "learning_rate": 4.001882146678527e-05,
      "loss": 2.0975,
      "step": 309700
    },
    {
      "epoch": 11.981281664539583,
      "grad_norm": 12.878053665161133,
      "learning_rate": 4.0015598612883684e-05,
      "loss": 2.1114,
      "step": 309800
    },
    {
      "epoch": 11.985149089221487,
      "grad_norm": 10.669044494628906,
      "learning_rate": 4.00123757589821e-05,
      "loss": 2.064,
      "step": 309900
    },
    {
      "epoch": 11.989016513903392,
      "grad_norm": 11.116040229797363,
      "learning_rate": 4.000915290508051e-05,
      "loss": 2.0753,
      "step": 310000
    },
    {
      "epoch": 11.992883938585296,
      "grad_norm": 8.69897174835205,
      "learning_rate": 4.000593005117892e-05,
      "loss": 2.1112,
      "step": 310100
    },
    {
      "epoch": 11.996751363267201,
      "grad_norm": 11.849800109863281,
      "learning_rate": 4.0002707197277336e-05,
      "loss": 2.0495,
      "step": 310200
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.9756526947021484,
      "eval_runtime": 2.983,
      "eval_samples_per_second": 456.249,
      "eval_steps_per_second": 456.249,
      "step": 310284
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.916762113571167,
      "eval_runtime": 55.372,
      "eval_samples_per_second": 466.969,
      "eval_steps_per_second": 466.969,
      "step": 310284
    },
    {
      "epoch": 12.000618787949104,
      "grad_norm": 11.08486270904541,
      "learning_rate": 3.999948434337575e-05,
      "loss": 2.0674,
      "step": 310300
    },
    {
      "epoch": 12.00448621263101,
      "grad_norm": 11.032096862792969,
      "learning_rate": 3.9996261489474165e-05,
      "loss": 2.0939,
      "step": 310400
    },
    {
      "epoch": 12.008353637312913,
      "grad_norm": 10.41511058807373,
      "learning_rate": 3.999303863557257e-05,
      "loss": 2.0306,
      "step": 310500
    },
    {
      "epoch": 12.012221061994818,
      "grad_norm": 11.23802661895752,
      "learning_rate": 3.998981578167099e-05,
      "loss": 2.1167,
      "step": 310600
    },
    {
      "epoch": 12.016088486676722,
      "grad_norm": 9.731145858764648,
      "learning_rate": 3.99865929277694e-05,
      "loss": 2.0605,
      "step": 310700
    },
    {
      "epoch": 12.019955911358627,
      "grad_norm": 11.940980911254883,
      "learning_rate": 3.998337007386782e-05,
      "loss": 2.1152,
      "step": 310800
    },
    {
      "epoch": 12.02382333604053,
      "grad_norm": 12.684216499328613,
      "learning_rate": 3.9980147219966225e-05,
      "loss": 2.0762,
      "step": 310900
    },
    {
      "epoch": 12.027690760722434,
      "grad_norm": 10.003880500793457,
      "learning_rate": 3.997692436606464e-05,
      "loss": 1.9742,
      "step": 311000
    },
    {
      "epoch": 12.03155818540434,
      "grad_norm": 11.145873069763184,
      "learning_rate": 3.9973701512163055e-05,
      "loss": 2.0622,
      "step": 311100
    },
    {
      "epoch": 12.035425610086243,
      "grad_norm": 12.474808692932129,
      "learning_rate": 3.997047865826147e-05,
      "loss": 2.0161,
      "step": 311200
    },
    {
      "epoch": 12.039293034768148,
      "grad_norm": 11.846702575683594,
      "learning_rate": 3.996725580435988e-05,
      "loss": 2.1594,
      "step": 311300
    },
    {
      "epoch": 12.043160459450052,
      "grad_norm": 9.32425594329834,
      "learning_rate": 3.996403295045829e-05,
      "loss": 2.1113,
      "step": 311400
    },
    {
      "epoch": 12.047027884131957,
      "grad_norm": 11.962687492370605,
      "learning_rate": 3.996081009655671e-05,
      "loss": 2.1166,
      "step": 311500
    },
    {
      "epoch": 12.05089530881386,
      "grad_norm": 10.109090805053711,
      "learning_rate": 3.995758724265512e-05,
      "loss": 2.0405,
      "step": 311600
    },
    {
      "epoch": 12.054762733495766,
      "grad_norm": 10.395160675048828,
      "learning_rate": 3.995436438875353e-05,
      "loss": 2.0907,
      "step": 311700
    },
    {
      "epoch": 12.05863015817767,
      "grad_norm": 10.588249206542969,
      "learning_rate": 3.9951141534851944e-05,
      "loss": 1.9924,
      "step": 311800
    },
    {
      "epoch": 12.062497582859574,
      "grad_norm": 13.211592674255371,
      "learning_rate": 3.994791868095036e-05,
      "loss": 2.0745,
      "step": 311900
    },
    {
      "epoch": 12.066365007541478,
      "grad_norm": 10.664931297302246,
      "learning_rate": 3.9944695827048774e-05,
      "loss": 1.9664,
      "step": 312000
    },
    {
      "epoch": 12.070232432223383,
      "grad_norm": 11.651747703552246,
      "learning_rate": 3.994147297314718e-05,
      "loss": 2.0954,
      "step": 312100
    },
    {
      "epoch": 12.074099856905287,
      "grad_norm": 12.805231094360352,
      "learning_rate": 3.9938250119245596e-05,
      "loss": 2.0544,
      "step": 312200
    },
    {
      "epoch": 12.077967281587192,
      "grad_norm": 14.699328422546387,
      "learning_rate": 3.993502726534401e-05,
      "loss": 2.0963,
      "step": 312300
    },
    {
      "epoch": 12.081834706269095,
      "grad_norm": 13.114782333374023,
      "learning_rate": 3.9931804411442426e-05,
      "loss": 2.0946,
      "step": 312400
    },
    {
      "epoch": 12.085702130950999,
      "grad_norm": 10.819397926330566,
      "learning_rate": 3.9928581557540834e-05,
      "loss": 2.0188,
      "step": 312500
    },
    {
      "epoch": 12.089569555632904,
      "grad_norm": 13.390198707580566,
      "learning_rate": 3.992535870363925e-05,
      "loss": 2.0904,
      "step": 312600
    },
    {
      "epoch": 12.093436980314808,
      "grad_norm": 11.879474639892578,
      "learning_rate": 3.992213584973766e-05,
      "loss": 2.0313,
      "step": 312700
    },
    {
      "epoch": 12.097304404996713,
      "grad_norm": 11.370153427124023,
      "learning_rate": 3.991891299583607e-05,
      "loss": 2.0527,
      "step": 312800
    },
    {
      "epoch": 12.101171829678616,
      "grad_norm": 9.49777889251709,
      "learning_rate": 3.9915690141934486e-05,
      "loss": 2.128,
      "step": 312900
    },
    {
      "epoch": 12.105039254360522,
      "grad_norm": 12.599761009216309,
      "learning_rate": 3.99124672880329e-05,
      "loss": 2.2255,
      "step": 313000
    },
    {
      "epoch": 12.108906679042425,
      "grad_norm": 10.06109619140625,
      "learning_rate": 3.9909244434131315e-05,
      "loss": 2.0932,
      "step": 313100
    },
    {
      "epoch": 12.11277410372433,
      "grad_norm": 12.870285034179688,
      "learning_rate": 3.990602158022972e-05,
      "loss": 2.1615,
      "step": 313200
    },
    {
      "epoch": 12.116641528406234,
      "grad_norm": 10.659003257751465,
      "learning_rate": 3.990279872632814e-05,
      "loss": 2.1187,
      "step": 313300
    },
    {
      "epoch": 12.12050895308814,
      "grad_norm": 11.364789962768555,
      "learning_rate": 3.989957587242655e-05,
      "loss": 2.1393,
      "step": 313400
    },
    {
      "epoch": 12.124376377770043,
      "grad_norm": 15.778013229370117,
      "learning_rate": 3.989635301852497e-05,
      "loss": 2.1113,
      "step": 313500
    },
    {
      "epoch": 12.128243802451948,
      "grad_norm": 15.285222053527832,
      "learning_rate": 3.9893130164623375e-05,
      "loss": 2.1087,
      "step": 313600
    },
    {
      "epoch": 12.132111227133851,
      "grad_norm": 9.637004852294922,
      "learning_rate": 3.988990731072179e-05,
      "loss": 2.1619,
      "step": 313700
    },
    {
      "epoch": 12.135978651815757,
      "grad_norm": 11.866507530212402,
      "learning_rate": 3.9886684456820205e-05,
      "loss": 2.0741,
      "step": 313800
    },
    {
      "epoch": 12.13984607649766,
      "grad_norm": 11.118804931640625,
      "learning_rate": 3.988346160291862e-05,
      "loss": 1.9957,
      "step": 313900
    },
    {
      "epoch": 12.143713501179565,
      "grad_norm": 10.896772384643555,
      "learning_rate": 3.988023874901703e-05,
      "loss": 2.0295,
      "step": 314000
    },
    {
      "epoch": 12.147580925861469,
      "grad_norm": 12.845324516296387,
      "learning_rate": 3.987701589511544e-05,
      "loss": 2.0703,
      "step": 314100
    },
    {
      "epoch": 12.151448350543372,
      "grad_norm": 12.151299476623535,
      "learning_rate": 3.987379304121386e-05,
      "loss": 2.0705,
      "step": 314200
    },
    {
      "epoch": 12.155315775225278,
      "grad_norm": 10.303585052490234,
      "learning_rate": 3.987057018731227e-05,
      "loss": 2.0797,
      "step": 314300
    },
    {
      "epoch": 12.159183199907181,
      "grad_norm": 13.676562309265137,
      "learning_rate": 3.986734733341068e-05,
      "loss": 2.1505,
      "step": 314400
    },
    {
      "epoch": 12.163050624589086,
      "grad_norm": 10.882390022277832,
      "learning_rate": 3.9864124479509094e-05,
      "loss": 2.0044,
      "step": 314500
    },
    {
      "epoch": 12.16691804927099,
      "grad_norm": 14.814681053161621,
      "learning_rate": 3.986090162560751e-05,
      "loss": 2.0765,
      "step": 314600
    },
    {
      "epoch": 12.170785473952895,
      "grad_norm": 11.632279396057129,
      "learning_rate": 3.9857678771705924e-05,
      "loss": 2.0973,
      "step": 314700
    },
    {
      "epoch": 12.174652898634799,
      "grad_norm": 11.561026573181152,
      "learning_rate": 3.985445591780433e-05,
      "loss": 2.1275,
      "step": 314800
    },
    {
      "epoch": 12.178520323316704,
      "grad_norm": 13.298796653747559,
      "learning_rate": 3.9851233063902746e-05,
      "loss": 2.0905,
      "step": 314900
    },
    {
      "epoch": 12.182387747998607,
      "grad_norm": 9.316096305847168,
      "learning_rate": 3.984801021000116e-05,
      "loss": 2.1095,
      "step": 315000
    },
    {
      "epoch": 12.186255172680513,
      "grad_norm": 9.74596118927002,
      "learning_rate": 3.9844787356099576e-05,
      "loss": 1.9621,
      "step": 315100
    },
    {
      "epoch": 12.190122597362416,
      "grad_norm": 10.921598434448242,
      "learning_rate": 3.9841564502197984e-05,
      "loss": 2.0472,
      "step": 315200
    },
    {
      "epoch": 12.193990022044321,
      "grad_norm": 11.640517234802246,
      "learning_rate": 3.98383416482964e-05,
      "loss": 2.0981,
      "step": 315300
    },
    {
      "epoch": 12.197857446726225,
      "grad_norm": 13.182424545288086,
      "learning_rate": 3.983511879439481e-05,
      "loss": 2.0706,
      "step": 315400
    },
    {
      "epoch": 12.20172487140813,
      "grad_norm": 10.176836013793945,
      "learning_rate": 3.983189594049323e-05,
      "loss": 2.0431,
      "step": 315500
    },
    {
      "epoch": 12.205592296090034,
      "grad_norm": 13.585079193115234,
      "learning_rate": 3.9828673086591636e-05,
      "loss": 2.0461,
      "step": 315600
    },
    {
      "epoch": 12.209459720771939,
      "grad_norm": 9.480303764343262,
      "learning_rate": 3.982545023269005e-05,
      "loss": 2.096,
      "step": 315700
    },
    {
      "epoch": 12.213327145453842,
      "grad_norm": 9.91309928894043,
      "learning_rate": 3.9822227378788465e-05,
      "loss": 2.0537,
      "step": 315800
    },
    {
      "epoch": 12.217194570135746,
      "grad_norm": 12.468255043029785,
      "learning_rate": 3.981900452488688e-05,
      "loss": 2.0738,
      "step": 315900
    },
    {
      "epoch": 12.221061994817651,
      "grad_norm": 11.497407913208008,
      "learning_rate": 3.9815781670985295e-05,
      "loss": 2.1718,
      "step": 316000
    },
    {
      "epoch": 12.224929419499555,
      "grad_norm": 10.275108337402344,
      "learning_rate": 3.98125588170837e-05,
      "loss": 2.033,
      "step": 316100
    },
    {
      "epoch": 12.22879684418146,
      "grad_norm": 11.980429649353027,
      "learning_rate": 3.980933596318212e-05,
      "loss": 2.0939,
      "step": 316200
    },
    {
      "epoch": 12.232664268863363,
      "grad_norm": 13.854622840881348,
      "learning_rate": 3.980611310928053e-05,
      "loss": 2.0716,
      "step": 316300
    },
    {
      "epoch": 12.236531693545269,
      "grad_norm": 11.152963638305664,
      "learning_rate": 3.980289025537895e-05,
      "loss": 2.1151,
      "step": 316400
    },
    {
      "epoch": 12.240399118227172,
      "grad_norm": 12.920010566711426,
      "learning_rate": 3.979966740147736e-05,
      "loss": 2.0564,
      "step": 316500
    },
    {
      "epoch": 12.244266542909077,
      "grad_norm": 12.081914901733398,
      "learning_rate": 3.9796444547575776e-05,
      "loss": 2.0924,
      "step": 316600
    },
    {
      "epoch": 12.24813396759098,
      "grad_norm": 11.729185104370117,
      "learning_rate": 3.9793221693674184e-05,
      "loss": 2.222,
      "step": 316700
    },
    {
      "epoch": 12.252001392272886,
      "grad_norm": 13.01778793334961,
      "learning_rate": 3.97899988397726e-05,
      "loss": 2.0563,
      "step": 316800
    },
    {
      "epoch": 12.25586881695479,
      "grad_norm": 11.040359497070312,
      "learning_rate": 3.9786775985871014e-05,
      "loss": 2.0706,
      "step": 316900
    },
    {
      "epoch": 12.259736241636695,
      "grad_norm": 10.690428733825684,
      "learning_rate": 3.978355313196943e-05,
      "loss": 2.1978,
      "step": 317000
    },
    {
      "epoch": 12.263603666318598,
      "grad_norm": 13.782716751098633,
      "learning_rate": 3.9780330278067836e-05,
      "loss": 2.1166,
      "step": 317100
    },
    {
      "epoch": 12.267471091000504,
      "grad_norm": 11.0513277053833,
      "learning_rate": 3.977710742416625e-05,
      "loss": 2.1137,
      "step": 317200
    },
    {
      "epoch": 12.271338515682407,
      "grad_norm": 16.250349044799805,
      "learning_rate": 3.9773884570264666e-05,
      "loss": 2.0317,
      "step": 317300
    },
    {
      "epoch": 12.27520594036431,
      "grad_norm": 14.749611854553223,
      "learning_rate": 3.977066171636308e-05,
      "loss": 2.0731,
      "step": 317400
    },
    {
      "epoch": 12.279073365046216,
      "grad_norm": 9.699971199035645,
      "learning_rate": 3.976743886246149e-05,
      "loss": 2.0637,
      "step": 317500
    },
    {
      "epoch": 12.28294078972812,
      "grad_norm": 12.49327564239502,
      "learning_rate": 3.97642160085599e-05,
      "loss": 2.053,
      "step": 317600
    },
    {
      "epoch": 12.286808214410025,
      "grad_norm": 9.095142364501953,
      "learning_rate": 3.976099315465832e-05,
      "loss": 2.029,
      "step": 317700
    },
    {
      "epoch": 12.290675639091928,
      "grad_norm": 13.973092079162598,
      "learning_rate": 3.975777030075673e-05,
      "loss": 2.1147,
      "step": 317800
    },
    {
      "epoch": 12.294543063773833,
      "grad_norm": 11.467143058776855,
      "learning_rate": 3.975454744685514e-05,
      "loss": 2.0611,
      "step": 317900
    },
    {
      "epoch": 12.298410488455737,
      "grad_norm": 10.73990535736084,
      "learning_rate": 3.9751324592953555e-05,
      "loss": 2.0464,
      "step": 318000
    },
    {
      "epoch": 12.302277913137642,
      "grad_norm": 12.277941703796387,
      "learning_rate": 3.974810173905197e-05,
      "loss": 1.9915,
      "step": 318100
    },
    {
      "epoch": 12.306145337819546,
      "grad_norm": 11.9603853225708,
      "learning_rate": 3.9744878885150385e-05,
      "loss": 2.04,
      "step": 318200
    },
    {
      "epoch": 12.31001276250145,
      "grad_norm": 11.446447372436523,
      "learning_rate": 3.974165603124879e-05,
      "loss": 2.1611,
      "step": 318300
    },
    {
      "epoch": 12.313880187183354,
      "grad_norm": 12.787148475646973,
      "learning_rate": 3.973843317734721e-05,
      "loss": 2.0304,
      "step": 318400
    },
    {
      "epoch": 12.31774761186526,
      "grad_norm": 11.386924743652344,
      "learning_rate": 3.973521032344562e-05,
      "loss": 2.0412,
      "step": 318500
    },
    {
      "epoch": 12.321615036547163,
      "grad_norm": 12.780463218688965,
      "learning_rate": 3.973198746954404e-05,
      "loss": 2.1293,
      "step": 318600
    },
    {
      "epoch": 12.325482461229068,
      "grad_norm": 11.633464813232422,
      "learning_rate": 3.9728764615642445e-05,
      "loss": 2.0404,
      "step": 318700
    },
    {
      "epoch": 12.329349885910972,
      "grad_norm": 12.707183837890625,
      "learning_rate": 3.972554176174086e-05,
      "loss": 2.0357,
      "step": 318800
    },
    {
      "epoch": 12.333217310592875,
      "grad_norm": 13.482468605041504,
      "learning_rate": 3.9722318907839274e-05,
      "loss": 2.0461,
      "step": 318900
    },
    {
      "epoch": 12.33708473527478,
      "grad_norm": 11.416601181030273,
      "learning_rate": 3.971909605393768e-05,
      "loss": 2.1396,
      "step": 319000
    },
    {
      "epoch": 12.340952159956684,
      "grad_norm": 10.853586196899414,
      "learning_rate": 3.97158732000361e-05,
      "loss": 2.0923,
      "step": 319100
    },
    {
      "epoch": 12.34481958463859,
      "grad_norm": 10.131381034851074,
      "learning_rate": 3.971265034613451e-05,
      "loss": 2.1527,
      "step": 319200
    },
    {
      "epoch": 12.348687009320493,
      "grad_norm": 11.888036727905273,
      "learning_rate": 3.9709427492232926e-05,
      "loss": 2.0607,
      "step": 319300
    },
    {
      "epoch": 12.352554434002398,
      "grad_norm": 11.885278701782227,
      "learning_rate": 3.9706204638331334e-05,
      "loss": 2.112,
      "step": 319400
    },
    {
      "epoch": 12.356421858684302,
      "grad_norm": 13.350400924682617,
      "learning_rate": 3.970298178442975e-05,
      "loss": 2.0553,
      "step": 319500
    },
    {
      "epoch": 12.360289283366207,
      "grad_norm": 10.949631690979004,
      "learning_rate": 3.9699758930528164e-05,
      "loss": 2.1207,
      "step": 319600
    },
    {
      "epoch": 12.36415670804811,
      "grad_norm": 12.546000480651855,
      "learning_rate": 3.969653607662658e-05,
      "loss": 2.1483,
      "step": 319700
    },
    {
      "epoch": 12.368024132730016,
      "grad_norm": 11.850205421447754,
      "learning_rate": 3.9693313222724986e-05,
      "loss": 2.0659,
      "step": 319800
    },
    {
      "epoch": 12.371891557411919,
      "grad_norm": 14.511720657348633,
      "learning_rate": 3.96900903688234e-05,
      "loss": 2.1097,
      "step": 319900
    },
    {
      "epoch": 12.375758982093824,
      "grad_norm": 10.393548011779785,
      "learning_rate": 3.9686867514921816e-05,
      "loss": 2.0905,
      "step": 320000
    },
    {
      "epoch": 12.379626406775728,
      "grad_norm": 13.09191608428955,
      "learning_rate": 3.968364466102023e-05,
      "loss": 2.0671,
      "step": 320100
    },
    {
      "epoch": 12.383493831457633,
      "grad_norm": 11.867613792419434,
      "learning_rate": 3.968042180711864e-05,
      "loss": 2.0917,
      "step": 320200
    },
    {
      "epoch": 12.387361256139537,
      "grad_norm": 10.034783363342285,
      "learning_rate": 3.967719895321705e-05,
      "loss": 2.1742,
      "step": 320300
    },
    {
      "epoch": 12.391228680821442,
      "grad_norm": 11.070539474487305,
      "learning_rate": 3.967397609931547e-05,
      "loss": 2.1328,
      "step": 320400
    },
    {
      "epoch": 12.395096105503345,
      "grad_norm": 12.456897735595703,
      "learning_rate": 3.967075324541388e-05,
      "loss": 2.085,
      "step": 320500
    },
    {
      "epoch": 12.398963530185249,
      "grad_norm": 9.402827262878418,
      "learning_rate": 3.966753039151229e-05,
      "loss": 2.1405,
      "step": 320600
    },
    {
      "epoch": 12.402830954867154,
      "grad_norm": 11.55655288696289,
      "learning_rate": 3.9664307537610705e-05,
      "loss": 2.0651,
      "step": 320700
    },
    {
      "epoch": 12.406698379549058,
      "grad_norm": 11.903255462646484,
      "learning_rate": 3.966108468370912e-05,
      "loss": 2.126,
      "step": 320800
    },
    {
      "epoch": 12.410565804230963,
      "grad_norm": 12.851812362670898,
      "learning_rate": 3.9657861829807535e-05,
      "loss": 2.0988,
      "step": 320900
    },
    {
      "epoch": 12.414433228912866,
      "grad_norm": 13.345283508300781,
      "learning_rate": 3.965463897590594e-05,
      "loss": 2.1601,
      "step": 321000
    },
    {
      "epoch": 12.418300653594772,
      "grad_norm": 12.13676643371582,
      "learning_rate": 3.965141612200436e-05,
      "loss": 2.0988,
      "step": 321100
    },
    {
      "epoch": 12.422168078276675,
      "grad_norm": 12.65881061553955,
      "learning_rate": 3.964819326810277e-05,
      "loss": 2.0895,
      "step": 321200
    },
    {
      "epoch": 12.42603550295858,
      "grad_norm": 11.271482467651367,
      "learning_rate": 3.964497041420119e-05,
      "loss": 2.0774,
      "step": 321300
    },
    {
      "epoch": 12.429902927640484,
      "grad_norm": 10.62571907043457,
      "learning_rate": 3.9641747560299595e-05,
      "loss": 2.0908,
      "step": 321400
    },
    {
      "epoch": 12.433770352322389,
      "grad_norm": 11.353075981140137,
      "learning_rate": 3.963852470639801e-05,
      "loss": 2.1328,
      "step": 321500
    },
    {
      "epoch": 12.437637777004293,
      "grad_norm": 9.775001525878906,
      "learning_rate": 3.9635301852496424e-05,
      "loss": 2.0729,
      "step": 321600
    },
    {
      "epoch": 12.441505201686198,
      "grad_norm": 10.598821640014648,
      "learning_rate": 3.963207899859483e-05,
      "loss": 2.1101,
      "step": 321700
    },
    {
      "epoch": 12.445372626368101,
      "grad_norm": 13.715356826782227,
      "learning_rate": 3.962885614469325e-05,
      "loss": 2.0511,
      "step": 321800
    },
    {
      "epoch": 12.449240051050007,
      "grad_norm": 11.10113525390625,
      "learning_rate": 3.962563329079166e-05,
      "loss": 2.172,
      "step": 321900
    },
    {
      "epoch": 12.45310747573191,
      "grad_norm": 12.597856521606445,
      "learning_rate": 3.9622410436890076e-05,
      "loss": 2.1166,
      "step": 322000
    },
    {
      "epoch": 12.456974900413815,
      "grad_norm": 8.223003387451172,
      "learning_rate": 3.9619187582988484e-05,
      "loss": 2.1153,
      "step": 322100
    },
    {
      "epoch": 12.460842325095719,
      "grad_norm": 11.675902366638184,
      "learning_rate": 3.96159647290869e-05,
      "loss": 2.1184,
      "step": 322200
    },
    {
      "epoch": 12.464709749777622,
      "grad_norm": 12.694991111755371,
      "learning_rate": 3.9612741875185314e-05,
      "loss": 2.0541,
      "step": 322300
    },
    {
      "epoch": 12.468577174459528,
      "grad_norm": 8.528470993041992,
      "learning_rate": 3.960951902128373e-05,
      "loss": 2.0871,
      "step": 322400
    },
    {
      "epoch": 12.472444599141431,
      "grad_norm": 9.92389965057373,
      "learning_rate": 3.960629616738214e-05,
      "loss": 2.057,
      "step": 322500
    },
    {
      "epoch": 12.476312023823336,
      "grad_norm": 10.639618873596191,
      "learning_rate": 3.960307331348055e-05,
      "loss": 2.0888,
      "step": 322600
    },
    {
      "epoch": 12.48017944850524,
      "grad_norm": 11.440218925476074,
      "learning_rate": 3.9599850459578966e-05,
      "loss": 2.0831,
      "step": 322700
    },
    {
      "epoch": 12.484046873187145,
      "grad_norm": 9.760416030883789,
      "learning_rate": 3.959662760567738e-05,
      "loss": 2.0823,
      "step": 322800
    },
    {
      "epoch": 12.487914297869048,
      "grad_norm": 11.412181854248047,
      "learning_rate": 3.9593404751775795e-05,
      "loss": 1.9981,
      "step": 322900
    },
    {
      "epoch": 12.491781722550954,
      "grad_norm": 11.958553314208984,
      "learning_rate": 3.959018189787421e-05,
      "loss": 2.0477,
      "step": 323000
    },
    {
      "epoch": 12.495649147232857,
      "grad_norm": 13.084078788757324,
      "learning_rate": 3.9586959043972625e-05,
      "loss": 2.1039,
      "step": 323100
    },
    {
      "epoch": 12.499516571914763,
      "grad_norm": 14.163217544555664,
      "learning_rate": 3.958373619007103e-05,
      "loss": 2.0468,
      "step": 323200
    },
    {
      "epoch": 12.503383996596666,
      "grad_norm": 12.668822288513184,
      "learning_rate": 3.958051333616945e-05,
      "loss": 2.0832,
      "step": 323300
    },
    {
      "epoch": 12.507251421278571,
      "grad_norm": 11.347288131713867,
      "learning_rate": 3.957729048226786e-05,
      "loss": 2.2035,
      "step": 323400
    },
    {
      "epoch": 12.511118845960475,
      "grad_norm": 9.615863800048828,
      "learning_rate": 3.957406762836628e-05,
      "loss": 2.1256,
      "step": 323500
    },
    {
      "epoch": 12.51498627064238,
      "grad_norm": 10.806936264038086,
      "learning_rate": 3.957084477446469e-05,
      "loss": 2.1482,
      "step": 323600
    },
    {
      "epoch": 12.518853695324283,
      "grad_norm": 12.66745376586914,
      "learning_rate": 3.95676219205631e-05,
      "loss": 2.1268,
      "step": 323700
    },
    {
      "epoch": 12.522721120006189,
      "grad_norm": 14.12264347076416,
      "learning_rate": 3.9564399066661514e-05,
      "loss": 2.1609,
      "step": 323800
    },
    {
      "epoch": 12.526588544688092,
      "grad_norm": 11.12618350982666,
      "learning_rate": 3.956117621275993e-05,
      "loss": 2.0565,
      "step": 323900
    },
    {
      "epoch": 12.530455969369996,
      "grad_norm": 12.833086967468262,
      "learning_rate": 3.9557953358858344e-05,
      "loss": 2.1706,
      "step": 324000
    },
    {
      "epoch": 12.534323394051901,
      "grad_norm": 13.254105567932129,
      "learning_rate": 3.955473050495675e-05,
      "loss": 2.0549,
      "step": 324100
    },
    {
      "epoch": 12.538190818733804,
      "grad_norm": 12.239315032958984,
      "learning_rate": 3.9551507651055166e-05,
      "loss": 2.1084,
      "step": 324200
    },
    {
      "epoch": 12.54205824341571,
      "grad_norm": 11.407892227172852,
      "learning_rate": 3.954828479715358e-05,
      "loss": 2.1223,
      "step": 324300
    },
    {
      "epoch": 12.545925668097613,
      "grad_norm": 10.622106552124023,
      "learning_rate": 3.9545061943251996e-05,
      "loss": 2.0348,
      "step": 324400
    },
    {
      "epoch": 12.549793092779518,
      "grad_norm": 12.411149024963379,
      "learning_rate": 3.9541839089350404e-05,
      "loss": 2.1344,
      "step": 324500
    },
    {
      "epoch": 12.553660517461422,
      "grad_norm": 10.801735877990723,
      "learning_rate": 3.953861623544882e-05,
      "loss": 2.086,
      "step": 324600
    },
    {
      "epoch": 12.557527942143327,
      "grad_norm": 11.184897422790527,
      "learning_rate": 3.953539338154723e-05,
      "loss": 1.9878,
      "step": 324700
    },
    {
      "epoch": 12.56139536682523,
      "grad_norm": 11.936906814575195,
      "learning_rate": 3.953217052764564e-05,
      "loss": 2.0713,
      "step": 324800
    },
    {
      "epoch": 12.565262791507136,
      "grad_norm": 10.597371101379395,
      "learning_rate": 3.9528947673744056e-05,
      "loss": 2.0463,
      "step": 324900
    },
    {
      "epoch": 12.56913021618904,
      "grad_norm": 13.751944541931152,
      "learning_rate": 3.952572481984247e-05,
      "loss": 2.1558,
      "step": 325000
    },
    {
      "epoch": 12.572997640870945,
      "grad_norm": 8.801424026489258,
      "learning_rate": 3.9522501965940885e-05,
      "loss": 2.0411,
      "step": 325100
    },
    {
      "epoch": 12.576865065552848,
      "grad_norm": 10.328293800354004,
      "learning_rate": 3.951927911203929e-05,
      "loss": 2.1113,
      "step": 325200
    },
    {
      "epoch": 12.580732490234753,
      "grad_norm": 11.433157920837402,
      "learning_rate": 3.951605625813771e-05,
      "loss": 2.0711,
      "step": 325300
    },
    {
      "epoch": 12.584599914916657,
      "grad_norm": 10.620773315429688,
      "learning_rate": 3.951283340423612e-05,
      "loss": 1.9631,
      "step": 325400
    },
    {
      "epoch": 12.588467339598562,
      "grad_norm": 11.814655303955078,
      "learning_rate": 3.950961055033454e-05,
      "loss": 2.1161,
      "step": 325500
    },
    {
      "epoch": 12.592334764280466,
      "grad_norm": 12.883830070495605,
      "learning_rate": 3.9506387696432945e-05,
      "loss": 2.1246,
      "step": 325600
    },
    {
      "epoch": 12.59620218896237,
      "grad_norm": 12.050837516784668,
      "learning_rate": 3.950316484253136e-05,
      "loss": 2.0834,
      "step": 325700
    },
    {
      "epoch": 12.600069613644274,
      "grad_norm": 12.746920585632324,
      "learning_rate": 3.9499941988629775e-05,
      "loss": 2.2004,
      "step": 325800
    },
    {
      "epoch": 12.603937038326178,
      "grad_norm": 13.672698974609375,
      "learning_rate": 3.949671913472819e-05,
      "loss": 2.1359,
      "step": 325900
    },
    {
      "epoch": 12.607804463008083,
      "grad_norm": 14.605350494384766,
      "learning_rate": 3.94934962808266e-05,
      "loss": 2.048,
      "step": 326000
    },
    {
      "epoch": 12.611671887689987,
      "grad_norm": 11.55402660369873,
      "learning_rate": 3.949027342692501e-05,
      "loss": 2.1011,
      "step": 326100
    },
    {
      "epoch": 12.615539312371892,
      "grad_norm": 11.531831741333008,
      "learning_rate": 3.948705057302343e-05,
      "loss": 2.1415,
      "step": 326200
    },
    {
      "epoch": 12.619406737053795,
      "grad_norm": 11.9316987991333,
      "learning_rate": 3.948382771912184e-05,
      "loss": 2.0722,
      "step": 326300
    },
    {
      "epoch": 12.6232741617357,
      "grad_norm": 11.300249099731445,
      "learning_rate": 3.948060486522025e-05,
      "loss": 2.0052,
      "step": 326400
    },
    {
      "epoch": 12.627141586417604,
      "grad_norm": 11.307364463806152,
      "learning_rate": 3.9477382011318664e-05,
      "loss": 1.9903,
      "step": 326500
    },
    {
      "epoch": 12.63100901109951,
      "grad_norm": 9.86521053314209,
      "learning_rate": 3.947415915741708e-05,
      "loss": 2.0915,
      "step": 326600
    },
    {
      "epoch": 12.634876435781413,
      "grad_norm": 10.60201358795166,
      "learning_rate": 3.9470936303515493e-05,
      "loss": 2.09,
      "step": 326700
    },
    {
      "epoch": 12.638743860463318,
      "grad_norm": 11.750555992126465,
      "learning_rate": 3.94677134496139e-05,
      "loss": 2.119,
      "step": 326800
    },
    {
      "epoch": 12.642611285145222,
      "grad_norm": 10.8677396774292,
      "learning_rate": 3.9464490595712316e-05,
      "loss": 2.0703,
      "step": 326900
    },
    {
      "epoch": 12.646478709827125,
      "grad_norm": 10.498128890991211,
      "learning_rate": 3.946126774181073e-05,
      "loss": 2.036,
      "step": 327000
    },
    {
      "epoch": 12.65034613450903,
      "grad_norm": 12.515515327453613,
      "learning_rate": 3.9458044887909146e-05,
      "loss": 2.1047,
      "step": 327100
    },
    {
      "epoch": 12.654213559190934,
      "grad_norm": 13.006855964660645,
      "learning_rate": 3.9454822034007554e-05,
      "loss": 2.1432,
      "step": 327200
    },
    {
      "epoch": 12.65808098387284,
      "grad_norm": 17.490375518798828,
      "learning_rate": 3.945159918010597e-05,
      "loss": 2.0314,
      "step": 327300
    },
    {
      "epoch": 12.661948408554743,
      "grad_norm": 12.141432762145996,
      "learning_rate": 3.944837632620438e-05,
      "loss": 2.1011,
      "step": 327400
    },
    {
      "epoch": 12.665815833236648,
      "grad_norm": 13.526537895202637,
      "learning_rate": 3.94451534723028e-05,
      "loss": 2.0763,
      "step": 327500
    },
    {
      "epoch": 12.669683257918551,
      "grad_norm": 12.041340827941895,
      "learning_rate": 3.9441930618401206e-05,
      "loss": 2.0487,
      "step": 327600
    },
    {
      "epoch": 12.673550682600457,
      "grad_norm": 10.667444229125977,
      "learning_rate": 3.943870776449962e-05,
      "loss": 2.1463,
      "step": 327700
    },
    {
      "epoch": 12.67741810728236,
      "grad_norm": 13.984503746032715,
      "learning_rate": 3.9435484910598035e-05,
      "loss": 2.0303,
      "step": 327800
    },
    {
      "epoch": 12.681285531964265,
      "grad_norm": 13.843255996704102,
      "learning_rate": 3.943226205669644e-05,
      "loss": 2.0584,
      "step": 327900
    },
    {
      "epoch": 12.685152956646169,
      "grad_norm": 13.271008491516113,
      "learning_rate": 3.942903920279486e-05,
      "loss": 2.2018,
      "step": 328000
    },
    {
      "epoch": 12.689020381328074,
      "grad_norm": 9.557713508605957,
      "learning_rate": 3.942581634889327e-05,
      "loss": 2.0699,
      "step": 328100
    },
    {
      "epoch": 12.692887806009978,
      "grad_norm": 12.385294914245605,
      "learning_rate": 3.942259349499169e-05,
      "loss": 2.075,
      "step": 328200
    },
    {
      "epoch": 12.696755230691883,
      "grad_norm": 11.812777519226074,
      "learning_rate": 3.9419370641090095e-05,
      "loss": 1.9648,
      "step": 328300
    },
    {
      "epoch": 12.700622655373786,
      "grad_norm": 11.14168643951416,
      "learning_rate": 3.941614778718851e-05,
      "loss": 2.1087,
      "step": 328400
    },
    {
      "epoch": 12.704490080055692,
      "grad_norm": 12.963309288024902,
      "learning_rate": 3.9412924933286925e-05,
      "loss": 2.1755,
      "step": 328500
    },
    {
      "epoch": 12.708357504737595,
      "grad_norm": 11.972062110900879,
      "learning_rate": 3.940970207938534e-05,
      "loss": 2.0386,
      "step": 328600
    },
    {
      "epoch": 12.712224929419499,
      "grad_norm": 11.546238899230957,
      "learning_rate": 3.940647922548375e-05,
      "loss": 2.1725,
      "step": 328700
    },
    {
      "epoch": 12.716092354101404,
      "grad_norm": 12.81481647491455,
      "learning_rate": 3.940325637158216e-05,
      "loss": 2.1261,
      "step": 328800
    },
    {
      "epoch": 12.719959778783307,
      "grad_norm": 10.295698165893555,
      "learning_rate": 3.940003351768058e-05,
      "loss": 2.0842,
      "step": 328900
    },
    {
      "epoch": 12.723827203465213,
      "grad_norm": 12.431792259216309,
      "learning_rate": 3.939681066377899e-05,
      "loss": 2.1302,
      "step": 329000
    },
    {
      "epoch": 12.727694628147116,
      "grad_norm": 13.020816802978516,
      "learning_rate": 3.93935878098774e-05,
      "loss": 2.0423,
      "step": 329100
    },
    {
      "epoch": 12.731562052829021,
      "grad_norm": 17.180652618408203,
      "learning_rate": 3.9390364955975814e-05,
      "loss": 2.0543,
      "step": 329200
    },
    {
      "epoch": 12.735429477510925,
      "grad_norm": 7.856204986572266,
      "learning_rate": 3.938714210207423e-05,
      "loss": 2.0925,
      "step": 329300
    },
    {
      "epoch": 12.73929690219283,
      "grad_norm": 14.816996574401855,
      "learning_rate": 3.9383919248172643e-05,
      "loss": 2.0708,
      "step": 329400
    },
    {
      "epoch": 12.743164326874734,
      "grad_norm": 11.72536563873291,
      "learning_rate": 3.938069639427106e-05,
      "loss": 2.0078,
      "step": 329500
    },
    {
      "epoch": 12.747031751556639,
      "grad_norm": 10.57793140411377,
      "learning_rate": 3.9377473540369466e-05,
      "loss": 2.0451,
      "step": 329600
    },
    {
      "epoch": 12.750899176238542,
      "grad_norm": 10.768970489501953,
      "learning_rate": 3.937425068646788e-05,
      "loss": 1.9899,
      "step": 329700
    },
    {
      "epoch": 12.754766600920448,
      "grad_norm": 13.376507759094238,
      "learning_rate": 3.9371027832566296e-05,
      "loss": 2.1852,
      "step": 329800
    },
    {
      "epoch": 12.758634025602351,
      "grad_norm": 12.52984619140625,
      "learning_rate": 3.936780497866471e-05,
      "loss": 2.0827,
      "step": 329900
    },
    {
      "epoch": 12.762501450284256,
      "grad_norm": 13.969466209411621,
      "learning_rate": 3.9364582124763125e-05,
      "loss": 2.1664,
      "step": 330000
    },
    {
      "epoch": 12.76636887496616,
      "grad_norm": 10.229002952575684,
      "learning_rate": 3.936135927086154e-05,
      "loss": 2.1166,
      "step": 330100
    },
    {
      "epoch": 12.770236299648065,
      "grad_norm": 12.013175964355469,
      "learning_rate": 3.935813641695995e-05,
      "loss": 2.0912,
      "step": 330200
    },
    {
      "epoch": 12.774103724329969,
      "grad_norm": 8.569968223571777,
      "learning_rate": 3.935491356305836e-05,
      "loss": 2.0687,
      "step": 330300
    },
    {
      "epoch": 12.777971149011872,
      "grad_norm": 14.885847091674805,
      "learning_rate": 3.935169070915678e-05,
      "loss": 2.1407,
      "step": 330400
    },
    {
      "epoch": 12.781838573693777,
      "grad_norm": 12.098048210144043,
      "learning_rate": 3.934846785525519e-05,
      "loss": 2.1045,
      "step": 330500
    },
    {
      "epoch": 12.785705998375681,
      "grad_norm": 11.710723876953125,
      "learning_rate": 3.93452450013536e-05,
      "loss": 2.0795,
      "step": 330600
    },
    {
      "epoch": 12.789573423057586,
      "grad_norm": 10.680438995361328,
      "learning_rate": 3.9342022147452014e-05,
      "loss": 2.0185,
      "step": 330700
    },
    {
      "epoch": 12.79344084773949,
      "grad_norm": 12.274395942687988,
      "learning_rate": 3.933879929355043e-05,
      "loss": 2.1584,
      "step": 330800
    },
    {
      "epoch": 12.797308272421395,
      "grad_norm": 14.333189010620117,
      "learning_rate": 3.9335576439648844e-05,
      "loss": 2.0626,
      "step": 330900
    },
    {
      "epoch": 12.801175697103298,
      "grad_norm": 10.607135772705078,
      "learning_rate": 3.933235358574725e-05,
      "loss": 2.0555,
      "step": 331000
    },
    {
      "epoch": 12.805043121785204,
      "grad_norm": 8.23664665222168,
      "learning_rate": 3.9329130731845667e-05,
      "loss": 2.1039,
      "step": 331100
    },
    {
      "epoch": 12.808910546467107,
      "grad_norm": 12.012152671813965,
      "learning_rate": 3.932590787794408e-05,
      "loss": 2.0967,
      "step": 331200
    },
    {
      "epoch": 12.812777971149012,
      "grad_norm": 10.388446807861328,
      "learning_rate": 3.9322685024042496e-05,
      "loss": 2.1376,
      "step": 331300
    },
    {
      "epoch": 12.816645395830916,
      "grad_norm": 12.15562629699707,
      "learning_rate": 3.9319462170140904e-05,
      "loss": 2.1519,
      "step": 331400
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 12.856409072875977,
      "learning_rate": 3.931623931623932e-05,
      "loss": 2.0775,
      "step": 331500
    },
    {
      "epoch": 12.824380245194725,
      "grad_norm": 7.4952497482299805,
      "learning_rate": 3.931301646233773e-05,
      "loss": 2.0194,
      "step": 331600
    },
    {
      "epoch": 12.82824766987663,
      "grad_norm": 12.06777286529541,
      "learning_rate": 3.930979360843615e-05,
      "loss": 2.0671,
      "step": 331700
    },
    {
      "epoch": 12.832115094558533,
      "grad_norm": 10.727712631225586,
      "learning_rate": 3.9306570754534556e-05,
      "loss": 2.0081,
      "step": 331800
    },
    {
      "epoch": 12.835982519240439,
      "grad_norm": 12.632894515991211,
      "learning_rate": 3.930334790063297e-05,
      "loss": 2.1142,
      "step": 331900
    },
    {
      "epoch": 12.839849943922342,
      "grad_norm": 11.528641700744629,
      "learning_rate": 3.9300125046731385e-05,
      "loss": 2.1751,
      "step": 332000
    },
    {
      "epoch": 12.843717368604246,
      "grad_norm": 8.898213386535645,
      "learning_rate": 3.92969021928298e-05,
      "loss": 2.0015,
      "step": 332100
    },
    {
      "epoch": 12.84758479328615,
      "grad_norm": 11.584930419921875,
      "learning_rate": 3.929367933892821e-05,
      "loss": 2.1127,
      "step": 332200
    },
    {
      "epoch": 12.851452217968054,
      "grad_norm": 9.477936744689941,
      "learning_rate": 3.929045648502662e-05,
      "loss": 2.0451,
      "step": 332300
    },
    {
      "epoch": 12.85531964264996,
      "grad_norm": 14.700743675231934,
      "learning_rate": 3.928723363112504e-05,
      "loss": 2.0288,
      "step": 332400
    },
    {
      "epoch": 12.859187067331863,
      "grad_norm": 12.35953426361084,
      "learning_rate": 3.928401077722345e-05,
      "loss": 2.153,
      "step": 332500
    },
    {
      "epoch": 12.863054492013768,
      "grad_norm": 10.428218841552734,
      "learning_rate": 3.928078792332186e-05,
      "loss": 2.0883,
      "step": 332600
    },
    {
      "epoch": 12.866921916695672,
      "grad_norm": 10.156609535217285,
      "learning_rate": 3.9277565069420275e-05,
      "loss": 2.1175,
      "step": 332700
    },
    {
      "epoch": 12.870789341377577,
      "grad_norm": 8.180585861206055,
      "learning_rate": 3.927434221551869e-05,
      "loss": 1.999,
      "step": 332800
    },
    {
      "epoch": 12.87465676605948,
      "grad_norm": 9.047425270080566,
      "learning_rate": 3.9271119361617104e-05,
      "loss": 2.069,
      "step": 332900
    },
    {
      "epoch": 12.878524190741386,
      "grad_norm": 11.026758193969727,
      "learning_rate": 3.926789650771551e-05,
      "loss": 2.1271,
      "step": 333000
    },
    {
      "epoch": 12.88239161542329,
      "grad_norm": 13.014184951782227,
      "learning_rate": 3.926467365381393e-05,
      "loss": 2.1002,
      "step": 333100
    },
    {
      "epoch": 12.886259040105195,
      "grad_norm": 11.137324333190918,
      "learning_rate": 3.926145079991234e-05,
      "loss": 2.0858,
      "step": 333200
    },
    {
      "epoch": 12.890126464787098,
      "grad_norm": 9.733728408813477,
      "learning_rate": 3.9258227946010756e-05,
      "loss": 2.1441,
      "step": 333300
    },
    {
      "epoch": 12.893993889469003,
      "grad_norm": 13.802302360534668,
      "learning_rate": 3.9255005092109164e-05,
      "loss": 2.1536,
      "step": 333400
    },
    {
      "epoch": 12.897861314150907,
      "grad_norm": 12.952417373657227,
      "learning_rate": 3.925178223820758e-05,
      "loss": 2.027,
      "step": 333500
    },
    {
      "epoch": 12.901728738832812,
      "grad_norm": 13.77076244354248,
      "learning_rate": 3.9248559384305994e-05,
      "loss": 2.1613,
      "step": 333600
    },
    {
      "epoch": 12.905596163514716,
      "grad_norm": 13.831302642822266,
      "learning_rate": 3.92453365304044e-05,
      "loss": 2.0717,
      "step": 333700
    },
    {
      "epoch": 12.909463588196619,
      "grad_norm": 14.067789077758789,
      "learning_rate": 3.9242113676502817e-05,
      "loss": 2.1615,
      "step": 333800
    },
    {
      "epoch": 12.913331012878524,
      "grad_norm": 14.330425262451172,
      "learning_rate": 3.923889082260123e-05,
      "loss": 2.0459,
      "step": 333900
    },
    {
      "epoch": 12.917198437560428,
      "grad_norm": 11.193867683410645,
      "learning_rate": 3.9235667968699646e-05,
      "loss": 2.143,
      "step": 334000
    },
    {
      "epoch": 12.921065862242333,
      "grad_norm": 10.228845596313477,
      "learning_rate": 3.9232445114798054e-05,
      "loss": 2.096,
      "step": 334100
    },
    {
      "epoch": 12.924933286924237,
      "grad_norm": 16.122846603393555,
      "learning_rate": 3.922922226089647e-05,
      "loss": 2.0059,
      "step": 334200
    },
    {
      "epoch": 12.928800711606142,
      "grad_norm": 13.461283683776855,
      "learning_rate": 3.922599940699488e-05,
      "loss": 2.0952,
      "step": 334300
    },
    {
      "epoch": 12.932668136288045,
      "grad_norm": 11.906560897827148,
      "learning_rate": 3.92227765530933e-05,
      "loss": 2.1259,
      "step": 334400
    },
    {
      "epoch": 12.93653556096995,
      "grad_norm": 13.536824226379395,
      "learning_rate": 3.9219553699191706e-05,
      "loss": 2.0875,
      "step": 334500
    },
    {
      "epoch": 12.940402985651854,
      "grad_norm": 16.593090057373047,
      "learning_rate": 3.921633084529012e-05,
      "loss": 2.1896,
      "step": 334600
    },
    {
      "epoch": 12.94427041033376,
      "grad_norm": 9.44835090637207,
      "learning_rate": 3.9213107991388535e-05,
      "loss": 2.075,
      "step": 334700
    },
    {
      "epoch": 12.948137835015663,
      "grad_norm": 14.868803024291992,
      "learning_rate": 3.920988513748695e-05,
      "loss": 2.0723,
      "step": 334800
    },
    {
      "epoch": 12.952005259697568,
      "grad_norm": 14.722436904907227,
      "learning_rate": 3.920666228358536e-05,
      "loss": 2.075,
      "step": 334900
    },
    {
      "epoch": 12.955872684379472,
      "grad_norm": 9.693235397338867,
      "learning_rate": 3.920343942968377e-05,
      "loss": 2.0472,
      "step": 335000
    },
    {
      "epoch": 12.959740109061377,
      "grad_norm": 13.245152473449707,
      "learning_rate": 3.920021657578219e-05,
      "loss": 1.9808,
      "step": 335100
    },
    {
      "epoch": 12.96360753374328,
      "grad_norm": 13.42609691619873,
      "learning_rate": 3.91969937218806e-05,
      "loss": 2.1455,
      "step": 335200
    },
    {
      "epoch": 12.967474958425186,
      "grad_norm": 9.778409957885742,
      "learning_rate": 3.919377086797901e-05,
      "loss": 2.0528,
      "step": 335300
    },
    {
      "epoch": 12.971342383107089,
      "grad_norm": 11.973627090454102,
      "learning_rate": 3.9190548014077425e-05,
      "loss": 2.016,
      "step": 335400
    },
    {
      "epoch": 12.975209807788993,
      "grad_norm": 12.618829727172852,
      "learning_rate": 3.918732516017584e-05,
      "loss": 2.0659,
      "step": 335500
    },
    {
      "epoch": 12.979077232470898,
      "grad_norm": 10.30490779876709,
      "learning_rate": 3.9184102306274254e-05,
      "loss": 2.0987,
      "step": 335600
    },
    {
      "epoch": 12.982944657152801,
      "grad_norm": 13.49170207977295,
      "learning_rate": 3.918087945237266e-05,
      "loss": 2.0649,
      "step": 335700
    },
    {
      "epoch": 12.986812081834707,
      "grad_norm": 13.571158409118652,
      "learning_rate": 3.917765659847108e-05,
      "loss": 2.1145,
      "step": 335800
    },
    {
      "epoch": 12.99067950651661,
      "grad_norm": 12.545452117919922,
      "learning_rate": 3.917443374456949e-05,
      "loss": 2.0832,
      "step": 335900
    },
    {
      "epoch": 12.994546931198515,
      "grad_norm": 12.547021865844727,
      "learning_rate": 3.9171210890667906e-05,
      "loss": 2.1008,
      "step": 336000
    },
    {
      "epoch": 12.998414355880419,
      "grad_norm": 10.684120178222656,
      "learning_rate": 3.9167988036766314e-05,
      "loss": 2.1174,
      "step": 336100
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.9581395387649536,
      "eval_runtime": 2.9199,
      "eval_samples_per_second": 466.114,
      "eval_steps_per_second": 466.114,
      "step": 336141
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.8946837186813354,
      "eval_runtime": 55.3444,
      "eval_samples_per_second": 467.202,
      "eval_steps_per_second": 467.202,
      "step": 336141
    },
    {
      "epoch": 13.002281780562324,
      "grad_norm": 11.756387710571289,
      "learning_rate": 3.916476518286473e-05,
      "loss": 2.0977,
      "step": 336200
    },
    {
      "epoch": 13.006149205244228,
      "grad_norm": 14.475016593933105,
      "learning_rate": 3.9161542328963144e-05,
      "loss": 2.1556,
      "step": 336300
    },
    {
      "epoch": 13.010016629926133,
      "grad_norm": 11.554543495178223,
      "learning_rate": 3.915831947506156e-05,
      "loss": 2.1776,
      "step": 336400
    },
    {
      "epoch": 13.013884054608036,
      "grad_norm": 12.48062801361084,
      "learning_rate": 3.915509662115997e-05,
      "loss": 2.1041,
      "step": 336500
    },
    {
      "epoch": 13.017751479289942,
      "grad_norm": 12.849072456359863,
      "learning_rate": 3.915187376725839e-05,
      "loss": 2.1049,
      "step": 336600
    },
    {
      "epoch": 13.021618903971845,
      "grad_norm": 9.32656478881836,
      "learning_rate": 3.9148650913356796e-05,
      "loss": 2.0972,
      "step": 336700
    },
    {
      "epoch": 13.02548632865375,
      "grad_norm": 10.991662979125977,
      "learning_rate": 3.914542805945521e-05,
      "loss": 2.0191,
      "step": 336800
    },
    {
      "epoch": 13.029353753335654,
      "grad_norm": 11.939051628112793,
      "learning_rate": 3.9142205205553625e-05,
      "loss": 2.0308,
      "step": 336900
    },
    {
      "epoch": 13.033221178017557,
      "grad_norm": 12.265892028808594,
      "learning_rate": 3.913898235165204e-05,
      "loss": 2.1085,
      "step": 337000
    },
    {
      "epoch": 13.037088602699463,
      "grad_norm": 11.01750373840332,
      "learning_rate": 3.9135759497750455e-05,
      "loss": 2.0861,
      "step": 337100
    },
    {
      "epoch": 13.040956027381366,
      "grad_norm": 12.724514961242676,
      "learning_rate": 3.913253664384886e-05,
      "loss": 2.097,
      "step": 337200
    },
    {
      "epoch": 13.044823452063271,
      "grad_norm": 16.538227081298828,
      "learning_rate": 3.912931378994728e-05,
      "loss": 2.0454,
      "step": 337300
    },
    {
      "epoch": 13.048690876745175,
      "grad_norm": 13.242935180664062,
      "learning_rate": 3.912609093604569e-05,
      "loss": 1.9511,
      "step": 337400
    },
    {
      "epoch": 13.05255830142708,
      "grad_norm": 11.006928443908691,
      "learning_rate": 3.912286808214411e-05,
      "loss": 2.0988,
      "step": 337500
    },
    {
      "epoch": 13.056425726108984,
      "grad_norm": 10.86876392364502,
      "learning_rate": 3.9119645228242515e-05,
      "loss": 2.162,
      "step": 337600
    },
    {
      "epoch": 13.060293150790889,
      "grad_norm": 13.614673614501953,
      "learning_rate": 3.911642237434093e-05,
      "loss": 2.1592,
      "step": 337700
    },
    {
      "epoch": 13.064160575472792,
      "grad_norm": 17.676830291748047,
      "learning_rate": 3.9113199520439344e-05,
      "loss": 2.0827,
      "step": 337800
    },
    {
      "epoch": 13.068028000154698,
      "grad_norm": 13.62649154663086,
      "learning_rate": 3.910997666653776e-05,
      "loss": 2.0747,
      "step": 337900
    },
    {
      "epoch": 13.071895424836601,
      "grad_norm": 10.46727466583252,
      "learning_rate": 3.910675381263617e-05,
      "loss": 2.1167,
      "step": 338000
    },
    {
      "epoch": 13.075762849518506,
      "grad_norm": 10.470040321350098,
      "learning_rate": 3.910353095873458e-05,
      "loss": 2.0585,
      "step": 338100
    },
    {
      "epoch": 13.07963027420041,
      "grad_norm": 14.653851509094238,
      "learning_rate": 3.9100308104832996e-05,
      "loss": 2.0395,
      "step": 338200
    },
    {
      "epoch": 13.083497698882315,
      "grad_norm": 9.752616882324219,
      "learning_rate": 3.909708525093141e-05,
      "loss": 2.0326,
      "step": 338300
    },
    {
      "epoch": 13.087365123564219,
      "grad_norm": 8.731721878051758,
      "learning_rate": 3.909386239702982e-05,
      "loss": 2.0915,
      "step": 338400
    },
    {
      "epoch": 13.091232548246122,
      "grad_norm": 9.855591773986816,
      "learning_rate": 3.9090639543128234e-05,
      "loss": 2.1233,
      "step": 338500
    },
    {
      "epoch": 13.095099972928027,
      "grad_norm": 10.296865463256836,
      "learning_rate": 3.908741668922665e-05,
      "loss": 2.1514,
      "step": 338600
    },
    {
      "epoch": 13.09896739760993,
      "grad_norm": 9.853141784667969,
      "learning_rate": 3.908419383532506e-05,
      "loss": 2.0342,
      "step": 338700
    },
    {
      "epoch": 13.102834822291836,
      "grad_norm": 12.645939826965332,
      "learning_rate": 3.908097098142347e-05,
      "loss": 2.0976,
      "step": 338800
    },
    {
      "epoch": 13.10670224697374,
      "grad_norm": 11.763850212097168,
      "learning_rate": 3.9077748127521886e-05,
      "loss": 2.0904,
      "step": 338900
    },
    {
      "epoch": 13.110569671655645,
      "grad_norm": 13.111255645751953,
      "learning_rate": 3.90745252736203e-05,
      "loss": 2.039,
      "step": 339000
    },
    {
      "epoch": 13.114437096337548,
      "grad_norm": 13.004050254821777,
      "learning_rate": 3.9071302419718715e-05,
      "loss": 2.1062,
      "step": 339100
    },
    {
      "epoch": 13.118304521019454,
      "grad_norm": 12.819035530090332,
      "learning_rate": 3.906807956581712e-05,
      "loss": 1.9921,
      "step": 339200
    },
    {
      "epoch": 13.122171945701357,
      "grad_norm": 9.72592544555664,
      "learning_rate": 3.906485671191554e-05,
      "loss": 1.9546,
      "step": 339300
    },
    {
      "epoch": 13.126039370383262,
      "grad_norm": 14.218213081359863,
      "learning_rate": 3.906163385801395e-05,
      "loss": 2.0599,
      "step": 339400
    },
    {
      "epoch": 13.129906795065166,
      "grad_norm": 9.09033203125,
      "learning_rate": 3.905841100411236e-05,
      "loss": 2.0283,
      "step": 339500
    },
    {
      "epoch": 13.133774219747071,
      "grad_norm": 15.228177070617676,
      "learning_rate": 3.9055188150210775e-05,
      "loss": 2.1082,
      "step": 339600
    },
    {
      "epoch": 13.137641644428975,
      "grad_norm": 10.984485626220703,
      "learning_rate": 3.905196529630919e-05,
      "loss": 2.0196,
      "step": 339700
    },
    {
      "epoch": 13.14150906911088,
      "grad_norm": 11.393359184265137,
      "learning_rate": 3.9048742442407605e-05,
      "loss": 2.0086,
      "step": 339800
    },
    {
      "epoch": 13.145376493792783,
      "grad_norm": 14.61003589630127,
      "learning_rate": 3.904551958850601e-05,
      "loss": 2.0238,
      "step": 339900
    },
    {
      "epoch": 13.149243918474689,
      "grad_norm": 10.711152076721191,
      "learning_rate": 3.904229673460443e-05,
      "loss": 2.0922,
      "step": 340000
    },
    {
      "epoch": 13.153111343156592,
      "grad_norm": 10.659966468811035,
      "learning_rate": 3.903907388070284e-05,
      "loss": 2.0487,
      "step": 340100
    },
    {
      "epoch": 13.156978767838496,
      "grad_norm": 10.521197319030762,
      "learning_rate": 3.903585102680126e-05,
      "loss": 2.084,
      "step": 340200
    },
    {
      "epoch": 13.1608461925204,
      "grad_norm": 9.848227500915527,
      "learning_rate": 3.9032628172899665e-05,
      "loss": 1.9931,
      "step": 340300
    },
    {
      "epoch": 13.164713617202304,
      "grad_norm": 13.477888107299805,
      "learning_rate": 3.902940531899808e-05,
      "loss": 2.0916,
      "step": 340400
    },
    {
      "epoch": 13.16858104188421,
      "grad_norm": 12.604951858520508,
      "learning_rate": 3.9026182465096494e-05,
      "loss": 2.0252,
      "step": 340500
    },
    {
      "epoch": 13.172448466566113,
      "grad_norm": 12.102986335754395,
      "learning_rate": 3.902295961119491e-05,
      "loss": 1.9867,
      "step": 340600
    },
    {
      "epoch": 13.176315891248018,
      "grad_norm": 11.567349433898926,
      "learning_rate": 3.901973675729332e-05,
      "loss": 2.0664,
      "step": 340700
    },
    {
      "epoch": 13.180183315929922,
      "grad_norm": 12.810446739196777,
      "learning_rate": 3.901651390339173e-05,
      "loss": 2.1555,
      "step": 340800
    },
    {
      "epoch": 13.184050740611827,
      "grad_norm": 12.885212898254395,
      "learning_rate": 3.9013291049490146e-05,
      "loss": 2.0935,
      "step": 340900
    },
    {
      "epoch": 13.18791816529373,
      "grad_norm": 9.286600112915039,
      "learning_rate": 3.901006819558856e-05,
      "loss": 2.0128,
      "step": 341000
    },
    {
      "epoch": 13.191785589975636,
      "grad_norm": 14.417582511901855,
      "learning_rate": 3.900684534168697e-05,
      "loss": 1.9994,
      "step": 341100
    },
    {
      "epoch": 13.19565301465754,
      "grad_norm": 11.098697662353516,
      "learning_rate": 3.9003622487785384e-05,
      "loss": 2.1041,
      "step": 341200
    },
    {
      "epoch": 13.199520439339445,
      "grad_norm": 10.481623649597168,
      "learning_rate": 3.90003996338838e-05,
      "loss": 2.0819,
      "step": 341300
    },
    {
      "epoch": 13.203387864021348,
      "grad_norm": 9.734305381774902,
      "learning_rate": 3.899717677998221e-05,
      "loss": 1.9806,
      "step": 341400
    },
    {
      "epoch": 13.207255288703253,
      "grad_norm": 11.467611312866211,
      "learning_rate": 3.899395392608062e-05,
      "loss": 2.2021,
      "step": 341500
    },
    {
      "epoch": 13.211122713385157,
      "grad_norm": 10.760424613952637,
      "learning_rate": 3.8990731072179036e-05,
      "loss": 2.0255,
      "step": 341600
    },
    {
      "epoch": 13.214990138067062,
      "grad_norm": 12.607697486877441,
      "learning_rate": 3.898750821827745e-05,
      "loss": 2.123,
      "step": 341700
    },
    {
      "epoch": 13.218857562748966,
      "grad_norm": 12.728407859802246,
      "learning_rate": 3.8984285364375865e-05,
      "loss": 2.1,
      "step": 341800
    },
    {
      "epoch": 13.222724987430869,
      "grad_norm": 9.439377784729004,
      "learning_rate": 3.898106251047427e-05,
      "loss": 2.0915,
      "step": 341900
    },
    {
      "epoch": 13.226592412112774,
      "grad_norm": 11.253865242004395,
      "learning_rate": 3.897783965657269e-05,
      "loss": 2.0696,
      "step": 342000
    },
    {
      "epoch": 13.230459836794678,
      "grad_norm": 9.854462623596191,
      "learning_rate": 3.89746168026711e-05,
      "loss": 2.0481,
      "step": 342100
    },
    {
      "epoch": 13.234327261476583,
      "grad_norm": 10.743441581726074,
      "learning_rate": 3.897139394876952e-05,
      "loss": 2.0249,
      "step": 342200
    },
    {
      "epoch": 13.238194686158486,
      "grad_norm": 11.924858093261719,
      "learning_rate": 3.8968171094867925e-05,
      "loss": 2.0048,
      "step": 342300
    },
    {
      "epoch": 13.242062110840392,
      "grad_norm": 11.761186599731445,
      "learning_rate": 3.896494824096634e-05,
      "loss": 2.0257,
      "step": 342400
    },
    {
      "epoch": 13.245929535522295,
      "grad_norm": 14.809159278869629,
      "learning_rate": 3.8961725387064755e-05,
      "loss": 2.0633,
      "step": 342500
    },
    {
      "epoch": 13.2497969602042,
      "grad_norm": 11.772006034851074,
      "learning_rate": 3.895850253316316e-05,
      "loss": 2.0788,
      "step": 342600
    },
    {
      "epoch": 13.253664384886104,
      "grad_norm": 12.124527931213379,
      "learning_rate": 3.895527967926158e-05,
      "loss": 2.0439,
      "step": 342700
    },
    {
      "epoch": 13.25753180956801,
      "grad_norm": 11.326528549194336,
      "learning_rate": 3.895205682535999e-05,
      "loss": 2.0445,
      "step": 342800
    },
    {
      "epoch": 13.261399234249913,
      "grad_norm": 10.623305320739746,
      "learning_rate": 3.894883397145841e-05,
      "loss": 2.0584,
      "step": 342900
    },
    {
      "epoch": 13.265266658931818,
      "grad_norm": 9.704967498779297,
      "learning_rate": 3.894561111755682e-05,
      "loss": 2.0993,
      "step": 343000
    },
    {
      "epoch": 13.269134083613721,
      "grad_norm": 14.165589332580566,
      "learning_rate": 3.894238826365523e-05,
      "loss": 2.075,
      "step": 343100
    },
    {
      "epoch": 13.273001508295627,
      "grad_norm": 11.779253959655762,
      "learning_rate": 3.8939165409753644e-05,
      "loss": 2.059,
      "step": 343200
    },
    {
      "epoch": 13.27686893297753,
      "grad_norm": 9.728741645812988,
      "learning_rate": 3.893594255585206e-05,
      "loss": 2.0836,
      "step": 343300
    },
    {
      "epoch": 13.280736357659435,
      "grad_norm": 11.669275283813477,
      "learning_rate": 3.8932719701950474e-05,
      "loss": 2.085,
      "step": 343400
    },
    {
      "epoch": 13.284603782341339,
      "grad_norm": 10.393528938293457,
      "learning_rate": 3.892949684804889e-05,
      "loss": 2.1021,
      "step": 343500
    },
    {
      "epoch": 13.288471207023242,
      "grad_norm": 10.692756652832031,
      "learning_rate": 3.89262739941473e-05,
      "loss": 2.1204,
      "step": 343600
    },
    {
      "epoch": 13.292338631705148,
      "grad_norm": 11.027936935424805,
      "learning_rate": 3.892305114024571e-05,
      "loss": 2.0398,
      "step": 343700
    },
    {
      "epoch": 13.296206056387051,
      "grad_norm": 12.990213394165039,
      "learning_rate": 3.8919828286344126e-05,
      "loss": 2.1186,
      "step": 343800
    },
    {
      "epoch": 13.300073481068956,
      "grad_norm": 15.88988208770752,
      "learning_rate": 3.891660543244254e-05,
      "loss": 2.0426,
      "step": 343900
    },
    {
      "epoch": 13.30394090575086,
      "grad_norm": 11.340044021606445,
      "learning_rate": 3.8913382578540955e-05,
      "loss": 2.1485,
      "step": 344000
    },
    {
      "epoch": 13.307808330432765,
      "grad_norm": 10.193942070007324,
      "learning_rate": 3.891015972463937e-05,
      "loss": 2.0899,
      "step": 344100
    },
    {
      "epoch": 13.311675755114669,
      "grad_norm": 15.693727493286133,
      "learning_rate": 3.890693687073778e-05,
      "loss": 2.1452,
      "step": 344200
    },
    {
      "epoch": 13.315543179796574,
      "grad_norm": 12.065960884094238,
      "learning_rate": 3.890371401683619e-05,
      "loss": 2.1046,
      "step": 344300
    },
    {
      "epoch": 13.319410604478477,
      "grad_norm": 13.037262916564941,
      "learning_rate": 3.890049116293461e-05,
      "loss": 2.111,
      "step": 344400
    },
    {
      "epoch": 13.323278029160383,
      "grad_norm": 11.152013778686523,
      "learning_rate": 3.889726830903302e-05,
      "loss": 2.0346,
      "step": 344500
    },
    {
      "epoch": 13.327145453842286,
      "grad_norm": 10.6881103515625,
      "learning_rate": 3.889404545513143e-05,
      "loss": 2.1348,
      "step": 344600
    },
    {
      "epoch": 13.331012878524191,
      "grad_norm": 10.88928508758545,
      "learning_rate": 3.8890822601229845e-05,
      "loss": 2.1332,
      "step": 344700
    },
    {
      "epoch": 13.334880303206095,
      "grad_norm": 13.067537307739258,
      "learning_rate": 3.888759974732826e-05,
      "loss": 1.9809,
      "step": 344800
    },
    {
      "epoch": 13.338747727888,
      "grad_norm": 13.719236373901367,
      "learning_rate": 3.8884376893426674e-05,
      "loss": 2.0862,
      "step": 344900
    },
    {
      "epoch": 13.342615152569904,
      "grad_norm": 10.126096725463867,
      "learning_rate": 3.888115403952508e-05,
      "loss": 2.031,
      "step": 345000
    },
    {
      "epoch": 13.346482577251807,
      "grad_norm": 11.942439079284668,
      "learning_rate": 3.88779311856235e-05,
      "loss": 2.0414,
      "step": 345100
    },
    {
      "epoch": 13.350350001933712,
      "grad_norm": 10.60016918182373,
      "learning_rate": 3.887470833172191e-05,
      "loss": 2.0452,
      "step": 345200
    },
    {
      "epoch": 13.354217426615616,
      "grad_norm": 14.133551597595215,
      "learning_rate": 3.887148547782032e-05,
      "loss": 1.9626,
      "step": 345300
    },
    {
      "epoch": 13.358084851297521,
      "grad_norm": 14.690507888793945,
      "learning_rate": 3.8868262623918734e-05,
      "loss": 2.0829,
      "step": 345400
    },
    {
      "epoch": 13.361952275979425,
      "grad_norm": 13.169389724731445,
      "learning_rate": 3.886503977001715e-05,
      "loss": 2.0703,
      "step": 345500
    },
    {
      "epoch": 13.36581970066133,
      "grad_norm": 13.19925594329834,
      "learning_rate": 3.8861816916115564e-05,
      "loss": 2.0496,
      "step": 345600
    },
    {
      "epoch": 13.369687125343233,
      "grad_norm": 9.126917839050293,
      "learning_rate": 3.885859406221397e-05,
      "loss": 2.009,
      "step": 345700
    },
    {
      "epoch": 13.373554550025139,
      "grad_norm": 9.220542907714844,
      "learning_rate": 3.8855371208312386e-05,
      "loss": 2.0313,
      "step": 345800
    },
    {
      "epoch": 13.377421974707042,
      "grad_norm": 13.893819808959961,
      "learning_rate": 3.88521483544108e-05,
      "loss": 2.0885,
      "step": 345900
    },
    {
      "epoch": 13.381289399388947,
      "grad_norm": 8.450571060180664,
      "learning_rate": 3.8848925500509216e-05,
      "loss": 2.08,
      "step": 346000
    },
    {
      "epoch": 13.385156824070851,
      "grad_norm": 13.99971866607666,
      "learning_rate": 3.8845702646607624e-05,
      "loss": 2.0294,
      "step": 346100
    },
    {
      "epoch": 13.389024248752756,
      "grad_norm": 15.389216423034668,
      "learning_rate": 3.884247979270604e-05,
      "loss": 2.0805,
      "step": 346200
    },
    {
      "epoch": 13.39289167343466,
      "grad_norm": 11.040234565734863,
      "learning_rate": 3.883925693880445e-05,
      "loss": 2.0199,
      "step": 346300
    },
    {
      "epoch": 13.396759098116565,
      "grad_norm": 9.971521377563477,
      "learning_rate": 3.883603408490287e-05,
      "loss": 1.9942,
      "step": 346400
    },
    {
      "epoch": 13.400626522798468,
      "grad_norm": 12.455142974853516,
      "learning_rate": 3.8832811231001276e-05,
      "loss": 2.1331,
      "step": 346500
    },
    {
      "epoch": 13.404493947480372,
      "grad_norm": 14.880796432495117,
      "learning_rate": 3.882958837709969e-05,
      "loss": 2.1135,
      "step": 346600
    },
    {
      "epoch": 13.408361372162277,
      "grad_norm": 8.628149032592773,
      "learning_rate": 3.8826365523198105e-05,
      "loss": 2.0508,
      "step": 346700
    },
    {
      "epoch": 13.41222879684418,
      "grad_norm": 12.537128448486328,
      "learning_rate": 3.882314266929652e-05,
      "loss": 2.0297,
      "step": 346800
    },
    {
      "epoch": 13.416096221526086,
      "grad_norm": 12.820640563964844,
      "learning_rate": 3.881991981539493e-05,
      "loss": 2.0958,
      "step": 346900
    },
    {
      "epoch": 13.41996364620799,
      "grad_norm": 12.426825523376465,
      "learning_rate": 3.881669696149334e-05,
      "loss": 2.0695,
      "step": 347000
    },
    {
      "epoch": 13.423831070889895,
      "grad_norm": 8.86127758026123,
      "learning_rate": 3.881347410759176e-05,
      "loss": 2.0387,
      "step": 347100
    },
    {
      "epoch": 13.427698495571798,
      "grad_norm": 11.621342658996582,
      "learning_rate": 3.881025125369017e-05,
      "loss": 1.9978,
      "step": 347200
    },
    {
      "epoch": 13.431565920253703,
      "grad_norm": 10.150379180908203,
      "learning_rate": 3.880702839978858e-05,
      "loss": 2.0076,
      "step": 347300
    },
    {
      "epoch": 13.435433344935607,
      "grad_norm": 9.630231857299805,
      "learning_rate": 3.8803805545886995e-05,
      "loss": 2.0618,
      "step": 347400
    },
    {
      "epoch": 13.439300769617512,
      "grad_norm": 12.823452949523926,
      "learning_rate": 3.880058269198541e-05,
      "loss": 2.0784,
      "step": 347500
    },
    {
      "epoch": 13.443168194299416,
      "grad_norm": 13.077746391296387,
      "learning_rate": 3.8797359838083824e-05,
      "loss": 2.0672,
      "step": 347600
    },
    {
      "epoch": 13.447035618981321,
      "grad_norm": 9.8488187789917,
      "learning_rate": 3.879413698418223e-05,
      "loss": 2.0907,
      "step": 347700
    },
    {
      "epoch": 13.450903043663224,
      "grad_norm": 10.952237129211426,
      "learning_rate": 3.879091413028065e-05,
      "loss": 1.9607,
      "step": 347800
    },
    {
      "epoch": 13.45477046834513,
      "grad_norm": 11.086790084838867,
      "learning_rate": 3.878769127637906e-05,
      "loss": 2.0932,
      "step": 347900
    },
    {
      "epoch": 13.458637893027033,
      "grad_norm": 10.489378929138184,
      "learning_rate": 3.8784468422477476e-05,
      "loss": 2.0595,
      "step": 348000
    },
    {
      "epoch": 13.462505317708938,
      "grad_norm": 11.157544136047363,
      "learning_rate": 3.8781245568575884e-05,
      "loss": 2.0832,
      "step": 348100
    },
    {
      "epoch": 13.466372742390842,
      "grad_norm": 11.173690795898438,
      "learning_rate": 3.87780227146743e-05,
      "loss": 2.164,
      "step": 348200
    },
    {
      "epoch": 13.470240167072745,
      "grad_norm": 10.197908401489258,
      "learning_rate": 3.8774799860772714e-05,
      "loss": 2.1037,
      "step": 348300
    },
    {
      "epoch": 13.47410759175465,
      "grad_norm": 13.645051956176758,
      "learning_rate": 3.877157700687112e-05,
      "loss": 2.1042,
      "step": 348400
    },
    {
      "epoch": 13.477975016436554,
      "grad_norm": 14.034122467041016,
      "learning_rate": 3.8768354152969536e-05,
      "loss": 1.9489,
      "step": 348500
    },
    {
      "epoch": 13.48184244111846,
      "grad_norm": 10.307709693908691,
      "learning_rate": 3.876513129906795e-05,
      "loss": 1.9773,
      "step": 348600
    },
    {
      "epoch": 13.485709865800363,
      "grad_norm": 11.560259819030762,
      "learning_rate": 3.8761908445166366e-05,
      "loss": 2.1294,
      "step": 348700
    },
    {
      "epoch": 13.489577290482268,
      "grad_norm": 12.863335609436035,
      "learning_rate": 3.8758685591264774e-05,
      "loss": 1.9234,
      "step": 348800
    },
    {
      "epoch": 13.493444715164172,
      "grad_norm": 10.075589179992676,
      "learning_rate": 3.875546273736319e-05,
      "loss": 2.1364,
      "step": 348900
    },
    {
      "epoch": 13.497312139846077,
      "grad_norm": 12.370304107666016,
      "learning_rate": 3.87522398834616e-05,
      "loss": 2.0873,
      "step": 349000
    },
    {
      "epoch": 13.50117956452798,
      "grad_norm": 12.912842750549316,
      "learning_rate": 3.874901702956002e-05,
      "loss": 2.0845,
      "step": 349100
    },
    {
      "epoch": 13.505046989209886,
      "grad_norm": 9.933316230773926,
      "learning_rate": 3.8745794175658426e-05,
      "loss": 2.0106,
      "step": 349200
    },
    {
      "epoch": 13.50891441389179,
      "grad_norm": 11.237961769104004,
      "learning_rate": 3.874257132175684e-05,
      "loss": 2.0818,
      "step": 349300
    },
    {
      "epoch": 13.512781838573694,
      "grad_norm": 13.608942985534668,
      "learning_rate": 3.8739348467855255e-05,
      "loss": 2.1122,
      "step": 349400
    },
    {
      "epoch": 13.516649263255598,
      "grad_norm": 12.755069732666016,
      "learning_rate": 3.873612561395367e-05,
      "loss": 2.0965,
      "step": 349500
    },
    {
      "epoch": 13.520516687937503,
      "grad_norm": 9.620293617248535,
      "learning_rate": 3.873290276005208e-05,
      "loss": 2.0733,
      "step": 349600
    },
    {
      "epoch": 13.524384112619407,
      "grad_norm": 11.837499618530273,
      "learning_rate": 3.872967990615049e-05,
      "loss": 1.8858,
      "step": 349700
    },
    {
      "epoch": 13.528251537301312,
      "grad_norm": 13.420316696166992,
      "learning_rate": 3.872645705224891e-05,
      "loss": 2.0788,
      "step": 349800
    },
    {
      "epoch": 13.532118961983215,
      "grad_norm": 11.759169578552246,
      "learning_rate": 3.872323419834732e-05,
      "loss": 2.0543,
      "step": 349900
    },
    {
      "epoch": 13.535986386665119,
      "grad_norm": 14.84985065460205,
      "learning_rate": 3.872001134444574e-05,
      "loss": 2.1275,
      "step": 350000
    },
    {
      "epoch": 13.539853811347024,
      "grad_norm": 11.610589981079102,
      "learning_rate": 3.871678849054415e-05,
      "loss": 2.0429,
      "step": 350100
    },
    {
      "epoch": 13.543721236028928,
      "grad_norm": 13.730817794799805,
      "learning_rate": 3.871356563664256e-05,
      "loss": 1.9605,
      "step": 350200
    },
    {
      "epoch": 13.547588660710833,
      "grad_norm": 12.070242881774902,
      "learning_rate": 3.8710342782740974e-05,
      "loss": 2.0197,
      "step": 350300
    },
    {
      "epoch": 13.551456085392736,
      "grad_norm": 13.857909202575684,
      "learning_rate": 3.870711992883939e-05,
      "loss": 2.0706,
      "step": 350400
    },
    {
      "epoch": 13.555323510074642,
      "grad_norm": 13.492887496948242,
      "learning_rate": 3.8703897074937803e-05,
      "loss": 2.1071,
      "step": 350500
    },
    {
      "epoch": 13.559190934756545,
      "grad_norm": 14.023866653442383,
      "learning_rate": 3.870067422103622e-05,
      "loss": 1.9954,
      "step": 350600
    },
    {
      "epoch": 13.56305835943845,
      "grad_norm": 11.762011528015137,
      "learning_rate": 3.869745136713463e-05,
      "loss": 2.067,
      "step": 350700
    },
    {
      "epoch": 13.566925784120354,
      "grad_norm": 14.423223495483398,
      "learning_rate": 3.869422851323304e-05,
      "loss": 2.1029,
      "step": 350800
    },
    {
      "epoch": 13.57079320880226,
      "grad_norm": 10.505967140197754,
      "learning_rate": 3.8691005659331456e-05,
      "loss": 2.1021,
      "step": 350900
    },
    {
      "epoch": 13.574660633484163,
      "grad_norm": 10.347037315368652,
      "learning_rate": 3.868778280542987e-05,
      "loss": 2.0651,
      "step": 351000
    },
    {
      "epoch": 13.578528058166068,
      "grad_norm": 10.911030769348145,
      "learning_rate": 3.8684559951528285e-05,
      "loss": 2.0138,
      "step": 351100
    },
    {
      "epoch": 13.582395482847971,
      "grad_norm": 12.081095695495605,
      "learning_rate": 3.868133709762669e-05,
      "loss": 1.964,
      "step": 351200
    },
    {
      "epoch": 13.586262907529877,
      "grad_norm": 12.430167198181152,
      "learning_rate": 3.867811424372511e-05,
      "loss": 2.0936,
      "step": 351300
    },
    {
      "epoch": 13.59013033221178,
      "grad_norm": 12.36895751953125,
      "learning_rate": 3.867489138982352e-05,
      "loss": 2.1275,
      "step": 351400
    },
    {
      "epoch": 13.593997756893685,
      "grad_norm": 17.042564392089844,
      "learning_rate": 3.867166853592193e-05,
      "loss": 2.0334,
      "step": 351500
    },
    {
      "epoch": 13.597865181575589,
      "grad_norm": 11.67762279510498,
      "learning_rate": 3.8668445682020345e-05,
      "loss": 2.1392,
      "step": 351600
    },
    {
      "epoch": 13.601732606257492,
      "grad_norm": 12.218835830688477,
      "learning_rate": 3.866522282811876e-05,
      "loss": 2.056,
      "step": 351700
    },
    {
      "epoch": 13.605600030939398,
      "grad_norm": 10.472810745239258,
      "learning_rate": 3.8661999974217174e-05,
      "loss": 2.0897,
      "step": 351800
    },
    {
      "epoch": 13.609467455621301,
      "grad_norm": 12.524881362915039,
      "learning_rate": 3.865877712031558e-05,
      "loss": 2.0534,
      "step": 351900
    },
    {
      "epoch": 13.613334880303206,
      "grad_norm": 13.14993667602539,
      "learning_rate": 3.8655554266414e-05,
      "loss": 2.1068,
      "step": 352000
    },
    {
      "epoch": 13.61720230498511,
      "grad_norm": 8.50854206085205,
      "learning_rate": 3.865233141251241e-05,
      "loss": 2.1059,
      "step": 352100
    },
    {
      "epoch": 13.621069729667015,
      "grad_norm": 10.217857360839844,
      "learning_rate": 3.8649108558610827e-05,
      "loss": 2.1456,
      "step": 352200
    },
    {
      "epoch": 13.624937154348919,
      "grad_norm": 8.07112979888916,
      "learning_rate": 3.8645885704709234e-05,
      "loss": 2.1008,
      "step": 352300
    },
    {
      "epoch": 13.628804579030824,
      "grad_norm": 9.947656631469727,
      "learning_rate": 3.864266285080765e-05,
      "loss": 2.0207,
      "step": 352400
    },
    {
      "epoch": 13.632672003712727,
      "grad_norm": 10.593799591064453,
      "learning_rate": 3.8639439996906064e-05,
      "loss": 2.0951,
      "step": 352500
    },
    {
      "epoch": 13.636539428394633,
      "grad_norm": 9.61341381072998,
      "learning_rate": 3.863621714300448e-05,
      "loss": 2.0601,
      "step": 352600
    },
    {
      "epoch": 13.640406853076536,
      "grad_norm": 10.571184158325195,
      "learning_rate": 3.8632994289102887e-05,
      "loss": 2.0874,
      "step": 352700
    },
    {
      "epoch": 13.644274277758441,
      "grad_norm": 12.152103424072266,
      "learning_rate": 3.86297714352013e-05,
      "loss": 2.1894,
      "step": 352800
    },
    {
      "epoch": 13.648141702440345,
      "grad_norm": 10.721452713012695,
      "learning_rate": 3.8626548581299716e-05,
      "loss": 1.8521,
      "step": 352900
    },
    {
      "epoch": 13.65200912712225,
      "grad_norm": 12.567581176757812,
      "learning_rate": 3.862332572739813e-05,
      "loss": 2.0845,
      "step": 353000
    },
    {
      "epoch": 13.655876551804154,
      "grad_norm": 10.662153244018555,
      "learning_rate": 3.862010287349654e-05,
      "loss": 2.1336,
      "step": 353100
    },
    {
      "epoch": 13.659743976486059,
      "grad_norm": 11.892882347106934,
      "learning_rate": 3.8616880019594953e-05,
      "loss": 2.0085,
      "step": 353200
    },
    {
      "epoch": 13.663611401167962,
      "grad_norm": 12.733327865600586,
      "learning_rate": 3.861365716569337e-05,
      "loss": 2.0973,
      "step": 353300
    },
    {
      "epoch": 13.667478825849866,
      "grad_norm": 10.85014820098877,
      "learning_rate": 3.861043431179178e-05,
      "loss": 2.0582,
      "step": 353400
    },
    {
      "epoch": 13.671346250531771,
      "grad_norm": 16.093347549438477,
      "learning_rate": 3.860721145789019e-05,
      "loss": 2.1436,
      "step": 353500
    },
    {
      "epoch": 13.675213675213675,
      "grad_norm": 12.719573020935059,
      "learning_rate": 3.8603988603988605e-05,
      "loss": 2.0276,
      "step": 353600
    },
    {
      "epoch": 13.67908109989558,
      "grad_norm": 11.026311874389648,
      "learning_rate": 3.860076575008702e-05,
      "loss": 2.0339,
      "step": 353700
    },
    {
      "epoch": 13.682948524577483,
      "grad_norm": 10.520283699035645,
      "learning_rate": 3.8597542896185435e-05,
      "loss": 2.0725,
      "step": 353800
    },
    {
      "epoch": 13.686815949259389,
      "grad_norm": 10.995012283325195,
      "learning_rate": 3.859432004228384e-05,
      "loss": 2.1228,
      "step": 353900
    },
    {
      "epoch": 13.690683373941292,
      "grad_norm": 12.284358978271484,
      "learning_rate": 3.859109718838226e-05,
      "loss": 1.9918,
      "step": 354000
    },
    {
      "epoch": 13.694550798623197,
      "grad_norm": 9.4657621383667,
      "learning_rate": 3.858787433448067e-05,
      "loss": 2.13,
      "step": 354100
    },
    {
      "epoch": 13.6984182233051,
      "grad_norm": 12.154213905334473,
      "learning_rate": 3.858465148057908e-05,
      "loss": 2.1535,
      "step": 354200
    },
    {
      "epoch": 13.702285647987006,
      "grad_norm": 13.635185241699219,
      "learning_rate": 3.8581428626677495e-05,
      "loss": 2.1218,
      "step": 354300
    },
    {
      "epoch": 13.70615307266891,
      "grad_norm": 8.913629531860352,
      "learning_rate": 3.857820577277591e-05,
      "loss": 1.9878,
      "step": 354400
    },
    {
      "epoch": 13.710020497350815,
      "grad_norm": 10.31062126159668,
      "learning_rate": 3.8574982918874324e-05,
      "loss": 2.0028,
      "step": 354500
    },
    {
      "epoch": 13.713887922032718,
      "grad_norm": 10.35273551940918,
      "learning_rate": 3.857176006497273e-05,
      "loss": 2.0545,
      "step": 354600
    },
    {
      "epoch": 13.717755346714622,
      "grad_norm": 14.150843620300293,
      "learning_rate": 3.856853721107115e-05,
      "loss": 2.1925,
      "step": 354700
    },
    {
      "epoch": 13.721622771396527,
      "grad_norm": 11.888983726501465,
      "learning_rate": 3.856531435716956e-05,
      "loss": 1.9844,
      "step": 354800
    },
    {
      "epoch": 13.72549019607843,
      "grad_norm": 14.299051284790039,
      "learning_rate": 3.8562091503267977e-05,
      "loss": 2.1335,
      "step": 354900
    },
    {
      "epoch": 13.729357620760336,
      "grad_norm": 10.913265228271484,
      "learning_rate": 3.8558868649366384e-05,
      "loss": 2.0498,
      "step": 355000
    },
    {
      "epoch": 13.73322504544224,
      "grad_norm": 10.880060195922852,
      "learning_rate": 3.85556457954648e-05,
      "loss": 2.184,
      "step": 355100
    },
    {
      "epoch": 13.737092470124145,
      "grad_norm": 11.908766746520996,
      "learning_rate": 3.8552422941563214e-05,
      "loss": 1.9953,
      "step": 355200
    },
    {
      "epoch": 13.740959894806048,
      "grad_norm": 12.741706848144531,
      "learning_rate": 3.854920008766163e-05,
      "loss": 2.1475,
      "step": 355300
    },
    {
      "epoch": 13.744827319487953,
      "grad_norm": 12.431841850280762,
      "learning_rate": 3.8545977233760037e-05,
      "loss": 2.2034,
      "step": 355400
    },
    {
      "epoch": 13.748694744169857,
      "grad_norm": 11.181601524353027,
      "learning_rate": 3.854275437985845e-05,
      "loss": 2.017,
      "step": 355500
    },
    {
      "epoch": 13.752562168851762,
      "grad_norm": 9.090540885925293,
      "learning_rate": 3.8539531525956866e-05,
      "loss": 2.0102,
      "step": 355600
    },
    {
      "epoch": 13.756429593533666,
      "grad_norm": 11.36900806427002,
      "learning_rate": 3.853630867205528e-05,
      "loss": 2.0221,
      "step": 355700
    },
    {
      "epoch": 13.76029701821557,
      "grad_norm": 9.764403343200684,
      "learning_rate": 3.853308581815369e-05,
      "loss": 2.1088,
      "step": 355800
    },
    {
      "epoch": 13.764164442897474,
      "grad_norm": 13.039961814880371,
      "learning_rate": 3.85298629642521e-05,
      "loss": 2.0052,
      "step": 355900
    },
    {
      "epoch": 13.76803186757938,
      "grad_norm": 10.697410583496094,
      "learning_rate": 3.852664011035052e-05,
      "loss": 2.0428,
      "step": 356000
    },
    {
      "epoch": 13.771899292261283,
      "grad_norm": 12.021796226501465,
      "learning_rate": 3.852341725644893e-05,
      "loss": 2.0805,
      "step": 356100
    },
    {
      "epoch": 13.775766716943188,
      "grad_norm": 10.750604629516602,
      "learning_rate": 3.852019440254734e-05,
      "loss": 2.0207,
      "step": 356200
    },
    {
      "epoch": 13.779634141625092,
      "grad_norm": 10.9052734375,
      "learning_rate": 3.8516971548645755e-05,
      "loss": 2.02,
      "step": 356300
    },
    {
      "epoch": 13.783501566306995,
      "grad_norm": 9.991568565368652,
      "learning_rate": 3.851374869474417e-05,
      "loss": 2.1059,
      "step": 356400
    },
    {
      "epoch": 13.7873689909889,
      "grad_norm": 10.874775886535645,
      "learning_rate": 3.8510525840842585e-05,
      "loss": 2.0624,
      "step": 356500
    },
    {
      "epoch": 13.791236415670804,
      "grad_norm": 9.53906536102295,
      "learning_rate": 3.8507302986941e-05,
      "loss": 2.0787,
      "step": 356600
    },
    {
      "epoch": 13.79510384035271,
      "grad_norm": 10.338940620422363,
      "learning_rate": 3.850408013303941e-05,
      "loss": 2.1072,
      "step": 356700
    },
    {
      "epoch": 13.798971265034613,
      "grad_norm": 7.517955780029297,
      "learning_rate": 3.850085727913782e-05,
      "loss": 2.0374,
      "step": 356800
    },
    {
      "epoch": 13.802838689716518,
      "grad_norm": 12.293749809265137,
      "learning_rate": 3.849763442523624e-05,
      "loss": 2.0068,
      "step": 356900
    },
    {
      "epoch": 13.806706114398422,
      "grad_norm": 9.645259857177734,
      "learning_rate": 3.849441157133465e-05,
      "loss": 2.0785,
      "step": 357000
    },
    {
      "epoch": 13.810573539080327,
      "grad_norm": 12.60483169555664,
      "learning_rate": 3.8491188717433066e-05,
      "loss": 2.0073,
      "step": 357100
    },
    {
      "epoch": 13.81444096376223,
      "grad_norm": 8.515189170837402,
      "learning_rate": 3.8487965863531474e-05,
      "loss": 2.0214,
      "step": 357200
    },
    {
      "epoch": 13.818308388444136,
      "grad_norm": 12.490724563598633,
      "learning_rate": 3.848474300962989e-05,
      "loss": 2.0165,
      "step": 357300
    },
    {
      "epoch": 13.822175813126039,
      "grad_norm": 10.814386367797852,
      "learning_rate": 3.8481520155728304e-05,
      "loss": 2.0547,
      "step": 357400
    },
    {
      "epoch": 13.826043237807944,
      "grad_norm": 11.99804401397705,
      "learning_rate": 3.847829730182672e-05,
      "loss": 2.0479,
      "step": 357500
    },
    {
      "epoch": 13.829910662489848,
      "grad_norm": 10.094080924987793,
      "learning_rate": 3.847507444792513e-05,
      "loss": 2.0395,
      "step": 357600
    },
    {
      "epoch": 13.833778087171753,
      "grad_norm": 10.359634399414062,
      "learning_rate": 3.847185159402354e-05,
      "loss": 1.9924,
      "step": 357700
    },
    {
      "epoch": 13.837645511853657,
      "grad_norm": 10.926173210144043,
      "learning_rate": 3.8468628740121956e-05,
      "loss": 2.0102,
      "step": 357800
    },
    {
      "epoch": 13.841512936535562,
      "grad_norm": 10.403054237365723,
      "learning_rate": 3.846540588622037e-05,
      "loss": 2.0184,
      "step": 357900
    },
    {
      "epoch": 13.845380361217465,
      "grad_norm": 12.520673751831055,
      "learning_rate": 3.8462183032318785e-05,
      "loss": 2.1763,
      "step": 358000
    },
    {
      "epoch": 13.849247785899369,
      "grad_norm": 10.944944381713867,
      "learning_rate": 3.845896017841719e-05,
      "loss": 2.1194,
      "step": 358100
    },
    {
      "epoch": 13.853115210581274,
      "grad_norm": 10.70614242553711,
      "learning_rate": 3.845573732451561e-05,
      "loss": 2.1326,
      "step": 358200
    },
    {
      "epoch": 13.856982635263178,
      "grad_norm": 11.624406814575195,
      "learning_rate": 3.845251447061402e-05,
      "loss": 2.0745,
      "step": 358300
    },
    {
      "epoch": 13.860850059945083,
      "grad_norm": 10.759686470031738,
      "learning_rate": 3.844929161671244e-05,
      "loss": 2.1385,
      "step": 358400
    },
    {
      "epoch": 13.864717484626986,
      "grad_norm": 12.154324531555176,
      "learning_rate": 3.8446068762810845e-05,
      "loss": 1.9945,
      "step": 358500
    },
    {
      "epoch": 13.868584909308892,
      "grad_norm": 11.584322929382324,
      "learning_rate": 3.844284590890926e-05,
      "loss": 2.0411,
      "step": 358600
    },
    {
      "epoch": 13.872452333990795,
      "grad_norm": 13.186208724975586,
      "learning_rate": 3.8439623055007675e-05,
      "loss": 2.0874,
      "step": 358700
    },
    {
      "epoch": 13.8763197586727,
      "grad_norm": 10.235909461975098,
      "learning_rate": 3.843640020110609e-05,
      "loss": 2.1268,
      "step": 358800
    },
    {
      "epoch": 13.880187183354604,
      "grad_norm": 13.166486740112305,
      "learning_rate": 3.84331773472045e-05,
      "loss": 2.0562,
      "step": 358900
    },
    {
      "epoch": 13.884054608036509,
      "grad_norm": 12.624417304992676,
      "learning_rate": 3.842995449330291e-05,
      "loss": 1.9955,
      "step": 359000
    },
    {
      "epoch": 13.887922032718413,
      "grad_norm": 8.077527046203613,
      "learning_rate": 3.842673163940133e-05,
      "loss": 2.0127,
      "step": 359100
    },
    {
      "epoch": 13.891789457400318,
      "grad_norm": 14.637639045715332,
      "learning_rate": 3.842350878549974e-05,
      "loss": 2.0434,
      "step": 359200
    },
    {
      "epoch": 13.895656882082221,
      "grad_norm": 10.758344650268555,
      "learning_rate": 3.842028593159815e-05,
      "loss": 2.0695,
      "step": 359300
    },
    {
      "epoch": 13.899524306764127,
      "grad_norm": 12.895002365112305,
      "learning_rate": 3.8417063077696564e-05,
      "loss": 2.1658,
      "step": 359400
    },
    {
      "epoch": 13.90339173144603,
      "grad_norm": 9.168810844421387,
      "learning_rate": 3.841384022379498e-05,
      "loss": 2.0733,
      "step": 359500
    },
    {
      "epoch": 13.907259156127935,
      "grad_norm": 12.873700141906738,
      "learning_rate": 3.8410617369893394e-05,
      "loss": 2.2168,
      "step": 359600
    },
    {
      "epoch": 13.911126580809839,
      "grad_norm": 12.833903312683105,
      "learning_rate": 3.84073945159918e-05,
      "loss": 2.0554,
      "step": 359700
    },
    {
      "epoch": 13.914994005491742,
      "grad_norm": 12.907344818115234,
      "learning_rate": 3.8404171662090216e-05,
      "loss": 2.0471,
      "step": 359800
    },
    {
      "epoch": 13.918861430173648,
      "grad_norm": 11.76864242553711,
      "learning_rate": 3.840094880818863e-05,
      "loss": 2.0752,
      "step": 359900
    },
    {
      "epoch": 13.922728854855551,
      "grad_norm": 10.348535537719727,
      "learning_rate": 3.839772595428704e-05,
      "loss": 2.0969,
      "step": 360000
    },
    {
      "epoch": 13.926596279537456,
      "grad_norm": 9.596004486083984,
      "learning_rate": 3.8394503100385454e-05,
      "loss": 2.0613,
      "step": 360100
    },
    {
      "epoch": 13.93046370421936,
      "grad_norm": 12.303717613220215,
      "learning_rate": 3.839128024648387e-05,
      "loss": 2.0872,
      "step": 360200
    },
    {
      "epoch": 13.934331128901265,
      "grad_norm": 7.637840747833252,
      "learning_rate": 3.838805739258228e-05,
      "loss": 2.0487,
      "step": 360300
    },
    {
      "epoch": 13.938198553583169,
      "grad_norm": 13.407577514648438,
      "learning_rate": 3.838483453868069e-05,
      "loss": 2.0809,
      "step": 360400
    },
    {
      "epoch": 13.942065978265074,
      "grad_norm": 11.56566047668457,
      "learning_rate": 3.8381611684779106e-05,
      "loss": 2.0475,
      "step": 360500
    },
    {
      "epoch": 13.945933402946977,
      "grad_norm": 12.86280345916748,
      "learning_rate": 3.837838883087752e-05,
      "loss": 2.0717,
      "step": 360600
    },
    {
      "epoch": 13.949800827628883,
      "grad_norm": 8.34166145324707,
      "learning_rate": 3.8375165976975935e-05,
      "loss": 2.0254,
      "step": 360700
    },
    {
      "epoch": 13.953668252310786,
      "grad_norm": 10.40384578704834,
      "learning_rate": 3.837194312307434e-05,
      "loss": 2.0202,
      "step": 360800
    },
    {
      "epoch": 13.957535676992691,
      "grad_norm": 11.890027046203613,
      "learning_rate": 3.836872026917276e-05,
      "loss": 2.0078,
      "step": 360900
    },
    {
      "epoch": 13.961403101674595,
      "grad_norm": 12.089394569396973,
      "learning_rate": 3.836549741527117e-05,
      "loss": 2.1039,
      "step": 361000
    },
    {
      "epoch": 13.9652705263565,
      "grad_norm": 9.65949535369873,
      "learning_rate": 3.836227456136959e-05,
      "loss": 2.079,
      "step": 361100
    },
    {
      "epoch": 13.969137951038404,
      "grad_norm": 12.133061408996582,
      "learning_rate": 3.8359051707467995e-05,
      "loss": 1.9832,
      "step": 361200
    },
    {
      "epoch": 13.973005375720309,
      "grad_norm": 10.629972457885742,
      "learning_rate": 3.835582885356641e-05,
      "loss": 2.0279,
      "step": 361300
    },
    {
      "epoch": 13.976872800402212,
      "grad_norm": 12.32148265838623,
      "learning_rate": 3.8352605999664825e-05,
      "loss": 2.0461,
      "step": 361400
    },
    {
      "epoch": 13.980740225084116,
      "grad_norm": 13.008061408996582,
      "learning_rate": 3.834938314576324e-05,
      "loss": 2.0447,
      "step": 361500
    },
    {
      "epoch": 13.984607649766021,
      "grad_norm": 11.089225769042969,
      "learning_rate": 3.834616029186165e-05,
      "loss": 2.1185,
      "step": 361600
    },
    {
      "epoch": 13.988475074447924,
      "grad_norm": 12.916071891784668,
      "learning_rate": 3.834293743796006e-05,
      "loss": 2.1029,
      "step": 361700
    },
    {
      "epoch": 13.99234249912983,
      "grad_norm": 14.15770435333252,
      "learning_rate": 3.833971458405848e-05,
      "loss": 2.1556,
      "step": 361800
    },
    {
      "epoch": 13.996209923811733,
      "grad_norm": 10.893966674804688,
      "learning_rate": 3.833649173015689e-05,
      "loss": 2.0242,
      "step": 361900
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.9483304023742676,
      "eval_runtime": 3.452,
      "eval_samples_per_second": 394.269,
      "eval_steps_per_second": 394.269,
      "step": 361998
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8773491382598877,
      "eval_runtime": 56.0882,
      "eval_samples_per_second": 461.006,
      "eval_steps_per_second": 461.006,
      "step": 361998
    },
    {
      "epoch": 14.000077348493638,
      "grad_norm": 12.546953201293945,
      "learning_rate": 3.83332688762553e-05,
      "loss": 2.0799,
      "step": 362000
    },
    {
      "epoch": 14.003944773175542,
      "grad_norm": 13.02121353149414,
      "learning_rate": 3.8330046022353714e-05,
      "loss": 2.0432,
      "step": 362100
    },
    {
      "epoch": 14.007812197857447,
      "grad_norm": 12.478940963745117,
      "learning_rate": 3.832682316845213e-05,
      "loss": 1.9714,
      "step": 362200
    },
    {
      "epoch": 14.01167962253935,
      "grad_norm": 11.40377140045166,
      "learning_rate": 3.8323600314550544e-05,
      "loss": 1.9956,
      "step": 362300
    },
    {
      "epoch": 14.015547047221256,
      "grad_norm": 10.8859224319458,
      "learning_rate": 3.832037746064895e-05,
      "loss": 2.0396,
      "step": 362400
    },
    {
      "epoch": 14.01941447190316,
      "grad_norm": 11.288029670715332,
      "learning_rate": 3.8317154606747366e-05,
      "loss": 2.0289,
      "step": 362500
    },
    {
      "epoch": 14.023281896585065,
      "grad_norm": 13.380599021911621,
      "learning_rate": 3.831393175284578e-05,
      "loss": 1.9492,
      "step": 362600
    },
    {
      "epoch": 14.027149321266968,
      "grad_norm": 12.362467765808105,
      "learning_rate": 3.8310708898944196e-05,
      "loss": 2.034,
      "step": 362700
    },
    {
      "epoch": 14.031016745948873,
      "grad_norm": 9.864434242248535,
      "learning_rate": 3.8307486045042604e-05,
      "loss": 2.0619,
      "step": 362800
    },
    {
      "epoch": 14.034884170630777,
      "grad_norm": 10.5956392288208,
      "learning_rate": 3.830426319114102e-05,
      "loss": 2.0923,
      "step": 362900
    },
    {
      "epoch": 14.03875159531268,
      "grad_norm": 12.86011791229248,
      "learning_rate": 3.830104033723943e-05,
      "loss": 2.1437,
      "step": 363000
    },
    {
      "epoch": 14.042619019994586,
      "grad_norm": 10.615618705749512,
      "learning_rate": 3.829781748333784e-05,
      "loss": 2.0744,
      "step": 363100
    },
    {
      "epoch": 14.04648644467649,
      "grad_norm": 11.940285682678223,
      "learning_rate": 3.8294594629436256e-05,
      "loss": 2.0947,
      "step": 363200
    },
    {
      "epoch": 14.050353869358394,
      "grad_norm": 13.902688026428223,
      "learning_rate": 3.829137177553467e-05,
      "loss": 2.064,
      "step": 363300
    },
    {
      "epoch": 14.054221294040298,
      "grad_norm": 11.595563888549805,
      "learning_rate": 3.8288148921633085e-05,
      "loss": 2.0593,
      "step": 363400
    },
    {
      "epoch": 14.058088718722203,
      "grad_norm": 10.346631050109863,
      "learning_rate": 3.82849260677315e-05,
      "loss": 2.0266,
      "step": 363500
    },
    {
      "epoch": 14.061956143404107,
      "grad_norm": 14.019046783447266,
      "learning_rate": 3.8281703213829915e-05,
      "loss": 2.096,
      "step": 363600
    },
    {
      "epoch": 14.065823568086012,
      "grad_norm": 10.012843132019043,
      "learning_rate": 3.827848035992832e-05,
      "loss": 1.995,
      "step": 363700
    },
    {
      "epoch": 14.069690992767915,
      "grad_norm": 10.832905769348145,
      "learning_rate": 3.827525750602674e-05,
      "loss": 2.177,
      "step": 363800
    },
    {
      "epoch": 14.07355841744982,
      "grad_norm": 10.160330772399902,
      "learning_rate": 3.827203465212515e-05,
      "loss": 2.0808,
      "step": 363900
    },
    {
      "epoch": 14.077425842131724,
      "grad_norm": 12.447175025939941,
      "learning_rate": 3.826881179822357e-05,
      "loss": 2.0907,
      "step": 364000
    },
    {
      "epoch": 14.08129326681363,
      "grad_norm": 11.946173667907715,
      "learning_rate": 3.826558894432198e-05,
      "loss": 2.0163,
      "step": 364100
    },
    {
      "epoch": 14.085160691495533,
      "grad_norm": 12.63170337677002,
      "learning_rate": 3.8262366090420396e-05,
      "loss": 2.0776,
      "step": 364200
    },
    {
      "epoch": 14.089028116177438,
      "grad_norm": 15.263638496398926,
      "learning_rate": 3.8259143236518804e-05,
      "loss": 1.993,
      "step": 364300
    },
    {
      "epoch": 14.092895540859342,
      "grad_norm": 11.206293106079102,
      "learning_rate": 3.825592038261722e-05,
      "loss": 2.1498,
      "step": 364400
    },
    {
      "epoch": 14.096762965541245,
      "grad_norm": 10.210675239562988,
      "learning_rate": 3.8252697528715634e-05,
      "loss": 1.9863,
      "step": 364500
    },
    {
      "epoch": 14.10063039022315,
      "grad_norm": 11.50064754486084,
      "learning_rate": 3.824947467481405e-05,
      "loss": 1.9804,
      "step": 364600
    },
    {
      "epoch": 14.104497814905054,
      "grad_norm": 12.470285415649414,
      "learning_rate": 3.8246251820912456e-05,
      "loss": 2.0521,
      "step": 364700
    },
    {
      "epoch": 14.10836523958696,
      "grad_norm": 12.153166770935059,
      "learning_rate": 3.824302896701087e-05,
      "loss": 2.0565,
      "step": 364800
    },
    {
      "epoch": 14.112232664268863,
      "grad_norm": 11.466582298278809,
      "learning_rate": 3.8239806113109286e-05,
      "loss": 2.0013,
      "step": 364900
    },
    {
      "epoch": 14.116100088950768,
      "grad_norm": 12.581575393676758,
      "learning_rate": 3.82365832592077e-05,
      "loss": 1.9932,
      "step": 365000
    },
    {
      "epoch": 14.119967513632671,
      "grad_norm": 11.821682929992676,
      "learning_rate": 3.823336040530611e-05,
      "loss": 1.9508,
      "step": 365100
    },
    {
      "epoch": 14.123834938314577,
      "grad_norm": 9.759664535522461,
      "learning_rate": 3.823013755140452e-05,
      "loss": 2.0157,
      "step": 365200
    },
    {
      "epoch": 14.12770236299648,
      "grad_norm": 9.727682113647461,
      "learning_rate": 3.822691469750294e-05,
      "loss": 2.0352,
      "step": 365300
    },
    {
      "epoch": 14.131569787678385,
      "grad_norm": 10.517401695251465,
      "learning_rate": 3.822369184360135e-05,
      "loss": 2.0332,
      "step": 365400
    },
    {
      "epoch": 14.135437212360289,
      "grad_norm": 10.688358306884766,
      "learning_rate": 3.822046898969976e-05,
      "loss": 2.0263,
      "step": 365500
    },
    {
      "epoch": 14.139304637042194,
      "grad_norm": 15.027900695800781,
      "learning_rate": 3.8217246135798175e-05,
      "loss": 2.072,
      "step": 365600
    },
    {
      "epoch": 14.143172061724098,
      "grad_norm": 12.931096076965332,
      "learning_rate": 3.821402328189659e-05,
      "loss": 2.1015,
      "step": 365700
    },
    {
      "epoch": 14.147039486406003,
      "grad_norm": 13.03602123260498,
      "learning_rate": 3.8210800427995005e-05,
      "loss": 1.9742,
      "step": 365800
    },
    {
      "epoch": 14.150906911087906,
      "grad_norm": 13.883986473083496,
      "learning_rate": 3.820757757409341e-05,
      "loss": 2.0945,
      "step": 365900
    },
    {
      "epoch": 14.154774335769812,
      "grad_norm": 11.315342903137207,
      "learning_rate": 3.820435472019183e-05,
      "loss": 2.0422,
      "step": 366000
    },
    {
      "epoch": 14.158641760451715,
      "grad_norm": 8.439579963684082,
      "learning_rate": 3.820113186629024e-05,
      "loss": 2.0529,
      "step": 366100
    },
    {
      "epoch": 14.162509185133619,
      "grad_norm": 13.473508834838867,
      "learning_rate": 3.819790901238865e-05,
      "loss": 1.9957,
      "step": 366200
    },
    {
      "epoch": 14.166376609815524,
      "grad_norm": 12.522563934326172,
      "learning_rate": 3.8194686158487065e-05,
      "loss": 2.0478,
      "step": 366300
    },
    {
      "epoch": 14.170244034497427,
      "grad_norm": 12.890594482421875,
      "learning_rate": 3.819146330458548e-05,
      "loss": 2.0864,
      "step": 366400
    },
    {
      "epoch": 14.174111459179333,
      "grad_norm": 12.922796249389648,
      "learning_rate": 3.8188240450683894e-05,
      "loss": 1.9906,
      "step": 366500
    },
    {
      "epoch": 14.177978883861236,
      "grad_norm": 11.878912925720215,
      "learning_rate": 3.81850175967823e-05,
      "loss": 1.9938,
      "step": 366600
    },
    {
      "epoch": 14.181846308543141,
      "grad_norm": 11.47463321685791,
      "learning_rate": 3.818179474288072e-05,
      "loss": 2.0458,
      "step": 366700
    },
    {
      "epoch": 14.185713733225045,
      "grad_norm": 11.543313980102539,
      "learning_rate": 3.817857188897913e-05,
      "loss": 2.0803,
      "step": 366800
    },
    {
      "epoch": 14.18958115790695,
      "grad_norm": 15.39698600769043,
      "learning_rate": 3.8175349035077546e-05,
      "loss": 2.0695,
      "step": 366900
    },
    {
      "epoch": 14.193448582588854,
      "grad_norm": 11.626725196838379,
      "learning_rate": 3.8172126181175954e-05,
      "loss": 2.0009,
      "step": 367000
    },
    {
      "epoch": 14.197316007270759,
      "grad_norm": 11.378826141357422,
      "learning_rate": 3.816890332727437e-05,
      "loss": 2.0554,
      "step": 367100
    },
    {
      "epoch": 14.201183431952662,
      "grad_norm": 12.126428604125977,
      "learning_rate": 3.8165680473372784e-05,
      "loss": 1.9737,
      "step": 367200
    },
    {
      "epoch": 14.205050856634568,
      "grad_norm": 11.83536148071289,
      "learning_rate": 3.81624576194712e-05,
      "loss": 2.0713,
      "step": 367300
    },
    {
      "epoch": 14.208918281316471,
      "grad_norm": 10.965169906616211,
      "learning_rate": 3.8159234765569606e-05,
      "loss": 2.1182,
      "step": 367400
    },
    {
      "epoch": 14.212785705998376,
      "grad_norm": 12.596538543701172,
      "learning_rate": 3.815601191166802e-05,
      "loss": 2.083,
      "step": 367500
    },
    {
      "epoch": 14.21665313068028,
      "grad_norm": 10.461350440979004,
      "learning_rate": 3.8152789057766436e-05,
      "loss": 2.0246,
      "step": 367600
    },
    {
      "epoch": 14.220520555362185,
      "grad_norm": 10.958130836486816,
      "learning_rate": 3.814956620386485e-05,
      "loss": 2.02,
      "step": 367700
    },
    {
      "epoch": 14.224387980044089,
      "grad_norm": 10.793801307678223,
      "learning_rate": 3.814634334996326e-05,
      "loss": 1.994,
      "step": 367800
    },
    {
      "epoch": 14.228255404725992,
      "grad_norm": 8.38589096069336,
      "learning_rate": 3.814312049606167e-05,
      "loss": 2.0485,
      "step": 367900
    },
    {
      "epoch": 14.232122829407897,
      "grad_norm": 11.511597633361816,
      "learning_rate": 3.813989764216009e-05,
      "loss": 2.0688,
      "step": 368000
    },
    {
      "epoch": 14.235990254089801,
      "grad_norm": 10.934189796447754,
      "learning_rate": 3.81366747882585e-05,
      "loss": 2.077,
      "step": 368100
    },
    {
      "epoch": 14.239857678771706,
      "grad_norm": 10.079431533813477,
      "learning_rate": 3.813345193435691e-05,
      "loss": 1.934,
      "step": 368200
    },
    {
      "epoch": 14.24372510345361,
      "grad_norm": 14.625017166137695,
      "learning_rate": 3.8130229080455325e-05,
      "loss": 2.0586,
      "step": 368300
    },
    {
      "epoch": 14.247592528135515,
      "grad_norm": 9.77029800415039,
      "learning_rate": 3.812700622655374e-05,
      "loss": 2.1473,
      "step": 368400
    },
    {
      "epoch": 14.251459952817418,
      "grad_norm": 12.715980529785156,
      "learning_rate": 3.8123783372652155e-05,
      "loss": 2.0439,
      "step": 368500
    },
    {
      "epoch": 14.255327377499324,
      "grad_norm": 14.868850708007812,
      "learning_rate": 3.812056051875056e-05,
      "loss": 2.0358,
      "step": 368600
    },
    {
      "epoch": 14.259194802181227,
      "grad_norm": 13.194962501525879,
      "learning_rate": 3.811733766484898e-05,
      "loss": 2.0619,
      "step": 368700
    },
    {
      "epoch": 14.263062226863132,
      "grad_norm": 12.725253105163574,
      "learning_rate": 3.811411481094739e-05,
      "loss": 1.9957,
      "step": 368800
    },
    {
      "epoch": 14.266929651545036,
      "grad_norm": 9.10289478302002,
      "learning_rate": 3.81108919570458e-05,
      "loss": 2.0763,
      "step": 368900
    },
    {
      "epoch": 14.270797076226941,
      "grad_norm": 9.651691436767578,
      "learning_rate": 3.8107669103144215e-05,
      "loss": 2.1356,
      "step": 369000
    },
    {
      "epoch": 14.274664500908845,
      "grad_norm": 10.581809043884277,
      "learning_rate": 3.810444624924263e-05,
      "loss": 1.9754,
      "step": 369100
    },
    {
      "epoch": 14.27853192559075,
      "grad_norm": 10.370506286621094,
      "learning_rate": 3.8101223395341044e-05,
      "loss": 2.0717,
      "step": 369200
    },
    {
      "epoch": 14.282399350272653,
      "grad_norm": 11.241429328918457,
      "learning_rate": 3.809800054143945e-05,
      "loss": 2.1293,
      "step": 369300
    },
    {
      "epoch": 14.286266774954559,
      "grad_norm": 12.584779739379883,
      "learning_rate": 3.809477768753787e-05,
      "loss": 2.1236,
      "step": 369400
    },
    {
      "epoch": 14.290134199636462,
      "grad_norm": 12.904745101928711,
      "learning_rate": 3.809155483363628e-05,
      "loss": 2.0204,
      "step": 369500
    },
    {
      "epoch": 14.294001624318366,
      "grad_norm": 10.985557556152344,
      "learning_rate": 3.8088331979734696e-05,
      "loss": 2.0181,
      "step": 369600
    },
    {
      "epoch": 14.297869049000271,
      "grad_norm": 12.608139038085938,
      "learning_rate": 3.8085109125833104e-05,
      "loss": 1.97,
      "step": 369700
    },
    {
      "epoch": 14.301736473682174,
      "grad_norm": 12.370758056640625,
      "learning_rate": 3.808188627193152e-05,
      "loss": 2.0715,
      "step": 369800
    },
    {
      "epoch": 14.30560389836408,
      "grad_norm": 14.183523178100586,
      "learning_rate": 3.8078663418029934e-05,
      "loss": 2.069,
      "step": 369900
    },
    {
      "epoch": 14.309471323045983,
      "grad_norm": 9.593809127807617,
      "learning_rate": 3.807544056412835e-05,
      "loss": 2.1395,
      "step": 370000
    },
    {
      "epoch": 14.313338747727888,
      "grad_norm": 12.481377601623535,
      "learning_rate": 3.807221771022676e-05,
      "loss": 2.0139,
      "step": 370100
    },
    {
      "epoch": 14.317206172409792,
      "grad_norm": 11.192742347717285,
      "learning_rate": 3.806899485632517e-05,
      "loss": 2.0379,
      "step": 370200
    },
    {
      "epoch": 14.321073597091697,
      "grad_norm": 8.760089874267578,
      "learning_rate": 3.8065772002423586e-05,
      "loss": 2.0349,
      "step": 370300
    },
    {
      "epoch": 14.3249410217736,
      "grad_norm": 14.723859786987305,
      "learning_rate": 3.8062549148522e-05,
      "loss": 2.0746,
      "step": 370400
    },
    {
      "epoch": 14.328808446455506,
      "grad_norm": 9.036595344543457,
      "learning_rate": 3.8059326294620415e-05,
      "loss": 2.0455,
      "step": 370500
    },
    {
      "epoch": 14.33267587113741,
      "grad_norm": 11.016973495483398,
      "learning_rate": 3.805610344071883e-05,
      "loss": 2.1227,
      "step": 370600
    },
    {
      "epoch": 14.336543295819315,
      "grad_norm": 16.698444366455078,
      "learning_rate": 3.805288058681724e-05,
      "loss": 2.0597,
      "step": 370700
    },
    {
      "epoch": 14.340410720501218,
      "grad_norm": 12.11678409576416,
      "learning_rate": 3.804965773291565e-05,
      "loss": 2.1279,
      "step": 370800
    },
    {
      "epoch": 14.344278145183123,
      "grad_norm": 10.615891456604004,
      "learning_rate": 3.804643487901407e-05,
      "loss": 2.0498,
      "step": 370900
    },
    {
      "epoch": 14.348145569865027,
      "grad_norm": 13.11497688293457,
      "learning_rate": 3.804321202511248e-05,
      "loss": 2.0697,
      "step": 371000
    },
    {
      "epoch": 14.35201299454693,
      "grad_norm": 12.221416473388672,
      "learning_rate": 3.8039989171210897e-05,
      "loss": 2.0377,
      "step": 371100
    },
    {
      "epoch": 14.355880419228836,
      "grad_norm": 10.507743835449219,
      "learning_rate": 3.803676631730931e-05,
      "loss": 2.0771,
      "step": 371200
    },
    {
      "epoch": 14.359747843910739,
      "grad_norm": 9.085274696350098,
      "learning_rate": 3.803354346340772e-05,
      "loss": 2.0127,
      "step": 371300
    },
    {
      "epoch": 14.363615268592644,
      "grad_norm": 12.60744571685791,
      "learning_rate": 3.8030320609506134e-05,
      "loss": 2.0462,
      "step": 371400
    },
    {
      "epoch": 14.367482693274548,
      "grad_norm": 12.614701271057129,
      "learning_rate": 3.802709775560455e-05,
      "loss": 2.0183,
      "step": 371500
    },
    {
      "epoch": 14.371350117956453,
      "grad_norm": 13.156415939331055,
      "learning_rate": 3.8023874901702963e-05,
      "loss": 2.0693,
      "step": 371600
    },
    {
      "epoch": 14.375217542638357,
      "grad_norm": 10.959251403808594,
      "learning_rate": 3.802065204780137e-05,
      "loss": 2.0621,
      "step": 371700
    },
    {
      "epoch": 14.379084967320262,
      "grad_norm": 12.288474082946777,
      "learning_rate": 3.8017429193899786e-05,
      "loss": 2.0628,
      "step": 371800
    },
    {
      "epoch": 14.382952392002165,
      "grad_norm": 12.42213249206543,
      "learning_rate": 3.80142063399982e-05,
      "loss": 2.1315,
      "step": 371900
    },
    {
      "epoch": 14.38681981668407,
      "grad_norm": 12.450550079345703,
      "learning_rate": 3.801098348609661e-05,
      "loss": 2.0157,
      "step": 372000
    },
    {
      "epoch": 14.390687241365974,
      "grad_norm": 16.002002716064453,
      "learning_rate": 3.8007760632195023e-05,
      "loss": 2.0632,
      "step": 372100
    },
    {
      "epoch": 14.39455466604788,
      "grad_norm": 9.801316261291504,
      "learning_rate": 3.800453777829344e-05,
      "loss": 2.0536,
      "step": 372200
    },
    {
      "epoch": 14.398422090729783,
      "grad_norm": 11.303422927856445,
      "learning_rate": 3.800131492439185e-05,
      "loss": 2.087,
      "step": 372300
    },
    {
      "epoch": 14.402289515411688,
      "grad_norm": 11.210943222045898,
      "learning_rate": 3.799809207049026e-05,
      "loss": 2.0633,
      "step": 372400
    },
    {
      "epoch": 14.406156940093592,
      "grad_norm": 13.892997741699219,
      "learning_rate": 3.7994869216588676e-05,
      "loss": 2.0761,
      "step": 372500
    },
    {
      "epoch": 14.410024364775495,
      "grad_norm": 11.449918746948242,
      "learning_rate": 3.799164636268709e-05,
      "loss": 1.9878,
      "step": 372600
    },
    {
      "epoch": 14.4138917894574,
      "grad_norm": 11.467351913452148,
      "learning_rate": 3.7988423508785505e-05,
      "loss": 2.1616,
      "step": 372700
    },
    {
      "epoch": 14.417759214139304,
      "grad_norm": 12.40307903289795,
      "learning_rate": 3.798520065488391e-05,
      "loss": 2.1142,
      "step": 372800
    },
    {
      "epoch": 14.421626638821209,
      "grad_norm": 14.044997215270996,
      "learning_rate": 3.798197780098233e-05,
      "loss": 2.0743,
      "step": 372900
    },
    {
      "epoch": 14.425494063503113,
      "grad_norm": 12.180047988891602,
      "learning_rate": 3.797875494708074e-05,
      "loss": 2.0096,
      "step": 373000
    },
    {
      "epoch": 14.429361488185018,
      "grad_norm": 19.92496681213379,
      "learning_rate": 3.797553209317916e-05,
      "loss": 2.0902,
      "step": 373100
    },
    {
      "epoch": 14.433228912866921,
      "grad_norm": 10.946344375610352,
      "learning_rate": 3.7972309239277565e-05,
      "loss": 2.111,
      "step": 373200
    },
    {
      "epoch": 14.437096337548827,
      "grad_norm": 10.284241676330566,
      "learning_rate": 3.796908638537598e-05,
      "loss": 2.128,
      "step": 373300
    },
    {
      "epoch": 14.44096376223073,
      "grad_norm": 9.22861385345459,
      "learning_rate": 3.7965863531474394e-05,
      "loss": 2.0807,
      "step": 373400
    },
    {
      "epoch": 14.444831186912635,
      "grad_norm": 11.468642234802246,
      "learning_rate": 3.796264067757281e-05,
      "loss": 1.9104,
      "step": 373500
    },
    {
      "epoch": 14.448698611594539,
      "grad_norm": 11.499999046325684,
      "learning_rate": 3.795941782367122e-05,
      "loss": 2.0741,
      "step": 373600
    },
    {
      "epoch": 14.452566036276444,
      "grad_norm": 9.479145050048828,
      "learning_rate": 3.795619496976963e-05,
      "loss": 2.0902,
      "step": 373700
    },
    {
      "epoch": 14.456433460958348,
      "grad_norm": 11.88328742980957,
      "learning_rate": 3.7952972115868047e-05,
      "loss": 2.0261,
      "step": 373800
    },
    {
      "epoch": 14.460300885640253,
      "grad_norm": 10.940041542053223,
      "learning_rate": 3.794974926196646e-05,
      "loss": 2.0148,
      "step": 373900
    },
    {
      "epoch": 14.464168310322156,
      "grad_norm": 13.134743690490723,
      "learning_rate": 3.794652640806487e-05,
      "loss": 2.0722,
      "step": 374000
    },
    {
      "epoch": 14.468035735004062,
      "grad_norm": 11.949238777160645,
      "learning_rate": 3.7943303554163284e-05,
      "loss": 2.0463,
      "step": 374100
    },
    {
      "epoch": 14.471903159685965,
      "grad_norm": 12.25637149810791,
      "learning_rate": 3.79400807002617e-05,
      "loss": 1.9524,
      "step": 374200
    },
    {
      "epoch": 14.475770584367869,
      "grad_norm": 11.374258041381836,
      "learning_rate": 3.793685784636011e-05,
      "loss": 2.0107,
      "step": 374300
    },
    {
      "epoch": 14.479638009049774,
      "grad_norm": 8.660565376281738,
      "learning_rate": 3.793363499245852e-05,
      "loss": 2.1135,
      "step": 374400
    },
    {
      "epoch": 14.483505433731677,
      "grad_norm": 10.86000919342041,
      "learning_rate": 3.7930412138556936e-05,
      "loss": 2.0041,
      "step": 374500
    },
    {
      "epoch": 14.487372858413583,
      "grad_norm": 13.175424575805664,
      "learning_rate": 3.792718928465535e-05,
      "loss": 2.0102,
      "step": 374600
    },
    {
      "epoch": 14.491240283095486,
      "grad_norm": 10.504756927490234,
      "learning_rate": 3.7923966430753765e-05,
      "loss": 2.0455,
      "step": 374700
    },
    {
      "epoch": 14.495107707777391,
      "grad_norm": 10.589262962341309,
      "learning_rate": 3.7920743576852173e-05,
      "loss": 1.9793,
      "step": 374800
    },
    {
      "epoch": 14.498975132459295,
      "grad_norm": 13.449655532836914,
      "learning_rate": 3.791752072295059e-05,
      "loss": 2.0529,
      "step": 374900
    },
    {
      "epoch": 14.5028425571412,
      "grad_norm": 11.543502807617188,
      "learning_rate": 3.7914297869049e-05,
      "loss": 1.9731,
      "step": 375000
    },
    {
      "epoch": 14.506709981823104,
      "grad_norm": 18.177412033081055,
      "learning_rate": 3.791107501514741e-05,
      "loss": 1.9956,
      "step": 375100
    },
    {
      "epoch": 14.510577406505009,
      "grad_norm": 11.104377746582031,
      "learning_rate": 3.7907852161245826e-05,
      "loss": 1.9985,
      "step": 375200
    },
    {
      "epoch": 14.514444831186912,
      "grad_norm": 17.60564422607422,
      "learning_rate": 3.790462930734424e-05,
      "loss": 1.9656,
      "step": 375300
    },
    {
      "epoch": 14.518312255868818,
      "grad_norm": 11.256163597106934,
      "learning_rate": 3.7901406453442655e-05,
      "loss": 2.1085,
      "step": 375400
    },
    {
      "epoch": 14.522179680550721,
      "grad_norm": 13.934362411499023,
      "learning_rate": 3.789818359954106e-05,
      "loss": 2.0451,
      "step": 375500
    },
    {
      "epoch": 14.526047105232626,
      "grad_norm": 11.301280975341797,
      "learning_rate": 3.789496074563948e-05,
      "loss": 2.0861,
      "step": 375600
    },
    {
      "epoch": 14.52991452991453,
      "grad_norm": 11.584980010986328,
      "learning_rate": 3.789173789173789e-05,
      "loss": 2.0632,
      "step": 375700
    },
    {
      "epoch": 14.533781954596435,
      "grad_norm": 11.504295349121094,
      "learning_rate": 3.788851503783631e-05,
      "loss": 2.1186,
      "step": 375800
    },
    {
      "epoch": 14.537649379278339,
      "grad_norm": 14.692548751831055,
      "learning_rate": 3.7885292183934715e-05,
      "loss": 2.0279,
      "step": 375900
    },
    {
      "epoch": 14.541516803960242,
      "grad_norm": 11.21243953704834,
      "learning_rate": 3.788206933003313e-05,
      "loss": 2.0747,
      "step": 376000
    },
    {
      "epoch": 14.545384228642147,
      "grad_norm": 11.867546081542969,
      "learning_rate": 3.7878846476131544e-05,
      "loss": 2.0241,
      "step": 376100
    },
    {
      "epoch": 14.54925165332405,
      "grad_norm": 10.00124740600586,
      "learning_rate": 3.787562362222996e-05,
      "loss": 2.0606,
      "step": 376200
    },
    {
      "epoch": 14.553119078005956,
      "grad_norm": 17.231307983398438,
      "learning_rate": 3.787240076832837e-05,
      "loss": 2.0741,
      "step": 376300
    },
    {
      "epoch": 14.55698650268786,
      "grad_norm": 12.365020751953125,
      "learning_rate": 3.786917791442678e-05,
      "loss": 2.0871,
      "step": 376400
    },
    {
      "epoch": 14.560853927369765,
      "grad_norm": 9.62291145324707,
      "learning_rate": 3.7865955060525197e-05,
      "loss": 2.0649,
      "step": 376500
    },
    {
      "epoch": 14.564721352051668,
      "grad_norm": 12.456429481506348,
      "learning_rate": 3.786273220662361e-05,
      "loss": 2.087,
      "step": 376600
    },
    {
      "epoch": 14.568588776733574,
      "grad_norm": 10.729238510131836,
      "learning_rate": 3.785950935272202e-05,
      "loss": 2.0191,
      "step": 376700
    },
    {
      "epoch": 14.572456201415477,
      "grad_norm": 11.04599666595459,
      "learning_rate": 3.7856286498820434e-05,
      "loss": 2.0222,
      "step": 376800
    },
    {
      "epoch": 14.576323626097382,
      "grad_norm": 8.840826988220215,
      "learning_rate": 3.785306364491885e-05,
      "loss": 2.0196,
      "step": 376900
    },
    {
      "epoch": 14.580191050779286,
      "grad_norm": 11.820469856262207,
      "learning_rate": 3.784984079101726e-05,
      "loss": 2.0666,
      "step": 377000
    },
    {
      "epoch": 14.584058475461191,
      "grad_norm": 12.40531063079834,
      "learning_rate": 3.784661793711568e-05,
      "loss": 2.0198,
      "step": 377100
    },
    {
      "epoch": 14.587925900143095,
      "grad_norm": 12.317008018493652,
      "learning_rate": 3.7843395083214086e-05,
      "loss": 1.9341,
      "step": 377200
    },
    {
      "epoch": 14.591793324825,
      "grad_norm": 9.08613395690918,
      "learning_rate": 3.78401722293125e-05,
      "loss": 1.9777,
      "step": 377300
    },
    {
      "epoch": 14.595660749506903,
      "grad_norm": 12.985050201416016,
      "learning_rate": 3.7836949375410915e-05,
      "loss": 2.0646,
      "step": 377400
    },
    {
      "epoch": 14.599528174188809,
      "grad_norm": 10.024765014648438,
      "learning_rate": 3.783372652150933e-05,
      "loss": 1.8795,
      "step": 377500
    },
    {
      "epoch": 14.603395598870712,
      "grad_norm": 13.116909980773926,
      "learning_rate": 3.7830503667607745e-05,
      "loss": 2.0207,
      "step": 377600
    },
    {
      "epoch": 14.607263023552616,
      "grad_norm": 12.506494522094727,
      "learning_rate": 3.782728081370616e-05,
      "loss": 2.0268,
      "step": 377700
    },
    {
      "epoch": 14.61113044823452,
      "grad_norm": 8.83899974822998,
      "learning_rate": 3.782405795980457e-05,
      "loss": 2.077,
      "step": 377800
    },
    {
      "epoch": 14.614997872916424,
      "grad_norm": 14.345816612243652,
      "learning_rate": 3.782083510590298e-05,
      "loss": 2.1298,
      "step": 377900
    },
    {
      "epoch": 14.61886529759833,
      "grad_norm": 12.08728313446045,
      "learning_rate": 3.78176122520014e-05,
      "loss": 2.0264,
      "step": 378000
    },
    {
      "epoch": 14.622732722280233,
      "grad_norm": 11.635446548461914,
      "learning_rate": 3.781438939809981e-05,
      "loss": 2.0556,
      "step": 378100
    },
    {
      "epoch": 14.626600146962138,
      "grad_norm": 12.85215950012207,
      "learning_rate": 3.781116654419822e-05,
      "loss": 2.1113,
      "step": 378200
    },
    {
      "epoch": 14.630467571644042,
      "grad_norm": 11.868785858154297,
      "learning_rate": 3.7807943690296634e-05,
      "loss": 2.1376,
      "step": 378300
    },
    {
      "epoch": 14.634334996325947,
      "grad_norm": 11.804426193237305,
      "learning_rate": 3.780472083639505e-05,
      "loss": 2.0554,
      "step": 378400
    },
    {
      "epoch": 14.63820242100785,
      "grad_norm": 11.40231990814209,
      "learning_rate": 3.7801497982493464e-05,
      "loss": 2.0665,
      "step": 378500
    },
    {
      "epoch": 14.642069845689756,
      "grad_norm": 11.761863708496094,
      "learning_rate": 3.779827512859187e-05,
      "loss": 2.0227,
      "step": 378600
    },
    {
      "epoch": 14.64593727037166,
      "grad_norm": 11.85198974609375,
      "learning_rate": 3.7795052274690286e-05,
      "loss": 2.1048,
      "step": 378700
    },
    {
      "epoch": 14.649804695053565,
      "grad_norm": 13.149349212646484,
      "learning_rate": 3.77918294207887e-05,
      "loss": 2.0556,
      "step": 378800
    },
    {
      "epoch": 14.653672119735468,
      "grad_norm": 14.097931861877441,
      "learning_rate": 3.7788606566887116e-05,
      "loss": 2.1344,
      "step": 378900
    },
    {
      "epoch": 14.657539544417373,
      "grad_norm": 12.512743949890137,
      "learning_rate": 3.7785383712985524e-05,
      "loss": 2.051,
      "step": 379000
    },
    {
      "epoch": 14.661406969099277,
      "grad_norm": 14.083845138549805,
      "learning_rate": 3.778216085908394e-05,
      "loss": 1.9914,
      "step": 379100
    },
    {
      "epoch": 14.665274393781182,
      "grad_norm": 10.819472312927246,
      "learning_rate": 3.777893800518235e-05,
      "loss": 1.9351,
      "step": 379200
    },
    {
      "epoch": 14.669141818463086,
      "grad_norm": 10.362967491149902,
      "learning_rate": 3.777571515128077e-05,
      "loss": 2.0667,
      "step": 379300
    },
    {
      "epoch": 14.673009243144989,
      "grad_norm": 10.87541389465332,
      "learning_rate": 3.7772492297379176e-05,
      "loss": 2.0063,
      "step": 379400
    },
    {
      "epoch": 14.676876667826894,
      "grad_norm": 11.305188179016113,
      "learning_rate": 3.776926944347759e-05,
      "loss": 2.037,
      "step": 379500
    },
    {
      "epoch": 14.680744092508798,
      "grad_norm": 11.429550170898438,
      "learning_rate": 3.7766046589576005e-05,
      "loss": 2.0112,
      "step": 379600
    },
    {
      "epoch": 14.684611517190703,
      "grad_norm": 11.726643562316895,
      "learning_rate": 3.776282373567442e-05,
      "loss": 2.0228,
      "step": 379700
    },
    {
      "epoch": 14.688478941872606,
      "grad_norm": 11.102075576782227,
      "learning_rate": 3.775960088177283e-05,
      "loss": 2.0702,
      "step": 379800
    },
    {
      "epoch": 14.692346366554512,
      "grad_norm": 11.515986442565918,
      "learning_rate": 3.775637802787124e-05,
      "loss": 1.9646,
      "step": 379900
    },
    {
      "epoch": 14.696213791236415,
      "grad_norm": 11.73817253112793,
      "learning_rate": 3.775315517396966e-05,
      "loss": 2.041,
      "step": 380000
    },
    {
      "epoch": 14.70008121591832,
      "grad_norm": 10.257081031799316,
      "learning_rate": 3.774993232006807e-05,
      "loss": 2.0489,
      "step": 380100
    },
    {
      "epoch": 14.703948640600224,
      "grad_norm": 12.097709655761719,
      "learning_rate": 3.774670946616648e-05,
      "loss": 2.0823,
      "step": 380200
    },
    {
      "epoch": 14.70781606528213,
      "grad_norm": 11.581377029418945,
      "learning_rate": 3.7743486612264895e-05,
      "loss": 2.1049,
      "step": 380300
    },
    {
      "epoch": 14.711683489964033,
      "grad_norm": 12.17480754852295,
      "learning_rate": 3.774026375836331e-05,
      "loss": 2.0465,
      "step": 380400
    },
    {
      "epoch": 14.715550914645938,
      "grad_norm": 9.433850288391113,
      "learning_rate": 3.7737040904461724e-05,
      "loss": 1.9852,
      "step": 380500
    },
    {
      "epoch": 14.719418339327841,
      "grad_norm": 10.797107696533203,
      "learning_rate": 3.773381805056013e-05,
      "loss": 2.0313,
      "step": 380600
    },
    {
      "epoch": 14.723285764009745,
      "grad_norm": 10.03657054901123,
      "learning_rate": 3.773059519665855e-05,
      "loss": 2.0358,
      "step": 380700
    },
    {
      "epoch": 14.72715318869165,
      "grad_norm": 8.51702880859375,
      "learning_rate": 3.772737234275696e-05,
      "loss": 2.0357,
      "step": 380800
    },
    {
      "epoch": 14.731020613373554,
      "grad_norm": 11.827646255493164,
      "learning_rate": 3.772414948885537e-05,
      "loss": 2.1087,
      "step": 380900
    },
    {
      "epoch": 14.734888038055459,
      "grad_norm": 13.747729301452637,
      "learning_rate": 3.7720926634953784e-05,
      "loss": 2.1039,
      "step": 381000
    },
    {
      "epoch": 14.738755462737362,
      "grad_norm": 12.453313827514648,
      "learning_rate": 3.77177037810522e-05,
      "loss": 2.0249,
      "step": 381100
    },
    {
      "epoch": 14.742622887419268,
      "grad_norm": 12.79183292388916,
      "learning_rate": 3.7714480927150614e-05,
      "loss": 2.1289,
      "step": 381200
    },
    {
      "epoch": 14.746490312101171,
      "grad_norm": 9.682287216186523,
      "learning_rate": 3.771125807324902e-05,
      "loss": 2.0963,
      "step": 381300
    },
    {
      "epoch": 14.750357736783076,
      "grad_norm": 8.965555191040039,
      "learning_rate": 3.7708035219347436e-05,
      "loss": 1.9901,
      "step": 381400
    },
    {
      "epoch": 14.75422516146498,
      "grad_norm": 12.61030387878418,
      "learning_rate": 3.770481236544585e-05,
      "loss": 1.9894,
      "step": 381500
    },
    {
      "epoch": 14.758092586146885,
      "grad_norm": 11.748000144958496,
      "learning_rate": 3.7701589511544266e-05,
      "loss": 2.0994,
      "step": 381600
    },
    {
      "epoch": 14.761960010828789,
      "grad_norm": 10.378682136535645,
      "learning_rate": 3.7698366657642674e-05,
      "loss": 2.054,
      "step": 381700
    },
    {
      "epoch": 14.765827435510694,
      "grad_norm": 10.348190307617188,
      "learning_rate": 3.769514380374109e-05,
      "loss": 2.0422,
      "step": 381800
    },
    {
      "epoch": 14.769694860192597,
      "grad_norm": 14.256443977355957,
      "learning_rate": 3.76919209498395e-05,
      "loss": 2.0346,
      "step": 381900
    },
    {
      "epoch": 14.773562284874503,
      "grad_norm": 11.579891204833984,
      "learning_rate": 3.768869809593792e-05,
      "loss": 2.0527,
      "step": 382000
    },
    {
      "epoch": 14.777429709556406,
      "grad_norm": 12.534709930419922,
      "learning_rate": 3.7685475242036326e-05,
      "loss": 1.9497,
      "step": 382100
    },
    {
      "epoch": 14.781297134238311,
      "grad_norm": 13.215188026428223,
      "learning_rate": 3.768225238813474e-05,
      "loss": 2.0862,
      "step": 382200
    },
    {
      "epoch": 14.785164558920215,
      "grad_norm": 12.41884994506836,
      "learning_rate": 3.7679029534233155e-05,
      "loss": 2.1233,
      "step": 382300
    },
    {
      "epoch": 14.789031983602118,
      "grad_norm": 11.993904113769531,
      "learning_rate": 3.767580668033157e-05,
      "loss": 2.054,
      "step": 382400
    },
    {
      "epoch": 14.792899408284024,
      "grad_norm": 13.942450523376465,
      "learning_rate": 3.767258382642998e-05,
      "loss": 2.0306,
      "step": 382500
    },
    {
      "epoch": 14.796766832965927,
      "grad_norm": 10.183316230773926,
      "learning_rate": 3.766936097252839e-05,
      "loss": 2.0419,
      "step": 382600
    },
    {
      "epoch": 14.800634257647832,
      "grad_norm": 9.715648651123047,
      "learning_rate": 3.766613811862681e-05,
      "loss": 2.0627,
      "step": 382700
    },
    {
      "epoch": 14.804501682329736,
      "grad_norm": 11.046485900878906,
      "learning_rate": 3.766291526472522e-05,
      "loss": 2.0454,
      "step": 382800
    },
    {
      "epoch": 14.808369107011641,
      "grad_norm": 10.221674919128418,
      "learning_rate": 3.765969241082363e-05,
      "loss": 1.9828,
      "step": 382900
    },
    {
      "epoch": 14.812236531693545,
      "grad_norm": 15.72909164428711,
      "learning_rate": 3.7656469556922045e-05,
      "loss": 2.0545,
      "step": 383000
    },
    {
      "epoch": 14.81610395637545,
      "grad_norm": 11.530068397521973,
      "learning_rate": 3.765324670302046e-05,
      "loss": 2.0293,
      "step": 383100
    },
    {
      "epoch": 14.819971381057353,
      "grad_norm": 8.972018241882324,
      "learning_rate": 3.7650023849118874e-05,
      "loss": 2.0341,
      "step": 383200
    },
    {
      "epoch": 14.823838805739259,
      "grad_norm": 8.686552047729492,
      "learning_rate": 3.764680099521728e-05,
      "loss": 2.1229,
      "step": 383300
    },
    {
      "epoch": 14.827706230421162,
      "grad_norm": 8.995134353637695,
      "learning_rate": 3.76435781413157e-05,
      "loss": 2.0823,
      "step": 383400
    },
    {
      "epoch": 14.831573655103067,
      "grad_norm": 13.587261199951172,
      "learning_rate": 3.764035528741411e-05,
      "loss": 2.026,
      "step": 383500
    },
    {
      "epoch": 14.835441079784971,
      "grad_norm": 10.128292083740234,
      "learning_rate": 3.7637132433512526e-05,
      "loss": 1.9747,
      "step": 383600
    },
    {
      "epoch": 14.839308504466876,
      "grad_norm": 11.726454734802246,
      "learning_rate": 3.7633909579610934e-05,
      "loss": 2.0041,
      "step": 383700
    },
    {
      "epoch": 14.84317592914878,
      "grad_norm": 10.49760913848877,
      "learning_rate": 3.763068672570935e-05,
      "loss": 1.9723,
      "step": 383800
    },
    {
      "epoch": 14.847043353830685,
      "grad_norm": 11.643194198608398,
      "learning_rate": 3.7627463871807764e-05,
      "loss": 2.0483,
      "step": 383900
    },
    {
      "epoch": 14.850910778512588,
      "grad_norm": 10.40866756439209,
      "learning_rate": 3.762424101790618e-05,
      "loss": 2.111,
      "step": 384000
    },
    {
      "epoch": 14.854778203194492,
      "grad_norm": 12.858503341674805,
      "learning_rate": 3.762101816400459e-05,
      "loss": 2.0437,
      "step": 384100
    },
    {
      "epoch": 14.858645627876397,
      "grad_norm": 12.282608985900879,
      "learning_rate": 3.761779531010301e-05,
      "loss": 2.051,
      "step": 384200
    },
    {
      "epoch": 14.8625130525583,
      "grad_norm": 9.20997428894043,
      "learning_rate": 3.7614572456201416e-05,
      "loss": 1.9631,
      "step": 384300
    },
    {
      "epoch": 14.866380477240206,
      "grad_norm": 8.593809127807617,
      "learning_rate": 3.761134960229983e-05,
      "loss": 2.0741,
      "step": 384400
    },
    {
      "epoch": 14.87024790192211,
      "grad_norm": 10.869553565979004,
      "learning_rate": 3.7608126748398245e-05,
      "loss": 2.1116,
      "step": 384500
    },
    {
      "epoch": 14.874115326604015,
      "grad_norm": 11.087749481201172,
      "learning_rate": 3.760490389449666e-05,
      "loss": 2.1152,
      "step": 384600
    },
    {
      "epoch": 14.877982751285918,
      "grad_norm": 13.35792350769043,
      "learning_rate": 3.7601681040595075e-05,
      "loss": 2.0359,
      "step": 384700
    },
    {
      "epoch": 14.881850175967823,
      "grad_norm": 11.303743362426758,
      "learning_rate": 3.759845818669348e-05,
      "loss": 2.0376,
      "step": 384800
    },
    {
      "epoch": 14.885717600649727,
      "grad_norm": 12.467034339904785,
      "learning_rate": 3.75952353327919e-05,
      "loss": 2.0321,
      "step": 384900
    },
    {
      "epoch": 14.889585025331632,
      "grad_norm": 12.145942687988281,
      "learning_rate": 3.759201247889031e-05,
      "loss": 2.0358,
      "step": 385000
    },
    {
      "epoch": 14.893452450013536,
      "grad_norm": 13.311016082763672,
      "learning_rate": 3.758878962498873e-05,
      "loss": 2.0376,
      "step": 385100
    },
    {
      "epoch": 14.897319874695441,
      "grad_norm": 10.80672836303711,
      "learning_rate": 3.7585566771087135e-05,
      "loss": 2.1024,
      "step": 385200
    },
    {
      "epoch": 14.901187299377344,
      "grad_norm": 10.281403541564941,
      "learning_rate": 3.758234391718555e-05,
      "loss": 2.0067,
      "step": 385300
    },
    {
      "epoch": 14.90505472405925,
      "grad_norm": 11.948553085327148,
      "learning_rate": 3.7579121063283964e-05,
      "loss": 2.027,
      "step": 385400
    },
    {
      "epoch": 14.908922148741153,
      "grad_norm": 8.756417274475098,
      "learning_rate": 3.757589820938238e-05,
      "loss": 2.046,
      "step": 385500
    },
    {
      "epoch": 14.912789573423058,
      "grad_norm": 13.020523071289062,
      "learning_rate": 3.757267535548079e-05,
      "loss": 1.9785,
      "step": 385600
    },
    {
      "epoch": 14.916656998104962,
      "grad_norm": 12.067437171936035,
      "learning_rate": 3.75694525015792e-05,
      "loss": 2.0934,
      "step": 385700
    },
    {
      "epoch": 14.920524422786865,
      "grad_norm": 10.223315238952637,
      "learning_rate": 3.7566229647677616e-05,
      "loss": 2.1106,
      "step": 385800
    },
    {
      "epoch": 14.92439184746877,
      "grad_norm": 9.864569664001465,
      "learning_rate": 3.756300679377603e-05,
      "loss": 2.0328,
      "step": 385900
    },
    {
      "epoch": 14.928259272150674,
      "grad_norm": 9.07880687713623,
      "learning_rate": 3.755978393987444e-05,
      "loss": 2.0404,
      "step": 386000
    },
    {
      "epoch": 14.93212669683258,
      "grad_norm": 11.978824615478516,
      "learning_rate": 3.7556561085972854e-05,
      "loss": 2.1237,
      "step": 386100
    },
    {
      "epoch": 14.935994121514483,
      "grad_norm": 12.277576446533203,
      "learning_rate": 3.755333823207127e-05,
      "loss": 1.9557,
      "step": 386200
    },
    {
      "epoch": 14.939861546196388,
      "grad_norm": 13.78252124786377,
      "learning_rate": 3.755011537816968e-05,
      "loss": 2.0545,
      "step": 386300
    },
    {
      "epoch": 14.943728970878292,
      "grad_norm": 12.3229341506958,
      "learning_rate": 3.754689252426809e-05,
      "loss": 2.0412,
      "step": 386400
    },
    {
      "epoch": 14.947596395560197,
      "grad_norm": 12.761336326599121,
      "learning_rate": 3.7543669670366506e-05,
      "loss": 2.0422,
      "step": 386500
    },
    {
      "epoch": 14.9514638202421,
      "grad_norm": 11.465359687805176,
      "learning_rate": 3.754044681646492e-05,
      "loss": 2.1021,
      "step": 386600
    },
    {
      "epoch": 14.955331244924006,
      "grad_norm": 8.923086166381836,
      "learning_rate": 3.753722396256333e-05,
      "loss": 2.0258,
      "step": 386700
    },
    {
      "epoch": 14.95919866960591,
      "grad_norm": 11.579809188842773,
      "learning_rate": 3.753400110866174e-05,
      "loss": 2.025,
      "step": 386800
    },
    {
      "epoch": 14.963066094287814,
      "grad_norm": 12.363740921020508,
      "learning_rate": 3.753077825476016e-05,
      "loss": 2.0519,
      "step": 386900
    },
    {
      "epoch": 14.966933518969718,
      "grad_norm": 12.419689178466797,
      "learning_rate": 3.752755540085857e-05,
      "loss": 2.0287,
      "step": 387000
    },
    {
      "epoch": 14.970800943651623,
      "grad_norm": 11.809819221496582,
      "learning_rate": 3.752433254695698e-05,
      "loss": 2.0516,
      "step": 387100
    },
    {
      "epoch": 14.974668368333527,
      "grad_norm": 14.655614852905273,
      "learning_rate": 3.7521109693055395e-05,
      "loss": 2.1093,
      "step": 387200
    },
    {
      "epoch": 14.978535793015432,
      "grad_norm": 10.454333305358887,
      "learning_rate": 3.751788683915381e-05,
      "loss": 2.0413,
      "step": 387300
    },
    {
      "epoch": 14.982403217697335,
      "grad_norm": 13.172496795654297,
      "learning_rate": 3.7514663985252225e-05,
      "loss": 2.0713,
      "step": 387400
    },
    {
      "epoch": 14.986270642379239,
      "grad_norm": 13.265356063842773,
      "learning_rate": 3.751144113135063e-05,
      "loss": 2.0431,
      "step": 387500
    },
    {
      "epoch": 14.990138067061144,
      "grad_norm": 15.795721054077148,
      "learning_rate": 3.750821827744905e-05,
      "loss": 2.0786,
      "step": 387600
    },
    {
      "epoch": 14.994005491743048,
      "grad_norm": 14.031447410583496,
      "learning_rate": 3.750499542354746e-05,
      "loss": 2.037,
      "step": 387700
    },
    {
      "epoch": 14.997872916424953,
      "grad_norm": 11.330159187316895,
      "learning_rate": 3.750177256964588e-05,
      "loss": 1.9398,
      "step": 387800
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.9357750415802002,
      "eval_runtime": 2.8908,
      "eval_samples_per_second": 470.797,
      "eval_steps_per_second": 470.797,
      "step": 387855
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.8602747917175293,
      "eval_runtime": 56.3962,
      "eval_samples_per_second": 458.488,
      "eval_steps_per_second": 458.488,
      "step": 387855
    },
    {
      "epoch": 15.001740341106856,
      "grad_norm": 11.534602165222168,
      "learning_rate": 3.7498549715744285e-05,
      "loss": 2.0558,
      "step": 387900
    },
    {
      "epoch": 15.005607765788762,
      "grad_norm": 13.561208724975586,
      "learning_rate": 3.74953268618427e-05,
      "loss": 2.0981,
      "step": 388000
    },
    {
      "epoch": 15.009475190470665,
      "grad_norm": 11.676918983459473,
      "learning_rate": 3.7492104007941114e-05,
      "loss": 1.9983,
      "step": 388100
    },
    {
      "epoch": 15.01334261515257,
      "grad_norm": 13.360871315002441,
      "learning_rate": 3.748888115403953e-05,
      "loss": 2.0162,
      "step": 388200
    },
    {
      "epoch": 15.017210039834474,
      "grad_norm": 8.81676959991455,
      "learning_rate": 3.748565830013794e-05,
      "loss": 1.9969,
      "step": 388300
    },
    {
      "epoch": 15.02107746451638,
      "grad_norm": 12.51404094696045,
      "learning_rate": 3.748243544623635e-05,
      "loss": 1.9988,
      "step": 388400
    },
    {
      "epoch": 15.024944889198283,
      "grad_norm": 13.590865135192871,
      "learning_rate": 3.7479212592334766e-05,
      "loss": 2.0109,
      "step": 388500
    },
    {
      "epoch": 15.028812313880188,
      "grad_norm": 9.001365661621094,
      "learning_rate": 3.747598973843318e-05,
      "loss": 2.0028,
      "step": 388600
    },
    {
      "epoch": 15.032679738562091,
      "grad_norm": 13.341007232666016,
      "learning_rate": 3.747276688453159e-05,
      "loss": 1.9356,
      "step": 388700
    },
    {
      "epoch": 15.036547163243997,
      "grad_norm": 12.535579681396484,
      "learning_rate": 3.7469544030630004e-05,
      "loss": 2.0562,
      "step": 388800
    },
    {
      "epoch": 15.0404145879259,
      "grad_norm": 11.614603996276855,
      "learning_rate": 3.746632117672842e-05,
      "loss": 2.0517,
      "step": 388900
    },
    {
      "epoch": 15.044282012607804,
      "grad_norm": 11.021957397460938,
      "learning_rate": 3.746309832282683e-05,
      "loss": 1.9856,
      "step": 389000
    },
    {
      "epoch": 15.048149437289709,
      "grad_norm": 10.971420288085938,
      "learning_rate": 3.745987546892524e-05,
      "loss": 2.0464,
      "step": 389100
    },
    {
      "epoch": 15.052016861971612,
      "grad_norm": 12.26659107208252,
      "learning_rate": 3.7456652615023656e-05,
      "loss": 1.9538,
      "step": 389200
    },
    {
      "epoch": 15.055884286653518,
      "grad_norm": 13.653789520263672,
      "learning_rate": 3.745342976112207e-05,
      "loss": 2.0427,
      "step": 389300
    },
    {
      "epoch": 15.059751711335421,
      "grad_norm": 11.978052139282227,
      "learning_rate": 3.7450206907220485e-05,
      "loss": 1.9969,
      "step": 389400
    },
    {
      "epoch": 15.063619136017326,
      "grad_norm": 11.703827857971191,
      "learning_rate": 3.744698405331889e-05,
      "loss": 2.0068,
      "step": 389500
    },
    {
      "epoch": 15.06748656069923,
      "grad_norm": 12.584127426147461,
      "learning_rate": 3.744376119941731e-05,
      "loss": 2.0481,
      "step": 389600
    },
    {
      "epoch": 15.071353985381135,
      "grad_norm": 13.712369918823242,
      "learning_rate": 3.744053834551572e-05,
      "loss": 2.0675,
      "step": 389700
    },
    {
      "epoch": 15.075221410063039,
      "grad_norm": 11.850781440734863,
      "learning_rate": 3.743731549161413e-05,
      "loss": 2.0882,
      "step": 389800
    },
    {
      "epoch": 15.079088834744944,
      "grad_norm": 13.982443809509277,
      "learning_rate": 3.7434092637712545e-05,
      "loss": 2.1886,
      "step": 389900
    },
    {
      "epoch": 15.082956259426847,
      "grad_norm": 10.651883125305176,
      "learning_rate": 3.743086978381096e-05,
      "loss": 2.0798,
      "step": 390000
    },
    {
      "epoch": 15.086823684108753,
      "grad_norm": 12.614531517028809,
      "learning_rate": 3.7427646929909375e-05,
      "loss": 2.0074,
      "step": 390100
    },
    {
      "epoch": 15.090691108790656,
      "grad_norm": 11.22480297088623,
      "learning_rate": 3.742442407600778e-05,
      "loss": 2.0878,
      "step": 390200
    },
    {
      "epoch": 15.094558533472561,
      "grad_norm": 13.845816612243652,
      "learning_rate": 3.74212012221062e-05,
      "loss": 1.9836,
      "step": 390300
    },
    {
      "epoch": 15.098425958154465,
      "grad_norm": 14.346379280090332,
      "learning_rate": 3.741797836820461e-05,
      "loss": 1.99,
      "step": 390400
    },
    {
      "epoch": 15.10229338283637,
      "grad_norm": 10.625504493713379,
      "learning_rate": 3.741475551430303e-05,
      "loss": 2.0083,
      "step": 390500
    },
    {
      "epoch": 15.106160807518274,
      "grad_norm": 10.97264575958252,
      "learning_rate": 3.741153266040144e-05,
      "loss": 2.0129,
      "step": 390600
    },
    {
      "epoch": 15.110028232200177,
      "grad_norm": 12.126142501831055,
      "learning_rate": 3.740830980649985e-05,
      "loss": 2.0782,
      "step": 390700
    },
    {
      "epoch": 15.113895656882082,
      "grad_norm": 12.403191566467285,
      "learning_rate": 3.7405086952598264e-05,
      "loss": 2.0369,
      "step": 390800
    },
    {
      "epoch": 15.117763081563986,
      "grad_norm": 14.246796607971191,
      "learning_rate": 3.740186409869668e-05,
      "loss": 2.0086,
      "step": 390900
    },
    {
      "epoch": 15.121630506245891,
      "grad_norm": 12.158159255981445,
      "learning_rate": 3.7398641244795094e-05,
      "loss": 2.0339,
      "step": 391000
    },
    {
      "epoch": 15.125497930927795,
      "grad_norm": 13.21264934539795,
      "learning_rate": 3.739541839089351e-05,
      "loss": 2.0144,
      "step": 391100
    },
    {
      "epoch": 15.1293653556097,
      "grad_norm": 11.073108673095703,
      "learning_rate": 3.739219553699192e-05,
      "loss": 2.1286,
      "step": 391200
    },
    {
      "epoch": 15.133232780291603,
      "grad_norm": 14.767334938049316,
      "learning_rate": 3.738897268309033e-05,
      "loss": 1.9816,
      "step": 391300
    },
    {
      "epoch": 15.137100204973509,
      "grad_norm": 14.38431167602539,
      "learning_rate": 3.7385749829188746e-05,
      "loss": 2.0751,
      "step": 391400
    },
    {
      "epoch": 15.140967629655412,
      "grad_norm": 12.247365951538086,
      "learning_rate": 3.738252697528716e-05,
      "loss": 1.9445,
      "step": 391500
    },
    {
      "epoch": 15.144835054337317,
      "grad_norm": 13.278000831604004,
      "learning_rate": 3.7379304121385575e-05,
      "loss": 2.0103,
      "step": 391600
    },
    {
      "epoch": 15.14870247901922,
      "grad_norm": 12.31915283203125,
      "learning_rate": 3.737608126748399e-05,
      "loss": 2.0182,
      "step": 391700
    },
    {
      "epoch": 15.152569903701126,
      "grad_norm": 13.671981811523438,
      "learning_rate": 3.73728584135824e-05,
      "loss": 2.0112,
      "step": 391800
    },
    {
      "epoch": 15.15643732838303,
      "grad_norm": 11.894695281982422,
      "learning_rate": 3.736963555968081e-05,
      "loss": 2.0776,
      "step": 391900
    },
    {
      "epoch": 15.160304753064935,
      "grad_norm": 11.760904312133789,
      "learning_rate": 3.736641270577923e-05,
      "loss": 2.0454,
      "step": 392000
    },
    {
      "epoch": 15.164172177746838,
      "grad_norm": 12.773859024047852,
      "learning_rate": 3.736318985187764e-05,
      "loss": 2.0401,
      "step": 392100
    },
    {
      "epoch": 15.168039602428742,
      "grad_norm": 10.767539978027344,
      "learning_rate": 3.735996699797605e-05,
      "loss": 2.0321,
      "step": 392200
    },
    {
      "epoch": 15.171907027110647,
      "grad_norm": 9.856311798095703,
      "learning_rate": 3.7356744144074465e-05,
      "loss": 2.0065,
      "step": 392300
    },
    {
      "epoch": 15.17577445179255,
      "grad_norm": 9.669180870056152,
      "learning_rate": 3.735352129017288e-05,
      "loss": 2.0344,
      "step": 392400
    },
    {
      "epoch": 15.179641876474456,
      "grad_norm": 13.515937805175781,
      "learning_rate": 3.735029843627129e-05,
      "loss": 2.074,
      "step": 392500
    },
    {
      "epoch": 15.18350930115636,
      "grad_norm": 12.623639106750488,
      "learning_rate": 3.73470755823697e-05,
      "loss": 2.1437,
      "step": 392600
    },
    {
      "epoch": 15.187376725838265,
      "grad_norm": 9.350954055786133,
      "learning_rate": 3.734385272846812e-05,
      "loss": 2.0222,
      "step": 392700
    },
    {
      "epoch": 15.191244150520168,
      "grad_norm": 10.934820175170898,
      "learning_rate": 3.734062987456653e-05,
      "loss": 2.0938,
      "step": 392800
    },
    {
      "epoch": 15.195111575202073,
      "grad_norm": 12.17609977722168,
      "learning_rate": 3.733740702066494e-05,
      "loss": 2.0117,
      "step": 392900
    },
    {
      "epoch": 15.198978999883977,
      "grad_norm": 10.943046569824219,
      "learning_rate": 3.7334184166763354e-05,
      "loss": 2.148,
      "step": 393000
    },
    {
      "epoch": 15.202846424565882,
      "grad_norm": 14.748483657836914,
      "learning_rate": 3.733096131286177e-05,
      "loss": 2.0235,
      "step": 393100
    },
    {
      "epoch": 15.206713849247786,
      "grad_norm": 10.248893737792969,
      "learning_rate": 3.7327738458960183e-05,
      "loss": 2.0004,
      "step": 393200
    },
    {
      "epoch": 15.21058127392969,
      "grad_norm": 10.920565605163574,
      "learning_rate": 3.732451560505859e-05,
      "loss": 1.9742,
      "step": 393300
    },
    {
      "epoch": 15.214448698611594,
      "grad_norm": 10.680951118469238,
      "learning_rate": 3.7321292751157006e-05,
      "loss": 2.0488,
      "step": 393400
    },
    {
      "epoch": 15.2183161232935,
      "grad_norm": 10.496176719665527,
      "learning_rate": 3.731806989725542e-05,
      "loss": 2.1431,
      "step": 393500
    },
    {
      "epoch": 15.222183547975403,
      "grad_norm": 13.90298843383789,
      "learning_rate": 3.7314847043353836e-05,
      "loss": 1.9935,
      "step": 393600
    },
    {
      "epoch": 15.226050972657308,
      "grad_norm": 12.097370147705078,
      "learning_rate": 3.7311624189452243e-05,
      "loss": 2.0381,
      "step": 393700
    },
    {
      "epoch": 15.229918397339212,
      "grad_norm": 10.855841636657715,
      "learning_rate": 3.730840133555066e-05,
      "loss": 2.134,
      "step": 393800
    },
    {
      "epoch": 15.233785822021115,
      "grad_norm": 16.650453567504883,
      "learning_rate": 3.730517848164907e-05,
      "loss": 2.0318,
      "step": 393900
    },
    {
      "epoch": 15.23765324670302,
      "grad_norm": 10.202226638793945,
      "learning_rate": 3.730195562774749e-05,
      "loss": 1.9974,
      "step": 394000
    },
    {
      "epoch": 15.241520671384924,
      "grad_norm": 11.726968765258789,
      "learning_rate": 3.7298732773845896e-05,
      "loss": 1.9932,
      "step": 394100
    },
    {
      "epoch": 15.24538809606683,
      "grad_norm": 10.800066947937012,
      "learning_rate": 3.729550991994431e-05,
      "loss": 2.0191,
      "step": 394200
    },
    {
      "epoch": 15.249255520748733,
      "grad_norm": 9.396190643310547,
      "learning_rate": 3.7292287066042725e-05,
      "loss": 2.0493,
      "step": 394300
    },
    {
      "epoch": 15.253122945430638,
      "grad_norm": 11.639763832092285,
      "learning_rate": 3.728906421214114e-05,
      "loss": 2.0479,
      "step": 394400
    },
    {
      "epoch": 15.256990370112542,
      "grad_norm": 10.822358131408691,
      "learning_rate": 3.728584135823955e-05,
      "loss": 2.1013,
      "step": 394500
    },
    {
      "epoch": 15.260857794794447,
      "grad_norm": 14.159066200256348,
      "learning_rate": 3.728261850433796e-05,
      "loss": 2.0845,
      "step": 394600
    },
    {
      "epoch": 15.26472521947635,
      "grad_norm": 13.833402633666992,
      "learning_rate": 3.727939565043638e-05,
      "loss": 2.0107,
      "step": 394700
    },
    {
      "epoch": 15.268592644158256,
      "grad_norm": 11.91431713104248,
      "learning_rate": 3.727617279653479e-05,
      "loss": 2.0261,
      "step": 394800
    },
    {
      "epoch": 15.272460068840159,
      "grad_norm": 10.82475757598877,
      "learning_rate": 3.72729499426332e-05,
      "loss": 2.0106,
      "step": 394900
    },
    {
      "epoch": 15.276327493522064,
      "grad_norm": 13.39942455291748,
      "learning_rate": 3.7269727088731615e-05,
      "loss": 2.0526,
      "step": 395000
    },
    {
      "epoch": 15.280194918203968,
      "grad_norm": 11.393189430236816,
      "learning_rate": 3.726650423483003e-05,
      "loss": 1.958,
      "step": 395100
    },
    {
      "epoch": 15.284062342885873,
      "grad_norm": 12.480761528015137,
      "learning_rate": 3.7263281380928444e-05,
      "loss": 1.9358,
      "step": 395200
    },
    {
      "epoch": 15.287929767567777,
      "grad_norm": 11.960737228393555,
      "learning_rate": 3.726005852702685e-05,
      "loss": 2.0125,
      "step": 395300
    },
    {
      "epoch": 15.291797192249682,
      "grad_norm": 9.580375671386719,
      "learning_rate": 3.7256835673125267e-05,
      "loss": 1.9744,
      "step": 395400
    },
    {
      "epoch": 15.295664616931585,
      "grad_norm": 11.24442195892334,
      "learning_rate": 3.725361281922368e-05,
      "loss": 2.007,
      "step": 395500
    },
    {
      "epoch": 15.299532041613489,
      "grad_norm": 9.976862907409668,
      "learning_rate": 3.725038996532209e-05,
      "loss": 1.9716,
      "step": 395600
    },
    {
      "epoch": 15.303399466295394,
      "grad_norm": 10.060029983520508,
      "learning_rate": 3.7247167111420504e-05,
      "loss": 2.0547,
      "step": 395700
    },
    {
      "epoch": 15.307266890977298,
      "grad_norm": 10.00253963470459,
      "learning_rate": 3.724394425751892e-05,
      "loss": 2.0225,
      "step": 395800
    },
    {
      "epoch": 15.311134315659203,
      "grad_norm": 12.594795227050781,
      "learning_rate": 3.7240721403617333e-05,
      "loss": 2.0664,
      "step": 395900
    },
    {
      "epoch": 15.315001740341106,
      "grad_norm": 11.001089096069336,
      "learning_rate": 3.723749854971574e-05,
      "loss": 1.9966,
      "step": 396000
    },
    {
      "epoch": 15.318869165023012,
      "grad_norm": 12.14818000793457,
      "learning_rate": 3.7234275695814156e-05,
      "loss": 2.0761,
      "step": 396100
    },
    {
      "epoch": 15.322736589704915,
      "grad_norm": 13.900445938110352,
      "learning_rate": 3.723105284191257e-05,
      "loss": 2.0612,
      "step": 396200
    },
    {
      "epoch": 15.32660401438682,
      "grad_norm": 12.116633415222168,
      "learning_rate": 3.7227829988010986e-05,
      "loss": 1.9269,
      "step": 396300
    },
    {
      "epoch": 15.330471439068724,
      "grad_norm": 9.568354606628418,
      "learning_rate": 3.7224607134109393e-05,
      "loss": 1.9865,
      "step": 396400
    },
    {
      "epoch": 15.334338863750629,
      "grad_norm": 12.711012840270996,
      "learning_rate": 3.722138428020781e-05,
      "loss": 2.0339,
      "step": 396500
    },
    {
      "epoch": 15.338206288432533,
      "grad_norm": 13.762995719909668,
      "learning_rate": 3.721816142630622e-05,
      "loss": 2.1025,
      "step": 396600
    },
    {
      "epoch": 15.342073713114438,
      "grad_norm": 19.821212768554688,
      "learning_rate": 3.721493857240464e-05,
      "loss": 2.0373,
      "step": 396700
    },
    {
      "epoch": 15.345941137796341,
      "grad_norm": 13.146659851074219,
      "learning_rate": 3.7211715718503046e-05,
      "loss": 2.025,
      "step": 396800
    },
    {
      "epoch": 15.349808562478247,
      "grad_norm": 10.875667572021484,
      "learning_rate": 3.720849286460146e-05,
      "loss": 2.0241,
      "step": 396900
    },
    {
      "epoch": 15.35367598716015,
      "grad_norm": 13.299224853515625,
      "learning_rate": 3.7205270010699875e-05,
      "loss": 2.0698,
      "step": 397000
    },
    {
      "epoch": 15.357543411842054,
      "grad_norm": 9.376497268676758,
      "learning_rate": 3.720204715679829e-05,
      "loss": 2.0459,
      "step": 397100
    },
    {
      "epoch": 15.361410836523959,
      "grad_norm": 15.423067092895508,
      "learning_rate": 3.71988243028967e-05,
      "loss": 2.1027,
      "step": 397200
    },
    {
      "epoch": 15.365278261205862,
      "grad_norm": 13.892753601074219,
      "learning_rate": 3.719560144899511e-05,
      "loss": 2.0018,
      "step": 397300
    },
    {
      "epoch": 15.369145685887768,
      "grad_norm": 11.782530784606934,
      "learning_rate": 3.719237859509353e-05,
      "loss": 2.1128,
      "step": 397400
    },
    {
      "epoch": 15.373013110569671,
      "grad_norm": 11.488407135009766,
      "learning_rate": 3.718915574119194e-05,
      "loss": 2.0857,
      "step": 397500
    },
    {
      "epoch": 15.376880535251576,
      "grad_norm": 12.839489936828613,
      "learning_rate": 3.7185932887290357e-05,
      "loss": 2.0359,
      "step": 397600
    },
    {
      "epoch": 15.38074795993348,
      "grad_norm": 11.063057899475098,
      "learning_rate": 3.718271003338877e-05,
      "loss": 2.0361,
      "step": 397700
    },
    {
      "epoch": 15.384615384615385,
      "grad_norm": 12.945592880249023,
      "learning_rate": 3.717948717948718e-05,
      "loss": 2.0059,
      "step": 397800
    },
    {
      "epoch": 15.388482809297289,
      "grad_norm": 9.23487663269043,
      "learning_rate": 3.7176264325585594e-05,
      "loss": 1.9947,
      "step": 397900
    },
    {
      "epoch": 15.392350233979194,
      "grad_norm": 12.064351081848145,
      "learning_rate": 3.717304147168401e-05,
      "loss": 2.0434,
      "step": 398000
    },
    {
      "epoch": 15.396217658661097,
      "grad_norm": 10.198315620422363,
      "learning_rate": 3.716981861778242e-05,
      "loss": 2.0204,
      "step": 398100
    },
    {
      "epoch": 15.400085083343003,
      "grad_norm": 10.903277397155762,
      "learning_rate": 3.716659576388084e-05,
      "loss": 2.0229,
      "step": 398200
    },
    {
      "epoch": 15.403952508024906,
      "grad_norm": 12.230273246765137,
      "learning_rate": 3.7163372909979246e-05,
      "loss": 1.9292,
      "step": 398300
    },
    {
      "epoch": 15.407819932706811,
      "grad_norm": 12.8617525100708,
      "learning_rate": 3.716015005607766e-05,
      "loss": 2.0222,
      "step": 398400
    },
    {
      "epoch": 15.411687357388715,
      "grad_norm": 11.33421516418457,
      "learning_rate": 3.7156927202176075e-05,
      "loss": 2.0799,
      "step": 398500
    },
    {
      "epoch": 15.41555478207062,
      "grad_norm": 9.113116264343262,
      "learning_rate": 3.715370434827449e-05,
      "loss": 1.9257,
      "step": 398600
    },
    {
      "epoch": 15.419422206752524,
      "grad_norm": 14.33721923828125,
      "learning_rate": 3.71504814943729e-05,
      "loss": 1.953,
      "step": 398700
    },
    {
      "epoch": 15.423289631434427,
      "grad_norm": 11.772531509399414,
      "learning_rate": 3.714725864047131e-05,
      "loss": 2.1945,
      "step": 398800
    },
    {
      "epoch": 15.427157056116332,
      "grad_norm": 11.686588287353516,
      "learning_rate": 3.714403578656973e-05,
      "loss": 1.9185,
      "step": 398900
    },
    {
      "epoch": 15.431024480798236,
      "grad_norm": 10.047111511230469,
      "learning_rate": 3.714081293266814e-05,
      "loss": 2.0448,
      "step": 399000
    },
    {
      "epoch": 15.434891905480141,
      "grad_norm": 12.178115844726562,
      "learning_rate": 3.713759007876655e-05,
      "loss": 2.1002,
      "step": 399100
    },
    {
      "epoch": 15.438759330162044,
      "grad_norm": 13.188581466674805,
      "learning_rate": 3.7134367224864965e-05,
      "loss": 2.094,
      "step": 399200
    },
    {
      "epoch": 15.44262675484395,
      "grad_norm": 9.559576988220215,
      "learning_rate": 3.713114437096338e-05,
      "loss": 1.9761,
      "step": 399300
    },
    {
      "epoch": 15.446494179525853,
      "grad_norm": 12.340450286865234,
      "learning_rate": 3.7127921517061794e-05,
      "loss": 1.9184,
      "step": 399400
    },
    {
      "epoch": 15.450361604207759,
      "grad_norm": 12.920228958129883,
      "learning_rate": 3.71246986631602e-05,
      "loss": 2.1296,
      "step": 399500
    },
    {
      "epoch": 15.454229028889662,
      "grad_norm": 10.031468391418457,
      "learning_rate": 3.712147580925862e-05,
      "loss": 2.0792,
      "step": 399600
    },
    {
      "epoch": 15.458096453571567,
      "grad_norm": 12.504465103149414,
      "learning_rate": 3.711825295535703e-05,
      "loss": 2.0207,
      "step": 399700
    },
    {
      "epoch": 15.46196387825347,
      "grad_norm": 12.767743110656738,
      "learning_rate": 3.7115030101455446e-05,
      "loss": 2.0455,
      "step": 399800
    },
    {
      "epoch": 15.465831302935376,
      "grad_norm": 11.208868026733398,
      "learning_rate": 3.7111807247553854e-05,
      "loss": 1.9721,
      "step": 399900
    },
    {
      "epoch": 15.46969872761728,
      "grad_norm": 10.396382331848145,
      "learning_rate": 3.710858439365227e-05,
      "loss": 1.9395,
      "step": 400000
    },
    {
      "epoch": 15.473566152299185,
      "grad_norm": 10.711552619934082,
      "learning_rate": 3.7105361539750684e-05,
      "loss": 1.9616,
      "step": 400100
    },
    {
      "epoch": 15.477433576981088,
      "grad_norm": 11.62398910522461,
      "learning_rate": 3.71021386858491e-05,
      "loss": 2.1018,
      "step": 400200
    },
    {
      "epoch": 15.481301001662992,
      "grad_norm": 9.843172073364258,
      "learning_rate": 3.7098915831947506e-05,
      "loss": 1.9973,
      "step": 400300
    },
    {
      "epoch": 15.485168426344897,
      "grad_norm": 14.693943977355957,
      "learning_rate": 3.709569297804592e-05,
      "loss": 2.0031,
      "step": 400400
    },
    {
      "epoch": 15.4890358510268,
      "grad_norm": 15.259029388427734,
      "learning_rate": 3.7092470124144336e-05,
      "loss": 2.0758,
      "step": 400500
    },
    {
      "epoch": 15.492903275708706,
      "grad_norm": 10.4456148147583,
      "learning_rate": 3.708924727024275e-05,
      "loss": 1.9811,
      "step": 400600
    },
    {
      "epoch": 15.49677070039061,
      "grad_norm": 10.91984748840332,
      "learning_rate": 3.708602441634116e-05,
      "loss": 1.982,
      "step": 400700
    },
    {
      "epoch": 15.500638125072514,
      "grad_norm": 10.409149169921875,
      "learning_rate": 3.708280156243957e-05,
      "loss": 2.0687,
      "step": 400800
    },
    {
      "epoch": 15.504505549754418,
      "grad_norm": 9.495760917663574,
      "learning_rate": 3.707957870853799e-05,
      "loss": 2.0177,
      "step": 400900
    },
    {
      "epoch": 15.508372974436323,
      "grad_norm": 10.635172843933105,
      "learning_rate": 3.70763558546364e-05,
      "loss": 2.0539,
      "step": 401000
    },
    {
      "epoch": 15.512240399118227,
      "grad_norm": 9.844493865966797,
      "learning_rate": 3.707313300073481e-05,
      "loss": 2.0919,
      "step": 401100
    },
    {
      "epoch": 15.516107823800132,
      "grad_norm": 13.009212493896484,
      "learning_rate": 3.7069910146833225e-05,
      "loss": 1.9858,
      "step": 401200
    },
    {
      "epoch": 15.519975248482035,
      "grad_norm": 11.502352714538574,
      "learning_rate": 3.706668729293164e-05,
      "loss": 1.9935,
      "step": 401300
    },
    {
      "epoch": 15.52384267316394,
      "grad_norm": 10.735321998596191,
      "learning_rate": 3.706346443903005e-05,
      "loss": 2.0384,
      "step": 401400
    },
    {
      "epoch": 15.527710097845844,
      "grad_norm": 14.190430641174316,
      "learning_rate": 3.706024158512846e-05,
      "loss": 1.9983,
      "step": 401500
    },
    {
      "epoch": 15.53157752252775,
      "grad_norm": 13.186361312866211,
      "learning_rate": 3.705701873122688e-05,
      "loss": 2.0686,
      "step": 401600
    },
    {
      "epoch": 15.535444947209653,
      "grad_norm": 12.119796752929688,
      "learning_rate": 3.705379587732529e-05,
      "loss": 2.1669,
      "step": 401700
    },
    {
      "epoch": 15.539312371891558,
      "grad_norm": 12.712727546691895,
      "learning_rate": 3.70505730234237e-05,
      "loss": 2.0314,
      "step": 401800
    },
    {
      "epoch": 15.543179796573462,
      "grad_norm": 10.822986602783203,
      "learning_rate": 3.7047350169522115e-05,
      "loss": 1.9678,
      "step": 401900
    },
    {
      "epoch": 15.547047221255365,
      "grad_norm": 11.841835021972656,
      "learning_rate": 3.704412731562053e-05,
      "loss": 1.9802,
      "step": 402000
    },
    {
      "epoch": 15.55091464593727,
      "grad_norm": 11.64769172668457,
      "learning_rate": 3.7040904461718944e-05,
      "loss": 2.066,
      "step": 402100
    },
    {
      "epoch": 15.554782070619174,
      "grad_norm": 10.253596305847168,
      "learning_rate": 3.703768160781735e-05,
      "loss": 2.0183,
      "step": 402200
    },
    {
      "epoch": 15.55864949530108,
      "grad_norm": 17.186777114868164,
      "learning_rate": 3.703445875391577e-05,
      "loss": 1.9443,
      "step": 402300
    },
    {
      "epoch": 15.562516919982983,
      "grad_norm": 11.575418472290039,
      "learning_rate": 3.703123590001418e-05,
      "loss": 2.1163,
      "step": 402400
    },
    {
      "epoch": 15.566384344664888,
      "grad_norm": 11.792813301086426,
      "learning_rate": 3.7028013046112596e-05,
      "loss": 1.977,
      "step": 402500
    },
    {
      "epoch": 15.570251769346791,
      "grad_norm": 11.338386535644531,
      "learning_rate": 3.7024790192211004e-05,
      "loss": 2.057,
      "step": 402600
    },
    {
      "epoch": 15.574119194028697,
      "grad_norm": 9.48423957824707,
      "learning_rate": 3.702156733830942e-05,
      "loss": 2.0574,
      "step": 402700
    },
    {
      "epoch": 15.5779866187106,
      "grad_norm": 11.34151554107666,
      "learning_rate": 3.7018344484407834e-05,
      "loss": 2.0128,
      "step": 402800
    },
    {
      "epoch": 15.581854043392505,
      "grad_norm": 9.944765090942383,
      "learning_rate": 3.701512163050625e-05,
      "loss": 2.0327,
      "step": 402900
    },
    {
      "epoch": 15.585721468074409,
      "grad_norm": 10.994424819946289,
      "learning_rate": 3.7011898776604656e-05,
      "loss": 1.9873,
      "step": 403000
    },
    {
      "epoch": 15.589588892756314,
      "grad_norm": 11.670552253723145,
      "learning_rate": 3.700867592270307e-05,
      "loss": 1.9867,
      "step": 403100
    },
    {
      "epoch": 15.593456317438218,
      "grad_norm": 14.354759216308594,
      "learning_rate": 3.7005453068801486e-05,
      "loss": 2.0902,
      "step": 403200
    },
    {
      "epoch": 15.597323742120123,
      "grad_norm": 11.807585716247559,
      "learning_rate": 3.70022302148999e-05,
      "loss": 2.0484,
      "step": 403300
    },
    {
      "epoch": 15.601191166802026,
      "grad_norm": 14.197295188903809,
      "learning_rate": 3.699900736099831e-05,
      "loss": 2.0203,
      "step": 403400
    },
    {
      "epoch": 15.605058591483932,
      "grad_norm": 14.982890129089355,
      "learning_rate": 3.699578450709672e-05,
      "loss": 2.0392,
      "step": 403500
    },
    {
      "epoch": 15.608926016165835,
      "grad_norm": 17.180795669555664,
      "learning_rate": 3.699256165319514e-05,
      "loss": 2.0342,
      "step": 403600
    },
    {
      "epoch": 15.612793440847739,
      "grad_norm": 12.001324653625488,
      "learning_rate": 3.698933879929355e-05,
      "loss": 2.0445,
      "step": 403700
    },
    {
      "epoch": 15.616660865529644,
      "grad_norm": 14.167016983032227,
      "learning_rate": 3.698611594539196e-05,
      "loss": 2.0403,
      "step": 403800
    },
    {
      "epoch": 15.620528290211547,
      "grad_norm": 10.255851745605469,
      "learning_rate": 3.6982893091490375e-05,
      "loss": 2.016,
      "step": 403900
    },
    {
      "epoch": 15.624395714893453,
      "grad_norm": 10.71552562713623,
      "learning_rate": 3.697967023758879e-05,
      "loss": 2.0231,
      "step": 404000
    },
    {
      "epoch": 15.628263139575356,
      "grad_norm": 10.076918601989746,
      "learning_rate": 3.6976447383687205e-05,
      "loss": 2.0151,
      "step": 404100
    },
    {
      "epoch": 15.632130564257261,
      "grad_norm": 9.600007057189941,
      "learning_rate": 3.697322452978561e-05,
      "loss": 2.0641,
      "step": 404200
    },
    {
      "epoch": 15.635997988939165,
      "grad_norm": 13.289969444274902,
      "learning_rate": 3.697000167588403e-05,
      "loss": 2.0146,
      "step": 404300
    },
    {
      "epoch": 15.63986541362107,
      "grad_norm": 11.969416618347168,
      "learning_rate": 3.696677882198244e-05,
      "loss": 2.0599,
      "step": 404400
    },
    {
      "epoch": 15.643732838302974,
      "grad_norm": 11.050987243652344,
      "learning_rate": 3.696355596808086e-05,
      "loss": 1.999,
      "step": 404500
    },
    {
      "epoch": 15.647600262984879,
      "grad_norm": 14.773950576782227,
      "learning_rate": 3.696033311417927e-05,
      "loss": 2.0087,
      "step": 404600
    },
    {
      "epoch": 15.651467687666782,
      "grad_norm": 9.594660758972168,
      "learning_rate": 3.6957110260277686e-05,
      "loss": 1.9772,
      "step": 404700
    },
    {
      "epoch": 15.655335112348688,
      "grad_norm": 12.380745887756348,
      "learning_rate": 3.6953887406376094e-05,
      "loss": 2.0637,
      "step": 404800
    },
    {
      "epoch": 15.659202537030591,
      "grad_norm": 11.068830490112305,
      "learning_rate": 3.695066455247451e-05,
      "loss": 1.9554,
      "step": 404900
    },
    {
      "epoch": 15.663069961712496,
      "grad_norm": 10.029542922973633,
      "learning_rate": 3.6947441698572924e-05,
      "loss": 1.9788,
      "step": 405000
    },
    {
      "epoch": 15.6669373863944,
      "grad_norm": 11.399308204650879,
      "learning_rate": 3.694421884467134e-05,
      "loss": 2.0121,
      "step": 405100
    },
    {
      "epoch": 15.670804811076305,
      "grad_norm": 13.713774681091309,
      "learning_rate": 3.694099599076975e-05,
      "loss": 2.0289,
      "step": 405200
    },
    {
      "epoch": 15.674672235758209,
      "grad_norm": 13.941951751708984,
      "learning_rate": 3.693777313686816e-05,
      "loss": 1.9538,
      "step": 405300
    },
    {
      "epoch": 15.678539660440112,
      "grad_norm": 8.13825798034668,
      "learning_rate": 3.6934550282966576e-05,
      "loss": 1.9959,
      "step": 405400
    },
    {
      "epoch": 15.682407085122017,
      "grad_norm": 14.92474365234375,
      "learning_rate": 3.693132742906499e-05,
      "loss": 2.1017,
      "step": 405500
    },
    {
      "epoch": 15.686274509803921,
      "grad_norm": 10.798905372619629,
      "learning_rate": 3.6928104575163405e-05,
      "loss": 2.0672,
      "step": 405600
    },
    {
      "epoch": 15.690141934485826,
      "grad_norm": 10.512929916381836,
      "learning_rate": 3.692488172126181e-05,
      "loss": 1.9138,
      "step": 405700
    },
    {
      "epoch": 15.69400935916773,
      "grad_norm": 12.680225372314453,
      "learning_rate": 3.692165886736023e-05,
      "loss": 1.9652,
      "step": 405800
    },
    {
      "epoch": 15.697876783849635,
      "grad_norm": 9.498578071594238,
      "learning_rate": 3.691843601345864e-05,
      "loss": 2.0587,
      "step": 405900
    },
    {
      "epoch": 15.701744208531538,
      "grad_norm": 13.163625717163086,
      "learning_rate": 3.691521315955706e-05,
      "loss": 2.0253,
      "step": 406000
    },
    {
      "epoch": 15.705611633213444,
      "grad_norm": 11.665506362915039,
      "learning_rate": 3.6911990305655465e-05,
      "loss": 1.9749,
      "step": 406100
    },
    {
      "epoch": 15.709479057895347,
      "grad_norm": 13.792552947998047,
      "learning_rate": 3.690876745175388e-05,
      "loss": 2.0524,
      "step": 406200
    },
    {
      "epoch": 15.713346482577252,
      "grad_norm": 13.37619400024414,
      "learning_rate": 3.6905544597852295e-05,
      "loss": 2.0597,
      "step": 406300
    },
    {
      "epoch": 15.717213907259156,
      "grad_norm": 9.90560531616211,
      "learning_rate": 3.690232174395071e-05,
      "loss": 2.0397,
      "step": 406400
    },
    {
      "epoch": 15.721081331941061,
      "grad_norm": 13.981917381286621,
      "learning_rate": 3.689909889004912e-05,
      "loss": 2.0286,
      "step": 406500
    },
    {
      "epoch": 15.724948756622965,
      "grad_norm": 14.038962364196777,
      "learning_rate": 3.689587603614753e-05,
      "loss": 2.1083,
      "step": 406600
    },
    {
      "epoch": 15.72881618130487,
      "grad_norm": 9.922000885009766,
      "learning_rate": 3.689265318224595e-05,
      "loss": 2.0045,
      "step": 406700
    },
    {
      "epoch": 15.732683605986773,
      "grad_norm": 8.572482109069824,
      "learning_rate": 3.688943032834436e-05,
      "loss": 1.9306,
      "step": 406800
    },
    {
      "epoch": 15.736551030668679,
      "grad_norm": 10.753363609313965,
      "learning_rate": 3.688620747444277e-05,
      "loss": 2.0207,
      "step": 406900
    },
    {
      "epoch": 15.740418455350582,
      "grad_norm": 14.265846252441406,
      "learning_rate": 3.6882984620541184e-05,
      "loss": 2.0213,
      "step": 407000
    },
    {
      "epoch": 15.744285880032486,
      "grad_norm": 8.211458206176758,
      "learning_rate": 3.68797617666396e-05,
      "loss": 2.0377,
      "step": 407100
    },
    {
      "epoch": 15.748153304714391,
      "grad_norm": 10.494550704956055,
      "learning_rate": 3.687653891273801e-05,
      "loss": 2.0986,
      "step": 407200
    },
    {
      "epoch": 15.752020729396294,
      "grad_norm": 9.633942604064941,
      "learning_rate": 3.687331605883642e-05,
      "loss": 2.009,
      "step": 407300
    },
    {
      "epoch": 15.7558881540782,
      "grad_norm": 11.232032775878906,
      "learning_rate": 3.6870093204934836e-05,
      "loss": 2.1008,
      "step": 407400
    },
    {
      "epoch": 15.759755578760103,
      "grad_norm": 7.035563945770264,
      "learning_rate": 3.686687035103325e-05,
      "loss": 2.0257,
      "step": 407500
    },
    {
      "epoch": 15.763623003442008,
      "grad_norm": 12.136284828186035,
      "learning_rate": 3.686364749713166e-05,
      "loss": 2.1705,
      "step": 407600
    },
    {
      "epoch": 15.767490428123912,
      "grad_norm": 13.023031234741211,
      "learning_rate": 3.6860424643230074e-05,
      "loss": 2.0137,
      "step": 407700
    },
    {
      "epoch": 15.771357852805817,
      "grad_norm": 10.926440238952637,
      "learning_rate": 3.685720178932849e-05,
      "loss": 2.0245,
      "step": 407800
    },
    {
      "epoch": 15.77522527748772,
      "grad_norm": 13.79948902130127,
      "learning_rate": 3.68539789354269e-05,
      "loss": 2.0012,
      "step": 407900
    },
    {
      "epoch": 15.779092702169626,
      "grad_norm": 11.027352333068848,
      "learning_rate": 3.685075608152531e-05,
      "loss": 2.0297,
      "step": 408000
    },
    {
      "epoch": 15.78296012685153,
      "grad_norm": 11.847843170166016,
      "learning_rate": 3.6847533227623726e-05,
      "loss": 1.9777,
      "step": 408100
    },
    {
      "epoch": 15.786827551533435,
      "grad_norm": 9.643610954284668,
      "learning_rate": 3.684431037372214e-05,
      "loss": 2.0967,
      "step": 408200
    },
    {
      "epoch": 15.790694976215338,
      "grad_norm": 11.926681518554688,
      "learning_rate": 3.6841087519820555e-05,
      "loss": 2.0811,
      "step": 408300
    },
    {
      "epoch": 15.794562400897242,
      "grad_norm": 10.32099723815918,
      "learning_rate": 3.683786466591896e-05,
      "loss": 2.0009,
      "step": 408400
    },
    {
      "epoch": 15.798429825579147,
      "grad_norm": 17.515350341796875,
      "learning_rate": 3.683464181201738e-05,
      "loss": 1.9493,
      "step": 408500
    },
    {
      "epoch": 15.80229725026105,
      "grad_norm": 13.235429763793945,
      "learning_rate": 3.683141895811579e-05,
      "loss": 2.0087,
      "step": 408600
    },
    {
      "epoch": 15.806164674942956,
      "grad_norm": 11.96848201751709,
      "learning_rate": 3.682819610421421e-05,
      "loss": 1.9636,
      "step": 408700
    },
    {
      "epoch": 15.81003209962486,
      "grad_norm": 10.47455883026123,
      "learning_rate": 3.6824973250312615e-05,
      "loss": 2.0249,
      "step": 408800
    },
    {
      "epoch": 15.813899524306764,
      "grad_norm": 9.271896362304688,
      "learning_rate": 3.682175039641103e-05,
      "loss": 2.0667,
      "step": 408900
    },
    {
      "epoch": 15.817766948988668,
      "grad_norm": 11.58320140838623,
      "learning_rate": 3.6818527542509445e-05,
      "loss": 1.938,
      "step": 409000
    },
    {
      "epoch": 15.821634373670573,
      "grad_norm": 10.677701950073242,
      "learning_rate": 3.681530468860786e-05,
      "loss": 1.9802,
      "step": 409100
    },
    {
      "epoch": 15.825501798352477,
      "grad_norm": 13.678606033325195,
      "learning_rate": 3.681208183470627e-05,
      "loss": 2.103,
      "step": 409200
    },
    {
      "epoch": 15.829369223034382,
      "grad_norm": 8.725906372070312,
      "learning_rate": 3.680885898080468e-05,
      "loss": 1.8876,
      "step": 409300
    },
    {
      "epoch": 15.833236647716285,
      "grad_norm": 10.381745338439941,
      "learning_rate": 3.68056361269031e-05,
      "loss": 2.0944,
      "step": 409400
    },
    {
      "epoch": 15.83710407239819,
      "grad_norm": 15.390759468078613,
      "learning_rate": 3.680241327300151e-05,
      "loss": 1.9815,
      "step": 409500
    },
    {
      "epoch": 15.840971497080094,
      "grad_norm": 10.005141258239746,
      "learning_rate": 3.679919041909992e-05,
      "loss": 1.969,
      "step": 409600
    },
    {
      "epoch": 15.844838921762,
      "grad_norm": 10.065951347351074,
      "learning_rate": 3.6795967565198334e-05,
      "loss": 2.1085,
      "step": 409700
    },
    {
      "epoch": 15.848706346443903,
      "grad_norm": 15.48322868347168,
      "learning_rate": 3.679274471129675e-05,
      "loss": 2.138,
      "step": 409800
    },
    {
      "epoch": 15.852573771125808,
      "grad_norm": 11.425226211547852,
      "learning_rate": 3.6789521857395164e-05,
      "loss": 2.072,
      "step": 409900
    },
    {
      "epoch": 15.856441195807712,
      "grad_norm": 10.334993362426758,
      "learning_rate": 3.678629900349357e-05,
      "loss": 2.0899,
      "step": 410000
    },
    {
      "epoch": 15.860308620489615,
      "grad_norm": 10.76663589477539,
      "learning_rate": 3.6783076149591986e-05,
      "loss": 1.9956,
      "step": 410100
    },
    {
      "epoch": 15.86417604517152,
      "grad_norm": 10.83914852142334,
      "learning_rate": 3.67798532956904e-05,
      "loss": 2.1261,
      "step": 410200
    },
    {
      "epoch": 15.868043469853424,
      "grad_norm": 13.40495777130127,
      "learning_rate": 3.677663044178881e-05,
      "loss": 2.0183,
      "step": 410300
    },
    {
      "epoch": 15.871910894535329,
      "grad_norm": 11.435572624206543,
      "learning_rate": 3.6773407587887224e-05,
      "loss": 2.0471,
      "step": 410400
    },
    {
      "epoch": 15.875778319217233,
      "grad_norm": 9.702546119689941,
      "learning_rate": 3.677018473398564e-05,
      "loss": 2.1204,
      "step": 410500
    },
    {
      "epoch": 15.879645743899138,
      "grad_norm": 9.24062442779541,
      "learning_rate": 3.676696188008405e-05,
      "loss": 2.0639,
      "step": 410600
    },
    {
      "epoch": 15.883513168581041,
      "grad_norm": 15.438364028930664,
      "learning_rate": 3.676373902618246e-05,
      "loss": 2.0119,
      "step": 410700
    },
    {
      "epoch": 15.887380593262947,
      "grad_norm": 11.796067237854004,
      "learning_rate": 3.6760516172280876e-05,
      "loss": 1.9061,
      "step": 410800
    },
    {
      "epoch": 15.89124801794485,
      "grad_norm": 9.551130294799805,
      "learning_rate": 3.675729331837929e-05,
      "loss": 2.0212,
      "step": 410900
    },
    {
      "epoch": 15.895115442626755,
      "grad_norm": 12.379215240478516,
      "learning_rate": 3.6754070464477705e-05,
      "loss": 2.0744,
      "step": 411000
    },
    {
      "epoch": 15.898982867308659,
      "grad_norm": 13.582704544067383,
      "learning_rate": 3.675084761057612e-05,
      "loss": 2.0229,
      "step": 411100
    },
    {
      "epoch": 15.902850291990564,
      "grad_norm": 14.319075584411621,
      "learning_rate": 3.6747624756674535e-05,
      "loss": 2.0541,
      "step": 411200
    },
    {
      "epoch": 15.906717716672468,
      "grad_norm": 10.975238800048828,
      "learning_rate": 3.674440190277294e-05,
      "loss": 2.0015,
      "step": 411300
    },
    {
      "epoch": 15.910585141354373,
      "grad_norm": 10.185163497924805,
      "learning_rate": 3.674117904887136e-05,
      "loss": 2.0045,
      "step": 411400
    },
    {
      "epoch": 15.914452566036276,
      "grad_norm": 10.022396087646484,
      "learning_rate": 3.673795619496977e-05,
      "loss": 2.0694,
      "step": 411500
    },
    {
      "epoch": 15.918319990718182,
      "grad_norm": 9.196491241455078,
      "learning_rate": 3.673473334106819e-05,
      "loss": 2.0685,
      "step": 411600
    },
    {
      "epoch": 15.922187415400085,
      "grad_norm": 11.888075828552246,
      "learning_rate": 3.67315104871666e-05,
      "loss": 2.0863,
      "step": 411700
    },
    {
      "epoch": 15.926054840081989,
      "grad_norm": 10.46728229522705,
      "learning_rate": 3.6728287633265016e-05,
      "loss": 1.9165,
      "step": 411800
    },
    {
      "epoch": 15.929922264763894,
      "grad_norm": 12.25393295288086,
      "learning_rate": 3.6725064779363424e-05,
      "loss": 2.0618,
      "step": 411900
    },
    {
      "epoch": 15.933789689445797,
      "grad_norm": 10.10190486907959,
      "learning_rate": 3.672184192546184e-05,
      "loss": 1.979,
      "step": 412000
    },
    {
      "epoch": 15.937657114127703,
      "grad_norm": 11.585561752319336,
      "learning_rate": 3.6718619071560254e-05,
      "loss": 1.9453,
      "step": 412100
    },
    {
      "epoch": 15.941524538809606,
      "grad_norm": 11.251741409301758,
      "learning_rate": 3.671539621765867e-05,
      "loss": 2.059,
      "step": 412200
    },
    {
      "epoch": 15.945391963491511,
      "grad_norm": 11.989375114440918,
      "learning_rate": 3.6712173363757076e-05,
      "loss": 2.037,
      "step": 412300
    },
    {
      "epoch": 15.949259388173415,
      "grad_norm": 10.103635787963867,
      "learning_rate": 3.670895050985549e-05,
      "loss": 1.9734,
      "step": 412400
    },
    {
      "epoch": 15.95312681285532,
      "grad_norm": 13.196846008300781,
      "learning_rate": 3.6705727655953906e-05,
      "loss": 2.068,
      "step": 412500
    },
    {
      "epoch": 15.956994237537224,
      "grad_norm": 11.249141693115234,
      "learning_rate": 3.670250480205232e-05,
      "loss": 2.0506,
      "step": 412600
    },
    {
      "epoch": 15.960861662219129,
      "grad_norm": 12.372084617614746,
      "learning_rate": 3.669928194815073e-05,
      "loss": 1.9857,
      "step": 412700
    },
    {
      "epoch": 15.964729086901032,
      "grad_norm": 10.89294147491455,
      "learning_rate": 3.669605909424914e-05,
      "loss": 2.0432,
      "step": 412800
    },
    {
      "epoch": 15.968596511582938,
      "grad_norm": 14.182608604431152,
      "learning_rate": 3.669283624034756e-05,
      "loss": 2.155,
      "step": 412900
    },
    {
      "epoch": 15.972463936264841,
      "grad_norm": 14.017027854919434,
      "learning_rate": 3.668961338644597e-05,
      "loss": 2.0991,
      "step": 413000
    },
    {
      "epoch": 15.976331360946746,
      "grad_norm": 9.149131774902344,
      "learning_rate": 3.668639053254438e-05,
      "loss": 2.1015,
      "step": 413100
    },
    {
      "epoch": 15.98019878562865,
      "grad_norm": 11.197954177856445,
      "learning_rate": 3.6683167678642795e-05,
      "loss": 2.0321,
      "step": 413200
    },
    {
      "epoch": 15.984066210310555,
      "grad_norm": 11.351103782653809,
      "learning_rate": 3.667994482474121e-05,
      "loss": 1.9162,
      "step": 413300
    },
    {
      "epoch": 15.987933634992459,
      "grad_norm": 14.79898452758789,
      "learning_rate": 3.667672197083962e-05,
      "loss": 2.0248,
      "step": 413400
    },
    {
      "epoch": 15.991801059674362,
      "grad_norm": 11.407318115234375,
      "learning_rate": 3.667349911693803e-05,
      "loss": 2.0838,
      "step": 413500
    },
    {
      "epoch": 15.995668484356267,
      "grad_norm": 11.840815544128418,
      "learning_rate": 3.667027626303645e-05,
      "loss": 1.9548,
      "step": 413600
    },
    {
      "epoch": 15.99953590903817,
      "grad_norm": 13.583636283874512,
      "learning_rate": 3.666705340913486e-05,
      "loss": 2.0026,
      "step": 413700
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.9182252883911133,
      "eval_runtime": 2.8983,
      "eval_samples_per_second": 469.585,
      "eval_steps_per_second": 469.585,
      "step": 413712
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.8371092081069946,
      "eval_runtime": 55.1889,
      "eval_samples_per_second": 468.518,
      "eval_steps_per_second": 468.518,
      "step": 413712
    },
    {
      "epoch": 16.003403333720076,
      "grad_norm": 12.021646499633789,
      "learning_rate": 3.666383055523327e-05,
      "loss": 2.0095,
      "step": 413800
    },
    {
      "epoch": 16.00727075840198,
      "grad_norm": 10.516844749450684,
      "learning_rate": 3.6660607701331685e-05,
      "loss": 2.0639,
      "step": 413900
    },
    {
      "epoch": 16.011138183083883,
      "grad_norm": 12.123835563659668,
      "learning_rate": 3.66573848474301e-05,
      "loss": 2.0083,
      "step": 414000
    },
    {
      "epoch": 16.01500560776579,
      "grad_norm": 9.595937728881836,
      "learning_rate": 3.6654161993528514e-05,
      "loss": 1.9194,
      "step": 414100
    },
    {
      "epoch": 16.018873032447694,
      "grad_norm": 9.856976509094238,
      "learning_rate": 3.665093913962692e-05,
      "loss": 2.0184,
      "step": 414200
    },
    {
      "epoch": 16.022740457129597,
      "grad_norm": 13.659892082214355,
      "learning_rate": 3.664771628572534e-05,
      "loss": 2.0195,
      "step": 414300
    },
    {
      "epoch": 16.0266078818115,
      "grad_norm": 17.66632652282715,
      "learning_rate": 3.664449343182375e-05,
      "loss": 2.0849,
      "step": 414400
    },
    {
      "epoch": 16.030475306493408,
      "grad_norm": 10.814037322998047,
      "learning_rate": 3.6641270577922166e-05,
      "loss": 1.9586,
      "step": 414500
    },
    {
      "epoch": 16.03434273117531,
      "grad_norm": 12.750444412231445,
      "learning_rate": 3.6638047724020574e-05,
      "loss": 2.0355,
      "step": 414600
    },
    {
      "epoch": 16.038210155857215,
      "grad_norm": 13.869945526123047,
      "learning_rate": 3.663482487011899e-05,
      "loss": 2.1085,
      "step": 414700
    },
    {
      "epoch": 16.042077580539118,
      "grad_norm": 12.92463207244873,
      "learning_rate": 3.6631602016217403e-05,
      "loss": 2.0223,
      "step": 414800
    },
    {
      "epoch": 16.045945005221025,
      "grad_norm": 11.134870529174805,
      "learning_rate": 3.662837916231582e-05,
      "loss": 2.0858,
      "step": 414900
    },
    {
      "epoch": 16.04981242990293,
      "grad_norm": 13.291902542114258,
      "learning_rate": 3.6625156308414226e-05,
      "loss": 2.0291,
      "step": 415000
    },
    {
      "epoch": 16.053679854584832,
      "grad_norm": 11.962566375732422,
      "learning_rate": 3.662193345451264e-05,
      "loss": 1.9468,
      "step": 415100
    },
    {
      "epoch": 16.057547279266736,
      "grad_norm": 12.650873184204102,
      "learning_rate": 3.6618710600611056e-05,
      "loss": 2.0815,
      "step": 415200
    },
    {
      "epoch": 16.06141470394864,
      "grad_norm": 10.547576904296875,
      "learning_rate": 3.661548774670947e-05,
      "loss": 2.0719,
      "step": 415300
    },
    {
      "epoch": 16.065282128630546,
      "grad_norm": 14.206832885742188,
      "learning_rate": 3.661226489280788e-05,
      "loss": 2.0457,
      "step": 415400
    },
    {
      "epoch": 16.06914955331245,
      "grad_norm": 11.921013832092285,
      "learning_rate": 3.660904203890629e-05,
      "loss": 2.0502,
      "step": 415500
    },
    {
      "epoch": 16.073016977994353,
      "grad_norm": 9.332862854003906,
      "learning_rate": 3.660581918500471e-05,
      "loss": 1.9634,
      "step": 415600
    },
    {
      "epoch": 16.076884402676257,
      "grad_norm": 10.585870742797852,
      "learning_rate": 3.660259633110312e-05,
      "loss": 1.9918,
      "step": 415700
    },
    {
      "epoch": 16.080751827358164,
      "grad_norm": 11.776983261108398,
      "learning_rate": 3.659937347720153e-05,
      "loss": 1.9922,
      "step": 415800
    },
    {
      "epoch": 16.084619252040067,
      "grad_norm": 14.072354316711426,
      "learning_rate": 3.6596150623299945e-05,
      "loss": 2.1028,
      "step": 415900
    },
    {
      "epoch": 16.08848667672197,
      "grad_norm": 11.382548332214355,
      "learning_rate": 3.659292776939836e-05,
      "loss": 1.9613,
      "step": 416000
    },
    {
      "epoch": 16.092354101403874,
      "grad_norm": 12.84201431274414,
      "learning_rate": 3.658970491549677e-05,
      "loss": 1.9726,
      "step": 416100
    },
    {
      "epoch": 16.09622152608578,
      "grad_norm": 9.38213062286377,
      "learning_rate": 3.658648206159518e-05,
      "loss": 2.0145,
      "step": 416200
    },
    {
      "epoch": 16.100088950767685,
      "grad_norm": 9.924878120422363,
      "learning_rate": 3.65832592076936e-05,
      "loss": 2.0808,
      "step": 416300
    },
    {
      "epoch": 16.103956375449588,
      "grad_norm": 11.361387252807617,
      "learning_rate": 3.658003635379201e-05,
      "loss": 1.8625,
      "step": 416400
    },
    {
      "epoch": 16.10782380013149,
      "grad_norm": 11.712323188781738,
      "learning_rate": 3.657681349989042e-05,
      "loss": 1.9946,
      "step": 416500
    },
    {
      "epoch": 16.111691224813395,
      "grad_norm": 10.507194519042969,
      "learning_rate": 3.6573590645988835e-05,
      "loss": 2.0902,
      "step": 416600
    },
    {
      "epoch": 16.115558649495302,
      "grad_norm": 12.679101943969727,
      "learning_rate": 3.657036779208725e-05,
      "loss": 1.9552,
      "step": 416700
    },
    {
      "epoch": 16.119426074177206,
      "grad_norm": 12.28944206237793,
      "learning_rate": 3.6567144938185664e-05,
      "loss": 1.9431,
      "step": 416800
    },
    {
      "epoch": 16.12329349885911,
      "grad_norm": 9.933554649353027,
      "learning_rate": 3.656392208428407e-05,
      "loss": 1.9458,
      "step": 416900
    },
    {
      "epoch": 16.127160923541012,
      "grad_norm": 12.011771202087402,
      "learning_rate": 3.656069923038249e-05,
      "loss": 1.9406,
      "step": 417000
    },
    {
      "epoch": 16.13102834822292,
      "grad_norm": 11.471906661987305,
      "learning_rate": 3.65574763764809e-05,
      "loss": 1.9747,
      "step": 417100
    },
    {
      "epoch": 16.134895772904823,
      "grad_norm": 12.213595390319824,
      "learning_rate": 3.6554253522579316e-05,
      "loss": 1.9629,
      "step": 417200
    },
    {
      "epoch": 16.138763197586727,
      "grad_norm": 13.481765747070312,
      "learning_rate": 3.6551030668677724e-05,
      "loss": 1.8774,
      "step": 417300
    },
    {
      "epoch": 16.14263062226863,
      "grad_norm": 9.103900909423828,
      "learning_rate": 3.654780781477614e-05,
      "loss": 1.9568,
      "step": 417400
    },
    {
      "epoch": 16.146498046950537,
      "grad_norm": 10.227888107299805,
      "learning_rate": 3.6544584960874553e-05,
      "loss": 1.9873,
      "step": 417500
    },
    {
      "epoch": 16.15036547163244,
      "grad_norm": 10.725950241088867,
      "learning_rate": 3.654136210697297e-05,
      "loss": 1.9547,
      "step": 417600
    },
    {
      "epoch": 16.154232896314344,
      "grad_norm": 12.011141777038574,
      "learning_rate": 3.653813925307138e-05,
      "loss": 1.9606,
      "step": 417700
    },
    {
      "epoch": 16.158100320996247,
      "grad_norm": 9.94796085357666,
      "learning_rate": 3.653491639916979e-05,
      "loss": 1.959,
      "step": 417800
    },
    {
      "epoch": 16.161967745678155,
      "grad_norm": 12.412221908569336,
      "learning_rate": 3.6531693545268206e-05,
      "loss": 1.9636,
      "step": 417900
    },
    {
      "epoch": 16.165835170360058,
      "grad_norm": 11.495282173156738,
      "learning_rate": 3.652847069136662e-05,
      "loss": 1.9641,
      "step": 418000
    },
    {
      "epoch": 16.16970259504196,
      "grad_norm": 11.069920539855957,
      "learning_rate": 3.6525247837465035e-05,
      "loss": 2.0985,
      "step": 418100
    },
    {
      "epoch": 16.173570019723865,
      "grad_norm": 10.712610244750977,
      "learning_rate": 3.652202498356345e-05,
      "loss": 2.0949,
      "step": 418200
    },
    {
      "epoch": 16.17743744440577,
      "grad_norm": 11.688252449035645,
      "learning_rate": 3.651880212966186e-05,
      "loss": 1.921,
      "step": 418300
    },
    {
      "epoch": 16.181304869087676,
      "grad_norm": 10.732654571533203,
      "learning_rate": 3.651557927576027e-05,
      "loss": 2.0556,
      "step": 418400
    },
    {
      "epoch": 16.18517229376958,
      "grad_norm": 12.547198295593262,
      "learning_rate": 3.651235642185869e-05,
      "loss": 2.0399,
      "step": 418500
    },
    {
      "epoch": 16.189039718451482,
      "grad_norm": 9.734375953674316,
      "learning_rate": 3.65091335679571e-05,
      "loss": 1.9752,
      "step": 418600
    },
    {
      "epoch": 16.192907143133386,
      "grad_norm": 12.548651695251465,
      "learning_rate": 3.6505910714055517e-05,
      "loss": 2.0306,
      "step": 418700
    },
    {
      "epoch": 16.196774567815293,
      "grad_norm": 13.49388599395752,
      "learning_rate": 3.650268786015393e-05,
      "loss": 2.056,
      "step": 418800
    },
    {
      "epoch": 16.200641992497196,
      "grad_norm": 12.464089393615723,
      "learning_rate": 3.649946500625234e-05,
      "loss": 2.0034,
      "step": 418900
    },
    {
      "epoch": 16.2045094171791,
      "grad_norm": 10.98482608795166,
      "learning_rate": 3.6496242152350754e-05,
      "loss": 2.0718,
      "step": 419000
    },
    {
      "epoch": 16.208376841861003,
      "grad_norm": 14.224602699279785,
      "learning_rate": 3.649301929844917e-05,
      "loss": 2.0648,
      "step": 419100
    },
    {
      "epoch": 16.21224426654291,
      "grad_norm": 11.461846351623535,
      "learning_rate": 3.6489796444547577e-05,
      "loss": 2.0165,
      "step": 419200
    },
    {
      "epoch": 16.216111691224814,
      "grad_norm": 11.68348503112793,
      "learning_rate": 3.648657359064599e-05,
      "loss": 2.0128,
      "step": 419300
    },
    {
      "epoch": 16.219979115906717,
      "grad_norm": 15.803165435791016,
      "learning_rate": 3.6483350736744406e-05,
      "loss": 1.9849,
      "step": 419400
    },
    {
      "epoch": 16.22384654058862,
      "grad_norm": 11.000089645385742,
      "learning_rate": 3.648012788284282e-05,
      "loss": 2.0279,
      "step": 419500
    },
    {
      "epoch": 16.227713965270528,
      "grad_norm": 10.913775444030762,
      "learning_rate": 3.647690502894123e-05,
      "loss": 1.9625,
      "step": 419600
    },
    {
      "epoch": 16.23158138995243,
      "grad_norm": 7.102810859680176,
      "learning_rate": 3.647368217503964e-05,
      "loss": 2.0399,
      "step": 419700
    },
    {
      "epoch": 16.235448814634335,
      "grad_norm": 12.414546966552734,
      "learning_rate": 3.647045932113806e-05,
      "loss": 2.0934,
      "step": 419800
    },
    {
      "epoch": 16.23931623931624,
      "grad_norm": 9.066064834594727,
      "learning_rate": 3.646723646723647e-05,
      "loss": 2.0657,
      "step": 419900
    },
    {
      "epoch": 16.243183663998142,
      "grad_norm": 10.633169174194336,
      "learning_rate": 3.646401361333488e-05,
      "loss": 1.9908,
      "step": 420000
    },
    {
      "epoch": 16.24705108868005,
      "grad_norm": 8.826971054077148,
      "learning_rate": 3.6460790759433295e-05,
      "loss": 1.9526,
      "step": 420100
    },
    {
      "epoch": 16.250918513361952,
      "grad_norm": 13.483778953552246,
      "learning_rate": 3.645756790553171e-05,
      "loss": 2.0449,
      "step": 420200
    },
    {
      "epoch": 16.254785938043856,
      "grad_norm": 10.45164680480957,
      "learning_rate": 3.6454345051630125e-05,
      "loss": 1.9767,
      "step": 420300
    },
    {
      "epoch": 16.25865336272576,
      "grad_norm": 10.881497383117676,
      "learning_rate": 3.645112219772853e-05,
      "loss": 2.028,
      "step": 420400
    },
    {
      "epoch": 16.262520787407666,
      "grad_norm": 9.707267761230469,
      "learning_rate": 3.644789934382695e-05,
      "loss": 2.0272,
      "step": 420500
    },
    {
      "epoch": 16.26638821208957,
      "grad_norm": 10.191171646118164,
      "learning_rate": 3.644467648992536e-05,
      "loss": 2.1237,
      "step": 420600
    },
    {
      "epoch": 16.270255636771473,
      "grad_norm": 12.448826789855957,
      "learning_rate": 3.644145363602378e-05,
      "loss": 1.9983,
      "step": 420700
    },
    {
      "epoch": 16.274123061453377,
      "grad_norm": 12.82862377166748,
      "learning_rate": 3.6438230782122185e-05,
      "loss": 2.0031,
      "step": 420800
    },
    {
      "epoch": 16.277990486135284,
      "grad_norm": 14.048587799072266,
      "learning_rate": 3.64350079282206e-05,
      "loss": 2.1054,
      "step": 420900
    },
    {
      "epoch": 16.281857910817187,
      "grad_norm": 11.758162498474121,
      "learning_rate": 3.6431785074319014e-05,
      "loss": 2.0283,
      "step": 421000
    },
    {
      "epoch": 16.28572533549909,
      "grad_norm": 8.208630561828613,
      "learning_rate": 3.642856222041743e-05,
      "loss": 2.0238,
      "step": 421100
    },
    {
      "epoch": 16.289592760180994,
      "grad_norm": 10.83880615234375,
      "learning_rate": 3.642533936651584e-05,
      "loss": 2.0046,
      "step": 421200
    },
    {
      "epoch": 16.2934601848629,
      "grad_norm": 12.50267219543457,
      "learning_rate": 3.642211651261425e-05,
      "loss": 2.0006,
      "step": 421300
    },
    {
      "epoch": 16.297327609544805,
      "grad_norm": 11.79675579071045,
      "learning_rate": 3.6418893658712666e-05,
      "loss": 2.0037,
      "step": 421400
    },
    {
      "epoch": 16.30119503422671,
      "grad_norm": 12.49272632598877,
      "learning_rate": 3.641567080481108e-05,
      "loss": 2.0256,
      "step": 421500
    },
    {
      "epoch": 16.305062458908612,
      "grad_norm": 12.308025360107422,
      "learning_rate": 3.641244795090949e-05,
      "loss": 2.0889,
      "step": 421600
    },
    {
      "epoch": 16.308929883590515,
      "grad_norm": 10.61874771118164,
      "learning_rate": 3.6409225097007904e-05,
      "loss": 2.0988,
      "step": 421700
    },
    {
      "epoch": 16.312797308272422,
      "grad_norm": 13.425207138061523,
      "learning_rate": 3.640600224310632e-05,
      "loss": 2.0685,
      "step": 421800
    },
    {
      "epoch": 16.316664732954326,
      "grad_norm": 17.591083526611328,
      "learning_rate": 3.640277938920473e-05,
      "loss": 2.0081,
      "step": 421900
    },
    {
      "epoch": 16.32053215763623,
      "grad_norm": 10.647876739501953,
      "learning_rate": 3.639955653530314e-05,
      "loss": 1.9982,
      "step": 422000
    },
    {
      "epoch": 16.324399582318133,
      "grad_norm": 10.93142318725586,
      "learning_rate": 3.6396333681401556e-05,
      "loss": 2.081,
      "step": 422100
    },
    {
      "epoch": 16.32826700700004,
      "grad_norm": 10.013824462890625,
      "learning_rate": 3.639311082749997e-05,
      "loss": 1.9877,
      "step": 422200
    },
    {
      "epoch": 16.332134431681943,
      "grad_norm": 14.269814491271973,
      "learning_rate": 3.638988797359838e-05,
      "loss": 2.0976,
      "step": 422300
    },
    {
      "epoch": 16.336001856363847,
      "grad_norm": 14.864277839660645,
      "learning_rate": 3.638666511969679e-05,
      "loss": 1.9517,
      "step": 422400
    },
    {
      "epoch": 16.33986928104575,
      "grad_norm": 10.679973602294922,
      "learning_rate": 3.638344226579521e-05,
      "loss": 2.0774,
      "step": 422500
    },
    {
      "epoch": 16.343736705727657,
      "grad_norm": 11.636682510375977,
      "learning_rate": 3.638021941189362e-05,
      "loss": 1.9334,
      "step": 422600
    },
    {
      "epoch": 16.34760413040956,
      "grad_norm": 11.530475616455078,
      "learning_rate": 3.637699655799203e-05,
      "loss": 1.9922,
      "step": 422700
    },
    {
      "epoch": 16.351471555091464,
      "grad_norm": 10.373662948608398,
      "learning_rate": 3.6373773704090445e-05,
      "loss": 1.9568,
      "step": 422800
    },
    {
      "epoch": 16.355338979773368,
      "grad_norm": 13.094053268432617,
      "learning_rate": 3.637055085018886e-05,
      "loss": 2.0757,
      "step": 422900
    },
    {
      "epoch": 16.359206404455275,
      "grad_norm": 11.428520202636719,
      "learning_rate": 3.6367327996287275e-05,
      "loss": 2.0356,
      "step": 423000
    },
    {
      "epoch": 16.36307382913718,
      "grad_norm": 12.867646217346191,
      "learning_rate": 3.636410514238568e-05,
      "loss": 2.0565,
      "step": 423100
    },
    {
      "epoch": 16.366941253819082,
      "grad_norm": 12.289139747619629,
      "learning_rate": 3.63608822884841e-05,
      "loss": 2.0977,
      "step": 423200
    },
    {
      "epoch": 16.370808678500985,
      "grad_norm": 19.526517868041992,
      "learning_rate": 3.635765943458251e-05,
      "loss": 1.9773,
      "step": 423300
    },
    {
      "epoch": 16.37467610318289,
      "grad_norm": 15.296196937561035,
      "learning_rate": 3.635443658068093e-05,
      "loss": 2.0259,
      "step": 423400
    },
    {
      "epoch": 16.378543527864796,
      "grad_norm": 14.797863960266113,
      "learning_rate": 3.6351213726779335e-05,
      "loss": 1.9898,
      "step": 423500
    },
    {
      "epoch": 16.3824109525467,
      "grad_norm": 9.824243545532227,
      "learning_rate": 3.634799087287775e-05,
      "loss": 2.0574,
      "step": 423600
    },
    {
      "epoch": 16.386278377228603,
      "grad_norm": 13.548989295959473,
      "learning_rate": 3.6344768018976164e-05,
      "loss": 1.9764,
      "step": 423700
    },
    {
      "epoch": 16.390145801910506,
      "grad_norm": 9.312068939208984,
      "learning_rate": 3.634154516507458e-05,
      "loss": 2.1493,
      "step": 423800
    },
    {
      "epoch": 16.394013226592413,
      "grad_norm": 9.32319164276123,
      "learning_rate": 3.633832231117299e-05,
      "loss": 2.0239,
      "step": 423900
    },
    {
      "epoch": 16.397880651274317,
      "grad_norm": 11.991674423217773,
      "learning_rate": 3.63350994572714e-05,
      "loss": 2.1208,
      "step": 424000
    },
    {
      "epoch": 16.40174807595622,
      "grad_norm": 11.056941986083984,
      "learning_rate": 3.6331876603369816e-05,
      "loss": 1.9761,
      "step": 424100
    },
    {
      "epoch": 16.405615500638124,
      "grad_norm": 13.637499809265137,
      "learning_rate": 3.632865374946823e-05,
      "loss": 2.0417,
      "step": 424200
    },
    {
      "epoch": 16.40948292532003,
      "grad_norm": 10.070799827575684,
      "learning_rate": 3.632543089556664e-05,
      "loss": 1.9584,
      "step": 424300
    },
    {
      "epoch": 16.413350350001934,
      "grad_norm": 12.366775512695312,
      "learning_rate": 3.6322208041665054e-05,
      "loss": 2.0416,
      "step": 424400
    },
    {
      "epoch": 16.417217774683838,
      "grad_norm": 11.633676528930664,
      "learning_rate": 3.631898518776347e-05,
      "loss": 2.0201,
      "step": 424500
    },
    {
      "epoch": 16.42108519936574,
      "grad_norm": 11.190370559692383,
      "learning_rate": 3.631576233386188e-05,
      "loss": 1.9926,
      "step": 424600
    },
    {
      "epoch": 16.42495262404765,
      "grad_norm": 9.38122844696045,
      "learning_rate": 3.63125394799603e-05,
      "loss": 1.9531,
      "step": 424700
    },
    {
      "epoch": 16.428820048729552,
      "grad_norm": 12.458704948425293,
      "learning_rate": 3.6309316626058706e-05,
      "loss": 2.0191,
      "step": 424800
    },
    {
      "epoch": 16.432687473411455,
      "grad_norm": 12.590045928955078,
      "learning_rate": 3.630609377215712e-05,
      "loss": 1.993,
      "step": 424900
    },
    {
      "epoch": 16.43655489809336,
      "grad_norm": 12.495153427124023,
      "learning_rate": 3.6302870918255535e-05,
      "loss": 2.0332,
      "step": 425000
    },
    {
      "epoch": 16.440422322775262,
      "grad_norm": 13.298175811767578,
      "learning_rate": 3.629964806435395e-05,
      "loss": 2.0983,
      "step": 425100
    },
    {
      "epoch": 16.44428974745717,
      "grad_norm": 11.52859878540039,
      "learning_rate": 3.6296425210452365e-05,
      "loss": 2.0344,
      "step": 425200
    },
    {
      "epoch": 16.448157172139073,
      "grad_norm": 6.7927021980285645,
      "learning_rate": 3.629320235655078e-05,
      "loss": 1.9721,
      "step": 425300
    },
    {
      "epoch": 16.452024596820976,
      "grad_norm": 12.303610801696777,
      "learning_rate": 3.628997950264919e-05,
      "loss": 1.9956,
      "step": 425400
    },
    {
      "epoch": 16.45589202150288,
      "grad_norm": 16.175495147705078,
      "learning_rate": 3.62867566487476e-05,
      "loss": 2.0257,
      "step": 425500
    },
    {
      "epoch": 16.459759446184787,
      "grad_norm": 13.34994888305664,
      "learning_rate": 3.628353379484602e-05,
      "loss": 2.1121,
      "step": 425600
    },
    {
      "epoch": 16.46362687086669,
      "grad_norm": 11.060567855834961,
      "learning_rate": 3.628031094094443e-05,
      "loss": 1.9222,
      "step": 425700
    },
    {
      "epoch": 16.467494295548594,
      "grad_norm": 10.834744453430176,
      "learning_rate": 3.627708808704284e-05,
      "loss": 2.0732,
      "step": 425800
    },
    {
      "epoch": 16.471361720230497,
      "grad_norm": 15.362113952636719,
      "learning_rate": 3.6273865233141254e-05,
      "loss": 1.989,
      "step": 425900
    },
    {
      "epoch": 16.475229144912404,
      "grad_norm": 11.645609855651855,
      "learning_rate": 3.627064237923967e-05,
      "loss": 2.0001,
      "step": 426000
    },
    {
      "epoch": 16.479096569594308,
      "grad_norm": 10.57128620147705,
      "learning_rate": 3.6267419525338084e-05,
      "loss": 2.0607,
      "step": 426100
    },
    {
      "epoch": 16.48296399427621,
      "grad_norm": 12.55673599243164,
      "learning_rate": 3.626419667143649e-05,
      "loss": 2.1381,
      "step": 426200
    },
    {
      "epoch": 16.486831418958115,
      "grad_norm": 12.157693862915039,
      "learning_rate": 3.6260973817534906e-05,
      "loss": 2.0017,
      "step": 426300
    },
    {
      "epoch": 16.49069884364002,
      "grad_norm": 11.131620407104492,
      "learning_rate": 3.625775096363332e-05,
      "loss": 2.0169,
      "step": 426400
    },
    {
      "epoch": 16.494566268321925,
      "grad_norm": 12.747894287109375,
      "learning_rate": 3.6254528109731736e-05,
      "loss": 2.0002,
      "step": 426500
    },
    {
      "epoch": 16.49843369300383,
      "grad_norm": 9.845946311950684,
      "learning_rate": 3.6251305255830144e-05,
      "loss": 1.9749,
      "step": 426600
    },
    {
      "epoch": 16.502301117685732,
      "grad_norm": 10.446860313415527,
      "learning_rate": 3.624808240192856e-05,
      "loss": 1.9184,
      "step": 426700
    },
    {
      "epoch": 16.506168542367636,
      "grad_norm": 17.44045066833496,
      "learning_rate": 3.624485954802697e-05,
      "loss": 1.9696,
      "step": 426800
    },
    {
      "epoch": 16.510035967049543,
      "grad_norm": 13.570634841918945,
      "learning_rate": 3.624163669412539e-05,
      "loss": 2.0622,
      "step": 426900
    },
    {
      "epoch": 16.513903391731446,
      "grad_norm": 12.407425880432129,
      "learning_rate": 3.6238413840223796e-05,
      "loss": 2.0136,
      "step": 427000
    },
    {
      "epoch": 16.51777081641335,
      "grad_norm": 13.809791564941406,
      "learning_rate": 3.623519098632221e-05,
      "loss": 2.0668,
      "step": 427100
    },
    {
      "epoch": 16.521638241095253,
      "grad_norm": 10.561518669128418,
      "learning_rate": 3.6231968132420625e-05,
      "loss": 2.0102,
      "step": 427200
    },
    {
      "epoch": 16.52550566577716,
      "grad_norm": 11.571052551269531,
      "learning_rate": 3.622874527851904e-05,
      "loss": 2.0532,
      "step": 427300
    },
    {
      "epoch": 16.529373090459064,
      "grad_norm": 11.474037170410156,
      "learning_rate": 3.622552242461745e-05,
      "loss": 2.0881,
      "step": 427400
    },
    {
      "epoch": 16.533240515140967,
      "grad_norm": 12.675887107849121,
      "learning_rate": 3.622229957071586e-05,
      "loss": 2.0506,
      "step": 427500
    },
    {
      "epoch": 16.53710793982287,
      "grad_norm": 11.438722610473633,
      "learning_rate": 3.621907671681428e-05,
      "loss": 2.0342,
      "step": 427600
    },
    {
      "epoch": 16.540975364504778,
      "grad_norm": 10.349104881286621,
      "learning_rate": 3.621585386291269e-05,
      "loss": 2.021,
      "step": 427700
    },
    {
      "epoch": 16.54484278918668,
      "grad_norm": 12.236124992370605,
      "learning_rate": 3.62126310090111e-05,
      "loss": 1.9963,
      "step": 427800
    },
    {
      "epoch": 16.548710213868585,
      "grad_norm": 8.693855285644531,
      "learning_rate": 3.6209408155109515e-05,
      "loss": 1.8733,
      "step": 427900
    },
    {
      "epoch": 16.55257763855049,
      "grad_norm": 10.491527557373047,
      "learning_rate": 3.620618530120793e-05,
      "loss": 1.9983,
      "step": 428000
    },
    {
      "epoch": 16.556445063232392,
      "grad_norm": 12.148173332214355,
      "learning_rate": 3.620296244730634e-05,
      "loss": 1.9833,
      "step": 428100
    },
    {
      "epoch": 16.5603124879143,
      "grad_norm": 12.472057342529297,
      "learning_rate": 3.619973959340475e-05,
      "loss": 2.0243,
      "step": 428200
    },
    {
      "epoch": 16.564179912596202,
      "grad_norm": 9.863770484924316,
      "learning_rate": 3.619651673950317e-05,
      "loss": 1.9721,
      "step": 428300
    },
    {
      "epoch": 16.568047337278106,
      "grad_norm": 12.779857635498047,
      "learning_rate": 3.619329388560158e-05,
      "loss": 2.0153,
      "step": 428400
    },
    {
      "epoch": 16.57191476196001,
      "grad_norm": 12.979996681213379,
      "learning_rate": 3.619007103169999e-05,
      "loss": 2.0302,
      "step": 428500
    },
    {
      "epoch": 16.575782186641916,
      "grad_norm": 13.180115699768066,
      "learning_rate": 3.6186848177798404e-05,
      "loss": 1.9978,
      "step": 428600
    },
    {
      "epoch": 16.57964961132382,
      "grad_norm": 10.437661170959473,
      "learning_rate": 3.618362532389682e-05,
      "loss": 2.0715,
      "step": 428700
    },
    {
      "epoch": 16.583517036005723,
      "grad_norm": 6.5248565673828125,
      "learning_rate": 3.6180402469995234e-05,
      "loss": 1.9719,
      "step": 428800
    },
    {
      "epoch": 16.587384460687627,
      "grad_norm": 10.911815643310547,
      "learning_rate": 3.617717961609364e-05,
      "loss": 2.0228,
      "step": 428900
    },
    {
      "epoch": 16.591251885369534,
      "grad_norm": 12.761127471923828,
      "learning_rate": 3.6173956762192056e-05,
      "loss": 2.0293,
      "step": 429000
    },
    {
      "epoch": 16.595119310051437,
      "grad_norm": 12.891700744628906,
      "learning_rate": 3.617073390829047e-05,
      "loss": 2.0472,
      "step": 429100
    },
    {
      "epoch": 16.59898673473334,
      "grad_norm": 13.816008567810059,
      "learning_rate": 3.6167511054388886e-05,
      "loss": 2.001,
      "step": 429200
    },
    {
      "epoch": 16.602854159415244,
      "grad_norm": 12.18094539642334,
      "learning_rate": 3.6164288200487294e-05,
      "loss": 1.9868,
      "step": 429300
    },
    {
      "epoch": 16.60672158409715,
      "grad_norm": 13.614611625671387,
      "learning_rate": 3.616106534658571e-05,
      "loss": 1.9809,
      "step": 429400
    },
    {
      "epoch": 16.610589008779055,
      "grad_norm": 12.949417114257812,
      "learning_rate": 3.615784249268412e-05,
      "loss": 1.9837,
      "step": 429500
    },
    {
      "epoch": 16.61445643346096,
      "grad_norm": 13.363903999328613,
      "learning_rate": 3.615461963878254e-05,
      "loss": 2.0038,
      "step": 429600
    },
    {
      "epoch": 16.618323858142862,
      "grad_norm": 9.773181915283203,
      "learning_rate": 3.6151396784880946e-05,
      "loss": 2.0361,
      "step": 429700
    },
    {
      "epoch": 16.622191282824765,
      "grad_norm": 8.567208290100098,
      "learning_rate": 3.614817393097936e-05,
      "loss": 2.0698,
      "step": 429800
    },
    {
      "epoch": 16.626058707506672,
      "grad_norm": 11.53718090057373,
      "learning_rate": 3.6144951077077775e-05,
      "loss": 2.0794,
      "step": 429900
    },
    {
      "epoch": 16.629926132188576,
      "grad_norm": 9.023552894592285,
      "learning_rate": 3.614172822317619e-05,
      "loss": 2.0111,
      "step": 430000
    },
    {
      "epoch": 16.63379355687048,
      "grad_norm": 10.858829498291016,
      "learning_rate": 3.61385053692746e-05,
      "loss": 1.938,
      "step": 430100
    },
    {
      "epoch": 16.637660981552383,
      "grad_norm": 11.861673355102539,
      "learning_rate": 3.613528251537301e-05,
      "loss": 2.0317,
      "step": 430200
    },
    {
      "epoch": 16.64152840623429,
      "grad_norm": 15.159345626831055,
      "learning_rate": 3.613205966147143e-05,
      "loss": 2.0551,
      "step": 430300
    },
    {
      "epoch": 16.645395830916193,
      "grad_norm": 10.206915855407715,
      "learning_rate": 3.612883680756984e-05,
      "loss": 1.9588,
      "step": 430400
    },
    {
      "epoch": 16.649263255598097,
      "grad_norm": 10.652905464172363,
      "learning_rate": 3.612561395366825e-05,
      "loss": 2.1121,
      "step": 430500
    },
    {
      "epoch": 16.65313068028,
      "grad_norm": 10.408825874328613,
      "learning_rate": 3.6122391099766665e-05,
      "loss": 1.9355,
      "step": 430600
    },
    {
      "epoch": 16.656998104961907,
      "grad_norm": 13.534503936767578,
      "learning_rate": 3.611916824586508e-05,
      "loss": 1.9908,
      "step": 430700
    },
    {
      "epoch": 16.66086552964381,
      "grad_norm": 9.15994644165039,
      "learning_rate": 3.611594539196349e-05,
      "loss": 2.0832,
      "step": 430800
    },
    {
      "epoch": 16.664732954325714,
      "grad_norm": 11.095738410949707,
      "learning_rate": 3.61127225380619e-05,
      "loss": 1.9824,
      "step": 430900
    },
    {
      "epoch": 16.668600379007618,
      "grad_norm": 12.949968338012695,
      "learning_rate": 3.610949968416032e-05,
      "loss": 1.9476,
      "step": 431000
    },
    {
      "epoch": 16.672467803689525,
      "grad_norm": 10.890007972717285,
      "learning_rate": 3.610627683025873e-05,
      "loss": 2.0498,
      "step": 431100
    },
    {
      "epoch": 16.67633522837143,
      "grad_norm": 9.57126522064209,
      "learning_rate": 3.6103053976357146e-05,
      "loss": 1.9415,
      "step": 431200
    },
    {
      "epoch": 16.680202653053332,
      "grad_norm": 9.231289863586426,
      "learning_rate": 3.6099831122455554e-05,
      "loss": 2.0251,
      "step": 431300
    },
    {
      "epoch": 16.684070077735235,
      "grad_norm": 11.32320785522461,
      "learning_rate": 3.609660826855397e-05,
      "loss": 1.9713,
      "step": 431400
    },
    {
      "epoch": 16.68793750241714,
      "grad_norm": 11.264028549194336,
      "learning_rate": 3.6093385414652384e-05,
      "loss": 2.156,
      "step": 431500
    },
    {
      "epoch": 16.691804927099046,
      "grad_norm": 11.083146095275879,
      "learning_rate": 3.60901625607508e-05,
      "loss": 2.0384,
      "step": 431600
    },
    {
      "epoch": 16.69567235178095,
      "grad_norm": 11.222525596618652,
      "learning_rate": 3.608693970684921e-05,
      "loss": 2.0676,
      "step": 431700
    },
    {
      "epoch": 16.699539776462853,
      "grad_norm": 10.58513355255127,
      "learning_rate": 3.608371685294762e-05,
      "loss": 2.0154,
      "step": 431800
    },
    {
      "epoch": 16.703407201144756,
      "grad_norm": 10.52405834197998,
      "learning_rate": 3.6080493999046036e-05,
      "loss": 2.0293,
      "step": 431900
    },
    {
      "epoch": 16.707274625826663,
      "grad_norm": 14.40787124633789,
      "learning_rate": 3.607727114514445e-05,
      "loss": 1.966,
      "step": 432000
    },
    {
      "epoch": 16.711142050508567,
      "grad_norm": 11.832537651062012,
      "learning_rate": 3.6074048291242865e-05,
      "loss": 1.9531,
      "step": 432100
    },
    {
      "epoch": 16.71500947519047,
      "grad_norm": 11.927221298217773,
      "learning_rate": 3.607082543734128e-05,
      "loss": 1.9659,
      "step": 432200
    },
    {
      "epoch": 16.718876899872374,
      "grad_norm": 9.538732528686523,
      "learning_rate": 3.6067602583439695e-05,
      "loss": 1.9406,
      "step": 432300
    },
    {
      "epoch": 16.72274432455428,
      "grad_norm": 11.446310043334961,
      "learning_rate": 3.60643797295381e-05,
      "loss": 1.9835,
      "step": 432400
    },
    {
      "epoch": 16.726611749236184,
      "grad_norm": 9.623844146728516,
      "learning_rate": 3.606115687563652e-05,
      "loss": 2.0041,
      "step": 432500
    },
    {
      "epoch": 16.730479173918088,
      "grad_norm": 10.905071258544922,
      "learning_rate": 3.605793402173493e-05,
      "loss": 1.9974,
      "step": 432600
    },
    {
      "epoch": 16.73434659859999,
      "grad_norm": 11.026200294494629,
      "learning_rate": 3.605471116783335e-05,
      "loss": 2.0155,
      "step": 432700
    },
    {
      "epoch": 16.7382140232819,
      "grad_norm": 10.734495162963867,
      "learning_rate": 3.6051488313931755e-05,
      "loss": 2.1076,
      "step": 432800
    },
    {
      "epoch": 16.742081447963802,
      "grad_norm": 9.883581161499023,
      "learning_rate": 3.604826546003017e-05,
      "loss": 1.9724,
      "step": 432900
    },
    {
      "epoch": 16.745948872645705,
      "grad_norm": 12.002717971801758,
      "learning_rate": 3.6045042606128584e-05,
      "loss": 1.9876,
      "step": 433000
    },
    {
      "epoch": 16.74981629732761,
      "grad_norm": 10.59022331237793,
      "learning_rate": 3.6041819752227e-05,
      "loss": 1.9736,
      "step": 433100
    },
    {
      "epoch": 16.753683722009512,
      "grad_norm": 11.686100006103516,
      "learning_rate": 3.603859689832541e-05,
      "loss": 2.0423,
      "step": 433200
    },
    {
      "epoch": 16.75755114669142,
      "grad_norm": 9.203398704528809,
      "learning_rate": 3.603537404442382e-05,
      "loss": 2.0714,
      "step": 433300
    },
    {
      "epoch": 16.761418571373323,
      "grad_norm": 9.882780075073242,
      "learning_rate": 3.6032151190522236e-05,
      "loss": 1.9413,
      "step": 433400
    },
    {
      "epoch": 16.765285996055226,
      "grad_norm": 10.007964134216309,
      "learning_rate": 3.602892833662065e-05,
      "loss": 2.0256,
      "step": 433500
    },
    {
      "epoch": 16.76915342073713,
      "grad_norm": 12.507938385009766,
      "learning_rate": 3.602570548271906e-05,
      "loss": 1.9809,
      "step": 433600
    },
    {
      "epoch": 16.773020845419037,
      "grad_norm": 9.901043891906738,
      "learning_rate": 3.6022482628817474e-05,
      "loss": 2.0739,
      "step": 433700
    },
    {
      "epoch": 16.77688827010094,
      "grad_norm": 8.805730819702148,
      "learning_rate": 3.601925977491589e-05,
      "loss": 1.9924,
      "step": 433800
    },
    {
      "epoch": 16.780755694782844,
      "grad_norm": 11.518319129943848,
      "learning_rate": 3.6016036921014296e-05,
      "loss": 1.9708,
      "step": 433900
    },
    {
      "epoch": 16.784623119464747,
      "grad_norm": 9.14870548248291,
      "learning_rate": 3.601281406711271e-05,
      "loss": 2.0082,
      "step": 434000
    },
    {
      "epoch": 16.788490544146654,
      "grad_norm": 10.205089569091797,
      "learning_rate": 3.6009591213211126e-05,
      "loss": 2.0493,
      "step": 434100
    },
    {
      "epoch": 16.792357968828558,
      "grad_norm": 10.824760437011719,
      "learning_rate": 3.600636835930954e-05,
      "loss": 2.0404,
      "step": 434200
    },
    {
      "epoch": 16.79622539351046,
      "grad_norm": 15.803567886352539,
      "learning_rate": 3.600314550540795e-05,
      "loss": 1.9265,
      "step": 434300
    },
    {
      "epoch": 16.800092818192365,
      "grad_norm": 9.616379737854004,
      "learning_rate": 3.599992265150636e-05,
      "loss": 2.0013,
      "step": 434400
    },
    {
      "epoch": 16.80396024287427,
      "grad_norm": 10.922205924987793,
      "learning_rate": 3.599669979760478e-05,
      "loss": 2.0538,
      "step": 434500
    },
    {
      "epoch": 16.807827667556175,
      "grad_norm": 12.151517868041992,
      "learning_rate": 3.599347694370319e-05,
      "loss": 1.9766,
      "step": 434600
    },
    {
      "epoch": 16.81169509223808,
      "grad_norm": 10.97607707977295,
      "learning_rate": 3.59902540898016e-05,
      "loss": 2.0151,
      "step": 434700
    },
    {
      "epoch": 16.815562516919982,
      "grad_norm": 14.006903648376465,
      "learning_rate": 3.5987031235900015e-05,
      "loss": 1.9751,
      "step": 434800
    },
    {
      "epoch": 16.819429941601886,
      "grad_norm": 12.021498680114746,
      "learning_rate": 3.598380838199843e-05,
      "loss": 1.9802,
      "step": 434900
    },
    {
      "epoch": 16.823297366283793,
      "grad_norm": 12.872945785522461,
      "learning_rate": 3.5980585528096845e-05,
      "loss": 1.9674,
      "step": 435000
    },
    {
      "epoch": 16.827164790965696,
      "grad_norm": 12.228897094726562,
      "learning_rate": 3.597736267419525e-05,
      "loss": 1.9926,
      "step": 435100
    },
    {
      "epoch": 16.8310322156476,
      "grad_norm": 12.355685234069824,
      "learning_rate": 3.597413982029367e-05,
      "loss": 1.9342,
      "step": 435200
    },
    {
      "epoch": 16.834899640329503,
      "grad_norm": 9.567582130432129,
      "learning_rate": 3.597091696639208e-05,
      "loss": 2.0335,
      "step": 435300
    },
    {
      "epoch": 16.83876706501141,
      "grad_norm": 8.14554500579834,
      "learning_rate": 3.59676941124905e-05,
      "loss": 1.9962,
      "step": 435400
    },
    {
      "epoch": 16.842634489693314,
      "grad_norm": 9.8487548828125,
      "learning_rate": 3.5964471258588905e-05,
      "loss": 1.9222,
      "step": 435500
    },
    {
      "epoch": 16.846501914375217,
      "grad_norm": 11.72823429107666,
      "learning_rate": 3.596124840468732e-05,
      "loss": 2.0576,
      "step": 435600
    },
    {
      "epoch": 16.85036933905712,
      "grad_norm": 10.972603797912598,
      "learning_rate": 3.5958025550785734e-05,
      "loss": 1.9481,
      "step": 435700
    },
    {
      "epoch": 16.854236763739028,
      "grad_norm": 12.413387298583984,
      "learning_rate": 3.595480269688415e-05,
      "loss": 2.077,
      "step": 435800
    },
    {
      "epoch": 16.85810418842093,
      "grad_norm": 13.936750411987305,
      "learning_rate": 3.595157984298256e-05,
      "loss": 1.9997,
      "step": 435900
    },
    {
      "epoch": 16.861971613102835,
      "grad_norm": 10.800885200500488,
      "learning_rate": 3.594835698908097e-05,
      "loss": 2.0453,
      "step": 436000
    },
    {
      "epoch": 16.86583903778474,
      "grad_norm": 12.349693298339844,
      "learning_rate": 3.5945134135179386e-05,
      "loss": 2.1206,
      "step": 436100
    },
    {
      "epoch": 16.869706462466645,
      "grad_norm": 14.343282699584961,
      "learning_rate": 3.59419112812778e-05,
      "loss": 2.0282,
      "step": 436200
    },
    {
      "epoch": 16.87357388714855,
      "grad_norm": 13.719758033752441,
      "learning_rate": 3.593868842737621e-05,
      "loss": 2.019,
      "step": 436300
    },
    {
      "epoch": 16.877441311830452,
      "grad_norm": 12.59419059753418,
      "learning_rate": 3.5935465573474624e-05,
      "loss": 2.0665,
      "step": 436400
    },
    {
      "epoch": 16.881308736512356,
      "grad_norm": 12.004485130310059,
      "learning_rate": 3.593224271957304e-05,
      "loss": 1.9912,
      "step": 436500
    },
    {
      "epoch": 16.88517616119426,
      "grad_norm": 10.708939552307129,
      "learning_rate": 3.592901986567145e-05,
      "loss": 2.0391,
      "step": 436600
    },
    {
      "epoch": 16.889043585876166,
      "grad_norm": 17.05048370361328,
      "learning_rate": 3.592579701176986e-05,
      "loss": 2.0196,
      "step": 436700
    },
    {
      "epoch": 16.89291101055807,
      "grad_norm": 10.768677711486816,
      "learning_rate": 3.5922574157868276e-05,
      "loss": 1.8986,
      "step": 436800
    },
    {
      "epoch": 16.896778435239973,
      "grad_norm": 18.498199462890625,
      "learning_rate": 3.591935130396669e-05,
      "loss": 2.0533,
      "step": 436900
    },
    {
      "epoch": 16.900645859921877,
      "grad_norm": 13.336004257202148,
      "learning_rate": 3.59161284500651e-05,
      "loss": 1.9498,
      "step": 437000
    },
    {
      "epoch": 16.904513284603784,
      "grad_norm": 13.812060356140137,
      "learning_rate": 3.591290559616351e-05,
      "loss": 2.074,
      "step": 437100
    },
    {
      "epoch": 16.908380709285687,
      "grad_norm": 12.536194801330566,
      "learning_rate": 3.590968274226193e-05,
      "loss": 2.023,
      "step": 437200
    },
    {
      "epoch": 16.91224813396759,
      "grad_norm": 12.626495361328125,
      "learning_rate": 3.590645988836034e-05,
      "loss": 2.0297,
      "step": 437300
    },
    {
      "epoch": 16.916115558649494,
      "grad_norm": 15.201457977294922,
      "learning_rate": 3.590323703445875e-05,
      "loss": 1.9833,
      "step": 437400
    },
    {
      "epoch": 16.9199829833314,
      "grad_norm": 12.846785545349121,
      "learning_rate": 3.5900014180557165e-05,
      "loss": 1.9512,
      "step": 437500
    },
    {
      "epoch": 16.923850408013305,
      "grad_norm": 15.987549781799316,
      "learning_rate": 3.589679132665558e-05,
      "loss": 1.9385,
      "step": 437600
    },
    {
      "epoch": 16.92771783269521,
      "grad_norm": 17.950946807861328,
      "learning_rate": 3.5893568472753995e-05,
      "loss": 2.0492,
      "step": 437700
    },
    {
      "epoch": 16.93158525737711,
      "grad_norm": 11.770240783691406,
      "learning_rate": 3.58903456188524e-05,
      "loss": 2.0081,
      "step": 437800
    },
    {
      "epoch": 16.935452682059015,
      "grad_norm": 12.480606079101562,
      "learning_rate": 3.588712276495082e-05,
      "loss": 2.0516,
      "step": 437900
    },
    {
      "epoch": 16.939320106740922,
      "grad_norm": 13.491971969604492,
      "learning_rate": 3.588389991104923e-05,
      "loss": 1.9955,
      "step": 438000
    },
    {
      "epoch": 16.943187531422826,
      "grad_norm": 11.558987617492676,
      "learning_rate": 3.588067705714765e-05,
      "loss": 1.9422,
      "step": 438100
    },
    {
      "epoch": 16.94705495610473,
      "grad_norm": 11.765769958496094,
      "learning_rate": 3.587745420324606e-05,
      "loss": 2.0256,
      "step": 438200
    },
    {
      "epoch": 16.950922380786633,
      "grad_norm": 11.272310256958008,
      "learning_rate": 3.587423134934447e-05,
      "loss": 2.0894,
      "step": 438300
    },
    {
      "epoch": 16.95478980546854,
      "grad_norm": 9.491583824157715,
      "learning_rate": 3.5871008495442884e-05,
      "loss": 1.9906,
      "step": 438400
    },
    {
      "epoch": 16.958657230150443,
      "grad_norm": 11.638400077819824,
      "learning_rate": 3.58677856415413e-05,
      "loss": 2.0489,
      "step": 438500
    },
    {
      "epoch": 16.962524654832347,
      "grad_norm": 11.444002151489258,
      "learning_rate": 3.5864562787639713e-05,
      "loss": 2.0162,
      "step": 438600
    },
    {
      "epoch": 16.96639207951425,
      "grad_norm": 12.44432258605957,
      "learning_rate": 3.586133993373813e-05,
      "loss": 2.082,
      "step": 438700
    },
    {
      "epoch": 16.970259504196157,
      "grad_norm": 11.152771949768066,
      "learning_rate": 3.585811707983654e-05,
      "loss": 2.0208,
      "step": 438800
    },
    {
      "epoch": 16.97412692887806,
      "grad_norm": 18.725460052490234,
      "learning_rate": 3.585489422593495e-05,
      "loss": 2.1174,
      "step": 438900
    },
    {
      "epoch": 16.977994353559964,
      "grad_norm": 11.196861267089844,
      "learning_rate": 3.5851671372033366e-05,
      "loss": 2.0208,
      "step": 439000
    },
    {
      "epoch": 16.981861778241868,
      "grad_norm": 10.276786804199219,
      "learning_rate": 3.584844851813178e-05,
      "loss": 1.9407,
      "step": 439100
    },
    {
      "epoch": 16.985729202923775,
      "grad_norm": 12.178160667419434,
      "learning_rate": 3.5845225664230195e-05,
      "loss": 2.0216,
      "step": 439200
    },
    {
      "epoch": 16.98959662760568,
      "grad_norm": 12.15538215637207,
      "learning_rate": 3.584200281032861e-05,
      "loss": 2.0427,
      "step": 439300
    },
    {
      "epoch": 16.99346405228758,
      "grad_norm": 10.915018081665039,
      "learning_rate": 3.583877995642702e-05,
      "loss": 2.0529,
      "step": 439400
    },
    {
      "epoch": 16.997331476969485,
      "grad_norm": 12.018815994262695,
      "learning_rate": 3.583555710252543e-05,
      "loss": 2.0197,
      "step": 439500
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.908896565437317,
      "eval_runtime": 3.2491,
      "eval_samples_per_second": 418.882,
      "eval_steps_per_second": 418.882,
      "step": 439569
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.824151635169983,
      "eval_runtime": 56.258,
      "eval_samples_per_second": 459.614,
      "eval_steps_per_second": 459.614,
      "step": 439569
    },
    {
      "epoch": 17.00119890165139,
      "grad_norm": 14.25723648071289,
      "learning_rate": 3.583233424862385e-05,
      "loss": 2.08,
      "step": 439600
    },
    {
      "epoch": 17.005066326333296,
      "grad_norm": 15.427739143371582,
      "learning_rate": 3.5829111394722255e-05,
      "loss": 1.9653,
      "step": 439700
    },
    {
      "epoch": 17.0089337510152,
      "grad_norm": 12.409151077270508,
      "learning_rate": 3.582588854082067e-05,
      "loss": 1.9534,
      "step": 439800
    },
    {
      "epoch": 17.012801175697103,
      "grad_norm": 11.375458717346191,
      "learning_rate": 3.5822665686919084e-05,
      "loss": 2.0018,
      "step": 439900
    },
    {
      "epoch": 17.016668600379006,
      "grad_norm": 12.131709098815918,
      "learning_rate": 3.58194428330175e-05,
      "loss": 1.9642,
      "step": 440000
    },
    {
      "epoch": 17.020536025060913,
      "grad_norm": 10.995100021362305,
      "learning_rate": 3.581621997911591e-05,
      "loss": 1.974,
      "step": 440100
    },
    {
      "epoch": 17.024403449742817,
      "grad_norm": 11.40230655670166,
      "learning_rate": 3.581299712521432e-05,
      "loss": 1.9872,
      "step": 440200
    },
    {
      "epoch": 17.02827087442472,
      "grad_norm": 10.612068176269531,
      "learning_rate": 3.5809774271312737e-05,
      "loss": 1.9365,
      "step": 440300
    },
    {
      "epoch": 17.032138299106624,
      "grad_norm": 10.505753517150879,
      "learning_rate": 3.580655141741115e-05,
      "loss": 1.9706,
      "step": 440400
    },
    {
      "epoch": 17.03600572378853,
      "grad_norm": 11.774758338928223,
      "learning_rate": 3.580332856350956e-05,
      "loss": 1.9925,
      "step": 440500
    },
    {
      "epoch": 17.039873148470434,
      "grad_norm": 11.41923999786377,
      "learning_rate": 3.5800105709607974e-05,
      "loss": 2.0092,
      "step": 440600
    },
    {
      "epoch": 17.043740573152338,
      "grad_norm": 11.874244689941406,
      "learning_rate": 3.579688285570639e-05,
      "loss": 2.0032,
      "step": 440700
    },
    {
      "epoch": 17.04760799783424,
      "grad_norm": 13.76876449584961,
      "learning_rate": 3.57936600018048e-05,
      "loss": 1.994,
      "step": 440800
    },
    {
      "epoch": 17.05147542251615,
      "grad_norm": 13.313185691833496,
      "learning_rate": 3.579043714790321e-05,
      "loss": 1.9585,
      "step": 440900
    },
    {
      "epoch": 17.05534284719805,
      "grad_norm": 11.050034523010254,
      "learning_rate": 3.5787214294001626e-05,
      "loss": 1.9877,
      "step": 441000
    },
    {
      "epoch": 17.059210271879955,
      "grad_norm": 13.274840354919434,
      "learning_rate": 3.578399144010004e-05,
      "loss": 2.02,
      "step": 441100
    },
    {
      "epoch": 17.06307769656186,
      "grad_norm": 11.828801155090332,
      "learning_rate": 3.5780768586198455e-05,
      "loss": 1.9588,
      "step": 441200
    },
    {
      "epoch": 17.066945121243762,
      "grad_norm": 10.96607494354248,
      "learning_rate": 3.5777545732296863e-05,
      "loss": 1.9899,
      "step": 441300
    },
    {
      "epoch": 17.07081254592567,
      "grad_norm": 12.720442771911621,
      "learning_rate": 3.577432287839528e-05,
      "loss": 2.0593,
      "step": 441400
    },
    {
      "epoch": 17.074679970607573,
      "grad_norm": 10.186674118041992,
      "learning_rate": 3.577110002449369e-05,
      "loss": 2.0038,
      "step": 441500
    },
    {
      "epoch": 17.078547395289476,
      "grad_norm": 14.28989028930664,
      "learning_rate": 3.576787717059211e-05,
      "loss": 1.952,
      "step": 441600
    },
    {
      "epoch": 17.08241481997138,
      "grad_norm": 14.641037940979004,
      "learning_rate": 3.5764654316690515e-05,
      "loss": 2.0273,
      "step": 441700
    },
    {
      "epoch": 17.086282244653287,
      "grad_norm": 13.326090812683105,
      "learning_rate": 3.576143146278893e-05,
      "loss": 2.0013,
      "step": 441800
    },
    {
      "epoch": 17.09014966933519,
      "grad_norm": 11.713618278503418,
      "learning_rate": 3.5758208608887345e-05,
      "loss": 2.0075,
      "step": 441900
    },
    {
      "epoch": 17.094017094017094,
      "grad_norm": 11.264535903930664,
      "learning_rate": 3.575498575498576e-05,
      "loss": 2.04,
      "step": 442000
    },
    {
      "epoch": 17.097884518698997,
      "grad_norm": 12.845887184143066,
      "learning_rate": 3.575176290108417e-05,
      "loss": 2.0075,
      "step": 442100
    },
    {
      "epoch": 17.101751943380904,
      "grad_norm": 13.559479713439941,
      "learning_rate": 3.574854004718258e-05,
      "loss": 1.954,
      "step": 442200
    },
    {
      "epoch": 17.105619368062808,
      "grad_norm": 9.056649208068848,
      "learning_rate": 3.5745317193281e-05,
      "loss": 1.9318,
      "step": 442300
    },
    {
      "epoch": 17.10948679274471,
      "grad_norm": 11.854992866516113,
      "learning_rate": 3.574209433937941e-05,
      "loss": 1.981,
      "step": 442400
    },
    {
      "epoch": 17.113354217426615,
      "grad_norm": 9.430566787719727,
      "learning_rate": 3.573887148547782e-05,
      "loss": 2.0168,
      "step": 442500
    },
    {
      "epoch": 17.117221642108518,
      "grad_norm": 13.523573875427246,
      "learning_rate": 3.5735648631576234e-05,
      "loss": 2.0625,
      "step": 442600
    },
    {
      "epoch": 17.121089066790425,
      "grad_norm": 9.615254402160645,
      "learning_rate": 3.573242577767465e-05,
      "loss": 2.063,
      "step": 442700
    },
    {
      "epoch": 17.12495649147233,
      "grad_norm": 10.374669075012207,
      "learning_rate": 3.572920292377306e-05,
      "loss": 1.9649,
      "step": 442800
    },
    {
      "epoch": 17.128823916154232,
      "grad_norm": 12.86184310913086,
      "learning_rate": 3.572598006987147e-05,
      "loss": 2.025,
      "step": 442900
    },
    {
      "epoch": 17.132691340836136,
      "grad_norm": 11.211685180664062,
      "learning_rate": 3.5722757215969887e-05,
      "loss": 2.0045,
      "step": 443000
    },
    {
      "epoch": 17.136558765518043,
      "grad_norm": 12.759593963623047,
      "learning_rate": 3.57195343620683e-05,
      "loss": 2.0232,
      "step": 443100
    },
    {
      "epoch": 17.140426190199946,
      "grad_norm": 10.844523429870605,
      "learning_rate": 3.571631150816671e-05,
      "loss": 1.9485,
      "step": 443200
    },
    {
      "epoch": 17.14429361488185,
      "grad_norm": 12.57497501373291,
      "learning_rate": 3.5713088654265124e-05,
      "loss": 2.0686,
      "step": 443300
    },
    {
      "epoch": 17.148161039563753,
      "grad_norm": 11.973633766174316,
      "learning_rate": 3.570986580036354e-05,
      "loss": 1.8758,
      "step": 443400
    },
    {
      "epoch": 17.15202846424566,
      "grad_norm": 12.033976554870605,
      "learning_rate": 3.570664294646195e-05,
      "loss": 1.9275,
      "step": 443500
    },
    {
      "epoch": 17.155895888927564,
      "grad_norm": 11.40320110321045,
      "learning_rate": 3.570342009256036e-05,
      "loss": 2.0182,
      "step": 443600
    },
    {
      "epoch": 17.159763313609467,
      "grad_norm": 14.941980361938477,
      "learning_rate": 3.5700197238658776e-05,
      "loss": 2.0179,
      "step": 443700
    },
    {
      "epoch": 17.16363073829137,
      "grad_norm": 12.418394088745117,
      "learning_rate": 3.569697438475719e-05,
      "loss": 1.9843,
      "step": 443800
    },
    {
      "epoch": 17.167498162973278,
      "grad_norm": 12.45712947845459,
      "learning_rate": 3.5693751530855605e-05,
      "loss": 1.9775,
      "step": 443900
    },
    {
      "epoch": 17.17136558765518,
      "grad_norm": 10.875858306884766,
      "learning_rate": 3.569052867695401e-05,
      "loss": 1.9911,
      "step": 444000
    },
    {
      "epoch": 17.175233012337085,
      "grad_norm": 7.59896993637085,
      "learning_rate": 3.568730582305243e-05,
      "loss": 1.9284,
      "step": 444100
    },
    {
      "epoch": 17.179100437018988,
      "grad_norm": 11.353879928588867,
      "learning_rate": 3.568408296915084e-05,
      "loss": 2.0072,
      "step": 444200
    },
    {
      "epoch": 17.18296786170089,
      "grad_norm": 12.039572715759277,
      "learning_rate": 3.568086011524926e-05,
      "loss": 2.044,
      "step": 444300
    },
    {
      "epoch": 17.1868352863828,
      "grad_norm": 11.213566780090332,
      "learning_rate": 3.5677637261347665e-05,
      "loss": 2.0394,
      "step": 444400
    },
    {
      "epoch": 17.190702711064702,
      "grad_norm": 10.048968315124512,
      "learning_rate": 3.567441440744608e-05,
      "loss": 1.9508,
      "step": 444500
    },
    {
      "epoch": 17.194570135746606,
      "grad_norm": 11.394248962402344,
      "learning_rate": 3.5671191553544495e-05,
      "loss": 1.9661,
      "step": 444600
    },
    {
      "epoch": 17.19843756042851,
      "grad_norm": 14.861924171447754,
      "learning_rate": 3.566796869964291e-05,
      "loss": 2.0092,
      "step": 444700
    },
    {
      "epoch": 17.202304985110416,
      "grad_norm": 11.10249137878418,
      "learning_rate": 3.566474584574132e-05,
      "loss": 1.8943,
      "step": 444800
    },
    {
      "epoch": 17.20617240979232,
      "grad_norm": 10.862663269042969,
      "learning_rate": 3.566152299183973e-05,
      "loss": 1.9485,
      "step": 444900
    },
    {
      "epoch": 17.210039834474223,
      "grad_norm": 8.159753799438477,
      "learning_rate": 3.565830013793815e-05,
      "loss": 1.9765,
      "step": 445000
    },
    {
      "epoch": 17.213907259156127,
      "grad_norm": 10.383018493652344,
      "learning_rate": 3.565507728403656e-05,
      "loss": 2.061,
      "step": 445100
    },
    {
      "epoch": 17.217774683838034,
      "grad_norm": 10.695944786071777,
      "learning_rate": 3.5651854430134976e-05,
      "loss": 1.9149,
      "step": 445200
    },
    {
      "epoch": 17.221642108519937,
      "grad_norm": 11.071045875549316,
      "learning_rate": 3.564863157623339e-05,
      "loss": 2.0133,
      "step": 445300
    },
    {
      "epoch": 17.22550953320184,
      "grad_norm": 11.009713172912598,
      "learning_rate": 3.56454087223318e-05,
      "loss": 1.9954,
      "step": 445400
    },
    {
      "epoch": 17.229376957883744,
      "grad_norm": 11.697495460510254,
      "learning_rate": 3.5642185868430214e-05,
      "loss": 1.9974,
      "step": 445500
    },
    {
      "epoch": 17.23324438256565,
      "grad_norm": 9.452864646911621,
      "learning_rate": 3.563896301452863e-05,
      "loss": 1.944,
      "step": 445600
    },
    {
      "epoch": 17.237111807247555,
      "grad_norm": 11.508393287658691,
      "learning_rate": 3.563574016062704e-05,
      "loss": 1.9007,
      "step": 445700
    },
    {
      "epoch": 17.240979231929458,
      "grad_norm": 13.503132820129395,
      "learning_rate": 3.563251730672546e-05,
      "loss": 1.9764,
      "step": 445800
    },
    {
      "epoch": 17.24484665661136,
      "grad_norm": 10.46109390258789,
      "learning_rate": 3.5629294452823866e-05,
      "loss": 2.0513,
      "step": 445900
    },
    {
      "epoch": 17.248714081293265,
      "grad_norm": 6.935852527618408,
      "learning_rate": 3.562607159892228e-05,
      "loss": 2.0621,
      "step": 446000
    },
    {
      "epoch": 17.252581505975172,
      "grad_norm": 12.701107025146484,
      "learning_rate": 3.5622848745020695e-05,
      "loss": 2.0681,
      "step": 446100
    },
    {
      "epoch": 17.256448930657076,
      "grad_norm": 14.493860244750977,
      "learning_rate": 3.561962589111911e-05,
      "loss": 2.0332,
      "step": 446200
    },
    {
      "epoch": 17.26031635533898,
      "grad_norm": 10.815418243408203,
      "learning_rate": 3.561640303721752e-05,
      "loss": 1.93,
      "step": 446300
    },
    {
      "epoch": 17.264183780020883,
      "grad_norm": 11.204500198364258,
      "learning_rate": 3.561318018331593e-05,
      "loss": 1.897,
      "step": 446400
    },
    {
      "epoch": 17.26805120470279,
      "grad_norm": 12.188545227050781,
      "learning_rate": 3.560995732941435e-05,
      "loss": 2.1061,
      "step": 446500
    },
    {
      "epoch": 17.271918629384693,
      "grad_norm": 11.155414581298828,
      "learning_rate": 3.560673447551276e-05,
      "loss": 1.8824,
      "step": 446600
    },
    {
      "epoch": 17.275786054066597,
      "grad_norm": 12.20650577545166,
      "learning_rate": 3.560351162161117e-05,
      "loss": 2.1452,
      "step": 446700
    },
    {
      "epoch": 17.2796534787485,
      "grad_norm": 13.62680435180664,
      "learning_rate": 3.5600288767709585e-05,
      "loss": 1.9556,
      "step": 446800
    },
    {
      "epoch": 17.283520903430407,
      "grad_norm": 12.602659225463867,
      "learning_rate": 3.5597065913808e-05,
      "loss": 1.9978,
      "step": 446900
    },
    {
      "epoch": 17.28738832811231,
      "grad_norm": 12.705059051513672,
      "learning_rate": 3.5593843059906414e-05,
      "loss": 2.033,
      "step": 447000
    },
    {
      "epoch": 17.291255752794214,
      "grad_norm": 12.41601276397705,
      "learning_rate": 3.559062020600482e-05,
      "loss": 1.9543,
      "step": 447100
    },
    {
      "epoch": 17.295123177476118,
      "grad_norm": 10.871098518371582,
      "learning_rate": 3.558739735210324e-05,
      "loss": 1.9967,
      "step": 447200
    },
    {
      "epoch": 17.298990602158025,
      "grad_norm": 11.437078475952148,
      "learning_rate": 3.558417449820165e-05,
      "loss": 1.9509,
      "step": 447300
    },
    {
      "epoch": 17.302858026839928,
      "grad_norm": 12.20739459991455,
      "learning_rate": 3.5580951644300066e-05,
      "loss": 1.8919,
      "step": 447400
    },
    {
      "epoch": 17.30672545152183,
      "grad_norm": 11.860054016113281,
      "learning_rate": 3.5577728790398474e-05,
      "loss": 1.9597,
      "step": 447500
    },
    {
      "epoch": 17.310592876203735,
      "grad_norm": 12.694429397583008,
      "learning_rate": 3.557450593649689e-05,
      "loss": 2.0331,
      "step": 447600
    },
    {
      "epoch": 17.31446030088564,
      "grad_norm": 17.42176055908203,
      "learning_rate": 3.5571283082595304e-05,
      "loss": 1.9208,
      "step": 447700
    },
    {
      "epoch": 17.318327725567546,
      "grad_norm": 12.202132225036621,
      "learning_rate": 3.556806022869372e-05,
      "loss": 2.0131,
      "step": 447800
    },
    {
      "epoch": 17.32219515024945,
      "grad_norm": 11.415687561035156,
      "learning_rate": 3.5564837374792126e-05,
      "loss": 2.0299,
      "step": 447900
    },
    {
      "epoch": 17.326062574931353,
      "grad_norm": 11.849753379821777,
      "learning_rate": 3.556161452089054e-05,
      "loss": 2.013,
      "step": 448000
    },
    {
      "epoch": 17.329929999613256,
      "grad_norm": 12.681344032287598,
      "learning_rate": 3.5558391666988956e-05,
      "loss": 2.0158,
      "step": 448100
    },
    {
      "epoch": 17.333797424295163,
      "grad_norm": 8.706510543823242,
      "learning_rate": 3.555516881308737e-05,
      "loss": 1.9648,
      "step": 448200
    },
    {
      "epoch": 17.337664848977067,
      "grad_norm": 10.593148231506348,
      "learning_rate": 3.555194595918578e-05,
      "loss": 2.0104,
      "step": 448300
    },
    {
      "epoch": 17.34153227365897,
      "grad_norm": 15.724381446838379,
      "learning_rate": 3.554872310528419e-05,
      "loss": 2.1188,
      "step": 448400
    },
    {
      "epoch": 17.345399698340874,
      "grad_norm": 12.231710433959961,
      "learning_rate": 3.554550025138261e-05,
      "loss": 2.0357,
      "step": 448500
    },
    {
      "epoch": 17.34926712302278,
      "grad_norm": 12.522404670715332,
      "learning_rate": 3.5542277397481016e-05,
      "loss": 2.0438,
      "step": 448600
    },
    {
      "epoch": 17.353134547704684,
      "grad_norm": 13.070465087890625,
      "learning_rate": 3.553905454357943e-05,
      "loss": 2.0275,
      "step": 448700
    },
    {
      "epoch": 17.357001972386588,
      "grad_norm": 11.003079414367676,
      "learning_rate": 3.5535831689677845e-05,
      "loss": 1.9255,
      "step": 448800
    },
    {
      "epoch": 17.36086939706849,
      "grad_norm": 9.746100425720215,
      "learning_rate": 3.553260883577626e-05,
      "loss": 2.0139,
      "step": 448900
    },
    {
      "epoch": 17.364736821750398,
      "grad_norm": 12.753843307495117,
      "learning_rate": 3.552938598187467e-05,
      "loss": 1.9868,
      "step": 449000
    },
    {
      "epoch": 17.3686042464323,
      "grad_norm": 13.921642303466797,
      "learning_rate": 3.552616312797308e-05,
      "loss": 2.0051,
      "step": 449100
    },
    {
      "epoch": 17.372471671114205,
      "grad_norm": 10.898523330688477,
      "learning_rate": 3.55229402740715e-05,
      "loss": 2.0182,
      "step": 449200
    },
    {
      "epoch": 17.37633909579611,
      "grad_norm": 8.693769454956055,
      "learning_rate": 3.551971742016991e-05,
      "loss": 1.939,
      "step": 449300
    },
    {
      "epoch": 17.380206520478012,
      "grad_norm": 9.590450286865234,
      "learning_rate": 3.551649456626832e-05,
      "loss": 2.0043,
      "step": 449400
    },
    {
      "epoch": 17.38407394515992,
      "grad_norm": 14.188753128051758,
      "learning_rate": 3.5513271712366735e-05,
      "loss": 1.9624,
      "step": 449500
    },
    {
      "epoch": 17.387941369841823,
      "grad_norm": 12.243080139160156,
      "learning_rate": 3.551004885846515e-05,
      "loss": 1.9744,
      "step": 449600
    },
    {
      "epoch": 17.391808794523726,
      "grad_norm": 15.326811790466309,
      "learning_rate": 3.5506826004563564e-05,
      "loss": 2.0048,
      "step": 449700
    },
    {
      "epoch": 17.39567621920563,
      "grad_norm": 15.102119445800781,
      "learning_rate": 3.550360315066197e-05,
      "loss": 2.0203,
      "step": 449800
    },
    {
      "epoch": 17.399543643887537,
      "grad_norm": 12.600934028625488,
      "learning_rate": 3.550038029676039e-05,
      "loss": 1.9321,
      "step": 449900
    },
    {
      "epoch": 17.40341106856944,
      "grad_norm": 9.666574478149414,
      "learning_rate": 3.54971574428588e-05,
      "loss": 1.9899,
      "step": 450000
    },
    {
      "epoch": 17.407278493251344,
      "grad_norm": 14.141508102416992,
      "learning_rate": 3.5493934588957216e-05,
      "loss": 1.9505,
      "step": 450100
    },
    {
      "epoch": 17.411145917933247,
      "grad_norm": 12.723197937011719,
      "learning_rate": 3.5490711735055624e-05,
      "loss": 2.0596,
      "step": 450200
    },
    {
      "epoch": 17.415013342615154,
      "grad_norm": 9.92098617553711,
      "learning_rate": 3.548748888115404e-05,
      "loss": 1.9993,
      "step": 450300
    },
    {
      "epoch": 17.418880767297058,
      "grad_norm": 10.616270065307617,
      "learning_rate": 3.5484266027252454e-05,
      "loss": 1.9338,
      "step": 450400
    },
    {
      "epoch": 17.42274819197896,
      "grad_norm": 11.628856658935547,
      "learning_rate": 3.548104317335087e-05,
      "loss": 1.9985,
      "step": 450500
    },
    {
      "epoch": 17.426615616660865,
      "grad_norm": 10.956991195678711,
      "learning_rate": 3.5477820319449276e-05,
      "loss": 2.0181,
      "step": 450600
    },
    {
      "epoch": 17.430483041342768,
      "grad_norm": 12.185863494873047,
      "learning_rate": 3.547459746554769e-05,
      "loss": 1.9009,
      "step": 450700
    },
    {
      "epoch": 17.434350466024675,
      "grad_norm": 11.840989112854004,
      "learning_rate": 3.5471374611646106e-05,
      "loss": 2.0012,
      "step": 450800
    },
    {
      "epoch": 17.43821789070658,
      "grad_norm": 10.013209342956543,
      "learning_rate": 3.546815175774452e-05,
      "loss": 1.8985,
      "step": 450900
    },
    {
      "epoch": 17.442085315388482,
      "grad_norm": 15.243973731994629,
      "learning_rate": 3.546492890384293e-05,
      "loss": 2.041,
      "step": 451000
    },
    {
      "epoch": 17.445952740070386,
      "grad_norm": 10.75255298614502,
      "learning_rate": 3.546170604994134e-05,
      "loss": 1.9893,
      "step": 451100
    },
    {
      "epoch": 17.449820164752293,
      "grad_norm": 11.094529151916504,
      "learning_rate": 3.545848319603976e-05,
      "loss": 2.0581,
      "step": 451200
    },
    {
      "epoch": 17.453687589434196,
      "grad_norm": 12.784249305725098,
      "learning_rate": 3.545526034213817e-05,
      "loss": 1.9051,
      "step": 451300
    },
    {
      "epoch": 17.4575550141161,
      "grad_norm": 13.762669563293457,
      "learning_rate": 3.545203748823658e-05,
      "loss": 1.9789,
      "step": 451400
    },
    {
      "epoch": 17.461422438798003,
      "grad_norm": 11.46554183959961,
      "learning_rate": 3.5448814634334995e-05,
      "loss": 1.9562,
      "step": 451500
    },
    {
      "epoch": 17.46528986347991,
      "grad_norm": 10.008102416992188,
      "learning_rate": 3.544559178043341e-05,
      "loss": 2.0028,
      "step": 451600
    },
    {
      "epoch": 17.469157288161814,
      "grad_norm": 9.82552719116211,
      "learning_rate": 3.5442368926531825e-05,
      "loss": 2.0136,
      "step": 451700
    },
    {
      "epoch": 17.473024712843717,
      "grad_norm": 9.45847225189209,
      "learning_rate": 3.543914607263023e-05,
      "loss": 1.9888,
      "step": 451800
    },
    {
      "epoch": 17.47689213752562,
      "grad_norm": 13.271608352661133,
      "learning_rate": 3.543592321872865e-05,
      "loss": 2.0791,
      "step": 451900
    },
    {
      "epoch": 17.480759562207528,
      "grad_norm": 7.590761661529541,
      "learning_rate": 3.543270036482706e-05,
      "loss": 1.9144,
      "step": 452000
    },
    {
      "epoch": 17.48462698688943,
      "grad_norm": 12.943465232849121,
      "learning_rate": 3.542947751092548e-05,
      "loss": 1.906,
      "step": 452100
    },
    {
      "epoch": 17.488494411571335,
      "grad_norm": 14.761919021606445,
      "learning_rate": 3.542625465702389e-05,
      "loss": 2.0777,
      "step": 452200
    },
    {
      "epoch": 17.492361836253238,
      "grad_norm": 14.31718635559082,
      "learning_rate": 3.5423031803122306e-05,
      "loss": 1.9687,
      "step": 452300
    },
    {
      "epoch": 17.496229260935145,
      "grad_norm": 14.262212753295898,
      "learning_rate": 3.5419808949220714e-05,
      "loss": 1.9473,
      "step": 452400
    },
    {
      "epoch": 17.50009668561705,
      "grad_norm": 13.752326011657715,
      "learning_rate": 3.541658609531913e-05,
      "loss": 2.0473,
      "step": 452500
    },
    {
      "epoch": 17.503964110298952,
      "grad_norm": 9.110642433166504,
      "learning_rate": 3.5413363241417544e-05,
      "loss": 2.0025,
      "step": 452600
    },
    {
      "epoch": 17.507831534980856,
      "grad_norm": 11.376420021057129,
      "learning_rate": 3.541014038751596e-05,
      "loss": 2.0776,
      "step": 452700
    },
    {
      "epoch": 17.51169895966276,
      "grad_norm": 11.828326225280762,
      "learning_rate": 3.540691753361437e-05,
      "loss": 2.1006,
      "step": 452800
    },
    {
      "epoch": 17.515566384344666,
      "grad_norm": 17.237449645996094,
      "learning_rate": 3.540369467971278e-05,
      "loss": 1.9779,
      "step": 452900
    },
    {
      "epoch": 17.51943380902657,
      "grad_norm": 9.843950271606445,
      "learning_rate": 3.5400471825811196e-05,
      "loss": 2.0549,
      "step": 453000
    },
    {
      "epoch": 17.523301233708473,
      "grad_norm": 10.943613052368164,
      "learning_rate": 3.539724897190961e-05,
      "loss": 2.0084,
      "step": 453100
    },
    {
      "epoch": 17.527168658390377,
      "grad_norm": 12.044181823730469,
      "learning_rate": 3.5394026118008025e-05,
      "loss": 1.9556,
      "step": 453200
    },
    {
      "epoch": 17.531036083072284,
      "grad_norm": 9.886974334716797,
      "learning_rate": 3.539080326410643e-05,
      "loss": 1.8581,
      "step": 453300
    },
    {
      "epoch": 17.534903507754187,
      "grad_norm": 10.74547290802002,
      "learning_rate": 3.538758041020485e-05,
      "loss": 2.0206,
      "step": 453400
    },
    {
      "epoch": 17.53877093243609,
      "grad_norm": 11.945623397827148,
      "learning_rate": 3.538435755630326e-05,
      "loss": 1.9803,
      "step": 453500
    },
    {
      "epoch": 17.542638357117994,
      "grad_norm": 9.360031127929688,
      "learning_rate": 3.538113470240168e-05,
      "loss": 1.9698,
      "step": 453600
    },
    {
      "epoch": 17.5465057817999,
      "grad_norm": 11.38215446472168,
      "learning_rate": 3.5377911848500085e-05,
      "loss": 2.0756,
      "step": 453700
    },
    {
      "epoch": 17.550373206481805,
      "grad_norm": 11.828238487243652,
      "learning_rate": 3.53746889945985e-05,
      "loss": 1.9481,
      "step": 453800
    },
    {
      "epoch": 17.554240631163708,
      "grad_norm": 12.853253364562988,
      "learning_rate": 3.5371466140696915e-05,
      "loss": 2.089,
      "step": 453900
    },
    {
      "epoch": 17.55810805584561,
      "grad_norm": 13.460853576660156,
      "learning_rate": 3.536824328679533e-05,
      "loss": 1.9871,
      "step": 454000
    },
    {
      "epoch": 17.561975480527515,
      "grad_norm": 11.52396297454834,
      "learning_rate": 3.536502043289374e-05,
      "loss": 2.0014,
      "step": 454100
    },
    {
      "epoch": 17.565842905209422,
      "grad_norm": 12.194908142089844,
      "learning_rate": 3.536179757899215e-05,
      "loss": 2.0618,
      "step": 454200
    },
    {
      "epoch": 17.569710329891326,
      "grad_norm": 11.021302223205566,
      "learning_rate": 3.535857472509057e-05,
      "loss": 1.945,
      "step": 454300
    },
    {
      "epoch": 17.57357775457323,
      "grad_norm": 16.65569496154785,
      "learning_rate": 3.5355351871188975e-05,
      "loss": 2.0067,
      "step": 454400
    },
    {
      "epoch": 17.577445179255133,
      "grad_norm": 9.379312515258789,
      "learning_rate": 3.535212901728739e-05,
      "loss": 2.0741,
      "step": 454500
    },
    {
      "epoch": 17.58131260393704,
      "grad_norm": 11.334046363830566,
      "learning_rate": 3.5348906163385804e-05,
      "loss": 1.9725,
      "step": 454600
    },
    {
      "epoch": 17.585180028618943,
      "grad_norm": 10.685446739196777,
      "learning_rate": 3.534568330948422e-05,
      "loss": 1.9921,
      "step": 454700
    },
    {
      "epoch": 17.589047453300847,
      "grad_norm": 14.594091415405273,
      "learning_rate": 3.534246045558263e-05,
      "loss": 2.0565,
      "step": 454800
    },
    {
      "epoch": 17.59291487798275,
      "grad_norm": 13.720730781555176,
      "learning_rate": 3.533923760168104e-05,
      "loss": 1.9639,
      "step": 454900
    },
    {
      "epoch": 17.596782302664657,
      "grad_norm": 12.99564266204834,
      "learning_rate": 3.5336014747779456e-05,
      "loss": 1.9751,
      "step": 455000
    },
    {
      "epoch": 17.60064972734656,
      "grad_norm": 11.297820091247559,
      "learning_rate": 3.533279189387787e-05,
      "loss": 1.988,
      "step": 455100
    },
    {
      "epoch": 17.604517152028464,
      "grad_norm": 11.09754467010498,
      "learning_rate": 3.532956903997628e-05,
      "loss": 1.9687,
      "step": 455200
    },
    {
      "epoch": 17.608384576710367,
      "grad_norm": 10.857353210449219,
      "learning_rate": 3.5326346186074694e-05,
      "loss": 2.0363,
      "step": 455300
    },
    {
      "epoch": 17.612252001392275,
      "grad_norm": 11.903020858764648,
      "learning_rate": 3.532312333217311e-05,
      "loss": 2.0787,
      "step": 455400
    },
    {
      "epoch": 17.616119426074178,
      "grad_norm": 9.476290702819824,
      "learning_rate": 3.531990047827152e-05,
      "loss": 2.0752,
      "step": 455500
    },
    {
      "epoch": 17.61998685075608,
      "grad_norm": 8.548230171203613,
      "learning_rate": 3.531667762436993e-05,
      "loss": 1.9713,
      "step": 455600
    },
    {
      "epoch": 17.623854275437985,
      "grad_norm": 14.232909202575684,
      "learning_rate": 3.5313454770468346e-05,
      "loss": 2.1044,
      "step": 455700
    },
    {
      "epoch": 17.62772170011989,
      "grad_norm": 10.623344421386719,
      "learning_rate": 3.531023191656676e-05,
      "loss": 1.8739,
      "step": 455800
    },
    {
      "epoch": 17.631589124801796,
      "grad_norm": 12.4641695022583,
      "learning_rate": 3.5307009062665175e-05,
      "loss": 2.0505,
      "step": 455900
    },
    {
      "epoch": 17.6354565494837,
      "grad_norm": 16.3182315826416,
      "learning_rate": 3.530378620876358e-05,
      "loss": 2.0184,
      "step": 456000
    },
    {
      "epoch": 17.639323974165602,
      "grad_norm": 13.004825592041016,
      "learning_rate": 3.5300563354862e-05,
      "loss": 2.0085,
      "step": 456100
    },
    {
      "epoch": 17.643191398847506,
      "grad_norm": 12.676782608032227,
      "learning_rate": 3.529734050096041e-05,
      "loss": 1.9658,
      "step": 456200
    },
    {
      "epoch": 17.647058823529413,
      "grad_norm": 10.269556045532227,
      "learning_rate": 3.529411764705883e-05,
      "loss": 2.0001,
      "step": 456300
    },
    {
      "epoch": 17.650926248211317,
      "grad_norm": 16.119365692138672,
      "learning_rate": 3.5290894793157235e-05,
      "loss": 2.0901,
      "step": 456400
    },
    {
      "epoch": 17.65479367289322,
      "grad_norm": 8.482534408569336,
      "learning_rate": 3.528767193925565e-05,
      "loss": 2.0037,
      "step": 456500
    },
    {
      "epoch": 17.658661097575123,
      "grad_norm": 12.6995267868042,
      "learning_rate": 3.5284449085354065e-05,
      "loss": 1.9704,
      "step": 456600
    },
    {
      "epoch": 17.66252852225703,
      "grad_norm": 8.810532569885254,
      "learning_rate": 3.528122623145248e-05,
      "loss": 1.9855,
      "step": 456700
    },
    {
      "epoch": 17.666395946938934,
      "grad_norm": 12.21247386932373,
      "learning_rate": 3.527800337755089e-05,
      "loss": 2.0012,
      "step": 456800
    },
    {
      "epoch": 17.670263371620837,
      "grad_norm": 12.628475189208984,
      "learning_rate": 3.52747805236493e-05,
      "loss": 2.0538,
      "step": 456900
    },
    {
      "epoch": 17.67413079630274,
      "grad_norm": 12.184904098510742,
      "learning_rate": 3.527155766974772e-05,
      "loss": 1.9294,
      "step": 457000
    },
    {
      "epoch": 17.677998220984648,
      "grad_norm": 16.622093200683594,
      "learning_rate": 3.526833481584613e-05,
      "loss": 2.0381,
      "step": 457100
    },
    {
      "epoch": 17.68186564566655,
      "grad_norm": 12.8829345703125,
      "learning_rate": 3.526511196194454e-05,
      "loss": 1.9419,
      "step": 457200
    },
    {
      "epoch": 17.685733070348455,
      "grad_norm": 9.585583686828613,
      "learning_rate": 3.5261889108042954e-05,
      "loss": 1.8717,
      "step": 457300
    },
    {
      "epoch": 17.68960049503036,
      "grad_norm": 11.733933448791504,
      "learning_rate": 3.525866625414137e-05,
      "loss": 1.9706,
      "step": 457400
    },
    {
      "epoch": 17.693467919712262,
      "grad_norm": 11.1733980178833,
      "learning_rate": 3.525544340023978e-05,
      "loss": 1.912,
      "step": 457500
    },
    {
      "epoch": 17.69733534439417,
      "grad_norm": 11.709440231323242,
      "learning_rate": 3.525222054633819e-05,
      "loss": 2.0271,
      "step": 457600
    },
    {
      "epoch": 17.701202769076072,
      "grad_norm": 13.671208381652832,
      "learning_rate": 3.5248997692436606e-05,
      "loss": 1.9263,
      "step": 457700
    },
    {
      "epoch": 17.705070193757976,
      "grad_norm": 10.836134910583496,
      "learning_rate": 3.524577483853502e-05,
      "loss": 2.0323,
      "step": 457800
    },
    {
      "epoch": 17.70893761843988,
      "grad_norm": 11.510272979736328,
      "learning_rate": 3.524255198463343e-05,
      "loss": 2.0641,
      "step": 457900
    },
    {
      "epoch": 17.712805043121786,
      "grad_norm": 9.561972618103027,
      "learning_rate": 3.5239329130731844e-05,
      "loss": 2.0011,
      "step": 458000
    },
    {
      "epoch": 17.71667246780369,
      "grad_norm": 10.421770095825195,
      "learning_rate": 3.523610627683026e-05,
      "loss": 1.9545,
      "step": 458100
    },
    {
      "epoch": 17.720539892485593,
      "grad_norm": 13.439590454101562,
      "learning_rate": 3.523288342292867e-05,
      "loss": 1.9975,
      "step": 458200
    },
    {
      "epoch": 17.724407317167497,
      "grad_norm": 12.191556930541992,
      "learning_rate": 3.522966056902708e-05,
      "loss": 2.066,
      "step": 458300
    },
    {
      "epoch": 17.728274741849404,
      "grad_norm": 13.994904518127441,
      "learning_rate": 3.5226437715125496e-05,
      "loss": 2.036,
      "step": 458400
    },
    {
      "epoch": 17.732142166531307,
      "grad_norm": 7.630679130554199,
      "learning_rate": 3.522321486122391e-05,
      "loss": 1.9895,
      "step": 458500
    },
    {
      "epoch": 17.73600959121321,
      "grad_norm": 14.092985153198242,
      "learning_rate": 3.5219992007322325e-05,
      "loss": 2.0952,
      "step": 458600
    },
    {
      "epoch": 17.739877015895114,
      "grad_norm": 12.299671173095703,
      "learning_rate": 3.521676915342074e-05,
      "loss": 1.9762,
      "step": 458700
    },
    {
      "epoch": 17.743744440577018,
      "grad_norm": 10.350970268249512,
      "learning_rate": 3.5213546299519155e-05,
      "loss": 1.9043,
      "step": 458800
    },
    {
      "epoch": 17.747611865258925,
      "grad_norm": 11.87861442565918,
      "learning_rate": 3.521032344561756e-05,
      "loss": 2.0632,
      "step": 458900
    },
    {
      "epoch": 17.75147928994083,
      "grad_norm": 14.94326400756836,
      "learning_rate": 3.520710059171598e-05,
      "loss": 1.9878,
      "step": 459000
    },
    {
      "epoch": 17.755346714622732,
      "grad_norm": 12.873722076416016,
      "learning_rate": 3.520387773781439e-05,
      "loss": 1.908,
      "step": 459100
    },
    {
      "epoch": 17.759214139304635,
      "grad_norm": 10.818506240844727,
      "learning_rate": 3.520065488391281e-05,
      "loss": 2.0553,
      "step": 459200
    },
    {
      "epoch": 17.763081563986542,
      "grad_norm": 9.560935020446777,
      "learning_rate": 3.519743203001122e-05,
      "loss": 2.0014,
      "step": 459300
    },
    {
      "epoch": 17.766948988668446,
      "grad_norm": 9.655401229858398,
      "learning_rate": 3.519420917610963e-05,
      "loss": 1.9403,
      "step": 459400
    },
    {
      "epoch": 17.77081641335035,
      "grad_norm": 12.604172706604004,
      "learning_rate": 3.5190986322208044e-05,
      "loss": 2.0245,
      "step": 459500
    },
    {
      "epoch": 17.774683838032253,
      "grad_norm": 10.383402824401855,
      "learning_rate": 3.518776346830646e-05,
      "loss": 1.9483,
      "step": 459600
    },
    {
      "epoch": 17.77855126271416,
      "grad_norm": 11.718358039855957,
      "learning_rate": 3.5184540614404873e-05,
      "loss": 1.9752,
      "step": 459700
    },
    {
      "epoch": 17.782418687396063,
      "grad_norm": 12.445989608764648,
      "learning_rate": 3.518131776050329e-05,
      "loss": 1.9571,
      "step": 459800
    },
    {
      "epoch": 17.786286112077967,
      "grad_norm": 14.340237617492676,
      "learning_rate": 3.5178094906601696e-05,
      "loss": 2.0287,
      "step": 459900
    },
    {
      "epoch": 17.79015353675987,
      "grad_norm": 9.601197242736816,
      "learning_rate": 3.517487205270011e-05,
      "loss": 1.974,
      "step": 460000
    },
    {
      "epoch": 17.794020961441777,
      "grad_norm": 12.344989776611328,
      "learning_rate": 3.5171649198798526e-05,
      "loss": 2.0188,
      "step": 460100
    },
    {
      "epoch": 17.79788838612368,
      "grad_norm": 13.196504592895508,
      "learning_rate": 3.516842634489694e-05,
      "loss": 2.0898,
      "step": 460200
    },
    {
      "epoch": 17.801755810805584,
      "grad_norm": 12.224393844604492,
      "learning_rate": 3.516520349099535e-05,
      "loss": 2.0209,
      "step": 460300
    },
    {
      "epoch": 17.805623235487488,
      "grad_norm": 10.587154388427734,
      "learning_rate": 3.516198063709376e-05,
      "loss": 2.029,
      "step": 460400
    },
    {
      "epoch": 17.809490660169395,
      "grad_norm": 12.49648666381836,
      "learning_rate": 3.515875778319218e-05,
      "loss": 1.9676,
      "step": 460500
    },
    {
      "epoch": 17.8133580848513,
      "grad_norm": 11.9224853515625,
      "learning_rate": 3.5155534929290586e-05,
      "loss": 2.096,
      "step": 460600
    },
    {
      "epoch": 17.817225509533202,
      "grad_norm": 12.530851364135742,
      "learning_rate": 3.5152312075389e-05,
      "loss": 2.122,
      "step": 460700
    },
    {
      "epoch": 17.821092934215105,
      "grad_norm": 10.838528633117676,
      "learning_rate": 3.5149089221487415e-05,
      "loss": 1.9739,
      "step": 460800
    },
    {
      "epoch": 17.82496035889701,
      "grad_norm": 15.96381950378418,
      "learning_rate": 3.514586636758583e-05,
      "loss": 2.176,
      "step": 460900
    },
    {
      "epoch": 17.828827783578916,
      "grad_norm": 12.888776779174805,
      "learning_rate": 3.514264351368424e-05,
      "loss": 1.9121,
      "step": 461000
    },
    {
      "epoch": 17.83269520826082,
      "grad_norm": 8.722955703735352,
      "learning_rate": 3.513942065978265e-05,
      "loss": 2.0278,
      "step": 461100
    },
    {
      "epoch": 17.836562632942723,
      "grad_norm": 9.10502815246582,
      "learning_rate": 3.513619780588107e-05,
      "loss": 2.0189,
      "step": 461200
    },
    {
      "epoch": 17.840430057624626,
      "grad_norm": 11.132493019104004,
      "learning_rate": 3.513297495197948e-05,
      "loss": 1.9959,
      "step": 461300
    },
    {
      "epoch": 17.844297482306533,
      "grad_norm": 11.076569557189941,
      "learning_rate": 3.512975209807789e-05,
      "loss": 1.9986,
      "step": 461400
    },
    {
      "epoch": 17.848164906988437,
      "grad_norm": 12.729048728942871,
      "learning_rate": 3.5126529244176304e-05,
      "loss": 2.0247,
      "step": 461500
    },
    {
      "epoch": 17.85203233167034,
      "grad_norm": 14.173090934753418,
      "learning_rate": 3.512330639027472e-05,
      "loss": 2.0996,
      "step": 461600
    },
    {
      "epoch": 17.855899756352244,
      "grad_norm": 9.78724479675293,
      "learning_rate": 3.5120083536373134e-05,
      "loss": 2.0174,
      "step": 461700
    },
    {
      "epoch": 17.85976718103415,
      "grad_norm": 11.461037635803223,
      "learning_rate": 3.511686068247154e-05,
      "loss": 2.0327,
      "step": 461800
    },
    {
      "epoch": 17.863634605716054,
      "grad_norm": 12.473219871520996,
      "learning_rate": 3.5113637828569957e-05,
      "loss": 2.0356,
      "step": 461900
    },
    {
      "epoch": 17.867502030397958,
      "grad_norm": 12.493620872497559,
      "learning_rate": 3.511041497466837e-05,
      "loss": 1.9239,
      "step": 462000
    },
    {
      "epoch": 17.87136945507986,
      "grad_norm": 10.267698287963867,
      "learning_rate": 3.5107192120766786e-05,
      "loss": 2.0677,
      "step": 462100
    },
    {
      "epoch": 17.875236879761765,
      "grad_norm": 14.29891586303711,
      "learning_rate": 3.5103969266865194e-05,
      "loss": 2.0157,
      "step": 462200
    },
    {
      "epoch": 17.879104304443672,
      "grad_norm": 12.371689796447754,
      "learning_rate": 3.510074641296361e-05,
      "loss": 1.9602,
      "step": 462300
    },
    {
      "epoch": 17.882971729125575,
      "grad_norm": 12.000995635986328,
      "learning_rate": 3.5097523559062023e-05,
      "loss": 1.9619,
      "step": 462400
    },
    {
      "epoch": 17.88683915380748,
      "grad_norm": 11.42136287689209,
      "learning_rate": 3.509430070516044e-05,
      "loss": 1.9504,
      "step": 462500
    },
    {
      "epoch": 17.890706578489382,
      "grad_norm": 10.479399681091309,
      "learning_rate": 3.5091077851258846e-05,
      "loss": 1.9926,
      "step": 462600
    },
    {
      "epoch": 17.89457400317129,
      "grad_norm": 15.685012817382812,
      "learning_rate": 3.508785499735726e-05,
      "loss": 1.9674,
      "step": 462700
    },
    {
      "epoch": 17.898441427853193,
      "grad_norm": 12.462252616882324,
      "learning_rate": 3.5084632143455675e-05,
      "loss": 2.0023,
      "step": 462800
    },
    {
      "epoch": 17.902308852535096,
      "grad_norm": 11.8638916015625,
      "learning_rate": 3.508140928955409e-05,
      "loss": 2.0077,
      "step": 462900
    },
    {
      "epoch": 17.906176277217,
      "grad_norm": 10.567826271057129,
      "learning_rate": 3.50781864356525e-05,
      "loss": 2.0365,
      "step": 463000
    },
    {
      "epoch": 17.910043701898907,
      "grad_norm": 11.127703666687012,
      "learning_rate": 3.507496358175091e-05,
      "loss": 2.0263,
      "step": 463100
    },
    {
      "epoch": 17.91391112658081,
      "grad_norm": 13.8289794921875,
      "learning_rate": 3.507174072784933e-05,
      "loss": 1.9679,
      "step": 463200
    },
    {
      "epoch": 17.917778551262714,
      "grad_norm": 9.68741512298584,
      "learning_rate": 3.5068517873947736e-05,
      "loss": 2.0477,
      "step": 463300
    },
    {
      "epoch": 17.921645975944617,
      "grad_norm": 11.733344078063965,
      "learning_rate": 3.506529502004615e-05,
      "loss": 1.9415,
      "step": 463400
    },
    {
      "epoch": 17.925513400626524,
      "grad_norm": 10.763736724853516,
      "learning_rate": 3.5062072166144565e-05,
      "loss": 2.0033,
      "step": 463500
    },
    {
      "epoch": 17.929380825308428,
      "grad_norm": 9.620341300964355,
      "learning_rate": 3.505884931224298e-05,
      "loss": 2.0273,
      "step": 463600
    },
    {
      "epoch": 17.93324824999033,
      "grad_norm": 9.939863204956055,
      "learning_rate": 3.505562645834139e-05,
      "loss": 1.9362,
      "step": 463700
    },
    {
      "epoch": 17.937115674672235,
      "grad_norm": 11.464398384094238,
      "learning_rate": 3.50524036044398e-05,
      "loss": 2.0154,
      "step": 463800
    },
    {
      "epoch": 17.94098309935414,
      "grad_norm": 11.522024154663086,
      "learning_rate": 3.504918075053822e-05,
      "loss": 2.0957,
      "step": 463900
    },
    {
      "epoch": 17.944850524036045,
      "grad_norm": 11.761419296264648,
      "learning_rate": 3.504595789663663e-05,
      "loss": 1.9306,
      "step": 464000
    },
    {
      "epoch": 17.94871794871795,
      "grad_norm": 13.91826343536377,
      "learning_rate": 3.504273504273504e-05,
      "loss": 2.08,
      "step": 464100
    },
    {
      "epoch": 17.952585373399852,
      "grad_norm": 12.827596664428711,
      "learning_rate": 3.5039512188833454e-05,
      "loss": 2.037,
      "step": 464200
    },
    {
      "epoch": 17.956452798081756,
      "grad_norm": 13.068564414978027,
      "learning_rate": 3.503628933493187e-05,
      "loss": 2.0271,
      "step": 464300
    },
    {
      "epoch": 17.960320222763663,
      "grad_norm": 12.436453819274902,
      "learning_rate": 3.5033066481030284e-05,
      "loss": 2.0652,
      "step": 464400
    },
    {
      "epoch": 17.964187647445566,
      "grad_norm": 11.534713745117188,
      "learning_rate": 3.502984362712869e-05,
      "loss": 2.0625,
      "step": 464500
    },
    {
      "epoch": 17.96805507212747,
      "grad_norm": 12.671856880187988,
      "learning_rate": 3.5026620773227107e-05,
      "loss": 2.1216,
      "step": 464600
    },
    {
      "epoch": 17.971922496809373,
      "grad_norm": 9.563522338867188,
      "learning_rate": 3.502339791932552e-05,
      "loss": 2.0398,
      "step": 464700
    },
    {
      "epoch": 17.97578992149128,
      "grad_norm": 12.95406436920166,
      "learning_rate": 3.5020175065423936e-05,
      "loss": 2.0055,
      "step": 464800
    },
    {
      "epoch": 17.979657346173184,
      "grad_norm": 11.275650978088379,
      "learning_rate": 3.5016952211522344e-05,
      "loss": 1.9957,
      "step": 464900
    },
    {
      "epoch": 17.983524770855087,
      "grad_norm": 14.804715156555176,
      "learning_rate": 3.501372935762076e-05,
      "loss": 1.9979,
      "step": 465000
    },
    {
      "epoch": 17.98739219553699,
      "grad_norm": 11.994686126708984,
      "learning_rate": 3.501050650371917e-05,
      "loss": 2.0407,
      "step": 465100
    },
    {
      "epoch": 17.991259620218898,
      "grad_norm": 12.825509071350098,
      "learning_rate": 3.500728364981759e-05,
      "loss": 2.0406,
      "step": 465200
    },
    {
      "epoch": 17.9951270449008,
      "grad_norm": 10.927116394042969,
      "learning_rate": 3.5004060795916e-05,
      "loss": 2.0258,
      "step": 465300
    },
    {
      "epoch": 17.998994469582705,
      "grad_norm": 14.266277313232422,
      "learning_rate": 3.500083794201441e-05,
      "loss": 1.9772,
      "step": 465400
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.8887048959732056,
      "eval_runtime": 2.9237,
      "eval_samples_per_second": 465.501,
      "eval_steps_per_second": 465.501,
      "step": 465426
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.7992522716522217,
      "eval_runtime": 56.4681,
      "eval_samples_per_second": 457.904,
      "eval_steps_per_second": 457.904,
      "step": 465426
    },
    {
      "epoch": 18.00286189426461,
      "grad_norm": 10.005010604858398,
      "learning_rate": 3.4997615088112825e-05,
      "loss": 2.035,
      "step": 465500
    },
    {
      "epoch": 18.006729318946512,
      "grad_norm": 13.52267074584961,
      "learning_rate": 3.499439223421124e-05,
      "loss": 1.9539,
      "step": 465600
    },
    {
      "epoch": 18.01059674362842,
      "grad_norm": 10.288278579711914,
      "learning_rate": 3.4991169380309655e-05,
      "loss": 2.001,
      "step": 465700
    },
    {
      "epoch": 18.014464168310322,
      "grad_norm": 13.780441284179688,
      "learning_rate": 3.498794652640807e-05,
      "loss": 2.0093,
      "step": 465800
    },
    {
      "epoch": 18.018331592992226,
      "grad_norm": 12.815439224243164,
      "learning_rate": 3.498472367250648e-05,
      "loss": 1.955,
      "step": 465900
    },
    {
      "epoch": 18.02219901767413,
      "grad_norm": 14.9670991897583,
      "learning_rate": 3.498150081860489e-05,
      "loss": 1.9926,
      "step": 466000
    },
    {
      "epoch": 18.026066442356036,
      "grad_norm": 9.67039680480957,
      "learning_rate": 3.497827796470331e-05,
      "loss": 1.9759,
      "step": 466100
    },
    {
      "epoch": 18.02993386703794,
      "grad_norm": 9.31072998046875,
      "learning_rate": 3.497505511080172e-05,
      "loss": 2.0174,
      "step": 466200
    },
    {
      "epoch": 18.033801291719843,
      "grad_norm": 10.725494384765625,
      "learning_rate": 3.4971832256900136e-05,
      "loss": 1.9335,
      "step": 466300
    },
    {
      "epoch": 18.037668716401747,
      "grad_norm": 9.879188537597656,
      "learning_rate": 3.4968609402998544e-05,
      "loss": 2.0489,
      "step": 466400
    },
    {
      "epoch": 18.041536141083654,
      "grad_norm": 11.987593650817871,
      "learning_rate": 3.496538654909696e-05,
      "loss": 1.9278,
      "step": 466500
    },
    {
      "epoch": 18.045403565765557,
      "grad_norm": 9.947284698486328,
      "learning_rate": 3.4962163695195374e-05,
      "loss": 1.8679,
      "step": 466600
    },
    {
      "epoch": 18.04927099044746,
      "grad_norm": 8.754851341247559,
      "learning_rate": 3.495894084129379e-05,
      "loss": 2.0113,
      "step": 466700
    },
    {
      "epoch": 18.053138415129364,
      "grad_norm": 14.725591659545898,
      "learning_rate": 3.4955717987392196e-05,
      "loss": 2.0356,
      "step": 466800
    },
    {
      "epoch": 18.05700583981127,
      "grad_norm": 11.598237991333008,
      "learning_rate": 3.495249513349061e-05,
      "loss": 2.036,
      "step": 466900
    },
    {
      "epoch": 18.060873264493175,
      "grad_norm": 11.535664558410645,
      "learning_rate": 3.4949272279589026e-05,
      "loss": 2.1432,
      "step": 467000
    },
    {
      "epoch": 18.06474068917508,
      "grad_norm": 9.127494812011719,
      "learning_rate": 3.494604942568744e-05,
      "loss": 2.0869,
      "step": 467100
    },
    {
      "epoch": 18.068608113856982,
      "grad_norm": 10.393512725830078,
      "learning_rate": 3.494282657178585e-05,
      "loss": 1.9989,
      "step": 467200
    },
    {
      "epoch": 18.072475538538885,
      "grad_norm": 11.890617370605469,
      "learning_rate": 3.493960371788426e-05,
      "loss": 1.9257,
      "step": 467300
    },
    {
      "epoch": 18.076342963220792,
      "grad_norm": 11.695043563842773,
      "learning_rate": 3.493638086398268e-05,
      "loss": 1.9329,
      "step": 467400
    },
    {
      "epoch": 18.080210387902696,
      "grad_norm": 8.82636547088623,
      "learning_rate": 3.493315801008109e-05,
      "loss": 1.8943,
      "step": 467500
    },
    {
      "epoch": 18.0840778125846,
      "grad_norm": 12.404742240905762,
      "learning_rate": 3.49299351561795e-05,
      "loss": 1.9177,
      "step": 467600
    },
    {
      "epoch": 18.087945237266503,
      "grad_norm": 8.637872695922852,
      "learning_rate": 3.4926712302277915e-05,
      "loss": 1.975,
      "step": 467700
    },
    {
      "epoch": 18.09181266194841,
      "grad_norm": 12.877799034118652,
      "learning_rate": 3.492348944837633e-05,
      "loss": 1.995,
      "step": 467800
    },
    {
      "epoch": 18.095680086630313,
      "grad_norm": 13.429688453674316,
      "learning_rate": 3.4920266594474745e-05,
      "loss": 1.9951,
      "step": 467900
    },
    {
      "epoch": 18.099547511312217,
      "grad_norm": 11.227941513061523,
      "learning_rate": 3.491704374057315e-05,
      "loss": 1.9597,
      "step": 468000
    },
    {
      "epoch": 18.10341493599412,
      "grad_norm": 11.610710144042969,
      "learning_rate": 3.491382088667157e-05,
      "loss": 2.031,
      "step": 468100
    },
    {
      "epoch": 18.107282360676027,
      "grad_norm": 11.713879585266113,
      "learning_rate": 3.491059803276998e-05,
      "loss": 1.9767,
      "step": 468200
    },
    {
      "epoch": 18.11114978535793,
      "grad_norm": 10.025410652160645,
      "learning_rate": 3.49073751788684e-05,
      "loss": 2.0123,
      "step": 468300
    },
    {
      "epoch": 18.115017210039834,
      "grad_norm": 15.566941261291504,
      "learning_rate": 3.4904152324966805e-05,
      "loss": 1.9718,
      "step": 468400
    },
    {
      "epoch": 18.118884634721738,
      "grad_norm": 13.077680587768555,
      "learning_rate": 3.490092947106522e-05,
      "loss": 2.0045,
      "step": 468500
    },
    {
      "epoch": 18.122752059403645,
      "grad_norm": 12.568461418151855,
      "learning_rate": 3.4897706617163634e-05,
      "loss": 1.9367,
      "step": 468600
    },
    {
      "epoch": 18.12661948408555,
      "grad_norm": 11.503432273864746,
      "learning_rate": 3.489448376326205e-05,
      "loss": 2.131,
      "step": 468700
    },
    {
      "epoch": 18.130486908767452,
      "grad_norm": 10.631775856018066,
      "learning_rate": 3.489126090936046e-05,
      "loss": 1.9941,
      "step": 468800
    },
    {
      "epoch": 18.134354333449355,
      "grad_norm": 9.469435691833496,
      "learning_rate": 3.488803805545887e-05,
      "loss": 1.9609,
      "step": 468900
    },
    {
      "epoch": 18.13822175813126,
      "grad_norm": 10.458630561828613,
      "learning_rate": 3.4884815201557286e-05,
      "loss": 1.8641,
      "step": 469000
    },
    {
      "epoch": 18.142089182813166,
      "grad_norm": 10.475102424621582,
      "learning_rate": 3.48815923476557e-05,
      "loss": 2.0196,
      "step": 469100
    },
    {
      "epoch": 18.14595660749507,
      "grad_norm": 12.575079917907715,
      "learning_rate": 3.487836949375411e-05,
      "loss": 1.9144,
      "step": 469200
    },
    {
      "epoch": 18.149824032176973,
      "grad_norm": 11.622980117797852,
      "learning_rate": 3.4875146639852524e-05,
      "loss": 1.9442,
      "step": 469300
    },
    {
      "epoch": 18.153691456858876,
      "grad_norm": 16.886743545532227,
      "learning_rate": 3.487192378595094e-05,
      "loss": 2.0077,
      "step": 469400
    },
    {
      "epoch": 18.157558881540783,
      "grad_norm": 10.064985275268555,
      "learning_rate": 3.4868700932049346e-05,
      "loss": 2.0006,
      "step": 469500
    },
    {
      "epoch": 18.161426306222687,
      "grad_norm": 11.792484283447266,
      "learning_rate": 3.486547807814776e-05,
      "loss": 1.9996,
      "step": 469600
    },
    {
      "epoch": 18.16529373090459,
      "grad_norm": 11.835709571838379,
      "learning_rate": 3.4862255224246176e-05,
      "loss": 1.9477,
      "step": 469700
    },
    {
      "epoch": 18.169161155586494,
      "grad_norm": 15.33974838256836,
      "learning_rate": 3.485903237034459e-05,
      "loss": 1.9694,
      "step": 469800
    },
    {
      "epoch": 18.1730285802684,
      "grad_norm": 7.540602207183838,
      "learning_rate": 3.4855809516443e-05,
      "loss": 1.8609,
      "step": 469900
    },
    {
      "epoch": 18.176896004950304,
      "grad_norm": 10.756011962890625,
      "learning_rate": 3.485258666254141e-05,
      "loss": 1.9835,
      "step": 470000
    },
    {
      "epoch": 18.180763429632208,
      "grad_norm": 9.845881462097168,
      "learning_rate": 3.484936380863983e-05,
      "loss": 2.0137,
      "step": 470100
    },
    {
      "epoch": 18.18463085431411,
      "grad_norm": 10.427397727966309,
      "learning_rate": 3.484614095473824e-05,
      "loss": 2.0078,
      "step": 470200
    },
    {
      "epoch": 18.188498278996015,
      "grad_norm": 12.056800842285156,
      "learning_rate": 3.484291810083665e-05,
      "loss": 1.9279,
      "step": 470300
    },
    {
      "epoch": 18.192365703677922,
      "grad_norm": 12.971203804016113,
      "learning_rate": 3.4839695246935065e-05,
      "loss": 1.9975,
      "step": 470400
    },
    {
      "epoch": 18.196233128359825,
      "grad_norm": 10.88034725189209,
      "learning_rate": 3.483647239303348e-05,
      "loss": 1.9872,
      "step": 470500
    },
    {
      "epoch": 18.20010055304173,
      "grad_norm": 10.152660369873047,
      "learning_rate": 3.4833249539131895e-05,
      "loss": 2.0335,
      "step": 470600
    },
    {
      "epoch": 18.203967977723632,
      "grad_norm": 8.919103622436523,
      "learning_rate": 3.48300266852303e-05,
      "loss": 1.8894,
      "step": 470700
    },
    {
      "epoch": 18.20783540240554,
      "grad_norm": 11.366605758666992,
      "learning_rate": 3.482680383132872e-05,
      "loss": 2.039,
      "step": 470800
    },
    {
      "epoch": 18.211702827087443,
      "grad_norm": 9.261733055114746,
      "learning_rate": 3.482358097742713e-05,
      "loss": 1.9846,
      "step": 470900
    },
    {
      "epoch": 18.215570251769346,
      "grad_norm": 11.909956932067871,
      "learning_rate": 3.482035812352555e-05,
      "loss": 2.0819,
      "step": 471000
    },
    {
      "epoch": 18.21943767645125,
      "grad_norm": 10.992744445800781,
      "learning_rate": 3.4817135269623955e-05,
      "loss": 1.9972,
      "step": 471100
    },
    {
      "epoch": 18.223305101133157,
      "grad_norm": 10.851037979125977,
      "learning_rate": 3.481391241572237e-05,
      "loss": 2.0314,
      "step": 471200
    },
    {
      "epoch": 18.22717252581506,
      "grad_norm": 9.949102401733398,
      "learning_rate": 3.4810689561820784e-05,
      "loss": 2.0478,
      "step": 471300
    },
    {
      "epoch": 18.231039950496964,
      "grad_norm": 10.764656066894531,
      "learning_rate": 3.48074667079192e-05,
      "loss": 2.0326,
      "step": 471400
    },
    {
      "epoch": 18.234907375178867,
      "grad_norm": 12.233837127685547,
      "learning_rate": 3.480424385401761e-05,
      "loss": 1.9282,
      "step": 471500
    },
    {
      "epoch": 18.238774799860774,
      "grad_norm": 9.468387603759766,
      "learning_rate": 3.480102100011602e-05,
      "loss": 1.9193,
      "step": 471600
    },
    {
      "epoch": 18.242642224542678,
      "grad_norm": 14.121856689453125,
      "learning_rate": 3.4797798146214436e-05,
      "loss": 2.0104,
      "step": 471700
    },
    {
      "epoch": 18.24650964922458,
      "grad_norm": 9.445984840393066,
      "learning_rate": 3.479457529231285e-05,
      "loss": 2.0928,
      "step": 471800
    },
    {
      "epoch": 18.250377073906485,
      "grad_norm": 9.829970359802246,
      "learning_rate": 3.479135243841126e-05,
      "loss": 1.9849,
      "step": 471900
    },
    {
      "epoch": 18.25424449858839,
      "grad_norm": 12.381720542907715,
      "learning_rate": 3.4788129584509674e-05,
      "loss": 2.03,
      "step": 472000
    },
    {
      "epoch": 18.258111923270295,
      "grad_norm": 11.076095581054688,
      "learning_rate": 3.478490673060809e-05,
      "loss": 1.9522,
      "step": 472100
    },
    {
      "epoch": 18.2619793479522,
      "grad_norm": 13.286026954650879,
      "learning_rate": 3.47816838767065e-05,
      "loss": 1.9899,
      "step": 472200
    },
    {
      "epoch": 18.265846772634102,
      "grad_norm": 13.728078842163086,
      "learning_rate": 3.477846102280492e-05,
      "loss": 1.943,
      "step": 472300
    },
    {
      "epoch": 18.269714197316006,
      "grad_norm": 10.889445304870605,
      "learning_rate": 3.4775238168903326e-05,
      "loss": 2.0336,
      "step": 472400
    },
    {
      "epoch": 18.273581621997913,
      "grad_norm": 10.25525951385498,
      "learning_rate": 3.477201531500174e-05,
      "loss": 1.8824,
      "step": 472500
    },
    {
      "epoch": 18.277449046679816,
      "grad_norm": 5.995217800140381,
      "learning_rate": 3.4768792461100155e-05,
      "loss": 1.9719,
      "step": 472600
    },
    {
      "epoch": 18.28131647136172,
      "grad_norm": 12.35975456237793,
      "learning_rate": 3.476556960719857e-05,
      "loss": 1.9327,
      "step": 472700
    },
    {
      "epoch": 18.285183896043623,
      "grad_norm": 10.703022003173828,
      "learning_rate": 3.4762346753296985e-05,
      "loss": 1.9605,
      "step": 472800
    },
    {
      "epoch": 18.28905132072553,
      "grad_norm": 10.367782592773438,
      "learning_rate": 3.47591238993954e-05,
      "loss": 1.8945,
      "step": 472900
    },
    {
      "epoch": 18.292918745407434,
      "grad_norm": 12.07590103149414,
      "learning_rate": 3.475590104549381e-05,
      "loss": 1.8897,
      "step": 473000
    },
    {
      "epoch": 18.296786170089337,
      "grad_norm": 11.608258247375488,
      "learning_rate": 3.475267819159222e-05,
      "loss": 2.0062,
      "step": 473100
    },
    {
      "epoch": 18.30065359477124,
      "grad_norm": 11.61516284942627,
      "learning_rate": 3.474945533769064e-05,
      "loss": 1.9918,
      "step": 473200
    },
    {
      "epoch": 18.304521019453148,
      "grad_norm": 11.537202835083008,
      "learning_rate": 3.474623248378905e-05,
      "loss": 1.9291,
      "step": 473300
    },
    {
      "epoch": 18.30838844413505,
      "grad_norm": 11.52198314666748,
      "learning_rate": 3.474300962988746e-05,
      "loss": 2.051,
      "step": 473400
    },
    {
      "epoch": 18.312255868816955,
      "grad_norm": 16.11650848388672,
      "learning_rate": 3.4739786775985874e-05,
      "loss": 2.0054,
      "step": 473500
    },
    {
      "epoch": 18.31612329349886,
      "grad_norm": 14.158716201782227,
      "learning_rate": 3.473656392208429e-05,
      "loss": 1.9864,
      "step": 473600
    },
    {
      "epoch": 18.31999071818076,
      "grad_norm": 12.21015739440918,
      "learning_rate": 3.4733341068182704e-05,
      "loss": 1.893,
      "step": 473700
    },
    {
      "epoch": 18.32385814286267,
      "grad_norm": 14.525944709777832,
      "learning_rate": 3.473011821428111e-05,
      "loss": 1.9768,
      "step": 473800
    },
    {
      "epoch": 18.327725567544572,
      "grad_norm": 10.166709899902344,
      "learning_rate": 3.4726895360379526e-05,
      "loss": 1.9136,
      "step": 473900
    },
    {
      "epoch": 18.331592992226476,
      "grad_norm": 11.550464630126953,
      "learning_rate": 3.472367250647794e-05,
      "loss": 1.9659,
      "step": 474000
    },
    {
      "epoch": 18.33546041690838,
      "grad_norm": 11.07856273651123,
      "learning_rate": 3.4720449652576356e-05,
      "loss": 1.9636,
      "step": 474100
    },
    {
      "epoch": 18.339327841590286,
      "grad_norm": 11.671180725097656,
      "learning_rate": 3.4717226798674764e-05,
      "loss": 1.9643,
      "step": 474200
    },
    {
      "epoch": 18.34319526627219,
      "grad_norm": 14.609330177307129,
      "learning_rate": 3.471400394477318e-05,
      "loss": 1.9717,
      "step": 474300
    },
    {
      "epoch": 18.347062690954093,
      "grad_norm": 9.703862190246582,
      "learning_rate": 3.471078109087159e-05,
      "loss": 1.9996,
      "step": 474400
    },
    {
      "epoch": 18.350930115635997,
      "grad_norm": 9.419635772705078,
      "learning_rate": 3.470755823697001e-05,
      "loss": 2.0089,
      "step": 474500
    },
    {
      "epoch": 18.354797540317904,
      "grad_norm": 10.295266151428223,
      "learning_rate": 3.4704335383068416e-05,
      "loss": 1.9136,
      "step": 474600
    },
    {
      "epoch": 18.358664964999807,
      "grad_norm": 10.373291969299316,
      "learning_rate": 3.470111252916683e-05,
      "loss": 1.9354,
      "step": 474700
    },
    {
      "epoch": 18.36253238968171,
      "grad_norm": 9.104701042175293,
      "learning_rate": 3.4697889675265245e-05,
      "loss": 2.0075,
      "step": 474800
    },
    {
      "epoch": 18.366399814363614,
      "grad_norm": 11.454283714294434,
      "learning_rate": 3.469466682136366e-05,
      "loss": 1.9314,
      "step": 474900
    },
    {
      "epoch": 18.37026723904552,
      "grad_norm": 14.269119262695312,
      "learning_rate": 3.469144396746207e-05,
      "loss": 1.9217,
      "step": 475000
    },
    {
      "epoch": 18.374134663727425,
      "grad_norm": 14.585107803344727,
      "learning_rate": 3.468822111356048e-05,
      "loss": 1.9766,
      "step": 475100
    },
    {
      "epoch": 18.37800208840933,
      "grad_norm": 11.349115371704102,
      "learning_rate": 3.46849982596589e-05,
      "loss": 1.9998,
      "step": 475200
    },
    {
      "epoch": 18.38186951309123,
      "grad_norm": 11.474403381347656,
      "learning_rate": 3.4681775405757305e-05,
      "loss": 2.099,
      "step": 475300
    },
    {
      "epoch": 18.385736937773135,
      "grad_norm": 10.162192344665527,
      "learning_rate": 3.467855255185572e-05,
      "loss": 2.0008,
      "step": 475400
    },
    {
      "epoch": 18.389604362455042,
      "grad_norm": 10.448466300964355,
      "learning_rate": 3.4675329697954135e-05,
      "loss": 1.9744,
      "step": 475500
    },
    {
      "epoch": 18.393471787136946,
      "grad_norm": 10.175948143005371,
      "learning_rate": 3.467210684405255e-05,
      "loss": 1.9721,
      "step": 475600
    },
    {
      "epoch": 18.39733921181885,
      "grad_norm": 17.41872215270996,
      "learning_rate": 3.466888399015096e-05,
      "loss": 2.05,
      "step": 475700
    },
    {
      "epoch": 18.401206636500753,
      "grad_norm": 10.082281112670898,
      "learning_rate": 3.466566113624937e-05,
      "loss": 1.8647,
      "step": 475800
    },
    {
      "epoch": 18.40507406118266,
      "grad_norm": 12.678253173828125,
      "learning_rate": 3.466243828234779e-05,
      "loss": 2.0237,
      "step": 475900
    },
    {
      "epoch": 18.408941485864563,
      "grad_norm": 9.756209373474121,
      "learning_rate": 3.46592154284462e-05,
      "loss": 1.9763,
      "step": 476000
    },
    {
      "epoch": 18.412808910546467,
      "grad_norm": 12.142709732055664,
      "learning_rate": 3.465599257454461e-05,
      "loss": 1.9693,
      "step": 476100
    },
    {
      "epoch": 18.41667633522837,
      "grad_norm": 11.931982040405273,
      "learning_rate": 3.4652769720643024e-05,
      "loss": 2.0666,
      "step": 476200
    },
    {
      "epoch": 18.420543759910277,
      "grad_norm": 14.432286262512207,
      "learning_rate": 3.464954686674144e-05,
      "loss": 2.0022,
      "step": 476300
    },
    {
      "epoch": 18.42441118459218,
      "grad_norm": 10.043205261230469,
      "learning_rate": 3.4646324012839854e-05,
      "loss": 2.0348,
      "step": 476400
    },
    {
      "epoch": 18.428278609274084,
      "grad_norm": 9.85566520690918,
      "learning_rate": 3.464310115893826e-05,
      "loss": 1.9581,
      "step": 476500
    },
    {
      "epoch": 18.432146033955988,
      "grad_norm": 10.724076271057129,
      "learning_rate": 3.4639878305036676e-05,
      "loss": 1.9947,
      "step": 476600
    },
    {
      "epoch": 18.436013458637895,
      "grad_norm": 8.983367919921875,
      "learning_rate": 3.463665545113509e-05,
      "loss": 2.0035,
      "step": 476700
    },
    {
      "epoch": 18.4398808833198,
      "grad_norm": 12.645133018493652,
      "learning_rate": 3.4633432597233506e-05,
      "loss": 2.0541,
      "step": 476800
    },
    {
      "epoch": 18.4437483080017,
      "grad_norm": 11.240616798400879,
      "learning_rate": 3.4630209743331914e-05,
      "loss": 1.9493,
      "step": 476900
    },
    {
      "epoch": 18.447615732683605,
      "grad_norm": 14.451534271240234,
      "learning_rate": 3.462698688943033e-05,
      "loss": 1.8963,
      "step": 477000
    },
    {
      "epoch": 18.45148315736551,
      "grad_norm": 9.974910736083984,
      "learning_rate": 3.462376403552874e-05,
      "loss": 1.9699,
      "step": 477100
    },
    {
      "epoch": 18.455350582047416,
      "grad_norm": 9.230223655700684,
      "learning_rate": 3.462054118162716e-05,
      "loss": 2.0976,
      "step": 477200
    },
    {
      "epoch": 18.45921800672932,
      "grad_norm": 10.606651306152344,
      "learning_rate": 3.4617318327725566e-05,
      "loss": 2.0146,
      "step": 477300
    },
    {
      "epoch": 18.463085431411223,
      "grad_norm": 12.242905616760254,
      "learning_rate": 3.461409547382398e-05,
      "loss": 2.0692,
      "step": 477400
    },
    {
      "epoch": 18.466952856093126,
      "grad_norm": 12.589415550231934,
      "learning_rate": 3.4610872619922395e-05,
      "loss": 2.0558,
      "step": 477500
    },
    {
      "epoch": 18.470820280775033,
      "grad_norm": 13.461743354797363,
      "learning_rate": 3.460764976602081e-05,
      "loss": 2.0326,
      "step": 477600
    },
    {
      "epoch": 18.474687705456937,
      "grad_norm": 13.84924030303955,
      "learning_rate": 3.460442691211922e-05,
      "loss": 2.0271,
      "step": 477700
    },
    {
      "epoch": 18.47855513013884,
      "grad_norm": 11.12893295288086,
      "learning_rate": 3.460120405821763e-05,
      "loss": 1.9737,
      "step": 477800
    },
    {
      "epoch": 18.482422554820744,
      "grad_norm": 10.787576675415039,
      "learning_rate": 3.459798120431605e-05,
      "loss": 2.0378,
      "step": 477900
    },
    {
      "epoch": 18.48628997950265,
      "grad_norm": 9.46369743347168,
      "learning_rate": 3.4594758350414455e-05,
      "loss": 1.9345,
      "step": 478000
    },
    {
      "epoch": 18.490157404184554,
      "grad_norm": 11.283554077148438,
      "learning_rate": 3.459153549651287e-05,
      "loss": 2.002,
      "step": 478100
    },
    {
      "epoch": 18.494024828866458,
      "grad_norm": 11.92957592010498,
      "learning_rate": 3.4588312642611285e-05,
      "loss": 1.9669,
      "step": 478200
    },
    {
      "epoch": 18.49789225354836,
      "grad_norm": 9.636665344238281,
      "learning_rate": 3.45850897887097e-05,
      "loss": 1.9143,
      "step": 478300
    },
    {
      "epoch": 18.501759678230265,
      "grad_norm": 13.956375122070312,
      "learning_rate": 3.458186693480811e-05,
      "loss": 2.0332,
      "step": 478400
    },
    {
      "epoch": 18.50562710291217,
      "grad_norm": 12.562793731689453,
      "learning_rate": 3.457864408090652e-05,
      "loss": 1.9758,
      "step": 478500
    },
    {
      "epoch": 18.509494527594075,
      "grad_norm": 9.362845420837402,
      "learning_rate": 3.457542122700494e-05,
      "loss": 1.9604,
      "step": 478600
    },
    {
      "epoch": 18.51336195227598,
      "grad_norm": 9.187226295471191,
      "learning_rate": 3.457219837310335e-05,
      "loss": 2.039,
      "step": 478700
    },
    {
      "epoch": 18.517229376957882,
      "grad_norm": 13.020210266113281,
      "learning_rate": 3.4568975519201766e-05,
      "loss": 1.963,
      "step": 478800
    },
    {
      "epoch": 18.52109680163979,
      "grad_norm": 11.739309310913086,
      "learning_rate": 3.4565752665300174e-05,
      "loss": 1.9012,
      "step": 478900
    },
    {
      "epoch": 18.524964226321693,
      "grad_norm": 9.08607292175293,
      "learning_rate": 3.456252981139859e-05,
      "loss": 1.9867,
      "step": 479000
    },
    {
      "epoch": 18.528831651003596,
      "grad_norm": 14.087519645690918,
      "learning_rate": 3.4559306957497004e-05,
      "loss": 2.0335,
      "step": 479100
    },
    {
      "epoch": 18.5326990756855,
      "grad_norm": 9.907097816467285,
      "learning_rate": 3.455608410359542e-05,
      "loss": 2.0388,
      "step": 479200
    },
    {
      "epoch": 18.536566500367407,
      "grad_norm": 10.63031005859375,
      "learning_rate": 3.455286124969383e-05,
      "loss": 2.0558,
      "step": 479300
    },
    {
      "epoch": 18.54043392504931,
      "grad_norm": 11.710163116455078,
      "learning_rate": 3.454963839579224e-05,
      "loss": 1.9156,
      "step": 479400
    },
    {
      "epoch": 18.544301349731214,
      "grad_norm": 11.171304702758789,
      "learning_rate": 3.4546415541890656e-05,
      "loss": 1.961,
      "step": 479500
    },
    {
      "epoch": 18.548168774413117,
      "grad_norm": 9.720553398132324,
      "learning_rate": 3.454319268798907e-05,
      "loss": 1.9926,
      "step": 479600
    },
    {
      "epoch": 18.552036199095024,
      "grad_norm": 11.579178810119629,
      "learning_rate": 3.4539969834087485e-05,
      "loss": 1.852,
      "step": 479700
    },
    {
      "epoch": 18.555903623776928,
      "grad_norm": 11.174751281738281,
      "learning_rate": 3.45367469801859e-05,
      "loss": 1.9723,
      "step": 479800
    },
    {
      "epoch": 18.55977104845883,
      "grad_norm": 9.793381690979004,
      "learning_rate": 3.4533524126284315e-05,
      "loss": 2.0082,
      "step": 479900
    },
    {
      "epoch": 18.563638473140735,
      "grad_norm": 11.16714096069336,
      "learning_rate": 3.453030127238272e-05,
      "loss": 2.0113,
      "step": 480000
    },
    {
      "epoch": 18.56750589782264,
      "grad_norm": 14.27344799041748,
      "learning_rate": 3.452707841848114e-05,
      "loss": 2.0087,
      "step": 480100
    },
    {
      "epoch": 18.571373322504545,
      "grad_norm": 12.254181861877441,
      "learning_rate": 3.452385556457955e-05,
      "loss": 2.0253,
      "step": 480200
    },
    {
      "epoch": 18.57524074718645,
      "grad_norm": 23.83035659790039,
      "learning_rate": 3.4520632710677967e-05,
      "loss": 2.056,
      "step": 480300
    },
    {
      "epoch": 18.579108171868352,
      "grad_norm": 12.885727882385254,
      "learning_rate": 3.4517409856776375e-05,
      "loss": 1.9942,
      "step": 480400
    },
    {
      "epoch": 18.582975596550256,
      "grad_norm": 11.21249008178711,
      "learning_rate": 3.451418700287479e-05,
      "loss": 1.9715,
      "step": 480500
    },
    {
      "epoch": 18.586843021232163,
      "grad_norm": 10.896095275878906,
      "learning_rate": 3.4510964148973204e-05,
      "loss": 1.9604,
      "step": 480600
    },
    {
      "epoch": 18.590710445914066,
      "grad_norm": 12.489214897155762,
      "learning_rate": 3.450774129507162e-05,
      "loss": 1.9947,
      "step": 480700
    },
    {
      "epoch": 18.59457787059597,
      "grad_norm": 11.83907413482666,
      "learning_rate": 3.450451844117003e-05,
      "loss": 2.0855,
      "step": 480800
    },
    {
      "epoch": 18.598445295277873,
      "grad_norm": 10.097872734069824,
      "learning_rate": 3.450129558726844e-05,
      "loss": 1.9319,
      "step": 480900
    },
    {
      "epoch": 18.60231271995978,
      "grad_norm": 9.001166343688965,
      "learning_rate": 3.4498072733366856e-05,
      "loss": 1.9988,
      "step": 481000
    },
    {
      "epoch": 18.606180144641684,
      "grad_norm": 12.206365585327148,
      "learning_rate": 3.4494849879465264e-05,
      "loss": 1.9693,
      "step": 481100
    },
    {
      "epoch": 18.610047569323587,
      "grad_norm": 12.277396202087402,
      "learning_rate": 3.449162702556368e-05,
      "loss": 1.9711,
      "step": 481200
    },
    {
      "epoch": 18.61391499400549,
      "grad_norm": 13.089810371398926,
      "learning_rate": 3.4488404171662093e-05,
      "loss": 2.0359,
      "step": 481300
    },
    {
      "epoch": 18.617782418687398,
      "grad_norm": 11.84416389465332,
      "learning_rate": 3.448518131776051e-05,
      "loss": 1.9331,
      "step": 481400
    },
    {
      "epoch": 18.6216498433693,
      "grad_norm": 12.942397117614746,
      "learning_rate": 3.4481958463858916e-05,
      "loss": 2.0651,
      "step": 481500
    },
    {
      "epoch": 18.625517268051205,
      "grad_norm": 13.069607734680176,
      "learning_rate": 3.447873560995733e-05,
      "loss": 1.9753,
      "step": 481600
    },
    {
      "epoch": 18.629384692733108,
      "grad_norm": 10.920541763305664,
      "learning_rate": 3.4475512756055746e-05,
      "loss": 1.9323,
      "step": 481700
    },
    {
      "epoch": 18.63325211741501,
      "grad_norm": 17.06311798095703,
      "learning_rate": 3.447228990215416e-05,
      "loss": 1.9758,
      "step": 481800
    },
    {
      "epoch": 18.63711954209692,
      "grad_norm": 13.542741775512695,
      "learning_rate": 3.446906704825257e-05,
      "loss": 2.0262,
      "step": 481900
    },
    {
      "epoch": 18.640986966778822,
      "grad_norm": 14.915122032165527,
      "learning_rate": 3.446584419435098e-05,
      "loss": 1.9536,
      "step": 482000
    },
    {
      "epoch": 18.644854391460726,
      "grad_norm": 11.434470176696777,
      "learning_rate": 3.44626213404494e-05,
      "loss": 1.9691,
      "step": 482100
    },
    {
      "epoch": 18.64872181614263,
      "grad_norm": 7.792438983917236,
      "learning_rate": 3.445939848654781e-05,
      "loss": 2.0011,
      "step": 482200
    },
    {
      "epoch": 18.652589240824536,
      "grad_norm": 11.820703506469727,
      "learning_rate": 3.445617563264622e-05,
      "loss": 1.9711,
      "step": 482300
    },
    {
      "epoch": 18.65645666550644,
      "grad_norm": 12.824889183044434,
      "learning_rate": 3.4452952778744635e-05,
      "loss": 1.8946,
      "step": 482400
    },
    {
      "epoch": 18.660324090188343,
      "grad_norm": 11.390522003173828,
      "learning_rate": 3.444972992484305e-05,
      "loss": 1.9736,
      "step": 482500
    },
    {
      "epoch": 18.664191514870247,
      "grad_norm": 14.344756126403809,
      "learning_rate": 3.4446507070941464e-05,
      "loss": 1.99,
      "step": 482600
    },
    {
      "epoch": 18.668058939552154,
      "grad_norm": 13.482889175415039,
      "learning_rate": 3.444328421703987e-05,
      "loss": 2.0649,
      "step": 482700
    },
    {
      "epoch": 18.671926364234057,
      "grad_norm": 10.070351600646973,
      "learning_rate": 3.444006136313829e-05,
      "loss": 2.0181,
      "step": 482800
    },
    {
      "epoch": 18.67579378891596,
      "grad_norm": 10.757452011108398,
      "learning_rate": 3.44368385092367e-05,
      "loss": 1.9586,
      "step": 482900
    },
    {
      "epoch": 18.679661213597864,
      "grad_norm": 13.017685890197754,
      "learning_rate": 3.4433615655335117e-05,
      "loss": 2.0555,
      "step": 483000
    },
    {
      "epoch": 18.68352863827977,
      "grad_norm": 12.366771697998047,
      "learning_rate": 3.4430392801433525e-05,
      "loss": 1.8892,
      "step": 483100
    },
    {
      "epoch": 18.687396062961675,
      "grad_norm": 11.689778327941895,
      "learning_rate": 3.442716994753194e-05,
      "loss": 1.9622,
      "step": 483200
    },
    {
      "epoch": 18.691263487643578,
      "grad_norm": 10.806187629699707,
      "learning_rate": 3.4423947093630354e-05,
      "loss": 1.9561,
      "step": 483300
    },
    {
      "epoch": 18.69513091232548,
      "grad_norm": 11.36177921295166,
      "learning_rate": 3.442072423972877e-05,
      "loss": 2.006,
      "step": 483400
    },
    {
      "epoch": 18.698998337007385,
      "grad_norm": 9.177041053771973,
      "learning_rate": 3.4417501385827177e-05,
      "loss": 2.0216,
      "step": 483500
    },
    {
      "epoch": 18.702865761689292,
      "grad_norm": 12.869048118591309,
      "learning_rate": 3.441427853192559e-05,
      "loss": 1.996,
      "step": 483600
    },
    {
      "epoch": 18.706733186371196,
      "grad_norm": 12.255070686340332,
      "learning_rate": 3.4411055678024006e-05,
      "loss": 2.0109,
      "step": 483700
    },
    {
      "epoch": 18.7106006110531,
      "grad_norm": 9.754448890686035,
      "learning_rate": 3.440783282412242e-05,
      "loss": 2.0857,
      "step": 483800
    },
    {
      "epoch": 18.714468035735003,
      "grad_norm": 13.249873161315918,
      "learning_rate": 3.440460997022083e-05,
      "loss": 2.0716,
      "step": 483900
    },
    {
      "epoch": 18.71833546041691,
      "grad_norm": 14.354537963867188,
      "learning_rate": 3.4401387116319243e-05,
      "loss": 1.9733,
      "step": 484000
    },
    {
      "epoch": 18.722202885098813,
      "grad_norm": 10.353731155395508,
      "learning_rate": 3.439816426241766e-05,
      "loss": 2.0283,
      "step": 484100
    },
    {
      "epoch": 18.726070309780717,
      "grad_norm": 14.315642356872559,
      "learning_rate": 3.4394941408516066e-05,
      "loss": 1.9971,
      "step": 484200
    },
    {
      "epoch": 18.72993773446262,
      "grad_norm": 12.269792556762695,
      "learning_rate": 3.439171855461448e-05,
      "loss": 1.9734,
      "step": 484300
    },
    {
      "epoch": 18.733805159144527,
      "grad_norm": 14.232600212097168,
      "learning_rate": 3.4388495700712896e-05,
      "loss": 2.0164,
      "step": 484400
    },
    {
      "epoch": 18.73767258382643,
      "grad_norm": 10.058403968811035,
      "learning_rate": 3.438527284681131e-05,
      "loss": 2.0719,
      "step": 484500
    },
    {
      "epoch": 18.741540008508334,
      "grad_norm": 9.998698234558105,
      "learning_rate": 3.438204999290972e-05,
      "loss": 1.964,
      "step": 484600
    },
    {
      "epoch": 18.745407433190238,
      "grad_norm": 11.927495956420898,
      "learning_rate": 3.437882713900813e-05,
      "loss": 1.987,
      "step": 484700
    },
    {
      "epoch": 18.749274857872145,
      "grad_norm": 11.104751586914062,
      "learning_rate": 3.437560428510655e-05,
      "loss": 1.9364,
      "step": 484800
    },
    {
      "epoch": 18.753142282554048,
      "grad_norm": 10.271718978881836,
      "learning_rate": 3.437238143120496e-05,
      "loss": 2.079,
      "step": 484900
    },
    {
      "epoch": 18.75700970723595,
      "grad_norm": 12.014190673828125,
      "learning_rate": 3.436915857730337e-05,
      "loss": 2.084,
      "step": 485000
    },
    {
      "epoch": 18.760877131917855,
      "grad_norm": 11.597301483154297,
      "learning_rate": 3.4365935723401785e-05,
      "loss": 2.0144,
      "step": 485100
    },
    {
      "epoch": 18.76474455659976,
      "grad_norm": 13.083868980407715,
      "learning_rate": 3.43627128695002e-05,
      "loss": 2.0508,
      "step": 485200
    },
    {
      "epoch": 18.768611981281666,
      "grad_norm": 10.712777137756348,
      "learning_rate": 3.4359490015598614e-05,
      "loss": 2.0081,
      "step": 485300
    },
    {
      "epoch": 18.77247940596357,
      "grad_norm": 10.815272331237793,
      "learning_rate": 3.435626716169702e-05,
      "loss": 1.9317,
      "step": 485400
    },
    {
      "epoch": 18.776346830645473,
      "grad_norm": 12.968709945678711,
      "learning_rate": 3.435304430779544e-05,
      "loss": 1.8846,
      "step": 485500
    },
    {
      "epoch": 18.780214255327376,
      "grad_norm": 12.663334846496582,
      "learning_rate": 3.434982145389385e-05,
      "loss": 2.0827,
      "step": 485600
    },
    {
      "epoch": 18.784081680009283,
      "grad_norm": 15.180802345275879,
      "learning_rate": 3.4346598599992267e-05,
      "loss": 2.0794,
      "step": 485700
    },
    {
      "epoch": 18.787949104691187,
      "grad_norm": 9.652352333068848,
      "learning_rate": 3.434337574609068e-05,
      "loss": 1.9345,
      "step": 485800
    },
    {
      "epoch": 18.79181652937309,
      "grad_norm": 14.122106552124023,
      "learning_rate": 3.434015289218909e-05,
      "loss": 1.9502,
      "step": 485900
    },
    {
      "epoch": 18.795683954054994,
      "grad_norm": 8.808982849121094,
      "learning_rate": 3.4336930038287504e-05,
      "loss": 1.9953,
      "step": 486000
    },
    {
      "epoch": 18.7995513787369,
      "grad_norm": 10.185270309448242,
      "learning_rate": 3.433370718438592e-05,
      "loss": 1.9531,
      "step": 486100
    },
    {
      "epoch": 18.803418803418804,
      "grad_norm": 11.601131439208984,
      "learning_rate": 3.433048433048433e-05,
      "loss": 1.985,
      "step": 486200
    },
    {
      "epoch": 18.807286228100708,
      "grad_norm": 9.731863021850586,
      "learning_rate": 3.432726147658275e-05,
      "loss": 2.0181,
      "step": 486300
    },
    {
      "epoch": 18.81115365278261,
      "grad_norm": 15.853670120239258,
      "learning_rate": 3.432403862268116e-05,
      "loss": 1.9671,
      "step": 486400
    },
    {
      "epoch": 18.815021077464515,
      "grad_norm": 10.053421020507812,
      "learning_rate": 3.432081576877957e-05,
      "loss": 2.0399,
      "step": 486500
    },
    {
      "epoch": 18.81888850214642,
      "grad_norm": 12.32987117767334,
      "learning_rate": 3.4317592914877985e-05,
      "loss": 1.9814,
      "step": 486600
    },
    {
      "epoch": 18.822755926828325,
      "grad_norm": 8.236515998840332,
      "learning_rate": 3.43143700609764e-05,
      "loss": 1.9651,
      "step": 486700
    },
    {
      "epoch": 18.82662335151023,
      "grad_norm": 12.529817581176758,
      "learning_rate": 3.4311147207074815e-05,
      "loss": 2.0282,
      "step": 486800
    },
    {
      "epoch": 18.830490776192132,
      "grad_norm": 10.52694320678711,
      "learning_rate": 3.430792435317322e-05,
      "loss": 1.9505,
      "step": 486900
    },
    {
      "epoch": 18.83435820087404,
      "grad_norm": 9.704458236694336,
      "learning_rate": 3.430470149927164e-05,
      "loss": 1.928,
      "step": 487000
    },
    {
      "epoch": 18.838225625555943,
      "grad_norm": 12.035703659057617,
      "learning_rate": 3.430147864537005e-05,
      "loss": 1.9598,
      "step": 487100
    },
    {
      "epoch": 18.842093050237846,
      "grad_norm": 11.528084754943848,
      "learning_rate": 3.429825579146847e-05,
      "loss": 2.0175,
      "step": 487200
    },
    {
      "epoch": 18.84596047491975,
      "grad_norm": 13.537750244140625,
      "learning_rate": 3.4295032937566875e-05,
      "loss": 2.0823,
      "step": 487300
    },
    {
      "epoch": 18.849827899601657,
      "grad_norm": 10.973648071289062,
      "learning_rate": 3.429181008366529e-05,
      "loss": 1.8389,
      "step": 487400
    },
    {
      "epoch": 18.85369532428356,
      "grad_norm": 11.310356140136719,
      "learning_rate": 3.4288587229763704e-05,
      "loss": 2.0069,
      "step": 487500
    },
    {
      "epoch": 18.857562748965464,
      "grad_norm": 9.686505317687988,
      "learning_rate": 3.428536437586212e-05,
      "loss": 1.8768,
      "step": 487600
    },
    {
      "epoch": 18.861430173647367,
      "grad_norm": 9.099408149719238,
      "learning_rate": 3.428214152196053e-05,
      "loss": 1.9435,
      "step": 487700
    },
    {
      "epoch": 18.865297598329274,
      "grad_norm": 10.18045711517334,
      "learning_rate": 3.427891866805894e-05,
      "loss": 1.9737,
      "step": 487800
    },
    {
      "epoch": 18.869165023011178,
      "grad_norm": 12.955791473388672,
      "learning_rate": 3.4275695814157356e-05,
      "loss": 1.9893,
      "step": 487900
    },
    {
      "epoch": 18.87303244769308,
      "grad_norm": 14.909749031066895,
      "learning_rate": 3.427247296025577e-05,
      "loss": 2.0409,
      "step": 488000
    },
    {
      "epoch": 18.876899872374985,
      "grad_norm": 16.79662322998047,
      "learning_rate": 3.426925010635418e-05,
      "loss": 2.0357,
      "step": 488100
    },
    {
      "epoch": 18.88076729705689,
      "grad_norm": 12.542925834655762,
      "learning_rate": 3.4266027252452594e-05,
      "loss": 2.0083,
      "step": 488200
    },
    {
      "epoch": 18.884634721738795,
      "grad_norm": 9.677563667297363,
      "learning_rate": 3.426280439855101e-05,
      "loss": 2.0575,
      "step": 488300
    },
    {
      "epoch": 18.8885021464207,
      "grad_norm": 16.331859588623047,
      "learning_rate": 3.425958154464942e-05,
      "loss": 1.9455,
      "step": 488400
    },
    {
      "epoch": 18.892369571102602,
      "grad_norm": 10.56763744354248,
      "learning_rate": 3.425635869074783e-05,
      "loss": 2.0327,
      "step": 488500
    },
    {
      "epoch": 18.896236995784506,
      "grad_norm": 9.558009147644043,
      "learning_rate": 3.4253135836846246e-05,
      "loss": 1.9384,
      "step": 488600
    },
    {
      "epoch": 18.900104420466413,
      "grad_norm": 12.108187675476074,
      "learning_rate": 3.424991298294466e-05,
      "loss": 2.0538,
      "step": 488700
    },
    {
      "epoch": 18.903971845148316,
      "grad_norm": 11.452807426452637,
      "learning_rate": 3.4246690129043075e-05,
      "loss": 1.9752,
      "step": 488800
    },
    {
      "epoch": 18.90783926983022,
      "grad_norm": 13.522415161132812,
      "learning_rate": 3.424346727514148e-05,
      "loss": 1.8984,
      "step": 488900
    },
    {
      "epoch": 18.911706694512123,
      "grad_norm": 12.174009323120117,
      "learning_rate": 3.42402444212399e-05,
      "loss": 2.0672,
      "step": 489000
    },
    {
      "epoch": 18.91557411919403,
      "grad_norm": 11.20670223236084,
      "learning_rate": 3.423702156733831e-05,
      "loss": 2.0196,
      "step": 489100
    },
    {
      "epoch": 18.919441543875934,
      "grad_norm": 10.515111923217773,
      "learning_rate": 3.423379871343673e-05,
      "loss": 1.9948,
      "step": 489200
    },
    {
      "epoch": 18.923308968557837,
      "grad_norm": 14.255430221557617,
      "learning_rate": 3.4230575859535135e-05,
      "loss": 1.9496,
      "step": 489300
    },
    {
      "epoch": 18.92717639323974,
      "grad_norm": 11.361198425292969,
      "learning_rate": 3.422735300563355e-05,
      "loss": 1.9581,
      "step": 489400
    },
    {
      "epoch": 18.931043817921648,
      "grad_norm": 11.983783721923828,
      "learning_rate": 3.4224130151731965e-05,
      "loss": 1.9511,
      "step": 489500
    },
    {
      "epoch": 18.93491124260355,
      "grad_norm": 9.90964126586914,
      "learning_rate": 3.422090729783038e-05,
      "loss": 1.968,
      "step": 489600
    },
    {
      "epoch": 18.938778667285455,
      "grad_norm": 11.526183128356934,
      "learning_rate": 3.421768444392879e-05,
      "loss": 2.0786,
      "step": 489700
    },
    {
      "epoch": 18.942646091967358,
      "grad_norm": 11.891003608703613,
      "learning_rate": 3.42144615900272e-05,
      "loss": 1.8877,
      "step": 489800
    },
    {
      "epoch": 18.94651351664926,
      "grad_norm": 9.687567710876465,
      "learning_rate": 3.421123873612562e-05,
      "loss": 1.9351,
      "step": 489900
    },
    {
      "epoch": 18.95038094133117,
      "grad_norm": 14.076382637023926,
      "learning_rate": 3.4208015882224025e-05,
      "loss": 1.8619,
      "step": 490000
    },
    {
      "epoch": 18.954248366013072,
      "grad_norm": 11.591322898864746,
      "learning_rate": 3.420479302832244e-05,
      "loss": 1.9607,
      "step": 490100
    },
    {
      "epoch": 18.958115790694976,
      "grad_norm": 11.634132385253906,
      "learning_rate": 3.4201570174420854e-05,
      "loss": 1.8819,
      "step": 490200
    },
    {
      "epoch": 18.96198321537688,
      "grad_norm": 11.255311012268066,
      "learning_rate": 3.419834732051927e-05,
      "loss": 2.028,
      "step": 490300
    },
    {
      "epoch": 18.965850640058786,
      "grad_norm": 12.74197769165039,
      "learning_rate": 3.419512446661768e-05,
      "loss": 1.921,
      "step": 490400
    },
    {
      "epoch": 18.96971806474069,
      "grad_norm": 11.094630241394043,
      "learning_rate": 3.419190161271609e-05,
      "loss": 2.0239,
      "step": 490500
    },
    {
      "epoch": 18.973585489422593,
      "grad_norm": 9.154404640197754,
      "learning_rate": 3.4188678758814506e-05,
      "loss": 1.996,
      "step": 490600
    },
    {
      "epoch": 18.977452914104497,
      "grad_norm": 12.357399940490723,
      "learning_rate": 3.418545590491292e-05,
      "loss": 1.9849,
      "step": 490700
    },
    {
      "epoch": 18.981320338786404,
      "grad_norm": 11.02751350402832,
      "learning_rate": 3.418223305101133e-05,
      "loss": 1.9317,
      "step": 490800
    },
    {
      "epoch": 18.985187763468307,
      "grad_norm": 11.278364181518555,
      "learning_rate": 3.4179010197109744e-05,
      "loss": 2.066,
      "step": 490900
    },
    {
      "epoch": 18.98905518815021,
      "grad_norm": 13.083639144897461,
      "learning_rate": 3.417578734320816e-05,
      "loss": 1.9232,
      "step": 491000
    },
    {
      "epoch": 18.992922612832114,
      "grad_norm": 8.675997734069824,
      "learning_rate": 3.417256448930657e-05,
      "loss": 1.947,
      "step": 491100
    },
    {
      "epoch": 18.99679003751402,
      "grad_norm": 18.8326473236084,
      "learning_rate": 3.416934163540498e-05,
      "loss": 2.101,
      "step": 491200
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.8775660991668701,
      "eval_runtime": 3.0727,
      "eval_samples_per_second": 442.931,
      "eval_steps_per_second": 442.931,
      "step": 491283
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.7881503105163574,
      "eval_runtime": 55.898,
      "eval_samples_per_second": 462.575,
      "eval_steps_per_second": 462.575,
      "step": 491283
    },
    {
      "epoch": 19.000657462195925,
      "grad_norm": 11.046653747558594,
      "learning_rate": 3.4166118781503396e-05,
      "loss": 1.9782,
      "step": 491300
    },
    {
      "epoch": 19.004524886877828,
      "grad_norm": 12.291722297668457,
      "learning_rate": 3.416289592760181e-05,
      "loss": 2.0188,
      "step": 491400
    },
    {
      "epoch": 19.00839231155973,
      "grad_norm": 13.672356605529785,
      "learning_rate": 3.4159673073700225e-05,
      "loss": 1.9969,
      "step": 491500
    },
    {
      "epoch": 19.012259736241635,
      "grad_norm": 11.684822082519531,
      "learning_rate": 3.415645021979863e-05,
      "loss": 1.9521,
      "step": 491600
    },
    {
      "epoch": 19.016127160923542,
      "grad_norm": 8.548807144165039,
      "learning_rate": 3.415322736589705e-05,
      "loss": 1.9184,
      "step": 491700
    },
    {
      "epoch": 19.019994585605446,
      "grad_norm": 13.529303550720215,
      "learning_rate": 3.415000451199546e-05,
      "loss": 1.9475,
      "step": 491800
    },
    {
      "epoch": 19.02386201028735,
      "grad_norm": 9.679991722106934,
      "learning_rate": 3.414678165809388e-05,
      "loss": 1.9735,
      "step": 491900
    },
    {
      "epoch": 19.027729434969253,
      "grad_norm": 12.503157615661621,
      "learning_rate": 3.4143558804192285e-05,
      "loss": 1.931,
      "step": 492000
    },
    {
      "epoch": 19.03159685965116,
      "grad_norm": 11.466426849365234,
      "learning_rate": 3.41403359502907e-05,
      "loss": 1.859,
      "step": 492100
    },
    {
      "epoch": 19.035464284333063,
      "grad_norm": 9.69103717803955,
      "learning_rate": 3.4137113096389115e-05,
      "loss": 1.9423,
      "step": 492200
    },
    {
      "epoch": 19.039331709014967,
      "grad_norm": 8.303189277648926,
      "learning_rate": 3.413389024248753e-05,
      "loss": 1.8486,
      "step": 492300
    },
    {
      "epoch": 19.04319913369687,
      "grad_norm": 11.885594367980957,
      "learning_rate": 3.413066738858594e-05,
      "loss": 1.9805,
      "step": 492400
    },
    {
      "epoch": 19.047066558378777,
      "grad_norm": 9.487313270568848,
      "learning_rate": 3.412744453468435e-05,
      "loss": 2.0112,
      "step": 492500
    },
    {
      "epoch": 19.05093398306068,
      "grad_norm": 13.054046630859375,
      "learning_rate": 3.412422168078277e-05,
      "loss": 1.9322,
      "step": 492600
    },
    {
      "epoch": 19.054801407742584,
      "grad_norm": 8.900514602661133,
      "learning_rate": 3.412099882688118e-05,
      "loss": 1.9643,
      "step": 492700
    },
    {
      "epoch": 19.058668832424488,
      "grad_norm": 14.070645332336426,
      "learning_rate": 3.4117775972979596e-05,
      "loss": 1.9143,
      "step": 492800
    },
    {
      "epoch": 19.062536257106395,
      "grad_norm": 11.080362319946289,
      "learning_rate": 3.4114553119078004e-05,
      "loss": 1.9383,
      "step": 492900
    },
    {
      "epoch": 19.066403681788298,
      "grad_norm": 14.726821899414062,
      "learning_rate": 3.411133026517642e-05,
      "loss": 2.0038,
      "step": 493000
    },
    {
      "epoch": 19.0702711064702,
      "grad_norm": 13.717143058776855,
      "learning_rate": 3.4108107411274834e-05,
      "loss": 1.9548,
      "step": 493100
    },
    {
      "epoch": 19.074138531152105,
      "grad_norm": 14.720232009887695,
      "learning_rate": 3.410488455737325e-05,
      "loss": 1.9212,
      "step": 493200
    },
    {
      "epoch": 19.07800595583401,
      "grad_norm": 12.052083969116211,
      "learning_rate": 3.410166170347166e-05,
      "loss": 1.9893,
      "step": 493300
    },
    {
      "epoch": 19.081873380515916,
      "grad_norm": 10.747411727905273,
      "learning_rate": 3.409843884957008e-05,
      "loss": 1.9417,
      "step": 493400
    },
    {
      "epoch": 19.08574080519782,
      "grad_norm": 12.270628929138184,
      "learning_rate": 3.4095215995668486e-05,
      "loss": 2.0279,
      "step": 493500
    },
    {
      "epoch": 19.089608229879723,
      "grad_norm": 16.298620223999023,
      "learning_rate": 3.40919931417669e-05,
      "loss": 1.977,
      "step": 493600
    },
    {
      "epoch": 19.093475654561626,
      "grad_norm": 11.370526313781738,
      "learning_rate": 3.4088770287865315e-05,
      "loss": 1.9858,
      "step": 493700
    },
    {
      "epoch": 19.097343079243533,
      "grad_norm": 13.676518440246582,
      "learning_rate": 3.408554743396373e-05,
      "loss": 1.9377,
      "step": 493800
    },
    {
      "epoch": 19.101210503925437,
      "grad_norm": 14.54637336730957,
      "learning_rate": 3.408232458006214e-05,
      "loss": 1.9077,
      "step": 493900
    },
    {
      "epoch": 19.10507792860734,
      "grad_norm": 9.027169227600098,
      "learning_rate": 3.407910172616055e-05,
      "loss": 1.9588,
      "step": 494000
    },
    {
      "epoch": 19.108945353289243,
      "grad_norm": 11.092695236206055,
      "learning_rate": 3.407587887225897e-05,
      "loss": 1.9628,
      "step": 494100
    },
    {
      "epoch": 19.11281277797115,
      "grad_norm": 8.283221244812012,
      "learning_rate": 3.407265601835738e-05,
      "loss": 1.9172,
      "step": 494200
    },
    {
      "epoch": 19.116680202653054,
      "grad_norm": 11.301032066345215,
      "learning_rate": 3.406943316445579e-05,
      "loss": 1.9722,
      "step": 494300
    },
    {
      "epoch": 19.120547627334957,
      "grad_norm": 11.28442668914795,
      "learning_rate": 3.4066210310554205e-05,
      "loss": 1.9726,
      "step": 494400
    },
    {
      "epoch": 19.12441505201686,
      "grad_norm": 11.784921646118164,
      "learning_rate": 3.406298745665262e-05,
      "loss": 1.956,
      "step": 494500
    },
    {
      "epoch": 19.128282476698768,
      "grad_norm": 15.093119621276855,
      "learning_rate": 3.4059764602751034e-05,
      "loss": 1.945,
      "step": 494600
    },
    {
      "epoch": 19.13214990138067,
      "grad_norm": 13.979812622070312,
      "learning_rate": 3.405654174884944e-05,
      "loss": 2.0222,
      "step": 494700
    },
    {
      "epoch": 19.136017326062575,
      "grad_norm": 8.368183135986328,
      "learning_rate": 3.405331889494786e-05,
      "loss": 1.9862,
      "step": 494800
    },
    {
      "epoch": 19.13988475074448,
      "grad_norm": 11.342537879943848,
      "learning_rate": 3.405009604104627e-05,
      "loss": 1.8793,
      "step": 494900
    },
    {
      "epoch": 19.143752175426382,
      "grad_norm": 10.818530082702637,
      "learning_rate": 3.4046873187144686e-05,
      "loss": 1.9349,
      "step": 495000
    },
    {
      "epoch": 19.14761960010829,
      "grad_norm": 11.436062812805176,
      "learning_rate": 3.4043650333243094e-05,
      "loss": 1.8532,
      "step": 495100
    },
    {
      "epoch": 19.151487024790192,
      "grad_norm": 13.36811637878418,
      "learning_rate": 3.404042747934151e-05,
      "loss": 1.934,
      "step": 495200
    },
    {
      "epoch": 19.155354449472096,
      "grad_norm": 13.101664543151855,
      "learning_rate": 3.4037204625439924e-05,
      "loss": 1.8678,
      "step": 495300
    },
    {
      "epoch": 19.159221874154,
      "grad_norm": 11.45952320098877,
      "learning_rate": 3.403398177153834e-05,
      "loss": 2.0782,
      "step": 495400
    },
    {
      "epoch": 19.163089298835907,
      "grad_norm": 12.055987358093262,
      "learning_rate": 3.4030758917636746e-05,
      "loss": 1.9785,
      "step": 495500
    },
    {
      "epoch": 19.16695672351781,
      "grad_norm": 12.011731147766113,
      "learning_rate": 3.402753606373516e-05,
      "loss": 2.0279,
      "step": 495600
    },
    {
      "epoch": 19.170824148199713,
      "grad_norm": 10.755154609680176,
      "learning_rate": 3.4024313209833576e-05,
      "loss": 1.978,
      "step": 495700
    },
    {
      "epoch": 19.174691572881617,
      "grad_norm": 12.544767379760742,
      "learning_rate": 3.4021090355931984e-05,
      "loss": 2.046,
      "step": 495800
    },
    {
      "epoch": 19.178558997563524,
      "grad_norm": 16.108861923217773,
      "learning_rate": 3.40178675020304e-05,
      "loss": 1.9765,
      "step": 495900
    },
    {
      "epoch": 19.182426422245427,
      "grad_norm": 9.913106918334961,
      "learning_rate": 3.401464464812881e-05,
      "loss": 1.9252,
      "step": 496000
    },
    {
      "epoch": 19.18629384692733,
      "grad_norm": 11.325067520141602,
      "learning_rate": 3.401142179422723e-05,
      "loss": 1.9971,
      "step": 496100
    },
    {
      "epoch": 19.190161271609234,
      "grad_norm": 15.371752738952637,
      "learning_rate": 3.4008198940325636e-05,
      "loss": 1.925,
      "step": 496200
    },
    {
      "epoch": 19.19402869629114,
      "grad_norm": 10.236251831054688,
      "learning_rate": 3.400497608642405e-05,
      "loss": 1.9543,
      "step": 496300
    },
    {
      "epoch": 19.197896120973045,
      "grad_norm": 11.938568115234375,
      "learning_rate": 3.4001753232522465e-05,
      "loss": 1.956,
      "step": 496400
    },
    {
      "epoch": 19.20176354565495,
      "grad_norm": 10.462433815002441,
      "learning_rate": 3.399853037862088e-05,
      "loss": 1.9873,
      "step": 496500
    },
    {
      "epoch": 19.205630970336852,
      "grad_norm": 11.96642017364502,
      "learning_rate": 3.399530752471929e-05,
      "loss": 1.8273,
      "step": 496600
    },
    {
      "epoch": 19.209498395018755,
      "grad_norm": 11.871981620788574,
      "learning_rate": 3.39920846708177e-05,
      "loss": 1.9871,
      "step": 496700
    },
    {
      "epoch": 19.213365819700662,
      "grad_norm": 13.479557037353516,
      "learning_rate": 3.398886181691612e-05,
      "loss": 1.9573,
      "step": 496800
    },
    {
      "epoch": 19.217233244382566,
      "grad_norm": 11.970083236694336,
      "learning_rate": 3.398563896301453e-05,
      "loss": 1.9341,
      "step": 496900
    },
    {
      "epoch": 19.22110066906447,
      "grad_norm": 11.689054489135742,
      "learning_rate": 3.398241610911294e-05,
      "loss": 1.9323,
      "step": 497000
    },
    {
      "epoch": 19.224968093746373,
      "grad_norm": 12.805140495300293,
      "learning_rate": 3.3979193255211355e-05,
      "loss": 1.9539,
      "step": 497100
    },
    {
      "epoch": 19.22883551842828,
      "grad_norm": 13.169370651245117,
      "learning_rate": 3.397597040130977e-05,
      "loss": 2.037,
      "step": 497200
    },
    {
      "epoch": 19.232702943110183,
      "grad_norm": 12.698145866394043,
      "learning_rate": 3.3972747547408184e-05,
      "loss": 1.99,
      "step": 497300
    },
    {
      "epoch": 19.236570367792087,
      "grad_norm": 12.376789093017578,
      "learning_rate": 3.396952469350659e-05,
      "loss": 1.9179,
      "step": 497400
    },
    {
      "epoch": 19.24043779247399,
      "grad_norm": 12.404440879821777,
      "learning_rate": 3.396630183960501e-05,
      "loss": 2.0009,
      "step": 497500
    },
    {
      "epoch": 19.244305217155897,
      "grad_norm": 10.645363807678223,
      "learning_rate": 3.396307898570342e-05,
      "loss": 1.9784,
      "step": 497600
    },
    {
      "epoch": 19.2481726418378,
      "grad_norm": 10.644525527954102,
      "learning_rate": 3.3959856131801836e-05,
      "loss": 1.9395,
      "step": 497700
    },
    {
      "epoch": 19.252040066519704,
      "grad_norm": 11.224438667297363,
      "learning_rate": 3.3956633277900244e-05,
      "loss": 1.9407,
      "step": 497800
    },
    {
      "epoch": 19.255907491201608,
      "grad_norm": 9.557148933410645,
      "learning_rate": 3.395341042399866e-05,
      "loss": 1.9429,
      "step": 497900
    },
    {
      "epoch": 19.25977491588351,
      "grad_norm": 11.311568260192871,
      "learning_rate": 3.3950187570097074e-05,
      "loss": 2.0708,
      "step": 498000
    },
    {
      "epoch": 19.26364234056542,
      "grad_norm": 11.142010688781738,
      "learning_rate": 3.394696471619549e-05,
      "loss": 2.0359,
      "step": 498100
    },
    {
      "epoch": 19.267509765247322,
      "grad_norm": 13.71757984161377,
      "learning_rate": 3.3943741862293896e-05,
      "loss": 1.9924,
      "step": 498200
    },
    {
      "epoch": 19.271377189929225,
      "grad_norm": 10.075182914733887,
      "learning_rate": 3.394051900839231e-05,
      "loss": 1.9666,
      "step": 498300
    },
    {
      "epoch": 19.27524461461113,
      "grad_norm": 9.534238815307617,
      "learning_rate": 3.3937296154490726e-05,
      "loss": 2.0348,
      "step": 498400
    },
    {
      "epoch": 19.279112039293036,
      "grad_norm": 11.838964462280273,
      "learning_rate": 3.393407330058914e-05,
      "loss": 1.9138,
      "step": 498500
    },
    {
      "epoch": 19.28297946397494,
      "grad_norm": 13.71650218963623,
      "learning_rate": 3.393085044668755e-05,
      "loss": 1.9698,
      "step": 498600
    },
    {
      "epoch": 19.286846888656843,
      "grad_norm": 12.475232124328613,
      "learning_rate": 3.392762759278596e-05,
      "loss": 1.9527,
      "step": 498700
    },
    {
      "epoch": 19.290714313338746,
      "grad_norm": 9.511919975280762,
      "learning_rate": 3.392440473888438e-05,
      "loss": 1.9197,
      "step": 498800
    },
    {
      "epoch": 19.294581738020653,
      "grad_norm": 9.725459098815918,
      "learning_rate": 3.3921181884982786e-05,
      "loss": 1.9372,
      "step": 498900
    },
    {
      "epoch": 19.298449162702557,
      "grad_norm": 10.9439058303833,
      "learning_rate": 3.39179590310812e-05,
      "loss": 1.994,
      "step": 499000
    },
    {
      "epoch": 19.30231658738446,
      "grad_norm": 10.518506050109863,
      "learning_rate": 3.3914736177179615e-05,
      "loss": 1.8451,
      "step": 499100
    },
    {
      "epoch": 19.306184012066364,
      "grad_norm": 8.824226379394531,
      "learning_rate": 3.391151332327803e-05,
      "loss": 1.9383,
      "step": 499200
    },
    {
      "epoch": 19.31005143674827,
      "grad_norm": 11.719497680664062,
      "learning_rate": 3.3908290469376445e-05,
      "loss": 1.9711,
      "step": 499300
    },
    {
      "epoch": 19.313918861430174,
      "grad_norm": 14.001493453979492,
      "learning_rate": 3.390506761547485e-05,
      "loss": 1.923,
      "step": 499400
    },
    {
      "epoch": 19.317786286112078,
      "grad_norm": 12.206024169921875,
      "learning_rate": 3.390184476157327e-05,
      "loss": 1.9374,
      "step": 499500
    },
    {
      "epoch": 19.32165371079398,
      "grad_norm": 10.279257774353027,
      "learning_rate": 3.389862190767168e-05,
      "loss": 2.0597,
      "step": 499600
    },
    {
      "epoch": 19.325521135475885,
      "grad_norm": 10.11270523071289,
      "learning_rate": 3.38953990537701e-05,
      "loss": 1.9652,
      "step": 499700
    },
    {
      "epoch": 19.329388560157792,
      "grad_norm": 11.308313369750977,
      "learning_rate": 3.389217619986851e-05,
      "loss": 1.9405,
      "step": 499800
    },
    {
      "epoch": 19.333255984839695,
      "grad_norm": 12.652823448181152,
      "learning_rate": 3.3888953345966926e-05,
      "loss": 1.9932,
      "step": 499900
    },
    {
      "epoch": 19.3371234095216,
      "grad_norm": 9.70322322845459,
      "learning_rate": 3.3885730492065334e-05,
      "loss": 1.9508,
      "step": 500000
    },
    {
      "epoch": 19.340990834203502,
      "grad_norm": 12.5215425491333,
      "learning_rate": 3.388250763816375e-05,
      "loss": 2.0491,
      "step": 500100
    },
    {
      "epoch": 19.34485825888541,
      "grad_norm": 12.658772468566895,
      "learning_rate": 3.3879284784262164e-05,
      "loss": 1.9173,
      "step": 500200
    },
    {
      "epoch": 19.348725683567313,
      "grad_norm": 11.654170989990234,
      "learning_rate": 3.387606193036058e-05,
      "loss": 1.9974,
      "step": 500300
    },
    {
      "epoch": 19.352593108249216,
      "grad_norm": 10.427641868591309,
      "learning_rate": 3.387283907645899e-05,
      "loss": 1.8682,
      "step": 500400
    },
    {
      "epoch": 19.35646053293112,
      "grad_norm": 11.204001426696777,
      "learning_rate": 3.38696162225574e-05,
      "loss": 1.9746,
      "step": 500500
    },
    {
      "epoch": 19.360327957613027,
      "grad_norm": 10.878442764282227,
      "learning_rate": 3.3866393368655816e-05,
      "loss": 2.0037,
      "step": 500600
    },
    {
      "epoch": 19.36419538229493,
      "grad_norm": 12.316967010498047,
      "learning_rate": 3.386317051475423e-05,
      "loss": 1.9641,
      "step": 500700
    },
    {
      "epoch": 19.368062806976834,
      "grad_norm": 10.037138938903809,
      "learning_rate": 3.3859947660852645e-05,
      "loss": 1.9892,
      "step": 500800
    },
    {
      "epoch": 19.371930231658737,
      "grad_norm": 12.327425956726074,
      "learning_rate": 3.385672480695105e-05,
      "loss": 2.0473,
      "step": 500900
    },
    {
      "epoch": 19.375797656340644,
      "grad_norm": 10.621933937072754,
      "learning_rate": 3.385350195304947e-05,
      "loss": 1.9798,
      "step": 501000
    },
    {
      "epoch": 19.379665081022548,
      "grad_norm": 9.214387893676758,
      "learning_rate": 3.385027909914788e-05,
      "loss": 1.9142,
      "step": 501100
    },
    {
      "epoch": 19.38353250570445,
      "grad_norm": 11.773460388183594,
      "learning_rate": 3.38470562452463e-05,
      "loss": 2.0057,
      "step": 501200
    },
    {
      "epoch": 19.387399930386355,
      "grad_norm": 11.576510429382324,
      "learning_rate": 3.3843833391344705e-05,
      "loss": 2.0168,
      "step": 501300
    },
    {
      "epoch": 19.39126735506826,
      "grad_norm": 10.405716896057129,
      "learning_rate": 3.384061053744312e-05,
      "loss": 1.8841,
      "step": 501400
    },
    {
      "epoch": 19.395134779750165,
      "grad_norm": 9.9407377243042,
      "learning_rate": 3.3837387683541535e-05,
      "loss": 2.018,
      "step": 501500
    },
    {
      "epoch": 19.39900220443207,
      "grad_norm": 13.18073558807373,
      "learning_rate": 3.383416482963995e-05,
      "loss": 1.9714,
      "step": 501600
    },
    {
      "epoch": 19.402869629113972,
      "grad_norm": 10.960156440734863,
      "learning_rate": 3.383094197573836e-05,
      "loss": 2.0058,
      "step": 501700
    },
    {
      "epoch": 19.406737053795876,
      "grad_norm": 12.710644721984863,
      "learning_rate": 3.382771912183677e-05,
      "loss": 1.987,
      "step": 501800
    },
    {
      "epoch": 19.410604478477783,
      "grad_norm": 14.598291397094727,
      "learning_rate": 3.382449626793519e-05,
      "loss": 2.0135,
      "step": 501900
    },
    {
      "epoch": 19.414471903159686,
      "grad_norm": 10.359521865844727,
      "learning_rate": 3.3821273414033595e-05,
      "loss": 2.0111,
      "step": 502000
    },
    {
      "epoch": 19.41833932784159,
      "grad_norm": 9.217312812805176,
      "learning_rate": 3.381805056013201e-05,
      "loss": 1.926,
      "step": 502100
    },
    {
      "epoch": 19.422206752523493,
      "grad_norm": 11.959532737731934,
      "learning_rate": 3.3814827706230424e-05,
      "loss": 1.9714,
      "step": 502200
    },
    {
      "epoch": 19.4260741772054,
      "grad_norm": 11.388813972473145,
      "learning_rate": 3.381160485232884e-05,
      "loss": 1.8579,
      "step": 502300
    },
    {
      "epoch": 19.429941601887304,
      "grad_norm": 12.606474876403809,
      "learning_rate": 3.380838199842725e-05,
      "loss": 2.0533,
      "step": 502400
    },
    {
      "epoch": 19.433809026569207,
      "grad_norm": 9.85898494720459,
      "learning_rate": 3.380515914452566e-05,
      "loss": 1.9523,
      "step": 502500
    },
    {
      "epoch": 19.43767645125111,
      "grad_norm": 12.404595375061035,
      "learning_rate": 3.3801936290624076e-05,
      "loss": 2.0347,
      "step": 502600
    },
    {
      "epoch": 19.441543875933018,
      "grad_norm": 9.4351806640625,
      "learning_rate": 3.379871343672249e-05,
      "loss": 1.9669,
      "step": 502700
    },
    {
      "epoch": 19.44541130061492,
      "grad_norm": 11.828418731689453,
      "learning_rate": 3.37954905828209e-05,
      "loss": 2.0188,
      "step": 502800
    },
    {
      "epoch": 19.449278725296825,
      "grad_norm": 9.925143241882324,
      "learning_rate": 3.3792267728919313e-05,
      "loss": 1.9341,
      "step": 502900
    },
    {
      "epoch": 19.45314614997873,
      "grad_norm": 12.061086654663086,
      "learning_rate": 3.378904487501773e-05,
      "loss": 1.9796,
      "step": 503000
    },
    {
      "epoch": 19.457013574660632,
      "grad_norm": 11.224982261657715,
      "learning_rate": 3.378582202111614e-05,
      "loss": 1.9349,
      "step": 503100
    },
    {
      "epoch": 19.46088099934254,
      "grad_norm": 11.295512199401855,
      "learning_rate": 3.378259916721455e-05,
      "loss": 1.9918,
      "step": 503200
    },
    {
      "epoch": 19.464748424024442,
      "grad_norm": 8.80672836303711,
      "learning_rate": 3.3779376313312966e-05,
      "loss": 1.9795,
      "step": 503300
    },
    {
      "epoch": 19.468615848706346,
      "grad_norm": 12.885388374328613,
      "learning_rate": 3.377615345941138e-05,
      "loss": 1.9167,
      "step": 503400
    },
    {
      "epoch": 19.47248327338825,
      "grad_norm": 17.183460235595703,
      "learning_rate": 3.3772930605509795e-05,
      "loss": 1.974,
      "step": 503500
    },
    {
      "epoch": 19.476350698070156,
      "grad_norm": 11.367822647094727,
      "learning_rate": 3.37697077516082e-05,
      "loss": 1.941,
      "step": 503600
    },
    {
      "epoch": 19.48021812275206,
      "grad_norm": 10.027334213256836,
      "learning_rate": 3.376648489770662e-05,
      "loss": 1.9859,
      "step": 503700
    },
    {
      "epoch": 19.484085547433963,
      "grad_norm": 11.806679725646973,
      "learning_rate": 3.376326204380503e-05,
      "loss": 1.9826,
      "step": 503800
    },
    {
      "epoch": 19.487952972115867,
      "grad_norm": 13.067209243774414,
      "learning_rate": 3.376003918990345e-05,
      "loss": 2.016,
      "step": 503900
    },
    {
      "epoch": 19.491820396797774,
      "grad_norm": 10.580060958862305,
      "learning_rate": 3.3756816336001855e-05,
      "loss": 2.0819,
      "step": 504000
    },
    {
      "epoch": 19.495687821479677,
      "grad_norm": 11.423087120056152,
      "learning_rate": 3.375359348210027e-05,
      "loss": 2.0167,
      "step": 504100
    },
    {
      "epoch": 19.49955524616158,
      "grad_norm": 9.456839561462402,
      "learning_rate": 3.3750370628198685e-05,
      "loss": 1.9523,
      "step": 504200
    },
    {
      "epoch": 19.503422670843484,
      "grad_norm": 15.237102508544922,
      "learning_rate": 3.37471477742971e-05,
      "loss": 1.958,
      "step": 504300
    },
    {
      "epoch": 19.50729009552539,
      "grad_norm": 8.864357948303223,
      "learning_rate": 3.374392492039551e-05,
      "loss": 1.9811,
      "step": 504400
    },
    {
      "epoch": 19.511157520207295,
      "grad_norm": 12.303229331970215,
      "learning_rate": 3.374070206649392e-05,
      "loss": 1.971,
      "step": 504500
    },
    {
      "epoch": 19.5150249448892,
      "grad_norm": 14.055471420288086,
      "learning_rate": 3.3737479212592337e-05,
      "loss": 2.0827,
      "step": 504600
    },
    {
      "epoch": 19.518892369571102,
      "grad_norm": 10.768365859985352,
      "learning_rate": 3.3734256358690745e-05,
      "loss": 2.0186,
      "step": 504700
    },
    {
      "epoch": 19.522759794253005,
      "grad_norm": 6.250207901000977,
      "learning_rate": 3.373103350478916e-05,
      "loss": 1.9402,
      "step": 504800
    },
    {
      "epoch": 19.526627218934912,
      "grad_norm": 13.92802906036377,
      "learning_rate": 3.3727810650887574e-05,
      "loss": 2.0194,
      "step": 504900
    },
    {
      "epoch": 19.530494643616816,
      "grad_norm": 11.419076919555664,
      "learning_rate": 3.372458779698599e-05,
      "loss": 1.9512,
      "step": 505000
    },
    {
      "epoch": 19.53436206829872,
      "grad_norm": 11.811808586120605,
      "learning_rate": 3.37213649430844e-05,
      "loss": 2.0305,
      "step": 505100
    },
    {
      "epoch": 19.538229492980623,
      "grad_norm": 10.95931339263916,
      "learning_rate": 3.371814208918281e-05,
      "loss": 2.0085,
      "step": 505200
    },
    {
      "epoch": 19.54209691766253,
      "grad_norm": 10.354674339294434,
      "learning_rate": 3.3714919235281226e-05,
      "loss": 1.9567,
      "step": 505300
    },
    {
      "epoch": 19.545964342344433,
      "grad_norm": 13.094364166259766,
      "learning_rate": 3.371169638137964e-05,
      "loss": 2.0713,
      "step": 505400
    },
    {
      "epoch": 19.549831767026337,
      "grad_norm": 9.062600135803223,
      "learning_rate": 3.370847352747805e-05,
      "loss": 1.9933,
      "step": 505500
    },
    {
      "epoch": 19.55369919170824,
      "grad_norm": 12.211784362792969,
      "learning_rate": 3.3705250673576463e-05,
      "loss": 1.8664,
      "step": 505600
    },
    {
      "epoch": 19.557566616390147,
      "grad_norm": 12.391058921813965,
      "learning_rate": 3.370202781967488e-05,
      "loss": 1.9719,
      "step": 505700
    },
    {
      "epoch": 19.56143404107205,
      "grad_norm": 12.31975269317627,
      "learning_rate": 3.369880496577329e-05,
      "loss": 1.9659,
      "step": 505800
    },
    {
      "epoch": 19.565301465753954,
      "grad_norm": 11.720390319824219,
      "learning_rate": 3.36955821118717e-05,
      "loss": 1.9393,
      "step": 505900
    },
    {
      "epoch": 19.569168890435858,
      "grad_norm": 11.785445213317871,
      "learning_rate": 3.3692359257970116e-05,
      "loss": 1.9833,
      "step": 506000
    },
    {
      "epoch": 19.57303631511776,
      "grad_norm": 10.18260383605957,
      "learning_rate": 3.368913640406853e-05,
      "loss": 1.9525,
      "step": 506100
    },
    {
      "epoch": 19.57690373979967,
      "grad_norm": 10.196749687194824,
      "learning_rate": 3.3685913550166945e-05,
      "loss": 1.9456,
      "step": 506200
    },
    {
      "epoch": 19.580771164481572,
      "grad_norm": 13.158945083618164,
      "learning_rate": 3.368269069626536e-05,
      "loss": 2.0436,
      "step": 506300
    },
    {
      "epoch": 19.584638589163475,
      "grad_norm": 9.13744068145752,
      "learning_rate": 3.3679467842363774e-05,
      "loss": 1.9596,
      "step": 506400
    },
    {
      "epoch": 19.58850601384538,
      "grad_norm": 10.044459342956543,
      "learning_rate": 3.367624498846218e-05,
      "loss": 2.0869,
      "step": 506500
    },
    {
      "epoch": 19.592373438527286,
      "grad_norm": 13.285534858703613,
      "learning_rate": 3.36730221345606e-05,
      "loss": 1.889,
      "step": 506600
    },
    {
      "epoch": 19.59624086320919,
      "grad_norm": 8.778848648071289,
      "learning_rate": 3.366979928065901e-05,
      "loss": 2.0232,
      "step": 506700
    },
    {
      "epoch": 19.600108287891093,
      "grad_norm": 11.86656665802002,
      "learning_rate": 3.3666576426757427e-05,
      "loss": 2.0697,
      "step": 506800
    },
    {
      "epoch": 19.603975712572996,
      "grad_norm": 12.803997039794922,
      "learning_rate": 3.366335357285584e-05,
      "loss": 1.9726,
      "step": 506900
    },
    {
      "epoch": 19.607843137254903,
      "grad_norm": 16.48769760131836,
      "learning_rate": 3.366013071895425e-05,
      "loss": 1.9722,
      "step": 507000
    },
    {
      "epoch": 19.611710561936807,
      "grad_norm": 14.34911060333252,
      "learning_rate": 3.3656907865052664e-05,
      "loss": 1.933,
      "step": 507100
    },
    {
      "epoch": 19.61557798661871,
      "grad_norm": 12.758133888244629,
      "learning_rate": 3.365368501115108e-05,
      "loss": 2.0282,
      "step": 507200
    },
    {
      "epoch": 19.619445411300614,
      "grad_norm": 13.099552154541016,
      "learning_rate": 3.365046215724949e-05,
      "loss": 1.9935,
      "step": 507300
    },
    {
      "epoch": 19.62331283598252,
      "grad_norm": 12.800621032714844,
      "learning_rate": 3.364723930334791e-05,
      "loss": 1.9833,
      "step": 507400
    },
    {
      "epoch": 19.627180260664424,
      "grad_norm": 11.775726318359375,
      "learning_rate": 3.3644016449446316e-05,
      "loss": 2.0388,
      "step": 507500
    },
    {
      "epoch": 19.631047685346328,
      "grad_norm": 11.801019668579102,
      "learning_rate": 3.364079359554473e-05,
      "loss": 2.036,
      "step": 507600
    },
    {
      "epoch": 19.63491511002823,
      "grad_norm": 12.916586875915527,
      "learning_rate": 3.3637570741643145e-05,
      "loss": 1.9887,
      "step": 507700
    },
    {
      "epoch": 19.63878253471014,
      "grad_norm": 11.812026977539062,
      "learning_rate": 3.363434788774155e-05,
      "loss": 1.9835,
      "step": 507800
    },
    {
      "epoch": 19.642649959392042,
      "grad_norm": 10.749338150024414,
      "learning_rate": 3.363112503383997e-05,
      "loss": 1.9862,
      "step": 507900
    },
    {
      "epoch": 19.646517384073945,
      "grad_norm": 12.066909790039062,
      "learning_rate": 3.362790217993838e-05,
      "loss": 1.9826,
      "step": 508000
    },
    {
      "epoch": 19.65038480875585,
      "grad_norm": 14.157292366027832,
      "learning_rate": 3.36246793260368e-05,
      "loss": 1.8248,
      "step": 508100
    },
    {
      "epoch": 19.654252233437752,
      "grad_norm": 10.661128997802734,
      "learning_rate": 3.3621456472135205e-05,
      "loss": 2.0274,
      "step": 508200
    },
    {
      "epoch": 19.65811965811966,
      "grad_norm": 11.216202735900879,
      "learning_rate": 3.361823361823362e-05,
      "loss": 1.9735,
      "step": 508300
    },
    {
      "epoch": 19.661987082801563,
      "grad_norm": 12.74725341796875,
      "learning_rate": 3.3615010764332035e-05,
      "loss": 2.0215,
      "step": 508400
    },
    {
      "epoch": 19.665854507483466,
      "grad_norm": 12.813178062438965,
      "learning_rate": 3.361178791043045e-05,
      "loss": 1.9405,
      "step": 508500
    },
    {
      "epoch": 19.66972193216537,
      "grad_norm": 10.681057929992676,
      "learning_rate": 3.360856505652886e-05,
      "loss": 1.8973,
      "step": 508600
    },
    {
      "epoch": 19.673589356847277,
      "grad_norm": 12.837202072143555,
      "learning_rate": 3.360534220262727e-05,
      "loss": 1.9189,
      "step": 508700
    },
    {
      "epoch": 19.67745678152918,
      "grad_norm": 8.290190696716309,
      "learning_rate": 3.360211934872569e-05,
      "loss": 1.9854,
      "step": 508800
    },
    {
      "epoch": 19.681324206211084,
      "grad_norm": 14.537089347839355,
      "learning_rate": 3.35988964948241e-05,
      "loss": 2.051,
      "step": 508900
    },
    {
      "epoch": 19.685191630892987,
      "grad_norm": 11.669821739196777,
      "learning_rate": 3.359567364092251e-05,
      "loss": 2.0042,
      "step": 509000
    },
    {
      "epoch": 19.689059055574894,
      "grad_norm": 9.681246757507324,
      "learning_rate": 3.3592450787020924e-05,
      "loss": 2.0012,
      "step": 509100
    },
    {
      "epoch": 19.692926480256798,
      "grad_norm": 9.373510360717773,
      "learning_rate": 3.358922793311934e-05,
      "loss": 2.0567,
      "step": 509200
    },
    {
      "epoch": 19.6967939049387,
      "grad_norm": 10.1951322555542,
      "learning_rate": 3.3586005079217754e-05,
      "loss": 1.9418,
      "step": 509300
    },
    {
      "epoch": 19.700661329620605,
      "grad_norm": 9.079500198364258,
      "learning_rate": 3.358278222531616e-05,
      "loss": 1.926,
      "step": 509400
    },
    {
      "epoch": 19.70452875430251,
      "grad_norm": 12.166501998901367,
      "learning_rate": 3.3579559371414576e-05,
      "loss": 2.0938,
      "step": 509500
    },
    {
      "epoch": 19.708396178984415,
      "grad_norm": 10.00393009185791,
      "learning_rate": 3.357633651751299e-05,
      "loss": 1.9091,
      "step": 509600
    },
    {
      "epoch": 19.71226360366632,
      "grad_norm": 12.760603904724121,
      "learning_rate": 3.3573113663611406e-05,
      "loss": 1.9618,
      "step": 509700
    },
    {
      "epoch": 19.716131028348222,
      "grad_norm": 10.613312721252441,
      "learning_rate": 3.3569890809709814e-05,
      "loss": 2.087,
      "step": 509800
    },
    {
      "epoch": 19.719998453030126,
      "grad_norm": 11.466876983642578,
      "learning_rate": 3.356666795580823e-05,
      "loss": 2.0419,
      "step": 509900
    },
    {
      "epoch": 19.723865877712033,
      "grad_norm": 15.113279342651367,
      "learning_rate": 3.356344510190664e-05,
      "loss": 1.9453,
      "step": 510000
    },
    {
      "epoch": 19.727733302393936,
      "grad_norm": 7.829555511474609,
      "learning_rate": 3.356022224800506e-05,
      "loss": 1.9901,
      "step": 510100
    },
    {
      "epoch": 19.73160072707584,
      "grad_norm": 13.07443904876709,
      "learning_rate": 3.3556999394103466e-05,
      "loss": 1.9581,
      "step": 510200
    },
    {
      "epoch": 19.735468151757743,
      "grad_norm": 12.981600761413574,
      "learning_rate": 3.355377654020188e-05,
      "loss": 1.9191,
      "step": 510300
    },
    {
      "epoch": 19.73933557643965,
      "grad_norm": 10.932723045349121,
      "learning_rate": 3.3550553686300295e-05,
      "loss": 1.936,
      "step": 510400
    },
    {
      "epoch": 19.743203001121554,
      "grad_norm": 12.332649230957031,
      "learning_rate": 3.35473308323987e-05,
      "loss": 1.9257,
      "step": 510500
    },
    {
      "epoch": 19.747070425803457,
      "grad_norm": 10.86422061920166,
      "learning_rate": 3.354410797849712e-05,
      "loss": 1.9967,
      "step": 510600
    },
    {
      "epoch": 19.75093785048536,
      "grad_norm": 10.191256523132324,
      "learning_rate": 3.354088512459553e-05,
      "loss": 2.023,
      "step": 510700
    },
    {
      "epoch": 19.754805275167268,
      "grad_norm": 11.617074966430664,
      "learning_rate": 3.353766227069395e-05,
      "loss": 1.9384,
      "step": 510800
    },
    {
      "epoch": 19.75867269984917,
      "grad_norm": 11.65316390991211,
      "learning_rate": 3.3534439416792355e-05,
      "loss": 2.009,
      "step": 510900
    },
    {
      "epoch": 19.762540124531075,
      "grad_norm": 13.604416847229004,
      "learning_rate": 3.353121656289077e-05,
      "loss": 1.9388,
      "step": 511000
    },
    {
      "epoch": 19.76640754921298,
      "grad_norm": 11.933077812194824,
      "learning_rate": 3.3527993708989185e-05,
      "loss": 2.0271,
      "step": 511100
    },
    {
      "epoch": 19.77027497389488,
      "grad_norm": 12.54553508758545,
      "learning_rate": 3.35247708550876e-05,
      "loss": 1.9122,
      "step": 511200
    },
    {
      "epoch": 19.77414239857679,
      "grad_norm": 10.023550033569336,
      "learning_rate": 3.352154800118601e-05,
      "loss": 1.9951,
      "step": 511300
    },
    {
      "epoch": 19.778009823258692,
      "grad_norm": 11.585577011108398,
      "learning_rate": 3.351832514728442e-05,
      "loss": 1.9271,
      "step": 511400
    },
    {
      "epoch": 19.781877247940596,
      "grad_norm": 15.886822700500488,
      "learning_rate": 3.351510229338284e-05,
      "loss": 2.0496,
      "step": 511500
    },
    {
      "epoch": 19.7857446726225,
      "grad_norm": 14.554695129394531,
      "learning_rate": 3.351187943948125e-05,
      "loss": 2.0314,
      "step": 511600
    },
    {
      "epoch": 19.789612097304406,
      "grad_norm": 7.225917339324951,
      "learning_rate": 3.350865658557966e-05,
      "loss": 2.046,
      "step": 511700
    },
    {
      "epoch": 19.79347952198631,
      "grad_norm": 14.479214668273926,
      "learning_rate": 3.3505433731678074e-05,
      "loss": 1.9801,
      "step": 511800
    },
    {
      "epoch": 19.797346946668213,
      "grad_norm": 11.619462013244629,
      "learning_rate": 3.350221087777649e-05,
      "loss": 1.966,
      "step": 511900
    },
    {
      "epoch": 19.801214371350117,
      "grad_norm": 8.047072410583496,
      "learning_rate": 3.3498988023874904e-05,
      "loss": 2.0277,
      "step": 512000
    },
    {
      "epoch": 19.805081796032024,
      "grad_norm": 12.976629257202148,
      "learning_rate": 3.349576516997331e-05,
      "loss": 2.0155,
      "step": 512100
    },
    {
      "epoch": 19.808949220713927,
      "grad_norm": 15.276653289794922,
      "learning_rate": 3.3492542316071726e-05,
      "loss": 2.0635,
      "step": 512200
    },
    {
      "epoch": 19.81281664539583,
      "grad_norm": 12.643255233764648,
      "learning_rate": 3.348931946217014e-05,
      "loss": 2.001,
      "step": 512300
    },
    {
      "epoch": 19.816684070077734,
      "grad_norm": 9.621638298034668,
      "learning_rate": 3.3486096608268556e-05,
      "loss": 1.9678,
      "step": 512400
    },
    {
      "epoch": 19.82055149475964,
      "grad_norm": 12.622395515441895,
      "learning_rate": 3.3482873754366964e-05,
      "loss": 1.9628,
      "step": 512500
    },
    {
      "epoch": 19.824418919441545,
      "grad_norm": 12.129331588745117,
      "learning_rate": 3.347965090046538e-05,
      "loss": 2.1037,
      "step": 512600
    },
    {
      "epoch": 19.82828634412345,
      "grad_norm": 11.755195617675781,
      "learning_rate": 3.347642804656379e-05,
      "loss": 1.9701,
      "step": 512700
    },
    {
      "epoch": 19.83215376880535,
      "grad_norm": 10.221755027770996,
      "learning_rate": 3.347320519266221e-05,
      "loss": 2.0085,
      "step": 512800
    },
    {
      "epoch": 19.836021193487255,
      "grad_norm": 9.846290588378906,
      "learning_rate": 3.3469982338760616e-05,
      "loss": 1.9422,
      "step": 512900
    },
    {
      "epoch": 19.839888618169162,
      "grad_norm": 16.18621063232422,
      "learning_rate": 3.346675948485903e-05,
      "loss": 1.9235,
      "step": 513000
    },
    {
      "epoch": 19.843756042851066,
      "grad_norm": 10.498984336853027,
      "learning_rate": 3.3463536630957445e-05,
      "loss": 1.8778,
      "step": 513100
    },
    {
      "epoch": 19.84762346753297,
      "grad_norm": 10.169963836669922,
      "learning_rate": 3.346031377705586e-05,
      "loss": 1.9447,
      "step": 513200
    },
    {
      "epoch": 19.851490892214873,
      "grad_norm": 11.646794319152832,
      "learning_rate": 3.3457090923154275e-05,
      "loss": 1.963,
      "step": 513300
    },
    {
      "epoch": 19.85535831689678,
      "grad_norm": 9.481164932250977,
      "learning_rate": 3.345386806925269e-05,
      "loss": 1.9865,
      "step": 513400
    },
    {
      "epoch": 19.859225741578683,
      "grad_norm": 11.490262985229492,
      "learning_rate": 3.34506452153511e-05,
      "loss": 1.9711,
      "step": 513500
    },
    {
      "epoch": 19.863093166260587,
      "grad_norm": 11.771527290344238,
      "learning_rate": 3.344742236144951e-05,
      "loss": 1.9785,
      "step": 513600
    },
    {
      "epoch": 19.86696059094249,
      "grad_norm": 12.427536964416504,
      "learning_rate": 3.344419950754793e-05,
      "loss": 1.8944,
      "step": 513700
    },
    {
      "epoch": 19.870828015624397,
      "grad_norm": 12.71563720703125,
      "learning_rate": 3.344097665364634e-05,
      "loss": 2.0302,
      "step": 513800
    },
    {
      "epoch": 19.8746954403063,
      "grad_norm": 10.626363754272461,
      "learning_rate": 3.3437753799744756e-05,
      "loss": 1.9416,
      "step": 513900
    },
    {
      "epoch": 19.878562864988204,
      "grad_norm": 12.777551651000977,
      "learning_rate": 3.3434530945843164e-05,
      "loss": 1.9947,
      "step": 514000
    },
    {
      "epoch": 19.882430289670108,
      "grad_norm": 12.405834197998047,
      "learning_rate": 3.343130809194158e-05,
      "loss": 2.0132,
      "step": 514100
    },
    {
      "epoch": 19.88629771435201,
      "grad_norm": 9.412781715393066,
      "learning_rate": 3.3428085238039994e-05,
      "loss": 1.9945,
      "step": 514200
    },
    {
      "epoch": 19.89016513903392,
      "grad_norm": 12.313973426818848,
      "learning_rate": 3.342486238413841e-05,
      "loss": 2.0632,
      "step": 514300
    },
    {
      "epoch": 19.89403256371582,
      "grad_norm": 15.299168586730957,
      "learning_rate": 3.3421639530236816e-05,
      "loss": 1.9442,
      "step": 514400
    },
    {
      "epoch": 19.897899988397725,
      "grad_norm": 11.169463157653809,
      "learning_rate": 3.341841667633523e-05,
      "loss": 2.0346,
      "step": 514500
    },
    {
      "epoch": 19.90176741307963,
      "grad_norm": 11.158249855041504,
      "learning_rate": 3.3415193822433646e-05,
      "loss": 1.9791,
      "step": 514600
    },
    {
      "epoch": 19.905634837761536,
      "grad_norm": 10.383540153503418,
      "learning_rate": 3.341197096853206e-05,
      "loss": 2.0034,
      "step": 514700
    },
    {
      "epoch": 19.90950226244344,
      "grad_norm": 10.646724700927734,
      "learning_rate": 3.340874811463047e-05,
      "loss": 1.8455,
      "step": 514800
    },
    {
      "epoch": 19.913369687125343,
      "grad_norm": 15.948451042175293,
      "learning_rate": 3.340552526072888e-05,
      "loss": 1.9513,
      "step": 514900
    },
    {
      "epoch": 19.917237111807246,
      "grad_norm": 12.513677597045898,
      "learning_rate": 3.34023024068273e-05,
      "loss": 1.8873,
      "step": 515000
    },
    {
      "epoch": 19.921104536489153,
      "grad_norm": 9.60792064666748,
      "learning_rate": 3.339907955292571e-05,
      "loss": 1.9783,
      "step": 515100
    },
    {
      "epoch": 19.924971961171057,
      "grad_norm": 13.150660514831543,
      "learning_rate": 3.339585669902412e-05,
      "loss": 1.9197,
      "step": 515200
    },
    {
      "epoch": 19.92883938585296,
      "grad_norm": 10.304027557373047,
      "learning_rate": 3.3392633845122535e-05,
      "loss": 2.0224,
      "step": 515300
    },
    {
      "epoch": 19.932706810534864,
      "grad_norm": 12.014476776123047,
      "learning_rate": 3.338941099122095e-05,
      "loss": 2.0183,
      "step": 515400
    },
    {
      "epoch": 19.93657423521677,
      "grad_norm": 11.48340892791748,
      "learning_rate": 3.3386188137319365e-05,
      "loss": 2.0235,
      "step": 515500
    },
    {
      "epoch": 19.940441659898674,
      "grad_norm": 12.931998252868652,
      "learning_rate": 3.338296528341777e-05,
      "loss": 2.0373,
      "step": 515600
    },
    {
      "epoch": 19.944309084580578,
      "grad_norm": 11.355813980102539,
      "learning_rate": 3.337974242951619e-05,
      "loss": 1.9282,
      "step": 515700
    },
    {
      "epoch": 19.94817650926248,
      "grad_norm": 16.18111801147461,
      "learning_rate": 3.33765195756146e-05,
      "loss": 2.0298,
      "step": 515800
    },
    {
      "epoch": 19.95204393394439,
      "grad_norm": 11.75201416015625,
      "learning_rate": 3.337329672171302e-05,
      "loss": 2.0491,
      "step": 515900
    },
    {
      "epoch": 19.95591135862629,
      "grad_norm": 12.920804977416992,
      "learning_rate": 3.3370073867811425e-05,
      "loss": 2.0092,
      "step": 516000
    },
    {
      "epoch": 19.959778783308195,
      "grad_norm": 9.451440811157227,
      "learning_rate": 3.336685101390984e-05,
      "loss": 1.9836,
      "step": 516100
    },
    {
      "epoch": 19.9636462079901,
      "grad_norm": 11.41563892364502,
      "learning_rate": 3.3363628160008254e-05,
      "loss": 1.9972,
      "step": 516200
    },
    {
      "epoch": 19.967513632672002,
      "grad_norm": 10.599041938781738,
      "learning_rate": 3.336040530610667e-05,
      "loss": 1.9875,
      "step": 516300
    },
    {
      "epoch": 19.97138105735391,
      "grad_norm": 15.910717964172363,
      "learning_rate": 3.335718245220508e-05,
      "loss": 1.9559,
      "step": 516400
    },
    {
      "epoch": 19.975248482035813,
      "grad_norm": 11.485212326049805,
      "learning_rate": 3.335395959830349e-05,
      "loss": 2.026,
      "step": 516500
    },
    {
      "epoch": 19.979115906717716,
      "grad_norm": 11.70007038116455,
      "learning_rate": 3.3350736744401906e-05,
      "loss": 2.0555,
      "step": 516600
    },
    {
      "epoch": 19.98298333139962,
      "grad_norm": 10.165843963623047,
      "learning_rate": 3.3347513890500314e-05,
      "loss": 1.9507,
      "step": 516700
    },
    {
      "epoch": 19.986850756081527,
      "grad_norm": 12.263859748840332,
      "learning_rate": 3.334429103659873e-05,
      "loss": 1.9981,
      "step": 516800
    },
    {
      "epoch": 19.99071818076343,
      "grad_norm": 11.985892295837402,
      "learning_rate": 3.3341068182697144e-05,
      "loss": 2.001,
      "step": 516900
    },
    {
      "epoch": 19.994585605445334,
      "grad_norm": 14.448424339294434,
      "learning_rate": 3.333784532879556e-05,
      "loss": 1.9326,
      "step": 517000
    },
    {
      "epoch": 19.998453030127237,
      "grad_norm": 12.816788673400879,
      "learning_rate": 3.3334622474893966e-05,
      "loss": 1.8911,
      "step": 517100
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.8787026405334473,
      "eval_runtime": 2.9451,
      "eval_samples_per_second": 462.117,
      "eval_steps_per_second": 462.117,
      "step": 517140
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.7820130586624146,
      "eval_runtime": 55.566,
      "eval_samples_per_second": 465.338,
      "eval_steps_per_second": 465.338,
      "step": 517140
    },
    {
      "epoch": 20.002320454809144,
      "grad_norm": 9.390501976013184,
      "learning_rate": 3.333139962099238e-05,
      "loss": 1.8793,
      "step": 517200
    },
    {
      "epoch": 20.006187879491048,
      "grad_norm": 12.069576263427734,
      "learning_rate": 3.3328176767090796e-05,
      "loss": 1.9442,
      "step": 517300
    },
    {
      "epoch": 20.01005530417295,
      "grad_norm": 12.101827621459961,
      "learning_rate": 3.332495391318921e-05,
      "loss": 1.9981,
      "step": 517400
    },
    {
      "epoch": 20.013922728854855,
      "grad_norm": 10.721508026123047,
      "learning_rate": 3.332173105928762e-05,
      "loss": 1.8752,
      "step": 517500
    },
    {
      "epoch": 20.017790153536758,
      "grad_norm": 12.991690635681152,
      "learning_rate": 3.331850820538603e-05,
      "loss": 1.9615,
      "step": 517600
    },
    {
      "epoch": 20.021657578218665,
      "grad_norm": 15.473638534545898,
      "learning_rate": 3.331528535148445e-05,
      "loss": 2.0541,
      "step": 517700
    },
    {
      "epoch": 20.02552500290057,
      "grad_norm": 11.89953899383545,
      "learning_rate": 3.331206249758286e-05,
      "loss": 1.9304,
      "step": 517800
    },
    {
      "epoch": 20.029392427582472,
      "grad_norm": 14.517668724060059,
      "learning_rate": 3.330883964368127e-05,
      "loss": 2.0068,
      "step": 517900
    },
    {
      "epoch": 20.033259852264376,
      "grad_norm": 10.819709777832031,
      "learning_rate": 3.3305616789779685e-05,
      "loss": 1.9612,
      "step": 518000
    },
    {
      "epoch": 20.037127276946283,
      "grad_norm": 9.941569328308105,
      "learning_rate": 3.33023939358781e-05,
      "loss": 2.0051,
      "step": 518100
    },
    {
      "epoch": 20.040994701628186,
      "grad_norm": 11.354941368103027,
      "learning_rate": 3.3299171081976515e-05,
      "loss": 1.9768,
      "step": 518200
    },
    {
      "epoch": 20.04486212631009,
      "grad_norm": 11.250520706176758,
      "learning_rate": 3.329594822807492e-05,
      "loss": 1.9034,
      "step": 518300
    },
    {
      "epoch": 20.048729550991993,
      "grad_norm": 12.044576644897461,
      "learning_rate": 3.329272537417334e-05,
      "loss": 1.8695,
      "step": 518400
    },
    {
      "epoch": 20.0525969756739,
      "grad_norm": 12.66866683959961,
      "learning_rate": 3.328950252027175e-05,
      "loss": 1.9365,
      "step": 518500
    },
    {
      "epoch": 20.056464400355804,
      "grad_norm": 7.329329013824463,
      "learning_rate": 3.328627966637017e-05,
      "loss": 1.8593,
      "step": 518600
    },
    {
      "epoch": 20.060331825037707,
      "grad_norm": 11.254920959472656,
      "learning_rate": 3.3283056812468575e-05,
      "loss": 2.0029,
      "step": 518700
    },
    {
      "epoch": 20.06419924971961,
      "grad_norm": 14.628804206848145,
      "learning_rate": 3.327983395856699e-05,
      "loss": 1.9405,
      "step": 518800
    },
    {
      "epoch": 20.068066674401518,
      "grad_norm": 14.64464282989502,
      "learning_rate": 3.3276611104665404e-05,
      "loss": 2.0035,
      "step": 518900
    },
    {
      "epoch": 20.07193409908342,
      "grad_norm": 10.534525871276855,
      "learning_rate": 3.327338825076382e-05,
      "loss": 1.8671,
      "step": 519000
    },
    {
      "epoch": 20.075801523765325,
      "grad_norm": 12.405021667480469,
      "learning_rate": 3.327016539686223e-05,
      "loss": 1.9819,
      "step": 519100
    },
    {
      "epoch": 20.079668948447228,
      "grad_norm": 15.267146110534668,
      "learning_rate": 3.326694254296064e-05,
      "loss": 1.9026,
      "step": 519200
    },
    {
      "epoch": 20.08353637312913,
      "grad_norm": 13.13026237487793,
      "learning_rate": 3.3263719689059056e-05,
      "loss": 1.8885,
      "step": 519300
    },
    {
      "epoch": 20.08740379781104,
      "grad_norm": 10.305809020996094,
      "learning_rate": 3.3260496835157464e-05,
      "loss": 2.008,
      "step": 519400
    },
    {
      "epoch": 20.091271222492942,
      "grad_norm": 8.445413589477539,
      "learning_rate": 3.325727398125588e-05,
      "loss": 1.8872,
      "step": 519500
    },
    {
      "epoch": 20.095138647174846,
      "grad_norm": 11.879201889038086,
      "learning_rate": 3.3254051127354294e-05,
      "loss": 1.9252,
      "step": 519600
    },
    {
      "epoch": 20.09900607185675,
      "grad_norm": 9.013364791870117,
      "learning_rate": 3.325082827345271e-05,
      "loss": 2.0174,
      "step": 519700
    },
    {
      "epoch": 20.102873496538656,
      "grad_norm": 11.356583595275879,
      "learning_rate": 3.324760541955112e-05,
      "loss": 1.9596,
      "step": 519800
    },
    {
      "epoch": 20.10674092122056,
      "grad_norm": 11.189157485961914,
      "learning_rate": 3.324438256564954e-05,
      "loss": 2.0077,
      "step": 519900
    },
    {
      "epoch": 20.110608345902463,
      "grad_norm": 11.673975944519043,
      "learning_rate": 3.3241159711747946e-05,
      "loss": 1.9791,
      "step": 520000
    },
    {
      "epoch": 20.114475770584367,
      "grad_norm": 11.607819557189941,
      "learning_rate": 3.323793685784636e-05,
      "loss": 1.9286,
      "step": 520100
    },
    {
      "epoch": 20.118343195266274,
      "grad_norm": 11.498991966247559,
      "learning_rate": 3.3234714003944775e-05,
      "loss": 1.9486,
      "step": 520200
    },
    {
      "epoch": 20.122210619948177,
      "grad_norm": 11.82861042022705,
      "learning_rate": 3.323149115004319e-05,
      "loss": 1.9911,
      "step": 520300
    },
    {
      "epoch": 20.12607804463008,
      "grad_norm": 15.75223159790039,
      "learning_rate": 3.3228268296141605e-05,
      "loss": 1.9754,
      "step": 520400
    },
    {
      "epoch": 20.129945469311984,
      "grad_norm": 12.938913345336914,
      "learning_rate": 3.322504544224001e-05,
      "loss": 1.9286,
      "step": 520500
    },
    {
      "epoch": 20.13381289399389,
      "grad_norm": 10.834878921508789,
      "learning_rate": 3.322182258833843e-05,
      "loss": 1.946,
      "step": 520600
    },
    {
      "epoch": 20.137680318675795,
      "grad_norm": 11.970227241516113,
      "learning_rate": 3.321859973443684e-05,
      "loss": 2.0446,
      "step": 520700
    },
    {
      "epoch": 20.141547743357698,
      "grad_norm": 12.266109466552734,
      "learning_rate": 3.321537688053526e-05,
      "loss": 1.9271,
      "step": 520800
    },
    {
      "epoch": 20.1454151680396,
      "grad_norm": 11.92402458190918,
      "learning_rate": 3.321215402663367e-05,
      "loss": 1.8595,
      "step": 520900
    },
    {
      "epoch": 20.149282592721505,
      "grad_norm": 11.616341590881348,
      "learning_rate": 3.320893117273208e-05,
      "loss": 1.9404,
      "step": 521000
    },
    {
      "epoch": 20.153150017403412,
      "grad_norm": 11.650385856628418,
      "learning_rate": 3.3205708318830494e-05,
      "loss": 1.9416,
      "step": 521100
    },
    {
      "epoch": 20.157017442085316,
      "grad_norm": 9.448734283447266,
      "learning_rate": 3.320248546492891e-05,
      "loss": 1.9787,
      "step": 521200
    },
    {
      "epoch": 20.16088486676722,
      "grad_norm": 11.834821701049805,
      "learning_rate": 3.3199262611027324e-05,
      "loss": 2.0124,
      "step": 521300
    },
    {
      "epoch": 20.164752291449123,
      "grad_norm": 11.377554893493652,
      "learning_rate": 3.319603975712573e-05,
      "loss": 2.0029,
      "step": 521400
    },
    {
      "epoch": 20.16861971613103,
      "grad_norm": 12.429413795471191,
      "learning_rate": 3.3192816903224146e-05,
      "loss": 1.911,
      "step": 521500
    },
    {
      "epoch": 20.172487140812933,
      "grad_norm": 13.45743179321289,
      "learning_rate": 3.318959404932256e-05,
      "loss": 1.9575,
      "step": 521600
    },
    {
      "epoch": 20.176354565494837,
      "grad_norm": 12.960821151733398,
      "learning_rate": 3.3186371195420976e-05,
      "loss": 1.9958,
      "step": 521700
    },
    {
      "epoch": 20.18022199017674,
      "grad_norm": 13.642590522766113,
      "learning_rate": 3.3183148341519384e-05,
      "loss": 1.9758,
      "step": 521800
    },
    {
      "epoch": 20.184089414858647,
      "grad_norm": 10.041693687438965,
      "learning_rate": 3.31799254876178e-05,
      "loss": 1.9417,
      "step": 521900
    },
    {
      "epoch": 20.18795683954055,
      "grad_norm": 11.888396263122559,
      "learning_rate": 3.317670263371621e-05,
      "loss": 1.9629,
      "step": 522000
    },
    {
      "epoch": 20.191824264222454,
      "grad_norm": 11.133282661437988,
      "learning_rate": 3.317347977981463e-05,
      "loss": 1.9998,
      "step": 522100
    },
    {
      "epoch": 20.195691688904358,
      "grad_norm": 16.650949478149414,
      "learning_rate": 3.3170256925913036e-05,
      "loss": 1.9427,
      "step": 522200
    },
    {
      "epoch": 20.199559113586265,
      "grad_norm": 15.05286979675293,
      "learning_rate": 3.316703407201145e-05,
      "loss": 2.0313,
      "step": 522300
    },
    {
      "epoch": 20.203426538268168,
      "grad_norm": 10.992714881896973,
      "learning_rate": 3.3163811218109865e-05,
      "loss": 2.0071,
      "step": 522400
    },
    {
      "epoch": 20.20729396295007,
      "grad_norm": 11.8672513961792,
      "learning_rate": 3.316058836420827e-05,
      "loss": 1.9322,
      "step": 522500
    },
    {
      "epoch": 20.211161387631975,
      "grad_norm": 12.005105972290039,
      "learning_rate": 3.315736551030669e-05,
      "loss": 1.9443,
      "step": 522600
    },
    {
      "epoch": 20.21502881231388,
      "grad_norm": 12.782511711120605,
      "learning_rate": 3.31541426564051e-05,
      "loss": 1.8625,
      "step": 522700
    },
    {
      "epoch": 20.218896236995786,
      "grad_norm": 10.793473243713379,
      "learning_rate": 3.315091980250352e-05,
      "loss": 1.9468,
      "step": 522800
    },
    {
      "epoch": 20.22276366167769,
      "grad_norm": 9.74477481842041,
      "learning_rate": 3.3147696948601925e-05,
      "loss": 1.9178,
      "step": 522900
    },
    {
      "epoch": 20.226631086359593,
      "grad_norm": 9.05294418334961,
      "learning_rate": 3.314447409470034e-05,
      "loss": 1.9642,
      "step": 523000
    },
    {
      "epoch": 20.230498511041496,
      "grad_norm": 18.54410743713379,
      "learning_rate": 3.3141251240798755e-05,
      "loss": 2.0121,
      "step": 523100
    },
    {
      "epoch": 20.234365935723403,
      "grad_norm": 10.636704444885254,
      "learning_rate": 3.313802838689717e-05,
      "loss": 1.8615,
      "step": 523200
    },
    {
      "epoch": 20.238233360405307,
      "grad_norm": 11.826639175415039,
      "learning_rate": 3.313480553299558e-05,
      "loss": 1.8782,
      "step": 523300
    },
    {
      "epoch": 20.24210078508721,
      "grad_norm": 19.778400421142578,
      "learning_rate": 3.313158267909399e-05,
      "loss": 1.8988,
      "step": 523400
    },
    {
      "epoch": 20.245968209769114,
      "grad_norm": 10.504754066467285,
      "learning_rate": 3.312835982519241e-05,
      "loss": 1.9932,
      "step": 523500
    },
    {
      "epoch": 20.24983563445102,
      "grad_norm": 12.890872955322266,
      "learning_rate": 3.312513697129082e-05,
      "loss": 1.9762,
      "step": 523600
    },
    {
      "epoch": 20.253703059132924,
      "grad_norm": 11.093001365661621,
      "learning_rate": 3.312191411738923e-05,
      "loss": 1.9755,
      "step": 523700
    },
    {
      "epoch": 20.257570483814828,
      "grad_norm": 12.087420463562012,
      "learning_rate": 3.3118691263487644e-05,
      "loss": 1.9358,
      "step": 523800
    },
    {
      "epoch": 20.26143790849673,
      "grad_norm": 9.638845443725586,
      "learning_rate": 3.311546840958606e-05,
      "loss": 1.9911,
      "step": 523900
    },
    {
      "epoch": 20.265305333178638,
      "grad_norm": 11.458436965942383,
      "learning_rate": 3.3112245555684473e-05,
      "loss": 1.9919,
      "step": 524000
    },
    {
      "epoch": 20.26917275786054,
      "grad_norm": 12.039852142333984,
      "learning_rate": 3.310902270178288e-05,
      "loss": 1.9314,
      "step": 524100
    },
    {
      "epoch": 20.273040182542445,
      "grad_norm": 11.992161750793457,
      "learning_rate": 3.3105799847881296e-05,
      "loss": 1.9722,
      "step": 524200
    },
    {
      "epoch": 20.27690760722435,
      "grad_norm": 13.780848503112793,
      "learning_rate": 3.310257699397971e-05,
      "loss": 1.9416,
      "step": 524300
    },
    {
      "epoch": 20.280775031906252,
      "grad_norm": 11.552785873413086,
      "learning_rate": 3.3099354140078126e-05,
      "loss": 2.0405,
      "step": 524400
    },
    {
      "epoch": 20.28464245658816,
      "grad_norm": 10.680215835571289,
      "learning_rate": 3.3096131286176534e-05,
      "loss": 1.93,
      "step": 524500
    },
    {
      "epoch": 20.288509881270063,
      "grad_norm": 14.46126651763916,
      "learning_rate": 3.309290843227495e-05,
      "loss": 1.9181,
      "step": 524600
    },
    {
      "epoch": 20.292377305951966,
      "grad_norm": 13.45578670501709,
      "learning_rate": 3.308968557837336e-05,
      "loss": 1.9878,
      "step": 524700
    },
    {
      "epoch": 20.29624473063387,
      "grad_norm": 14.941014289855957,
      "learning_rate": 3.308646272447178e-05,
      "loss": 1.9158,
      "step": 524800
    },
    {
      "epoch": 20.300112155315777,
      "grad_norm": 13.172148704528809,
      "learning_rate": 3.3083239870570186e-05,
      "loss": 1.9479,
      "step": 524900
    },
    {
      "epoch": 20.30397957999768,
      "grad_norm": 11.50112247467041,
      "learning_rate": 3.30800170166686e-05,
      "loss": 1.9354,
      "step": 525000
    },
    {
      "epoch": 20.307847004679584,
      "grad_norm": 8.84270191192627,
      "learning_rate": 3.3076794162767015e-05,
      "loss": 2.0484,
      "step": 525100
    },
    {
      "epoch": 20.311714429361487,
      "grad_norm": 11.418846130371094,
      "learning_rate": 3.307357130886543e-05,
      "loss": 2.0162,
      "step": 525200
    },
    {
      "epoch": 20.315581854043394,
      "grad_norm": 14.974199295043945,
      "learning_rate": 3.307034845496384e-05,
      "loss": 1.9963,
      "step": 525300
    },
    {
      "epoch": 20.319449278725298,
      "grad_norm": 9.702277183532715,
      "learning_rate": 3.306712560106225e-05,
      "loss": 2.0362,
      "step": 525400
    },
    {
      "epoch": 20.3233167034072,
      "grad_norm": 11.57895278930664,
      "learning_rate": 3.306390274716067e-05,
      "loss": 1.9542,
      "step": 525500
    },
    {
      "epoch": 20.327184128089105,
      "grad_norm": 12.850075721740723,
      "learning_rate": 3.3060679893259075e-05,
      "loss": 2.0003,
      "step": 525600
    },
    {
      "epoch": 20.331051552771008,
      "grad_norm": 9.890901565551758,
      "learning_rate": 3.305745703935749e-05,
      "loss": 1.9795,
      "step": 525700
    },
    {
      "epoch": 20.334918977452915,
      "grad_norm": 15.43196964263916,
      "learning_rate": 3.3054234185455905e-05,
      "loss": 1.9688,
      "step": 525800
    },
    {
      "epoch": 20.33878640213482,
      "grad_norm": 10.764851570129395,
      "learning_rate": 3.305101133155432e-05,
      "loss": 2.0835,
      "step": 525900
    },
    {
      "epoch": 20.342653826816722,
      "grad_norm": 11.68798828125,
      "learning_rate": 3.304778847765273e-05,
      "loss": 1.9794,
      "step": 526000
    },
    {
      "epoch": 20.346521251498626,
      "grad_norm": 11.518617630004883,
      "learning_rate": 3.304456562375114e-05,
      "loss": 2.0207,
      "step": 526100
    },
    {
      "epoch": 20.350388676180533,
      "grad_norm": 10.878718376159668,
      "learning_rate": 3.304134276984956e-05,
      "loss": 1.9381,
      "step": 526200
    },
    {
      "epoch": 20.354256100862436,
      "grad_norm": 9.044668197631836,
      "learning_rate": 3.303811991594797e-05,
      "loss": 1.7962,
      "step": 526300
    },
    {
      "epoch": 20.35812352554434,
      "grad_norm": 9.03222942352295,
      "learning_rate": 3.3034897062046386e-05,
      "loss": 1.9413,
      "step": 526400
    },
    {
      "epoch": 20.361990950226243,
      "grad_norm": 12.333403587341309,
      "learning_rate": 3.3031674208144794e-05,
      "loss": 1.9853,
      "step": 526500
    },
    {
      "epoch": 20.36585837490815,
      "grad_norm": 11.412066459655762,
      "learning_rate": 3.302845135424321e-05,
      "loss": 1.9719,
      "step": 526600
    },
    {
      "epoch": 20.369725799590054,
      "grad_norm": 11.600567817687988,
      "learning_rate": 3.3025228500341623e-05,
      "loss": 2.0372,
      "step": 526700
    },
    {
      "epoch": 20.373593224271957,
      "grad_norm": 10.205421447753906,
      "learning_rate": 3.302200564644004e-05,
      "loss": 2.0236,
      "step": 526800
    },
    {
      "epoch": 20.37746064895386,
      "grad_norm": 11.889165878295898,
      "learning_rate": 3.301878279253845e-05,
      "loss": 1.9557,
      "step": 526900
    },
    {
      "epoch": 20.381328073635768,
      "grad_norm": 12.360740661621094,
      "learning_rate": 3.301555993863686e-05,
      "loss": 1.8836,
      "step": 527000
    },
    {
      "epoch": 20.38519549831767,
      "grad_norm": 11.390288352966309,
      "learning_rate": 3.3012337084735276e-05,
      "loss": 1.9273,
      "step": 527100
    },
    {
      "epoch": 20.389062922999575,
      "grad_norm": 13.949321746826172,
      "learning_rate": 3.300911423083369e-05,
      "loss": 1.9064,
      "step": 527200
    },
    {
      "epoch": 20.392930347681478,
      "grad_norm": 11.110386848449707,
      "learning_rate": 3.3005891376932105e-05,
      "loss": 2.0463,
      "step": 527300
    },
    {
      "epoch": 20.39679777236338,
      "grad_norm": 13.093164443969727,
      "learning_rate": 3.300266852303052e-05,
      "loss": 1.8951,
      "step": 527400
    },
    {
      "epoch": 20.40066519704529,
      "grad_norm": 11.980854988098145,
      "learning_rate": 3.2999445669128934e-05,
      "loss": 1.9875,
      "step": 527500
    },
    {
      "epoch": 20.404532621727192,
      "grad_norm": 14.63743782043457,
      "learning_rate": 3.299622281522734e-05,
      "loss": 1.9811,
      "step": 527600
    },
    {
      "epoch": 20.408400046409096,
      "grad_norm": 11.668069839477539,
      "learning_rate": 3.299299996132576e-05,
      "loss": 2.0401,
      "step": 527700
    },
    {
      "epoch": 20.412267471091,
      "grad_norm": 11.123485565185547,
      "learning_rate": 3.298977710742417e-05,
      "loss": 1.9515,
      "step": 527800
    },
    {
      "epoch": 20.416134895772906,
      "grad_norm": 16.023252487182617,
      "learning_rate": 3.2986554253522587e-05,
      "loss": 1.9885,
      "step": 527900
    },
    {
      "epoch": 20.42000232045481,
      "grad_norm": 10.256082534790039,
      "learning_rate": 3.2983331399620994e-05,
      "loss": 1.9971,
      "step": 528000
    },
    {
      "epoch": 20.423869745136713,
      "grad_norm": 11.69927978515625,
      "learning_rate": 3.298010854571941e-05,
      "loss": 1.9345,
      "step": 528100
    },
    {
      "epoch": 20.427737169818617,
      "grad_norm": 16.88886833190918,
      "learning_rate": 3.2976885691817824e-05,
      "loss": 2.0259,
      "step": 528200
    },
    {
      "epoch": 20.431604594500524,
      "grad_norm": 11.15805435180664,
      "learning_rate": 3.297366283791623e-05,
      "loss": 2.0028,
      "step": 528300
    },
    {
      "epoch": 20.435472019182427,
      "grad_norm": 8.593632698059082,
      "learning_rate": 3.2970439984014647e-05,
      "loss": 1.862,
      "step": 528400
    },
    {
      "epoch": 20.43933944386433,
      "grad_norm": 10.742854118347168,
      "learning_rate": 3.296721713011306e-05,
      "loss": 1.9429,
      "step": 528500
    },
    {
      "epoch": 20.443206868546234,
      "grad_norm": 11.971490859985352,
      "learning_rate": 3.2963994276211476e-05,
      "loss": 2.0168,
      "step": 528600
    },
    {
      "epoch": 20.44707429322814,
      "grad_norm": 12.2086181640625,
      "learning_rate": 3.2960771422309884e-05,
      "loss": 1.9644,
      "step": 528700
    },
    {
      "epoch": 20.450941717910045,
      "grad_norm": 11.205409049987793,
      "learning_rate": 3.29575485684083e-05,
      "loss": 1.9487,
      "step": 528800
    },
    {
      "epoch": 20.454809142591948,
      "grad_norm": 10.34272575378418,
      "learning_rate": 3.295432571450671e-05,
      "loss": 2.0742,
      "step": 528900
    },
    {
      "epoch": 20.45867656727385,
      "grad_norm": 9.991304397583008,
      "learning_rate": 3.295110286060513e-05,
      "loss": 1.9629,
      "step": 529000
    },
    {
      "epoch": 20.462543991955755,
      "grad_norm": 10.630831718444824,
      "learning_rate": 3.2947880006703536e-05,
      "loss": 2.0091,
      "step": 529100
    },
    {
      "epoch": 20.466411416637662,
      "grad_norm": 14.992595672607422,
      "learning_rate": 3.294465715280195e-05,
      "loss": 1.968,
      "step": 529200
    },
    {
      "epoch": 20.470278841319566,
      "grad_norm": 15.436202049255371,
      "learning_rate": 3.2941434298900365e-05,
      "loss": 1.8931,
      "step": 529300
    },
    {
      "epoch": 20.47414626600147,
      "grad_norm": 13.419049263000488,
      "learning_rate": 3.293821144499878e-05,
      "loss": 1.8809,
      "step": 529400
    },
    {
      "epoch": 20.478013690683373,
      "grad_norm": 11.48664379119873,
      "learning_rate": 3.293498859109719e-05,
      "loss": 1.8984,
      "step": 529500
    },
    {
      "epoch": 20.48188111536528,
      "grad_norm": 10.694807052612305,
      "learning_rate": 3.29317657371956e-05,
      "loss": 1.8545,
      "step": 529600
    },
    {
      "epoch": 20.485748540047183,
      "grad_norm": 12.814095497131348,
      "learning_rate": 3.292854288329402e-05,
      "loss": 1.9475,
      "step": 529700
    },
    {
      "epoch": 20.489615964729087,
      "grad_norm": 9.451384544372559,
      "learning_rate": 3.292532002939243e-05,
      "loss": 2.017,
      "step": 529800
    },
    {
      "epoch": 20.49348338941099,
      "grad_norm": 12.168623924255371,
      "learning_rate": 3.292209717549084e-05,
      "loss": 1.9991,
      "step": 529900
    },
    {
      "epoch": 20.497350814092897,
      "grad_norm": 12.436840057373047,
      "learning_rate": 3.2918874321589255e-05,
      "loss": 1.9752,
      "step": 530000
    },
    {
      "epoch": 20.5012182387748,
      "grad_norm": 10.524528503417969,
      "learning_rate": 3.291565146768767e-05,
      "loss": 1.8976,
      "step": 530100
    },
    {
      "epoch": 20.505085663456704,
      "grad_norm": 11.636070251464844,
      "learning_rate": 3.2912428613786084e-05,
      "loss": 1.9886,
      "step": 530200
    },
    {
      "epoch": 20.508953088138608,
      "grad_norm": 12.262568473815918,
      "learning_rate": 3.290920575988449e-05,
      "loss": 1.8939,
      "step": 530300
    },
    {
      "epoch": 20.51282051282051,
      "grad_norm": 16.99293327331543,
      "learning_rate": 3.290598290598291e-05,
      "loss": 1.9554,
      "step": 530400
    },
    {
      "epoch": 20.516687937502418,
      "grad_norm": 13.54806137084961,
      "learning_rate": 3.290276005208132e-05,
      "loss": 1.8195,
      "step": 530500
    },
    {
      "epoch": 20.52055536218432,
      "grad_norm": 12.008817672729492,
      "learning_rate": 3.2899537198179736e-05,
      "loss": 2.0447,
      "step": 530600
    },
    {
      "epoch": 20.524422786866225,
      "grad_norm": 11.435531616210938,
      "learning_rate": 3.2896314344278144e-05,
      "loss": 1.8486,
      "step": 530700
    },
    {
      "epoch": 20.52829021154813,
      "grad_norm": 10.088969230651855,
      "learning_rate": 3.289309149037656e-05,
      "loss": 1.9941,
      "step": 530800
    },
    {
      "epoch": 20.532157636230036,
      "grad_norm": 10.074352264404297,
      "learning_rate": 3.2889868636474974e-05,
      "loss": 1.9388,
      "step": 530900
    },
    {
      "epoch": 20.53602506091194,
      "grad_norm": 12.167940139770508,
      "learning_rate": 3.288664578257339e-05,
      "loss": 1.9763,
      "step": 531000
    },
    {
      "epoch": 20.539892485593843,
      "grad_norm": 15.455307960510254,
      "learning_rate": 3.2883422928671797e-05,
      "loss": 2.117,
      "step": 531100
    },
    {
      "epoch": 20.543759910275746,
      "grad_norm": 11.477644920349121,
      "learning_rate": 3.288020007477021e-05,
      "loss": 1.8764,
      "step": 531200
    },
    {
      "epoch": 20.547627334957653,
      "grad_norm": 13.252863883972168,
      "learning_rate": 3.2876977220868626e-05,
      "loss": 1.9569,
      "step": 531300
    },
    {
      "epoch": 20.551494759639557,
      "grad_norm": 10.850361824035645,
      "learning_rate": 3.2873754366967034e-05,
      "loss": 1.9115,
      "step": 531400
    },
    {
      "epoch": 20.55536218432146,
      "grad_norm": 10.0289306640625,
      "learning_rate": 3.287053151306545e-05,
      "loss": 1.9701,
      "step": 531500
    },
    {
      "epoch": 20.559229609003363,
      "grad_norm": 12.13852596282959,
      "learning_rate": 3.286730865916386e-05,
      "loss": 1.9536,
      "step": 531600
    },
    {
      "epoch": 20.56309703368527,
      "grad_norm": 14.308060646057129,
      "learning_rate": 3.286408580526228e-05,
      "loss": 1.9226,
      "step": 531700
    },
    {
      "epoch": 20.566964458367174,
      "grad_norm": 8.836174011230469,
      "learning_rate": 3.2860862951360686e-05,
      "loss": 2.0053,
      "step": 531800
    },
    {
      "epoch": 20.570831883049078,
      "grad_norm": 8.267105102539062,
      "learning_rate": 3.28576400974591e-05,
      "loss": 1.9127,
      "step": 531900
    },
    {
      "epoch": 20.57469930773098,
      "grad_norm": 9.089983940124512,
      "learning_rate": 3.2854417243557515e-05,
      "loss": 1.935,
      "step": 532000
    },
    {
      "epoch": 20.578566732412888,
      "grad_norm": 9.488656044006348,
      "learning_rate": 3.285119438965593e-05,
      "loss": 1.8724,
      "step": 532100
    },
    {
      "epoch": 20.58243415709479,
      "grad_norm": 10.334014892578125,
      "learning_rate": 3.284797153575434e-05,
      "loss": 1.8943,
      "step": 532200
    },
    {
      "epoch": 20.586301581776695,
      "grad_norm": 8.880658149719238,
      "learning_rate": 3.284474868185275e-05,
      "loss": 1.9911,
      "step": 532300
    },
    {
      "epoch": 20.5901690064586,
      "grad_norm": 13.529391288757324,
      "learning_rate": 3.284152582795117e-05,
      "loss": 2.0057,
      "step": 532400
    },
    {
      "epoch": 20.594036431140502,
      "grad_norm": 13.061372756958008,
      "learning_rate": 3.283830297404958e-05,
      "loss": 1.9691,
      "step": 532500
    },
    {
      "epoch": 20.59790385582241,
      "grad_norm": 12.53704833984375,
      "learning_rate": 3.283508012014799e-05,
      "loss": 1.9264,
      "step": 532600
    },
    {
      "epoch": 20.601771280504313,
      "grad_norm": 11.941364288330078,
      "learning_rate": 3.2831857266246405e-05,
      "loss": 1.9971,
      "step": 532700
    },
    {
      "epoch": 20.605638705186216,
      "grad_norm": 13.050113677978516,
      "learning_rate": 3.282863441234482e-05,
      "loss": 1.9945,
      "step": 532800
    },
    {
      "epoch": 20.60950612986812,
      "grad_norm": 9.44764518737793,
      "learning_rate": 3.2825411558443234e-05,
      "loss": 2.0339,
      "step": 532900
    },
    {
      "epoch": 20.613373554550027,
      "grad_norm": 13.220799446105957,
      "learning_rate": 3.282218870454164e-05,
      "loss": 1.8917,
      "step": 533000
    },
    {
      "epoch": 20.61724097923193,
      "grad_norm": 10.716415405273438,
      "learning_rate": 3.281896585064006e-05,
      "loss": 1.9191,
      "step": 533100
    },
    {
      "epoch": 20.621108403913833,
      "grad_norm": 9.567976951599121,
      "learning_rate": 3.281574299673847e-05,
      "loss": 1.9977,
      "step": 533200
    },
    {
      "epoch": 20.624975828595737,
      "grad_norm": 8.67825698852539,
      "learning_rate": 3.2812520142836886e-05,
      "loss": 1.9033,
      "step": 533300
    },
    {
      "epoch": 20.628843253277644,
      "grad_norm": 9.948248863220215,
      "learning_rate": 3.28092972889353e-05,
      "loss": 1.9139,
      "step": 533400
    },
    {
      "epoch": 20.632710677959547,
      "grad_norm": 12.498125076293945,
      "learning_rate": 3.280607443503371e-05,
      "loss": 1.9607,
      "step": 533500
    },
    {
      "epoch": 20.63657810264145,
      "grad_norm": 10.499032974243164,
      "learning_rate": 3.2802851581132124e-05,
      "loss": 1.9402,
      "step": 533600
    },
    {
      "epoch": 20.640445527323354,
      "grad_norm": 12.09762954711914,
      "learning_rate": 3.279962872723054e-05,
      "loss": 1.9935,
      "step": 533700
    },
    {
      "epoch": 20.644312952005258,
      "grad_norm": 11.964604377746582,
      "learning_rate": 3.279640587332895e-05,
      "loss": 1.9902,
      "step": 533800
    },
    {
      "epoch": 20.648180376687165,
      "grad_norm": 15.14090633392334,
      "learning_rate": 3.279318301942737e-05,
      "loss": 1.9764,
      "step": 533900
    },
    {
      "epoch": 20.65204780136907,
      "grad_norm": 10.305315971374512,
      "learning_rate": 3.278996016552578e-05,
      "loss": 1.9694,
      "step": 534000
    },
    {
      "epoch": 20.655915226050972,
      "grad_norm": 10.704071998596191,
      "learning_rate": 3.278673731162419e-05,
      "loss": 1.9738,
      "step": 534100
    },
    {
      "epoch": 20.659782650732875,
      "grad_norm": 10.94746208190918,
      "learning_rate": 3.2783514457722605e-05,
      "loss": 2.0149,
      "step": 534200
    },
    {
      "epoch": 20.663650075414782,
      "grad_norm": 8.701678276062012,
      "learning_rate": 3.278029160382102e-05,
      "loss": 1.9586,
      "step": 534300
    },
    {
      "epoch": 20.667517500096686,
      "grad_norm": 13.375387191772461,
      "learning_rate": 3.2777068749919435e-05,
      "loss": 1.9034,
      "step": 534400
    },
    {
      "epoch": 20.67138492477859,
      "grad_norm": 14.72133731842041,
      "learning_rate": 3.277384589601784e-05,
      "loss": 2.0122,
      "step": 534500
    },
    {
      "epoch": 20.675252349460493,
      "grad_norm": 10.057302474975586,
      "learning_rate": 3.277062304211626e-05,
      "loss": 2.0061,
      "step": 534600
    },
    {
      "epoch": 20.6791197741424,
      "grad_norm": 12.324995994567871,
      "learning_rate": 3.276740018821467e-05,
      "loss": 1.9375,
      "step": 534700
    },
    {
      "epoch": 20.682987198824303,
      "grad_norm": 13.390810012817383,
      "learning_rate": 3.276417733431309e-05,
      "loss": 1.993,
      "step": 534800
    },
    {
      "epoch": 20.686854623506207,
      "grad_norm": 9.168275833129883,
      "learning_rate": 3.2760954480411495e-05,
      "loss": 2.0514,
      "step": 534900
    },
    {
      "epoch": 20.69072204818811,
      "grad_norm": 11.591302871704102,
      "learning_rate": 3.275773162650991e-05,
      "loss": 2.0226,
      "step": 535000
    },
    {
      "epoch": 20.694589472870017,
      "grad_norm": 10.989969253540039,
      "learning_rate": 3.2754508772608324e-05,
      "loss": 1.9298,
      "step": 535100
    },
    {
      "epoch": 20.69845689755192,
      "grad_norm": 12.258225440979004,
      "learning_rate": 3.275128591870674e-05,
      "loss": 1.8938,
      "step": 535200
    },
    {
      "epoch": 20.702324322233824,
      "grad_norm": 13.947580337524414,
      "learning_rate": 3.274806306480515e-05,
      "loss": 1.8987,
      "step": 535300
    },
    {
      "epoch": 20.706191746915728,
      "grad_norm": 10.407459259033203,
      "learning_rate": 3.274484021090356e-05,
      "loss": 2.0527,
      "step": 535400
    },
    {
      "epoch": 20.71005917159763,
      "grad_norm": 15.928696632385254,
      "learning_rate": 3.2741617357001976e-05,
      "loss": 1.9749,
      "step": 535500
    },
    {
      "epoch": 20.71392659627954,
      "grad_norm": 9.356978416442871,
      "learning_rate": 3.273839450310039e-05,
      "loss": 1.9928,
      "step": 535600
    },
    {
      "epoch": 20.717794020961442,
      "grad_norm": 11.691914558410645,
      "learning_rate": 3.27351716491988e-05,
      "loss": 1.9656,
      "step": 535700
    },
    {
      "epoch": 20.721661445643345,
      "grad_norm": 14.197556495666504,
      "learning_rate": 3.2731948795297214e-05,
      "loss": 1.9907,
      "step": 535800
    },
    {
      "epoch": 20.72552887032525,
      "grad_norm": 9.026344299316406,
      "learning_rate": 3.272872594139563e-05,
      "loss": 1.9414,
      "step": 535900
    },
    {
      "epoch": 20.729396295007156,
      "grad_norm": 11.497088432312012,
      "learning_rate": 3.272550308749404e-05,
      "loss": 1.9504,
      "step": 536000
    },
    {
      "epoch": 20.73326371968906,
      "grad_norm": 12.064813613891602,
      "learning_rate": 3.272228023359245e-05,
      "loss": 2.0115,
      "step": 536100
    },
    {
      "epoch": 20.737131144370963,
      "grad_norm": 11.027233123779297,
      "learning_rate": 3.2719057379690866e-05,
      "loss": 2.0264,
      "step": 536200
    },
    {
      "epoch": 20.740998569052866,
      "grad_norm": 13.69657039642334,
      "learning_rate": 3.271583452578928e-05,
      "loss": 2.0652,
      "step": 536300
    },
    {
      "epoch": 20.744865993734773,
      "grad_norm": 10.097698211669922,
      "learning_rate": 3.2712611671887695e-05,
      "loss": 1.8958,
      "step": 536400
    },
    {
      "epoch": 20.748733418416677,
      "grad_norm": 12.914131164550781,
      "learning_rate": 3.27093888179861e-05,
      "loss": 1.9904,
      "step": 536500
    },
    {
      "epoch": 20.75260084309858,
      "grad_norm": 9.287284851074219,
      "learning_rate": 3.270616596408452e-05,
      "loss": 2.0917,
      "step": 536600
    },
    {
      "epoch": 20.756468267780484,
      "grad_norm": 9.275880813598633,
      "learning_rate": 3.270294311018293e-05,
      "loss": 1.9855,
      "step": 536700
    },
    {
      "epoch": 20.76033569246239,
      "grad_norm": 9.476478576660156,
      "learning_rate": 3.269972025628135e-05,
      "loss": 1.9198,
      "step": 536800
    },
    {
      "epoch": 20.764203117144294,
      "grad_norm": 9.82663631439209,
      "learning_rate": 3.2696497402379755e-05,
      "loss": 1.9002,
      "step": 536900
    },
    {
      "epoch": 20.768070541826198,
      "grad_norm": 13.180843353271484,
      "learning_rate": 3.269327454847817e-05,
      "loss": 1.9202,
      "step": 537000
    },
    {
      "epoch": 20.7719379665081,
      "grad_norm": 12.645471572875977,
      "learning_rate": 3.2690051694576585e-05,
      "loss": 1.8871,
      "step": 537100
    },
    {
      "epoch": 20.775805391190005,
      "grad_norm": 13.189806938171387,
      "learning_rate": 3.268682884067499e-05,
      "loss": 1.9757,
      "step": 537200
    },
    {
      "epoch": 20.779672815871912,
      "grad_norm": 13.302081108093262,
      "learning_rate": 3.268360598677341e-05,
      "loss": 1.9859,
      "step": 537300
    },
    {
      "epoch": 20.783540240553815,
      "grad_norm": 12.478982925415039,
      "learning_rate": 3.268038313287182e-05,
      "loss": 1.8466,
      "step": 537400
    },
    {
      "epoch": 20.78740766523572,
      "grad_norm": 12.02667236328125,
      "learning_rate": 3.267716027897024e-05,
      "loss": 1.9472,
      "step": 537500
    },
    {
      "epoch": 20.791275089917622,
      "grad_norm": 11.864479064941406,
      "learning_rate": 3.2673937425068645e-05,
      "loss": 2.0739,
      "step": 537600
    },
    {
      "epoch": 20.79514251459953,
      "grad_norm": 10.794646263122559,
      "learning_rate": 3.267071457116706e-05,
      "loss": 1.9329,
      "step": 537700
    },
    {
      "epoch": 20.799009939281433,
      "grad_norm": 11.579652786254883,
      "learning_rate": 3.2667491717265474e-05,
      "loss": 1.861,
      "step": 537800
    },
    {
      "epoch": 20.802877363963336,
      "grad_norm": 12.96745777130127,
      "learning_rate": 3.266426886336389e-05,
      "loss": 2.0407,
      "step": 537900
    },
    {
      "epoch": 20.80674478864524,
      "grad_norm": 9.497430801391602,
      "learning_rate": 3.26610460094623e-05,
      "loss": 1.9511,
      "step": 538000
    },
    {
      "epoch": 20.810612213327147,
      "grad_norm": 13.05698013305664,
      "learning_rate": 3.265782315556071e-05,
      "loss": 2.0684,
      "step": 538100
    },
    {
      "epoch": 20.81447963800905,
      "grad_norm": 13.600759506225586,
      "learning_rate": 3.2654600301659126e-05,
      "loss": 2.0356,
      "step": 538200
    },
    {
      "epoch": 20.818347062690954,
      "grad_norm": 11.272326469421387,
      "learning_rate": 3.265137744775754e-05,
      "loss": 1.9185,
      "step": 538300
    },
    {
      "epoch": 20.822214487372857,
      "grad_norm": 10.468070030212402,
      "learning_rate": 3.264815459385595e-05,
      "loss": 2.003,
      "step": 538400
    },
    {
      "epoch": 20.826081912054764,
      "grad_norm": 13.764303207397461,
      "learning_rate": 3.2644931739954364e-05,
      "loss": 2.0535,
      "step": 538500
    },
    {
      "epoch": 20.829949336736668,
      "grad_norm": 9.260623931884766,
      "learning_rate": 3.264170888605278e-05,
      "loss": 1.9927,
      "step": 538600
    },
    {
      "epoch": 20.83381676141857,
      "grad_norm": 12.38245964050293,
      "learning_rate": 3.263848603215119e-05,
      "loss": 2.0204,
      "step": 538700
    },
    {
      "epoch": 20.837684186100475,
      "grad_norm": 13.312631607055664,
      "learning_rate": 3.26352631782496e-05,
      "loss": 1.9459,
      "step": 538800
    },
    {
      "epoch": 20.84155161078238,
      "grad_norm": 11.979909896850586,
      "learning_rate": 3.2632040324348016e-05,
      "loss": 1.8983,
      "step": 538900
    },
    {
      "epoch": 20.845419035464285,
      "grad_norm": 10.953612327575684,
      "learning_rate": 3.262881747044643e-05,
      "loss": 1.9428,
      "step": 539000
    },
    {
      "epoch": 20.84928646014619,
      "grad_norm": 9.245698928833008,
      "learning_rate": 3.2625594616544845e-05,
      "loss": 1.9521,
      "step": 539100
    },
    {
      "epoch": 20.853153884828092,
      "grad_norm": 12.169333457946777,
      "learning_rate": 3.262237176264325e-05,
      "loss": 1.9774,
      "step": 539200
    },
    {
      "epoch": 20.857021309509996,
      "grad_norm": 11.196528434753418,
      "learning_rate": 3.261914890874167e-05,
      "loss": 1.9422,
      "step": 539300
    },
    {
      "epoch": 20.860888734191903,
      "grad_norm": 9.192267417907715,
      "learning_rate": 3.261592605484008e-05,
      "loss": 1.967,
      "step": 539400
    },
    {
      "epoch": 20.864756158873806,
      "grad_norm": 10.99664306640625,
      "learning_rate": 3.26127032009385e-05,
      "loss": 2.0079,
      "step": 539500
    },
    {
      "epoch": 20.86862358355571,
      "grad_norm": 13.27007007598877,
      "learning_rate": 3.2609480347036905e-05,
      "loss": 1.9146,
      "step": 539600
    },
    {
      "epoch": 20.872491008237613,
      "grad_norm": 12.716085433959961,
      "learning_rate": 3.260625749313532e-05,
      "loss": 1.9148,
      "step": 539700
    },
    {
      "epoch": 20.87635843291952,
      "grad_norm": 11.881378173828125,
      "learning_rate": 3.2603034639233735e-05,
      "loss": 2.0458,
      "step": 539800
    },
    {
      "epoch": 20.880225857601424,
      "grad_norm": 12.247344970703125,
      "learning_rate": 3.259981178533215e-05,
      "loss": 1.9556,
      "step": 539900
    },
    {
      "epoch": 20.884093282283327,
      "grad_norm": 12.239415168762207,
      "learning_rate": 3.259658893143056e-05,
      "loss": 1.961,
      "step": 540000
    },
    {
      "epoch": 20.88796070696523,
      "grad_norm": 13.698219299316406,
      "learning_rate": 3.259336607752897e-05,
      "loss": 2.0112,
      "step": 540100
    },
    {
      "epoch": 20.891828131647138,
      "grad_norm": 11.372782707214355,
      "learning_rate": 3.259014322362739e-05,
      "loss": 1.8975,
      "step": 540200
    },
    {
      "epoch": 20.89569555632904,
      "grad_norm": 10.896540641784668,
      "learning_rate": 3.25869203697258e-05,
      "loss": 2.0032,
      "step": 540300
    },
    {
      "epoch": 20.899562981010945,
      "grad_norm": 9.752511024475098,
      "learning_rate": 3.2583697515824216e-05,
      "loss": 1.9999,
      "step": 540400
    },
    {
      "epoch": 20.90343040569285,
      "grad_norm": 13.497262954711914,
      "learning_rate": 3.2580474661922624e-05,
      "loss": 2.0204,
      "step": 540500
    },
    {
      "epoch": 20.907297830374752,
      "grad_norm": 12.127925872802734,
      "learning_rate": 3.257725180802104e-05,
      "loss": 2.0137,
      "step": 540600
    },
    {
      "epoch": 20.91116525505666,
      "grad_norm": 14.07883358001709,
      "learning_rate": 3.2574028954119454e-05,
      "loss": 1.9175,
      "step": 540700
    },
    {
      "epoch": 20.915032679738562,
      "grad_norm": 12.532373428344727,
      "learning_rate": 3.257080610021787e-05,
      "loss": 2.0136,
      "step": 540800
    },
    {
      "epoch": 20.918900104420466,
      "grad_norm": 12.146723747253418,
      "learning_rate": 3.256758324631628e-05,
      "loss": 1.9902,
      "step": 540900
    },
    {
      "epoch": 20.92276752910237,
      "grad_norm": 13.066095352172852,
      "learning_rate": 3.25643603924147e-05,
      "loss": 2.0034,
      "step": 541000
    },
    {
      "epoch": 20.926634953784276,
      "grad_norm": 12.238691329956055,
      "learning_rate": 3.2561137538513106e-05,
      "loss": 1.9238,
      "step": 541100
    },
    {
      "epoch": 20.93050237846618,
      "grad_norm": 9.536880493164062,
      "learning_rate": 3.255791468461152e-05,
      "loss": 1.9542,
      "step": 541200
    },
    {
      "epoch": 20.934369803148083,
      "grad_norm": 9.73835277557373,
      "learning_rate": 3.2554691830709935e-05,
      "loss": 1.9629,
      "step": 541300
    },
    {
      "epoch": 20.938237227829987,
      "grad_norm": 9.803072929382324,
      "learning_rate": 3.255146897680835e-05,
      "loss": 1.8922,
      "step": 541400
    },
    {
      "epoch": 20.942104652511894,
      "grad_norm": 11.11021614074707,
      "learning_rate": 3.254824612290676e-05,
      "loss": 1.9761,
      "step": 541500
    },
    {
      "epoch": 20.945972077193797,
      "grad_norm": 10.527188301086426,
      "learning_rate": 3.254502326900517e-05,
      "loss": 1.913,
      "step": 541600
    },
    {
      "epoch": 20.9498395018757,
      "grad_norm": 10.560257911682129,
      "learning_rate": 3.254180041510359e-05,
      "loss": 1.986,
      "step": 541700
    },
    {
      "epoch": 20.953706926557604,
      "grad_norm": 11.243815422058105,
      "learning_rate": 3.2538577561202e-05,
      "loss": 1.9111,
      "step": 541800
    },
    {
      "epoch": 20.957574351239508,
      "grad_norm": 12.02757740020752,
      "learning_rate": 3.253535470730041e-05,
      "loss": 1.981,
      "step": 541900
    },
    {
      "epoch": 20.961441775921415,
      "grad_norm": 12.113614082336426,
      "learning_rate": 3.2532131853398825e-05,
      "loss": 2.0144,
      "step": 542000
    },
    {
      "epoch": 20.96530920060332,
      "grad_norm": 10.201061248779297,
      "learning_rate": 3.252890899949724e-05,
      "loss": 1.9941,
      "step": 542100
    },
    {
      "epoch": 20.969176625285222,
      "grad_norm": 11.29018497467041,
      "learning_rate": 3.2525686145595654e-05,
      "loss": 1.9884,
      "step": 542200
    },
    {
      "epoch": 20.973044049967125,
      "grad_norm": 12.016775131225586,
      "learning_rate": 3.252246329169406e-05,
      "loss": 2.0053,
      "step": 542300
    },
    {
      "epoch": 20.976911474649032,
      "grad_norm": 6.048555374145508,
      "learning_rate": 3.251924043779248e-05,
      "loss": 1.9809,
      "step": 542400
    },
    {
      "epoch": 20.980778899330936,
      "grad_norm": 12.5831880569458,
      "learning_rate": 3.251601758389089e-05,
      "loss": 1.9048,
      "step": 542500
    },
    {
      "epoch": 20.98464632401284,
      "grad_norm": 11.178972244262695,
      "learning_rate": 3.2512794729989306e-05,
      "loss": 2.0431,
      "step": 542600
    },
    {
      "epoch": 20.988513748694743,
      "grad_norm": 10.997410774230957,
      "learning_rate": 3.2509571876087714e-05,
      "loss": 1.962,
      "step": 542700
    },
    {
      "epoch": 20.99238117337665,
      "grad_norm": 11.832659721374512,
      "learning_rate": 3.250634902218613e-05,
      "loss": 2.0033,
      "step": 542800
    },
    {
      "epoch": 20.996248598058553,
      "grad_norm": 10.348467826843262,
      "learning_rate": 3.2503126168284544e-05,
      "loss": 1.9646,
      "step": 542900
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.8615968227386475,
      "eval_runtime": 2.9093,
      "eval_samples_per_second": 467.816,
      "eval_steps_per_second": 467.816,
      "step": 542997
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.7627536058425903,
      "eval_runtime": 56.6361,
      "eval_samples_per_second": 456.546,
      "eval_steps_per_second": 456.546,
      "step": 542997
    },
    {
      "epoch": 21.000116022740457,
      "grad_norm": 9.433751106262207,
      "learning_rate": 3.249990331438295e-05,
      "loss": 1.9408,
      "step": 543000
    },
    {
      "epoch": 21.00398344742236,
      "grad_norm": 8.16064167022705,
      "learning_rate": 3.2496680460481366e-05,
      "loss": 1.9886,
      "step": 543100
    },
    {
      "epoch": 21.007850872104267,
      "grad_norm": 9.422682762145996,
      "learning_rate": 3.249345760657978e-05,
      "loss": 1.9218,
      "step": 543200
    },
    {
      "epoch": 21.01171829678617,
      "grad_norm": 15.439485549926758,
      "learning_rate": 3.2490234752678196e-05,
      "loss": 1.881,
      "step": 543300
    },
    {
      "epoch": 21.015585721468074,
      "grad_norm": 8.659259796142578,
      "learning_rate": 3.2487011898776604e-05,
      "loss": 1.9826,
      "step": 543400
    },
    {
      "epoch": 21.019453146149978,
      "grad_norm": 11.988325119018555,
      "learning_rate": 3.248378904487502e-05,
      "loss": 1.9729,
      "step": 543500
    },
    {
      "epoch": 21.02332057083188,
      "grad_norm": 12.196402549743652,
      "learning_rate": 3.248056619097343e-05,
      "loss": 2.0042,
      "step": 543600
    },
    {
      "epoch": 21.02718799551379,
      "grad_norm": 11.315871238708496,
      "learning_rate": 3.247734333707185e-05,
      "loss": 1.935,
      "step": 543700
    },
    {
      "epoch": 21.031055420195692,
      "grad_norm": 10.87523365020752,
      "learning_rate": 3.2474120483170256e-05,
      "loss": 1.9923,
      "step": 543800
    },
    {
      "epoch": 21.034922844877595,
      "grad_norm": 9.554450035095215,
      "learning_rate": 3.247089762926867e-05,
      "loss": 1.8877,
      "step": 543900
    },
    {
      "epoch": 21.0387902695595,
      "grad_norm": 10.739008903503418,
      "learning_rate": 3.2467674775367085e-05,
      "loss": 2.0737,
      "step": 544000
    },
    {
      "epoch": 21.042657694241406,
      "grad_norm": 10.915227890014648,
      "learning_rate": 3.24644519214655e-05,
      "loss": 1.8985,
      "step": 544100
    },
    {
      "epoch": 21.04652511892331,
      "grad_norm": 14.899052619934082,
      "learning_rate": 3.246122906756391e-05,
      "loss": 1.9487,
      "step": 544200
    },
    {
      "epoch": 21.050392543605213,
      "grad_norm": 9.397151947021484,
      "learning_rate": 3.245800621366232e-05,
      "loss": 1.9968,
      "step": 544300
    },
    {
      "epoch": 21.054259968287116,
      "grad_norm": 13.01054859161377,
      "learning_rate": 3.245478335976074e-05,
      "loss": 1.9298,
      "step": 544400
    },
    {
      "epoch": 21.058127392969023,
      "grad_norm": 11.80240535736084,
      "learning_rate": 3.245156050585915e-05,
      "loss": 1.9155,
      "step": 544500
    },
    {
      "epoch": 21.061994817650927,
      "grad_norm": 11.15269947052002,
      "learning_rate": 3.244833765195756e-05,
      "loss": 1.9239,
      "step": 544600
    },
    {
      "epoch": 21.06586224233283,
      "grad_norm": 9.03274154663086,
      "learning_rate": 3.2445114798055975e-05,
      "loss": 1.9612,
      "step": 544700
    },
    {
      "epoch": 21.069729667014734,
      "grad_norm": 10.333163261413574,
      "learning_rate": 3.244189194415439e-05,
      "loss": 1.9799,
      "step": 544800
    },
    {
      "epoch": 21.07359709169664,
      "grad_norm": 9.513940811157227,
      "learning_rate": 3.2438669090252804e-05,
      "loss": 1.9238,
      "step": 544900
    },
    {
      "epoch": 21.077464516378544,
      "grad_norm": 13.375922203063965,
      "learning_rate": 3.243544623635121e-05,
      "loss": 1.9017,
      "step": 545000
    },
    {
      "epoch": 21.081331941060448,
      "grad_norm": 9.722756385803223,
      "learning_rate": 3.243222338244963e-05,
      "loss": 1.8966,
      "step": 545100
    },
    {
      "epoch": 21.08519936574235,
      "grad_norm": 10.778932571411133,
      "learning_rate": 3.242900052854804e-05,
      "loss": 1.9452,
      "step": 545200
    },
    {
      "epoch": 21.089066790424255,
      "grad_norm": 13.275747299194336,
      "learning_rate": 3.2425777674646456e-05,
      "loss": 1.9215,
      "step": 545300
    },
    {
      "epoch": 21.092934215106162,
      "grad_norm": 10.694902420043945,
      "learning_rate": 3.2422554820744864e-05,
      "loss": 1.9079,
      "step": 545400
    },
    {
      "epoch": 21.096801639788065,
      "grad_norm": 12.92179012298584,
      "learning_rate": 3.241933196684328e-05,
      "loss": 2.0153,
      "step": 545500
    },
    {
      "epoch": 21.10066906446997,
      "grad_norm": 11.54035472869873,
      "learning_rate": 3.2416109112941694e-05,
      "loss": 1.8529,
      "step": 545600
    },
    {
      "epoch": 21.104536489151872,
      "grad_norm": 12.838034629821777,
      "learning_rate": 3.241288625904011e-05,
      "loss": 2.0322,
      "step": 545700
    },
    {
      "epoch": 21.10840391383378,
      "grad_norm": 10.749011039733887,
      "learning_rate": 3.2409663405138516e-05,
      "loss": 1.9135,
      "step": 545800
    },
    {
      "epoch": 21.112271338515683,
      "grad_norm": 10.925284385681152,
      "learning_rate": 3.240644055123693e-05,
      "loss": 1.8702,
      "step": 545900
    },
    {
      "epoch": 21.116138763197586,
      "grad_norm": 14.18183708190918,
      "learning_rate": 3.2403217697335346e-05,
      "loss": 1.8346,
      "step": 546000
    },
    {
      "epoch": 21.12000618787949,
      "grad_norm": 9.921245574951172,
      "learning_rate": 3.2399994843433754e-05,
      "loss": 1.9475,
      "step": 546100
    },
    {
      "epoch": 21.123873612561397,
      "grad_norm": 11.849308013916016,
      "learning_rate": 3.239677198953217e-05,
      "loss": 1.9412,
      "step": 546200
    },
    {
      "epoch": 21.1277410372433,
      "grad_norm": 10.550450325012207,
      "learning_rate": 3.239354913563058e-05,
      "loss": 1.9891,
      "step": 546300
    },
    {
      "epoch": 21.131608461925204,
      "grad_norm": 13.073102951049805,
      "learning_rate": 3.2390326281729e-05,
      "loss": 1.9182,
      "step": 546400
    },
    {
      "epoch": 21.135475886607107,
      "grad_norm": 13.134651184082031,
      "learning_rate": 3.2387103427827406e-05,
      "loss": 1.9985,
      "step": 546500
    },
    {
      "epoch": 21.139343311289014,
      "grad_norm": 10.656153678894043,
      "learning_rate": 3.238388057392582e-05,
      "loss": 1.951,
      "step": 546600
    },
    {
      "epoch": 21.143210735970918,
      "grad_norm": 11.729897499084473,
      "learning_rate": 3.2380657720024235e-05,
      "loss": 1.997,
      "step": 546700
    },
    {
      "epoch": 21.14707816065282,
      "grad_norm": 11.41192626953125,
      "learning_rate": 3.237743486612265e-05,
      "loss": 1.8608,
      "step": 546800
    },
    {
      "epoch": 21.150945585334725,
      "grad_norm": 11.089668273925781,
      "learning_rate": 3.2374212012221065e-05,
      "loss": 1.9943,
      "step": 546900
    },
    {
      "epoch": 21.15481301001663,
      "grad_norm": 11.265835762023926,
      "learning_rate": 3.237098915831947e-05,
      "loss": 1.9461,
      "step": 547000
    },
    {
      "epoch": 21.158680434698535,
      "grad_norm": 9.157642364501953,
      "learning_rate": 3.236776630441789e-05,
      "loss": 1.9474,
      "step": 547100
    },
    {
      "epoch": 21.16254785938044,
      "grad_norm": 11.637524604797363,
      "learning_rate": 3.23645434505163e-05,
      "loss": 2.0282,
      "step": 547200
    },
    {
      "epoch": 21.166415284062342,
      "grad_norm": 11.208806037902832,
      "learning_rate": 3.236132059661472e-05,
      "loss": 1.8359,
      "step": 547300
    },
    {
      "epoch": 21.170282708744246,
      "grad_norm": 11.3710298538208,
      "learning_rate": 3.235809774271313e-05,
      "loss": 1.8345,
      "step": 547400
    },
    {
      "epoch": 21.174150133426153,
      "grad_norm": 8.88263988494873,
      "learning_rate": 3.2354874888811546e-05,
      "loss": 1.9537,
      "step": 547500
    },
    {
      "epoch": 21.178017558108056,
      "grad_norm": 12.674646377563477,
      "learning_rate": 3.2351652034909954e-05,
      "loss": 1.8604,
      "step": 547600
    },
    {
      "epoch": 21.18188498278996,
      "grad_norm": 11.547818183898926,
      "learning_rate": 3.234842918100837e-05,
      "loss": 1.9244,
      "step": 547700
    },
    {
      "epoch": 21.185752407471863,
      "grad_norm": 15.33128833770752,
      "learning_rate": 3.2345206327106783e-05,
      "loss": 2.018,
      "step": 547800
    },
    {
      "epoch": 21.18961983215377,
      "grad_norm": 11.673737525939941,
      "learning_rate": 3.23419834732052e-05,
      "loss": 1.9571,
      "step": 547900
    },
    {
      "epoch": 21.193487256835674,
      "grad_norm": 10.6172513961792,
      "learning_rate": 3.233876061930361e-05,
      "loss": 1.9322,
      "step": 548000
    },
    {
      "epoch": 21.197354681517577,
      "grad_norm": 16.91451072692871,
      "learning_rate": 3.233553776540202e-05,
      "loss": 1.9978,
      "step": 548100
    },
    {
      "epoch": 21.20122210619948,
      "grad_norm": 9.798935890197754,
      "learning_rate": 3.2332314911500436e-05,
      "loss": 1.9089,
      "step": 548200
    },
    {
      "epoch": 21.205089530881388,
      "grad_norm": 16.9283447265625,
      "learning_rate": 3.232909205759885e-05,
      "loss": 1.9258,
      "step": 548300
    },
    {
      "epoch": 21.20895695556329,
      "grad_norm": 9.416345596313477,
      "learning_rate": 3.2325869203697265e-05,
      "loss": 1.906,
      "step": 548400
    },
    {
      "epoch": 21.212824380245195,
      "grad_norm": 13.712960243225098,
      "learning_rate": 3.232264634979567e-05,
      "loss": 1.9293,
      "step": 548500
    },
    {
      "epoch": 21.2166918049271,
      "grad_norm": 11.468886375427246,
      "learning_rate": 3.231942349589409e-05,
      "loss": 1.9237,
      "step": 548600
    },
    {
      "epoch": 21.220559229609,
      "grad_norm": 13.406902313232422,
      "learning_rate": 3.23162006419925e-05,
      "loss": 2.0547,
      "step": 548700
    },
    {
      "epoch": 21.22442665429091,
      "grad_norm": 15.258849143981934,
      "learning_rate": 3.231297778809092e-05,
      "loss": 2.0307,
      "step": 548800
    },
    {
      "epoch": 21.228294078972812,
      "grad_norm": 10.536828994750977,
      "learning_rate": 3.2309754934189325e-05,
      "loss": 1.9746,
      "step": 548900
    },
    {
      "epoch": 21.232161503654716,
      "grad_norm": 9.64212417602539,
      "learning_rate": 3.230653208028774e-05,
      "loss": 1.9678,
      "step": 549000
    },
    {
      "epoch": 21.23602892833662,
      "grad_norm": 10.13193130493164,
      "learning_rate": 3.2303309226386154e-05,
      "loss": 1.957,
      "step": 549100
    },
    {
      "epoch": 21.239896353018526,
      "grad_norm": 9.667799949645996,
      "learning_rate": 3.230008637248456e-05,
      "loss": 1.8719,
      "step": 549200
    },
    {
      "epoch": 21.24376377770043,
      "grad_norm": 11.303938865661621,
      "learning_rate": 3.229686351858298e-05,
      "loss": 1.958,
      "step": 549300
    },
    {
      "epoch": 21.247631202382333,
      "grad_norm": 9.90566349029541,
      "learning_rate": 3.229364066468139e-05,
      "loss": 1.9521,
      "step": 549400
    },
    {
      "epoch": 21.251498627064237,
      "grad_norm": 14.789237022399902,
      "learning_rate": 3.2290417810779807e-05,
      "loss": 1.9756,
      "step": 549500
    },
    {
      "epoch": 21.255366051746144,
      "grad_norm": 11.835318565368652,
      "learning_rate": 3.2287194956878214e-05,
      "loss": 1.8933,
      "step": 549600
    },
    {
      "epoch": 21.259233476428047,
      "grad_norm": 13.496338844299316,
      "learning_rate": 3.228397210297663e-05,
      "loss": 1.9869,
      "step": 549700
    },
    {
      "epoch": 21.26310090110995,
      "grad_norm": 11.834104537963867,
      "learning_rate": 3.2280749249075044e-05,
      "loss": 1.9151,
      "step": 549800
    },
    {
      "epoch": 21.266968325791854,
      "grad_norm": 11.047627449035645,
      "learning_rate": 3.227752639517346e-05,
      "loss": 1.9766,
      "step": 549900
    },
    {
      "epoch": 21.270835750473758,
      "grad_norm": 9.933846473693848,
      "learning_rate": 3.2274303541271867e-05,
      "loss": 1.9635,
      "step": 550000
    },
    {
      "epoch": 21.274703175155665,
      "grad_norm": 11.745553016662598,
      "learning_rate": 3.227108068737028e-05,
      "loss": 1.9389,
      "step": 550100
    },
    {
      "epoch": 21.27857059983757,
      "grad_norm": 9.73734188079834,
      "learning_rate": 3.2267857833468696e-05,
      "loss": 1.9755,
      "step": 550200
    },
    {
      "epoch": 21.28243802451947,
      "grad_norm": 13.339045524597168,
      "learning_rate": 3.226463497956711e-05,
      "loss": 1.9802,
      "step": 550300
    },
    {
      "epoch": 21.286305449201375,
      "grad_norm": 10.904074668884277,
      "learning_rate": 3.226141212566552e-05,
      "loss": 1.9373,
      "step": 550400
    },
    {
      "epoch": 21.290172873883282,
      "grad_norm": 13.032923698425293,
      "learning_rate": 3.2258189271763933e-05,
      "loss": 1.9471,
      "step": 550500
    },
    {
      "epoch": 21.294040298565186,
      "grad_norm": 10.756152153015137,
      "learning_rate": 3.225496641786235e-05,
      "loss": 1.9415,
      "step": 550600
    },
    {
      "epoch": 21.29790772324709,
      "grad_norm": 10.347269058227539,
      "learning_rate": 3.225174356396076e-05,
      "loss": 1.8877,
      "step": 550700
    },
    {
      "epoch": 21.301775147928993,
      "grad_norm": 11.577449798583984,
      "learning_rate": 3.224852071005917e-05,
      "loss": 1.9273,
      "step": 550800
    },
    {
      "epoch": 21.3056425726109,
      "grad_norm": 11.338038444519043,
      "learning_rate": 3.2245297856157585e-05,
      "loss": 1.9397,
      "step": 550900
    },
    {
      "epoch": 21.309509997292803,
      "grad_norm": 12.504341125488281,
      "learning_rate": 3.2242075002256e-05,
      "loss": 1.8855,
      "step": 551000
    },
    {
      "epoch": 21.313377421974707,
      "grad_norm": 8.972794532775879,
      "learning_rate": 3.2238852148354415e-05,
      "loss": 1.9282,
      "step": 551100
    },
    {
      "epoch": 21.31724484665661,
      "grad_norm": 15.404769897460938,
      "learning_rate": 3.223562929445282e-05,
      "loss": 1.9491,
      "step": 551200
    },
    {
      "epoch": 21.321112271338517,
      "grad_norm": 13.236534118652344,
      "learning_rate": 3.223240644055124e-05,
      "loss": 2.0082,
      "step": 551300
    },
    {
      "epoch": 21.32497969602042,
      "grad_norm": 12.522126197814941,
      "learning_rate": 3.222918358664965e-05,
      "loss": 1.9491,
      "step": 551400
    },
    {
      "epoch": 21.328847120702324,
      "grad_norm": 11.517631530761719,
      "learning_rate": 3.222596073274807e-05,
      "loss": 1.943,
      "step": 551500
    },
    {
      "epoch": 21.332714545384228,
      "grad_norm": 11.341838836669922,
      "learning_rate": 3.2222737878846475e-05,
      "loss": 1.9806,
      "step": 551600
    },
    {
      "epoch": 21.33658197006613,
      "grad_norm": 12.467967987060547,
      "learning_rate": 3.221951502494489e-05,
      "loss": 1.9427,
      "step": 551700
    },
    {
      "epoch": 21.34044939474804,
      "grad_norm": 12.700751304626465,
      "learning_rate": 3.2216292171043304e-05,
      "loss": 1.9904,
      "step": 551800
    },
    {
      "epoch": 21.34431681942994,
      "grad_norm": 11.218817710876465,
      "learning_rate": 3.221306931714171e-05,
      "loss": 1.9591,
      "step": 551900
    },
    {
      "epoch": 21.348184244111845,
      "grad_norm": 10.61703872680664,
      "learning_rate": 3.220984646324013e-05,
      "loss": 2.0113,
      "step": 552000
    },
    {
      "epoch": 21.35205166879375,
      "grad_norm": 10.403509140014648,
      "learning_rate": 3.220662360933854e-05,
      "loss": 1.9535,
      "step": 552100
    },
    {
      "epoch": 21.355919093475656,
      "grad_norm": 13.18564510345459,
      "learning_rate": 3.2203400755436957e-05,
      "loss": 1.9063,
      "step": 552200
    },
    {
      "epoch": 21.35978651815756,
      "grad_norm": 8.960789680480957,
      "learning_rate": 3.2200177901535364e-05,
      "loss": 1.9751,
      "step": 552300
    },
    {
      "epoch": 21.363653942839463,
      "grad_norm": 11.11913013458252,
      "learning_rate": 3.219695504763378e-05,
      "loss": 1.878,
      "step": 552400
    },
    {
      "epoch": 21.367521367521366,
      "grad_norm": 10.727557182312012,
      "learning_rate": 3.2193732193732194e-05,
      "loss": 1.962,
      "step": 552500
    },
    {
      "epoch": 21.371388792203273,
      "grad_norm": 9.23705005645752,
      "learning_rate": 3.219050933983061e-05,
      "loss": 1.9816,
      "step": 552600
    },
    {
      "epoch": 21.375256216885177,
      "grad_norm": 12.07923412322998,
      "learning_rate": 3.2187286485929017e-05,
      "loss": 1.9072,
      "step": 552700
    },
    {
      "epoch": 21.37912364156708,
      "grad_norm": 9.296977043151855,
      "learning_rate": 3.218406363202743e-05,
      "loss": 1.8994,
      "step": 552800
    },
    {
      "epoch": 21.382991066248984,
      "grad_norm": 14.345210075378418,
      "learning_rate": 3.2180840778125846e-05,
      "loss": 1.9439,
      "step": 552900
    },
    {
      "epoch": 21.38685849093089,
      "grad_norm": 14.234925270080566,
      "learning_rate": 3.217761792422426e-05,
      "loss": 1.9894,
      "step": 553000
    },
    {
      "epoch": 21.390725915612794,
      "grad_norm": 8.038656234741211,
      "learning_rate": 3.217439507032267e-05,
      "loss": 1.9613,
      "step": 553100
    },
    {
      "epoch": 21.394593340294698,
      "grad_norm": 10.000920295715332,
      "learning_rate": 3.217117221642108e-05,
      "loss": 2.0067,
      "step": 553200
    },
    {
      "epoch": 21.3984607649766,
      "grad_norm": 12.490534782409668,
      "learning_rate": 3.21679493625195e-05,
      "loss": 1.9282,
      "step": 553300
    },
    {
      "epoch": 21.402328189658505,
      "grad_norm": 11.105527877807617,
      "learning_rate": 3.216472650861791e-05,
      "loss": 1.904,
      "step": 553400
    },
    {
      "epoch": 21.40619561434041,
      "grad_norm": 13.436415672302246,
      "learning_rate": 3.216150365471632e-05,
      "loss": 1.9332,
      "step": 553500
    },
    {
      "epoch": 21.410063039022315,
      "grad_norm": 9.366287231445312,
      "learning_rate": 3.2158280800814735e-05,
      "loss": 1.8398,
      "step": 553600
    },
    {
      "epoch": 21.41393046370422,
      "grad_norm": 11.504213333129883,
      "learning_rate": 3.215505794691315e-05,
      "loss": 1.9924,
      "step": 553700
    },
    {
      "epoch": 21.417797888386122,
      "grad_norm": 11.171661376953125,
      "learning_rate": 3.2151835093011565e-05,
      "loss": 1.9547,
      "step": 553800
    },
    {
      "epoch": 21.42166531306803,
      "grad_norm": 13.72227668762207,
      "learning_rate": 3.214861223910998e-05,
      "loss": 1.9347,
      "step": 553900
    },
    {
      "epoch": 21.425532737749933,
      "grad_norm": 11.746005058288574,
      "learning_rate": 3.214538938520839e-05,
      "loss": 1.9734,
      "step": 554000
    },
    {
      "epoch": 21.429400162431836,
      "grad_norm": 8.81971549987793,
      "learning_rate": 3.21421665313068e-05,
      "loss": 1.9348,
      "step": 554100
    },
    {
      "epoch": 21.43326758711374,
      "grad_norm": 10.48559856414795,
      "learning_rate": 3.213894367740522e-05,
      "loss": 1.9164,
      "step": 554200
    },
    {
      "epoch": 21.437135011795647,
      "grad_norm": 14.760580062866211,
      "learning_rate": 3.213572082350363e-05,
      "loss": 1.9896,
      "step": 554300
    },
    {
      "epoch": 21.44100243647755,
      "grad_norm": 11.267854690551758,
      "learning_rate": 3.2132497969602046e-05,
      "loss": 2.0006,
      "step": 554400
    },
    {
      "epoch": 21.444869861159454,
      "grad_norm": 11.383665084838867,
      "learning_rate": 3.212927511570046e-05,
      "loss": 1.9613,
      "step": 554500
    },
    {
      "epoch": 21.448737285841357,
      "grad_norm": 7.369920253753662,
      "learning_rate": 3.212605226179887e-05,
      "loss": 2.0274,
      "step": 554600
    },
    {
      "epoch": 21.452604710523264,
      "grad_norm": 14.032035827636719,
      "learning_rate": 3.2122829407897284e-05,
      "loss": 2.009,
      "step": 554700
    },
    {
      "epoch": 21.456472135205168,
      "grad_norm": 10.56934928894043,
      "learning_rate": 3.21196065539957e-05,
      "loss": 2.0353,
      "step": 554800
    },
    {
      "epoch": 21.46033955988707,
      "grad_norm": 8.684595108032227,
      "learning_rate": 3.211638370009411e-05,
      "loss": 1.9793,
      "step": 554900
    },
    {
      "epoch": 21.464206984568975,
      "grad_norm": 12.840811729431152,
      "learning_rate": 3.211316084619252e-05,
      "loss": 1.9829,
      "step": 555000
    },
    {
      "epoch": 21.468074409250878,
      "grad_norm": 11.999329566955566,
      "learning_rate": 3.2109937992290936e-05,
      "loss": 1.8521,
      "step": 555100
    },
    {
      "epoch": 21.471941833932785,
      "grad_norm": 12.57791805267334,
      "learning_rate": 3.210671513838935e-05,
      "loss": 1.9148,
      "step": 555200
    },
    {
      "epoch": 21.47580925861469,
      "grad_norm": 11.048452377319336,
      "learning_rate": 3.2103492284487765e-05,
      "loss": 1.9946,
      "step": 555300
    },
    {
      "epoch": 21.479676683296592,
      "grad_norm": 12.12086296081543,
      "learning_rate": 3.210026943058617e-05,
      "loss": 2.0215,
      "step": 555400
    },
    {
      "epoch": 21.483544107978496,
      "grad_norm": 10.382543563842773,
      "learning_rate": 3.209704657668459e-05,
      "loss": 2.0008,
      "step": 555500
    },
    {
      "epoch": 21.487411532660403,
      "grad_norm": 9.64744758605957,
      "learning_rate": 3.2093823722783e-05,
      "loss": 1.9511,
      "step": 555600
    },
    {
      "epoch": 21.491278957342306,
      "grad_norm": 11.849069595336914,
      "learning_rate": 3.209060086888142e-05,
      "loss": 2.0132,
      "step": 555700
    },
    {
      "epoch": 21.49514638202421,
      "grad_norm": 11.474235534667969,
      "learning_rate": 3.2087378014979825e-05,
      "loss": 1.9261,
      "step": 555800
    },
    {
      "epoch": 21.499013806706113,
      "grad_norm": 10.492133140563965,
      "learning_rate": 3.208415516107824e-05,
      "loss": 1.9269,
      "step": 555900
    },
    {
      "epoch": 21.50288123138802,
      "grad_norm": 12.654250144958496,
      "learning_rate": 3.2080932307176655e-05,
      "loss": 1.9178,
      "step": 556000
    },
    {
      "epoch": 21.506748656069924,
      "grad_norm": 9.706853866577148,
      "learning_rate": 3.207770945327507e-05,
      "loss": 1.9558,
      "step": 556100
    },
    {
      "epoch": 21.510616080751827,
      "grad_norm": 15.256176948547363,
      "learning_rate": 3.207448659937348e-05,
      "loss": 1.9828,
      "step": 556200
    },
    {
      "epoch": 21.51448350543373,
      "grad_norm": 16.04564094543457,
      "learning_rate": 3.207126374547189e-05,
      "loss": 2.0085,
      "step": 556300
    },
    {
      "epoch": 21.518350930115638,
      "grad_norm": 13.060090065002441,
      "learning_rate": 3.206804089157031e-05,
      "loss": 1.9234,
      "step": 556400
    },
    {
      "epoch": 21.52221835479754,
      "grad_norm": 11.296422958374023,
      "learning_rate": 3.206481803766872e-05,
      "loss": 2.0231,
      "step": 556500
    },
    {
      "epoch": 21.526085779479445,
      "grad_norm": 9.248391151428223,
      "learning_rate": 3.206159518376713e-05,
      "loss": 1.9812,
      "step": 556600
    },
    {
      "epoch": 21.529953204161348,
      "grad_norm": 12.064491271972656,
      "learning_rate": 3.2058372329865544e-05,
      "loss": 1.9977,
      "step": 556700
    },
    {
      "epoch": 21.53382062884325,
      "grad_norm": 11.583858489990234,
      "learning_rate": 3.205514947596396e-05,
      "loss": 1.8878,
      "step": 556800
    },
    {
      "epoch": 21.53768805352516,
      "grad_norm": 8.165403366088867,
      "learning_rate": 3.2051926622062374e-05,
      "loss": 1.9607,
      "step": 556900
    },
    {
      "epoch": 21.541555478207062,
      "grad_norm": 11.486810684204102,
      "learning_rate": 3.204870376816078e-05,
      "loss": 1.9032,
      "step": 557000
    },
    {
      "epoch": 21.545422902888966,
      "grad_norm": 12.58320426940918,
      "learning_rate": 3.2045480914259196e-05,
      "loss": 1.8298,
      "step": 557100
    },
    {
      "epoch": 21.54929032757087,
      "grad_norm": 9.017304420471191,
      "learning_rate": 3.204225806035761e-05,
      "loss": 1.9566,
      "step": 557200
    },
    {
      "epoch": 21.553157752252776,
      "grad_norm": 12.052515029907227,
      "learning_rate": 3.2039035206456026e-05,
      "loss": 1.9266,
      "step": 557300
    },
    {
      "epoch": 21.55702517693468,
      "grad_norm": 14.331160545349121,
      "learning_rate": 3.2035812352554434e-05,
      "loss": 1.9053,
      "step": 557400
    },
    {
      "epoch": 21.560892601616583,
      "grad_norm": 15.136734008789062,
      "learning_rate": 3.203258949865285e-05,
      "loss": 1.9406,
      "step": 557500
    },
    {
      "epoch": 21.564760026298487,
      "grad_norm": 9.501829147338867,
      "learning_rate": 3.202936664475126e-05,
      "loss": 1.9796,
      "step": 557600
    },
    {
      "epoch": 21.568627450980394,
      "grad_norm": 12.047146797180176,
      "learning_rate": 3.202614379084967e-05,
      "loss": 2.0269,
      "step": 557700
    },
    {
      "epoch": 21.572494875662297,
      "grad_norm": 9.337682723999023,
      "learning_rate": 3.2022920936948086e-05,
      "loss": 1.9841,
      "step": 557800
    },
    {
      "epoch": 21.5763623003442,
      "grad_norm": 10.30461311340332,
      "learning_rate": 3.20196980830465e-05,
      "loss": 1.9912,
      "step": 557900
    },
    {
      "epoch": 21.580229725026104,
      "grad_norm": 13.45627212524414,
      "learning_rate": 3.2016475229144915e-05,
      "loss": 1.9563,
      "step": 558000
    },
    {
      "epoch": 21.584097149708008,
      "grad_norm": 12.988226890563965,
      "learning_rate": 3.201325237524332e-05,
      "loss": 1.9689,
      "step": 558100
    },
    {
      "epoch": 21.587964574389915,
      "grad_norm": 11.038273811340332,
      "learning_rate": 3.201002952134174e-05,
      "loss": 1.9676,
      "step": 558200
    },
    {
      "epoch": 21.591831999071818,
      "grad_norm": 20.634557723999023,
      "learning_rate": 3.200680666744015e-05,
      "loss": 1.963,
      "step": 558300
    },
    {
      "epoch": 21.59569942375372,
      "grad_norm": 10.74431324005127,
      "learning_rate": 3.200358381353857e-05,
      "loss": 1.9458,
      "step": 558400
    },
    {
      "epoch": 21.599566848435625,
      "grad_norm": 7.317821502685547,
      "learning_rate": 3.2000360959636975e-05,
      "loss": 1.8641,
      "step": 558500
    },
    {
      "epoch": 21.603434273117532,
      "grad_norm": 13.055290222167969,
      "learning_rate": 3.199713810573539e-05,
      "loss": 1.8921,
      "step": 558600
    },
    {
      "epoch": 21.607301697799436,
      "grad_norm": 9.509249687194824,
      "learning_rate": 3.1993915251833805e-05,
      "loss": 1.9051,
      "step": 558700
    },
    {
      "epoch": 21.61116912248134,
      "grad_norm": 13.707341194152832,
      "learning_rate": 3.199069239793222e-05,
      "loss": 1.9399,
      "step": 558800
    },
    {
      "epoch": 21.615036547163243,
      "grad_norm": 13.87275218963623,
      "learning_rate": 3.198746954403063e-05,
      "loss": 1.9238,
      "step": 558900
    },
    {
      "epoch": 21.61890397184515,
      "grad_norm": 11.266060829162598,
      "learning_rate": 3.198424669012904e-05,
      "loss": 1.9828,
      "step": 559000
    },
    {
      "epoch": 21.622771396527053,
      "grad_norm": 8.308743476867676,
      "learning_rate": 3.198102383622746e-05,
      "loss": 2.0218,
      "step": 559100
    },
    {
      "epoch": 21.626638821208957,
      "grad_norm": 11.30610179901123,
      "learning_rate": 3.197780098232587e-05,
      "loss": 1.9276,
      "step": 559200
    },
    {
      "epoch": 21.63050624589086,
      "grad_norm": 9.296642303466797,
      "learning_rate": 3.197457812842428e-05,
      "loss": 1.9808,
      "step": 559300
    },
    {
      "epoch": 21.634373670572767,
      "grad_norm": 12.048040390014648,
      "learning_rate": 3.1971355274522694e-05,
      "loss": 1.8947,
      "step": 559400
    },
    {
      "epoch": 21.63824109525467,
      "grad_norm": 11.108382225036621,
      "learning_rate": 3.196813242062111e-05,
      "loss": 1.9523,
      "step": 559500
    },
    {
      "epoch": 21.642108519936574,
      "grad_norm": 10.968759536743164,
      "learning_rate": 3.1964909566719524e-05,
      "loss": 1.96,
      "step": 559600
    },
    {
      "epoch": 21.645975944618478,
      "grad_norm": 9.953871726989746,
      "learning_rate": 3.196168671281793e-05,
      "loss": 1.8173,
      "step": 559700
    },
    {
      "epoch": 21.649843369300385,
      "grad_norm": 11.41211986541748,
      "learning_rate": 3.1958463858916346e-05,
      "loss": 1.9444,
      "step": 559800
    },
    {
      "epoch": 21.653710793982288,
      "grad_norm": 11.790159225463867,
      "learning_rate": 3.195524100501476e-05,
      "loss": 1.9435,
      "step": 559900
    },
    {
      "epoch": 21.65757821866419,
      "grad_norm": 11.826577186584473,
      "learning_rate": 3.1952018151113176e-05,
      "loss": 1.9675,
      "step": 560000
    },
    {
      "epoch": 21.661445643346095,
      "grad_norm": 11.228059768676758,
      "learning_rate": 3.1948795297211584e-05,
      "loss": 2.0229,
      "step": 560100
    },
    {
      "epoch": 21.665313068028,
      "grad_norm": 13.603163719177246,
      "learning_rate": 3.194557244331e-05,
      "loss": 1.9076,
      "step": 560200
    },
    {
      "epoch": 21.669180492709906,
      "grad_norm": 13.04451847076416,
      "learning_rate": 3.194234958940841e-05,
      "loss": 1.965,
      "step": 560300
    },
    {
      "epoch": 21.67304791739181,
      "grad_norm": 8.657764434814453,
      "learning_rate": 3.193912673550683e-05,
      "loss": 1.9596,
      "step": 560400
    },
    {
      "epoch": 21.676915342073713,
      "grad_norm": 11.39532470703125,
      "learning_rate": 3.1935903881605236e-05,
      "loss": 2.0192,
      "step": 560500
    },
    {
      "epoch": 21.680782766755616,
      "grad_norm": 9.699912071228027,
      "learning_rate": 3.193268102770365e-05,
      "loss": 2.0598,
      "step": 560600
    },
    {
      "epoch": 21.684650191437523,
      "grad_norm": 11.092398643493652,
      "learning_rate": 3.1929458173802065e-05,
      "loss": 2.0009,
      "step": 560700
    },
    {
      "epoch": 21.688517616119427,
      "grad_norm": 11.778462409973145,
      "learning_rate": 3.192623531990048e-05,
      "loss": 2.0994,
      "step": 560800
    },
    {
      "epoch": 21.69238504080133,
      "grad_norm": 13.956496238708496,
      "learning_rate": 3.1923012465998895e-05,
      "loss": 1.8715,
      "step": 560900
    },
    {
      "epoch": 21.696252465483234,
      "grad_norm": 8.293417930603027,
      "learning_rate": 3.191978961209731e-05,
      "loss": 1.9432,
      "step": 561000
    },
    {
      "epoch": 21.70011989016514,
      "grad_norm": 10.887736320495605,
      "learning_rate": 3.191656675819572e-05,
      "loss": 1.939,
      "step": 561100
    },
    {
      "epoch": 21.703987314847044,
      "grad_norm": 10.825682640075684,
      "learning_rate": 3.191334390429413e-05,
      "loss": 1.9651,
      "step": 561200
    },
    {
      "epoch": 21.707854739528948,
      "grad_norm": 13.757002830505371,
      "learning_rate": 3.191012105039255e-05,
      "loss": 1.9936,
      "step": 561300
    },
    {
      "epoch": 21.71172216421085,
      "grad_norm": 12.779071807861328,
      "learning_rate": 3.190689819649096e-05,
      "loss": 2.0117,
      "step": 561400
    },
    {
      "epoch": 21.715589588892755,
      "grad_norm": 13.525382995605469,
      "learning_rate": 3.1903675342589376e-05,
      "loss": 1.9818,
      "step": 561500
    },
    {
      "epoch": 21.71945701357466,
      "grad_norm": 11.73403549194336,
      "learning_rate": 3.1900452488687784e-05,
      "loss": 2.0625,
      "step": 561600
    },
    {
      "epoch": 21.723324438256565,
      "grad_norm": 14.598854064941406,
      "learning_rate": 3.18972296347862e-05,
      "loss": 2.0047,
      "step": 561700
    },
    {
      "epoch": 21.72719186293847,
      "grad_norm": 10.931650161743164,
      "learning_rate": 3.1894006780884614e-05,
      "loss": 1.9528,
      "step": 561800
    },
    {
      "epoch": 21.731059287620372,
      "grad_norm": 11.218318939208984,
      "learning_rate": 3.189078392698303e-05,
      "loss": 1.9735,
      "step": 561900
    },
    {
      "epoch": 21.73492671230228,
      "grad_norm": 11.299520492553711,
      "learning_rate": 3.1887561073081436e-05,
      "loss": 1.9806,
      "step": 562000
    },
    {
      "epoch": 21.738794136984183,
      "grad_norm": 12.37022876739502,
      "learning_rate": 3.188433821917985e-05,
      "loss": 1.9764,
      "step": 562100
    },
    {
      "epoch": 21.742661561666086,
      "grad_norm": 12.806134223937988,
      "learning_rate": 3.1881115365278266e-05,
      "loss": 1.9443,
      "step": 562200
    },
    {
      "epoch": 21.74652898634799,
      "grad_norm": 9.969719886779785,
      "learning_rate": 3.187789251137668e-05,
      "loss": 2.0082,
      "step": 562300
    },
    {
      "epoch": 21.750396411029897,
      "grad_norm": 12.8241605758667,
      "learning_rate": 3.187466965747509e-05,
      "loss": 1.9834,
      "step": 562400
    },
    {
      "epoch": 21.7542638357118,
      "grad_norm": 12.326242446899414,
      "learning_rate": 3.18714468035735e-05,
      "loss": 1.9597,
      "step": 562500
    },
    {
      "epoch": 21.758131260393704,
      "grad_norm": 7.744256019592285,
      "learning_rate": 3.186822394967192e-05,
      "loss": 1.8926,
      "step": 562600
    },
    {
      "epoch": 21.761998685075607,
      "grad_norm": 13.94312858581543,
      "learning_rate": 3.186500109577033e-05,
      "loss": 1.9721,
      "step": 562700
    },
    {
      "epoch": 21.765866109757514,
      "grad_norm": 10.499282836914062,
      "learning_rate": 3.186177824186874e-05,
      "loss": 1.9246,
      "step": 562800
    },
    {
      "epoch": 21.769733534439418,
      "grad_norm": 10.968173027038574,
      "learning_rate": 3.1858555387967155e-05,
      "loss": 1.9754,
      "step": 562900
    },
    {
      "epoch": 21.77360095912132,
      "grad_norm": 13.201515197753906,
      "learning_rate": 3.185533253406557e-05,
      "loss": 1.9353,
      "step": 563000
    },
    {
      "epoch": 21.777468383803225,
      "grad_norm": 8.094886779785156,
      "learning_rate": 3.1852109680163985e-05,
      "loss": 1.8968,
      "step": 563100
    },
    {
      "epoch": 21.781335808485128,
      "grad_norm": 9.65850830078125,
      "learning_rate": 3.184888682626239e-05,
      "loss": 1.8773,
      "step": 563200
    },
    {
      "epoch": 21.785203233167035,
      "grad_norm": 10.223491668701172,
      "learning_rate": 3.184566397236081e-05,
      "loss": 1.8834,
      "step": 563300
    },
    {
      "epoch": 21.78907065784894,
      "grad_norm": 12.370924949645996,
      "learning_rate": 3.184244111845922e-05,
      "loss": 1.9202,
      "step": 563400
    },
    {
      "epoch": 21.792938082530842,
      "grad_norm": 9.808614730834961,
      "learning_rate": 3.183921826455764e-05,
      "loss": 1.9137,
      "step": 563500
    },
    {
      "epoch": 21.796805507212746,
      "grad_norm": 11.4528226852417,
      "learning_rate": 3.1835995410656045e-05,
      "loss": 1.9142,
      "step": 563600
    },
    {
      "epoch": 21.800672931894653,
      "grad_norm": 11.752333641052246,
      "learning_rate": 3.183277255675446e-05,
      "loss": 1.9126,
      "step": 563700
    },
    {
      "epoch": 21.804540356576556,
      "grad_norm": 13.73262882232666,
      "learning_rate": 3.1829549702852874e-05,
      "loss": 2.0003,
      "step": 563800
    },
    {
      "epoch": 21.80840778125846,
      "grad_norm": 10.830195426940918,
      "learning_rate": 3.182632684895128e-05,
      "loss": 1.8646,
      "step": 563900
    },
    {
      "epoch": 21.812275205940363,
      "grad_norm": 9.672962188720703,
      "learning_rate": 3.18231039950497e-05,
      "loss": 1.9088,
      "step": 564000
    },
    {
      "epoch": 21.81614263062227,
      "grad_norm": 9.131632804870605,
      "learning_rate": 3.181988114114811e-05,
      "loss": 1.8778,
      "step": 564100
    },
    {
      "epoch": 21.820010055304174,
      "grad_norm": 9.366297721862793,
      "learning_rate": 3.1816658287246526e-05,
      "loss": 1.9369,
      "step": 564200
    },
    {
      "epoch": 21.823877479986077,
      "grad_norm": 12.233320236206055,
      "learning_rate": 3.1813435433344934e-05,
      "loss": 2.0946,
      "step": 564300
    },
    {
      "epoch": 21.82774490466798,
      "grad_norm": 14.790034294128418,
      "learning_rate": 3.181021257944335e-05,
      "loss": 2.0515,
      "step": 564400
    },
    {
      "epoch": 21.831612329349888,
      "grad_norm": 8.162677764892578,
      "learning_rate": 3.1806989725541764e-05,
      "loss": 2.0542,
      "step": 564500
    },
    {
      "epoch": 21.83547975403179,
      "grad_norm": 12.899685859680176,
      "learning_rate": 3.180376687164018e-05,
      "loss": 1.8944,
      "step": 564600
    },
    {
      "epoch": 21.839347178713695,
      "grad_norm": 12.868078231811523,
      "learning_rate": 3.1800544017738586e-05,
      "loss": 1.9441,
      "step": 564700
    },
    {
      "epoch": 21.843214603395598,
      "grad_norm": 19.077438354492188,
      "learning_rate": 3.1797321163837e-05,
      "loss": 1.9896,
      "step": 564800
    },
    {
      "epoch": 21.8470820280775,
      "grad_norm": 12.135356903076172,
      "learning_rate": 3.1794098309935416e-05,
      "loss": 1.961,
      "step": 564900
    },
    {
      "epoch": 21.85094945275941,
      "grad_norm": 12.940077781677246,
      "learning_rate": 3.179087545603383e-05,
      "loss": 1.9606,
      "step": 565000
    },
    {
      "epoch": 21.854816877441312,
      "grad_norm": 12.933627128601074,
      "learning_rate": 3.178765260213224e-05,
      "loss": 1.8708,
      "step": 565100
    },
    {
      "epoch": 21.858684302123216,
      "grad_norm": 9.449642181396484,
      "learning_rate": 3.178442974823065e-05,
      "loss": 1.9922,
      "step": 565200
    },
    {
      "epoch": 21.86255172680512,
      "grad_norm": 9.381365776062012,
      "learning_rate": 3.178120689432907e-05,
      "loss": 1.89,
      "step": 565300
    },
    {
      "epoch": 21.866419151487026,
      "grad_norm": 10.745489120483398,
      "learning_rate": 3.177798404042748e-05,
      "loss": 1.941,
      "step": 565400
    },
    {
      "epoch": 21.87028657616893,
      "grad_norm": 13.589143753051758,
      "learning_rate": 3.177476118652589e-05,
      "loss": 1.96,
      "step": 565500
    },
    {
      "epoch": 21.874154000850833,
      "grad_norm": 14.041296005249023,
      "learning_rate": 3.1771538332624305e-05,
      "loss": 1.9636,
      "step": 565600
    },
    {
      "epoch": 21.878021425532737,
      "grad_norm": 11.115300178527832,
      "learning_rate": 3.176831547872272e-05,
      "loss": 1.9767,
      "step": 565700
    },
    {
      "epoch": 21.881888850214644,
      "grad_norm": 10.620367050170898,
      "learning_rate": 3.1765092624821135e-05,
      "loss": 1.9887,
      "step": 565800
    },
    {
      "epoch": 21.885756274896547,
      "grad_norm": 13.781676292419434,
      "learning_rate": 3.176186977091954e-05,
      "loss": 1.8915,
      "step": 565900
    },
    {
      "epoch": 21.88962369957845,
      "grad_norm": 13.329108238220215,
      "learning_rate": 3.175864691701796e-05,
      "loss": 1.94,
      "step": 566000
    },
    {
      "epoch": 21.893491124260354,
      "grad_norm": 13.568280220031738,
      "learning_rate": 3.175542406311637e-05,
      "loss": 1.9121,
      "step": 566100
    },
    {
      "epoch": 21.897358548942258,
      "grad_norm": 10.927081108093262,
      "learning_rate": 3.175220120921479e-05,
      "loss": 2.0012,
      "step": 566200
    },
    {
      "epoch": 21.901225973624165,
      "grad_norm": 11.562010765075684,
      "learning_rate": 3.1748978355313195e-05,
      "loss": 1.9308,
      "step": 566300
    },
    {
      "epoch": 21.905093398306068,
      "grad_norm": 10.392244338989258,
      "learning_rate": 3.174575550141161e-05,
      "loss": 1.9377,
      "step": 566400
    },
    {
      "epoch": 21.90896082298797,
      "grad_norm": 12.79166316986084,
      "learning_rate": 3.1742532647510024e-05,
      "loss": 2.0005,
      "step": 566500
    },
    {
      "epoch": 21.912828247669875,
      "grad_norm": 11.923255920410156,
      "learning_rate": 3.173930979360843e-05,
      "loss": 2.012,
      "step": 566600
    },
    {
      "epoch": 21.916695672351782,
      "grad_norm": 8.954994201660156,
      "learning_rate": 3.173608693970685e-05,
      "loss": 1.8991,
      "step": 566700
    },
    {
      "epoch": 21.920563097033686,
      "grad_norm": 12.360183715820312,
      "learning_rate": 3.173286408580526e-05,
      "loss": 1.9754,
      "step": 566800
    },
    {
      "epoch": 21.92443052171559,
      "grad_norm": 15.895060539245605,
      "learning_rate": 3.1729641231903676e-05,
      "loss": 1.8925,
      "step": 566900
    },
    {
      "epoch": 21.928297946397493,
      "grad_norm": 13.60330581665039,
      "learning_rate": 3.1726418378002084e-05,
      "loss": 2.01,
      "step": 567000
    },
    {
      "epoch": 21.9321653710794,
      "grad_norm": 13.155865669250488,
      "learning_rate": 3.17231955241005e-05,
      "loss": 1.9225,
      "step": 567100
    },
    {
      "epoch": 21.936032795761303,
      "grad_norm": 12.41745662689209,
      "learning_rate": 3.1719972670198914e-05,
      "loss": 1.9235,
      "step": 567200
    },
    {
      "epoch": 21.939900220443207,
      "grad_norm": 13.171353340148926,
      "learning_rate": 3.171674981629733e-05,
      "loss": 2.0042,
      "step": 567300
    },
    {
      "epoch": 21.94376764512511,
      "grad_norm": 12.154280662536621,
      "learning_rate": 3.171352696239574e-05,
      "loss": 1.9297,
      "step": 567400
    },
    {
      "epoch": 21.947635069807017,
      "grad_norm": 13.258075714111328,
      "learning_rate": 3.171030410849416e-05,
      "loss": 1.8937,
      "step": 567500
    },
    {
      "epoch": 21.95150249448892,
      "grad_norm": 10.079212188720703,
      "learning_rate": 3.1707081254592566e-05,
      "loss": 1.878,
      "step": 567600
    },
    {
      "epoch": 21.955369919170824,
      "grad_norm": 13.268107414245605,
      "learning_rate": 3.170385840069098e-05,
      "loss": 2.0078,
      "step": 567700
    },
    {
      "epoch": 21.959237343852728,
      "grad_norm": 11.247305870056152,
      "learning_rate": 3.1700635546789395e-05,
      "loss": 1.9485,
      "step": 567800
    },
    {
      "epoch": 21.963104768534635,
      "grad_norm": 10.754036903381348,
      "learning_rate": 3.169741269288781e-05,
      "loss": 1.9551,
      "step": 567900
    },
    {
      "epoch": 21.966972193216538,
      "grad_norm": 11.957921028137207,
      "learning_rate": 3.1694189838986225e-05,
      "loss": 2.0483,
      "step": 568000
    },
    {
      "epoch": 21.97083961789844,
      "grad_norm": 10.815051078796387,
      "learning_rate": 3.169096698508463e-05,
      "loss": 2.0181,
      "step": 568100
    },
    {
      "epoch": 21.974707042580345,
      "grad_norm": 10.433394432067871,
      "learning_rate": 3.168774413118305e-05,
      "loss": 2.0313,
      "step": 568200
    },
    {
      "epoch": 21.97857446726225,
      "grad_norm": 11.349059104919434,
      "learning_rate": 3.168452127728146e-05,
      "loss": 1.9949,
      "step": 568300
    },
    {
      "epoch": 21.982441891944156,
      "grad_norm": 11.774797439575195,
      "learning_rate": 3.1681298423379877e-05,
      "loss": 1.923,
      "step": 568400
    },
    {
      "epoch": 21.98630931662606,
      "grad_norm": 10.777837753295898,
      "learning_rate": 3.167807556947829e-05,
      "loss": 1.9318,
      "step": 568500
    },
    {
      "epoch": 21.990176741307963,
      "grad_norm": 11.140510559082031,
      "learning_rate": 3.16748527155767e-05,
      "loss": 1.8997,
      "step": 568600
    },
    {
      "epoch": 21.994044165989866,
      "grad_norm": 12.824825286865234,
      "learning_rate": 3.1671629861675114e-05,
      "loss": 1.9718,
      "step": 568700
    },
    {
      "epoch": 21.997911590671773,
      "grad_norm": 16.774673461914062,
      "learning_rate": 3.166840700777353e-05,
      "loss": 1.9978,
      "step": 568800
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.8571510314941406,
      "eval_runtime": 3.1177,
      "eval_samples_per_second": 436.538,
      "eval_steps_per_second": 436.538,
      "step": 568854
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.754714012145996,
      "eval_runtime": 55.7401,
      "eval_samples_per_second": 463.885,
      "eval_steps_per_second": 463.885,
      "step": 568854
    },
    {
      "epoch": 22.001779015353677,
      "grad_norm": 10.728059768676758,
      "learning_rate": 3.1665184153871943e-05,
      "loss": 1.9327,
      "step": 568900
    },
    {
      "epoch": 22.00564644003558,
      "grad_norm": 12.2382230758667,
      "learning_rate": 3.166196129997035e-05,
      "loss": 1.8715,
      "step": 569000
    },
    {
      "epoch": 22.009513864717484,
      "grad_norm": 11.935489654541016,
      "learning_rate": 3.1658738446068766e-05,
      "loss": 1.9315,
      "step": 569100
    },
    {
      "epoch": 22.01338128939939,
      "grad_norm": 9.340638160705566,
      "learning_rate": 3.165551559216718e-05,
      "loss": 1.9255,
      "step": 569200
    },
    {
      "epoch": 22.017248714081294,
      "grad_norm": 17.888277053833008,
      "learning_rate": 3.1652292738265596e-05,
      "loss": 2.0102,
      "step": 569300
    },
    {
      "epoch": 22.021116138763198,
      "grad_norm": 11.179647445678711,
      "learning_rate": 3.1649069884364003e-05,
      "loss": 1.9767,
      "step": 569400
    },
    {
      "epoch": 22.0249835634451,
      "grad_norm": 11.776289939880371,
      "learning_rate": 3.164584703046242e-05,
      "loss": 1.9593,
      "step": 569500
    },
    {
      "epoch": 22.028850988127004,
      "grad_norm": 12.362682342529297,
      "learning_rate": 3.164262417656083e-05,
      "loss": 1.9055,
      "step": 569600
    },
    {
      "epoch": 22.03271841280891,
      "grad_norm": 14.175612449645996,
      "learning_rate": 3.163940132265924e-05,
      "loss": 1.8842,
      "step": 569700
    },
    {
      "epoch": 22.036585837490815,
      "grad_norm": 13.630487442016602,
      "learning_rate": 3.1636178468757656e-05,
      "loss": 1.9815,
      "step": 569800
    },
    {
      "epoch": 22.04045326217272,
      "grad_norm": 14.474071502685547,
      "learning_rate": 3.163295561485607e-05,
      "loss": 1.9292,
      "step": 569900
    },
    {
      "epoch": 22.044320686854622,
      "grad_norm": 12.921235084533691,
      "learning_rate": 3.1629732760954485e-05,
      "loss": 1.8949,
      "step": 570000
    },
    {
      "epoch": 22.04818811153653,
      "grad_norm": 9.237482070922852,
      "learning_rate": 3.162650990705289e-05,
      "loss": 2.0142,
      "step": 570100
    },
    {
      "epoch": 22.052055536218433,
      "grad_norm": 12.494257926940918,
      "learning_rate": 3.162328705315131e-05,
      "loss": 2.0232,
      "step": 570200
    },
    {
      "epoch": 22.055922960900336,
      "grad_norm": 11.985196113586426,
      "learning_rate": 3.162006419924972e-05,
      "loss": 1.938,
      "step": 570300
    },
    {
      "epoch": 22.05979038558224,
      "grad_norm": 12.09316349029541,
      "learning_rate": 3.161684134534814e-05,
      "loss": 1.9409,
      "step": 570400
    },
    {
      "epoch": 22.063657810264147,
      "grad_norm": 11.056905746459961,
      "learning_rate": 3.1613618491446545e-05,
      "loss": 1.861,
      "step": 570500
    },
    {
      "epoch": 22.06752523494605,
      "grad_norm": 10.46507453918457,
      "learning_rate": 3.161039563754496e-05,
      "loss": 2.0724,
      "step": 570600
    },
    {
      "epoch": 22.071392659627953,
      "grad_norm": 12.615557670593262,
      "learning_rate": 3.1607172783643374e-05,
      "loss": 1.9476,
      "step": 570700
    },
    {
      "epoch": 22.075260084309857,
      "grad_norm": 13.34173583984375,
      "learning_rate": 3.160394992974179e-05,
      "loss": 1.9314,
      "step": 570800
    },
    {
      "epoch": 22.079127508991764,
      "grad_norm": 15.425816535949707,
      "learning_rate": 3.16007270758402e-05,
      "loss": 1.957,
      "step": 570900
    },
    {
      "epoch": 22.082994933673668,
      "grad_norm": 9.13109016418457,
      "learning_rate": 3.159750422193861e-05,
      "loss": 1.9866,
      "step": 571000
    },
    {
      "epoch": 22.08686235835557,
      "grad_norm": 16.5411376953125,
      "learning_rate": 3.1594281368037027e-05,
      "loss": 1.8752,
      "step": 571100
    },
    {
      "epoch": 22.090729783037474,
      "grad_norm": 14.733259201049805,
      "learning_rate": 3.159105851413544e-05,
      "loss": 1.9018,
      "step": 571200
    },
    {
      "epoch": 22.094597207719378,
      "grad_norm": 8.193763732910156,
      "learning_rate": 3.158783566023385e-05,
      "loss": 1.8903,
      "step": 571300
    },
    {
      "epoch": 22.098464632401285,
      "grad_norm": 12.091109275817871,
      "learning_rate": 3.1584612806332264e-05,
      "loss": 2.0057,
      "step": 571400
    },
    {
      "epoch": 22.10233205708319,
      "grad_norm": 10.852288246154785,
      "learning_rate": 3.158138995243068e-05,
      "loss": 1.8073,
      "step": 571500
    },
    {
      "epoch": 22.106199481765092,
      "grad_norm": 10.377897262573242,
      "learning_rate": 3.1578167098529093e-05,
      "loss": 2.0272,
      "step": 571600
    },
    {
      "epoch": 22.110066906446995,
      "grad_norm": 9.411834716796875,
      "learning_rate": 3.15749442446275e-05,
      "loss": 1.9013,
      "step": 571700
    },
    {
      "epoch": 22.113934331128902,
      "grad_norm": 10.908102989196777,
      "learning_rate": 3.1571721390725916e-05,
      "loss": 1.9993,
      "step": 571800
    },
    {
      "epoch": 22.117801755810806,
      "grad_norm": 9.942466735839844,
      "learning_rate": 3.156849853682433e-05,
      "loss": 1.9175,
      "step": 571900
    },
    {
      "epoch": 22.12166918049271,
      "grad_norm": 9.827316284179688,
      "learning_rate": 3.1565275682922745e-05,
      "loss": 1.971,
      "step": 572000
    },
    {
      "epoch": 22.125536605174613,
      "grad_norm": 10.737100601196289,
      "learning_rate": 3.1562052829021153e-05,
      "loss": 1.9673,
      "step": 572100
    },
    {
      "epoch": 22.12940402985652,
      "grad_norm": 11.817700386047363,
      "learning_rate": 3.155882997511957e-05,
      "loss": 1.8903,
      "step": 572200
    },
    {
      "epoch": 22.133271454538423,
      "grad_norm": 9.258076667785645,
      "learning_rate": 3.155560712121798e-05,
      "loss": 1.9136,
      "step": 572300
    },
    {
      "epoch": 22.137138879220327,
      "grad_norm": 10.360185623168945,
      "learning_rate": 3.15523842673164e-05,
      "loss": 1.9609,
      "step": 572400
    },
    {
      "epoch": 22.14100630390223,
      "grad_norm": 12.558355331420898,
      "learning_rate": 3.1549161413414806e-05,
      "loss": 1.8509,
      "step": 572500
    },
    {
      "epoch": 22.144873728584137,
      "grad_norm": 10.12084674835205,
      "learning_rate": 3.154593855951322e-05,
      "loss": 1.8983,
      "step": 572600
    },
    {
      "epoch": 22.14874115326604,
      "grad_norm": 9.847100257873535,
      "learning_rate": 3.1542715705611635e-05,
      "loss": 2.0438,
      "step": 572700
    },
    {
      "epoch": 22.152608577947944,
      "grad_norm": 11.1553373336792,
      "learning_rate": 3.153949285171004e-05,
      "loss": 1.8919,
      "step": 572800
    },
    {
      "epoch": 22.156476002629848,
      "grad_norm": 11.373212814331055,
      "learning_rate": 3.153626999780846e-05,
      "loss": 1.9208,
      "step": 572900
    },
    {
      "epoch": 22.16034342731175,
      "grad_norm": 11.876757621765137,
      "learning_rate": 3.153304714390687e-05,
      "loss": 1.9287,
      "step": 573000
    },
    {
      "epoch": 22.16421085199366,
      "grad_norm": 8.821236610412598,
      "learning_rate": 3.152982429000529e-05,
      "loss": 1.9737,
      "step": 573100
    },
    {
      "epoch": 22.168078276675562,
      "grad_norm": 15.79629135131836,
      "learning_rate": 3.1526601436103695e-05,
      "loss": 1.8953,
      "step": 573200
    },
    {
      "epoch": 22.171945701357465,
      "grad_norm": 11.30783748626709,
      "learning_rate": 3.152337858220211e-05,
      "loss": 2.0344,
      "step": 573300
    },
    {
      "epoch": 22.17581312603937,
      "grad_norm": 11.548101425170898,
      "learning_rate": 3.1520155728300524e-05,
      "loss": 1.8698,
      "step": 573400
    },
    {
      "epoch": 22.179680550721276,
      "grad_norm": 12.714119911193848,
      "learning_rate": 3.151693287439894e-05,
      "loss": 1.9288,
      "step": 573500
    },
    {
      "epoch": 22.18354797540318,
      "grad_norm": 9.656938552856445,
      "learning_rate": 3.151371002049735e-05,
      "loss": 1.8987,
      "step": 573600
    },
    {
      "epoch": 22.187415400085083,
      "grad_norm": 9.39018726348877,
      "learning_rate": 3.151048716659576e-05,
      "loss": 1.8571,
      "step": 573700
    },
    {
      "epoch": 22.191282824766986,
      "grad_norm": 12.173136711120605,
      "learning_rate": 3.1507264312694177e-05,
      "loss": 2.0312,
      "step": 573800
    },
    {
      "epoch": 22.195150249448893,
      "grad_norm": 11.140910148620605,
      "learning_rate": 3.150404145879259e-05,
      "loss": 1.9675,
      "step": 573900
    },
    {
      "epoch": 22.199017674130797,
      "grad_norm": 13.798931121826172,
      "learning_rate": 3.1500818604891e-05,
      "loss": 1.8538,
      "step": 574000
    },
    {
      "epoch": 22.2028850988127,
      "grad_norm": 10.528294563293457,
      "learning_rate": 3.1497595750989414e-05,
      "loss": 1.8749,
      "step": 574100
    },
    {
      "epoch": 22.206752523494604,
      "grad_norm": 11.905989646911621,
      "learning_rate": 3.149437289708783e-05,
      "loss": 1.8587,
      "step": 574200
    },
    {
      "epoch": 22.21061994817651,
      "grad_norm": 11.117085456848145,
      "learning_rate": 3.149115004318624e-05,
      "loss": 1.9907,
      "step": 574300
    },
    {
      "epoch": 22.214487372858414,
      "grad_norm": 12.195898056030273,
      "learning_rate": 3.148792718928466e-05,
      "loss": 1.9517,
      "step": 574400
    },
    {
      "epoch": 22.218354797540318,
      "grad_norm": 7.867995262145996,
      "learning_rate": 3.148470433538307e-05,
      "loss": 1.9783,
      "step": 574500
    },
    {
      "epoch": 22.22222222222222,
      "grad_norm": 12.369136810302734,
      "learning_rate": 3.148148148148148e-05,
      "loss": 1.8704,
      "step": 574600
    },
    {
      "epoch": 22.226089646904125,
      "grad_norm": 11.957169532775879,
      "learning_rate": 3.1478258627579895e-05,
      "loss": 2.0427,
      "step": 574700
    },
    {
      "epoch": 22.229957071586032,
      "grad_norm": 10.88159465789795,
      "learning_rate": 3.147503577367831e-05,
      "loss": 1.9099,
      "step": 574800
    },
    {
      "epoch": 22.233824496267935,
      "grad_norm": 11.810711860656738,
      "learning_rate": 3.1471812919776725e-05,
      "loss": 1.922,
      "step": 574900
    },
    {
      "epoch": 22.23769192094984,
      "grad_norm": 10.7093505859375,
      "learning_rate": 3.146859006587514e-05,
      "loss": 1.8968,
      "step": 575000
    },
    {
      "epoch": 22.241559345631742,
      "grad_norm": 14.470551490783691,
      "learning_rate": 3.1465367211973554e-05,
      "loss": 1.9274,
      "step": 575100
    },
    {
      "epoch": 22.24542677031365,
      "grad_norm": 10.159784317016602,
      "learning_rate": 3.146214435807196e-05,
      "loss": 1.9346,
      "step": 575200
    },
    {
      "epoch": 22.249294194995553,
      "grad_norm": 12.754447937011719,
      "learning_rate": 3.145892150417038e-05,
      "loss": 1.8145,
      "step": 575300
    },
    {
      "epoch": 22.253161619677456,
      "grad_norm": 11.153693199157715,
      "learning_rate": 3.145569865026879e-05,
      "loss": 1.8457,
      "step": 575400
    },
    {
      "epoch": 22.25702904435936,
      "grad_norm": 9.685009002685547,
      "learning_rate": 3.14524757963672e-05,
      "loss": 1.9585,
      "step": 575500
    },
    {
      "epoch": 22.260896469041267,
      "grad_norm": 11.616397857666016,
      "learning_rate": 3.1449252942465614e-05,
      "loss": 1.9572,
      "step": 575600
    },
    {
      "epoch": 22.26476389372317,
      "grad_norm": 10.697144508361816,
      "learning_rate": 3.144603008856403e-05,
      "loss": 1.9672,
      "step": 575700
    },
    {
      "epoch": 22.268631318405074,
      "grad_norm": 10.400172233581543,
      "learning_rate": 3.1442807234662444e-05,
      "loss": 1.8791,
      "step": 575800
    },
    {
      "epoch": 22.272498743086977,
      "grad_norm": 12.627497673034668,
      "learning_rate": 3.143958438076085e-05,
      "loss": 1.8961,
      "step": 575900
    },
    {
      "epoch": 22.276366167768884,
      "grad_norm": 10.607818603515625,
      "learning_rate": 3.1436361526859266e-05,
      "loss": 1.9188,
      "step": 576000
    },
    {
      "epoch": 22.280233592450788,
      "grad_norm": 10.135842323303223,
      "learning_rate": 3.143313867295768e-05,
      "loss": 1.9054,
      "step": 576100
    },
    {
      "epoch": 22.28410101713269,
      "grad_norm": 10.828940391540527,
      "learning_rate": 3.1429915819056096e-05,
      "loss": 1.85,
      "step": 576200
    },
    {
      "epoch": 22.287968441814595,
      "grad_norm": 12.22252368927002,
      "learning_rate": 3.1426692965154504e-05,
      "loss": 1.9219,
      "step": 576300
    },
    {
      "epoch": 22.2918358664965,
      "grad_norm": 9.605043411254883,
      "learning_rate": 3.142347011125292e-05,
      "loss": 1.8853,
      "step": 576400
    },
    {
      "epoch": 22.295703291178405,
      "grad_norm": 11.173370361328125,
      "learning_rate": 3.142024725735133e-05,
      "loss": 1.9479,
      "step": 576500
    },
    {
      "epoch": 22.29957071586031,
      "grad_norm": 10.848770141601562,
      "learning_rate": 3.141702440344975e-05,
      "loss": 1.8921,
      "step": 576600
    },
    {
      "epoch": 22.303438140542212,
      "grad_norm": 14.316408157348633,
      "learning_rate": 3.1413801549548156e-05,
      "loss": 1.895,
      "step": 576700
    },
    {
      "epoch": 22.307305565224116,
      "grad_norm": 14.218815803527832,
      "learning_rate": 3.141057869564657e-05,
      "loss": 1.9564,
      "step": 576800
    },
    {
      "epoch": 22.311172989906023,
      "grad_norm": 10.239666938781738,
      "learning_rate": 3.1407355841744985e-05,
      "loss": 1.9072,
      "step": 576900
    },
    {
      "epoch": 22.315040414587926,
      "grad_norm": 10.594828605651855,
      "learning_rate": 3.14041329878434e-05,
      "loss": 1.9319,
      "step": 577000
    },
    {
      "epoch": 22.31890783926983,
      "grad_norm": 9.745281219482422,
      "learning_rate": 3.140091013394181e-05,
      "loss": 1.9448,
      "step": 577100
    },
    {
      "epoch": 22.322775263951733,
      "grad_norm": 10.4877290725708,
      "learning_rate": 3.139768728004022e-05,
      "loss": 1.8845,
      "step": 577200
    },
    {
      "epoch": 22.32664268863364,
      "grad_norm": 12.093071937561035,
      "learning_rate": 3.139446442613864e-05,
      "loss": 1.9332,
      "step": 577300
    },
    {
      "epoch": 22.330510113315544,
      "grad_norm": 15.306307792663574,
      "learning_rate": 3.139124157223705e-05,
      "loss": 2.0131,
      "step": 577400
    },
    {
      "epoch": 22.334377537997447,
      "grad_norm": 14.103626251220703,
      "learning_rate": 3.138801871833546e-05,
      "loss": 1.8891,
      "step": 577500
    },
    {
      "epoch": 22.33824496267935,
      "grad_norm": 10.430315971374512,
      "learning_rate": 3.1384795864433875e-05,
      "loss": 1.9385,
      "step": 577600
    },
    {
      "epoch": 22.342112387361254,
      "grad_norm": 13.147355079650879,
      "learning_rate": 3.138157301053229e-05,
      "loss": 1.8696,
      "step": 577700
    },
    {
      "epoch": 22.34597981204316,
      "grad_norm": 10.313250541687012,
      "learning_rate": 3.1378350156630704e-05,
      "loss": 2.0189,
      "step": 577800
    },
    {
      "epoch": 22.349847236725065,
      "grad_norm": 11.13047981262207,
      "learning_rate": 3.137512730272911e-05,
      "loss": 1.9558,
      "step": 577900
    },
    {
      "epoch": 22.35371466140697,
      "grad_norm": 12.651592254638672,
      "learning_rate": 3.137190444882753e-05,
      "loss": 1.9553,
      "step": 578000
    },
    {
      "epoch": 22.357582086088872,
      "grad_norm": 12.148755073547363,
      "learning_rate": 3.136868159492594e-05,
      "loss": 1.8731,
      "step": 578100
    },
    {
      "epoch": 22.36144951077078,
      "grad_norm": 12.289462089538574,
      "learning_rate": 3.1365458741024356e-05,
      "loss": 2.0438,
      "step": 578200
    },
    {
      "epoch": 22.365316935452682,
      "grad_norm": 13.420547485351562,
      "learning_rate": 3.1362235887122764e-05,
      "loss": 1.9706,
      "step": 578300
    },
    {
      "epoch": 22.369184360134586,
      "grad_norm": 12.697436332702637,
      "learning_rate": 3.135901303322118e-05,
      "loss": 1.8593,
      "step": 578400
    },
    {
      "epoch": 22.37305178481649,
      "grad_norm": 11.05574893951416,
      "learning_rate": 3.1355790179319594e-05,
      "loss": 2.0011,
      "step": 578500
    },
    {
      "epoch": 22.376919209498396,
      "grad_norm": 11.94681453704834,
      "learning_rate": 3.1352567325418e-05,
      "loss": 1.8587,
      "step": 578600
    },
    {
      "epoch": 22.3807866341803,
      "grad_norm": 15.754256248474121,
      "learning_rate": 3.1349344471516416e-05,
      "loss": 1.8587,
      "step": 578700
    },
    {
      "epoch": 22.384654058862203,
      "grad_norm": 12.887615203857422,
      "learning_rate": 3.134612161761483e-05,
      "loss": 1.9331,
      "step": 578800
    },
    {
      "epoch": 22.388521483544107,
      "grad_norm": 10.079483032226562,
      "learning_rate": 3.1342898763713246e-05,
      "loss": 1.9523,
      "step": 578900
    },
    {
      "epoch": 22.392388908226014,
      "grad_norm": 9.677968978881836,
      "learning_rate": 3.1339675909811654e-05,
      "loss": 2.0075,
      "step": 579000
    },
    {
      "epoch": 22.396256332907917,
      "grad_norm": 13.225032806396484,
      "learning_rate": 3.133645305591007e-05,
      "loss": 1.9212,
      "step": 579100
    },
    {
      "epoch": 22.40012375758982,
      "grad_norm": 10.3654146194458,
      "learning_rate": 3.133323020200848e-05,
      "loss": 1.9892,
      "step": 579200
    },
    {
      "epoch": 22.403991182271724,
      "grad_norm": 11.723320960998535,
      "learning_rate": 3.13300073481069e-05,
      "loss": 1.9364,
      "step": 579300
    },
    {
      "epoch": 22.407858606953628,
      "grad_norm": 11.431703567504883,
      "learning_rate": 3.1326784494205306e-05,
      "loss": 1.9677,
      "step": 579400
    },
    {
      "epoch": 22.411726031635535,
      "grad_norm": 15.630956649780273,
      "learning_rate": 3.132356164030372e-05,
      "loss": 1.9996,
      "step": 579500
    },
    {
      "epoch": 22.41559345631744,
      "grad_norm": 12.253894805908203,
      "learning_rate": 3.1320338786402135e-05,
      "loss": 1.8834,
      "step": 579600
    },
    {
      "epoch": 22.419460880999342,
      "grad_norm": 11.686810493469238,
      "learning_rate": 3.131711593250055e-05,
      "loss": 1.8687,
      "step": 579700
    },
    {
      "epoch": 22.423328305681245,
      "grad_norm": 14.825596809387207,
      "learning_rate": 3.131389307859896e-05,
      "loss": 1.8958,
      "step": 579800
    },
    {
      "epoch": 22.427195730363152,
      "grad_norm": 10.644143104553223,
      "learning_rate": 3.131067022469737e-05,
      "loss": 1.9451,
      "step": 579900
    },
    {
      "epoch": 22.431063155045056,
      "grad_norm": 10.74345588684082,
      "learning_rate": 3.130744737079579e-05,
      "loss": 1.9811,
      "step": 580000
    },
    {
      "epoch": 22.43493057972696,
      "grad_norm": 9.874383926391602,
      "learning_rate": 3.13042245168942e-05,
      "loss": 1.9341,
      "step": 580100
    },
    {
      "epoch": 22.438798004408863,
      "grad_norm": 12.901958465576172,
      "learning_rate": 3.130100166299261e-05,
      "loss": 2.0595,
      "step": 580200
    },
    {
      "epoch": 22.44266542909077,
      "grad_norm": 10.673757553100586,
      "learning_rate": 3.1297778809091025e-05,
      "loss": 1.9091,
      "step": 580300
    },
    {
      "epoch": 22.446532853772673,
      "grad_norm": 11.409587860107422,
      "learning_rate": 3.129455595518944e-05,
      "loss": 1.9212,
      "step": 580400
    },
    {
      "epoch": 22.450400278454577,
      "grad_norm": 11.889755249023438,
      "learning_rate": 3.1291333101287854e-05,
      "loss": 2.0535,
      "step": 580500
    },
    {
      "epoch": 22.45426770313648,
      "grad_norm": 12.433993339538574,
      "learning_rate": 3.128811024738626e-05,
      "loss": 1.9412,
      "step": 580600
    },
    {
      "epoch": 22.458135127818387,
      "grad_norm": 11.448529243469238,
      "learning_rate": 3.128488739348468e-05,
      "loss": 2.0124,
      "step": 580700
    },
    {
      "epoch": 22.46200255250029,
      "grad_norm": 9.937092781066895,
      "learning_rate": 3.128166453958309e-05,
      "loss": 2.0386,
      "step": 580800
    },
    {
      "epoch": 22.465869977182194,
      "grad_norm": 9.550670623779297,
      "learning_rate": 3.1278441685681506e-05,
      "loss": 1.8971,
      "step": 580900
    },
    {
      "epoch": 22.469737401864098,
      "grad_norm": 11.18630313873291,
      "learning_rate": 3.127521883177992e-05,
      "loss": 2.0104,
      "step": 581000
    },
    {
      "epoch": 22.473604826546,
      "grad_norm": 12.522377014160156,
      "learning_rate": 3.127199597787833e-05,
      "loss": 1.9676,
      "step": 581100
    },
    {
      "epoch": 22.47747225122791,
      "grad_norm": 10.376226425170898,
      "learning_rate": 3.1268773123976744e-05,
      "loss": 1.9972,
      "step": 581200
    },
    {
      "epoch": 22.481339675909812,
      "grad_norm": 13.674899101257324,
      "learning_rate": 3.126555027007516e-05,
      "loss": 1.8516,
      "step": 581300
    },
    {
      "epoch": 22.485207100591715,
      "grad_norm": 12.690074920654297,
      "learning_rate": 3.126232741617357e-05,
      "loss": 2.0019,
      "step": 581400
    },
    {
      "epoch": 22.48907452527362,
      "grad_norm": 11.605311393737793,
      "learning_rate": 3.125910456227199e-05,
      "loss": 1.9485,
      "step": 581500
    },
    {
      "epoch": 22.492941949955526,
      "grad_norm": 11.419500350952148,
      "learning_rate": 3.1255881708370396e-05,
      "loss": 1.9724,
      "step": 581600
    },
    {
      "epoch": 22.49680937463743,
      "grad_norm": 10.19396686553955,
      "learning_rate": 3.125265885446881e-05,
      "loss": 1.9626,
      "step": 581700
    },
    {
      "epoch": 22.500676799319333,
      "grad_norm": 12.549864768981934,
      "learning_rate": 3.1249436000567225e-05,
      "loss": 1.9227,
      "step": 581800
    },
    {
      "epoch": 22.504544224001236,
      "grad_norm": 11.657898902893066,
      "learning_rate": 3.124621314666564e-05,
      "loss": 1.9614,
      "step": 581900
    },
    {
      "epoch": 22.508411648683143,
      "grad_norm": 11.92011547088623,
      "learning_rate": 3.1242990292764055e-05,
      "loss": 1.9735,
      "step": 582000
    },
    {
      "epoch": 22.512279073365047,
      "grad_norm": 9.054399490356445,
      "learning_rate": 3.123976743886246e-05,
      "loss": 1.9016,
      "step": 582100
    },
    {
      "epoch": 22.51614649804695,
      "grad_norm": 9.682090759277344,
      "learning_rate": 3.123654458496088e-05,
      "loss": 1.9413,
      "step": 582200
    },
    {
      "epoch": 22.520013922728854,
      "grad_norm": 11.683586120605469,
      "learning_rate": 3.123332173105929e-05,
      "loss": 1.9499,
      "step": 582300
    },
    {
      "epoch": 22.52388134741076,
      "grad_norm": 11.670284271240234,
      "learning_rate": 3.123009887715771e-05,
      "loss": 1.968,
      "step": 582400
    },
    {
      "epoch": 22.527748772092664,
      "grad_norm": 14.613905906677246,
      "learning_rate": 3.1226876023256115e-05,
      "loss": 1.9319,
      "step": 582500
    },
    {
      "epoch": 22.531616196774568,
      "grad_norm": 10.383652687072754,
      "learning_rate": 3.122365316935453e-05,
      "loss": 1.9462,
      "step": 582600
    },
    {
      "epoch": 22.53548362145647,
      "grad_norm": 11.507166862487793,
      "learning_rate": 3.1220430315452944e-05,
      "loss": 1.9235,
      "step": 582700
    },
    {
      "epoch": 22.539351046138375,
      "grad_norm": 10.094585418701172,
      "learning_rate": 3.121720746155136e-05,
      "loss": 1.9141,
      "step": 582800
    },
    {
      "epoch": 22.543218470820282,
      "grad_norm": 12.303718566894531,
      "learning_rate": 3.121398460764977e-05,
      "loss": 2.0243,
      "step": 582900
    },
    {
      "epoch": 22.547085895502185,
      "grad_norm": 16.412473678588867,
      "learning_rate": 3.121076175374818e-05,
      "loss": 1.969,
      "step": 583000
    },
    {
      "epoch": 22.55095332018409,
      "grad_norm": 10.416237831115723,
      "learning_rate": 3.1207538899846596e-05,
      "loss": 1.96,
      "step": 583100
    },
    {
      "epoch": 22.554820744865992,
      "grad_norm": 10.996378898620605,
      "learning_rate": 3.120431604594501e-05,
      "loss": 1.875,
      "step": 583200
    },
    {
      "epoch": 22.5586881695479,
      "grad_norm": 10.729705810546875,
      "learning_rate": 3.120109319204342e-05,
      "loss": 1.9237,
      "step": 583300
    },
    {
      "epoch": 22.562555594229803,
      "grad_norm": 10.133386611938477,
      "learning_rate": 3.1197870338141834e-05,
      "loss": 1.8722,
      "step": 583400
    },
    {
      "epoch": 22.566423018911706,
      "grad_norm": 13.186906814575195,
      "learning_rate": 3.119464748424025e-05,
      "loss": 1.9818,
      "step": 583500
    },
    {
      "epoch": 22.57029044359361,
      "grad_norm": 10.991201400756836,
      "learning_rate": 3.119142463033866e-05,
      "loss": 1.9801,
      "step": 583600
    },
    {
      "epoch": 22.574157868275517,
      "grad_norm": 9.83256721496582,
      "learning_rate": 3.118820177643707e-05,
      "loss": 1.9507,
      "step": 583700
    },
    {
      "epoch": 22.57802529295742,
      "grad_norm": 12.501453399658203,
      "learning_rate": 3.1184978922535486e-05,
      "loss": 1.8864,
      "step": 583800
    },
    {
      "epoch": 22.581892717639324,
      "grad_norm": 6.909158706665039,
      "learning_rate": 3.11817560686339e-05,
      "loss": 1.8955,
      "step": 583900
    },
    {
      "epoch": 22.585760142321227,
      "grad_norm": 10.455395698547363,
      "learning_rate": 3.1178533214732315e-05,
      "loss": 2.0106,
      "step": 584000
    },
    {
      "epoch": 22.589627567003134,
      "grad_norm": 12.25067138671875,
      "learning_rate": 3.117531036083072e-05,
      "loss": 1.865,
      "step": 584100
    },
    {
      "epoch": 22.593494991685038,
      "grad_norm": 11.2021484375,
      "learning_rate": 3.117208750692914e-05,
      "loss": 1.9792,
      "step": 584200
    },
    {
      "epoch": 22.59736241636694,
      "grad_norm": 11.901408195495605,
      "learning_rate": 3.116886465302755e-05,
      "loss": 2.0222,
      "step": 584300
    },
    {
      "epoch": 22.601229841048845,
      "grad_norm": 15.457164764404297,
      "learning_rate": 3.116564179912596e-05,
      "loss": 1.9328,
      "step": 584400
    },
    {
      "epoch": 22.60509726573075,
      "grad_norm": 13.163129806518555,
      "learning_rate": 3.1162418945224375e-05,
      "loss": 2.0092,
      "step": 584500
    },
    {
      "epoch": 22.608964690412655,
      "grad_norm": 10.902183532714844,
      "learning_rate": 3.115919609132279e-05,
      "loss": 2.0722,
      "step": 584600
    },
    {
      "epoch": 22.61283211509456,
      "grad_norm": 13.956937789916992,
      "learning_rate": 3.1155973237421205e-05,
      "loss": 1.9697,
      "step": 584700
    },
    {
      "epoch": 22.616699539776462,
      "grad_norm": 16.73821449279785,
      "learning_rate": 3.115275038351961e-05,
      "loss": 1.8985,
      "step": 584800
    },
    {
      "epoch": 22.620566964458366,
      "grad_norm": 13.584456443786621,
      "learning_rate": 3.114952752961803e-05,
      "loss": 1.9259,
      "step": 584900
    },
    {
      "epoch": 22.624434389140273,
      "grad_norm": 13.22336196899414,
      "learning_rate": 3.114630467571644e-05,
      "loss": 1.9315,
      "step": 585000
    },
    {
      "epoch": 22.628301813822176,
      "grad_norm": 12.552691459655762,
      "learning_rate": 3.114308182181486e-05,
      "loss": 1.9412,
      "step": 585100
    },
    {
      "epoch": 22.63216923850408,
      "grad_norm": 13.727254867553711,
      "learning_rate": 3.1139858967913265e-05,
      "loss": 1.9073,
      "step": 585200
    },
    {
      "epoch": 22.636036663185983,
      "grad_norm": 11.179430961608887,
      "learning_rate": 3.113663611401168e-05,
      "loss": 1.9395,
      "step": 585300
    },
    {
      "epoch": 22.63990408786789,
      "grad_norm": 12.391655921936035,
      "learning_rate": 3.1133413260110094e-05,
      "loss": 1.9973,
      "step": 585400
    },
    {
      "epoch": 22.643771512549794,
      "grad_norm": 13.301309585571289,
      "learning_rate": 3.113019040620851e-05,
      "loss": 1.9769,
      "step": 585500
    },
    {
      "epoch": 22.647638937231697,
      "grad_norm": 11.756794929504395,
      "learning_rate": 3.112696755230692e-05,
      "loss": 1.8579,
      "step": 585600
    },
    {
      "epoch": 22.6515063619136,
      "grad_norm": 10.269189834594727,
      "learning_rate": 3.112374469840533e-05,
      "loss": 1.9795,
      "step": 585700
    },
    {
      "epoch": 22.655373786595504,
      "grad_norm": 12.618172645568848,
      "learning_rate": 3.1120521844503746e-05,
      "loss": 1.9821,
      "step": 585800
    },
    {
      "epoch": 22.65924121127741,
      "grad_norm": 8.464500427246094,
      "learning_rate": 3.111729899060216e-05,
      "loss": 1.8898,
      "step": 585900
    },
    {
      "epoch": 22.663108635959315,
      "grad_norm": 12.737760543823242,
      "learning_rate": 3.111407613670057e-05,
      "loss": 1.9341,
      "step": 586000
    },
    {
      "epoch": 22.66697606064122,
      "grad_norm": 11.272001266479492,
      "learning_rate": 3.1110853282798984e-05,
      "loss": 1.9025,
      "step": 586100
    },
    {
      "epoch": 22.67084348532312,
      "grad_norm": 12.907387733459473,
      "learning_rate": 3.11076304288974e-05,
      "loss": 2.0486,
      "step": 586200
    },
    {
      "epoch": 22.67471091000503,
      "grad_norm": 10.472944259643555,
      "learning_rate": 3.110440757499581e-05,
      "loss": 1.8785,
      "step": 586300
    },
    {
      "epoch": 22.678578334686932,
      "grad_norm": 11.377880096435547,
      "learning_rate": 3.110118472109422e-05,
      "loss": 1.9276,
      "step": 586400
    },
    {
      "epoch": 22.682445759368836,
      "grad_norm": 11.868687629699707,
      "learning_rate": 3.1097961867192636e-05,
      "loss": 1.9098,
      "step": 586500
    },
    {
      "epoch": 22.68631318405074,
      "grad_norm": 11.601211547851562,
      "learning_rate": 3.109473901329105e-05,
      "loss": 1.9788,
      "step": 586600
    },
    {
      "epoch": 22.690180608732646,
      "grad_norm": 9.935133934020996,
      "learning_rate": 3.1091516159389465e-05,
      "loss": 1.9033,
      "step": 586700
    },
    {
      "epoch": 22.69404803341455,
      "grad_norm": 10.45801067352295,
      "learning_rate": 3.108829330548787e-05,
      "loss": 1.9131,
      "step": 586800
    },
    {
      "epoch": 22.697915458096453,
      "grad_norm": 13.244763374328613,
      "learning_rate": 3.108507045158629e-05,
      "loss": 1.9693,
      "step": 586900
    },
    {
      "epoch": 22.701782882778357,
      "grad_norm": 10.905937194824219,
      "learning_rate": 3.10818475976847e-05,
      "loss": 1.8748,
      "step": 587000
    },
    {
      "epoch": 22.705650307460264,
      "grad_norm": 11.626962661743164,
      "learning_rate": 3.107862474378312e-05,
      "loss": 1.8861,
      "step": 587100
    },
    {
      "epoch": 22.709517732142167,
      "grad_norm": 11.628402709960938,
      "learning_rate": 3.1075401889881525e-05,
      "loss": 2.0433,
      "step": 587200
    },
    {
      "epoch": 22.71338515682407,
      "grad_norm": 13.117419242858887,
      "learning_rate": 3.107217903597994e-05,
      "loss": 1.8644,
      "step": 587300
    },
    {
      "epoch": 22.717252581505974,
      "grad_norm": 11.97436237335205,
      "learning_rate": 3.1068956182078355e-05,
      "loss": 1.9312,
      "step": 587400
    },
    {
      "epoch": 22.72112000618788,
      "grad_norm": 13.933368682861328,
      "learning_rate": 3.106573332817677e-05,
      "loss": 1.9788,
      "step": 587500
    },
    {
      "epoch": 22.724987430869785,
      "grad_norm": 10.654586791992188,
      "learning_rate": 3.106251047427518e-05,
      "loss": 1.932,
      "step": 587600
    },
    {
      "epoch": 22.72885485555169,
      "grad_norm": 13.055471420288086,
      "learning_rate": 3.105928762037359e-05,
      "loss": 1.9046,
      "step": 587700
    },
    {
      "epoch": 22.73272228023359,
      "grad_norm": 10.847041130065918,
      "learning_rate": 3.105606476647201e-05,
      "loss": 2.0304,
      "step": 587800
    },
    {
      "epoch": 22.736589704915495,
      "grad_norm": 7.088468551635742,
      "learning_rate": 3.105284191257042e-05,
      "loss": 1.9012,
      "step": 587900
    },
    {
      "epoch": 22.740457129597402,
      "grad_norm": 11.623092651367188,
      "learning_rate": 3.1049619058668836e-05,
      "loss": 1.9627,
      "step": 588000
    },
    {
      "epoch": 22.744324554279306,
      "grad_norm": 12.090855598449707,
      "learning_rate": 3.1046396204767244e-05,
      "loss": 1.9977,
      "step": 588100
    },
    {
      "epoch": 22.74819197896121,
      "grad_norm": 11.42138385772705,
      "learning_rate": 3.104317335086566e-05,
      "loss": 1.9995,
      "step": 588200
    },
    {
      "epoch": 22.752059403643113,
      "grad_norm": 16.370378494262695,
      "learning_rate": 3.1039950496964074e-05,
      "loss": 2.0401,
      "step": 588300
    },
    {
      "epoch": 22.75592682832502,
      "grad_norm": 10.338757514953613,
      "learning_rate": 3.103672764306249e-05,
      "loss": 1.8887,
      "step": 588400
    },
    {
      "epoch": 22.759794253006923,
      "grad_norm": 12.636094093322754,
      "learning_rate": 3.10335047891609e-05,
      "loss": 1.9548,
      "step": 588500
    },
    {
      "epoch": 22.763661677688827,
      "grad_norm": 10.094388008117676,
      "learning_rate": 3.103028193525932e-05,
      "loss": 1.9847,
      "step": 588600
    },
    {
      "epoch": 22.76752910237073,
      "grad_norm": 13.419581413269043,
      "learning_rate": 3.1027059081357726e-05,
      "loss": 1.9785,
      "step": 588700
    },
    {
      "epoch": 22.771396527052637,
      "grad_norm": 14.306370735168457,
      "learning_rate": 3.102383622745614e-05,
      "loss": 1.9272,
      "step": 588800
    },
    {
      "epoch": 22.77526395173454,
      "grad_norm": 11.816031455993652,
      "learning_rate": 3.1020613373554555e-05,
      "loss": 1.9405,
      "step": 588900
    },
    {
      "epoch": 22.779131376416444,
      "grad_norm": 15.289453506469727,
      "learning_rate": 3.101739051965297e-05,
      "loss": 1.9523,
      "step": 589000
    },
    {
      "epoch": 22.782998801098348,
      "grad_norm": 13.136404991149902,
      "learning_rate": 3.101416766575138e-05,
      "loss": 1.9867,
      "step": 589100
    },
    {
      "epoch": 22.78686622578025,
      "grad_norm": 12.334575653076172,
      "learning_rate": 3.101094481184979e-05,
      "loss": 1.9056,
      "step": 589200
    },
    {
      "epoch": 22.79073365046216,
      "grad_norm": 13.033491134643555,
      "learning_rate": 3.100772195794821e-05,
      "loss": 1.9959,
      "step": 589300
    },
    {
      "epoch": 22.79460107514406,
      "grad_norm": 9.516260147094727,
      "learning_rate": 3.100449910404662e-05,
      "loss": 1.8938,
      "step": 589400
    },
    {
      "epoch": 22.798468499825965,
      "grad_norm": 11.786172866821289,
      "learning_rate": 3.100127625014503e-05,
      "loss": 1.9234,
      "step": 589500
    },
    {
      "epoch": 22.80233592450787,
      "grad_norm": 10.408788681030273,
      "learning_rate": 3.0998053396243445e-05,
      "loss": 1.9235,
      "step": 589600
    },
    {
      "epoch": 22.806203349189776,
      "grad_norm": 10.622430801391602,
      "learning_rate": 3.099483054234186e-05,
      "loss": 1.8973,
      "step": 589700
    },
    {
      "epoch": 22.81007077387168,
      "grad_norm": 11.273124694824219,
      "learning_rate": 3.0991607688440274e-05,
      "loss": 1.9402,
      "step": 589800
    },
    {
      "epoch": 22.813938198553583,
      "grad_norm": 13.090264320373535,
      "learning_rate": 3.098838483453868e-05,
      "loss": 1.9031,
      "step": 589900
    },
    {
      "epoch": 22.817805623235486,
      "grad_norm": 17.7367000579834,
      "learning_rate": 3.09851619806371e-05,
      "loss": 1.9352,
      "step": 590000
    },
    {
      "epoch": 22.821673047917393,
      "grad_norm": 15.928621292114258,
      "learning_rate": 3.098193912673551e-05,
      "loss": 2.0868,
      "step": 590100
    },
    {
      "epoch": 22.825540472599297,
      "grad_norm": 12.780375480651855,
      "learning_rate": 3.097871627283392e-05,
      "loss": 1.9278,
      "step": 590200
    },
    {
      "epoch": 22.8294078972812,
      "grad_norm": 16.179798126220703,
      "learning_rate": 3.0975493418932334e-05,
      "loss": 1.9267,
      "step": 590300
    },
    {
      "epoch": 22.833275321963104,
      "grad_norm": 13.138165473937988,
      "learning_rate": 3.097227056503075e-05,
      "loss": 1.9758,
      "step": 590400
    },
    {
      "epoch": 22.83714274664501,
      "grad_norm": 14.195097923278809,
      "learning_rate": 3.0969047711129163e-05,
      "loss": 1.8726,
      "step": 590500
    },
    {
      "epoch": 22.841010171326914,
      "grad_norm": 13.038961410522461,
      "learning_rate": 3.096582485722757e-05,
      "loss": 1.8957,
      "step": 590600
    },
    {
      "epoch": 22.844877596008818,
      "grad_norm": 9.50483226776123,
      "learning_rate": 3.0962602003325986e-05,
      "loss": 1.9651,
      "step": 590700
    },
    {
      "epoch": 22.84874502069072,
      "grad_norm": 10.46185302734375,
      "learning_rate": 3.09593791494244e-05,
      "loss": 1.9318,
      "step": 590800
    },
    {
      "epoch": 22.852612445372625,
      "grad_norm": 9.782614707946777,
      "learning_rate": 3.0956156295522816e-05,
      "loss": 1.8971,
      "step": 590900
    },
    {
      "epoch": 22.85647987005453,
      "grad_norm": 10.637063980102539,
      "learning_rate": 3.0952933441621223e-05,
      "loss": 1.8761,
      "step": 591000
    },
    {
      "epoch": 22.860347294736435,
      "grad_norm": 14.40300464630127,
      "learning_rate": 3.094971058771964e-05,
      "loss": 1.8926,
      "step": 591100
    },
    {
      "epoch": 22.86421471941834,
      "grad_norm": 11.711874961853027,
      "learning_rate": 3.094648773381805e-05,
      "loss": 1.9043,
      "step": 591200
    },
    {
      "epoch": 22.868082144100242,
      "grad_norm": 11.990022659301758,
      "learning_rate": 3.094326487991647e-05,
      "loss": 1.9003,
      "step": 591300
    },
    {
      "epoch": 22.87194956878215,
      "grad_norm": 13.232009887695312,
      "learning_rate": 3.0940042026014876e-05,
      "loss": 1.9197,
      "step": 591400
    },
    {
      "epoch": 22.875816993464053,
      "grad_norm": 12.601941108703613,
      "learning_rate": 3.093681917211329e-05,
      "loss": 1.857,
      "step": 591500
    },
    {
      "epoch": 22.879684418145956,
      "grad_norm": 13.281923294067383,
      "learning_rate": 3.0933596318211705e-05,
      "loss": 1.9818,
      "step": 591600
    },
    {
      "epoch": 22.88355184282786,
      "grad_norm": 13.293615341186523,
      "learning_rate": 3.093037346431012e-05,
      "loss": 1.9572,
      "step": 591700
    },
    {
      "epoch": 22.887419267509767,
      "grad_norm": 8.451886177062988,
      "learning_rate": 3.092715061040853e-05,
      "loss": 1.9573,
      "step": 591800
    },
    {
      "epoch": 22.89128669219167,
      "grad_norm": 8.45464038848877,
      "learning_rate": 3.092392775650694e-05,
      "loss": 2.0668,
      "step": 591900
    },
    {
      "epoch": 22.895154116873574,
      "grad_norm": 8.879986763000488,
      "learning_rate": 3.092070490260536e-05,
      "loss": 1.9473,
      "step": 592000
    },
    {
      "epoch": 22.899021541555477,
      "grad_norm": 11.280611038208008,
      "learning_rate": 3.091748204870377e-05,
      "loss": 2.0009,
      "step": 592100
    },
    {
      "epoch": 22.902888966237384,
      "grad_norm": 10.84536075592041,
      "learning_rate": 3.091425919480218e-05,
      "loss": 1.9764,
      "step": 592200
    },
    {
      "epoch": 22.906756390919288,
      "grad_norm": 11.306437492370605,
      "learning_rate": 3.0911036340900595e-05,
      "loss": 1.9316,
      "step": 592300
    },
    {
      "epoch": 22.91062381560119,
      "grad_norm": 15.2020263671875,
      "learning_rate": 3.090781348699901e-05,
      "loss": 1.8834,
      "step": 592400
    },
    {
      "epoch": 22.914491240283095,
      "grad_norm": 9.793076515197754,
      "learning_rate": 3.0904590633097424e-05,
      "loss": 1.9752,
      "step": 592500
    },
    {
      "epoch": 22.918358664964998,
      "grad_norm": 13.900616645812988,
      "learning_rate": 3.090136777919583e-05,
      "loss": 1.8549,
      "step": 592600
    },
    {
      "epoch": 22.922226089646905,
      "grad_norm": 12.938186645507812,
      "learning_rate": 3.0898144925294247e-05,
      "loss": 2.0298,
      "step": 592700
    },
    {
      "epoch": 22.92609351432881,
      "grad_norm": 11.794950485229492,
      "learning_rate": 3.089492207139266e-05,
      "loss": 1.9393,
      "step": 592800
    },
    {
      "epoch": 22.929960939010712,
      "grad_norm": 11.292407035827637,
      "learning_rate": 3.0891699217491076e-05,
      "loss": 1.9694,
      "step": 592900
    },
    {
      "epoch": 22.933828363692616,
      "grad_norm": 14.83870792388916,
      "learning_rate": 3.0888476363589484e-05,
      "loss": 1.9165,
      "step": 593000
    },
    {
      "epoch": 22.937695788374523,
      "grad_norm": 12.196274757385254,
      "learning_rate": 3.08852535096879e-05,
      "loss": 1.8683,
      "step": 593100
    },
    {
      "epoch": 22.941563213056426,
      "grad_norm": 12.416932106018066,
      "learning_rate": 3.0882030655786313e-05,
      "loss": 2.069,
      "step": 593200
    },
    {
      "epoch": 22.94543063773833,
      "grad_norm": 10.210254669189453,
      "learning_rate": 3.087880780188472e-05,
      "loss": 1.9436,
      "step": 593300
    },
    {
      "epoch": 22.949298062420233,
      "grad_norm": 7.993058204650879,
      "learning_rate": 3.0875584947983136e-05,
      "loss": 2.0456,
      "step": 593400
    },
    {
      "epoch": 22.95316548710214,
      "grad_norm": 11.274087905883789,
      "learning_rate": 3.087236209408155e-05,
      "loss": 2.0191,
      "step": 593500
    },
    {
      "epoch": 22.957032911784044,
      "grad_norm": 10.984508514404297,
      "learning_rate": 3.0869139240179966e-05,
      "loss": 1.8867,
      "step": 593600
    },
    {
      "epoch": 22.960900336465947,
      "grad_norm": 12.906208992004395,
      "learning_rate": 3.0865916386278373e-05,
      "loss": 2.1264,
      "step": 593700
    },
    {
      "epoch": 22.96476776114785,
      "grad_norm": 12.976371765136719,
      "learning_rate": 3.086269353237679e-05,
      "loss": 1.9606,
      "step": 593800
    },
    {
      "epoch": 22.968635185829754,
      "grad_norm": 11.994250297546387,
      "learning_rate": 3.08594706784752e-05,
      "loss": 1.8391,
      "step": 593900
    },
    {
      "epoch": 22.97250261051166,
      "grad_norm": 11.782527923583984,
      "learning_rate": 3.085624782457362e-05,
      "loss": 1.9173,
      "step": 594000
    },
    {
      "epoch": 22.976370035193565,
      "grad_norm": 13.537403106689453,
      "learning_rate": 3.0853024970672026e-05,
      "loss": 1.9444,
      "step": 594100
    },
    {
      "epoch": 22.980237459875468,
      "grad_norm": 10.712040901184082,
      "learning_rate": 3.084980211677044e-05,
      "loss": 1.9552,
      "step": 594200
    },
    {
      "epoch": 22.98410488455737,
      "grad_norm": 9.014880180358887,
      "learning_rate": 3.0846579262868855e-05,
      "loss": 2.0039,
      "step": 594300
    },
    {
      "epoch": 22.98797230923928,
      "grad_norm": 11.121018409729004,
      "learning_rate": 3.084335640896727e-05,
      "loss": 1.9499,
      "step": 594400
    },
    {
      "epoch": 22.991839733921182,
      "grad_norm": 11.250275611877441,
      "learning_rate": 3.0840133555065684e-05,
      "loss": 1.9336,
      "step": 594500
    },
    {
      "epoch": 22.995707158603086,
      "grad_norm": 14.500447273254395,
      "learning_rate": 3.083691070116409e-05,
      "loss": 1.9811,
      "step": 594600
    },
    {
      "epoch": 22.99957458328499,
      "grad_norm": 9.208086967468262,
      "learning_rate": 3.083368784726251e-05,
      "loss": 1.866,
      "step": 594700
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.8536946773529053,
      "eval_runtime": 2.9033,
      "eval_samples_per_second": 468.779,
      "eval_steps_per_second": 468.779,
      "step": 594711
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.7457489967346191,
      "eval_runtime": 56.4752,
      "eval_samples_per_second": 457.847,
      "eval_steps_per_second": 457.847,
      "step": 594711
    },
    {
      "epoch": 23.003442007966896,
      "grad_norm": 12.099754333496094,
      "learning_rate": 3.083046499336092e-05,
      "loss": 2.036,
      "step": 594800
    },
    {
      "epoch": 23.0073094326488,
      "grad_norm": 10.701902389526367,
      "learning_rate": 3.0827242139459337e-05,
      "loss": 1.8764,
      "step": 594900
    },
    {
      "epoch": 23.011176857330703,
      "grad_norm": 12.87820053100586,
      "learning_rate": 3.082401928555775e-05,
      "loss": 1.8848,
      "step": 595000
    },
    {
      "epoch": 23.015044282012607,
      "grad_norm": 14.830135345458984,
      "learning_rate": 3.0820796431656166e-05,
      "loss": 1.9514,
      "step": 595100
    },
    {
      "epoch": 23.018911706694514,
      "grad_norm": 10.883978843688965,
      "learning_rate": 3.0817573577754574e-05,
      "loss": 1.9176,
      "step": 595200
    },
    {
      "epoch": 23.022779131376417,
      "grad_norm": 12.022329330444336,
      "learning_rate": 3.081435072385299e-05,
      "loss": 1.8902,
      "step": 595300
    },
    {
      "epoch": 23.02664655605832,
      "grad_norm": 12.544157981872559,
      "learning_rate": 3.08111278699514e-05,
      "loss": 1.7893,
      "step": 595400
    },
    {
      "epoch": 23.030513980740224,
      "grad_norm": 10.171365737915039,
      "learning_rate": 3.080790501604982e-05,
      "loss": 1.8826,
      "step": 595500
    },
    {
      "epoch": 23.034381405422128,
      "grad_norm": 7.757248401641846,
      "learning_rate": 3.080468216214823e-05,
      "loss": 1.9549,
      "step": 595600
    },
    {
      "epoch": 23.038248830104035,
      "grad_norm": 9.373832702636719,
      "learning_rate": 3.080145930824664e-05,
      "loss": 1.9216,
      "step": 595700
    },
    {
      "epoch": 23.042116254785938,
      "grad_norm": 12.17882251739502,
      "learning_rate": 3.0798236454345055e-05,
      "loss": 1.8841,
      "step": 595800
    },
    {
      "epoch": 23.04598367946784,
      "grad_norm": 10.675216674804688,
      "learning_rate": 3.079501360044347e-05,
      "loss": 1.9028,
      "step": 595900
    },
    {
      "epoch": 23.049851104149745,
      "grad_norm": 14.174368858337402,
      "learning_rate": 3.0791790746541885e-05,
      "loss": 1.888,
      "step": 596000
    },
    {
      "epoch": 23.053718528831652,
      "grad_norm": 6.948979377746582,
      "learning_rate": 3.078856789264029e-05,
      "loss": 1.9346,
      "step": 596100
    },
    {
      "epoch": 23.057585953513556,
      "grad_norm": 14.278422355651855,
      "learning_rate": 3.078534503873871e-05,
      "loss": 1.8585,
      "step": 596200
    },
    {
      "epoch": 23.06145337819546,
      "grad_norm": 7.791586875915527,
      "learning_rate": 3.078212218483712e-05,
      "loss": 1.9559,
      "step": 596300
    },
    {
      "epoch": 23.065320802877363,
      "grad_norm": 13.7113618850708,
      "learning_rate": 3.077889933093553e-05,
      "loss": 1.9075,
      "step": 596400
    },
    {
      "epoch": 23.06918822755927,
      "grad_norm": 18.79593849182129,
      "learning_rate": 3.0775676477033945e-05,
      "loss": 1.9206,
      "step": 596500
    },
    {
      "epoch": 23.073055652241173,
      "grad_norm": 11.39380168914795,
      "learning_rate": 3.077245362313236e-05,
      "loss": 1.8378,
      "step": 596600
    },
    {
      "epoch": 23.076923076923077,
      "grad_norm": 12.406632423400879,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 1.9091,
      "step": 596700
    },
    {
      "epoch": 23.08079050160498,
      "grad_norm": 12.274910926818848,
      "learning_rate": 3.076600791532918e-05,
      "loss": 1.8001,
      "step": 596800
    },
    {
      "epoch": 23.084657926286887,
      "grad_norm": 10.60616683959961,
      "learning_rate": 3.07627850614276e-05,
      "loss": 1.971,
      "step": 596900
    },
    {
      "epoch": 23.08852535096879,
      "grad_norm": 13.002034187316895,
      "learning_rate": 3.075956220752601e-05,
      "loss": 1.9114,
      "step": 597000
    },
    {
      "epoch": 23.092392775650694,
      "grad_norm": 9.311384201049805,
      "learning_rate": 3.0756339353624426e-05,
      "loss": 1.9954,
      "step": 597100
    },
    {
      "epoch": 23.096260200332598,
      "grad_norm": 13.078998565673828,
      "learning_rate": 3.0753116499722834e-05,
      "loss": 1.9094,
      "step": 597200
    },
    {
      "epoch": 23.1001276250145,
      "grad_norm": 10.514145851135254,
      "learning_rate": 3.074989364582125e-05,
      "loss": 1.9139,
      "step": 597300
    },
    {
      "epoch": 23.103995049696408,
      "grad_norm": 10.626062393188477,
      "learning_rate": 3.0746670791919664e-05,
      "loss": 1.8827,
      "step": 597400
    },
    {
      "epoch": 23.10786247437831,
      "grad_norm": 9.296671867370605,
      "learning_rate": 3.074344793801808e-05,
      "loss": 1.8906,
      "step": 597500
    },
    {
      "epoch": 23.111729899060215,
      "grad_norm": 11.192138671875,
      "learning_rate": 3.0740225084116486e-05,
      "loss": 1.9473,
      "step": 597600
    },
    {
      "epoch": 23.11559732374212,
      "grad_norm": 10.23419189453125,
      "learning_rate": 3.07370022302149e-05,
      "loss": 1.9549,
      "step": 597700
    },
    {
      "epoch": 23.119464748424026,
      "grad_norm": 15.473020553588867,
      "learning_rate": 3.0733779376313316e-05,
      "loss": 1.9071,
      "step": 597800
    },
    {
      "epoch": 23.12333217310593,
      "grad_norm": 11.405651092529297,
      "learning_rate": 3.073055652241173e-05,
      "loss": 1.9345,
      "step": 597900
    },
    {
      "epoch": 23.127199597787833,
      "grad_norm": 9.47753620147705,
      "learning_rate": 3.072733366851014e-05,
      "loss": 1.8455,
      "step": 598000
    },
    {
      "epoch": 23.131067022469736,
      "grad_norm": 9.808222770690918,
      "learning_rate": 3.072411081460855e-05,
      "loss": 1.8289,
      "step": 598100
    },
    {
      "epoch": 23.134934447151643,
      "grad_norm": 14.7467041015625,
      "learning_rate": 3.072088796070697e-05,
      "loss": 1.9601,
      "step": 598200
    },
    {
      "epoch": 23.138801871833547,
      "grad_norm": 10.524614334106445,
      "learning_rate": 3.071766510680538e-05,
      "loss": 1.9056,
      "step": 598300
    },
    {
      "epoch": 23.14266929651545,
      "grad_norm": 9.955933570861816,
      "learning_rate": 3.071444225290379e-05,
      "loss": 1.8785,
      "step": 598400
    },
    {
      "epoch": 23.146536721197354,
      "grad_norm": 13.306835174560547,
      "learning_rate": 3.0711219399002205e-05,
      "loss": 1.9158,
      "step": 598500
    },
    {
      "epoch": 23.15040414587926,
      "grad_norm": 12.106074333190918,
      "learning_rate": 3.070799654510062e-05,
      "loss": 1.916,
      "step": 598600
    },
    {
      "epoch": 23.154271570561164,
      "grad_norm": 9.648918151855469,
      "learning_rate": 3.0704773691199035e-05,
      "loss": 2.0199,
      "step": 598700
    },
    {
      "epoch": 23.158138995243068,
      "grad_norm": 14.611355781555176,
      "learning_rate": 3.070155083729744e-05,
      "loss": 1.8975,
      "step": 598800
    },
    {
      "epoch": 23.16200641992497,
      "grad_norm": 10.124432563781738,
      "learning_rate": 3.069832798339586e-05,
      "loss": 1.882,
      "step": 598900
    },
    {
      "epoch": 23.165873844606875,
      "grad_norm": 12.41244125366211,
      "learning_rate": 3.069510512949427e-05,
      "loss": 1.9543,
      "step": 599000
    },
    {
      "epoch": 23.16974126928878,
      "grad_norm": 12.22333812713623,
      "learning_rate": 3.069188227559268e-05,
      "loss": 1.9472,
      "step": 599100
    },
    {
      "epoch": 23.173608693970685,
      "grad_norm": 10.028517723083496,
      "learning_rate": 3.0688659421691095e-05,
      "loss": 1.9156,
      "step": 599200
    },
    {
      "epoch": 23.17747611865259,
      "grad_norm": 17.130495071411133,
      "learning_rate": 3.068543656778951e-05,
      "loss": 1.9612,
      "step": 599300
    },
    {
      "epoch": 23.181343543334492,
      "grad_norm": 11.535268783569336,
      "learning_rate": 3.0682213713887924e-05,
      "loss": 1.9147,
      "step": 599400
    },
    {
      "epoch": 23.1852109680164,
      "grad_norm": 12.802667617797852,
      "learning_rate": 3.067899085998633e-05,
      "loss": 1.8965,
      "step": 599500
    },
    {
      "epoch": 23.189078392698303,
      "grad_norm": 10.06557846069336,
      "learning_rate": 3.067576800608475e-05,
      "loss": 1.8916,
      "step": 599600
    },
    {
      "epoch": 23.192945817380206,
      "grad_norm": 11.567605972290039,
      "learning_rate": 3.067254515218316e-05,
      "loss": 1.8302,
      "step": 599700
    },
    {
      "epoch": 23.19681324206211,
      "grad_norm": 9.765522956848145,
      "learning_rate": 3.0669322298281576e-05,
      "loss": 1.9285,
      "step": 599800
    },
    {
      "epoch": 23.200680666744017,
      "grad_norm": 11.58801555633545,
      "learning_rate": 3.0666099444379984e-05,
      "loss": 1.9511,
      "step": 599900
    },
    {
      "epoch": 23.20454809142592,
      "grad_norm": 12.58854866027832,
      "learning_rate": 3.06628765904784e-05,
      "loss": 1.8237,
      "step": 600000
    },
    {
      "epoch": 23.208415516107824,
      "grad_norm": 13.90377426147461,
      "learning_rate": 3.0659653736576814e-05,
      "loss": 2.0163,
      "step": 600100
    },
    {
      "epoch": 23.212282940789727,
      "grad_norm": 10.369170188903809,
      "learning_rate": 3.065643088267523e-05,
      "loss": 1.9789,
      "step": 600200
    },
    {
      "epoch": 23.216150365471634,
      "grad_norm": 11.55816650390625,
      "learning_rate": 3.0653208028773636e-05,
      "loss": 1.8358,
      "step": 600300
    },
    {
      "epoch": 23.220017790153538,
      "grad_norm": 9.145398139953613,
      "learning_rate": 3.064998517487205e-05,
      "loss": 1.9628,
      "step": 600400
    },
    {
      "epoch": 23.22388521483544,
      "grad_norm": 12.501433372497559,
      "learning_rate": 3.0646762320970466e-05,
      "loss": 2.0312,
      "step": 600500
    },
    {
      "epoch": 23.227752639517345,
      "grad_norm": 11.509309768676758,
      "learning_rate": 3.064353946706888e-05,
      "loss": 1.9639,
      "step": 600600
    },
    {
      "epoch": 23.231620064199248,
      "grad_norm": 11.117547035217285,
      "learning_rate": 3.064031661316729e-05,
      "loss": 2.0298,
      "step": 600700
    },
    {
      "epoch": 23.235487488881155,
      "grad_norm": 13.322529792785645,
      "learning_rate": 3.06370937592657e-05,
      "loss": 1.8902,
      "step": 600800
    },
    {
      "epoch": 23.23935491356306,
      "grad_norm": 16.85236930847168,
      "learning_rate": 3.063387090536412e-05,
      "loss": 2.0497,
      "step": 600900
    },
    {
      "epoch": 23.243222338244962,
      "grad_norm": 12.80714225769043,
      "learning_rate": 3.063064805146253e-05,
      "loss": 1.9286,
      "step": 601000
    },
    {
      "epoch": 23.247089762926866,
      "grad_norm": 10.743857383728027,
      "learning_rate": 3.062742519756094e-05,
      "loss": 1.949,
      "step": 601100
    },
    {
      "epoch": 23.250957187608773,
      "grad_norm": 10.054449081420898,
      "learning_rate": 3.0624202343659355e-05,
      "loss": 1.924,
      "step": 601200
    },
    {
      "epoch": 23.254824612290676,
      "grad_norm": 12.861787796020508,
      "learning_rate": 3.062097948975777e-05,
      "loss": 2.0361,
      "step": 601300
    },
    {
      "epoch": 23.25869203697258,
      "grad_norm": 12.240084648132324,
      "learning_rate": 3.0617756635856185e-05,
      "loss": 1.9535,
      "step": 601400
    },
    {
      "epoch": 23.262559461654483,
      "grad_norm": 10.293131828308105,
      "learning_rate": 3.06145337819546e-05,
      "loss": 1.8475,
      "step": 601500
    },
    {
      "epoch": 23.26642688633639,
      "grad_norm": 9.767114639282227,
      "learning_rate": 3.061131092805301e-05,
      "loss": 1.9516,
      "step": 601600
    },
    {
      "epoch": 23.270294311018294,
      "grad_norm": 11.355301856994629,
      "learning_rate": 3.060808807415142e-05,
      "loss": 1.8908,
      "step": 601700
    },
    {
      "epoch": 23.274161735700197,
      "grad_norm": 9.611248016357422,
      "learning_rate": 3.060486522024984e-05,
      "loss": 1.9897,
      "step": 601800
    },
    {
      "epoch": 23.2780291603821,
      "grad_norm": 14.995983123779297,
      "learning_rate": 3.060164236634825e-05,
      "loss": 1.935,
      "step": 601900
    },
    {
      "epoch": 23.281896585064008,
      "grad_norm": 11.101402282714844,
      "learning_rate": 3.0598419512446666e-05,
      "loss": 1.8809,
      "step": 602000
    },
    {
      "epoch": 23.28576400974591,
      "grad_norm": 12.302668571472168,
      "learning_rate": 3.059519665854508e-05,
      "loss": 1.9559,
      "step": 602100
    },
    {
      "epoch": 23.289631434427815,
      "grad_norm": 10.660518646240234,
      "learning_rate": 3.059197380464349e-05,
      "loss": 1.8926,
      "step": 602200
    },
    {
      "epoch": 23.293498859109718,
      "grad_norm": 10.238565444946289,
      "learning_rate": 3.0588750950741904e-05,
      "loss": 1.9025,
      "step": 602300
    },
    {
      "epoch": 23.29736628379162,
      "grad_norm": 13.053279876708984,
      "learning_rate": 3.058552809684032e-05,
      "loss": 2.0195,
      "step": 602400
    },
    {
      "epoch": 23.30123370847353,
      "grad_norm": 12.701576232910156,
      "learning_rate": 3.058230524293873e-05,
      "loss": 2.0241,
      "step": 602500
    },
    {
      "epoch": 23.305101133155432,
      "grad_norm": 10.596165657043457,
      "learning_rate": 3.057908238903714e-05,
      "loss": 1.8785,
      "step": 602600
    },
    {
      "epoch": 23.308968557837336,
      "grad_norm": 10.957924842834473,
      "learning_rate": 3.0575859535135556e-05,
      "loss": 1.881,
      "step": 602700
    },
    {
      "epoch": 23.31283598251924,
      "grad_norm": 12.225674629211426,
      "learning_rate": 3.057263668123397e-05,
      "loss": 1.9206,
      "step": 602800
    },
    {
      "epoch": 23.316703407201146,
      "grad_norm": 11.176908493041992,
      "learning_rate": 3.0569413827332385e-05,
      "loss": 1.9317,
      "step": 602900
    },
    {
      "epoch": 23.32057083188305,
      "grad_norm": 11.840152740478516,
      "learning_rate": 3.056619097343079e-05,
      "loss": 1.9272,
      "step": 603000
    },
    {
      "epoch": 23.324438256564953,
      "grad_norm": 14.775362014770508,
      "learning_rate": 3.056296811952921e-05,
      "loss": 1.8458,
      "step": 603100
    },
    {
      "epoch": 23.328305681246857,
      "grad_norm": 14.858321189880371,
      "learning_rate": 3.055974526562762e-05,
      "loss": 1.8524,
      "step": 603200
    },
    {
      "epoch": 23.332173105928764,
      "grad_norm": 10.817424774169922,
      "learning_rate": 3.055652241172604e-05,
      "loss": 1.9558,
      "step": 603300
    },
    {
      "epoch": 23.336040530610667,
      "grad_norm": 10.178722381591797,
      "learning_rate": 3.0553299557824445e-05,
      "loss": 1.9128,
      "step": 603400
    },
    {
      "epoch": 23.33990795529257,
      "grad_norm": 9.815494537353516,
      "learning_rate": 3.055007670392286e-05,
      "loss": 1.8879,
      "step": 603500
    },
    {
      "epoch": 23.343775379974474,
      "grad_norm": 12.076278686523438,
      "learning_rate": 3.0546853850021275e-05,
      "loss": 1.951,
      "step": 603600
    },
    {
      "epoch": 23.34764280465638,
      "grad_norm": 10.737238883972168,
      "learning_rate": 3.054363099611969e-05,
      "loss": 1.9866,
      "step": 603700
    },
    {
      "epoch": 23.351510229338285,
      "grad_norm": 13.836966514587402,
      "learning_rate": 3.05404081422181e-05,
      "loss": 1.8729,
      "step": 603800
    },
    {
      "epoch": 23.355377654020188,
      "grad_norm": 12.389140129089355,
      "learning_rate": 3.053718528831651e-05,
      "loss": 1.8667,
      "step": 603900
    },
    {
      "epoch": 23.35924507870209,
      "grad_norm": 15.820881843566895,
      "learning_rate": 3.053396243441493e-05,
      "loss": 1.9101,
      "step": 604000
    },
    {
      "epoch": 23.363112503383995,
      "grad_norm": 13.109928131103516,
      "learning_rate": 3.053073958051334e-05,
      "loss": 1.908,
      "step": 604100
    },
    {
      "epoch": 23.366979928065902,
      "grad_norm": 10.941108703613281,
      "learning_rate": 3.052751672661175e-05,
      "loss": 1.9213,
      "step": 604200
    },
    {
      "epoch": 23.370847352747806,
      "grad_norm": 15.264795303344727,
      "learning_rate": 3.0524293872710164e-05,
      "loss": 2.0155,
      "step": 604300
    },
    {
      "epoch": 23.37471477742971,
      "grad_norm": 10.89205265045166,
      "learning_rate": 3.052107101880858e-05,
      "loss": 1.9448,
      "step": 604400
    },
    {
      "epoch": 23.378582202111613,
      "grad_norm": 10.694682121276855,
      "learning_rate": 3.0517848164906994e-05,
      "loss": 1.9657,
      "step": 604500
    },
    {
      "epoch": 23.38244962679352,
      "grad_norm": 10.984284400939941,
      "learning_rate": 3.05146253110054e-05,
      "loss": 1.9044,
      "step": 604600
    },
    {
      "epoch": 23.386317051475423,
      "grad_norm": 15.41408634185791,
      "learning_rate": 3.0511402457103816e-05,
      "loss": 1.9093,
      "step": 604700
    },
    {
      "epoch": 23.390184476157327,
      "grad_norm": 12.542863845825195,
      "learning_rate": 3.050817960320223e-05,
      "loss": 2.0724,
      "step": 604800
    },
    {
      "epoch": 23.39405190083923,
      "grad_norm": 12.57156753540039,
      "learning_rate": 3.050495674930064e-05,
      "loss": 1.903,
      "step": 604900
    },
    {
      "epoch": 23.397919325521137,
      "grad_norm": 12.37317943572998,
      "learning_rate": 3.0501733895399054e-05,
      "loss": 1.9191,
      "step": 605000
    },
    {
      "epoch": 23.40178675020304,
      "grad_norm": 10.37865924835205,
      "learning_rate": 3.049851104149747e-05,
      "loss": 1.8721,
      "step": 605100
    },
    {
      "epoch": 23.405654174884944,
      "grad_norm": 15.068524360656738,
      "learning_rate": 3.0495288187595883e-05,
      "loss": 2.1015,
      "step": 605200
    },
    {
      "epoch": 23.409521599566848,
      "grad_norm": 10.513670921325684,
      "learning_rate": 3.049206533369429e-05,
      "loss": 2.0286,
      "step": 605300
    },
    {
      "epoch": 23.41338902424875,
      "grad_norm": 10.802963256835938,
      "learning_rate": 3.0488842479792706e-05,
      "loss": 1.8564,
      "step": 605400
    },
    {
      "epoch": 23.417256448930658,
      "grad_norm": 11.000362396240234,
      "learning_rate": 3.048561962589112e-05,
      "loss": 1.8452,
      "step": 605500
    },
    {
      "epoch": 23.42112387361256,
      "grad_norm": 12.122247695922852,
      "learning_rate": 3.0482396771989535e-05,
      "loss": 1.888,
      "step": 605600
    },
    {
      "epoch": 23.424991298294465,
      "grad_norm": 10.046609878540039,
      "learning_rate": 3.0479173918087943e-05,
      "loss": 1.9118,
      "step": 605700
    },
    {
      "epoch": 23.42885872297637,
      "grad_norm": 12.573945999145508,
      "learning_rate": 3.0475951064186358e-05,
      "loss": 1.9705,
      "step": 605800
    },
    {
      "epoch": 23.432726147658276,
      "grad_norm": 12.69742202758789,
      "learning_rate": 3.0472728210284773e-05,
      "loss": 1.9284,
      "step": 605900
    },
    {
      "epoch": 23.43659357234018,
      "grad_norm": 13.548162460327148,
      "learning_rate": 3.0469505356383187e-05,
      "loss": 1.9512,
      "step": 606000
    },
    {
      "epoch": 23.440460997022083,
      "grad_norm": 14.606477737426758,
      "learning_rate": 3.0466282502481595e-05,
      "loss": 1.9176,
      "step": 606100
    },
    {
      "epoch": 23.444328421703986,
      "grad_norm": 9.333675384521484,
      "learning_rate": 3.046305964858001e-05,
      "loss": 1.9354,
      "step": 606200
    },
    {
      "epoch": 23.448195846385893,
      "grad_norm": 13.368635177612305,
      "learning_rate": 3.0459836794678425e-05,
      "loss": 1.9385,
      "step": 606300
    },
    {
      "epoch": 23.452063271067797,
      "grad_norm": 9.83220386505127,
      "learning_rate": 3.045661394077684e-05,
      "loss": 1.8945,
      "step": 606400
    },
    {
      "epoch": 23.4559306957497,
      "grad_norm": 14.592270851135254,
      "learning_rate": 3.045339108687525e-05,
      "loss": 1.9445,
      "step": 606500
    },
    {
      "epoch": 23.459798120431604,
      "grad_norm": 14.153792381286621,
      "learning_rate": 3.0450168232973662e-05,
      "loss": 1.9356,
      "step": 606600
    },
    {
      "epoch": 23.46366554511351,
      "grad_norm": 12.160542488098145,
      "learning_rate": 3.0446945379072077e-05,
      "loss": 1.9532,
      "step": 606700
    },
    {
      "epoch": 23.467532969795414,
      "grad_norm": 10.767410278320312,
      "learning_rate": 3.044372252517049e-05,
      "loss": 1.9749,
      "step": 606800
    },
    {
      "epoch": 23.471400394477318,
      "grad_norm": 11.876501083374023,
      "learning_rate": 3.0440499671268903e-05,
      "loss": 1.9789,
      "step": 606900
    },
    {
      "epoch": 23.47526781915922,
      "grad_norm": 10.136801719665527,
      "learning_rate": 3.0437276817367318e-05,
      "loss": 1.9449,
      "step": 607000
    },
    {
      "epoch": 23.479135243841124,
      "grad_norm": 9.038233757019043,
      "learning_rate": 3.0434053963465732e-05,
      "loss": 1.9113,
      "step": 607100
    },
    {
      "epoch": 23.48300266852303,
      "grad_norm": 8.821910858154297,
      "learning_rate": 3.0430831109564144e-05,
      "loss": 1.863,
      "step": 607200
    },
    {
      "epoch": 23.486870093204935,
      "grad_norm": 13.3518648147583,
      "learning_rate": 3.0427608255662555e-05,
      "loss": 1.9493,
      "step": 607300
    },
    {
      "epoch": 23.49073751788684,
      "grad_norm": 13.288904190063477,
      "learning_rate": 3.042438540176097e-05,
      "loss": 1.9881,
      "step": 607400
    },
    {
      "epoch": 23.494604942568742,
      "grad_norm": 10.367901802062988,
      "learning_rate": 3.0421162547859384e-05,
      "loss": 1.9102,
      "step": 607500
    },
    {
      "epoch": 23.49847236725065,
      "grad_norm": 14.530156135559082,
      "learning_rate": 3.04179396939578e-05,
      "loss": 1.9357,
      "step": 607600
    },
    {
      "epoch": 23.502339791932553,
      "grad_norm": 9.323161125183105,
      "learning_rate": 3.0414716840056207e-05,
      "loss": 1.8815,
      "step": 607700
    },
    {
      "epoch": 23.506207216614456,
      "grad_norm": 10.662081718444824,
      "learning_rate": 3.0411493986154622e-05,
      "loss": 1.9376,
      "step": 607800
    },
    {
      "epoch": 23.51007464129636,
      "grad_norm": 14.381828308105469,
      "learning_rate": 3.0408271132253036e-05,
      "loss": 2.016,
      "step": 607900
    },
    {
      "epoch": 23.513942065978267,
      "grad_norm": 10.197052955627441,
      "learning_rate": 3.0405048278351444e-05,
      "loss": 1.9612,
      "step": 608000
    },
    {
      "epoch": 23.51780949066017,
      "grad_norm": 20.88423728942871,
      "learning_rate": 3.040182542444986e-05,
      "loss": 1.8992,
      "step": 608100
    },
    {
      "epoch": 23.521676915342073,
      "grad_norm": 13.306681632995605,
      "learning_rate": 3.0398602570548274e-05,
      "loss": 1.8759,
      "step": 608200
    },
    {
      "epoch": 23.525544340023977,
      "grad_norm": 11.958810806274414,
      "learning_rate": 3.039537971664669e-05,
      "loss": 1.9111,
      "step": 608300
    },
    {
      "epoch": 23.529411764705884,
      "grad_norm": 11.35340690612793,
      "learning_rate": 3.0392156862745097e-05,
      "loss": 1.8861,
      "step": 608400
    },
    {
      "epoch": 23.533279189387788,
      "grad_norm": 14.560795783996582,
      "learning_rate": 3.038893400884351e-05,
      "loss": 1.8805,
      "step": 608500
    },
    {
      "epoch": 23.53714661406969,
      "grad_norm": 11.608955383300781,
      "learning_rate": 3.0385711154941926e-05,
      "loss": 1.9881,
      "step": 608600
    },
    {
      "epoch": 23.541014038751594,
      "grad_norm": 9.621917724609375,
      "learning_rate": 3.038248830104034e-05,
      "loss": 2.0659,
      "step": 608700
    },
    {
      "epoch": 23.544881463433498,
      "grad_norm": 10.791004180908203,
      "learning_rate": 3.037926544713875e-05,
      "loss": 1.8385,
      "step": 608800
    },
    {
      "epoch": 23.548748888115405,
      "grad_norm": 13.458955764770508,
      "learning_rate": 3.0376042593237163e-05,
      "loss": 1.9048,
      "step": 608900
    },
    {
      "epoch": 23.55261631279731,
      "grad_norm": 12.805280685424805,
      "learning_rate": 3.0372819739335578e-05,
      "loss": 1.9198,
      "step": 609000
    },
    {
      "epoch": 23.556483737479212,
      "grad_norm": 10.918386459350586,
      "learning_rate": 3.0369596885433993e-05,
      "loss": 1.8568,
      "step": 609100
    },
    {
      "epoch": 23.560351162161115,
      "grad_norm": 12.952312469482422,
      "learning_rate": 3.03663740315324e-05,
      "loss": 1.9089,
      "step": 609200
    },
    {
      "epoch": 23.564218586843023,
      "grad_norm": 12.456427574157715,
      "learning_rate": 3.0363151177630815e-05,
      "loss": 1.8501,
      "step": 609300
    },
    {
      "epoch": 23.568086011524926,
      "grad_norm": 9.24498176574707,
      "learning_rate": 3.035992832372923e-05,
      "loss": 1.8334,
      "step": 609400
    },
    {
      "epoch": 23.57195343620683,
      "grad_norm": 12.987010955810547,
      "learning_rate": 3.0356705469827645e-05,
      "loss": 1.9375,
      "step": 609500
    },
    {
      "epoch": 23.575820860888733,
      "grad_norm": 10.760828018188477,
      "learning_rate": 3.0353482615926053e-05,
      "loss": 2.0044,
      "step": 609600
    },
    {
      "epoch": 23.57968828557064,
      "grad_norm": 13.904851913452148,
      "learning_rate": 3.0350259762024468e-05,
      "loss": 1.9995,
      "step": 609700
    },
    {
      "epoch": 23.583555710252543,
      "grad_norm": 9.227226257324219,
      "learning_rate": 3.0347036908122882e-05,
      "loss": 1.9787,
      "step": 609800
    },
    {
      "epoch": 23.587423134934447,
      "grad_norm": 10.906621932983398,
      "learning_rate": 3.0343814054221297e-05,
      "loss": 1.8031,
      "step": 609900
    },
    {
      "epoch": 23.59129055961635,
      "grad_norm": 13.517463684082031,
      "learning_rate": 3.0340591200319708e-05,
      "loss": 1.944,
      "step": 610000
    },
    {
      "epoch": 23.595157984298258,
      "grad_norm": 11.093526840209961,
      "learning_rate": 3.0337368346418123e-05,
      "loss": 2.0109,
      "step": 610100
    },
    {
      "epoch": 23.59902540898016,
      "grad_norm": 11.499490737915039,
      "learning_rate": 3.0334145492516534e-05,
      "loss": 1.9461,
      "step": 610200
    },
    {
      "epoch": 23.602892833662064,
      "grad_norm": 11.154483795166016,
      "learning_rate": 3.033092263861495e-05,
      "loss": 1.9036,
      "step": 610300
    },
    {
      "epoch": 23.606760258343968,
      "grad_norm": 11.507582664489746,
      "learning_rate": 3.032769978471336e-05,
      "loss": 1.9671,
      "step": 610400
    },
    {
      "epoch": 23.61062768302587,
      "grad_norm": 13.008528709411621,
      "learning_rate": 3.0324476930811775e-05,
      "loss": 1.9071,
      "step": 610500
    },
    {
      "epoch": 23.61449510770778,
      "grad_norm": 10.264010429382324,
      "learning_rate": 3.032125407691019e-05,
      "loss": 1.9552,
      "step": 610600
    },
    {
      "epoch": 23.618362532389682,
      "grad_norm": 16.783266067504883,
      "learning_rate": 3.03180312230086e-05,
      "loss": 1.8513,
      "step": 610700
    },
    {
      "epoch": 23.622229957071585,
      "grad_norm": 11.678749084472656,
      "learning_rate": 3.0314808369107012e-05,
      "loss": 1.9486,
      "step": 610800
    },
    {
      "epoch": 23.62609738175349,
      "grad_norm": 12.135079383850098,
      "learning_rate": 3.0311585515205427e-05,
      "loss": 1.9054,
      "step": 610900
    },
    {
      "epoch": 23.629964806435396,
      "grad_norm": 11.004216194152832,
      "learning_rate": 3.0308362661303842e-05,
      "loss": 1.9506,
      "step": 611000
    },
    {
      "epoch": 23.6338322311173,
      "grad_norm": 10.43076229095459,
      "learning_rate": 3.030513980740225e-05,
      "loss": 1.9471,
      "step": 611100
    },
    {
      "epoch": 23.637699655799203,
      "grad_norm": 9.745853424072266,
      "learning_rate": 3.0301916953500665e-05,
      "loss": 1.888,
      "step": 611200
    },
    {
      "epoch": 23.641567080481106,
      "grad_norm": 16.53101921081543,
      "learning_rate": 3.029869409959908e-05,
      "loss": 1.9377,
      "step": 611300
    },
    {
      "epoch": 23.645434505163013,
      "grad_norm": 10.238204002380371,
      "learning_rate": 3.0295471245697494e-05,
      "loss": 1.9378,
      "step": 611400
    },
    {
      "epoch": 23.649301929844917,
      "grad_norm": 10.495282173156738,
      "learning_rate": 3.0292248391795902e-05,
      "loss": 1.9342,
      "step": 611500
    },
    {
      "epoch": 23.65316935452682,
      "grad_norm": 10.222478866577148,
      "learning_rate": 3.0289025537894317e-05,
      "loss": 1.9617,
      "step": 611600
    },
    {
      "epoch": 23.657036779208724,
      "grad_norm": 10.963642120361328,
      "learning_rate": 3.028580268399273e-05,
      "loss": 1.9817,
      "step": 611700
    },
    {
      "epoch": 23.66090420389063,
      "grad_norm": 9.479826927185059,
      "learning_rate": 3.0282579830091146e-05,
      "loss": 1.9006,
      "step": 611800
    },
    {
      "epoch": 23.664771628572534,
      "grad_norm": 12.222702026367188,
      "learning_rate": 3.0279356976189554e-05,
      "loss": 1.9488,
      "step": 611900
    },
    {
      "epoch": 23.668639053254438,
      "grad_norm": 9.748347282409668,
      "learning_rate": 3.027613412228797e-05,
      "loss": 1.8767,
      "step": 612000
    },
    {
      "epoch": 23.67250647793634,
      "grad_norm": 14.004711151123047,
      "learning_rate": 3.0272911268386383e-05,
      "loss": 1.9448,
      "step": 612100
    },
    {
      "epoch": 23.676373902618245,
      "grad_norm": 12.366403579711914,
      "learning_rate": 3.0269688414484798e-05,
      "loss": 1.8956,
      "step": 612200
    },
    {
      "epoch": 23.680241327300152,
      "grad_norm": 11.489027976989746,
      "learning_rate": 3.0266465560583206e-05,
      "loss": 1.9843,
      "step": 612300
    },
    {
      "epoch": 23.684108751982055,
      "grad_norm": 10.484990119934082,
      "learning_rate": 3.026324270668162e-05,
      "loss": 1.9707,
      "step": 612400
    },
    {
      "epoch": 23.68797617666396,
      "grad_norm": 10.588822364807129,
      "learning_rate": 3.0260019852780036e-05,
      "loss": 1.948,
      "step": 612500
    },
    {
      "epoch": 23.691843601345862,
      "grad_norm": 11.66013240814209,
      "learning_rate": 3.025679699887845e-05,
      "loss": 1.9537,
      "step": 612600
    },
    {
      "epoch": 23.69571102602777,
      "grad_norm": 10.658722877502441,
      "learning_rate": 3.0253574144976858e-05,
      "loss": 1.9257,
      "step": 612700
    },
    {
      "epoch": 23.699578450709673,
      "grad_norm": 13.264520645141602,
      "learning_rate": 3.0250351291075273e-05,
      "loss": 1.9043,
      "step": 612800
    },
    {
      "epoch": 23.703445875391576,
      "grad_norm": 11.766412734985352,
      "learning_rate": 3.0247128437173688e-05,
      "loss": 2.0434,
      "step": 612900
    },
    {
      "epoch": 23.70731330007348,
      "grad_norm": 19.062135696411133,
      "learning_rate": 3.0243905583272102e-05,
      "loss": 1.902,
      "step": 613000
    },
    {
      "epoch": 23.711180724755387,
      "grad_norm": 11.72336196899414,
      "learning_rate": 3.024068272937051e-05,
      "loss": 1.9728,
      "step": 613100
    },
    {
      "epoch": 23.71504814943729,
      "grad_norm": 9.974749565124512,
      "learning_rate": 3.0237459875468925e-05,
      "loss": 2.0332,
      "step": 613200
    },
    {
      "epoch": 23.718915574119194,
      "grad_norm": 11.108268737792969,
      "learning_rate": 3.023423702156734e-05,
      "loss": 1.9087,
      "step": 613300
    },
    {
      "epoch": 23.722782998801097,
      "grad_norm": 9.982172966003418,
      "learning_rate": 3.0231014167665754e-05,
      "loss": 1.9982,
      "step": 613400
    },
    {
      "epoch": 23.726650423483,
      "grad_norm": 13.445577621459961,
      "learning_rate": 3.0227791313764166e-05,
      "loss": 1.9576,
      "step": 613500
    },
    {
      "epoch": 23.730517848164908,
      "grad_norm": 12.899209022521973,
      "learning_rate": 3.022456845986258e-05,
      "loss": 1.9204,
      "step": 613600
    },
    {
      "epoch": 23.73438527284681,
      "grad_norm": 11.89380931854248,
      "learning_rate": 3.0221345605960992e-05,
      "loss": 1.947,
      "step": 613700
    },
    {
      "epoch": 23.738252697528715,
      "grad_norm": 11.697754859924316,
      "learning_rate": 3.0218122752059403e-05,
      "loss": 2.0032,
      "step": 613800
    },
    {
      "epoch": 23.74212012221062,
      "grad_norm": 9.943197250366211,
      "learning_rate": 3.0214899898157818e-05,
      "loss": 2.0019,
      "step": 613900
    },
    {
      "epoch": 23.745987546892525,
      "grad_norm": 13.867461204528809,
      "learning_rate": 3.0211677044256233e-05,
      "loss": 2.0538,
      "step": 614000
    },
    {
      "epoch": 23.74985497157443,
      "grad_norm": 13.604217529296875,
      "learning_rate": 3.0208454190354647e-05,
      "loss": 1.9219,
      "step": 614100
    },
    {
      "epoch": 23.753722396256332,
      "grad_norm": 11.80996036529541,
      "learning_rate": 3.0205231336453055e-05,
      "loss": 1.9437,
      "step": 614200
    },
    {
      "epoch": 23.757589820938236,
      "grad_norm": 11.76960277557373,
      "learning_rate": 3.020200848255147e-05,
      "loss": 1.9937,
      "step": 614300
    },
    {
      "epoch": 23.761457245620143,
      "grad_norm": 10.439050674438477,
      "learning_rate": 3.0198785628649885e-05,
      "loss": 1.8739,
      "step": 614400
    },
    {
      "epoch": 23.765324670302046,
      "grad_norm": 9.079019546508789,
      "learning_rate": 3.01955627747483e-05,
      "loss": 1.9311,
      "step": 614500
    },
    {
      "epoch": 23.76919209498395,
      "grad_norm": 12.474963188171387,
      "learning_rate": 3.0192339920846707e-05,
      "loss": 1.9302,
      "step": 614600
    },
    {
      "epoch": 23.773059519665853,
      "grad_norm": 12.313192367553711,
      "learning_rate": 3.0189117066945122e-05,
      "loss": 1.9292,
      "step": 614700
    },
    {
      "epoch": 23.77692694434776,
      "grad_norm": 10.804821968078613,
      "learning_rate": 3.0185894213043537e-05,
      "loss": 1.8965,
      "step": 614800
    },
    {
      "epoch": 23.780794369029664,
      "grad_norm": 9.398482322692871,
      "learning_rate": 3.018267135914195e-05,
      "loss": 1.8999,
      "step": 614900
    },
    {
      "epoch": 23.784661793711567,
      "grad_norm": 10.809781074523926,
      "learning_rate": 3.017944850524036e-05,
      "loss": 1.9601,
      "step": 615000
    },
    {
      "epoch": 23.78852921839347,
      "grad_norm": 11.782292366027832,
      "learning_rate": 3.0176225651338774e-05,
      "loss": 1.9075,
      "step": 615100
    },
    {
      "epoch": 23.792396643075378,
      "grad_norm": 13.11349868774414,
      "learning_rate": 3.017300279743719e-05,
      "loss": 1.8664,
      "step": 615200
    },
    {
      "epoch": 23.79626406775728,
      "grad_norm": 13.25147533416748,
      "learning_rate": 3.0169779943535604e-05,
      "loss": 1.8764,
      "step": 615300
    },
    {
      "epoch": 23.800131492439185,
      "grad_norm": 11.469949722290039,
      "learning_rate": 3.016655708963401e-05,
      "loss": 1.9343,
      "step": 615400
    },
    {
      "epoch": 23.80399891712109,
      "grad_norm": 19.30390739440918,
      "learning_rate": 3.0163334235732426e-05,
      "loss": 1.9355,
      "step": 615500
    },
    {
      "epoch": 23.807866341802992,
      "grad_norm": 11.688794136047363,
      "learning_rate": 3.016011138183084e-05,
      "loss": 2.0306,
      "step": 615600
    },
    {
      "epoch": 23.8117337664849,
      "grad_norm": 9.20025634765625,
      "learning_rate": 3.0156888527929256e-05,
      "loss": 1.9376,
      "step": 615700
    },
    {
      "epoch": 23.815601191166802,
      "grad_norm": 12.664337158203125,
      "learning_rate": 3.0153665674027664e-05,
      "loss": 1.9147,
      "step": 615800
    },
    {
      "epoch": 23.819468615848706,
      "grad_norm": 12.83159351348877,
      "learning_rate": 3.015044282012608e-05,
      "loss": 1.9188,
      "step": 615900
    },
    {
      "epoch": 23.82333604053061,
      "grad_norm": 9.078990936279297,
      "learning_rate": 3.0147219966224493e-05,
      "loss": 1.8675,
      "step": 616000
    },
    {
      "epoch": 23.827203465212516,
      "grad_norm": 11.752542495727539,
      "learning_rate": 3.0143997112322908e-05,
      "loss": 1.9146,
      "step": 616100
    },
    {
      "epoch": 23.83107088989442,
      "grad_norm": 15.082550048828125,
      "learning_rate": 3.0140774258421316e-05,
      "loss": 1.8884,
      "step": 616200
    },
    {
      "epoch": 23.834938314576323,
      "grad_norm": 11.150413513183594,
      "learning_rate": 3.013755140451973e-05,
      "loss": 1.865,
      "step": 616300
    },
    {
      "epoch": 23.838805739258227,
      "grad_norm": 10.286865234375,
      "learning_rate": 3.0134328550618145e-05,
      "loss": 1.9008,
      "step": 616400
    },
    {
      "epoch": 23.842673163940134,
      "grad_norm": 12.637125015258789,
      "learning_rate": 3.013110569671656e-05,
      "loss": 2.0358,
      "step": 616500
    },
    {
      "epoch": 23.846540588622037,
      "grad_norm": 11.478046417236328,
      "learning_rate": 3.0127882842814968e-05,
      "loss": 2.0093,
      "step": 616600
    },
    {
      "epoch": 23.85040801330394,
      "grad_norm": 20.471797943115234,
      "learning_rate": 3.0124659988913383e-05,
      "loss": 2.021,
      "step": 616700
    },
    {
      "epoch": 23.854275437985844,
      "grad_norm": 14.307507514953613,
      "learning_rate": 3.0121437135011797e-05,
      "loss": 1.9072,
      "step": 616800
    },
    {
      "epoch": 23.858142862667748,
      "grad_norm": 12.301807403564453,
      "learning_rate": 3.011821428111021e-05,
      "loss": 1.9396,
      "step": 616900
    },
    {
      "epoch": 23.862010287349655,
      "grad_norm": 11.804319381713867,
      "learning_rate": 3.0114991427208623e-05,
      "loss": 1.936,
      "step": 617000
    },
    {
      "epoch": 23.86587771203156,
      "grad_norm": 9.763543128967285,
      "learning_rate": 3.0111768573307038e-05,
      "loss": 1.9178,
      "step": 617100
    },
    {
      "epoch": 23.869745136713462,
      "grad_norm": 12.31003475189209,
      "learning_rate": 3.010854571940545e-05,
      "loss": 1.9635,
      "step": 617200
    },
    {
      "epoch": 23.873612561395365,
      "grad_norm": 14.951393127441406,
      "learning_rate": 3.010532286550386e-05,
      "loss": 1.9191,
      "step": 617300
    },
    {
      "epoch": 23.877479986077272,
      "grad_norm": 10.881134986877441,
      "learning_rate": 3.0102100011602275e-05,
      "loss": 1.9543,
      "step": 617400
    },
    {
      "epoch": 23.881347410759176,
      "grad_norm": 11.262552261352539,
      "learning_rate": 3.009887715770069e-05,
      "loss": 1.8924,
      "step": 617500
    },
    {
      "epoch": 23.88521483544108,
      "grad_norm": 11.835556983947754,
      "learning_rate": 3.0095654303799105e-05,
      "loss": 2.0853,
      "step": 617600
    },
    {
      "epoch": 23.889082260122983,
      "grad_norm": 9.361689567565918,
      "learning_rate": 3.0092431449897513e-05,
      "loss": 1.9543,
      "step": 617700
    },
    {
      "epoch": 23.89294968480489,
      "grad_norm": 14.07402515411377,
      "learning_rate": 3.0089208595995928e-05,
      "loss": 1.93,
      "step": 617800
    },
    {
      "epoch": 23.896817109486793,
      "grad_norm": 11.713255882263184,
      "learning_rate": 3.0085985742094342e-05,
      "loss": 1.9042,
      "step": 617900
    },
    {
      "epoch": 23.900684534168697,
      "grad_norm": 11.598801612854004,
      "learning_rate": 3.0082762888192757e-05,
      "loss": 1.9145,
      "step": 618000
    },
    {
      "epoch": 23.9045519588506,
      "grad_norm": 12.345815658569336,
      "learning_rate": 3.0079540034291165e-05,
      "loss": 1.9798,
      "step": 618100
    },
    {
      "epoch": 23.908419383532507,
      "grad_norm": 11.848281860351562,
      "learning_rate": 3.007631718038958e-05,
      "loss": 1.9428,
      "step": 618200
    },
    {
      "epoch": 23.91228680821441,
      "grad_norm": 11.398295402526855,
      "learning_rate": 3.0073094326487994e-05,
      "loss": 1.9514,
      "step": 618300
    },
    {
      "epoch": 23.916154232896314,
      "grad_norm": 11.765664100646973,
      "learning_rate": 3.006987147258641e-05,
      "loss": 1.8976,
      "step": 618400
    },
    {
      "epoch": 23.920021657578218,
      "grad_norm": 10.771308898925781,
      "learning_rate": 3.0066648618684817e-05,
      "loss": 2.0527,
      "step": 618500
    },
    {
      "epoch": 23.92388908226012,
      "grad_norm": 10.8596830368042,
      "learning_rate": 3.0063425764783232e-05,
      "loss": 1.9872,
      "step": 618600
    },
    {
      "epoch": 23.92775650694203,
      "grad_norm": 7.755762577056885,
      "learning_rate": 3.0060202910881646e-05,
      "loss": 1.9437,
      "step": 618700
    },
    {
      "epoch": 23.931623931623932,
      "grad_norm": 11.87186336517334,
      "learning_rate": 3.005698005698006e-05,
      "loss": 1.9217,
      "step": 618800
    },
    {
      "epoch": 23.935491356305835,
      "grad_norm": 12.437631607055664,
      "learning_rate": 3.005375720307847e-05,
      "loss": 1.9523,
      "step": 618900
    },
    {
      "epoch": 23.93935878098774,
      "grad_norm": 7.613861560821533,
      "learning_rate": 3.0050534349176884e-05,
      "loss": 2.0147,
      "step": 619000
    },
    {
      "epoch": 23.943226205669646,
      "grad_norm": 10.057343482971191,
      "learning_rate": 3.00473114952753e-05,
      "loss": 1.9364,
      "step": 619100
    },
    {
      "epoch": 23.94709363035155,
      "grad_norm": 16.79730987548828,
      "learning_rate": 3.0044088641373713e-05,
      "loss": 1.946,
      "step": 619200
    },
    {
      "epoch": 23.950961055033453,
      "grad_norm": 12.686100959777832,
      "learning_rate": 3.004086578747212e-05,
      "loss": 1.9474,
      "step": 619300
    },
    {
      "epoch": 23.954828479715356,
      "grad_norm": 13.305441856384277,
      "learning_rate": 3.0037642933570536e-05,
      "loss": 1.9607,
      "step": 619400
    },
    {
      "epoch": 23.958695904397263,
      "grad_norm": 11.068869590759277,
      "learning_rate": 3.003442007966895e-05,
      "loss": 1.9266,
      "step": 619500
    },
    {
      "epoch": 23.962563329079167,
      "grad_norm": 10.057092666625977,
      "learning_rate": 3.0031197225767365e-05,
      "loss": 1.9283,
      "step": 619600
    },
    {
      "epoch": 23.96643075376107,
      "grad_norm": 14.845276832580566,
      "learning_rate": 3.0027974371865773e-05,
      "loss": 1.9497,
      "step": 619700
    },
    {
      "epoch": 23.970298178442974,
      "grad_norm": 12.055340766906738,
      "learning_rate": 3.0024751517964188e-05,
      "loss": 2.0427,
      "step": 619800
    },
    {
      "epoch": 23.97416560312488,
      "grad_norm": 11.154781341552734,
      "learning_rate": 3.0021528664062603e-05,
      "loss": 1.86,
      "step": 619900
    },
    {
      "epoch": 23.978033027806784,
      "grad_norm": 12.701870918273926,
      "learning_rate": 3.0018305810161014e-05,
      "loss": 1.8867,
      "step": 620000
    },
    {
      "epoch": 23.981900452488688,
      "grad_norm": 9.549511909484863,
      "learning_rate": 3.0015082956259425e-05,
      "loss": 1.9071,
      "step": 620100
    },
    {
      "epoch": 23.98576787717059,
      "grad_norm": 14.997761726379395,
      "learning_rate": 3.001186010235784e-05,
      "loss": 1.9727,
      "step": 620200
    },
    {
      "epoch": 23.989635301852495,
      "grad_norm": 9.27330493927002,
      "learning_rate": 3.0008637248456255e-05,
      "loss": 1.9601,
      "step": 620300
    },
    {
      "epoch": 23.993502726534402,
      "grad_norm": 18.03535270690918,
      "learning_rate": 3.0005414394554666e-05,
      "loss": 2.0372,
      "step": 620400
    },
    {
      "epoch": 23.997370151216305,
      "grad_norm": 10.615900039672852,
      "learning_rate": 3.000219154065308e-05,
      "loss": 1.9954,
      "step": 620500
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.8350473642349243,
      "eval_runtime": 2.984,
      "eval_samples_per_second": 456.097,
      "eval_steps_per_second": 456.097,
      "step": 620568
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.7267158031463623,
      "eval_runtime": 57.3436,
      "eval_samples_per_second": 450.913,
      "eval_steps_per_second": 450.913,
      "step": 620568
    },
    {
      "epoch": 24.00123757589821,
      "grad_norm": 9.862686157226562,
      "learning_rate": 2.9998968686751496e-05,
      "loss": 1.8704,
      "step": 620600
    },
    {
      "epoch": 24.005105000580112,
      "grad_norm": 11.791900634765625,
      "learning_rate": 2.9995745832849907e-05,
      "loss": 1.8534,
      "step": 620700
    },
    {
      "epoch": 24.00897242526202,
      "grad_norm": 11.395930290222168,
      "learning_rate": 2.9992522978948318e-05,
      "loss": 1.9944,
      "step": 620800
    },
    {
      "epoch": 24.012839849943923,
      "grad_norm": 13.574162483215332,
      "learning_rate": 2.9989300125046733e-05,
      "loss": 2.0272,
      "step": 620900
    },
    {
      "epoch": 24.016707274625826,
      "grad_norm": 10.628472328186035,
      "learning_rate": 2.9986077271145148e-05,
      "loss": 1.9249,
      "step": 621000
    },
    {
      "epoch": 24.02057469930773,
      "grad_norm": 7.905666351318359,
      "learning_rate": 2.9982854417243562e-05,
      "loss": 1.8762,
      "step": 621100
    },
    {
      "epoch": 24.024442123989637,
      "grad_norm": 9.427042007446289,
      "learning_rate": 2.997963156334197e-05,
      "loss": 1.8318,
      "step": 621200
    },
    {
      "epoch": 24.02830954867154,
      "grad_norm": 9.777791023254395,
      "learning_rate": 2.9976408709440385e-05,
      "loss": 1.8584,
      "step": 621300
    },
    {
      "epoch": 24.032176973353444,
      "grad_norm": 13.478569030761719,
      "learning_rate": 2.99731858555388e-05,
      "loss": 1.8006,
      "step": 621400
    },
    {
      "epoch": 24.036044398035347,
      "grad_norm": 16.837480545043945,
      "learning_rate": 2.9969963001637215e-05,
      "loss": 1.9165,
      "step": 621500
    },
    {
      "epoch": 24.039911822717254,
      "grad_norm": 11.30403995513916,
      "learning_rate": 2.9966740147735622e-05,
      "loss": 1.8916,
      "step": 621600
    },
    {
      "epoch": 24.043779247399158,
      "grad_norm": 12.766064643859863,
      "learning_rate": 2.9963517293834037e-05,
      "loss": 1.9308,
      "step": 621700
    },
    {
      "epoch": 24.04764667208106,
      "grad_norm": 10.932448387145996,
      "learning_rate": 2.9960294439932452e-05,
      "loss": 1.9062,
      "step": 621800
    },
    {
      "epoch": 24.051514096762965,
      "grad_norm": 11.737322807312012,
      "learning_rate": 2.9957071586030867e-05,
      "loss": 2.0192,
      "step": 621900
    },
    {
      "epoch": 24.05538152144487,
      "grad_norm": 10.164874076843262,
      "learning_rate": 2.9953848732129275e-05,
      "loss": 1.87,
      "step": 622000
    },
    {
      "epoch": 24.059248946126775,
      "grad_norm": 10.292791366577148,
      "learning_rate": 2.995062587822769e-05,
      "loss": 1.8508,
      "step": 622100
    },
    {
      "epoch": 24.06311637080868,
      "grad_norm": 10.337624549865723,
      "learning_rate": 2.9947403024326104e-05,
      "loss": 1.9699,
      "step": 622200
    },
    {
      "epoch": 24.066983795490582,
      "grad_norm": 16.474594116210938,
      "learning_rate": 2.994418017042452e-05,
      "loss": 1.752,
      "step": 622300
    },
    {
      "epoch": 24.070851220172486,
      "grad_norm": 12.204733848571777,
      "learning_rate": 2.9940957316522927e-05,
      "loss": 1.8722,
      "step": 622400
    },
    {
      "epoch": 24.074718644854393,
      "grad_norm": 7.967617988586426,
      "learning_rate": 2.993773446262134e-05,
      "loss": 1.9587,
      "step": 622500
    },
    {
      "epoch": 24.078586069536296,
      "grad_norm": 12.14735221862793,
      "learning_rate": 2.9934511608719756e-05,
      "loss": 1.927,
      "step": 622600
    },
    {
      "epoch": 24.0824534942182,
      "grad_norm": 12.999506950378418,
      "learning_rate": 2.9931288754818164e-05,
      "loss": 1.9091,
      "step": 622700
    },
    {
      "epoch": 24.086320918900103,
      "grad_norm": 10.811800003051758,
      "learning_rate": 2.992806590091658e-05,
      "loss": 2.0384,
      "step": 622800
    },
    {
      "epoch": 24.09018834358201,
      "grad_norm": 8.037919998168945,
      "learning_rate": 2.9924843047014994e-05,
      "loss": 2.0296,
      "step": 622900
    },
    {
      "epoch": 24.094055768263914,
      "grad_norm": 14.194815635681152,
      "learning_rate": 2.9921620193113408e-05,
      "loss": 1.8465,
      "step": 623000
    },
    {
      "epoch": 24.097923192945817,
      "grad_norm": 10.139790534973145,
      "learning_rate": 2.9918397339211816e-05,
      "loss": 1.9243,
      "step": 623100
    },
    {
      "epoch": 24.10179061762772,
      "grad_norm": 12.331119537353516,
      "learning_rate": 2.991517448531023e-05,
      "loss": 1.8943,
      "step": 623200
    },
    {
      "epoch": 24.105658042309624,
      "grad_norm": 17.535371780395508,
      "learning_rate": 2.9911951631408646e-05,
      "loss": 1.9729,
      "step": 623300
    },
    {
      "epoch": 24.10952546699153,
      "grad_norm": 12.437600135803223,
      "learning_rate": 2.990872877750706e-05,
      "loss": 2.0139,
      "step": 623400
    },
    {
      "epoch": 24.113392891673435,
      "grad_norm": 9.872363090515137,
      "learning_rate": 2.990550592360547e-05,
      "loss": 1.8922,
      "step": 623500
    },
    {
      "epoch": 24.11726031635534,
      "grad_norm": 9.936777114868164,
      "learning_rate": 2.9902283069703886e-05,
      "loss": 2.0015,
      "step": 623600
    },
    {
      "epoch": 24.12112774103724,
      "grad_norm": 12.376137733459473,
      "learning_rate": 2.9899060215802298e-05,
      "loss": 1.8559,
      "step": 623700
    },
    {
      "epoch": 24.12499516571915,
      "grad_norm": 11.205794334411621,
      "learning_rate": 2.9895837361900712e-05,
      "loss": 2.0622,
      "step": 623800
    },
    {
      "epoch": 24.128862590401052,
      "grad_norm": 11.657971382141113,
      "learning_rate": 2.9892614507999124e-05,
      "loss": 1.9509,
      "step": 623900
    },
    {
      "epoch": 24.132730015082956,
      "grad_norm": 10.284214973449707,
      "learning_rate": 2.988939165409754e-05,
      "loss": 1.8765,
      "step": 624000
    },
    {
      "epoch": 24.13659743976486,
      "grad_norm": 12.210516929626465,
      "learning_rate": 2.9886168800195953e-05,
      "loss": 1.9671,
      "step": 624100
    },
    {
      "epoch": 24.140464864446766,
      "grad_norm": 9.928376197814941,
      "learning_rate": 2.9882945946294365e-05,
      "loss": 1.8914,
      "step": 624200
    },
    {
      "epoch": 24.14433228912867,
      "grad_norm": 11.5859375,
      "learning_rate": 2.9879723092392776e-05,
      "loss": 1.9386,
      "step": 624300
    },
    {
      "epoch": 24.148199713810573,
      "grad_norm": 10.683667182922363,
      "learning_rate": 2.987650023849119e-05,
      "loss": 1.9172,
      "step": 624400
    },
    {
      "epoch": 24.152067138492477,
      "grad_norm": 10.117612838745117,
      "learning_rate": 2.9873277384589605e-05,
      "loss": 1.8913,
      "step": 624500
    },
    {
      "epoch": 24.155934563174384,
      "grad_norm": 12.272972106933594,
      "learning_rate": 2.987005453068802e-05,
      "loss": 1.9886,
      "step": 624600
    },
    {
      "epoch": 24.159801987856287,
      "grad_norm": 10.916911125183105,
      "learning_rate": 2.9866831676786428e-05,
      "loss": 1.8649,
      "step": 624700
    },
    {
      "epoch": 24.16366941253819,
      "grad_norm": 11.350485801696777,
      "learning_rate": 2.9863608822884843e-05,
      "loss": 1.9061,
      "step": 624800
    },
    {
      "epoch": 24.167536837220094,
      "grad_norm": 12.899367332458496,
      "learning_rate": 2.9860385968983257e-05,
      "loss": 1.928,
      "step": 624900
    },
    {
      "epoch": 24.171404261901998,
      "grad_norm": 11.572989463806152,
      "learning_rate": 2.9857163115081672e-05,
      "loss": 1.9027,
      "step": 625000
    },
    {
      "epoch": 24.175271686583905,
      "grad_norm": 10.372188568115234,
      "learning_rate": 2.985394026118008e-05,
      "loss": 1.8885,
      "step": 625100
    },
    {
      "epoch": 24.17913911126581,
      "grad_norm": 12.281258583068848,
      "learning_rate": 2.9850717407278495e-05,
      "loss": 1.8781,
      "step": 625200
    },
    {
      "epoch": 24.18300653594771,
      "grad_norm": 10.951348304748535,
      "learning_rate": 2.984749455337691e-05,
      "loss": 1.8749,
      "step": 625300
    },
    {
      "epoch": 24.186873960629615,
      "grad_norm": 9.88352108001709,
      "learning_rate": 2.9844271699475324e-05,
      "loss": 1.9051,
      "step": 625400
    },
    {
      "epoch": 24.190741385311522,
      "grad_norm": 9.95029354095459,
      "learning_rate": 2.9841048845573732e-05,
      "loss": 1.9385,
      "step": 625500
    },
    {
      "epoch": 24.194608809993426,
      "grad_norm": 11.572073936462402,
      "learning_rate": 2.9837825991672147e-05,
      "loss": 1.9032,
      "step": 625600
    },
    {
      "epoch": 24.19847623467533,
      "grad_norm": 11.807558059692383,
      "learning_rate": 2.983460313777056e-05,
      "loss": 1.9369,
      "step": 625700
    },
    {
      "epoch": 24.202343659357233,
      "grad_norm": 10.382061958312988,
      "learning_rate": 2.983138028386897e-05,
      "loss": 1.9082,
      "step": 625800
    },
    {
      "epoch": 24.20621108403914,
      "grad_norm": 15.819867134094238,
      "learning_rate": 2.9828157429967384e-05,
      "loss": 1.9209,
      "step": 625900
    },
    {
      "epoch": 24.210078508721043,
      "grad_norm": 11.496562004089355,
      "learning_rate": 2.98249345760658e-05,
      "loss": 1.97,
      "step": 626000
    },
    {
      "epoch": 24.213945933402947,
      "grad_norm": 11.345792770385742,
      "learning_rate": 2.9821711722164214e-05,
      "loss": 1.9796,
      "step": 626100
    },
    {
      "epoch": 24.21781335808485,
      "grad_norm": 10.443910598754883,
      "learning_rate": 2.981848886826262e-05,
      "loss": 1.9084,
      "step": 626200
    },
    {
      "epoch": 24.221680782766757,
      "grad_norm": 10.641298294067383,
      "learning_rate": 2.9815266014361036e-05,
      "loss": 1.8678,
      "step": 626300
    },
    {
      "epoch": 24.22554820744866,
      "grad_norm": 9.733983993530273,
      "learning_rate": 2.981204316045945e-05,
      "loss": 1.8559,
      "step": 626400
    },
    {
      "epoch": 24.229415632130564,
      "grad_norm": 9.178678512573242,
      "learning_rate": 2.9808820306557866e-05,
      "loss": 1.9409,
      "step": 626500
    },
    {
      "epoch": 24.233283056812468,
      "grad_norm": 9.655515670776367,
      "learning_rate": 2.9805597452656274e-05,
      "loss": 1.8787,
      "step": 626600
    },
    {
      "epoch": 24.23715048149437,
      "grad_norm": 11.960797309875488,
      "learning_rate": 2.980237459875469e-05,
      "loss": 1.9875,
      "step": 626700
    },
    {
      "epoch": 24.24101790617628,
      "grad_norm": 8.788390159606934,
      "learning_rate": 2.9799151744853103e-05,
      "loss": 1.8562,
      "step": 626800
    },
    {
      "epoch": 24.24488533085818,
      "grad_norm": 12.151947975158691,
      "learning_rate": 2.9795928890951518e-05,
      "loss": 1.9193,
      "step": 626900
    },
    {
      "epoch": 24.248752755540085,
      "grad_norm": 11.532279968261719,
      "learning_rate": 2.979270603704993e-05,
      "loss": 2.0028,
      "step": 627000
    },
    {
      "epoch": 24.25262018022199,
      "grad_norm": 12.823039054870605,
      "learning_rate": 2.9789483183148344e-05,
      "loss": 1.9791,
      "step": 627100
    },
    {
      "epoch": 24.256487604903896,
      "grad_norm": 11.837945938110352,
      "learning_rate": 2.9786260329246755e-05,
      "loss": 1.9558,
      "step": 627200
    },
    {
      "epoch": 24.2603550295858,
      "grad_norm": 11.516345977783203,
      "learning_rate": 2.978303747534517e-05,
      "loss": 1.987,
      "step": 627300
    },
    {
      "epoch": 24.264222454267703,
      "grad_norm": 14.636873245239258,
      "learning_rate": 2.977981462144358e-05,
      "loss": 1.9078,
      "step": 627400
    },
    {
      "epoch": 24.268089878949606,
      "grad_norm": 11.885490417480469,
      "learning_rate": 2.9776591767541996e-05,
      "loss": 1.871,
      "step": 627500
    },
    {
      "epoch": 24.271957303631513,
      "grad_norm": 10.959979057312012,
      "learning_rate": 2.977336891364041e-05,
      "loss": 1.8709,
      "step": 627600
    },
    {
      "epoch": 24.275824728313417,
      "grad_norm": 15.16318416595459,
      "learning_rate": 2.9770146059738825e-05,
      "loss": 1.9245,
      "step": 627700
    },
    {
      "epoch": 24.27969215299532,
      "grad_norm": 12.556910514831543,
      "learning_rate": 2.9766923205837233e-05,
      "loss": 1.9127,
      "step": 627800
    },
    {
      "epoch": 24.283559577677224,
      "grad_norm": 12.06483268737793,
      "learning_rate": 2.9763700351935648e-05,
      "loss": 1.8634,
      "step": 627900
    },
    {
      "epoch": 24.28742700235913,
      "grad_norm": 8.996109962463379,
      "learning_rate": 2.9760477498034063e-05,
      "loss": 1.8923,
      "step": 628000
    },
    {
      "epoch": 24.291294427041034,
      "grad_norm": 11.933485984802246,
      "learning_rate": 2.9757254644132478e-05,
      "loss": 1.9036,
      "step": 628100
    },
    {
      "epoch": 24.295161851722938,
      "grad_norm": 8.955997467041016,
      "learning_rate": 2.9754031790230885e-05,
      "loss": 1.8487,
      "step": 628200
    },
    {
      "epoch": 24.29902927640484,
      "grad_norm": 12.142707824707031,
      "learning_rate": 2.97508089363293e-05,
      "loss": 1.861,
      "step": 628300
    },
    {
      "epoch": 24.302896701086745,
      "grad_norm": 11.724360466003418,
      "learning_rate": 2.9747586082427715e-05,
      "loss": 1.9262,
      "step": 628400
    },
    {
      "epoch": 24.30676412576865,
      "grad_norm": 12.011828422546387,
      "learning_rate": 2.9744363228526123e-05,
      "loss": 1.813,
      "step": 628500
    },
    {
      "epoch": 24.310631550450555,
      "grad_norm": 10.128535270690918,
      "learning_rate": 2.9741140374624538e-05,
      "loss": 1.9194,
      "step": 628600
    },
    {
      "epoch": 24.31449897513246,
      "grad_norm": 8.706751823425293,
      "learning_rate": 2.9737917520722952e-05,
      "loss": 1.9579,
      "step": 628700
    },
    {
      "epoch": 24.318366399814362,
      "grad_norm": 18.133657455444336,
      "learning_rate": 2.9734694666821367e-05,
      "loss": 1.9422,
      "step": 628800
    },
    {
      "epoch": 24.32223382449627,
      "grad_norm": 13.637792587280273,
      "learning_rate": 2.9731471812919775e-05,
      "loss": 1.8838,
      "step": 628900
    },
    {
      "epoch": 24.326101249178173,
      "grad_norm": 14.044381141662598,
      "learning_rate": 2.972824895901819e-05,
      "loss": 1.9979,
      "step": 629000
    },
    {
      "epoch": 24.329968673860076,
      "grad_norm": 13.845101356506348,
      "learning_rate": 2.9725026105116604e-05,
      "loss": 1.9393,
      "step": 629100
    },
    {
      "epoch": 24.33383609854198,
      "grad_norm": 14.4876070022583,
      "learning_rate": 2.972180325121502e-05,
      "loss": 1.9103,
      "step": 629200
    },
    {
      "epoch": 24.337703523223887,
      "grad_norm": 10.957061767578125,
      "learning_rate": 2.9718580397313427e-05,
      "loss": 1.943,
      "step": 629300
    },
    {
      "epoch": 24.34157094790579,
      "grad_norm": 11.572601318359375,
      "learning_rate": 2.9715357543411842e-05,
      "loss": 2.01,
      "step": 629400
    },
    {
      "epoch": 24.345438372587694,
      "grad_norm": 12.105769157409668,
      "learning_rate": 2.9712134689510256e-05,
      "loss": 1.929,
      "step": 629500
    },
    {
      "epoch": 24.349305797269597,
      "grad_norm": 12.641583442687988,
      "learning_rate": 2.970891183560867e-05,
      "loss": 1.9243,
      "step": 629600
    },
    {
      "epoch": 24.3531732219515,
      "grad_norm": 10.177271842956543,
      "learning_rate": 2.970568898170708e-05,
      "loss": 1.8443,
      "step": 629700
    },
    {
      "epoch": 24.357040646633408,
      "grad_norm": 11.112250328063965,
      "learning_rate": 2.9702466127805494e-05,
      "loss": 1.9717,
      "step": 629800
    },
    {
      "epoch": 24.36090807131531,
      "grad_norm": 11.369294166564941,
      "learning_rate": 2.969924327390391e-05,
      "loss": 1.9047,
      "step": 629900
    },
    {
      "epoch": 24.364775495997215,
      "grad_norm": 12.110970497131348,
      "learning_rate": 2.9696020420002323e-05,
      "loss": 1.8712,
      "step": 630000
    },
    {
      "epoch": 24.368642920679118,
      "grad_norm": 9.271222114562988,
      "learning_rate": 2.969279756610073e-05,
      "loss": 1.9567,
      "step": 630100
    },
    {
      "epoch": 24.372510345361025,
      "grad_norm": 13.038156509399414,
      "learning_rate": 2.9689574712199146e-05,
      "loss": 1.8802,
      "step": 630200
    },
    {
      "epoch": 24.37637777004293,
      "grad_norm": 14.10823917388916,
      "learning_rate": 2.968635185829756e-05,
      "loss": 1.9012,
      "step": 630300
    },
    {
      "epoch": 24.380245194724832,
      "grad_norm": 13.068326950073242,
      "learning_rate": 2.9683129004395975e-05,
      "loss": 1.934,
      "step": 630400
    },
    {
      "epoch": 24.384112619406736,
      "grad_norm": 8.147910118103027,
      "learning_rate": 2.9679906150494387e-05,
      "loss": 1.9262,
      "step": 630500
    },
    {
      "epoch": 24.387980044088643,
      "grad_norm": 10.363069534301758,
      "learning_rate": 2.96766832965928e-05,
      "loss": 1.8411,
      "step": 630600
    },
    {
      "epoch": 24.391847468770546,
      "grad_norm": 12.679045677185059,
      "learning_rate": 2.9673460442691213e-05,
      "loss": 1.963,
      "step": 630700
    },
    {
      "epoch": 24.39571489345245,
      "grad_norm": 9.796772956848145,
      "learning_rate": 2.9670237588789628e-05,
      "loss": 1.9297,
      "step": 630800
    },
    {
      "epoch": 24.399582318134353,
      "grad_norm": 14.130754470825195,
      "learning_rate": 2.966701473488804e-05,
      "loss": 1.8531,
      "step": 630900
    },
    {
      "epoch": 24.40344974281626,
      "grad_norm": 10.381528854370117,
      "learning_rate": 2.9663791880986454e-05,
      "loss": 1.8148,
      "step": 631000
    },
    {
      "epoch": 24.407317167498164,
      "grad_norm": 7.100360870361328,
      "learning_rate": 2.9660569027084868e-05,
      "loss": 1.9501,
      "step": 631100
    },
    {
      "epoch": 24.411184592180067,
      "grad_norm": 11.557506561279297,
      "learning_rate": 2.9657346173183283e-05,
      "loss": 2.0166,
      "step": 631200
    },
    {
      "epoch": 24.41505201686197,
      "grad_norm": 13.5043363571167,
      "learning_rate": 2.965412331928169e-05,
      "loss": 1.8632,
      "step": 631300
    },
    {
      "epoch": 24.418919441543878,
      "grad_norm": 11.073205947875977,
      "learning_rate": 2.9650900465380106e-05,
      "loss": 1.9482,
      "step": 631400
    },
    {
      "epoch": 24.42278686622578,
      "grad_norm": 10.811500549316406,
      "learning_rate": 2.964767761147852e-05,
      "loss": 1.9729,
      "step": 631500
    },
    {
      "epoch": 24.426654290907685,
      "grad_norm": 11.07588005065918,
      "learning_rate": 2.964445475757693e-05,
      "loss": 1.9483,
      "step": 631600
    },
    {
      "epoch": 24.430521715589588,
      "grad_norm": 11.206698417663574,
      "learning_rate": 2.9641231903675343e-05,
      "loss": 1.869,
      "step": 631700
    },
    {
      "epoch": 24.43438914027149,
      "grad_norm": 11.693953514099121,
      "learning_rate": 2.9638009049773758e-05,
      "loss": 1.947,
      "step": 631800
    },
    {
      "epoch": 24.4382565649534,
      "grad_norm": 13.680398941040039,
      "learning_rate": 2.9634786195872172e-05,
      "loss": 1.9996,
      "step": 631900
    },
    {
      "epoch": 24.442123989635302,
      "grad_norm": 11.2418212890625,
      "learning_rate": 2.963156334197058e-05,
      "loss": 1.8845,
      "step": 632000
    },
    {
      "epoch": 24.445991414317206,
      "grad_norm": 11.860308647155762,
      "learning_rate": 2.9628340488068995e-05,
      "loss": 1.989,
      "step": 632100
    },
    {
      "epoch": 24.44985883899911,
      "grad_norm": 10.470290184020996,
      "learning_rate": 2.962511763416741e-05,
      "loss": 1.9255,
      "step": 632200
    },
    {
      "epoch": 24.453726263681016,
      "grad_norm": 14.743673324584961,
      "learning_rate": 2.9621894780265825e-05,
      "loss": 1.9259,
      "step": 632300
    },
    {
      "epoch": 24.45759368836292,
      "grad_norm": 15.89249324798584,
      "learning_rate": 2.9618671926364233e-05,
      "loss": 1.9718,
      "step": 632400
    },
    {
      "epoch": 24.461461113044823,
      "grad_norm": 9.652631759643555,
      "learning_rate": 2.9615449072462647e-05,
      "loss": 1.9665,
      "step": 632500
    },
    {
      "epoch": 24.465328537726727,
      "grad_norm": 11.667583465576172,
      "learning_rate": 2.9612226218561062e-05,
      "loss": 1.9494,
      "step": 632600
    },
    {
      "epoch": 24.469195962408634,
      "grad_norm": 10.908443450927734,
      "learning_rate": 2.9609003364659477e-05,
      "loss": 1.9061,
      "step": 632700
    },
    {
      "epoch": 24.473063387090537,
      "grad_norm": 13.494609832763672,
      "learning_rate": 2.9605780510757885e-05,
      "loss": 1.9554,
      "step": 632800
    },
    {
      "epoch": 24.47693081177244,
      "grad_norm": 11.993178367614746,
      "learning_rate": 2.96025576568563e-05,
      "loss": 1.9631,
      "step": 632900
    },
    {
      "epoch": 24.480798236454344,
      "grad_norm": 8.293231010437012,
      "learning_rate": 2.9599334802954714e-05,
      "loss": 1.8517,
      "step": 633000
    },
    {
      "epoch": 24.484665661136248,
      "grad_norm": 10.755790710449219,
      "learning_rate": 2.959611194905313e-05,
      "loss": 1.8606,
      "step": 633100
    },
    {
      "epoch": 24.488533085818155,
      "grad_norm": 11.667804718017578,
      "learning_rate": 2.9592889095151537e-05,
      "loss": 1.9156,
      "step": 633200
    },
    {
      "epoch": 24.492400510500058,
      "grad_norm": 13.10517692565918,
      "learning_rate": 2.958966624124995e-05,
      "loss": 1.853,
      "step": 633300
    },
    {
      "epoch": 24.49626793518196,
      "grad_norm": 12.340086936950684,
      "learning_rate": 2.9586443387348366e-05,
      "loss": 1.9186,
      "step": 633400
    },
    {
      "epoch": 24.500135359863865,
      "grad_norm": 9.001893997192383,
      "learning_rate": 2.958322053344678e-05,
      "loss": 1.846,
      "step": 633500
    },
    {
      "epoch": 24.504002784545772,
      "grad_norm": 11.026748657226562,
      "learning_rate": 2.9579997679545192e-05,
      "loss": 1.8935,
      "step": 633600
    },
    {
      "epoch": 24.507870209227676,
      "grad_norm": 13.320302963256836,
      "learning_rate": 2.9576774825643604e-05,
      "loss": 1.8841,
      "step": 633700
    },
    {
      "epoch": 24.51173763390958,
      "grad_norm": 12.15826416015625,
      "learning_rate": 2.9573551971742018e-05,
      "loss": 1.9332,
      "step": 633800
    },
    {
      "epoch": 24.515605058591483,
      "grad_norm": 13.356429100036621,
      "learning_rate": 2.9570329117840433e-05,
      "loss": 1.877,
      "step": 633900
    },
    {
      "epoch": 24.51947248327339,
      "grad_norm": 12.176788330078125,
      "learning_rate": 2.9567106263938844e-05,
      "loss": 1.938,
      "step": 634000
    },
    {
      "epoch": 24.523339907955293,
      "grad_norm": 13.951658248901367,
      "learning_rate": 2.956388341003726e-05,
      "loss": 1.9104,
      "step": 634100
    },
    {
      "epoch": 24.527207332637197,
      "grad_norm": 15.29089069366455,
      "learning_rate": 2.956066055613567e-05,
      "loss": 1.9407,
      "step": 634200
    },
    {
      "epoch": 24.5310747573191,
      "grad_norm": 11.990694046020508,
      "learning_rate": 2.9557437702234085e-05,
      "loss": 1.9906,
      "step": 634300
    },
    {
      "epoch": 24.534942182001007,
      "grad_norm": 10.683087348937988,
      "learning_rate": 2.9554214848332496e-05,
      "loss": 1.9689,
      "step": 634400
    },
    {
      "epoch": 24.53880960668291,
      "grad_norm": 9.523512840270996,
      "learning_rate": 2.955099199443091e-05,
      "loss": 1.8884,
      "step": 634500
    },
    {
      "epoch": 24.542677031364814,
      "grad_norm": 9.698379516601562,
      "learning_rate": 2.9547769140529326e-05,
      "loss": 1.8692,
      "step": 634600
    },
    {
      "epoch": 24.546544456046718,
      "grad_norm": 11.162995338439941,
      "learning_rate": 2.9544546286627734e-05,
      "loss": 1.9663,
      "step": 634700
    },
    {
      "epoch": 24.55041188072862,
      "grad_norm": 9.958077430725098,
      "learning_rate": 2.954132343272615e-05,
      "loss": 1.97,
      "step": 634800
    },
    {
      "epoch": 24.554279305410528,
      "grad_norm": 11.589990615844727,
      "learning_rate": 2.9538100578824563e-05,
      "loss": 1.9426,
      "step": 634900
    },
    {
      "epoch": 24.55814673009243,
      "grad_norm": 10.685566902160645,
      "learning_rate": 2.9534877724922978e-05,
      "loss": 1.9114,
      "step": 635000
    },
    {
      "epoch": 24.562014154774335,
      "grad_norm": 10.881525039672852,
      "learning_rate": 2.9531654871021386e-05,
      "loss": 1.9311,
      "step": 635100
    },
    {
      "epoch": 24.56588157945624,
      "grad_norm": 14.786526679992676,
      "learning_rate": 2.95284320171198e-05,
      "loss": 1.9278,
      "step": 635200
    },
    {
      "epoch": 24.569749004138146,
      "grad_norm": 12.642601013183594,
      "learning_rate": 2.9525209163218215e-05,
      "loss": 1.8975,
      "step": 635300
    },
    {
      "epoch": 24.57361642882005,
      "grad_norm": 12.034262657165527,
      "learning_rate": 2.952198630931663e-05,
      "loss": 2.0445,
      "step": 635400
    },
    {
      "epoch": 24.577483853501953,
      "grad_norm": 10.473280906677246,
      "learning_rate": 2.9518763455415038e-05,
      "loss": 1.8967,
      "step": 635500
    },
    {
      "epoch": 24.581351278183856,
      "grad_norm": 11.572813987731934,
      "learning_rate": 2.9515540601513453e-05,
      "loss": 1.9425,
      "step": 635600
    },
    {
      "epoch": 24.585218702865763,
      "grad_norm": 10.239384651184082,
      "learning_rate": 2.9512317747611867e-05,
      "loss": 1.8363,
      "step": 635700
    },
    {
      "epoch": 24.589086127547667,
      "grad_norm": 16.153291702270508,
      "learning_rate": 2.9509094893710282e-05,
      "loss": 1.9837,
      "step": 635800
    },
    {
      "epoch": 24.59295355222957,
      "grad_norm": 13.457636833190918,
      "learning_rate": 2.950587203980869e-05,
      "loss": 1.915,
      "step": 635900
    },
    {
      "epoch": 24.596820976911474,
      "grad_norm": 8.975634574890137,
      "learning_rate": 2.9502649185907105e-05,
      "loss": 1.9129,
      "step": 636000
    },
    {
      "epoch": 24.60068840159338,
      "grad_norm": 10.686858177185059,
      "learning_rate": 2.949942633200552e-05,
      "loss": 2.0135,
      "step": 636100
    },
    {
      "epoch": 24.604555826275284,
      "grad_norm": 10.157376289367676,
      "learning_rate": 2.9496203478103934e-05,
      "loss": 1.9066,
      "step": 636200
    },
    {
      "epoch": 24.608423250957188,
      "grad_norm": 12.322555541992188,
      "learning_rate": 2.9492980624202342e-05,
      "loss": 1.7781,
      "step": 636300
    },
    {
      "epoch": 24.61229067563909,
      "grad_norm": 11.057071685791016,
      "learning_rate": 2.9489757770300757e-05,
      "loss": 1.9742,
      "step": 636400
    },
    {
      "epoch": 24.616158100320995,
      "grad_norm": 11.999028205871582,
      "learning_rate": 2.948653491639917e-05,
      "loss": 2.0211,
      "step": 636500
    },
    {
      "epoch": 24.6200255250029,
      "grad_norm": 7.593944072723389,
      "learning_rate": 2.9483312062497586e-05,
      "loss": 1.949,
      "step": 636600
    },
    {
      "epoch": 24.623892949684805,
      "grad_norm": 8.662504196166992,
      "learning_rate": 2.9480089208595994e-05,
      "loss": 1.9653,
      "step": 636700
    },
    {
      "epoch": 24.62776037436671,
      "grad_norm": 10.493558883666992,
      "learning_rate": 2.947686635469441e-05,
      "loss": 1.8915,
      "step": 636800
    },
    {
      "epoch": 24.631627799048612,
      "grad_norm": 9.689637184143066,
      "learning_rate": 2.9473643500792824e-05,
      "loss": 1.8854,
      "step": 636900
    },
    {
      "epoch": 24.63549522373052,
      "grad_norm": 9.935234069824219,
      "learning_rate": 2.947042064689124e-05,
      "loss": 1.9723,
      "step": 637000
    },
    {
      "epoch": 24.639362648412423,
      "grad_norm": 11.605478286743164,
      "learning_rate": 2.946719779298965e-05,
      "loss": 1.9284,
      "step": 637100
    },
    {
      "epoch": 24.643230073094326,
      "grad_norm": 9.977714538574219,
      "learning_rate": 2.946397493908806e-05,
      "loss": 1.9447,
      "step": 637200
    },
    {
      "epoch": 24.64709749777623,
      "grad_norm": 11.679163932800293,
      "learning_rate": 2.9460752085186476e-05,
      "loss": 1.8774,
      "step": 637300
    },
    {
      "epoch": 24.650964922458137,
      "grad_norm": 12.297719955444336,
      "learning_rate": 2.9457529231284887e-05,
      "loss": 1.9375,
      "step": 637400
    },
    {
      "epoch": 24.65483234714004,
      "grad_norm": 10.126336097717285,
      "learning_rate": 2.9454306377383302e-05,
      "loss": 1.9282,
      "step": 637500
    },
    {
      "epoch": 24.658699771821944,
      "grad_norm": 11.905830383300781,
      "learning_rate": 2.9451083523481717e-05,
      "loss": 1.8054,
      "step": 637600
    },
    {
      "epoch": 24.662567196503847,
      "grad_norm": 11.392614364624023,
      "learning_rate": 2.9447860669580128e-05,
      "loss": 1.9257,
      "step": 637700
    },
    {
      "epoch": 24.66643462118575,
      "grad_norm": 10.6311674118042,
      "learning_rate": 2.944463781567854e-05,
      "loss": 1.8366,
      "step": 637800
    },
    {
      "epoch": 24.670302045867658,
      "grad_norm": 11.62946605682373,
      "learning_rate": 2.9441414961776954e-05,
      "loss": 1.9226,
      "step": 637900
    },
    {
      "epoch": 24.67416947054956,
      "grad_norm": 12.131521224975586,
      "learning_rate": 2.943819210787537e-05,
      "loss": 1.8987,
      "step": 638000
    },
    {
      "epoch": 24.678036895231465,
      "grad_norm": 9.72515869140625,
      "learning_rate": 2.9434969253973783e-05,
      "loss": 1.9242,
      "step": 638100
    },
    {
      "epoch": 24.681904319913368,
      "grad_norm": 12.894661903381348,
      "learning_rate": 2.943174640007219e-05,
      "loss": 1.894,
      "step": 638200
    },
    {
      "epoch": 24.685771744595275,
      "grad_norm": 10.9190034866333,
      "learning_rate": 2.9428523546170606e-05,
      "loss": 1.9518,
      "step": 638300
    },
    {
      "epoch": 24.68963916927718,
      "grad_norm": 11.910819053649902,
      "learning_rate": 2.942530069226902e-05,
      "loss": 1.9174,
      "step": 638400
    },
    {
      "epoch": 24.693506593959082,
      "grad_norm": 12.185574531555176,
      "learning_rate": 2.9422077838367435e-05,
      "loss": 1.9933,
      "step": 638500
    },
    {
      "epoch": 24.697374018640986,
      "grad_norm": 11.740331649780273,
      "learning_rate": 2.9418854984465843e-05,
      "loss": 1.922,
      "step": 638600
    },
    {
      "epoch": 24.701241443322893,
      "grad_norm": 11.615517616271973,
      "learning_rate": 2.9415632130564258e-05,
      "loss": 1.8636,
      "step": 638700
    },
    {
      "epoch": 24.705108868004796,
      "grad_norm": 16.38825035095215,
      "learning_rate": 2.9412409276662673e-05,
      "loss": 1.8912,
      "step": 638800
    },
    {
      "epoch": 24.7089762926867,
      "grad_norm": 16.220394134521484,
      "learning_rate": 2.9409186422761088e-05,
      "loss": 1.9148,
      "step": 638900
    },
    {
      "epoch": 24.712843717368603,
      "grad_norm": 10.249483108520508,
      "learning_rate": 2.9405963568859496e-05,
      "loss": 1.9175,
      "step": 639000
    },
    {
      "epoch": 24.71671114205051,
      "grad_norm": 12.268861770629883,
      "learning_rate": 2.940274071495791e-05,
      "loss": 1.9519,
      "step": 639100
    },
    {
      "epoch": 24.720578566732414,
      "grad_norm": 11.501108169555664,
      "learning_rate": 2.9399517861056325e-05,
      "loss": 1.9072,
      "step": 639200
    },
    {
      "epoch": 24.724445991414317,
      "grad_norm": 10.377958297729492,
      "learning_rate": 2.939629500715474e-05,
      "loss": 1.9562,
      "step": 639300
    },
    {
      "epoch": 24.72831341609622,
      "grad_norm": 13.651962280273438,
      "learning_rate": 2.9393072153253148e-05,
      "loss": 1.8494,
      "step": 639400
    },
    {
      "epoch": 24.732180840778128,
      "grad_norm": 11.814162254333496,
      "learning_rate": 2.9389849299351562e-05,
      "loss": 1.9077,
      "step": 639500
    },
    {
      "epoch": 24.73604826546003,
      "grad_norm": 14.312699317932129,
      "learning_rate": 2.9386626445449977e-05,
      "loss": 2.03,
      "step": 639600
    },
    {
      "epoch": 24.739915690141935,
      "grad_norm": 9.86036491394043,
      "learning_rate": 2.9383403591548392e-05,
      "loss": 1.9228,
      "step": 639700
    },
    {
      "epoch": 24.743783114823838,
      "grad_norm": 9.840469360351562,
      "learning_rate": 2.93801807376468e-05,
      "loss": 1.9251,
      "step": 639800
    },
    {
      "epoch": 24.74765053950574,
      "grad_norm": 10.093948364257812,
      "learning_rate": 2.9376957883745214e-05,
      "loss": 1.9041,
      "step": 639900
    },
    {
      "epoch": 24.75151796418765,
      "grad_norm": 12.734726905822754,
      "learning_rate": 2.937373502984363e-05,
      "loss": 2.0316,
      "step": 640000
    },
    {
      "epoch": 24.755385388869552,
      "grad_norm": 11.755725860595703,
      "learning_rate": 2.9370512175942044e-05,
      "loss": 2.0085,
      "step": 640100
    },
    {
      "epoch": 24.759252813551456,
      "grad_norm": 10.260049819946289,
      "learning_rate": 2.9367289322040452e-05,
      "loss": 1.9601,
      "step": 640200
    },
    {
      "epoch": 24.76312023823336,
      "grad_norm": 10.742199897766113,
      "learning_rate": 2.9364066468138867e-05,
      "loss": 2.0086,
      "step": 640300
    },
    {
      "epoch": 24.766987662915266,
      "grad_norm": 14.171218872070312,
      "learning_rate": 2.936084361423728e-05,
      "loss": 1.9656,
      "step": 640400
    },
    {
      "epoch": 24.77085508759717,
      "grad_norm": 11.622735977172852,
      "learning_rate": 2.9357620760335693e-05,
      "loss": 2.0249,
      "step": 640500
    },
    {
      "epoch": 24.774722512279073,
      "grad_norm": 9.44349193572998,
      "learning_rate": 2.9354397906434107e-05,
      "loss": 1.9192,
      "step": 640600
    },
    {
      "epoch": 24.778589936960977,
      "grad_norm": 12.198981285095215,
      "learning_rate": 2.935117505253252e-05,
      "loss": 1.9789,
      "step": 640700
    },
    {
      "epoch": 24.782457361642884,
      "grad_norm": 11.345090866088867,
      "learning_rate": 2.9347952198630933e-05,
      "loss": 1.9415,
      "step": 640800
    },
    {
      "epoch": 24.786324786324787,
      "grad_norm": 11.2763032913208,
      "learning_rate": 2.9344729344729345e-05,
      "loss": 1.9322,
      "step": 640900
    },
    {
      "epoch": 24.79019221100669,
      "grad_norm": 10.955979347229004,
      "learning_rate": 2.934150649082776e-05,
      "loss": 1.873,
      "step": 641000
    },
    {
      "epoch": 24.794059635688594,
      "grad_norm": 13.499302864074707,
      "learning_rate": 2.9338283636926174e-05,
      "loss": 1.8503,
      "step": 641100
    },
    {
      "epoch": 24.797927060370498,
      "grad_norm": 11.873401641845703,
      "learning_rate": 2.933506078302459e-05,
      "loss": 2.0626,
      "step": 641200
    },
    {
      "epoch": 24.801794485052405,
      "grad_norm": 11.46214771270752,
      "learning_rate": 2.9331837929122997e-05,
      "loss": 1.8996,
      "step": 641300
    },
    {
      "epoch": 24.805661909734308,
      "grad_norm": 10.954554557800293,
      "learning_rate": 2.932861507522141e-05,
      "loss": 1.9911,
      "step": 641400
    },
    {
      "epoch": 24.80952933441621,
      "grad_norm": 9.815448760986328,
      "learning_rate": 2.9325392221319826e-05,
      "loss": 1.8586,
      "step": 641500
    },
    {
      "epoch": 24.813396759098115,
      "grad_norm": 9.218300819396973,
      "learning_rate": 2.932216936741824e-05,
      "loss": 1.8795,
      "step": 641600
    },
    {
      "epoch": 24.817264183780022,
      "grad_norm": 11.883923530578613,
      "learning_rate": 2.931894651351665e-05,
      "loss": 1.8821,
      "step": 641700
    },
    {
      "epoch": 24.821131608461926,
      "grad_norm": 9.703805923461914,
      "learning_rate": 2.9315723659615064e-05,
      "loss": 1.9898,
      "step": 641800
    },
    {
      "epoch": 24.82499903314383,
      "grad_norm": 12.575380325317383,
      "learning_rate": 2.9312500805713478e-05,
      "loss": 1.8593,
      "step": 641900
    },
    {
      "epoch": 24.828866457825733,
      "grad_norm": 10.395145416259766,
      "learning_rate": 2.9309277951811893e-05,
      "loss": 1.8914,
      "step": 642000
    },
    {
      "epoch": 24.83273388250764,
      "grad_norm": 9.91580581665039,
      "learning_rate": 2.93060550979103e-05,
      "loss": 2.0761,
      "step": 642100
    },
    {
      "epoch": 24.836601307189543,
      "grad_norm": 12.061068534851074,
      "learning_rate": 2.9302832244008716e-05,
      "loss": 1.7984,
      "step": 642200
    },
    {
      "epoch": 24.840468731871447,
      "grad_norm": 10.669514656066895,
      "learning_rate": 2.929960939010713e-05,
      "loss": 1.9252,
      "step": 642300
    },
    {
      "epoch": 24.84433615655335,
      "grad_norm": 11.325336456298828,
      "learning_rate": 2.9296386536205545e-05,
      "loss": 1.9634,
      "step": 642400
    },
    {
      "epoch": 24.848203581235257,
      "grad_norm": 15.311784744262695,
      "learning_rate": 2.9293163682303953e-05,
      "loss": 2.0421,
      "step": 642500
    },
    {
      "epoch": 24.85207100591716,
      "grad_norm": 13.179986000061035,
      "learning_rate": 2.9289940828402368e-05,
      "loss": 1.9356,
      "step": 642600
    },
    {
      "epoch": 24.855938430599064,
      "grad_norm": 14.01297378540039,
      "learning_rate": 2.9286717974500782e-05,
      "loss": 1.9556,
      "step": 642700
    },
    {
      "epoch": 24.859805855280968,
      "grad_norm": 12.295361518859863,
      "learning_rate": 2.9283495120599197e-05,
      "loss": 1.8347,
      "step": 642800
    },
    {
      "epoch": 24.86367327996287,
      "grad_norm": 9.992021560668945,
      "learning_rate": 2.9280272266697605e-05,
      "loss": 2.0887,
      "step": 642900
    },
    {
      "epoch": 24.867540704644778,
      "grad_norm": 12.394855499267578,
      "learning_rate": 2.927704941279602e-05,
      "loss": 1.879,
      "step": 643000
    },
    {
      "epoch": 24.87140812932668,
      "grad_norm": 10.63721752166748,
      "learning_rate": 2.9273826558894435e-05,
      "loss": 1.9295,
      "step": 643100
    },
    {
      "epoch": 24.875275554008585,
      "grad_norm": 11.606547355651855,
      "learning_rate": 2.927060370499285e-05,
      "loss": 1.9474,
      "step": 643200
    },
    {
      "epoch": 24.87914297869049,
      "grad_norm": 13.77302074432373,
      "learning_rate": 2.9267380851091257e-05,
      "loss": 1.9424,
      "step": 643300
    },
    {
      "epoch": 24.883010403372396,
      "grad_norm": 10.557296752929688,
      "learning_rate": 2.9264157997189672e-05,
      "loss": 1.9009,
      "step": 643400
    },
    {
      "epoch": 24.8868778280543,
      "grad_norm": 15.328001022338867,
      "learning_rate": 2.9260935143288087e-05,
      "loss": 2.0214,
      "step": 643500
    },
    {
      "epoch": 24.890745252736203,
      "grad_norm": 9.581204414367676,
      "learning_rate": 2.9257712289386498e-05,
      "loss": 1.9544,
      "step": 643600
    },
    {
      "epoch": 24.894612677418106,
      "grad_norm": 12.700230598449707,
      "learning_rate": 2.925448943548491e-05,
      "loss": 1.8944,
      "step": 643700
    },
    {
      "epoch": 24.898480102100013,
      "grad_norm": 11.579093933105469,
      "learning_rate": 2.9251266581583324e-05,
      "loss": 1.9554,
      "step": 643800
    },
    {
      "epoch": 24.902347526781917,
      "grad_norm": 13.091628074645996,
      "learning_rate": 2.924804372768174e-05,
      "loss": 1.9562,
      "step": 643900
    },
    {
      "epoch": 24.90621495146382,
      "grad_norm": 10.859641075134277,
      "learning_rate": 2.924482087378015e-05,
      "loss": 1.9412,
      "step": 644000
    },
    {
      "epoch": 24.910082376145724,
      "grad_norm": 11.605403900146484,
      "learning_rate": 2.9241598019878565e-05,
      "loss": 2.0044,
      "step": 644100
    },
    {
      "epoch": 24.91394980082763,
      "grad_norm": 10.02055549621582,
      "learning_rate": 2.9238375165976976e-05,
      "loss": 1.9309,
      "step": 644200
    },
    {
      "epoch": 24.917817225509534,
      "grad_norm": 10.670034408569336,
      "learning_rate": 2.923515231207539e-05,
      "loss": 1.8736,
      "step": 644300
    },
    {
      "epoch": 24.921684650191438,
      "grad_norm": 10.97595500946045,
      "learning_rate": 2.9231929458173802e-05,
      "loss": 1.9895,
      "step": 644400
    },
    {
      "epoch": 24.92555207487334,
      "grad_norm": 10.845731735229492,
      "learning_rate": 2.9228706604272217e-05,
      "loss": 1.9825,
      "step": 644500
    },
    {
      "epoch": 24.929419499555244,
      "grad_norm": 12.853666305541992,
      "learning_rate": 2.922548375037063e-05,
      "loss": 1.8873,
      "step": 644600
    },
    {
      "epoch": 24.93328692423715,
      "grad_norm": 11.08603286743164,
      "learning_rate": 2.9222260896469046e-05,
      "loss": 2.0279,
      "step": 644700
    },
    {
      "epoch": 24.937154348919055,
      "grad_norm": 12.38287353515625,
      "learning_rate": 2.9219038042567454e-05,
      "loss": 1.8494,
      "step": 644800
    },
    {
      "epoch": 24.94102177360096,
      "grad_norm": 10.36829948425293,
      "learning_rate": 2.921581518866587e-05,
      "loss": 1.8605,
      "step": 644900
    },
    {
      "epoch": 24.944889198282862,
      "grad_norm": 10.016824722290039,
      "learning_rate": 2.9212592334764284e-05,
      "loss": 1.9336,
      "step": 645000
    },
    {
      "epoch": 24.94875662296477,
      "grad_norm": 13.355728149414062,
      "learning_rate": 2.92093694808627e-05,
      "loss": 1.9002,
      "step": 645100
    },
    {
      "epoch": 24.952624047646673,
      "grad_norm": 13.871438026428223,
      "learning_rate": 2.9206146626961106e-05,
      "loss": 1.9153,
      "step": 645200
    },
    {
      "epoch": 24.956491472328576,
      "grad_norm": 10.193703651428223,
      "learning_rate": 2.920292377305952e-05,
      "loss": 1.9377,
      "step": 645300
    },
    {
      "epoch": 24.96035889701048,
      "grad_norm": 15.296486854553223,
      "learning_rate": 2.9199700919157936e-05,
      "loss": 1.81,
      "step": 645400
    },
    {
      "epoch": 24.964226321692387,
      "grad_norm": 12.316786766052246,
      "learning_rate": 2.919647806525635e-05,
      "loss": 1.9342,
      "step": 645500
    },
    {
      "epoch": 24.96809374637429,
      "grad_norm": 13.8554105758667,
      "learning_rate": 2.919325521135476e-05,
      "loss": 1.9205,
      "step": 645600
    },
    {
      "epoch": 24.971961171056194,
      "grad_norm": 11.883000373840332,
      "learning_rate": 2.9190032357453173e-05,
      "loss": 1.8672,
      "step": 645700
    },
    {
      "epoch": 24.975828595738097,
      "grad_norm": 10.377998352050781,
      "learning_rate": 2.9186809503551588e-05,
      "loss": 1.8385,
      "step": 645800
    },
    {
      "epoch": 24.979696020420004,
      "grad_norm": 13.649711608886719,
      "learning_rate": 2.9183586649650003e-05,
      "loss": 1.9379,
      "step": 645900
    },
    {
      "epoch": 24.983563445101908,
      "grad_norm": 12.719956398010254,
      "learning_rate": 2.918036379574841e-05,
      "loss": 1.9319,
      "step": 646000
    },
    {
      "epoch": 24.98743086978381,
      "grad_norm": 11.065492630004883,
      "learning_rate": 2.9177140941846825e-05,
      "loss": 1.9034,
      "step": 646100
    },
    {
      "epoch": 24.991298294465714,
      "grad_norm": 10.913311004638672,
      "learning_rate": 2.917391808794524e-05,
      "loss": 1.8755,
      "step": 646200
    },
    {
      "epoch": 24.995165719147618,
      "grad_norm": 9.369866371154785,
      "learning_rate": 2.9170695234043648e-05,
      "loss": 2.005,
      "step": 646300
    },
    {
      "epoch": 24.999033143829525,
      "grad_norm": 10.590106964111328,
      "learning_rate": 2.9167472380142063e-05,
      "loss": 1.948,
      "step": 646400
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.8284302949905396,
      "eval_runtime": 2.9691,
      "eval_samples_per_second": 458.394,
      "eval_steps_per_second": 458.394,
      "step": 646425
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.7174640893936157,
      "eval_runtime": 56.3806,
      "eval_samples_per_second": 458.615,
      "eval_steps_per_second": 458.615,
      "step": 646425
    },
    {
      "epoch": 25.00290056851143,
      "grad_norm": 11.704806327819824,
      "learning_rate": 2.9164249526240477e-05,
      "loss": 1.926,
      "step": 646500
    },
    {
      "epoch": 25.006767993193332,
      "grad_norm": 10.772401809692383,
      "learning_rate": 2.9161026672338892e-05,
      "loss": 1.941,
      "step": 646600
    },
    {
      "epoch": 25.010635417875235,
      "grad_norm": 10.88681697845459,
      "learning_rate": 2.91578038184373e-05,
      "loss": 1.9033,
      "step": 646700
    },
    {
      "epoch": 25.014502842557143,
      "grad_norm": 13.419661521911621,
      "learning_rate": 2.9154580964535715e-05,
      "loss": 1.876,
      "step": 646800
    },
    {
      "epoch": 25.018370267239046,
      "grad_norm": 10.962054252624512,
      "learning_rate": 2.915135811063413e-05,
      "loss": 1.9052,
      "step": 646900
    },
    {
      "epoch": 25.02223769192095,
      "grad_norm": 12.16053295135498,
      "learning_rate": 2.9148135256732544e-05,
      "loss": 1.8409,
      "step": 647000
    },
    {
      "epoch": 25.026105116602853,
      "grad_norm": 9.474464416503906,
      "learning_rate": 2.9144912402830956e-05,
      "loss": 1.9986,
      "step": 647100
    },
    {
      "epoch": 25.02997254128476,
      "grad_norm": 13.669227600097656,
      "learning_rate": 2.9141689548929367e-05,
      "loss": 2.0294,
      "step": 647200
    },
    {
      "epoch": 25.033839965966663,
      "grad_norm": 12.766109466552734,
      "learning_rate": 2.913846669502778e-05,
      "loss": 1.9428,
      "step": 647300
    },
    {
      "epoch": 25.037707390648567,
      "grad_norm": 13.637130737304688,
      "learning_rate": 2.9135243841126196e-05,
      "loss": 1.7894,
      "step": 647400
    },
    {
      "epoch": 25.04157481533047,
      "grad_norm": 10.093170166015625,
      "learning_rate": 2.9132020987224608e-05,
      "loss": 1.9419,
      "step": 647500
    },
    {
      "epoch": 25.045442240012378,
      "grad_norm": 10.046866416931152,
      "learning_rate": 2.9128798133323022e-05,
      "loss": 1.8578,
      "step": 647600
    },
    {
      "epoch": 25.04930966469428,
      "grad_norm": 12.134111404418945,
      "learning_rate": 2.9125575279421434e-05,
      "loss": 1.8462,
      "step": 647700
    },
    {
      "epoch": 25.053177089376184,
      "grad_norm": 10.84161376953125,
      "learning_rate": 2.912235242551985e-05,
      "loss": 1.7885,
      "step": 647800
    },
    {
      "epoch": 25.057044514058088,
      "grad_norm": 12.634198188781738,
      "learning_rate": 2.911912957161826e-05,
      "loss": 1.917,
      "step": 647900
    },
    {
      "epoch": 25.06091193873999,
      "grad_norm": 12.048439025878906,
      "learning_rate": 2.9115906717716674e-05,
      "loss": 1.8421,
      "step": 648000
    },
    {
      "epoch": 25.0647793634219,
      "grad_norm": 14.331436157226562,
      "learning_rate": 2.911268386381509e-05,
      "loss": 1.8763,
      "step": 648100
    },
    {
      "epoch": 25.068646788103802,
      "grad_norm": 12.46949577331543,
      "learning_rate": 2.9109461009913504e-05,
      "loss": 1.8529,
      "step": 648200
    },
    {
      "epoch": 25.072514212785705,
      "grad_norm": 11.630935668945312,
      "learning_rate": 2.9106238156011912e-05,
      "loss": 1.8974,
      "step": 648300
    },
    {
      "epoch": 25.07638163746761,
      "grad_norm": 10.952264785766602,
      "learning_rate": 2.9103015302110327e-05,
      "loss": 1.9745,
      "step": 648400
    },
    {
      "epoch": 25.080249062149516,
      "grad_norm": 10.760712623596191,
      "learning_rate": 2.909979244820874e-05,
      "loss": 1.8945,
      "step": 648500
    },
    {
      "epoch": 25.08411648683142,
      "grad_norm": 11.484964370727539,
      "learning_rate": 2.9096569594307156e-05,
      "loss": 1.8833,
      "step": 648600
    },
    {
      "epoch": 25.087983911513323,
      "grad_norm": 10.215897560119629,
      "learning_rate": 2.9093346740405564e-05,
      "loss": 1.9074,
      "step": 648700
    },
    {
      "epoch": 25.091851336195226,
      "grad_norm": 9.798935890197754,
      "learning_rate": 2.909012388650398e-05,
      "loss": 1.867,
      "step": 648800
    },
    {
      "epoch": 25.095718760877133,
      "grad_norm": 10.356955528259277,
      "learning_rate": 2.9086901032602393e-05,
      "loss": 1.8706,
      "step": 648900
    },
    {
      "epoch": 25.099586185559037,
      "grad_norm": 10.244857788085938,
      "learning_rate": 2.9083678178700808e-05,
      "loss": 1.9578,
      "step": 649000
    },
    {
      "epoch": 25.10345361024094,
      "grad_norm": 10.92906665802002,
      "learning_rate": 2.9080455324799216e-05,
      "loss": 1.8322,
      "step": 649100
    },
    {
      "epoch": 25.107321034922844,
      "grad_norm": 9.098209381103516,
      "learning_rate": 2.907723247089763e-05,
      "loss": 1.9153,
      "step": 649200
    },
    {
      "epoch": 25.111188459604747,
      "grad_norm": 11.635343551635742,
      "learning_rate": 2.9074009616996045e-05,
      "loss": 1.8959,
      "step": 649300
    },
    {
      "epoch": 25.115055884286654,
      "grad_norm": 8.486555099487305,
      "learning_rate": 2.9070786763094453e-05,
      "loss": 1.8303,
      "step": 649400
    },
    {
      "epoch": 25.118923308968558,
      "grad_norm": 12.917845726013184,
      "learning_rate": 2.9067563909192868e-05,
      "loss": 1.8921,
      "step": 649500
    },
    {
      "epoch": 25.12279073365046,
      "grad_norm": 10.813299179077148,
      "learning_rate": 2.9064341055291283e-05,
      "loss": 1.8386,
      "step": 649600
    },
    {
      "epoch": 25.126658158332365,
      "grad_norm": 13.215615272521973,
      "learning_rate": 2.9061118201389698e-05,
      "loss": 1.9936,
      "step": 649700
    },
    {
      "epoch": 25.130525583014272,
      "grad_norm": 9.702838897705078,
      "learning_rate": 2.9057895347488106e-05,
      "loss": 1.962,
      "step": 649800
    },
    {
      "epoch": 25.134393007696175,
      "grad_norm": 10.175250053405762,
      "learning_rate": 2.905467249358652e-05,
      "loss": 1.9259,
      "step": 649900
    },
    {
      "epoch": 25.13826043237808,
      "grad_norm": 10.86536693572998,
      "learning_rate": 2.9051449639684935e-05,
      "loss": 1.9794,
      "step": 650000
    },
    {
      "epoch": 25.142127857059982,
      "grad_norm": 11.12943172454834,
      "learning_rate": 2.904822678578335e-05,
      "loss": 1.9232,
      "step": 650100
    },
    {
      "epoch": 25.14599528174189,
      "grad_norm": 13.993701934814453,
      "learning_rate": 2.9045003931881758e-05,
      "loss": 1.9588,
      "step": 650200
    },
    {
      "epoch": 25.149862706423793,
      "grad_norm": 11.603261947631836,
      "learning_rate": 2.9041781077980172e-05,
      "loss": 1.8613,
      "step": 650300
    },
    {
      "epoch": 25.153730131105696,
      "grad_norm": 11.435636520385742,
      "learning_rate": 2.9038558224078587e-05,
      "loss": 1.9666,
      "step": 650400
    },
    {
      "epoch": 25.1575975557876,
      "grad_norm": 11.818293571472168,
      "learning_rate": 2.9035335370177002e-05,
      "loss": 1.8862,
      "step": 650500
    },
    {
      "epoch": 25.161464980469507,
      "grad_norm": 9.605096817016602,
      "learning_rate": 2.9032112516275413e-05,
      "loss": 1.9617,
      "step": 650600
    },
    {
      "epoch": 25.16533240515141,
      "grad_norm": 11.12777328491211,
      "learning_rate": 2.9028889662373824e-05,
      "loss": 1.8785,
      "step": 650700
    },
    {
      "epoch": 25.169199829833314,
      "grad_norm": 10.730030059814453,
      "learning_rate": 2.902566680847224e-05,
      "loss": 1.9741,
      "step": 650800
    },
    {
      "epoch": 25.173067254515217,
      "grad_norm": 12.606711387634277,
      "learning_rate": 2.9022443954570654e-05,
      "loss": 1.9305,
      "step": 650900
    },
    {
      "epoch": 25.17693467919712,
      "grad_norm": 11.965813636779785,
      "learning_rate": 2.9019221100669065e-05,
      "loss": 1.8855,
      "step": 651000
    },
    {
      "epoch": 25.180802103879028,
      "grad_norm": 10.409204483032227,
      "learning_rate": 2.901599824676748e-05,
      "loss": 1.8664,
      "step": 651100
    },
    {
      "epoch": 25.18466952856093,
      "grad_norm": 10.752880096435547,
      "learning_rate": 2.9012775392865895e-05,
      "loss": 1.828,
      "step": 651200
    },
    {
      "epoch": 25.188536953242835,
      "grad_norm": 10.051460266113281,
      "learning_rate": 2.9009552538964306e-05,
      "loss": 1.9074,
      "step": 651300
    },
    {
      "epoch": 25.19240437792474,
      "grad_norm": 10.361153602600098,
      "learning_rate": 2.9006329685062717e-05,
      "loss": 1.9344,
      "step": 651400
    },
    {
      "epoch": 25.196271802606645,
      "grad_norm": 12.175171852111816,
      "learning_rate": 2.9003106831161132e-05,
      "loss": 1.909,
      "step": 651500
    },
    {
      "epoch": 25.20013922728855,
      "grad_norm": 12.877927780151367,
      "learning_rate": 2.8999883977259547e-05,
      "loss": 1.9383,
      "step": 651600
    },
    {
      "epoch": 25.204006651970452,
      "grad_norm": 9.076918601989746,
      "learning_rate": 2.899666112335796e-05,
      "loss": 2.0205,
      "step": 651700
    },
    {
      "epoch": 25.207874076652356,
      "grad_norm": 9.894486427307129,
      "learning_rate": 2.899343826945637e-05,
      "loss": 1.9222,
      "step": 651800
    },
    {
      "epoch": 25.211741501334263,
      "grad_norm": 11.564398765563965,
      "learning_rate": 2.8990215415554784e-05,
      "loss": 1.8981,
      "step": 651900
    },
    {
      "epoch": 25.215608926016166,
      "grad_norm": 10.034646034240723,
      "learning_rate": 2.89869925616532e-05,
      "loss": 1.9462,
      "step": 652000
    },
    {
      "epoch": 25.21947635069807,
      "grad_norm": 11.266105651855469,
      "learning_rate": 2.8983769707751607e-05,
      "loss": 1.8472,
      "step": 652100
    },
    {
      "epoch": 25.223343775379973,
      "grad_norm": 12.331742286682129,
      "learning_rate": 2.898054685385002e-05,
      "loss": 1.9294,
      "step": 652200
    },
    {
      "epoch": 25.22721120006188,
      "grad_norm": 12.647380828857422,
      "learning_rate": 2.8977323999948436e-05,
      "loss": 1.858,
      "step": 652300
    },
    {
      "epoch": 25.231078624743784,
      "grad_norm": 9.3121976852417,
      "learning_rate": 2.897410114604685e-05,
      "loss": 1.8741,
      "step": 652400
    },
    {
      "epoch": 25.234946049425687,
      "grad_norm": 11.058799743652344,
      "learning_rate": 2.897087829214526e-05,
      "loss": 1.9216,
      "step": 652500
    },
    {
      "epoch": 25.23881347410759,
      "grad_norm": 8.916219711303711,
      "learning_rate": 2.8967655438243674e-05,
      "loss": 1.8699,
      "step": 652600
    },
    {
      "epoch": 25.242680898789494,
      "grad_norm": 11.145222663879395,
      "learning_rate": 2.8964432584342088e-05,
      "loss": 1.9485,
      "step": 652700
    },
    {
      "epoch": 25.2465483234714,
      "grad_norm": 12.121015548706055,
      "learning_rate": 2.8961209730440503e-05,
      "loss": 1.8839,
      "step": 652800
    },
    {
      "epoch": 25.250415748153305,
      "grad_norm": 16.302284240722656,
      "learning_rate": 2.895798687653891e-05,
      "loss": 1.8823,
      "step": 652900
    },
    {
      "epoch": 25.25428317283521,
      "grad_norm": 11.854979515075684,
      "learning_rate": 2.8954764022637326e-05,
      "loss": 2.0505,
      "step": 653000
    },
    {
      "epoch": 25.258150597517112,
      "grad_norm": 12.50126838684082,
      "learning_rate": 2.895154116873574e-05,
      "loss": 1.9098,
      "step": 653100
    },
    {
      "epoch": 25.26201802219902,
      "grad_norm": 11.541099548339844,
      "learning_rate": 2.8948318314834155e-05,
      "loss": 1.9521,
      "step": 653200
    },
    {
      "epoch": 25.265885446880922,
      "grad_norm": 7.681949138641357,
      "learning_rate": 2.8945095460932563e-05,
      "loss": 1.9279,
      "step": 653300
    },
    {
      "epoch": 25.269752871562826,
      "grad_norm": 14.30124568939209,
      "learning_rate": 2.8941872607030978e-05,
      "loss": 1.9229,
      "step": 653400
    },
    {
      "epoch": 25.27362029624473,
      "grad_norm": 12.133774757385254,
      "learning_rate": 2.8938649753129393e-05,
      "loss": 1.9611,
      "step": 653500
    },
    {
      "epoch": 25.277487720926636,
      "grad_norm": 14.152634620666504,
      "learning_rate": 2.8935426899227807e-05,
      "loss": 1.8771,
      "step": 653600
    },
    {
      "epoch": 25.28135514560854,
      "grad_norm": 11.386222839355469,
      "learning_rate": 2.8932204045326215e-05,
      "loss": 2.0166,
      "step": 653700
    },
    {
      "epoch": 25.285222570290443,
      "grad_norm": 9.913365364074707,
      "learning_rate": 2.892898119142463e-05,
      "loss": 1.8755,
      "step": 653800
    },
    {
      "epoch": 25.289089994972347,
      "grad_norm": 11.037422180175781,
      "learning_rate": 2.8925758337523045e-05,
      "loss": 1.9095,
      "step": 653900
    },
    {
      "epoch": 25.292957419654254,
      "grad_norm": 11.107389450073242,
      "learning_rate": 2.892253548362146e-05,
      "loss": 1.8997,
      "step": 654000
    },
    {
      "epoch": 25.296824844336157,
      "grad_norm": 11.52307415008545,
      "learning_rate": 2.891931262971987e-05,
      "loss": 1.8421,
      "step": 654100
    },
    {
      "epoch": 25.30069226901806,
      "grad_norm": 8.827221870422363,
      "learning_rate": 2.8916089775818282e-05,
      "loss": 1.875,
      "step": 654200
    },
    {
      "epoch": 25.304559693699964,
      "grad_norm": 8.964377403259277,
      "learning_rate": 2.8912866921916697e-05,
      "loss": 1.855,
      "step": 654300
    },
    {
      "epoch": 25.308427118381868,
      "grad_norm": 14.131254196166992,
      "learning_rate": 2.890964406801511e-05,
      "loss": 1.8797,
      "step": 654400
    },
    {
      "epoch": 25.312294543063775,
      "grad_norm": 8.697935104370117,
      "learning_rate": 2.8906421214113523e-05,
      "loss": 1.8834,
      "step": 654500
    },
    {
      "epoch": 25.31616196774568,
      "grad_norm": 11.637290000915527,
      "learning_rate": 2.8903198360211937e-05,
      "loss": 1.9898,
      "step": 654600
    },
    {
      "epoch": 25.320029392427582,
      "grad_norm": 12.087831497192383,
      "learning_rate": 2.8899975506310352e-05,
      "loss": 1.957,
      "step": 654700
    },
    {
      "epoch": 25.323896817109485,
      "grad_norm": 11.359023094177246,
      "learning_rate": 2.8896752652408764e-05,
      "loss": 1.8522,
      "step": 654800
    },
    {
      "epoch": 25.327764241791392,
      "grad_norm": 11.153312683105469,
      "learning_rate": 2.8893529798507175e-05,
      "loss": 1.9208,
      "step": 654900
    },
    {
      "epoch": 25.331631666473296,
      "grad_norm": 10.649428367614746,
      "learning_rate": 2.889030694460559e-05,
      "loss": 1.9964,
      "step": 655000
    },
    {
      "epoch": 25.3354990911552,
      "grad_norm": 10.626051902770996,
      "learning_rate": 2.8887084090704004e-05,
      "loss": 1.913,
      "step": 655100
    },
    {
      "epoch": 25.339366515837103,
      "grad_norm": 11.930201530456543,
      "learning_rate": 2.8883861236802412e-05,
      "loss": 1.8186,
      "step": 655200
    },
    {
      "epoch": 25.34323394051901,
      "grad_norm": 15.95987606048584,
      "learning_rate": 2.8880638382900827e-05,
      "loss": 1.944,
      "step": 655300
    },
    {
      "epoch": 25.347101365200913,
      "grad_norm": 11.948741912841797,
      "learning_rate": 2.887741552899924e-05,
      "loss": 1.9482,
      "step": 655400
    },
    {
      "epoch": 25.350968789882817,
      "grad_norm": 12.94104290008545,
      "learning_rate": 2.8874192675097656e-05,
      "loss": 1.8909,
      "step": 655500
    },
    {
      "epoch": 25.35483621456472,
      "grad_norm": 12.031423568725586,
      "learning_rate": 2.8870969821196064e-05,
      "loss": 1.929,
      "step": 655600
    },
    {
      "epoch": 25.358703639246627,
      "grad_norm": 13.980213165283203,
      "learning_rate": 2.886774696729448e-05,
      "loss": 1.9064,
      "step": 655700
    },
    {
      "epoch": 25.36257106392853,
      "grad_norm": 14.001734733581543,
      "learning_rate": 2.8864524113392894e-05,
      "loss": 1.8602,
      "step": 655800
    },
    {
      "epoch": 25.366438488610434,
      "grad_norm": 8.091471672058105,
      "learning_rate": 2.886130125949131e-05,
      "loss": 1.8614,
      "step": 655900
    },
    {
      "epoch": 25.370305913292338,
      "grad_norm": 10.84245777130127,
      "learning_rate": 2.8858078405589716e-05,
      "loss": 1.9877,
      "step": 656000
    },
    {
      "epoch": 25.37417333797424,
      "grad_norm": 13.11562442779541,
      "learning_rate": 2.885485555168813e-05,
      "loss": 1.8356,
      "step": 656100
    },
    {
      "epoch": 25.37804076265615,
      "grad_norm": 13.295510292053223,
      "learning_rate": 2.8851632697786546e-05,
      "loss": 1.9596,
      "step": 656200
    },
    {
      "epoch": 25.381908187338052,
      "grad_norm": 9.716360092163086,
      "learning_rate": 2.884840984388496e-05,
      "loss": 1.9316,
      "step": 656300
    },
    {
      "epoch": 25.385775612019955,
      "grad_norm": 12.520732879638672,
      "learning_rate": 2.884518698998337e-05,
      "loss": 1.9191,
      "step": 656400
    },
    {
      "epoch": 25.38964303670186,
      "grad_norm": 12.77517032623291,
      "learning_rate": 2.8841964136081783e-05,
      "loss": 1.9029,
      "step": 656500
    },
    {
      "epoch": 25.393510461383766,
      "grad_norm": 11.85058307647705,
      "learning_rate": 2.8838741282180198e-05,
      "loss": 1.9038,
      "step": 656600
    },
    {
      "epoch": 25.39737788606567,
      "grad_norm": 11.952821731567383,
      "learning_rate": 2.8835518428278613e-05,
      "loss": 1.9809,
      "step": 656700
    },
    {
      "epoch": 25.401245310747573,
      "grad_norm": 9.157511711120605,
      "learning_rate": 2.883229557437702e-05,
      "loss": 1.9249,
      "step": 656800
    },
    {
      "epoch": 25.405112735429476,
      "grad_norm": 9.792071342468262,
      "learning_rate": 2.8829072720475435e-05,
      "loss": 2.0043,
      "step": 656900
    },
    {
      "epoch": 25.408980160111383,
      "grad_norm": 10.65526294708252,
      "learning_rate": 2.882584986657385e-05,
      "loss": 1.9035,
      "step": 657000
    },
    {
      "epoch": 25.412847584793287,
      "grad_norm": 9.895769119262695,
      "learning_rate": 2.8822627012672265e-05,
      "loss": 1.888,
      "step": 657100
    },
    {
      "epoch": 25.41671500947519,
      "grad_norm": 11.226187705993652,
      "learning_rate": 2.8819404158770673e-05,
      "loss": 1.9174,
      "step": 657200
    },
    {
      "epoch": 25.420582434157094,
      "grad_norm": 10.364959716796875,
      "learning_rate": 2.8816181304869087e-05,
      "loss": 1.9717,
      "step": 657300
    },
    {
      "epoch": 25.424449858838997,
      "grad_norm": 12.72443675994873,
      "learning_rate": 2.8812958450967502e-05,
      "loss": 1.9626,
      "step": 657400
    },
    {
      "epoch": 25.428317283520904,
      "grad_norm": 11.318177223205566,
      "learning_rate": 2.8809735597065917e-05,
      "loss": 1.8704,
      "step": 657500
    },
    {
      "epoch": 25.432184708202808,
      "grad_norm": 11.280342102050781,
      "learning_rate": 2.8806512743164328e-05,
      "loss": 1.9768,
      "step": 657600
    },
    {
      "epoch": 25.43605213288471,
      "grad_norm": 13.990995407104492,
      "learning_rate": 2.880328988926274e-05,
      "loss": 1.9353,
      "step": 657700
    },
    {
      "epoch": 25.439919557566615,
      "grad_norm": 11.353334426879883,
      "learning_rate": 2.8800067035361154e-05,
      "loss": 1.8936,
      "step": 657800
    },
    {
      "epoch": 25.443786982248522,
      "grad_norm": 19.18231773376465,
      "learning_rate": 2.879684418145957e-05,
      "loss": 1.8643,
      "step": 657900
    },
    {
      "epoch": 25.447654406930425,
      "grad_norm": 11.068706512451172,
      "learning_rate": 2.879362132755798e-05,
      "loss": 1.9722,
      "step": 658000
    },
    {
      "epoch": 25.45152183161233,
      "grad_norm": 11.302826881408691,
      "learning_rate": 2.8790398473656395e-05,
      "loss": 1.8606,
      "step": 658100
    },
    {
      "epoch": 25.455389256294232,
      "grad_norm": 8.94210433959961,
      "learning_rate": 2.878717561975481e-05,
      "loss": 1.8798,
      "step": 658200
    },
    {
      "epoch": 25.45925668097614,
      "grad_norm": 11.54252815246582,
      "learning_rate": 2.8783952765853218e-05,
      "loss": 1.8681,
      "step": 658300
    },
    {
      "epoch": 25.463124105658043,
      "grad_norm": 14.21048355102539,
      "learning_rate": 2.8780729911951632e-05,
      "loss": 1.9569,
      "step": 658400
    },
    {
      "epoch": 25.466991530339946,
      "grad_norm": 12.485029220581055,
      "learning_rate": 2.8777507058050047e-05,
      "loss": 1.9128,
      "step": 658500
    },
    {
      "epoch": 25.47085895502185,
      "grad_norm": 11.398103713989258,
      "learning_rate": 2.8774284204148462e-05,
      "loss": 2.0125,
      "step": 658600
    },
    {
      "epoch": 25.474726379703757,
      "grad_norm": 13.780439376831055,
      "learning_rate": 2.877106135024687e-05,
      "loss": 1.8828,
      "step": 658700
    },
    {
      "epoch": 25.47859380438566,
      "grad_norm": 14.86538314819336,
      "learning_rate": 2.8767838496345284e-05,
      "loss": 1.9249,
      "step": 658800
    },
    {
      "epoch": 25.482461229067564,
      "grad_norm": 10.743584632873535,
      "learning_rate": 2.87646156424437e-05,
      "loss": 1.8955,
      "step": 658900
    },
    {
      "epoch": 25.486328653749467,
      "grad_norm": 11.980968475341797,
      "learning_rate": 2.8761392788542114e-05,
      "loss": 1.9075,
      "step": 659000
    },
    {
      "epoch": 25.49019607843137,
      "grad_norm": 11.028621673583984,
      "learning_rate": 2.8758169934640522e-05,
      "loss": 1.9095,
      "step": 659100
    },
    {
      "epoch": 25.494063503113278,
      "grad_norm": 10.154972076416016,
      "learning_rate": 2.8754947080738937e-05,
      "loss": 1.9134,
      "step": 659200
    },
    {
      "epoch": 25.49793092779518,
      "grad_norm": 11.589262008666992,
      "learning_rate": 2.875172422683735e-05,
      "loss": 1.9018,
      "step": 659300
    },
    {
      "epoch": 25.501798352477085,
      "grad_norm": 12.548396110534668,
      "learning_rate": 2.8748501372935766e-05,
      "loss": 1.8874,
      "step": 659400
    },
    {
      "epoch": 25.50566577715899,
      "grad_norm": 9.376158714294434,
      "learning_rate": 2.8745278519034174e-05,
      "loss": 1.8502,
      "step": 659500
    },
    {
      "epoch": 25.509533201840895,
      "grad_norm": 10.43017292022705,
      "learning_rate": 2.874205566513259e-05,
      "loss": 1.8737,
      "step": 659600
    },
    {
      "epoch": 25.5134006265228,
      "grad_norm": 11.188373565673828,
      "learning_rate": 2.8738832811231003e-05,
      "loss": 1.8876,
      "step": 659700
    },
    {
      "epoch": 25.517268051204702,
      "grad_norm": 14.861586570739746,
      "learning_rate": 2.8735609957329418e-05,
      "loss": 1.9059,
      "step": 659800
    },
    {
      "epoch": 25.521135475886606,
      "grad_norm": 10.91962718963623,
      "learning_rate": 2.8732387103427826e-05,
      "loss": 2.0131,
      "step": 659900
    },
    {
      "epoch": 25.525002900568513,
      "grad_norm": 11.153552055358887,
      "learning_rate": 2.872916424952624e-05,
      "loss": 2.0275,
      "step": 660000
    },
    {
      "epoch": 25.528870325250416,
      "grad_norm": 12.395554542541504,
      "learning_rate": 2.8725941395624655e-05,
      "loss": 1.8403,
      "step": 660100
    },
    {
      "epoch": 25.53273774993232,
      "grad_norm": 10.268364906311035,
      "learning_rate": 2.872271854172307e-05,
      "loss": 1.9314,
      "step": 660200
    },
    {
      "epoch": 25.536605174614223,
      "grad_norm": 14.35249137878418,
      "learning_rate": 2.8719495687821478e-05,
      "loss": 1.9449,
      "step": 660300
    },
    {
      "epoch": 25.54047259929613,
      "grad_norm": 11.232030868530273,
      "learning_rate": 2.8716272833919893e-05,
      "loss": 1.88,
      "step": 660400
    },
    {
      "epoch": 25.544340023978034,
      "grad_norm": 9.228219032287598,
      "learning_rate": 2.8713049980018308e-05,
      "loss": 2.0097,
      "step": 660500
    },
    {
      "epoch": 25.548207448659937,
      "grad_norm": 13.245159149169922,
      "learning_rate": 2.8709827126116722e-05,
      "loss": 1.9633,
      "step": 660600
    },
    {
      "epoch": 25.55207487334184,
      "grad_norm": 9.218217849731445,
      "learning_rate": 2.870660427221513e-05,
      "loss": 1.8856,
      "step": 660700
    },
    {
      "epoch": 25.555942298023744,
      "grad_norm": 10.026134490966797,
      "learning_rate": 2.8703381418313545e-05,
      "loss": 1.9711,
      "step": 660800
    },
    {
      "epoch": 25.55980972270565,
      "grad_norm": 11.948596000671387,
      "learning_rate": 2.870015856441196e-05,
      "loss": 1.9195,
      "step": 660900
    },
    {
      "epoch": 25.563677147387555,
      "grad_norm": 9.813128471374512,
      "learning_rate": 2.869693571051037e-05,
      "loss": 1.9001,
      "step": 661000
    },
    {
      "epoch": 25.56754457206946,
      "grad_norm": 12.789851188659668,
      "learning_rate": 2.8693712856608786e-05,
      "loss": 1.8709,
      "step": 661100
    },
    {
      "epoch": 25.571411996751362,
      "grad_norm": 8.958125114440918,
      "learning_rate": 2.86904900027072e-05,
      "loss": 1.9091,
      "step": 661200
    },
    {
      "epoch": 25.57527942143327,
      "grad_norm": 16.15604591369629,
      "learning_rate": 2.8687267148805612e-05,
      "loss": 1.8324,
      "step": 661300
    },
    {
      "epoch": 25.579146846115172,
      "grad_norm": 12.791898727416992,
      "learning_rate": 2.8684044294904023e-05,
      "loss": 1.9698,
      "step": 661400
    },
    {
      "epoch": 25.583014270797076,
      "grad_norm": 13.833728790283203,
      "learning_rate": 2.8680821441002438e-05,
      "loss": 2.0112,
      "step": 661500
    },
    {
      "epoch": 25.58688169547898,
      "grad_norm": 14.44502067565918,
      "learning_rate": 2.8677598587100853e-05,
      "loss": 1.9154,
      "step": 661600
    },
    {
      "epoch": 25.590749120160886,
      "grad_norm": 10.080633163452148,
      "learning_rate": 2.8674375733199267e-05,
      "loss": 1.8876,
      "step": 661700
    },
    {
      "epoch": 25.59461654484279,
      "grad_norm": 12.133612632751465,
      "learning_rate": 2.8671152879297675e-05,
      "loss": 1.8863,
      "step": 661800
    },
    {
      "epoch": 25.598483969524693,
      "grad_norm": 11.43824577331543,
      "learning_rate": 2.866793002539609e-05,
      "loss": 1.9454,
      "step": 661900
    },
    {
      "epoch": 25.602351394206597,
      "grad_norm": 12.041015625,
      "learning_rate": 2.8664707171494505e-05,
      "loss": 1.8882,
      "step": 662000
    },
    {
      "epoch": 25.606218818888504,
      "grad_norm": 16.154632568359375,
      "learning_rate": 2.866148431759292e-05,
      "loss": 1.8805,
      "step": 662100
    },
    {
      "epoch": 25.610086243570407,
      "grad_norm": 11.608232498168945,
      "learning_rate": 2.8658261463691327e-05,
      "loss": 1.8545,
      "step": 662200
    },
    {
      "epoch": 25.61395366825231,
      "grad_norm": 9.825016975402832,
      "learning_rate": 2.8655038609789742e-05,
      "loss": 1.9268,
      "step": 662300
    },
    {
      "epoch": 25.617821092934214,
      "grad_norm": 8.075712203979492,
      "learning_rate": 2.8651815755888157e-05,
      "loss": 1.9568,
      "step": 662400
    },
    {
      "epoch": 25.621688517616118,
      "grad_norm": 12.875141143798828,
      "learning_rate": 2.864859290198657e-05,
      "loss": 1.8938,
      "step": 662500
    },
    {
      "epoch": 25.625555942298025,
      "grad_norm": 15.797078132629395,
      "learning_rate": 2.864537004808498e-05,
      "loss": 1.8279,
      "step": 662600
    },
    {
      "epoch": 25.62942336697993,
      "grad_norm": 15.003007888793945,
      "learning_rate": 2.8642147194183394e-05,
      "loss": 1.9961,
      "step": 662700
    },
    {
      "epoch": 25.63329079166183,
      "grad_norm": 13.429194450378418,
      "learning_rate": 2.863892434028181e-05,
      "loss": 1.9109,
      "step": 662800
    },
    {
      "epoch": 25.637158216343735,
      "grad_norm": 12.821756362915039,
      "learning_rate": 2.8635701486380224e-05,
      "loss": 1.9735,
      "step": 662900
    },
    {
      "epoch": 25.641025641025642,
      "grad_norm": 13.703903198242188,
      "learning_rate": 2.863247863247863e-05,
      "loss": 1.8742,
      "step": 663000
    },
    {
      "epoch": 25.644893065707546,
      "grad_norm": 10.327195167541504,
      "learning_rate": 2.8629255778577046e-05,
      "loss": 1.9189,
      "step": 663100
    },
    {
      "epoch": 25.64876049038945,
      "grad_norm": 11.743925094604492,
      "learning_rate": 2.862603292467546e-05,
      "loss": 1.9318,
      "step": 663200
    },
    {
      "epoch": 25.652627915071353,
      "grad_norm": 13.436934471130371,
      "learning_rate": 2.8622810070773876e-05,
      "loss": 1.8557,
      "step": 663300
    },
    {
      "epoch": 25.65649533975326,
      "grad_norm": 13.547642707824707,
      "learning_rate": 2.8619587216872284e-05,
      "loss": 1.8914,
      "step": 663400
    },
    {
      "epoch": 25.660362764435163,
      "grad_norm": 11.647493362426758,
      "learning_rate": 2.86163643629707e-05,
      "loss": 1.9106,
      "step": 663500
    },
    {
      "epoch": 25.664230189117067,
      "grad_norm": 11.526570320129395,
      "learning_rate": 2.8613141509069113e-05,
      "loss": 1.8057,
      "step": 663600
    },
    {
      "epoch": 25.66809761379897,
      "grad_norm": 13.595610618591309,
      "learning_rate": 2.8609918655167528e-05,
      "loss": 1.9038,
      "step": 663700
    },
    {
      "epoch": 25.671965038480877,
      "grad_norm": 11.69527816772461,
      "learning_rate": 2.8606695801265936e-05,
      "loss": 2.0213,
      "step": 663800
    },
    {
      "epoch": 25.67583246316278,
      "grad_norm": 10.356736183166504,
      "learning_rate": 2.860347294736435e-05,
      "loss": 1.9694,
      "step": 663900
    },
    {
      "epoch": 25.679699887844684,
      "grad_norm": 10.81124210357666,
      "learning_rate": 2.8600250093462765e-05,
      "loss": 1.8991,
      "step": 664000
    },
    {
      "epoch": 25.683567312526588,
      "grad_norm": 10.645792961120605,
      "learning_rate": 2.8597027239561176e-05,
      "loss": 1.9846,
      "step": 664100
    },
    {
      "epoch": 25.68743473720849,
      "grad_norm": 9.63863754272461,
      "learning_rate": 2.8593804385659588e-05,
      "loss": 1.9267,
      "step": 664200
    },
    {
      "epoch": 25.6913021618904,
      "grad_norm": 11.737249374389648,
      "learning_rate": 2.8590581531758003e-05,
      "loss": 1.9269,
      "step": 664300
    },
    {
      "epoch": 25.6951695865723,
      "grad_norm": 6.5693440437316895,
      "learning_rate": 2.8587358677856417e-05,
      "loss": 1.8936,
      "step": 664400
    },
    {
      "epoch": 25.699037011254205,
      "grad_norm": 11.741089820861816,
      "learning_rate": 2.858413582395483e-05,
      "loss": 2.0139,
      "step": 664500
    },
    {
      "epoch": 25.70290443593611,
      "grad_norm": 10.878127098083496,
      "learning_rate": 2.8580912970053243e-05,
      "loss": 1.8617,
      "step": 664600
    },
    {
      "epoch": 25.706771860618016,
      "grad_norm": 10.023816108703613,
      "learning_rate": 2.8577690116151658e-05,
      "loss": 1.9091,
      "step": 664700
    },
    {
      "epoch": 25.71063928529992,
      "grad_norm": 13.384778022766113,
      "learning_rate": 2.857446726225007e-05,
      "loss": 1.8927,
      "step": 664800
    },
    {
      "epoch": 25.714506709981823,
      "grad_norm": 8.090738296508789,
      "learning_rate": 2.857124440834848e-05,
      "loss": 1.8677,
      "step": 664900
    },
    {
      "epoch": 25.718374134663726,
      "grad_norm": 10.749825477600098,
      "learning_rate": 2.8568021554446895e-05,
      "loss": 1.9707,
      "step": 665000
    },
    {
      "epoch": 25.722241559345633,
      "grad_norm": 9.286651611328125,
      "learning_rate": 2.856479870054531e-05,
      "loss": 1.8598,
      "step": 665100
    },
    {
      "epoch": 25.726108984027537,
      "grad_norm": 11.746593475341797,
      "learning_rate": 2.8561575846643725e-05,
      "loss": 1.9562,
      "step": 665200
    },
    {
      "epoch": 25.72997640870944,
      "grad_norm": 16.997291564941406,
      "learning_rate": 2.8558352992742133e-05,
      "loss": 1.884,
      "step": 665300
    },
    {
      "epoch": 25.733843833391344,
      "grad_norm": 14.363795280456543,
      "learning_rate": 2.8555130138840547e-05,
      "loss": 1.9461,
      "step": 665400
    },
    {
      "epoch": 25.737711258073247,
      "grad_norm": 10.997274398803711,
      "learning_rate": 2.8551907284938962e-05,
      "loss": 1.9141,
      "step": 665500
    },
    {
      "epoch": 25.741578682755154,
      "grad_norm": 12.714414596557617,
      "learning_rate": 2.8548684431037377e-05,
      "loss": 1.8512,
      "step": 665600
    },
    {
      "epoch": 25.745446107437058,
      "grad_norm": 10.112106323242188,
      "learning_rate": 2.8545461577135785e-05,
      "loss": 1.9315,
      "step": 665700
    },
    {
      "epoch": 25.74931353211896,
      "grad_norm": 12.279695510864258,
      "learning_rate": 2.85422387232342e-05,
      "loss": 1.8932,
      "step": 665800
    },
    {
      "epoch": 25.753180956800865,
      "grad_norm": 13.013842582702637,
      "learning_rate": 2.8539015869332614e-05,
      "loss": 1.9285,
      "step": 665900
    },
    {
      "epoch": 25.75704838148277,
      "grad_norm": 11.404423713684082,
      "learning_rate": 2.853579301543103e-05,
      "loss": 1.9086,
      "step": 666000
    },
    {
      "epoch": 25.760915806164675,
      "grad_norm": 11.37708568572998,
      "learning_rate": 2.8532570161529437e-05,
      "loss": 1.8286,
      "step": 666100
    },
    {
      "epoch": 25.76478323084658,
      "grad_norm": 8.466482162475586,
      "learning_rate": 2.852934730762785e-05,
      "loss": 1.8725,
      "step": 666200
    },
    {
      "epoch": 25.768650655528482,
      "grad_norm": 10.262216567993164,
      "learning_rate": 2.8526124453726266e-05,
      "loss": 1.9225,
      "step": 666300
    },
    {
      "epoch": 25.77251808021039,
      "grad_norm": 10.237848281860352,
      "learning_rate": 2.852290159982468e-05,
      "loss": 1.9749,
      "step": 666400
    },
    {
      "epoch": 25.776385504892293,
      "grad_norm": 10.23146915435791,
      "learning_rate": 2.851967874592309e-05,
      "loss": 1.9594,
      "step": 666500
    },
    {
      "epoch": 25.780252929574196,
      "grad_norm": 13.831192016601562,
      "learning_rate": 2.8516455892021504e-05,
      "loss": 1.9309,
      "step": 666600
    },
    {
      "epoch": 25.7841203542561,
      "grad_norm": 10.5147066116333,
      "learning_rate": 2.851323303811992e-05,
      "loss": 1.8926,
      "step": 666700
    },
    {
      "epoch": 25.787987778938007,
      "grad_norm": 13.387725830078125,
      "learning_rate": 2.8510010184218333e-05,
      "loss": 1.9401,
      "step": 666800
    },
    {
      "epoch": 25.79185520361991,
      "grad_norm": 12.774141311645508,
      "learning_rate": 2.850678733031674e-05,
      "loss": 1.9807,
      "step": 666900
    },
    {
      "epoch": 25.795722628301814,
      "grad_norm": 9.692770957946777,
      "learning_rate": 2.8503564476415156e-05,
      "loss": 1.985,
      "step": 667000
    },
    {
      "epoch": 25.799590052983717,
      "grad_norm": 9.832450866699219,
      "learning_rate": 2.850034162251357e-05,
      "loss": 1.8598,
      "step": 667100
    },
    {
      "epoch": 25.803457477665624,
      "grad_norm": 13.525670051574707,
      "learning_rate": 2.849711876861198e-05,
      "loss": 1.9904,
      "step": 667200
    },
    {
      "epoch": 25.807324902347528,
      "grad_norm": 10.692376136779785,
      "learning_rate": 2.8493895914710393e-05,
      "loss": 1.8914,
      "step": 667300
    },
    {
      "epoch": 25.81119232702943,
      "grad_norm": 9.985641479492188,
      "learning_rate": 2.8490673060808808e-05,
      "loss": 1.9625,
      "step": 667400
    },
    {
      "epoch": 25.815059751711335,
      "grad_norm": 8.94828987121582,
      "learning_rate": 2.8487450206907223e-05,
      "loss": 1.9165,
      "step": 667500
    },
    {
      "epoch": 25.818927176393238,
      "grad_norm": 12.447376251220703,
      "learning_rate": 2.8484227353005634e-05,
      "loss": 1.9799,
      "step": 667600
    },
    {
      "epoch": 25.822794601075145,
      "grad_norm": 11.560698509216309,
      "learning_rate": 2.8481004499104045e-05,
      "loss": 1.9099,
      "step": 667700
    },
    {
      "epoch": 25.82666202575705,
      "grad_norm": 12.323627471923828,
      "learning_rate": 2.847778164520246e-05,
      "loss": 2.0501,
      "step": 667800
    },
    {
      "epoch": 25.830529450438952,
      "grad_norm": 10.826394081115723,
      "learning_rate": 2.8474558791300875e-05,
      "loss": 1.8689,
      "step": 667900
    },
    {
      "epoch": 25.834396875120856,
      "grad_norm": 12.548142433166504,
      "learning_rate": 2.8471335937399286e-05,
      "loss": 1.9109,
      "step": 668000
    },
    {
      "epoch": 25.838264299802763,
      "grad_norm": 10.943618774414062,
      "learning_rate": 2.84681130834977e-05,
      "loss": 1.8517,
      "step": 668100
    },
    {
      "epoch": 25.842131724484666,
      "grad_norm": 13.663525581359863,
      "learning_rate": 2.8464890229596116e-05,
      "loss": 1.9765,
      "step": 668200
    },
    {
      "epoch": 25.84599914916657,
      "grad_norm": 16.080930709838867,
      "learning_rate": 2.8461667375694527e-05,
      "loss": 1.911,
      "step": 668300
    },
    {
      "epoch": 25.849866573848473,
      "grad_norm": 10.912250518798828,
      "learning_rate": 2.8458444521792938e-05,
      "loss": 1.9114,
      "step": 668400
    },
    {
      "epoch": 25.85373399853038,
      "grad_norm": 10.271814346313477,
      "learning_rate": 2.8455221667891353e-05,
      "loss": 1.9243,
      "step": 668500
    },
    {
      "epoch": 25.857601423212284,
      "grad_norm": 11.241741180419922,
      "learning_rate": 2.8451998813989768e-05,
      "loss": 1.8461,
      "step": 668600
    },
    {
      "epoch": 25.861468847894187,
      "grad_norm": 10.672182083129883,
      "learning_rate": 2.8448775960088182e-05,
      "loss": 1.9386,
      "step": 668700
    },
    {
      "epoch": 25.86533627257609,
      "grad_norm": 9.55301570892334,
      "learning_rate": 2.844555310618659e-05,
      "loss": 1.993,
      "step": 668800
    },
    {
      "epoch": 25.869203697257994,
      "grad_norm": 14.107525825500488,
      "learning_rate": 2.8442330252285005e-05,
      "loss": 1.9323,
      "step": 668900
    },
    {
      "epoch": 25.8730711219399,
      "grad_norm": 10.679279327392578,
      "learning_rate": 2.843910739838342e-05,
      "loss": 2.0209,
      "step": 669000
    },
    {
      "epoch": 25.876938546621805,
      "grad_norm": 12.16932201385498,
      "learning_rate": 2.8435884544481834e-05,
      "loss": 1.9488,
      "step": 669100
    },
    {
      "epoch": 25.880805971303708,
      "grad_norm": 10.399823188781738,
      "learning_rate": 2.8432661690580242e-05,
      "loss": 1.849,
      "step": 669200
    },
    {
      "epoch": 25.88467339598561,
      "grad_norm": 11.467963218688965,
      "learning_rate": 2.8429438836678657e-05,
      "loss": 2.0054,
      "step": 669300
    },
    {
      "epoch": 25.88854082066752,
      "grad_norm": 10.432415008544922,
      "learning_rate": 2.8426215982777072e-05,
      "loss": 1.9085,
      "step": 669400
    },
    {
      "epoch": 25.892408245349422,
      "grad_norm": 9.759613037109375,
      "learning_rate": 2.8422993128875487e-05,
      "loss": 1.9169,
      "step": 669500
    },
    {
      "epoch": 25.896275670031326,
      "grad_norm": 13.318682670593262,
      "learning_rate": 2.8419770274973894e-05,
      "loss": 1.9929,
      "step": 669600
    },
    {
      "epoch": 25.90014309471323,
      "grad_norm": 13.213533401489258,
      "learning_rate": 2.841654742107231e-05,
      "loss": 1.9451,
      "step": 669700
    },
    {
      "epoch": 25.904010519395136,
      "grad_norm": 11.50870418548584,
      "learning_rate": 2.8413324567170724e-05,
      "loss": 1.9489,
      "step": 669800
    },
    {
      "epoch": 25.90787794407704,
      "grad_norm": 10.291129112243652,
      "learning_rate": 2.8410101713269132e-05,
      "loss": 1.911,
      "step": 669900
    },
    {
      "epoch": 25.911745368758943,
      "grad_norm": 10.032800674438477,
      "learning_rate": 2.8406878859367547e-05,
      "loss": 1.992,
      "step": 670000
    },
    {
      "epoch": 25.915612793440847,
      "grad_norm": 15.202216148376465,
      "learning_rate": 2.840365600546596e-05,
      "loss": 1.8611,
      "step": 670100
    },
    {
      "epoch": 25.919480218122754,
      "grad_norm": 14.509581565856934,
      "learning_rate": 2.8400433151564376e-05,
      "loss": 1.8811,
      "step": 670200
    },
    {
      "epoch": 25.923347642804657,
      "grad_norm": 12.871255874633789,
      "learning_rate": 2.8397210297662784e-05,
      "loss": 1.9588,
      "step": 670300
    },
    {
      "epoch": 25.92721506748656,
      "grad_norm": 11.359987258911133,
      "learning_rate": 2.83939874437612e-05,
      "loss": 1.9235,
      "step": 670400
    },
    {
      "epoch": 25.931082492168464,
      "grad_norm": 10.770505905151367,
      "learning_rate": 2.8390764589859613e-05,
      "loss": 1.8679,
      "step": 670500
    },
    {
      "epoch": 25.934949916850368,
      "grad_norm": 12.44576358795166,
      "learning_rate": 2.8387541735958028e-05,
      "loss": 1.8893,
      "step": 670600
    },
    {
      "epoch": 25.938817341532275,
      "grad_norm": 13.731206893920898,
      "learning_rate": 2.8384318882056436e-05,
      "loss": 1.9153,
      "step": 670700
    },
    {
      "epoch": 25.942684766214178,
      "grad_norm": 11.466458320617676,
      "learning_rate": 2.838109602815485e-05,
      "loss": 1.9722,
      "step": 670800
    },
    {
      "epoch": 25.94655219089608,
      "grad_norm": 13.439523696899414,
      "learning_rate": 2.8377873174253266e-05,
      "loss": 1.9129,
      "step": 670900
    },
    {
      "epoch": 25.950419615577985,
      "grad_norm": 13.918485641479492,
      "learning_rate": 2.837465032035168e-05,
      "loss": 1.8934,
      "step": 671000
    },
    {
      "epoch": 25.954287040259892,
      "grad_norm": 9.317654609680176,
      "learning_rate": 2.837142746645009e-05,
      "loss": 1.9114,
      "step": 671100
    },
    {
      "epoch": 25.958154464941796,
      "grad_norm": 11.657633781433105,
      "learning_rate": 2.8368204612548506e-05,
      "loss": 1.8641,
      "step": 671200
    },
    {
      "epoch": 25.9620218896237,
      "grad_norm": 11.628530502319336,
      "learning_rate": 2.8364981758646918e-05,
      "loss": 1.9459,
      "step": 671300
    },
    {
      "epoch": 25.965889314305603,
      "grad_norm": 15.323867797851562,
      "learning_rate": 2.8361758904745332e-05,
      "loss": 1.8438,
      "step": 671400
    },
    {
      "epoch": 25.96975673898751,
      "grad_norm": 7.1549072265625,
      "learning_rate": 2.8358536050843744e-05,
      "loss": 1.8253,
      "step": 671500
    },
    {
      "epoch": 25.973624163669413,
      "grad_norm": 12.931324005126953,
      "learning_rate": 2.835531319694216e-05,
      "loss": 1.9141,
      "step": 671600
    },
    {
      "epoch": 25.977491588351317,
      "grad_norm": 13.344236373901367,
      "learning_rate": 2.8352090343040573e-05,
      "loss": 1.9819,
      "step": 671700
    },
    {
      "epoch": 25.98135901303322,
      "grad_norm": 12.712124824523926,
      "learning_rate": 2.8348867489138984e-05,
      "loss": 1.8762,
      "step": 671800
    },
    {
      "epoch": 25.985226437715127,
      "grad_norm": 12.529549598693848,
      "learning_rate": 2.8345644635237396e-05,
      "loss": 1.9549,
      "step": 671900
    },
    {
      "epoch": 25.98909386239703,
      "grad_norm": 13.082327842712402,
      "learning_rate": 2.834242178133581e-05,
      "loss": 1.9197,
      "step": 672000
    },
    {
      "epoch": 25.992961287078934,
      "grad_norm": 10.809154510498047,
      "learning_rate": 2.8339198927434225e-05,
      "loss": 1.9548,
      "step": 672100
    },
    {
      "epoch": 25.996828711760838,
      "grad_norm": 11.25632095336914,
      "learning_rate": 2.833597607353264e-05,
      "loss": 1.8775,
      "step": 672200
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.8261629343032837,
      "eval_runtime": 2.9317,
      "eval_samples_per_second": 464.235,
      "eval_steps_per_second": 464.235,
      "step": 672282
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.7105836868286133,
      "eval_runtime": 55.6963,
      "eval_samples_per_second": 464.25,
      "eval_steps_per_second": 464.25,
      "step": 672282
    },
    {
      "epoch": 26.00069613644274,
      "grad_norm": 4.967226982116699,
      "learning_rate": 2.8332753219631048e-05,
      "loss": 1.8441,
      "step": 672300
    },
    {
      "epoch": 26.004563561124648,
      "grad_norm": 9.311031341552734,
      "learning_rate": 2.8329530365729463e-05,
      "loss": 1.9221,
      "step": 672400
    },
    {
      "epoch": 26.00843098580655,
      "grad_norm": 12.744074821472168,
      "learning_rate": 2.8326307511827877e-05,
      "loss": 1.9175,
      "step": 672500
    },
    {
      "epoch": 26.012298410488455,
      "grad_norm": 12.625657081604004,
      "learning_rate": 2.8323084657926292e-05,
      "loss": 1.847,
      "step": 672600
    },
    {
      "epoch": 26.01616583517036,
      "grad_norm": 10.664139747619629,
      "learning_rate": 2.83198618040247e-05,
      "loss": 2.0748,
      "step": 672700
    },
    {
      "epoch": 26.020033259852266,
      "grad_norm": 14.17372989654541,
      "learning_rate": 2.8316638950123115e-05,
      "loss": 1.9206,
      "step": 672800
    },
    {
      "epoch": 26.02390068453417,
      "grad_norm": 9.152694702148438,
      "learning_rate": 2.831341609622153e-05,
      "loss": 1.8892,
      "step": 672900
    },
    {
      "epoch": 26.027768109216073,
      "grad_norm": 10.740896224975586,
      "learning_rate": 2.8310193242319937e-05,
      "loss": 1.9098,
      "step": 673000
    },
    {
      "epoch": 26.031635533897976,
      "grad_norm": 11.074974060058594,
      "learning_rate": 2.8306970388418352e-05,
      "loss": 1.8411,
      "step": 673100
    },
    {
      "epoch": 26.035502958579883,
      "grad_norm": 11.620920181274414,
      "learning_rate": 2.8303747534516767e-05,
      "loss": 1.9059,
      "step": 673200
    },
    {
      "epoch": 26.039370383261787,
      "grad_norm": 14.376270294189453,
      "learning_rate": 2.830052468061518e-05,
      "loss": 1.8486,
      "step": 673300
    },
    {
      "epoch": 26.04323780794369,
      "grad_norm": 11.213286399841309,
      "learning_rate": 2.829730182671359e-05,
      "loss": 1.8751,
      "step": 673400
    },
    {
      "epoch": 26.047105232625594,
      "grad_norm": 12.877257347106934,
      "learning_rate": 2.8294078972812004e-05,
      "loss": 1.8887,
      "step": 673500
    },
    {
      "epoch": 26.0509726573075,
      "grad_norm": 11.863152503967285,
      "learning_rate": 2.829085611891042e-05,
      "loss": 1.9294,
      "step": 673600
    },
    {
      "epoch": 26.054840081989404,
      "grad_norm": 11.837122917175293,
      "learning_rate": 2.8287633265008834e-05,
      "loss": 1.8746,
      "step": 673700
    },
    {
      "epoch": 26.058707506671308,
      "grad_norm": 8.658631324768066,
      "learning_rate": 2.828441041110724e-05,
      "loss": 1.8794,
      "step": 673800
    },
    {
      "epoch": 26.06257493135321,
      "grad_norm": 10.82052230834961,
      "learning_rate": 2.8281187557205656e-05,
      "loss": 1.9051,
      "step": 673900
    },
    {
      "epoch": 26.066442356035115,
      "grad_norm": 15.62354564666748,
      "learning_rate": 2.827796470330407e-05,
      "loss": 1.9257,
      "step": 674000
    },
    {
      "epoch": 26.07030978071702,
      "grad_norm": 11.073202133178711,
      "learning_rate": 2.8274741849402486e-05,
      "loss": 1.8961,
      "step": 674100
    },
    {
      "epoch": 26.074177205398925,
      "grad_norm": 9.089163780212402,
      "learning_rate": 2.8271518995500894e-05,
      "loss": 1.8569,
      "step": 674200
    },
    {
      "epoch": 26.07804463008083,
      "grad_norm": 11.746365547180176,
      "learning_rate": 2.826829614159931e-05,
      "loss": 1.8823,
      "step": 674300
    },
    {
      "epoch": 26.081912054762732,
      "grad_norm": 11.252623558044434,
      "learning_rate": 2.8265073287697723e-05,
      "loss": 1.8398,
      "step": 674400
    },
    {
      "epoch": 26.08577947944464,
      "grad_norm": 13.056465148925781,
      "learning_rate": 2.8261850433796138e-05,
      "loss": 1.883,
      "step": 674500
    },
    {
      "epoch": 26.089646904126543,
      "grad_norm": 9.901094436645508,
      "learning_rate": 2.825862757989455e-05,
      "loss": 1.9695,
      "step": 674600
    },
    {
      "epoch": 26.093514328808446,
      "grad_norm": 11.435437202453613,
      "learning_rate": 2.8255404725992964e-05,
      "loss": 1.8528,
      "step": 674700
    },
    {
      "epoch": 26.09738175349035,
      "grad_norm": 12.350879669189453,
      "learning_rate": 2.8252181872091375e-05,
      "loss": 1.8957,
      "step": 674800
    },
    {
      "epoch": 26.101249178172257,
      "grad_norm": 14.826251029968262,
      "learning_rate": 2.824895901818979e-05,
      "loss": 1.9526,
      "step": 674900
    },
    {
      "epoch": 26.10511660285416,
      "grad_norm": 11.980419158935547,
      "learning_rate": 2.82457361642882e-05,
      "loss": 1.8715,
      "step": 675000
    },
    {
      "epoch": 26.108984027536064,
      "grad_norm": 11.551656723022461,
      "learning_rate": 2.8242513310386616e-05,
      "loss": 1.9517,
      "step": 675100
    },
    {
      "epoch": 26.112851452217967,
      "grad_norm": 13.130983352661133,
      "learning_rate": 2.823929045648503e-05,
      "loss": 2.0425,
      "step": 675200
    },
    {
      "epoch": 26.116718876899874,
      "grad_norm": 10.061570167541504,
      "learning_rate": 2.8236067602583442e-05,
      "loss": 1.8348,
      "step": 675300
    },
    {
      "epoch": 26.120586301581778,
      "grad_norm": 10.731689453125,
      "learning_rate": 2.8232844748681853e-05,
      "loss": 1.8858,
      "step": 675400
    },
    {
      "epoch": 26.12445372626368,
      "grad_norm": 8.009567260742188,
      "learning_rate": 2.8229621894780268e-05,
      "loss": 1.8754,
      "step": 675500
    },
    {
      "epoch": 26.128321150945585,
      "grad_norm": 7.389472007751465,
      "learning_rate": 2.8226399040878683e-05,
      "loss": 1.8271,
      "step": 675600
    },
    {
      "epoch": 26.132188575627488,
      "grad_norm": 10.468932151794434,
      "learning_rate": 2.822317618697709e-05,
      "loss": 1.8429,
      "step": 675700
    },
    {
      "epoch": 26.136056000309395,
      "grad_norm": 13.279903411865234,
      "learning_rate": 2.8219953333075505e-05,
      "loss": 1.9616,
      "step": 675800
    },
    {
      "epoch": 26.1399234249913,
      "grad_norm": 10.646480560302734,
      "learning_rate": 2.821673047917392e-05,
      "loss": 2.003,
      "step": 675900
    },
    {
      "epoch": 26.143790849673202,
      "grad_norm": 11.146443367004395,
      "learning_rate": 2.8213507625272335e-05,
      "loss": 1.8562,
      "step": 676000
    },
    {
      "epoch": 26.147658274355106,
      "grad_norm": 9.478334426879883,
      "learning_rate": 2.8210284771370743e-05,
      "loss": 1.8829,
      "step": 676100
    },
    {
      "epoch": 26.151525699037013,
      "grad_norm": 13.318564414978027,
      "learning_rate": 2.8207061917469157e-05,
      "loss": 1.9657,
      "step": 676200
    },
    {
      "epoch": 26.155393123718916,
      "grad_norm": 10.69528579711914,
      "learning_rate": 2.8203839063567572e-05,
      "loss": 1.9633,
      "step": 676300
    },
    {
      "epoch": 26.15926054840082,
      "grad_norm": 12.229293823242188,
      "learning_rate": 2.8200616209665987e-05,
      "loss": 1.9293,
      "step": 676400
    },
    {
      "epoch": 26.163127973082723,
      "grad_norm": 14.537592887878418,
      "learning_rate": 2.8197393355764395e-05,
      "loss": 1.8863,
      "step": 676500
    },
    {
      "epoch": 26.16699539776463,
      "grad_norm": 11.942540168762207,
      "learning_rate": 2.819417050186281e-05,
      "loss": 1.8103,
      "step": 676600
    },
    {
      "epoch": 26.170862822446534,
      "grad_norm": 11.090120315551758,
      "learning_rate": 2.8190947647961224e-05,
      "loss": 1.8987,
      "step": 676700
    },
    {
      "epoch": 26.174730247128437,
      "grad_norm": 16.382240295410156,
      "learning_rate": 2.818772479405964e-05,
      "loss": 1.8732,
      "step": 676800
    },
    {
      "epoch": 26.17859767181034,
      "grad_norm": 12.270853042602539,
      "learning_rate": 2.8184501940158047e-05,
      "loss": 1.7912,
      "step": 676900
    },
    {
      "epoch": 26.182465096492244,
      "grad_norm": 10.851460456848145,
      "learning_rate": 2.818127908625646e-05,
      "loss": 1.8909,
      "step": 677000
    },
    {
      "epoch": 26.18633252117415,
      "grad_norm": 10.706392288208008,
      "learning_rate": 2.8178056232354876e-05,
      "loss": 1.8634,
      "step": 677100
    },
    {
      "epoch": 26.190199945856055,
      "grad_norm": 10.999808311462402,
      "learning_rate": 2.817483337845329e-05,
      "loss": 1.9754,
      "step": 677200
    },
    {
      "epoch": 26.194067370537958,
      "grad_norm": 12.794817924499512,
      "learning_rate": 2.81716105245517e-05,
      "loss": 1.8675,
      "step": 677300
    },
    {
      "epoch": 26.19793479521986,
      "grad_norm": 10.790096282958984,
      "learning_rate": 2.8168387670650114e-05,
      "loss": 1.8523,
      "step": 677400
    },
    {
      "epoch": 26.20180221990177,
      "grad_norm": 13.231049537658691,
      "learning_rate": 2.816516481674853e-05,
      "loss": 1.9227,
      "step": 677500
    },
    {
      "epoch": 26.205669644583672,
      "grad_norm": 9.224059104919434,
      "learning_rate": 2.8161941962846943e-05,
      "loss": 1.9296,
      "step": 677600
    },
    {
      "epoch": 26.209537069265576,
      "grad_norm": 17.477035522460938,
      "learning_rate": 2.815871910894535e-05,
      "loss": 1.9909,
      "step": 677700
    },
    {
      "epoch": 26.21340449394748,
      "grad_norm": 12.207939147949219,
      "learning_rate": 2.8155496255043766e-05,
      "loss": 1.8924,
      "step": 677800
    },
    {
      "epoch": 26.217271918629386,
      "grad_norm": 13.989520072937012,
      "learning_rate": 2.815227340114218e-05,
      "loss": 1.9167,
      "step": 677900
    },
    {
      "epoch": 26.22113934331129,
      "grad_norm": 12.333054542541504,
      "learning_rate": 2.8149050547240595e-05,
      "loss": 1.9086,
      "step": 678000
    },
    {
      "epoch": 26.225006767993193,
      "grad_norm": 14.346110343933105,
      "learning_rate": 2.8145827693339007e-05,
      "loss": 1.9323,
      "step": 678100
    },
    {
      "epoch": 26.228874192675097,
      "grad_norm": 11.648381233215332,
      "learning_rate": 2.814260483943742e-05,
      "loss": 1.8723,
      "step": 678200
    },
    {
      "epoch": 26.232741617357004,
      "grad_norm": 16.641756057739258,
      "learning_rate": 2.8139381985535833e-05,
      "loss": 1.9074,
      "step": 678300
    },
    {
      "epoch": 26.236609042038907,
      "grad_norm": 12.897930145263672,
      "learning_rate": 2.8136159131634247e-05,
      "loss": 1.9431,
      "step": 678400
    },
    {
      "epoch": 26.24047646672081,
      "grad_norm": 15.376374244689941,
      "learning_rate": 2.813293627773266e-05,
      "loss": 1.8444,
      "step": 678500
    },
    {
      "epoch": 26.244343891402714,
      "grad_norm": 12.351699829101562,
      "learning_rate": 2.8129713423831073e-05,
      "loss": 1.8916,
      "step": 678600
    },
    {
      "epoch": 26.248211316084618,
      "grad_norm": 11.936447143554688,
      "learning_rate": 2.8126490569929488e-05,
      "loss": 1.9274,
      "step": 678700
    },
    {
      "epoch": 26.252078740766525,
      "grad_norm": 12.49002742767334,
      "learning_rate": 2.8123267716027896e-05,
      "loss": 1.9812,
      "step": 678800
    },
    {
      "epoch": 26.255946165448428,
      "grad_norm": 12.943333625793457,
      "learning_rate": 2.812004486212631e-05,
      "loss": 1.8234,
      "step": 678900
    },
    {
      "epoch": 26.25981359013033,
      "grad_norm": 14.93945026397705,
      "learning_rate": 2.8116822008224726e-05,
      "loss": 1.7907,
      "step": 679000
    },
    {
      "epoch": 26.263681014812235,
      "grad_norm": 12.570383071899414,
      "learning_rate": 2.811359915432314e-05,
      "loss": 1.9273,
      "step": 679100
    },
    {
      "epoch": 26.267548439494142,
      "grad_norm": 9.21160888671875,
      "learning_rate": 2.8110376300421548e-05,
      "loss": 1.8399,
      "step": 679200
    },
    {
      "epoch": 26.271415864176046,
      "grad_norm": 9.102436065673828,
      "learning_rate": 2.8107153446519963e-05,
      "loss": 1.883,
      "step": 679300
    },
    {
      "epoch": 26.27528328885795,
      "grad_norm": 12.614981651306152,
      "learning_rate": 2.8103930592618378e-05,
      "loss": 1.8619,
      "step": 679400
    },
    {
      "epoch": 26.279150713539853,
      "grad_norm": 11.92741584777832,
      "learning_rate": 2.8100707738716792e-05,
      "loss": 1.9947,
      "step": 679500
    },
    {
      "epoch": 26.28301813822176,
      "grad_norm": 11.712018013000488,
      "learning_rate": 2.80974848848152e-05,
      "loss": 1.8913,
      "step": 679600
    },
    {
      "epoch": 26.286885562903663,
      "grad_norm": 12.073199272155762,
      "learning_rate": 2.8094262030913615e-05,
      "loss": 1.9266,
      "step": 679700
    },
    {
      "epoch": 26.290752987585567,
      "grad_norm": 16.27954864501953,
      "learning_rate": 2.809103917701203e-05,
      "loss": 1.8933,
      "step": 679800
    },
    {
      "epoch": 26.29462041226747,
      "grad_norm": 12.223284721374512,
      "learning_rate": 2.8087816323110444e-05,
      "loss": 1.9576,
      "step": 679900
    },
    {
      "epoch": 26.298487836949377,
      "grad_norm": 13.723958969116211,
      "learning_rate": 2.8084593469208852e-05,
      "loss": 1.9692,
      "step": 680000
    },
    {
      "epoch": 26.30235526163128,
      "grad_norm": 7.588448524475098,
      "learning_rate": 2.8081370615307267e-05,
      "loss": 1.7983,
      "step": 680100
    },
    {
      "epoch": 26.306222686313184,
      "grad_norm": 10.564194679260254,
      "learning_rate": 2.8078147761405682e-05,
      "loss": 1.8094,
      "step": 680200
    },
    {
      "epoch": 26.310090110995088,
      "grad_norm": 12.07267951965332,
      "learning_rate": 2.8074924907504097e-05,
      "loss": 1.8599,
      "step": 680300
    },
    {
      "epoch": 26.31395753567699,
      "grad_norm": 12.480559349060059,
      "learning_rate": 2.8071702053602505e-05,
      "loss": 1.916,
      "step": 680400
    },
    {
      "epoch": 26.317824960358898,
      "grad_norm": 15.735258102416992,
      "learning_rate": 2.806847919970092e-05,
      "loss": 1.9482,
      "step": 680500
    },
    {
      "epoch": 26.3216923850408,
      "grad_norm": 12.067534446716309,
      "learning_rate": 2.8065256345799334e-05,
      "loss": 1.8705,
      "step": 680600
    },
    {
      "epoch": 26.325559809722705,
      "grad_norm": 11.525053024291992,
      "learning_rate": 2.806203349189775e-05,
      "loss": 1.9464,
      "step": 680700
    },
    {
      "epoch": 26.32942723440461,
      "grad_norm": 10.715479850769043,
      "learning_rate": 2.8058810637996157e-05,
      "loss": 1.8858,
      "step": 680800
    },
    {
      "epoch": 26.333294659086516,
      "grad_norm": 10.404182434082031,
      "learning_rate": 2.805558778409457e-05,
      "loss": 1.8987,
      "step": 680900
    },
    {
      "epoch": 26.33716208376842,
      "grad_norm": 12.32959270477295,
      "learning_rate": 2.8052364930192986e-05,
      "loss": 1.8231,
      "step": 681000
    },
    {
      "epoch": 26.341029508450323,
      "grad_norm": 11.481315612792969,
      "learning_rate": 2.80491420762914e-05,
      "loss": 1.9902,
      "step": 681100
    },
    {
      "epoch": 26.344896933132226,
      "grad_norm": 11.81773567199707,
      "learning_rate": 2.804591922238981e-05,
      "loss": 1.9754,
      "step": 681200
    },
    {
      "epoch": 26.348764357814133,
      "grad_norm": 11.877660751342773,
      "learning_rate": 2.8042696368488223e-05,
      "loss": 1.9178,
      "step": 681300
    },
    {
      "epoch": 26.352631782496037,
      "grad_norm": 12.020439147949219,
      "learning_rate": 2.8039473514586638e-05,
      "loss": 1.8839,
      "step": 681400
    },
    {
      "epoch": 26.35649920717794,
      "grad_norm": 13.195014953613281,
      "learning_rate": 2.8036250660685053e-05,
      "loss": 1.9144,
      "step": 681500
    },
    {
      "epoch": 26.360366631859844,
      "grad_norm": 10.99659538269043,
      "learning_rate": 2.8033027806783464e-05,
      "loss": 1.8776,
      "step": 681600
    },
    {
      "epoch": 26.36423405654175,
      "grad_norm": 11.22713565826416,
      "learning_rate": 2.802980495288188e-05,
      "loss": 1.945,
      "step": 681700
    },
    {
      "epoch": 26.368101481223654,
      "grad_norm": 15.774924278259277,
      "learning_rate": 2.802658209898029e-05,
      "loss": 1.8956,
      "step": 681800
    },
    {
      "epoch": 26.371968905905558,
      "grad_norm": 9.384725570678711,
      "learning_rate": 2.80233592450787e-05,
      "loss": 1.8695,
      "step": 681900
    },
    {
      "epoch": 26.37583633058746,
      "grad_norm": 9.803264617919922,
      "learning_rate": 2.8020136391177116e-05,
      "loss": 1.9336,
      "step": 682000
    },
    {
      "epoch": 26.379703755269365,
      "grad_norm": 11.55099105834961,
      "learning_rate": 2.801691353727553e-05,
      "loss": 1.9047,
      "step": 682100
    },
    {
      "epoch": 26.38357117995127,
      "grad_norm": 11.76889705657959,
      "learning_rate": 2.8013690683373946e-05,
      "loss": 1.8599,
      "step": 682200
    },
    {
      "epoch": 26.387438604633175,
      "grad_norm": 21.463951110839844,
      "learning_rate": 2.8010467829472354e-05,
      "loss": 1.9305,
      "step": 682300
    },
    {
      "epoch": 26.39130602931508,
      "grad_norm": 9.856096267700195,
      "learning_rate": 2.800724497557077e-05,
      "loss": 1.8911,
      "step": 682400
    },
    {
      "epoch": 26.395173453996982,
      "grad_norm": 13.301331520080566,
      "learning_rate": 2.8004022121669183e-05,
      "loss": 1.8443,
      "step": 682500
    },
    {
      "epoch": 26.39904087867889,
      "grad_norm": 13.465537071228027,
      "learning_rate": 2.8000799267767598e-05,
      "loss": 1.8768,
      "step": 682600
    },
    {
      "epoch": 26.402908303360793,
      "grad_norm": 11.43371295928955,
      "learning_rate": 2.7997576413866006e-05,
      "loss": 1.9252,
      "step": 682700
    },
    {
      "epoch": 26.406775728042696,
      "grad_norm": 13.4212646484375,
      "learning_rate": 2.799435355996442e-05,
      "loss": 1.836,
      "step": 682800
    },
    {
      "epoch": 26.4106431527246,
      "grad_norm": 15.227080345153809,
      "learning_rate": 2.7991130706062835e-05,
      "loss": 1.8612,
      "step": 682900
    },
    {
      "epoch": 26.414510577406507,
      "grad_norm": 9.622657775878906,
      "learning_rate": 2.798790785216125e-05,
      "loss": 1.8805,
      "step": 683000
    },
    {
      "epoch": 26.41837800208841,
      "grad_norm": 13.284480094909668,
      "learning_rate": 2.7984684998259658e-05,
      "loss": 1.8345,
      "step": 683100
    },
    {
      "epoch": 26.422245426770314,
      "grad_norm": 8.578556060791016,
      "learning_rate": 2.7981462144358073e-05,
      "loss": 1.8886,
      "step": 683200
    },
    {
      "epoch": 26.426112851452217,
      "grad_norm": 11.599964141845703,
      "learning_rate": 2.7978239290456487e-05,
      "loss": 2.0203,
      "step": 683300
    },
    {
      "epoch": 26.429980276134124,
      "grad_norm": 13.148285865783691,
      "learning_rate": 2.7975016436554902e-05,
      "loss": 1.9282,
      "step": 683400
    },
    {
      "epoch": 26.433847700816028,
      "grad_norm": 13.92176342010498,
      "learning_rate": 2.797179358265331e-05,
      "loss": 1.9308,
      "step": 683500
    },
    {
      "epoch": 26.43771512549793,
      "grad_norm": 11.276632308959961,
      "learning_rate": 2.7968570728751725e-05,
      "loss": 1.8602,
      "step": 683600
    },
    {
      "epoch": 26.441582550179834,
      "grad_norm": 14.488104820251465,
      "learning_rate": 2.796534787485014e-05,
      "loss": 1.865,
      "step": 683700
    },
    {
      "epoch": 26.445449974861738,
      "grad_norm": 10.859784126281738,
      "learning_rate": 2.7962125020948554e-05,
      "loss": 1.8758,
      "step": 683800
    },
    {
      "epoch": 26.449317399543645,
      "grad_norm": 13.630388259887695,
      "learning_rate": 2.7958902167046962e-05,
      "loss": 1.984,
      "step": 683900
    },
    {
      "epoch": 26.45318482422555,
      "grad_norm": 10.839652061462402,
      "learning_rate": 2.7955679313145377e-05,
      "loss": 1.9807,
      "step": 684000
    },
    {
      "epoch": 26.457052248907452,
      "grad_norm": 14.039647102355957,
      "learning_rate": 2.795245645924379e-05,
      "loss": 1.8497,
      "step": 684100
    },
    {
      "epoch": 26.460919673589355,
      "grad_norm": 11.146355628967285,
      "learning_rate": 2.7949233605342206e-05,
      "loss": 1.9079,
      "step": 684200
    },
    {
      "epoch": 26.464787098271263,
      "grad_norm": 12.034926414489746,
      "learning_rate": 2.7946010751440614e-05,
      "loss": 1.8936,
      "step": 684300
    },
    {
      "epoch": 26.468654522953166,
      "grad_norm": 14.878978729248047,
      "learning_rate": 2.794278789753903e-05,
      "loss": 1.846,
      "step": 684400
    },
    {
      "epoch": 26.47252194763507,
      "grad_norm": 14.489058494567871,
      "learning_rate": 2.7939565043637444e-05,
      "loss": 1.955,
      "step": 684500
    },
    {
      "epoch": 26.476389372316973,
      "grad_norm": 10.347281455993652,
      "learning_rate": 2.7936342189735855e-05,
      "loss": 1.907,
      "step": 684600
    },
    {
      "epoch": 26.48025679699888,
      "grad_norm": 12.987201690673828,
      "learning_rate": 2.793311933583427e-05,
      "loss": 1.8655,
      "step": 684700
    },
    {
      "epoch": 26.484124221680784,
      "grad_norm": 12.070642471313477,
      "learning_rate": 2.792989648193268e-05,
      "loss": 1.8957,
      "step": 684800
    },
    {
      "epoch": 26.487991646362687,
      "grad_norm": 12.238673210144043,
      "learning_rate": 2.7926673628031096e-05,
      "loss": 1.9216,
      "step": 684900
    },
    {
      "epoch": 26.49185907104459,
      "grad_norm": 10.9982271194458,
      "learning_rate": 2.7923450774129507e-05,
      "loss": 1.928,
      "step": 685000
    },
    {
      "epoch": 26.495726495726494,
      "grad_norm": 13.173718452453613,
      "learning_rate": 2.7920227920227922e-05,
      "loss": 1.9644,
      "step": 685100
    },
    {
      "epoch": 26.4995939204084,
      "grad_norm": 14.585979461669922,
      "learning_rate": 2.7917005066326336e-05,
      "loss": 1.8577,
      "step": 685200
    },
    {
      "epoch": 26.503461345090304,
      "grad_norm": 11.991018295288086,
      "learning_rate": 2.7913782212424748e-05,
      "loss": 1.9939,
      "step": 685300
    },
    {
      "epoch": 26.507328769772208,
      "grad_norm": 11.128902435302734,
      "learning_rate": 2.791055935852316e-05,
      "loss": 1.9188,
      "step": 685400
    },
    {
      "epoch": 26.51119619445411,
      "grad_norm": 12.295269012451172,
      "learning_rate": 2.7907336504621574e-05,
      "loss": 1.8855,
      "step": 685500
    },
    {
      "epoch": 26.51506361913602,
      "grad_norm": 12.863810539245605,
      "learning_rate": 2.790411365071999e-05,
      "loss": 1.7978,
      "step": 685600
    },
    {
      "epoch": 26.518931043817922,
      "grad_norm": 11.970446586608887,
      "learning_rate": 2.7900890796818403e-05,
      "loss": 1.9819,
      "step": 685700
    },
    {
      "epoch": 26.522798468499825,
      "grad_norm": 11.662203788757324,
      "learning_rate": 2.789766794291681e-05,
      "loss": 1.9511,
      "step": 685800
    },
    {
      "epoch": 26.52666589318173,
      "grad_norm": 11.237203598022461,
      "learning_rate": 2.7894445089015226e-05,
      "loss": 1.8983,
      "step": 685900
    },
    {
      "epoch": 26.530533317863636,
      "grad_norm": 10.570401191711426,
      "learning_rate": 2.789122223511364e-05,
      "loss": 1.9473,
      "step": 686000
    },
    {
      "epoch": 26.53440074254554,
      "grad_norm": 14.20502758026123,
      "learning_rate": 2.7887999381212055e-05,
      "loss": 1.8251,
      "step": 686100
    },
    {
      "epoch": 26.538268167227443,
      "grad_norm": 12.567012786865234,
      "learning_rate": 2.7884776527310463e-05,
      "loss": 1.8399,
      "step": 686200
    },
    {
      "epoch": 26.542135591909346,
      "grad_norm": 12.478629112243652,
      "learning_rate": 2.7881553673408878e-05,
      "loss": 1.8842,
      "step": 686300
    },
    {
      "epoch": 26.546003016591253,
      "grad_norm": 8.566268920898438,
      "learning_rate": 2.7878330819507293e-05,
      "loss": 1.9234,
      "step": 686400
    },
    {
      "epoch": 26.549870441273157,
      "grad_norm": 9.365978240966797,
      "learning_rate": 2.7875107965605707e-05,
      "loss": 1.9,
      "step": 686500
    },
    {
      "epoch": 26.55373786595506,
      "grad_norm": 12.414697647094727,
      "learning_rate": 2.7871885111704115e-05,
      "loss": 1.8588,
      "step": 686600
    },
    {
      "epoch": 26.557605290636964,
      "grad_norm": 12.617908477783203,
      "learning_rate": 2.786866225780253e-05,
      "loss": 1.9323,
      "step": 686700
    },
    {
      "epoch": 26.56147271531887,
      "grad_norm": 13.687946319580078,
      "learning_rate": 2.7865439403900945e-05,
      "loss": 1.9629,
      "step": 686800
    },
    {
      "epoch": 26.565340140000774,
      "grad_norm": 8.9879732131958,
      "learning_rate": 2.786221654999936e-05,
      "loss": 1.9305,
      "step": 686900
    },
    {
      "epoch": 26.569207564682678,
      "grad_norm": 9.490856170654297,
      "learning_rate": 2.7858993696097768e-05,
      "loss": 1.8931,
      "step": 687000
    },
    {
      "epoch": 26.57307498936458,
      "grad_norm": 13.5429048538208,
      "learning_rate": 2.7855770842196182e-05,
      "loss": 1.9132,
      "step": 687100
    },
    {
      "epoch": 26.576942414046485,
      "grad_norm": 11.070352554321289,
      "learning_rate": 2.7852547988294597e-05,
      "loss": 1.8923,
      "step": 687200
    },
    {
      "epoch": 26.580809838728392,
      "grad_norm": 11.239723205566406,
      "learning_rate": 2.784932513439301e-05,
      "loss": 1.8987,
      "step": 687300
    },
    {
      "epoch": 26.584677263410295,
      "grad_norm": 14.96126937866211,
      "learning_rate": 2.784610228049142e-05,
      "loss": 1.886,
      "step": 687400
    },
    {
      "epoch": 26.5885446880922,
      "grad_norm": 8.994189262390137,
      "learning_rate": 2.7842879426589834e-05,
      "loss": 1.8636,
      "step": 687500
    },
    {
      "epoch": 26.592412112774102,
      "grad_norm": 10.977505683898926,
      "learning_rate": 2.783965657268825e-05,
      "loss": 1.9486,
      "step": 687600
    },
    {
      "epoch": 26.59627953745601,
      "grad_norm": 12.359375953674316,
      "learning_rate": 2.7836433718786657e-05,
      "loss": 1.9705,
      "step": 687700
    },
    {
      "epoch": 26.600146962137913,
      "grad_norm": 9.970285415649414,
      "learning_rate": 2.7833210864885072e-05,
      "loss": 1.9348,
      "step": 687800
    },
    {
      "epoch": 26.604014386819816,
      "grad_norm": 13.19030475616455,
      "learning_rate": 2.7829988010983486e-05,
      "loss": 1.977,
      "step": 687900
    },
    {
      "epoch": 26.60788181150172,
      "grad_norm": 13.386427879333496,
      "learning_rate": 2.78267651570819e-05,
      "loss": 1.927,
      "step": 688000
    },
    {
      "epoch": 26.611749236183627,
      "grad_norm": 10.322999000549316,
      "learning_rate": 2.7823542303180312e-05,
      "loss": 1.9018,
      "step": 688100
    },
    {
      "epoch": 26.61561666086553,
      "grad_norm": 13.493572235107422,
      "learning_rate": 2.7820319449278727e-05,
      "loss": 1.9813,
      "step": 688200
    },
    {
      "epoch": 26.619484085547434,
      "grad_norm": 10.357641220092773,
      "learning_rate": 2.781709659537714e-05,
      "loss": 1.9372,
      "step": 688300
    },
    {
      "epoch": 26.623351510229337,
      "grad_norm": 8.94115924835205,
      "learning_rate": 2.7813873741475553e-05,
      "loss": 1.8592,
      "step": 688400
    },
    {
      "epoch": 26.62721893491124,
      "grad_norm": 13.470206260681152,
      "learning_rate": 2.7810650887573965e-05,
      "loss": 1.9717,
      "step": 688500
    },
    {
      "epoch": 26.631086359593148,
      "grad_norm": 14.470213890075684,
      "learning_rate": 2.780742803367238e-05,
      "loss": 1.8699,
      "step": 688600
    },
    {
      "epoch": 26.63495378427505,
      "grad_norm": 10.447269439697266,
      "learning_rate": 2.7804205179770794e-05,
      "loss": 1.8853,
      "step": 688700
    },
    {
      "epoch": 26.638821208956955,
      "grad_norm": 14.052392959594727,
      "learning_rate": 2.780098232586921e-05,
      "loss": 2.0007,
      "step": 688800
    },
    {
      "epoch": 26.64268863363886,
      "grad_norm": 11.76364803314209,
      "learning_rate": 2.7797759471967617e-05,
      "loss": 1.9835,
      "step": 688900
    },
    {
      "epoch": 26.646556058320765,
      "grad_norm": 13.578579902648926,
      "learning_rate": 2.779453661806603e-05,
      "loss": 1.8437,
      "step": 689000
    },
    {
      "epoch": 26.65042348300267,
      "grad_norm": 9.161601066589355,
      "learning_rate": 2.7791313764164446e-05,
      "loss": 1.8241,
      "step": 689100
    },
    {
      "epoch": 26.654290907684572,
      "grad_norm": 12.234819412231445,
      "learning_rate": 2.778809091026286e-05,
      "loss": 1.9058,
      "step": 689200
    },
    {
      "epoch": 26.658158332366476,
      "grad_norm": 13.03593635559082,
      "learning_rate": 2.778486805636127e-05,
      "loss": 1.9048,
      "step": 689300
    },
    {
      "epoch": 26.662025757048383,
      "grad_norm": 14.147451400756836,
      "learning_rate": 2.7781645202459683e-05,
      "loss": 1.9904,
      "step": 689400
    },
    {
      "epoch": 26.665893181730286,
      "grad_norm": 11.993876457214355,
      "learning_rate": 2.7778422348558098e-05,
      "loss": 1.9036,
      "step": 689500
    },
    {
      "epoch": 26.66976060641219,
      "grad_norm": 11.40287971496582,
      "learning_rate": 2.7775199494656513e-05,
      "loss": 1.9541,
      "step": 689600
    },
    {
      "epoch": 26.673628031094093,
      "grad_norm": 13.139699935913086,
      "learning_rate": 2.777197664075492e-05,
      "loss": 1.9197,
      "step": 689700
    },
    {
      "epoch": 26.677495455776,
      "grad_norm": 9.8485746383667,
      "learning_rate": 2.7768753786853336e-05,
      "loss": 2.014,
      "step": 689800
    },
    {
      "epoch": 26.681362880457904,
      "grad_norm": 14.506327629089355,
      "learning_rate": 2.776553093295175e-05,
      "loss": 1.9687,
      "step": 689900
    },
    {
      "epoch": 26.685230305139807,
      "grad_norm": 14.449021339416504,
      "learning_rate": 2.7762308079050165e-05,
      "loss": 1.8949,
      "step": 690000
    },
    {
      "epoch": 26.68909772982171,
      "grad_norm": 14.346259117126465,
      "learning_rate": 2.7759085225148573e-05,
      "loss": 2.0079,
      "step": 690100
    },
    {
      "epoch": 26.692965154503614,
      "grad_norm": 14.277039527893066,
      "learning_rate": 2.7755862371246988e-05,
      "loss": 1.8938,
      "step": 690200
    },
    {
      "epoch": 26.69683257918552,
      "grad_norm": 11.808562278747559,
      "learning_rate": 2.7752639517345402e-05,
      "loss": 1.8749,
      "step": 690300
    },
    {
      "epoch": 26.700700003867425,
      "grad_norm": 10.020272254943848,
      "learning_rate": 2.7749416663443817e-05,
      "loss": 1.961,
      "step": 690400
    },
    {
      "epoch": 26.70456742854933,
      "grad_norm": 9.251239776611328,
      "learning_rate": 2.7746193809542225e-05,
      "loss": 1.948,
      "step": 690500
    },
    {
      "epoch": 26.708434853231232,
      "grad_norm": 13.493854522705078,
      "learning_rate": 2.774297095564064e-05,
      "loss": 1.829,
      "step": 690600
    },
    {
      "epoch": 26.71230227791314,
      "grad_norm": 8.89011287689209,
      "learning_rate": 2.7739748101739054e-05,
      "loss": 2.0277,
      "step": 690700
    },
    {
      "epoch": 26.716169702595042,
      "grad_norm": 10.831506729125977,
      "learning_rate": 2.7736525247837462e-05,
      "loss": 1.8657,
      "step": 690800
    },
    {
      "epoch": 26.720037127276946,
      "grad_norm": 9.721129417419434,
      "learning_rate": 2.7733302393935877e-05,
      "loss": 1.8729,
      "step": 690900
    },
    {
      "epoch": 26.72390455195885,
      "grad_norm": 16.9970760345459,
      "learning_rate": 2.7730079540034292e-05,
      "loss": 1.9211,
      "step": 691000
    },
    {
      "epoch": 26.727771976640756,
      "grad_norm": 10.34642219543457,
      "learning_rate": 2.7726856686132707e-05,
      "loss": 1.884,
      "step": 691100
    },
    {
      "epoch": 26.73163940132266,
      "grad_norm": 13.805222511291504,
      "learning_rate": 2.7723633832231115e-05,
      "loss": 1.9741,
      "step": 691200
    },
    {
      "epoch": 26.735506826004563,
      "grad_norm": 12.416687965393066,
      "learning_rate": 2.772041097832953e-05,
      "loss": 1.8894,
      "step": 691300
    },
    {
      "epoch": 26.739374250686467,
      "grad_norm": 13.193074226379395,
      "learning_rate": 2.7717188124427944e-05,
      "loss": 1.9559,
      "step": 691400
    },
    {
      "epoch": 26.743241675368374,
      "grad_norm": 11.726273536682129,
      "learning_rate": 2.771396527052636e-05,
      "loss": 1.9268,
      "step": 691500
    },
    {
      "epoch": 26.747109100050277,
      "grad_norm": 10.4136323928833,
      "learning_rate": 2.771074241662477e-05,
      "loss": 1.9025,
      "step": 691600
    },
    {
      "epoch": 26.75097652473218,
      "grad_norm": 13.330280303955078,
      "learning_rate": 2.7707519562723185e-05,
      "loss": 1.918,
      "step": 691700
    },
    {
      "epoch": 26.754843949414084,
      "grad_norm": 10.23220443725586,
      "learning_rate": 2.7704296708821596e-05,
      "loss": 1.9039,
      "step": 691800
    },
    {
      "epoch": 26.758711374095988,
      "grad_norm": 15.644449234008789,
      "learning_rate": 2.770107385492001e-05,
      "loss": 1.9148,
      "step": 691900
    },
    {
      "epoch": 26.762578798777895,
      "grad_norm": 8.548273086547852,
      "learning_rate": 2.7697851001018422e-05,
      "loss": 1.8439,
      "step": 692000
    },
    {
      "epoch": 26.7664462234598,
      "grad_norm": 7.999567031860352,
      "learning_rate": 2.7694628147116837e-05,
      "loss": 1.8894,
      "step": 692100
    },
    {
      "epoch": 26.770313648141702,
      "grad_norm": 14.029953956604004,
      "learning_rate": 2.769140529321525e-05,
      "loss": 1.9567,
      "step": 692200
    },
    {
      "epoch": 26.774181072823605,
      "grad_norm": 11.710389137268066,
      "learning_rate": 2.7688182439313666e-05,
      "loss": 1.933,
      "step": 692300
    },
    {
      "epoch": 26.778048497505512,
      "grad_norm": 12.070476531982422,
      "learning_rate": 2.7684959585412074e-05,
      "loss": 1.8535,
      "step": 692400
    },
    {
      "epoch": 26.781915922187416,
      "grad_norm": 12.708480834960938,
      "learning_rate": 2.768173673151049e-05,
      "loss": 1.8471,
      "step": 692500
    },
    {
      "epoch": 26.78578334686932,
      "grad_norm": 15.13976764678955,
      "learning_rate": 2.7678513877608904e-05,
      "loss": 1.8572,
      "step": 692600
    },
    {
      "epoch": 26.789650771551223,
      "grad_norm": 12.015718460083008,
      "learning_rate": 2.767529102370732e-05,
      "loss": 1.9146,
      "step": 692700
    },
    {
      "epoch": 26.79351819623313,
      "grad_norm": 3.613715171813965,
      "learning_rate": 2.7672068169805726e-05,
      "loss": 1.9032,
      "step": 692800
    },
    {
      "epoch": 26.797385620915033,
      "grad_norm": 13.240569114685059,
      "learning_rate": 2.766884531590414e-05,
      "loss": 1.8173,
      "step": 692900
    },
    {
      "epoch": 26.801253045596937,
      "grad_norm": 11.968879699707031,
      "learning_rate": 2.7665622462002556e-05,
      "loss": 1.8389,
      "step": 693000
    },
    {
      "epoch": 26.80512047027884,
      "grad_norm": 14.423417091369629,
      "learning_rate": 2.766239960810097e-05,
      "loss": 1.8906,
      "step": 693100
    },
    {
      "epoch": 26.808987894960744,
      "grad_norm": 9.200627326965332,
      "learning_rate": 2.765917675419938e-05,
      "loss": 1.9026,
      "step": 693200
    },
    {
      "epoch": 26.81285531964265,
      "grad_norm": 11.9367036819458,
      "learning_rate": 2.7655953900297793e-05,
      "loss": 1.9046,
      "step": 693300
    },
    {
      "epoch": 26.816722744324554,
      "grad_norm": 6.541742324829102,
      "learning_rate": 2.7652731046396208e-05,
      "loss": 1.9984,
      "step": 693400
    },
    {
      "epoch": 26.820590169006458,
      "grad_norm": 11.515996932983398,
      "learning_rate": 2.7649508192494616e-05,
      "loss": 1.8279,
      "step": 693500
    },
    {
      "epoch": 26.82445759368836,
      "grad_norm": 9.61855411529541,
      "learning_rate": 2.764628533859303e-05,
      "loss": 1.9629,
      "step": 693600
    },
    {
      "epoch": 26.82832501837027,
      "grad_norm": 11.803035736083984,
      "learning_rate": 2.7643062484691445e-05,
      "loss": 1.8623,
      "step": 693700
    },
    {
      "epoch": 26.832192443052172,
      "grad_norm": 18.76756477355957,
      "learning_rate": 2.763983963078986e-05,
      "loss": 1.9949,
      "step": 693800
    },
    {
      "epoch": 26.836059867734075,
      "grad_norm": 12.360801696777344,
      "learning_rate": 2.7636616776888268e-05,
      "loss": 1.8543,
      "step": 693900
    },
    {
      "epoch": 26.83992729241598,
      "grad_norm": 12.266473770141602,
      "learning_rate": 2.7633393922986683e-05,
      "loss": 1.9999,
      "step": 694000
    },
    {
      "epoch": 26.843794717097886,
      "grad_norm": 13.081186294555664,
      "learning_rate": 2.7630171069085097e-05,
      "loss": 1.8801,
      "step": 694100
    },
    {
      "epoch": 26.84766214177979,
      "grad_norm": 11.452038764953613,
      "learning_rate": 2.7626948215183512e-05,
      "loss": 1.9393,
      "step": 694200
    },
    {
      "epoch": 26.851529566461693,
      "grad_norm": 11.540647506713867,
      "learning_rate": 2.762372536128192e-05,
      "loss": 1.8701,
      "step": 694300
    },
    {
      "epoch": 26.855396991143596,
      "grad_norm": 10.817153930664062,
      "learning_rate": 2.7620502507380335e-05,
      "loss": 1.858,
      "step": 694400
    },
    {
      "epoch": 26.859264415825503,
      "grad_norm": 9.950570106506348,
      "learning_rate": 2.761727965347875e-05,
      "loss": 1.91,
      "step": 694500
    },
    {
      "epoch": 26.863131840507407,
      "grad_norm": 12.415382385253906,
      "learning_rate": 2.7614056799577164e-05,
      "loss": 1.8943,
      "step": 694600
    },
    {
      "epoch": 26.86699926518931,
      "grad_norm": 12.498266220092773,
      "learning_rate": 2.7610833945675575e-05,
      "loss": 1.8837,
      "step": 694700
    },
    {
      "epoch": 26.870866689871214,
      "grad_norm": 9.043560981750488,
      "learning_rate": 2.7607611091773987e-05,
      "loss": 1.9652,
      "step": 694800
    },
    {
      "epoch": 26.87473411455312,
      "grad_norm": 15.538724899291992,
      "learning_rate": 2.76043882378724e-05,
      "loss": 1.8434,
      "step": 694900
    },
    {
      "epoch": 26.878601539235024,
      "grad_norm": 12.039607048034668,
      "learning_rate": 2.7601165383970816e-05,
      "loss": 1.9512,
      "step": 695000
    },
    {
      "epoch": 26.882468963916928,
      "grad_norm": 12.47131633758545,
      "learning_rate": 2.7597942530069228e-05,
      "loss": 1.9237,
      "step": 695100
    },
    {
      "epoch": 26.88633638859883,
      "grad_norm": 11.444074630737305,
      "learning_rate": 2.7594719676167642e-05,
      "loss": 1.9672,
      "step": 695200
    },
    {
      "epoch": 26.890203813280735,
      "grad_norm": 11.070237159729004,
      "learning_rate": 2.7591496822266054e-05,
      "loss": 1.862,
      "step": 695300
    },
    {
      "epoch": 26.894071237962642,
      "grad_norm": 11.964783668518066,
      "learning_rate": 2.758827396836447e-05,
      "loss": 1.9606,
      "step": 695400
    },
    {
      "epoch": 26.897938662644545,
      "grad_norm": 7.386235237121582,
      "learning_rate": 2.758505111446288e-05,
      "loss": 1.8802,
      "step": 695500
    },
    {
      "epoch": 26.90180608732645,
      "grad_norm": 9.509105682373047,
      "learning_rate": 2.7581828260561294e-05,
      "loss": 1.8836,
      "step": 695600
    },
    {
      "epoch": 26.905673512008352,
      "grad_norm": 15.19799518585205,
      "learning_rate": 2.757860540665971e-05,
      "loss": 1.9088,
      "step": 695700
    },
    {
      "epoch": 26.90954093669026,
      "grad_norm": 12.745139122009277,
      "learning_rate": 2.7575382552758124e-05,
      "loss": 1.8932,
      "step": 695800
    },
    {
      "epoch": 26.913408361372163,
      "grad_norm": 11.923823356628418,
      "learning_rate": 2.7572159698856532e-05,
      "loss": 2.0791,
      "step": 695900
    },
    {
      "epoch": 26.917275786054066,
      "grad_norm": 13.419790267944336,
      "learning_rate": 2.7568936844954946e-05,
      "loss": 1.9316,
      "step": 696000
    },
    {
      "epoch": 26.92114321073597,
      "grad_norm": 10.019102096557617,
      "learning_rate": 2.756571399105336e-05,
      "loss": 1.8744,
      "step": 696100
    },
    {
      "epoch": 26.925010635417877,
      "grad_norm": 13.32447624206543,
      "learning_rate": 2.7562491137151776e-05,
      "loss": 1.951,
      "step": 696200
    },
    {
      "epoch": 26.92887806009978,
      "grad_norm": 11.826476097106934,
      "learning_rate": 2.7559268283250184e-05,
      "loss": 1.9736,
      "step": 696300
    },
    {
      "epoch": 26.932745484781684,
      "grad_norm": 13.170411109924316,
      "learning_rate": 2.75560454293486e-05,
      "loss": 1.8646,
      "step": 696400
    },
    {
      "epoch": 26.936612909463587,
      "grad_norm": 10.422353744506836,
      "learning_rate": 2.7552822575447013e-05,
      "loss": 2.0238,
      "step": 696500
    },
    {
      "epoch": 26.94048033414549,
      "grad_norm": 9.098774909973145,
      "learning_rate": 2.754959972154542e-05,
      "loss": 1.9838,
      "step": 696600
    },
    {
      "epoch": 26.944347758827398,
      "grad_norm": 12.476550102233887,
      "learning_rate": 2.7546376867643836e-05,
      "loss": 1.9354,
      "step": 696700
    },
    {
      "epoch": 26.9482151835093,
      "grad_norm": 9.193187713623047,
      "learning_rate": 2.754315401374225e-05,
      "loss": 1.8946,
      "step": 696800
    },
    {
      "epoch": 26.952082608191205,
      "grad_norm": 10.06137752532959,
      "learning_rate": 2.7539931159840665e-05,
      "loss": 1.9334,
      "step": 696900
    },
    {
      "epoch": 26.95595003287311,
      "grad_norm": 11.524246215820312,
      "learning_rate": 2.7536708305939073e-05,
      "loss": 1.9122,
      "step": 697000
    },
    {
      "epoch": 26.959817457555015,
      "grad_norm": 12.900289535522461,
      "learning_rate": 2.7533485452037488e-05,
      "loss": 1.9214,
      "step": 697100
    },
    {
      "epoch": 26.96368488223692,
      "grad_norm": 7.710867881774902,
      "learning_rate": 2.7530262598135903e-05,
      "loss": 1.8887,
      "step": 697200
    },
    {
      "epoch": 26.967552306918822,
      "grad_norm": 13.336105346679688,
      "learning_rate": 2.7527039744234317e-05,
      "loss": 1.9588,
      "step": 697300
    },
    {
      "epoch": 26.971419731600726,
      "grad_norm": 11.96669864654541,
      "learning_rate": 2.7523816890332725e-05,
      "loss": 1.9474,
      "step": 697400
    },
    {
      "epoch": 26.975287156282633,
      "grad_norm": 10.557592391967773,
      "learning_rate": 2.752059403643114e-05,
      "loss": 1.8734,
      "step": 697500
    },
    {
      "epoch": 26.979154580964536,
      "grad_norm": 13.096076011657715,
      "learning_rate": 2.7517371182529555e-05,
      "loss": 1.9051,
      "step": 697600
    },
    {
      "epoch": 26.98302200564644,
      "grad_norm": 15.382283210754395,
      "learning_rate": 2.751414832862797e-05,
      "loss": 1.8678,
      "step": 697700
    },
    {
      "epoch": 26.986889430328343,
      "grad_norm": 11.399906158447266,
      "learning_rate": 2.7510925474726378e-05,
      "loss": 2.0016,
      "step": 697800
    },
    {
      "epoch": 26.99075685501025,
      "grad_norm": 10.506149291992188,
      "learning_rate": 2.7507702620824792e-05,
      "loss": 1.9014,
      "step": 697900
    },
    {
      "epoch": 26.994624279692154,
      "grad_norm": 9.708687782287598,
      "learning_rate": 2.7504479766923207e-05,
      "loss": 1.9063,
      "step": 698000
    },
    {
      "epoch": 26.998491704374057,
      "grad_norm": 15.755060195922852,
      "learning_rate": 2.750125691302162e-05,
      "loss": 1.9578,
      "step": 698100
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.82716703414917,
      "eval_runtime": 3.0438,
      "eval_samples_per_second": 447.136,
      "eval_steps_per_second": 447.136,
      "step": 698139
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.710261583328247,
      "eval_runtime": 55.3092,
      "eval_samples_per_second": 467.499,
      "eval_steps_per_second": 467.499,
      "step": 698139
    },
    {
      "epoch": 27.00235912905596,
      "grad_norm": 11.174064636230469,
      "learning_rate": 2.7498034059120033e-05,
      "loss": 1.9075,
      "step": 698200
    },
    {
      "epoch": 27.006226553737864,
      "grad_norm": 11.728272438049316,
      "learning_rate": 2.7494811205218444e-05,
      "loss": 1.8862,
      "step": 698300
    },
    {
      "epoch": 27.01009397841977,
      "grad_norm": 11.490373611450195,
      "learning_rate": 2.749158835131686e-05,
      "loss": 1.8785,
      "step": 698400
    },
    {
      "epoch": 27.013961403101675,
      "grad_norm": 10.12479019165039,
      "learning_rate": 2.7488365497415274e-05,
      "loss": 1.889,
      "step": 698500
    },
    {
      "epoch": 27.01782882778358,
      "grad_norm": 13.626665115356445,
      "learning_rate": 2.7485142643513685e-05,
      "loss": 1.8728,
      "step": 698600
    },
    {
      "epoch": 27.021696252465482,
      "grad_norm": 11.094825744628906,
      "learning_rate": 2.74819197896121e-05,
      "loss": 1.8297,
      "step": 698700
    },
    {
      "epoch": 27.02556367714739,
      "grad_norm": 11.111618041992188,
      "learning_rate": 2.747869693571051e-05,
      "loss": 1.8371,
      "step": 698800
    },
    {
      "epoch": 27.029431101829292,
      "grad_norm": 10.544278144836426,
      "learning_rate": 2.7475474081808926e-05,
      "loss": 1.892,
      "step": 698900
    },
    {
      "epoch": 27.033298526511196,
      "grad_norm": 10.77847671508789,
      "learning_rate": 2.7472251227907337e-05,
      "loss": 1.8249,
      "step": 699000
    },
    {
      "epoch": 27.0371659511931,
      "grad_norm": 12.510627746582031,
      "learning_rate": 2.7469028374005752e-05,
      "loss": 2.0073,
      "step": 699100
    },
    {
      "epoch": 27.041033375875006,
      "grad_norm": 15.543595314025879,
      "learning_rate": 2.7465805520104167e-05,
      "loss": 1.9615,
      "step": 699200
    },
    {
      "epoch": 27.04490080055691,
      "grad_norm": 10.837850570678711,
      "learning_rate": 2.7462582666202575e-05,
      "loss": 1.9221,
      "step": 699300
    },
    {
      "epoch": 27.048768225238813,
      "grad_norm": 11.993606567382812,
      "learning_rate": 2.745935981230099e-05,
      "loss": 1.9062,
      "step": 699400
    },
    {
      "epoch": 27.052635649920717,
      "grad_norm": 11.465254783630371,
      "learning_rate": 2.7456136958399404e-05,
      "loss": 1.9124,
      "step": 699500
    },
    {
      "epoch": 27.056503074602624,
      "grad_norm": 9.602221488952637,
      "learning_rate": 2.745291410449782e-05,
      "loss": 1.8954,
      "step": 699600
    },
    {
      "epoch": 27.060370499284527,
      "grad_norm": 9.099469184875488,
      "learning_rate": 2.7449691250596227e-05,
      "loss": 1.9405,
      "step": 699700
    },
    {
      "epoch": 27.06423792396643,
      "grad_norm": 9.425131797790527,
      "learning_rate": 2.744646839669464e-05,
      "loss": 1.8596,
      "step": 699800
    },
    {
      "epoch": 27.068105348648334,
      "grad_norm": 11.004919052124023,
      "learning_rate": 2.7443245542793056e-05,
      "loss": 1.8672,
      "step": 699900
    },
    {
      "epoch": 27.071972773330238,
      "grad_norm": 9.131939888000488,
      "learning_rate": 2.744002268889147e-05,
      "loss": 1.9299,
      "step": 700000
    },
    {
      "epoch": 27.075840198012145,
      "grad_norm": 11.467215538024902,
      "learning_rate": 2.743679983498988e-05,
      "loss": 1.9628,
      "step": 700100
    },
    {
      "epoch": 27.07970762269405,
      "grad_norm": 11.73619556427002,
      "learning_rate": 2.7433576981088293e-05,
      "loss": 1.8572,
      "step": 700200
    },
    {
      "epoch": 27.083575047375952,
      "grad_norm": 12.030643463134766,
      "learning_rate": 2.7430354127186708e-05,
      "loss": 1.9453,
      "step": 700300
    },
    {
      "epoch": 27.087442472057855,
      "grad_norm": 14.771222114562988,
      "learning_rate": 2.7427131273285123e-05,
      "loss": 1.8677,
      "step": 700400
    },
    {
      "epoch": 27.091309896739762,
      "grad_norm": 9.091192245483398,
      "learning_rate": 2.742390841938353e-05,
      "loss": 1.8597,
      "step": 700500
    },
    {
      "epoch": 27.095177321421666,
      "grad_norm": 13.059319496154785,
      "learning_rate": 2.7420685565481946e-05,
      "loss": 1.9461,
      "step": 700600
    },
    {
      "epoch": 27.09904474610357,
      "grad_norm": 13.739763259887695,
      "learning_rate": 2.741746271158036e-05,
      "loss": 2.0035,
      "step": 700700
    },
    {
      "epoch": 27.102912170785473,
      "grad_norm": 10.889991760253906,
      "learning_rate": 2.7414239857678775e-05,
      "loss": 1.9612,
      "step": 700800
    },
    {
      "epoch": 27.10677959546738,
      "grad_norm": 8.305695533752441,
      "learning_rate": 2.7411017003777183e-05,
      "loss": 1.8544,
      "step": 700900
    },
    {
      "epoch": 27.110647020149283,
      "grad_norm": 11.764240264892578,
      "learning_rate": 2.7407794149875598e-05,
      "loss": 1.9912,
      "step": 701000
    },
    {
      "epoch": 27.114514444831187,
      "grad_norm": 10.160849571228027,
      "learning_rate": 2.7404571295974012e-05,
      "loss": 1.8982,
      "step": 701100
    },
    {
      "epoch": 27.11838186951309,
      "grad_norm": 9.060684204101562,
      "learning_rate": 2.7401348442072427e-05,
      "loss": 1.8811,
      "step": 701200
    },
    {
      "epoch": 27.122249294194997,
      "grad_norm": 9.780295372009277,
      "learning_rate": 2.7398125588170835e-05,
      "loss": 1.8949,
      "step": 701300
    },
    {
      "epoch": 27.1261167188769,
      "grad_norm": 17.89268684387207,
      "learning_rate": 2.739490273426925e-05,
      "loss": 1.8775,
      "step": 701400
    },
    {
      "epoch": 27.129984143558804,
      "grad_norm": 15.479350090026855,
      "learning_rate": 2.7391679880367665e-05,
      "loss": 1.9246,
      "step": 701500
    },
    {
      "epoch": 27.133851568240708,
      "grad_norm": 9.916108131408691,
      "learning_rate": 2.738845702646608e-05,
      "loss": 2.0154,
      "step": 701600
    },
    {
      "epoch": 27.13771899292261,
      "grad_norm": 15.13213062286377,
      "learning_rate": 2.738523417256449e-05,
      "loss": 1.8652,
      "step": 701700
    },
    {
      "epoch": 27.14158641760452,
      "grad_norm": 13.779732704162598,
      "learning_rate": 2.7382011318662902e-05,
      "loss": 1.873,
      "step": 701800
    },
    {
      "epoch": 27.14545384228642,
      "grad_norm": 8.673503875732422,
      "learning_rate": 2.7378788464761317e-05,
      "loss": 1.869,
      "step": 701900
    },
    {
      "epoch": 27.149321266968325,
      "grad_norm": 10.397204399108887,
      "learning_rate": 2.737556561085973e-05,
      "loss": 1.8519,
      "step": 702000
    },
    {
      "epoch": 27.15318869165023,
      "grad_norm": 13.046916961669922,
      "learning_rate": 2.7372342756958143e-05,
      "loss": 1.851,
      "step": 702100
    },
    {
      "epoch": 27.157056116332136,
      "grad_norm": 11.502816200256348,
      "learning_rate": 2.7369119903056557e-05,
      "loss": 1.9652,
      "step": 702200
    },
    {
      "epoch": 27.16092354101404,
      "grad_norm": 14.33730697631836,
      "learning_rate": 2.7365897049154972e-05,
      "loss": 1.8449,
      "step": 702300
    },
    {
      "epoch": 27.164790965695943,
      "grad_norm": 10.878085136413574,
      "learning_rate": 2.736267419525338e-05,
      "loss": 1.9715,
      "step": 702400
    },
    {
      "epoch": 27.168658390377846,
      "grad_norm": 11.707261085510254,
      "learning_rate": 2.7359451341351795e-05,
      "loss": 1.8629,
      "step": 702500
    },
    {
      "epoch": 27.172525815059753,
      "grad_norm": 12.816046714782715,
      "learning_rate": 2.735622848745021e-05,
      "loss": 1.9868,
      "step": 702600
    },
    {
      "epoch": 27.176393239741657,
      "grad_norm": 9.824009895324707,
      "learning_rate": 2.7353005633548624e-05,
      "loss": 1.8018,
      "step": 702700
    },
    {
      "epoch": 27.18026066442356,
      "grad_norm": 12.531991004943848,
      "learning_rate": 2.7349782779647032e-05,
      "loss": 1.8535,
      "step": 702800
    },
    {
      "epoch": 27.184128089105464,
      "grad_norm": 12.313395500183105,
      "learning_rate": 2.7346559925745447e-05,
      "loss": 1.9502,
      "step": 702900
    },
    {
      "epoch": 27.187995513787367,
      "grad_norm": 9.423483848571777,
      "learning_rate": 2.734333707184386e-05,
      "loss": 1.9512,
      "step": 703000
    },
    {
      "epoch": 27.191862938469274,
      "grad_norm": 9.177047729492188,
      "learning_rate": 2.7340114217942276e-05,
      "loss": 1.9783,
      "step": 703100
    },
    {
      "epoch": 27.195730363151178,
      "grad_norm": 14.569173812866211,
      "learning_rate": 2.7336891364040684e-05,
      "loss": 1.8692,
      "step": 703200
    },
    {
      "epoch": 27.19959778783308,
      "grad_norm": 11.056305885314941,
      "learning_rate": 2.73336685101391e-05,
      "loss": 1.8832,
      "step": 703300
    },
    {
      "epoch": 27.203465212514985,
      "grad_norm": 9.256452560424805,
      "learning_rate": 2.7330445656237514e-05,
      "loss": 1.9851,
      "step": 703400
    },
    {
      "epoch": 27.20733263719689,
      "grad_norm": 11.992176055908203,
      "learning_rate": 2.732722280233593e-05,
      "loss": 1.832,
      "step": 703500
    },
    {
      "epoch": 27.211200061878795,
      "grad_norm": 7.655876159667969,
      "learning_rate": 2.7323999948434336e-05,
      "loss": 2.0148,
      "step": 703600
    },
    {
      "epoch": 27.2150674865607,
      "grad_norm": 11.5708589553833,
      "learning_rate": 2.732077709453275e-05,
      "loss": 1.9146,
      "step": 703700
    },
    {
      "epoch": 27.218934911242602,
      "grad_norm": 10.63083553314209,
      "learning_rate": 2.7317554240631166e-05,
      "loss": 1.8708,
      "step": 703800
    },
    {
      "epoch": 27.22280233592451,
      "grad_norm": 10.201925277709961,
      "learning_rate": 2.731433138672958e-05,
      "loss": 1.8786,
      "step": 703900
    },
    {
      "epoch": 27.226669760606413,
      "grad_norm": 11.261048316955566,
      "learning_rate": 2.731110853282799e-05,
      "loss": 1.8648,
      "step": 704000
    },
    {
      "epoch": 27.230537185288316,
      "grad_norm": 10.830483436584473,
      "learning_rate": 2.7307885678926403e-05,
      "loss": 1.9099,
      "step": 704100
    },
    {
      "epoch": 27.23440460997022,
      "grad_norm": 11.483845710754395,
      "learning_rate": 2.7304662825024818e-05,
      "loss": 1.8782,
      "step": 704200
    },
    {
      "epoch": 27.238272034652127,
      "grad_norm": 13.201725006103516,
      "learning_rate": 2.7301439971123233e-05,
      "loss": 1.8329,
      "step": 704300
    },
    {
      "epoch": 27.24213945933403,
      "grad_norm": 12.014787673950195,
      "learning_rate": 2.729821711722164e-05,
      "loss": 1.9324,
      "step": 704400
    },
    {
      "epoch": 27.246006884015934,
      "grad_norm": 12.745774269104004,
      "learning_rate": 2.7294994263320055e-05,
      "loss": 1.8563,
      "step": 704500
    },
    {
      "epoch": 27.249874308697837,
      "grad_norm": 10.980487823486328,
      "learning_rate": 2.729177140941847e-05,
      "loss": 1.9003,
      "step": 704600
    },
    {
      "epoch": 27.25374173337974,
      "grad_norm": 12.651556968688965,
      "learning_rate": 2.7288548555516885e-05,
      "loss": 1.8669,
      "step": 704700
    },
    {
      "epoch": 27.257609158061648,
      "grad_norm": 13.070158958435059,
      "learning_rate": 2.7285325701615293e-05,
      "loss": 1.8299,
      "step": 704800
    },
    {
      "epoch": 27.26147658274355,
      "grad_norm": 11.827059745788574,
      "learning_rate": 2.7282102847713707e-05,
      "loss": 1.9558,
      "step": 704900
    },
    {
      "epoch": 27.265344007425455,
      "grad_norm": 9.845069885253906,
      "learning_rate": 2.7278879993812122e-05,
      "loss": 1.9088,
      "step": 705000
    },
    {
      "epoch": 27.269211432107358,
      "grad_norm": 15.554210662841797,
      "learning_rate": 2.7275657139910537e-05,
      "loss": 1.8699,
      "step": 705100
    },
    {
      "epoch": 27.273078856789265,
      "grad_norm": 12.602030754089355,
      "learning_rate": 2.7272434286008948e-05,
      "loss": 1.9095,
      "step": 705200
    },
    {
      "epoch": 27.27694628147117,
      "grad_norm": 10.488130569458008,
      "learning_rate": 2.726921143210736e-05,
      "loss": 1.8804,
      "step": 705300
    },
    {
      "epoch": 27.280813706153072,
      "grad_norm": 9.824565887451172,
      "learning_rate": 2.7265988578205774e-05,
      "loss": 1.859,
      "step": 705400
    },
    {
      "epoch": 27.284681130834976,
      "grad_norm": 12.086629867553711,
      "learning_rate": 2.7262765724304185e-05,
      "loss": 1.903,
      "step": 705500
    },
    {
      "epoch": 27.288548555516883,
      "grad_norm": 12.786304473876953,
      "learning_rate": 2.72595428704026e-05,
      "loss": 1.8854,
      "step": 705600
    },
    {
      "epoch": 27.292415980198786,
      "grad_norm": 11.654560089111328,
      "learning_rate": 2.7256320016501015e-05,
      "loss": 1.9338,
      "step": 705700
    },
    {
      "epoch": 27.29628340488069,
      "grad_norm": 11.119030952453613,
      "learning_rate": 2.725309716259943e-05,
      "loss": 1.8335,
      "step": 705800
    },
    {
      "epoch": 27.300150829562593,
      "grad_norm": 10.976640701293945,
      "learning_rate": 2.7249874308697838e-05,
      "loss": 1.8328,
      "step": 705900
    },
    {
      "epoch": 27.3040182542445,
      "grad_norm": 12.21138858795166,
      "learning_rate": 2.7246651454796252e-05,
      "loss": 1.9128,
      "step": 706000
    },
    {
      "epoch": 27.307885678926404,
      "grad_norm": 7.936827659606934,
      "learning_rate": 2.7243428600894667e-05,
      "loss": 1.8437,
      "step": 706100
    },
    {
      "epoch": 27.311753103608307,
      "grad_norm": 12.713046073913574,
      "learning_rate": 2.7240205746993082e-05,
      "loss": 1.9051,
      "step": 706200
    },
    {
      "epoch": 27.31562052829021,
      "grad_norm": 12.151057243347168,
      "learning_rate": 2.723698289309149e-05,
      "loss": 1.9076,
      "step": 706300
    },
    {
      "epoch": 27.319487952972114,
      "grad_norm": 15.396937370300293,
      "learning_rate": 2.7233760039189904e-05,
      "loss": 1.9244,
      "step": 706400
    },
    {
      "epoch": 27.32335537765402,
      "grad_norm": 13.05044937133789,
      "learning_rate": 2.723053718528832e-05,
      "loss": 1.8624,
      "step": 706500
    },
    {
      "epoch": 27.327222802335925,
      "grad_norm": 10.309314727783203,
      "learning_rate": 2.7227314331386734e-05,
      "loss": 1.9051,
      "step": 706600
    },
    {
      "epoch": 27.331090227017828,
      "grad_norm": 9.935120582580566,
      "learning_rate": 2.7224091477485142e-05,
      "loss": 1.9249,
      "step": 706700
    },
    {
      "epoch": 27.33495765169973,
      "grad_norm": 14.481149673461914,
      "learning_rate": 2.7220868623583556e-05,
      "loss": 1.9438,
      "step": 706800
    },
    {
      "epoch": 27.33882507638164,
      "grad_norm": 13.017180442810059,
      "learning_rate": 2.721764576968197e-05,
      "loss": 1.8976,
      "step": 706900
    },
    {
      "epoch": 27.342692501063542,
      "grad_norm": 9.208905220031738,
      "learning_rate": 2.7214422915780386e-05,
      "loss": 1.9299,
      "step": 707000
    },
    {
      "epoch": 27.346559925745446,
      "grad_norm": 10.599068641662598,
      "learning_rate": 2.7211200061878794e-05,
      "loss": 1.9272,
      "step": 707100
    },
    {
      "epoch": 27.35042735042735,
      "grad_norm": 13.487796783447266,
      "learning_rate": 2.720797720797721e-05,
      "loss": 1.8813,
      "step": 707200
    },
    {
      "epoch": 27.354294775109256,
      "grad_norm": 15.26370620727539,
      "learning_rate": 2.7204754354075623e-05,
      "loss": 2.0208,
      "step": 707300
    },
    {
      "epoch": 27.35816219979116,
      "grad_norm": 14.314475059509277,
      "learning_rate": 2.7201531500174038e-05,
      "loss": 1.877,
      "step": 707400
    },
    {
      "epoch": 27.362029624473063,
      "grad_norm": 12.315410614013672,
      "learning_rate": 2.7198308646272446e-05,
      "loss": 1.896,
      "step": 707500
    },
    {
      "epoch": 27.365897049154967,
      "grad_norm": 13.365654945373535,
      "learning_rate": 2.719508579237086e-05,
      "loss": 1.9494,
      "step": 707600
    },
    {
      "epoch": 27.369764473836874,
      "grad_norm": 11.048070907592773,
      "learning_rate": 2.7191862938469275e-05,
      "loss": 1.939,
      "step": 707700
    },
    {
      "epoch": 27.373631898518777,
      "grad_norm": 8.81114673614502,
      "learning_rate": 2.718864008456769e-05,
      "loss": 1.8315,
      "step": 707800
    },
    {
      "epoch": 27.37749932320068,
      "grad_norm": 10.885940551757812,
      "learning_rate": 2.7185417230666098e-05,
      "loss": 1.8531,
      "step": 707900
    },
    {
      "epoch": 27.381366747882584,
      "grad_norm": 12.368677139282227,
      "learning_rate": 2.7182194376764513e-05,
      "loss": 1.9694,
      "step": 708000
    },
    {
      "epoch": 27.385234172564488,
      "grad_norm": 4.648375034332275,
      "learning_rate": 2.7178971522862927e-05,
      "loss": 1.9526,
      "step": 708100
    },
    {
      "epoch": 27.389101597246395,
      "grad_norm": 11.357564926147461,
      "learning_rate": 2.717574866896134e-05,
      "loss": 1.8266,
      "step": 708200
    },
    {
      "epoch": 27.392969021928298,
      "grad_norm": 11.912269592285156,
      "learning_rate": 2.717252581505975e-05,
      "loss": 1.8843,
      "step": 708300
    },
    {
      "epoch": 27.3968364466102,
      "grad_norm": 12.471513748168945,
      "learning_rate": 2.7169302961158165e-05,
      "loss": 1.8479,
      "step": 708400
    },
    {
      "epoch": 27.400703871292105,
      "grad_norm": 12.2394380569458,
      "learning_rate": 2.716608010725658e-05,
      "loss": 2.0143,
      "step": 708500
    },
    {
      "epoch": 27.404571295974012,
      "grad_norm": 16.172861099243164,
      "learning_rate": 2.716285725335499e-05,
      "loss": 1.8415,
      "step": 708600
    },
    {
      "epoch": 27.408438720655916,
      "grad_norm": 10.529309272766113,
      "learning_rate": 2.7159634399453406e-05,
      "loss": 1.9426,
      "step": 708700
    },
    {
      "epoch": 27.41230614533782,
      "grad_norm": 10.78543472290039,
      "learning_rate": 2.7156411545551817e-05,
      "loss": 1.8746,
      "step": 708800
    },
    {
      "epoch": 27.416173570019723,
      "grad_norm": 11.192814826965332,
      "learning_rate": 2.715318869165023e-05,
      "loss": 1.9103,
      "step": 708900
    },
    {
      "epoch": 27.42004099470163,
      "grad_norm": 9.89348030090332,
      "learning_rate": 2.7149965837748643e-05,
      "loss": 1.7967,
      "step": 709000
    },
    {
      "epoch": 27.423908419383533,
      "grad_norm": 10.515097618103027,
      "learning_rate": 2.7146742983847058e-05,
      "loss": 1.8252,
      "step": 709100
    },
    {
      "epoch": 27.427775844065437,
      "grad_norm": 10.68566608428955,
      "learning_rate": 2.7143520129945472e-05,
      "loss": 1.8668,
      "step": 709200
    },
    {
      "epoch": 27.43164326874734,
      "grad_norm": 13.641412734985352,
      "learning_rate": 2.7140297276043887e-05,
      "loss": 1.921,
      "step": 709300
    },
    {
      "epoch": 27.435510693429247,
      "grad_norm": 11.792590141296387,
      "learning_rate": 2.7137074422142295e-05,
      "loss": 1.9374,
      "step": 709400
    },
    {
      "epoch": 27.43937811811115,
      "grad_norm": 10.744359970092773,
      "learning_rate": 2.713385156824071e-05,
      "loss": 1.913,
      "step": 709500
    },
    {
      "epoch": 27.443245542793054,
      "grad_norm": 12.909772872924805,
      "learning_rate": 2.7130628714339125e-05,
      "loss": 1.8727,
      "step": 709600
    },
    {
      "epoch": 27.447112967474958,
      "grad_norm": 13.64584732055664,
      "learning_rate": 2.712740586043754e-05,
      "loss": 1.8744,
      "step": 709700
    },
    {
      "epoch": 27.45098039215686,
      "grad_norm": 12.089228630065918,
      "learning_rate": 2.7124183006535947e-05,
      "loss": 1.8427,
      "step": 709800
    },
    {
      "epoch": 27.454847816838768,
      "grad_norm": 13.238092422485352,
      "learning_rate": 2.7120960152634362e-05,
      "loss": 1.909,
      "step": 709900
    },
    {
      "epoch": 27.45871524152067,
      "grad_norm": 7.669419288635254,
      "learning_rate": 2.7117737298732777e-05,
      "loss": 1.8466,
      "step": 710000
    },
    {
      "epoch": 27.462582666202575,
      "grad_norm": 10.7297945022583,
      "learning_rate": 2.711451444483119e-05,
      "loss": 1.9005,
      "step": 710100
    },
    {
      "epoch": 27.46645009088448,
      "grad_norm": 9.903322219848633,
      "learning_rate": 2.71112915909296e-05,
      "loss": 1.9385,
      "step": 710200
    },
    {
      "epoch": 27.470317515566386,
      "grad_norm": 9.900919914245605,
      "learning_rate": 2.7108068737028014e-05,
      "loss": 1.902,
      "step": 710300
    },
    {
      "epoch": 27.47418494024829,
      "grad_norm": 9.854846000671387,
      "learning_rate": 2.710484588312643e-05,
      "loss": 1.8189,
      "step": 710400
    },
    {
      "epoch": 27.478052364930193,
      "grad_norm": 10.958911895751953,
      "learning_rate": 2.7101623029224843e-05,
      "loss": 1.8393,
      "step": 710500
    },
    {
      "epoch": 27.481919789612096,
      "grad_norm": 12.025893211364746,
      "learning_rate": 2.709840017532325e-05,
      "loss": 1.9208,
      "step": 710600
    },
    {
      "epoch": 27.485787214294003,
      "grad_norm": 12.901520729064941,
      "learning_rate": 2.7095177321421666e-05,
      "loss": 1.9052,
      "step": 710700
    },
    {
      "epoch": 27.489654638975907,
      "grad_norm": 11.969494819641113,
      "learning_rate": 2.709195446752008e-05,
      "loss": 1.9672,
      "step": 710800
    },
    {
      "epoch": 27.49352206365781,
      "grad_norm": 11.243829727172852,
      "learning_rate": 2.7088731613618496e-05,
      "loss": 1.9026,
      "step": 710900
    },
    {
      "epoch": 27.497389488339714,
      "grad_norm": 8.217475891113281,
      "learning_rate": 2.7085508759716904e-05,
      "loss": 1.8123,
      "step": 711000
    },
    {
      "epoch": 27.50125691302162,
      "grad_norm": 10.88902473449707,
      "learning_rate": 2.7082285905815318e-05,
      "loss": 1.839,
      "step": 711100
    },
    {
      "epoch": 27.505124337703524,
      "grad_norm": 11.464116096496582,
      "learning_rate": 2.7079063051913733e-05,
      "loss": 1.8437,
      "step": 711200
    },
    {
      "epoch": 27.508991762385428,
      "grad_norm": 11.35968017578125,
      "learning_rate": 2.707584019801214e-05,
      "loss": 1.9328,
      "step": 711300
    },
    {
      "epoch": 27.51285918706733,
      "grad_norm": 12.21569538116455,
      "learning_rate": 2.7072617344110556e-05,
      "loss": 1.9253,
      "step": 711400
    },
    {
      "epoch": 27.516726611749235,
      "grad_norm": 12.868651390075684,
      "learning_rate": 2.706939449020897e-05,
      "loss": 1.914,
      "step": 711500
    },
    {
      "epoch": 27.52059403643114,
      "grad_norm": 9.377484321594238,
      "learning_rate": 2.7066171636307385e-05,
      "loss": 1.8206,
      "step": 711600
    },
    {
      "epoch": 27.524461461113045,
      "grad_norm": 8.438525199890137,
      "learning_rate": 2.7062948782405796e-05,
      "loss": 1.9142,
      "step": 711700
    },
    {
      "epoch": 27.52832888579495,
      "grad_norm": 9.142205238342285,
      "learning_rate": 2.7059725928504208e-05,
      "loss": 1.9403,
      "step": 711800
    },
    {
      "epoch": 27.532196310476852,
      "grad_norm": 13.39201831817627,
      "learning_rate": 2.7056503074602622e-05,
      "loss": 1.9524,
      "step": 711900
    },
    {
      "epoch": 27.53606373515876,
      "grad_norm": 10.533781051635742,
      "learning_rate": 2.7053280220701037e-05,
      "loss": 1.9389,
      "step": 712000
    },
    {
      "epoch": 27.539931159840663,
      "grad_norm": 15.311283111572266,
      "learning_rate": 2.705005736679945e-05,
      "loss": 1.87,
      "step": 712100
    },
    {
      "epoch": 27.543798584522566,
      "grad_norm": 12.180122375488281,
      "learning_rate": 2.7046834512897863e-05,
      "loss": 1.9755,
      "step": 712200
    },
    {
      "epoch": 27.54766600920447,
      "grad_norm": 12.343596458435059,
      "learning_rate": 2.7043611658996278e-05,
      "loss": 2.0218,
      "step": 712300
    },
    {
      "epoch": 27.551533433886377,
      "grad_norm": 13.09390640258789,
      "learning_rate": 2.704038880509469e-05,
      "loss": 1.7754,
      "step": 712400
    },
    {
      "epoch": 27.55540085856828,
      "grad_norm": 14.790614128112793,
      "learning_rate": 2.70371659511931e-05,
      "loss": 1.8808,
      "step": 712500
    },
    {
      "epoch": 27.559268283250184,
      "grad_norm": 11.122286796569824,
      "learning_rate": 2.7033943097291515e-05,
      "loss": 1.9884,
      "step": 712600
    },
    {
      "epoch": 27.563135707932087,
      "grad_norm": 11.442768096923828,
      "learning_rate": 2.703072024338993e-05,
      "loss": 1.8477,
      "step": 712700
    },
    {
      "epoch": 27.56700313261399,
      "grad_norm": 11.994924545288086,
      "learning_rate": 2.7027497389488345e-05,
      "loss": 1.881,
      "step": 712800
    },
    {
      "epoch": 27.570870557295898,
      "grad_norm": 11.964248657226562,
      "learning_rate": 2.7024274535586753e-05,
      "loss": 1.9562,
      "step": 712900
    },
    {
      "epoch": 27.5747379819778,
      "grad_norm": 12.008692741394043,
      "learning_rate": 2.7021051681685167e-05,
      "loss": 1.8198,
      "step": 713000
    },
    {
      "epoch": 27.578605406659705,
      "grad_norm": 12.59883975982666,
      "learning_rate": 2.7017828827783582e-05,
      "loss": 1.9215,
      "step": 713100
    },
    {
      "epoch": 27.582472831341608,
      "grad_norm": 12.737196922302246,
      "learning_rate": 2.7014605973881997e-05,
      "loss": 1.9344,
      "step": 713200
    },
    {
      "epoch": 27.586340256023515,
      "grad_norm": 13.0471773147583,
      "learning_rate": 2.7011383119980405e-05,
      "loss": 1.9408,
      "step": 713300
    },
    {
      "epoch": 27.59020768070542,
      "grad_norm": 8.299524307250977,
      "learning_rate": 2.700816026607882e-05,
      "loss": 1.9993,
      "step": 713400
    },
    {
      "epoch": 27.594075105387322,
      "grad_norm": 11.434243202209473,
      "learning_rate": 2.7004937412177234e-05,
      "loss": 1.8926,
      "step": 713500
    },
    {
      "epoch": 27.597942530069226,
      "grad_norm": 10.077898979187012,
      "learning_rate": 2.700171455827565e-05,
      "loss": 1.834,
      "step": 713600
    },
    {
      "epoch": 27.601809954751133,
      "grad_norm": 16.30132484436035,
      "learning_rate": 2.6998491704374057e-05,
      "loss": 1.856,
      "step": 713700
    },
    {
      "epoch": 27.605677379433036,
      "grad_norm": 11.055831909179688,
      "learning_rate": 2.699526885047247e-05,
      "loss": 1.8611,
      "step": 713800
    },
    {
      "epoch": 27.60954480411494,
      "grad_norm": 14.392481803894043,
      "learning_rate": 2.6992045996570886e-05,
      "loss": 1.9717,
      "step": 713900
    },
    {
      "epoch": 27.613412228796843,
      "grad_norm": 13.155285835266113,
      "learning_rate": 2.69888231426693e-05,
      "loss": 1.896,
      "step": 714000
    },
    {
      "epoch": 27.61727965347875,
      "grad_norm": 10.689051628112793,
      "learning_rate": 2.698560028876771e-05,
      "loss": 1.8298,
      "step": 714100
    },
    {
      "epoch": 27.621147078160654,
      "grad_norm": 9.937967300415039,
      "learning_rate": 2.6982377434866124e-05,
      "loss": 1.9,
      "step": 714200
    },
    {
      "epoch": 27.625014502842557,
      "grad_norm": 12.524499893188477,
      "learning_rate": 2.697915458096454e-05,
      "loss": 1.9165,
      "step": 714300
    },
    {
      "epoch": 27.62888192752446,
      "grad_norm": 10.77385139465332,
      "learning_rate": 2.6975931727062946e-05,
      "loss": 1.9377,
      "step": 714400
    },
    {
      "epoch": 27.632749352206364,
      "grad_norm": 13.847176551818848,
      "learning_rate": 2.697270887316136e-05,
      "loss": 1.8766,
      "step": 714500
    },
    {
      "epoch": 27.63661677688827,
      "grad_norm": 12.633294105529785,
      "learning_rate": 2.6969486019259776e-05,
      "loss": 1.9062,
      "step": 714600
    },
    {
      "epoch": 27.640484201570175,
      "grad_norm": 10.766459465026855,
      "learning_rate": 2.696626316535819e-05,
      "loss": 1.9248,
      "step": 714700
    },
    {
      "epoch": 27.644351626252078,
      "grad_norm": 10.974190711975098,
      "learning_rate": 2.69630403114566e-05,
      "loss": 1.8832,
      "step": 714800
    },
    {
      "epoch": 27.64821905093398,
      "grad_norm": 11.809045791625977,
      "learning_rate": 2.6959817457555013e-05,
      "loss": 1.9155,
      "step": 714900
    },
    {
      "epoch": 27.65208647561589,
      "grad_norm": 10.551279067993164,
      "learning_rate": 2.6956594603653428e-05,
      "loss": 1.9201,
      "step": 715000
    },
    {
      "epoch": 27.655953900297792,
      "grad_norm": 12.763294219970703,
      "learning_rate": 2.6953371749751843e-05,
      "loss": 1.8814,
      "step": 715100
    },
    {
      "epoch": 27.659821324979696,
      "grad_norm": 15.24910831451416,
      "learning_rate": 2.6950148895850254e-05,
      "loss": 1.934,
      "step": 715200
    },
    {
      "epoch": 27.6636887496616,
      "grad_norm": 12.099116325378418,
      "learning_rate": 2.6946926041948665e-05,
      "loss": 1.909,
      "step": 715300
    },
    {
      "epoch": 27.667556174343506,
      "grad_norm": 10.74392032623291,
      "learning_rate": 2.694370318804708e-05,
      "loss": 1.8949,
      "step": 715400
    },
    {
      "epoch": 27.67142359902541,
      "grad_norm": 12.03961181640625,
      "learning_rate": 2.6940480334145495e-05,
      "loss": 1.9641,
      "step": 715500
    },
    {
      "epoch": 27.675291023707313,
      "grad_norm": 11.0294828414917,
      "learning_rate": 2.6937257480243906e-05,
      "loss": 1.8721,
      "step": 715600
    },
    {
      "epoch": 27.679158448389217,
      "grad_norm": 11.513895988464355,
      "learning_rate": 2.693403462634232e-05,
      "loss": 1.8293,
      "step": 715700
    },
    {
      "epoch": 27.683025873071124,
      "grad_norm": 15.030210494995117,
      "learning_rate": 2.6930811772440735e-05,
      "loss": 1.8658,
      "step": 715800
    },
    {
      "epoch": 27.686893297753027,
      "grad_norm": 13.39730167388916,
      "learning_rate": 2.6927588918539147e-05,
      "loss": 1.8468,
      "step": 715900
    },
    {
      "epoch": 27.69076072243493,
      "grad_norm": 11.422859191894531,
      "learning_rate": 2.6924366064637558e-05,
      "loss": 1.9484,
      "step": 716000
    },
    {
      "epoch": 27.694628147116834,
      "grad_norm": 10.558079719543457,
      "learning_rate": 2.6921143210735973e-05,
      "loss": 1.8536,
      "step": 716100
    },
    {
      "epoch": 27.698495571798738,
      "grad_norm": 10.448528289794922,
      "learning_rate": 2.6917920356834388e-05,
      "loss": 1.9385,
      "step": 716200
    },
    {
      "epoch": 27.702362996480645,
      "grad_norm": 12.16829776763916,
      "learning_rate": 2.6914697502932802e-05,
      "loss": 1.8178,
      "step": 716300
    },
    {
      "epoch": 27.706230421162548,
      "grad_norm": 9.837135314941406,
      "learning_rate": 2.691147464903121e-05,
      "loss": 1.84,
      "step": 716400
    },
    {
      "epoch": 27.71009784584445,
      "grad_norm": 10.852429389953613,
      "learning_rate": 2.6908251795129625e-05,
      "loss": 1.8476,
      "step": 716500
    },
    {
      "epoch": 27.713965270526355,
      "grad_norm": 10.366986274719238,
      "learning_rate": 2.690502894122804e-05,
      "loss": 2.0784,
      "step": 716600
    },
    {
      "epoch": 27.717832695208262,
      "grad_norm": 12.645373344421387,
      "learning_rate": 2.6901806087326454e-05,
      "loss": 1.7727,
      "step": 716700
    },
    {
      "epoch": 27.721700119890166,
      "grad_norm": 10.887133598327637,
      "learning_rate": 2.6898583233424862e-05,
      "loss": 1.9067,
      "step": 716800
    },
    {
      "epoch": 27.72556754457207,
      "grad_norm": 12.502751350402832,
      "learning_rate": 2.6895360379523277e-05,
      "loss": 1.8762,
      "step": 716900
    },
    {
      "epoch": 27.729434969253973,
      "grad_norm": 9.78072738647461,
      "learning_rate": 2.6892137525621692e-05,
      "loss": 1.7711,
      "step": 717000
    },
    {
      "epoch": 27.73330239393588,
      "grad_norm": 12.322662353515625,
      "learning_rate": 2.68889146717201e-05,
      "loss": 1.9273,
      "step": 717100
    },
    {
      "epoch": 27.737169818617783,
      "grad_norm": 12.672708511352539,
      "learning_rate": 2.6885691817818514e-05,
      "loss": 1.9366,
      "step": 717200
    },
    {
      "epoch": 27.741037243299687,
      "grad_norm": 7.525558948516846,
      "learning_rate": 2.688246896391693e-05,
      "loss": 1.9432,
      "step": 717300
    },
    {
      "epoch": 27.74490466798159,
      "grad_norm": 12.072035789489746,
      "learning_rate": 2.6879246110015344e-05,
      "loss": 1.8823,
      "step": 717400
    },
    {
      "epoch": 27.748772092663497,
      "grad_norm": 14.155722618103027,
      "learning_rate": 2.6876023256113752e-05,
      "loss": 1.9009,
      "step": 717500
    },
    {
      "epoch": 27.7526395173454,
      "grad_norm": 14.639680862426758,
      "learning_rate": 2.6872800402212167e-05,
      "loss": 1.9201,
      "step": 717600
    },
    {
      "epoch": 27.756506942027304,
      "grad_norm": 10.451550483703613,
      "learning_rate": 2.686957754831058e-05,
      "loss": 1.8637,
      "step": 717700
    },
    {
      "epoch": 27.760374366709208,
      "grad_norm": 10.972878456115723,
      "learning_rate": 2.6866354694408996e-05,
      "loss": 1.8758,
      "step": 717800
    },
    {
      "epoch": 27.76424179139111,
      "grad_norm": 9.718591690063477,
      "learning_rate": 2.6863131840507404e-05,
      "loss": 1.8391,
      "step": 717900
    },
    {
      "epoch": 27.768109216073018,
      "grad_norm": 9.25059700012207,
      "learning_rate": 2.685990898660582e-05,
      "loss": 1.9128,
      "step": 718000
    },
    {
      "epoch": 27.77197664075492,
      "grad_norm": 10.792571067810059,
      "learning_rate": 2.6856686132704233e-05,
      "loss": 1.868,
      "step": 718100
    },
    {
      "epoch": 27.775844065436825,
      "grad_norm": 13.609807968139648,
      "learning_rate": 2.6853463278802648e-05,
      "loss": 1.9253,
      "step": 718200
    },
    {
      "epoch": 27.77971149011873,
      "grad_norm": 12.032144546508789,
      "learning_rate": 2.6850240424901056e-05,
      "loss": 1.9219,
      "step": 718300
    },
    {
      "epoch": 27.783578914800636,
      "grad_norm": 11.047609329223633,
      "learning_rate": 2.684701757099947e-05,
      "loss": 1.9159,
      "step": 718400
    },
    {
      "epoch": 27.78744633948254,
      "grad_norm": 10.657853126525879,
      "learning_rate": 2.6843794717097885e-05,
      "loss": 1.9546,
      "step": 718500
    },
    {
      "epoch": 27.791313764164443,
      "grad_norm": 11.46687126159668,
      "learning_rate": 2.68405718631963e-05,
      "loss": 1.9568,
      "step": 718600
    },
    {
      "epoch": 27.795181188846346,
      "grad_norm": 10.771796226501465,
      "learning_rate": 2.683734900929471e-05,
      "loss": 1.9328,
      "step": 718700
    },
    {
      "epoch": 27.799048613528253,
      "grad_norm": 8.147661209106445,
      "learning_rate": 2.6834126155393123e-05,
      "loss": 1.9837,
      "step": 718800
    },
    {
      "epoch": 27.802916038210157,
      "grad_norm": 11.055681228637695,
      "learning_rate": 2.6830903301491538e-05,
      "loss": 1.958,
      "step": 718900
    },
    {
      "epoch": 27.80678346289206,
      "grad_norm": 13.527156829833984,
      "learning_rate": 2.6827680447589952e-05,
      "loss": 1.9255,
      "step": 719000
    },
    {
      "epoch": 27.810650887573964,
      "grad_norm": 11.981157302856445,
      "learning_rate": 2.6824457593688364e-05,
      "loss": 1.8457,
      "step": 719100
    },
    {
      "epoch": 27.81451831225587,
      "grad_norm": 11.135747909545898,
      "learning_rate": 2.6821234739786778e-05,
      "loss": 1.9412,
      "step": 719200
    },
    {
      "epoch": 27.818385736937774,
      "grad_norm": 9.860536575317383,
      "learning_rate": 2.6818011885885193e-05,
      "loss": 1.9206,
      "step": 719300
    },
    {
      "epoch": 27.822253161619678,
      "grad_norm": 10.048833847045898,
      "learning_rate": 2.6814789031983604e-05,
      "loss": 1.8271,
      "step": 719400
    },
    {
      "epoch": 27.82612058630158,
      "grad_norm": 13.698262214660645,
      "learning_rate": 2.6811566178082016e-05,
      "loss": 1.9112,
      "step": 719500
    },
    {
      "epoch": 27.829988010983485,
      "grad_norm": 11.80825424194336,
      "learning_rate": 2.680834332418043e-05,
      "loss": 1.8604,
      "step": 719600
    },
    {
      "epoch": 27.83385543566539,
      "grad_norm": 11.693683624267578,
      "learning_rate": 2.6805120470278845e-05,
      "loss": 1.7776,
      "step": 719700
    },
    {
      "epoch": 27.837722860347295,
      "grad_norm": 13.702624320983887,
      "learning_rate": 2.680189761637726e-05,
      "loss": 1.8738,
      "step": 719800
    },
    {
      "epoch": 27.8415902850292,
      "grad_norm": 10.514307022094727,
      "learning_rate": 2.6798674762475668e-05,
      "loss": 1.9107,
      "step": 719900
    },
    {
      "epoch": 27.845457709711102,
      "grad_norm": 12.619561195373535,
      "learning_rate": 2.6795451908574082e-05,
      "loss": 1.9272,
      "step": 720000
    },
    {
      "epoch": 27.84932513439301,
      "grad_norm": 13.883452415466309,
      "learning_rate": 2.6792229054672497e-05,
      "loss": 1.8661,
      "step": 720100
    },
    {
      "epoch": 27.853192559074913,
      "grad_norm": 9.058218955993652,
      "learning_rate": 2.6789006200770905e-05,
      "loss": 1.9461,
      "step": 720200
    },
    {
      "epoch": 27.857059983756816,
      "grad_norm": 10.6585054397583,
      "learning_rate": 2.678578334686932e-05,
      "loss": 1.8014,
      "step": 720300
    },
    {
      "epoch": 27.86092740843872,
      "grad_norm": 11.824750900268555,
      "learning_rate": 2.6782560492967735e-05,
      "loss": 2.0008,
      "step": 720400
    },
    {
      "epoch": 27.864794833120627,
      "grad_norm": 13.472317695617676,
      "learning_rate": 2.677933763906615e-05,
      "loss": 1.9049,
      "step": 720500
    },
    {
      "epoch": 27.86866225780253,
      "grad_norm": 11.768457412719727,
      "learning_rate": 2.6776114785164557e-05,
      "loss": 1.843,
      "step": 720600
    },
    {
      "epoch": 27.872529682484434,
      "grad_norm": 14.150179862976074,
      "learning_rate": 2.6772891931262972e-05,
      "loss": 1.9136,
      "step": 720700
    },
    {
      "epoch": 27.876397107166337,
      "grad_norm": 11.919349670410156,
      "learning_rate": 2.6769669077361387e-05,
      "loss": 1.9507,
      "step": 720800
    },
    {
      "epoch": 27.88026453184824,
      "grad_norm": 12.345259666442871,
      "learning_rate": 2.67664462234598e-05,
      "loss": 1.9109,
      "step": 720900
    },
    {
      "epoch": 27.884131956530148,
      "grad_norm": 4.761096000671387,
      "learning_rate": 2.676322336955821e-05,
      "loss": 1.8933,
      "step": 721000
    },
    {
      "epoch": 27.88799938121205,
      "grad_norm": 9.603145599365234,
      "learning_rate": 2.6760000515656624e-05,
      "loss": 1.8036,
      "step": 721100
    },
    {
      "epoch": 27.891866805893955,
      "grad_norm": 13.065153121948242,
      "learning_rate": 2.675677766175504e-05,
      "loss": 1.9939,
      "step": 721200
    },
    {
      "epoch": 27.895734230575858,
      "grad_norm": 10.896829605102539,
      "learning_rate": 2.6753554807853453e-05,
      "loss": 1.8457,
      "step": 721300
    },
    {
      "epoch": 27.899601655257765,
      "grad_norm": 10.672990798950195,
      "learning_rate": 2.675033195395186e-05,
      "loss": 1.9333,
      "step": 721400
    },
    {
      "epoch": 27.90346907993967,
      "grad_norm": 10.993732452392578,
      "learning_rate": 2.6747109100050276e-05,
      "loss": 1.9161,
      "step": 721500
    },
    {
      "epoch": 27.907336504621572,
      "grad_norm": 12.1024169921875,
      "learning_rate": 2.674388624614869e-05,
      "loss": 1.9486,
      "step": 721600
    },
    {
      "epoch": 27.911203929303475,
      "grad_norm": 10.324338912963867,
      "learning_rate": 2.6740663392247106e-05,
      "loss": 1.8474,
      "step": 721700
    },
    {
      "epoch": 27.915071353985383,
      "grad_norm": 13.364219665527344,
      "learning_rate": 2.6737440538345514e-05,
      "loss": 1.861,
      "step": 721800
    },
    {
      "epoch": 27.918938778667286,
      "grad_norm": 10.118342399597168,
      "learning_rate": 2.6734217684443928e-05,
      "loss": 1.8531,
      "step": 721900
    },
    {
      "epoch": 27.92280620334919,
      "grad_norm": 11.351042747497559,
      "learning_rate": 2.6730994830542343e-05,
      "loss": 1.8831,
      "step": 722000
    },
    {
      "epoch": 27.926673628031093,
      "grad_norm": 12.720782279968262,
      "learning_rate": 2.6727771976640758e-05,
      "loss": 1.921,
      "step": 722100
    },
    {
      "epoch": 27.930541052713,
      "grad_norm": 10.990471839904785,
      "learning_rate": 2.672454912273917e-05,
      "loss": 1.829,
      "step": 722200
    },
    {
      "epoch": 27.934408477394904,
      "grad_norm": 12.672819137573242,
      "learning_rate": 2.6721326268837584e-05,
      "loss": 1.98,
      "step": 722300
    },
    {
      "epoch": 27.938275902076807,
      "grad_norm": 11.289618492126465,
      "learning_rate": 2.6718103414935995e-05,
      "loss": 1.9387,
      "step": 722400
    },
    {
      "epoch": 27.94214332675871,
      "grad_norm": 11.90450382232666,
      "learning_rate": 2.671488056103441e-05,
      "loss": 1.9226,
      "step": 722500
    },
    {
      "epoch": 27.946010751440618,
      "grad_norm": 7.484055519104004,
      "learning_rate": 2.671165770713282e-05,
      "loss": 1.8156,
      "step": 722600
    },
    {
      "epoch": 27.94987817612252,
      "grad_norm": 11.039642333984375,
      "learning_rate": 2.6708434853231236e-05,
      "loss": 1.9191,
      "step": 722700
    },
    {
      "epoch": 27.953745600804424,
      "grad_norm": 11.104000091552734,
      "learning_rate": 2.670521199932965e-05,
      "loss": 1.835,
      "step": 722800
    },
    {
      "epoch": 27.957613025486328,
      "grad_norm": 11.939865112304688,
      "learning_rate": 2.670198914542806e-05,
      "loss": 1.9431,
      "step": 722900
    },
    {
      "epoch": 27.96148045016823,
      "grad_norm": 14.183554649353027,
      "learning_rate": 2.6698766291526473e-05,
      "loss": 1.9985,
      "step": 723000
    },
    {
      "epoch": 27.96534787485014,
      "grad_norm": 10.929337501525879,
      "learning_rate": 2.6695543437624888e-05,
      "loss": 1.9522,
      "step": 723100
    },
    {
      "epoch": 27.969215299532042,
      "grad_norm": 9.564581871032715,
      "learning_rate": 2.6692320583723303e-05,
      "loss": 1.8705,
      "step": 723200
    },
    {
      "epoch": 27.973082724213945,
      "grad_norm": 10.136907577514648,
      "learning_rate": 2.668909772982171e-05,
      "loss": 1.9388,
      "step": 723300
    },
    {
      "epoch": 27.97695014889585,
      "grad_norm": 11.015439987182617,
      "learning_rate": 2.6685874875920125e-05,
      "loss": 1.9471,
      "step": 723400
    },
    {
      "epoch": 27.980817573577756,
      "grad_norm": 7.9082207679748535,
      "learning_rate": 2.668265202201854e-05,
      "loss": 1.872,
      "step": 723500
    },
    {
      "epoch": 27.98468499825966,
      "grad_norm": 9.458039283752441,
      "learning_rate": 2.6679429168116955e-05,
      "loss": 1.8715,
      "step": 723600
    },
    {
      "epoch": 27.988552422941563,
      "grad_norm": 10.407174110412598,
      "learning_rate": 2.6676206314215363e-05,
      "loss": 1.9833,
      "step": 723700
    },
    {
      "epoch": 27.992419847623466,
      "grad_norm": 12.480496406555176,
      "learning_rate": 2.6672983460313777e-05,
      "loss": 1.9427,
      "step": 723800
    },
    {
      "epoch": 27.996287272305374,
      "grad_norm": 15.130596160888672,
      "learning_rate": 2.6669760606412192e-05,
      "loss": 1.9343,
      "step": 723900
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.8209019899368286,
      "eval_runtime": 3.0119,
      "eval_samples_per_second": 451.871,
      "eval_steps_per_second": 451.871,
      "step": 723996
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.7000113725662231,
      "eval_runtime": 56.7416,
      "eval_samples_per_second": 455.698,
      "eval_steps_per_second": 455.698,
      "step": 723996
    },
    {
      "epoch": 28.000154696987277,
      "grad_norm": 11.69951057434082,
      "learning_rate": 2.6666537752510607e-05,
      "loss": 1.8629,
      "step": 724000
    },
    {
      "epoch": 28.00402212166918,
      "grad_norm": 9.569136619567871,
      "learning_rate": 2.6663314898609015e-05,
      "loss": 1.7954,
      "step": 724100
    },
    {
      "epoch": 28.007889546351084,
      "grad_norm": 10.095988273620605,
      "learning_rate": 2.666009204470743e-05,
      "loss": 1.8005,
      "step": 724200
    },
    {
      "epoch": 28.011756971032987,
      "grad_norm": 9.656373023986816,
      "learning_rate": 2.6656869190805844e-05,
      "loss": 1.8857,
      "step": 724300
    },
    {
      "epoch": 28.015624395714894,
      "grad_norm": 13.016688346862793,
      "learning_rate": 2.665364633690426e-05,
      "loss": 1.8519,
      "step": 724400
    },
    {
      "epoch": 28.019491820396798,
      "grad_norm": 10.509819984436035,
      "learning_rate": 2.6650423483002667e-05,
      "loss": 1.9046,
      "step": 724500
    },
    {
      "epoch": 28.0233592450787,
      "grad_norm": 10.097930908203125,
      "learning_rate": 2.664720062910108e-05,
      "loss": 1.8166,
      "step": 724600
    },
    {
      "epoch": 28.027226669760605,
      "grad_norm": 12.71667194366455,
      "learning_rate": 2.6643977775199496e-05,
      "loss": 1.8792,
      "step": 724700
    },
    {
      "epoch": 28.031094094442512,
      "grad_norm": 13.54661750793457,
      "learning_rate": 2.664075492129791e-05,
      "loss": 1.9342,
      "step": 724800
    },
    {
      "epoch": 28.034961519124415,
      "grad_norm": 9.85973072052002,
      "learning_rate": 2.663753206739632e-05,
      "loss": 1.96,
      "step": 724900
    },
    {
      "epoch": 28.03882894380632,
      "grad_norm": 10.219542503356934,
      "learning_rate": 2.6634309213494734e-05,
      "loss": 1.9005,
      "step": 725000
    },
    {
      "epoch": 28.042696368488222,
      "grad_norm": 12.466680526733398,
      "learning_rate": 2.663108635959315e-05,
      "loss": 2.0126,
      "step": 725100
    },
    {
      "epoch": 28.04656379317013,
      "grad_norm": 10.240289688110352,
      "learning_rate": 2.6627863505691563e-05,
      "loss": 1.8611,
      "step": 725200
    },
    {
      "epoch": 28.050431217852033,
      "grad_norm": 9.390679359436035,
      "learning_rate": 2.662464065178997e-05,
      "loss": 1.9621,
      "step": 725300
    },
    {
      "epoch": 28.054298642533936,
      "grad_norm": 10.345499038696289,
      "learning_rate": 2.6621417797888386e-05,
      "loss": 1.8771,
      "step": 725400
    },
    {
      "epoch": 28.05816606721584,
      "grad_norm": 10.737800598144531,
      "learning_rate": 2.66181949439868e-05,
      "loss": 1.8396,
      "step": 725500
    },
    {
      "epoch": 28.062033491897747,
      "grad_norm": 14.666619300842285,
      "learning_rate": 2.6614972090085215e-05,
      "loss": 1.8709,
      "step": 725600
    },
    {
      "epoch": 28.06590091657965,
      "grad_norm": 11.547652244567871,
      "learning_rate": 2.6611749236183627e-05,
      "loss": 1.8096,
      "step": 725700
    },
    {
      "epoch": 28.069768341261554,
      "grad_norm": 9.67858600616455,
      "learning_rate": 2.660852638228204e-05,
      "loss": 1.762,
      "step": 725800
    },
    {
      "epoch": 28.073635765943457,
      "grad_norm": 12.040745735168457,
      "learning_rate": 2.6605303528380453e-05,
      "loss": 1.8944,
      "step": 725900
    },
    {
      "epoch": 28.07750319062536,
      "grad_norm": 13.250872611999512,
      "learning_rate": 2.6602080674478864e-05,
      "loss": 1.8556,
      "step": 726000
    },
    {
      "epoch": 28.081370615307268,
      "grad_norm": 11.84195613861084,
      "learning_rate": 2.659885782057728e-05,
      "loss": 2.0265,
      "step": 726100
    },
    {
      "epoch": 28.08523803998917,
      "grad_norm": 10.669943809509277,
      "learning_rate": 2.6595634966675693e-05,
      "loss": 1.9041,
      "step": 726200
    },
    {
      "epoch": 28.089105464671075,
      "grad_norm": 10.271818161010742,
      "learning_rate": 2.6592412112774108e-05,
      "loss": 1.8515,
      "step": 726300
    },
    {
      "epoch": 28.09297288935298,
      "grad_norm": 15.99098014831543,
      "learning_rate": 2.6589189258872516e-05,
      "loss": 1.9135,
      "step": 726400
    },
    {
      "epoch": 28.096840314034885,
      "grad_norm": 10.332996368408203,
      "learning_rate": 2.658596640497093e-05,
      "loss": 1.8067,
      "step": 726500
    },
    {
      "epoch": 28.10070773871679,
      "grad_norm": 6.769928932189941,
      "learning_rate": 2.6582743551069345e-05,
      "loss": 1.9135,
      "step": 726600
    },
    {
      "epoch": 28.104575163398692,
      "grad_norm": 12.60755729675293,
      "learning_rate": 2.657952069716776e-05,
      "loss": 1.8842,
      "step": 726700
    },
    {
      "epoch": 28.108442588080596,
      "grad_norm": 9.878229141235352,
      "learning_rate": 2.6576297843266168e-05,
      "loss": 1.9335,
      "step": 726800
    },
    {
      "epoch": 28.112310012762503,
      "grad_norm": 12.458242416381836,
      "learning_rate": 2.6573074989364583e-05,
      "loss": 1.9056,
      "step": 726900
    },
    {
      "epoch": 28.116177437444406,
      "grad_norm": 12.384782791137695,
      "learning_rate": 2.6569852135462998e-05,
      "loss": 1.9175,
      "step": 727000
    },
    {
      "epoch": 28.12004486212631,
      "grad_norm": 12.246411323547363,
      "learning_rate": 2.6566629281561412e-05,
      "loss": 1.8208,
      "step": 727100
    },
    {
      "epoch": 28.123912286808213,
      "grad_norm": 8.220796585083008,
      "learning_rate": 2.656340642765982e-05,
      "loss": 1.8135,
      "step": 727200
    },
    {
      "epoch": 28.12777971149012,
      "grad_norm": 10.323867797851562,
      "learning_rate": 2.6560183573758235e-05,
      "loss": 1.9367,
      "step": 727300
    },
    {
      "epoch": 28.131647136172024,
      "grad_norm": 10.312368392944336,
      "learning_rate": 2.655696071985665e-05,
      "loss": 1.9454,
      "step": 727400
    },
    {
      "epoch": 28.135514560853927,
      "grad_norm": 13.892879486083984,
      "learning_rate": 2.6553737865955064e-05,
      "loss": 1.929,
      "step": 727500
    },
    {
      "epoch": 28.13938198553583,
      "grad_norm": 8.59738826751709,
      "learning_rate": 2.6550515012053472e-05,
      "loss": 1.8936,
      "step": 727600
    },
    {
      "epoch": 28.143249410217734,
      "grad_norm": 11.47845458984375,
      "learning_rate": 2.6547292158151887e-05,
      "loss": 1.9187,
      "step": 727700
    },
    {
      "epoch": 28.14711683489964,
      "grad_norm": 8.546863555908203,
      "learning_rate": 2.6544069304250302e-05,
      "loss": 1.8686,
      "step": 727800
    },
    {
      "epoch": 28.150984259581545,
      "grad_norm": 11.413311958312988,
      "learning_rate": 2.6540846450348716e-05,
      "loss": 1.9118,
      "step": 727900
    },
    {
      "epoch": 28.15485168426345,
      "grad_norm": 10.925437927246094,
      "learning_rate": 2.6537623596447124e-05,
      "loss": 1.8748,
      "step": 728000
    },
    {
      "epoch": 28.158719108945352,
      "grad_norm": 15.866225242614746,
      "learning_rate": 2.653440074254554e-05,
      "loss": 1.9378,
      "step": 728100
    },
    {
      "epoch": 28.16258653362726,
      "grad_norm": 11.629924774169922,
      "learning_rate": 2.6531177888643954e-05,
      "loss": 1.9063,
      "step": 728200
    },
    {
      "epoch": 28.166453958309162,
      "grad_norm": 10.994534492492676,
      "learning_rate": 2.652795503474237e-05,
      "loss": 1.9244,
      "step": 728300
    },
    {
      "epoch": 28.170321382991066,
      "grad_norm": 12.500568389892578,
      "learning_rate": 2.6524732180840777e-05,
      "loss": 1.9145,
      "step": 728400
    },
    {
      "epoch": 28.17418880767297,
      "grad_norm": 11.178553581237793,
      "learning_rate": 2.652150932693919e-05,
      "loss": 1.9227,
      "step": 728500
    },
    {
      "epoch": 28.178056232354876,
      "grad_norm": 9.993972778320312,
      "learning_rate": 2.6518286473037606e-05,
      "loss": 1.924,
      "step": 728600
    },
    {
      "epoch": 28.18192365703678,
      "grad_norm": 12.311429023742676,
      "learning_rate": 2.651506361913602e-05,
      "loss": 1.9022,
      "step": 728700
    },
    {
      "epoch": 28.185791081718683,
      "grad_norm": 12.65117359161377,
      "learning_rate": 2.651184076523443e-05,
      "loss": 1.8795,
      "step": 728800
    },
    {
      "epoch": 28.189658506400587,
      "grad_norm": 12.055984497070312,
      "learning_rate": 2.6508617911332843e-05,
      "loss": 1.8996,
      "step": 728900
    },
    {
      "epoch": 28.19352593108249,
      "grad_norm": 10.202653884887695,
      "learning_rate": 2.6505395057431258e-05,
      "loss": 1.8469,
      "step": 729000
    },
    {
      "epoch": 28.197393355764397,
      "grad_norm": 11.935049057006836,
      "learning_rate": 2.650217220352967e-05,
      "loss": 1.8632,
      "step": 729100
    },
    {
      "epoch": 28.2012607804463,
      "grad_norm": 11.778205871582031,
      "learning_rate": 2.6498949349628084e-05,
      "loss": 1.9596,
      "step": 729200
    },
    {
      "epoch": 28.205128205128204,
      "grad_norm": 12.523521423339844,
      "learning_rate": 2.64957264957265e-05,
      "loss": 1.7653,
      "step": 729300
    },
    {
      "epoch": 28.208995629810108,
      "grad_norm": 12.426876068115234,
      "learning_rate": 2.649250364182491e-05,
      "loss": 1.9516,
      "step": 729400
    },
    {
      "epoch": 28.212863054492015,
      "grad_norm": 10.086416244506836,
      "learning_rate": 2.648928078792332e-05,
      "loss": 1.9137,
      "step": 729500
    },
    {
      "epoch": 28.21673047917392,
      "grad_norm": 9.939221382141113,
      "learning_rate": 2.6486057934021736e-05,
      "loss": 1.9251,
      "step": 729600
    },
    {
      "epoch": 28.220597903855822,
      "grad_norm": 13.611919403076172,
      "learning_rate": 2.648283508012015e-05,
      "loss": 1.9635,
      "step": 729700
    },
    {
      "epoch": 28.224465328537725,
      "grad_norm": 13.40694808959961,
      "learning_rate": 2.6479612226218566e-05,
      "loss": 1.9353,
      "step": 729800
    },
    {
      "epoch": 28.228332753219632,
      "grad_norm": 10.495402336120605,
      "learning_rate": 2.6476389372316974e-05,
      "loss": 1.7971,
      "step": 729900
    },
    {
      "epoch": 28.232200177901536,
      "grad_norm": 10.772442817687988,
      "learning_rate": 2.6473166518415388e-05,
      "loss": 1.8709,
      "step": 730000
    },
    {
      "epoch": 28.23606760258344,
      "grad_norm": 10.839568138122559,
      "learning_rate": 2.6469943664513803e-05,
      "loss": 1.8822,
      "step": 730100
    },
    {
      "epoch": 28.239935027265343,
      "grad_norm": 11.434833526611328,
      "learning_rate": 2.6466720810612218e-05,
      "loss": 1.8467,
      "step": 730200
    },
    {
      "epoch": 28.24380245194725,
      "grad_norm": 10.363372802734375,
      "learning_rate": 2.6463497956710626e-05,
      "loss": 1.8391,
      "step": 730300
    },
    {
      "epoch": 28.247669876629153,
      "grad_norm": 11.92956256866455,
      "learning_rate": 2.646027510280904e-05,
      "loss": 1.9311,
      "step": 730400
    },
    {
      "epoch": 28.251537301311057,
      "grad_norm": 14.642618179321289,
      "learning_rate": 2.6457052248907455e-05,
      "loss": 1.8732,
      "step": 730500
    },
    {
      "epoch": 28.25540472599296,
      "grad_norm": 11.055922508239746,
      "learning_rate": 2.645382939500587e-05,
      "loss": 1.838,
      "step": 730600
    },
    {
      "epoch": 28.259272150674864,
      "grad_norm": 10.189532279968262,
      "learning_rate": 2.6450606541104278e-05,
      "loss": 1.9194,
      "step": 730700
    },
    {
      "epoch": 28.26313957535677,
      "grad_norm": 11.395431518554688,
      "learning_rate": 2.6447383687202692e-05,
      "loss": 1.855,
      "step": 730800
    },
    {
      "epoch": 28.267007000038674,
      "grad_norm": 14.880539894104004,
      "learning_rate": 2.6444160833301107e-05,
      "loss": 1.8125,
      "step": 730900
    },
    {
      "epoch": 28.270874424720578,
      "grad_norm": 11.829964637756348,
      "learning_rate": 2.6440937979399522e-05,
      "loss": 1.8254,
      "step": 731000
    },
    {
      "epoch": 28.27474184940248,
      "grad_norm": 11.893333435058594,
      "learning_rate": 2.643771512549793e-05,
      "loss": 1.958,
      "step": 731100
    },
    {
      "epoch": 28.27860927408439,
      "grad_norm": 10.879724502563477,
      "learning_rate": 2.6434492271596345e-05,
      "loss": 1.9318,
      "step": 731200
    },
    {
      "epoch": 28.282476698766292,
      "grad_norm": 12.283649444580078,
      "learning_rate": 2.643126941769476e-05,
      "loss": 1.8115,
      "step": 731300
    },
    {
      "epoch": 28.286344123448195,
      "grad_norm": 15.571151733398438,
      "learning_rate": 2.6428046563793174e-05,
      "loss": 1.8748,
      "step": 731400
    },
    {
      "epoch": 28.2902115481301,
      "grad_norm": 9.44009017944336,
      "learning_rate": 2.6424823709891582e-05,
      "loss": 1.8413,
      "step": 731500
    },
    {
      "epoch": 28.294078972812006,
      "grad_norm": 11.706223487854004,
      "learning_rate": 2.6421600855989997e-05,
      "loss": 1.9212,
      "step": 731600
    },
    {
      "epoch": 28.29794639749391,
      "grad_norm": 6.820990085601807,
      "learning_rate": 2.641837800208841e-05,
      "loss": 1.8155,
      "step": 731700
    },
    {
      "epoch": 28.301813822175813,
      "grad_norm": 12.90414047241211,
      "learning_rate": 2.641515514818682e-05,
      "loss": 1.9455,
      "step": 731800
    },
    {
      "epoch": 28.305681246857716,
      "grad_norm": 10.982348442077637,
      "learning_rate": 2.6411932294285234e-05,
      "loss": 1.8934,
      "step": 731900
    },
    {
      "epoch": 28.309548671539623,
      "grad_norm": 10.69682502746582,
      "learning_rate": 2.640870944038365e-05,
      "loss": 1.9401,
      "step": 732000
    },
    {
      "epoch": 28.313416096221527,
      "grad_norm": 12.611950874328613,
      "learning_rate": 2.6405486586482064e-05,
      "loss": 1.836,
      "step": 732100
    },
    {
      "epoch": 28.31728352090343,
      "grad_norm": 12.850048065185547,
      "learning_rate": 2.6402263732580475e-05,
      "loss": 1.8624,
      "step": 732200
    },
    {
      "epoch": 28.321150945585334,
      "grad_norm": 10.07723617553711,
      "learning_rate": 2.639904087867889e-05,
      "loss": 1.9151,
      "step": 732300
    },
    {
      "epoch": 28.325018370267237,
      "grad_norm": 11.605517387390137,
      "learning_rate": 2.63958180247773e-05,
      "loss": 1.8512,
      "step": 732400
    },
    {
      "epoch": 28.328885794949144,
      "grad_norm": 15.387962341308594,
      "learning_rate": 2.6392595170875716e-05,
      "loss": 1.7741,
      "step": 732500
    },
    {
      "epoch": 28.332753219631048,
      "grad_norm": 10.92740535736084,
      "learning_rate": 2.6389372316974127e-05,
      "loss": 1.819,
      "step": 732600
    },
    {
      "epoch": 28.33662064431295,
      "grad_norm": 10.466639518737793,
      "learning_rate": 2.638614946307254e-05,
      "loss": 1.895,
      "step": 732700
    },
    {
      "epoch": 28.340488068994855,
      "grad_norm": 13.074224472045898,
      "learning_rate": 2.6382926609170956e-05,
      "loss": 1.9112,
      "step": 732800
    },
    {
      "epoch": 28.344355493676762,
      "grad_norm": 9.711127281188965,
      "learning_rate": 2.6379703755269368e-05,
      "loss": 1.8712,
      "step": 732900
    },
    {
      "epoch": 28.348222918358665,
      "grad_norm": 13.167988777160645,
      "learning_rate": 2.637648090136778e-05,
      "loss": 1.9329,
      "step": 733000
    },
    {
      "epoch": 28.35209034304057,
      "grad_norm": 10.768244743347168,
      "learning_rate": 2.6373258047466194e-05,
      "loss": 2.0594,
      "step": 733100
    },
    {
      "epoch": 28.355957767722472,
      "grad_norm": 11.153780937194824,
      "learning_rate": 2.637003519356461e-05,
      "loss": 1.832,
      "step": 733200
    },
    {
      "epoch": 28.35982519240438,
      "grad_norm": 10.941670417785645,
      "learning_rate": 2.6366812339663023e-05,
      "loss": 1.9361,
      "step": 733300
    },
    {
      "epoch": 28.363692617086283,
      "grad_norm": 12.629938125610352,
      "learning_rate": 2.636358948576143e-05,
      "loss": 1.7802,
      "step": 733400
    },
    {
      "epoch": 28.367560041768186,
      "grad_norm": 12.762979507446289,
      "learning_rate": 2.6360366631859846e-05,
      "loss": 1.9664,
      "step": 733500
    },
    {
      "epoch": 28.37142746645009,
      "grad_norm": 12.115859985351562,
      "learning_rate": 2.635714377795826e-05,
      "loss": 1.8831,
      "step": 733600
    },
    {
      "epoch": 28.375294891131997,
      "grad_norm": 11.663097381591797,
      "learning_rate": 2.6353920924056675e-05,
      "loss": 1.8927,
      "step": 733700
    },
    {
      "epoch": 28.3791623158139,
      "grad_norm": 11.139838218688965,
      "learning_rate": 2.6350698070155083e-05,
      "loss": 1.8363,
      "step": 733800
    },
    {
      "epoch": 28.383029740495804,
      "grad_norm": 12.358491897583008,
      "learning_rate": 2.6347475216253498e-05,
      "loss": 1.8566,
      "step": 733900
    },
    {
      "epoch": 28.386897165177707,
      "grad_norm": 8.590426445007324,
      "learning_rate": 2.6344252362351913e-05,
      "loss": 1.8762,
      "step": 734000
    },
    {
      "epoch": 28.39076458985961,
      "grad_norm": 17.297016143798828,
      "learning_rate": 2.6341029508450327e-05,
      "loss": 1.9226,
      "step": 734100
    },
    {
      "epoch": 28.394632014541518,
      "grad_norm": 11.315415382385254,
      "learning_rate": 2.6337806654548735e-05,
      "loss": 1.8831,
      "step": 734200
    },
    {
      "epoch": 28.39849943922342,
      "grad_norm": 13.856926918029785,
      "learning_rate": 2.633458380064715e-05,
      "loss": 1.8893,
      "step": 734300
    },
    {
      "epoch": 28.402366863905325,
      "grad_norm": 13.46117115020752,
      "learning_rate": 2.6331360946745565e-05,
      "loss": 1.877,
      "step": 734400
    },
    {
      "epoch": 28.40623428858723,
      "grad_norm": 10.86879825592041,
      "learning_rate": 2.632813809284398e-05,
      "loss": 1.8533,
      "step": 734500
    },
    {
      "epoch": 28.410101713269135,
      "grad_norm": 10.675661087036133,
      "learning_rate": 2.6324915238942387e-05,
      "loss": 1.8836,
      "step": 734600
    },
    {
      "epoch": 28.41396913795104,
      "grad_norm": 11.579418182373047,
      "learning_rate": 2.6321692385040802e-05,
      "loss": 1.8952,
      "step": 734700
    },
    {
      "epoch": 28.417836562632942,
      "grad_norm": 12.640826225280762,
      "learning_rate": 2.6318469531139217e-05,
      "loss": 1.8637,
      "step": 734800
    },
    {
      "epoch": 28.421703987314846,
      "grad_norm": 11.591548919677734,
      "learning_rate": 2.6315246677237625e-05,
      "loss": 1.9056,
      "step": 734900
    },
    {
      "epoch": 28.425571411996753,
      "grad_norm": 11.81842041015625,
      "learning_rate": 2.631202382333604e-05,
      "loss": 1.824,
      "step": 735000
    },
    {
      "epoch": 28.429438836678656,
      "grad_norm": 13.850239753723145,
      "learning_rate": 2.6308800969434454e-05,
      "loss": 1.8791,
      "step": 735100
    },
    {
      "epoch": 28.43330626136056,
      "grad_norm": 10.900840759277344,
      "learning_rate": 2.630557811553287e-05,
      "loss": 1.8335,
      "step": 735200
    },
    {
      "epoch": 28.437173686042463,
      "grad_norm": 9.830056190490723,
      "learning_rate": 2.6302355261631277e-05,
      "loss": 1.9302,
      "step": 735300
    },
    {
      "epoch": 28.44104111072437,
      "grad_norm": 12.79870319366455,
      "learning_rate": 2.629913240772969e-05,
      "loss": 1.9126,
      "step": 735400
    },
    {
      "epoch": 28.444908535406274,
      "grad_norm": 12.669560432434082,
      "learning_rate": 2.6295909553828106e-05,
      "loss": 1.8729,
      "step": 735500
    },
    {
      "epoch": 28.448775960088177,
      "grad_norm": 13.86888313293457,
      "learning_rate": 2.629268669992652e-05,
      "loss": 1.8363,
      "step": 735600
    },
    {
      "epoch": 28.45264338477008,
      "grad_norm": 11.450601577758789,
      "learning_rate": 2.6289463846024932e-05,
      "loss": 1.855,
      "step": 735700
    },
    {
      "epoch": 28.456510809451984,
      "grad_norm": 11.158554077148438,
      "learning_rate": 2.6286240992123347e-05,
      "loss": 1.8491,
      "step": 735800
    },
    {
      "epoch": 28.46037823413389,
      "grad_norm": 9.736039161682129,
      "learning_rate": 2.628301813822176e-05,
      "loss": 1.8263,
      "step": 735900
    },
    {
      "epoch": 28.464245658815795,
      "grad_norm": 11.505692481994629,
      "learning_rate": 2.6279795284320173e-05,
      "loss": 1.9,
      "step": 736000
    },
    {
      "epoch": 28.4681130834977,
      "grad_norm": 13.342988967895508,
      "learning_rate": 2.6276572430418584e-05,
      "loss": 1.9318,
      "step": 736100
    },
    {
      "epoch": 28.471980508179602,
      "grad_norm": 13.273746490478516,
      "learning_rate": 2.6273349576517e-05,
      "loss": 1.908,
      "step": 736200
    },
    {
      "epoch": 28.47584793286151,
      "grad_norm": 11.144681930541992,
      "learning_rate": 2.6270126722615414e-05,
      "loss": 1.8028,
      "step": 736300
    },
    {
      "epoch": 28.479715357543412,
      "grad_norm": 12.86885929107666,
      "learning_rate": 2.6266903868713825e-05,
      "loss": 1.8514,
      "step": 736400
    },
    {
      "epoch": 28.483582782225316,
      "grad_norm": 13.283705711364746,
      "learning_rate": 2.6263681014812237e-05,
      "loss": 1.8649,
      "step": 736500
    },
    {
      "epoch": 28.48745020690722,
      "grad_norm": 13.690502166748047,
      "learning_rate": 2.626045816091065e-05,
      "loss": 1.8576,
      "step": 736600
    },
    {
      "epoch": 28.491317631589126,
      "grad_norm": 12.25740909576416,
      "learning_rate": 2.6257235307009066e-05,
      "loss": 1.972,
      "step": 736700
    },
    {
      "epoch": 28.49518505627103,
      "grad_norm": 10.817577362060547,
      "learning_rate": 2.625401245310748e-05,
      "loss": 1.934,
      "step": 736800
    },
    {
      "epoch": 28.499052480952933,
      "grad_norm": 18.59334373474121,
      "learning_rate": 2.625078959920589e-05,
      "loss": 1.8682,
      "step": 736900
    },
    {
      "epoch": 28.502919905634837,
      "grad_norm": 13.769783020019531,
      "learning_rate": 2.6247566745304303e-05,
      "loss": 1.9226,
      "step": 737000
    },
    {
      "epoch": 28.50678733031674,
      "grad_norm": 11.301526069641113,
      "learning_rate": 2.6244343891402718e-05,
      "loss": 1.7952,
      "step": 737100
    },
    {
      "epoch": 28.510654754998647,
      "grad_norm": 10.740323066711426,
      "learning_rate": 2.6241121037501133e-05,
      "loss": 1.9294,
      "step": 737200
    },
    {
      "epoch": 28.51452217968055,
      "grad_norm": 13.329328536987305,
      "learning_rate": 2.623789818359954e-05,
      "loss": 1.9297,
      "step": 737300
    },
    {
      "epoch": 28.518389604362454,
      "grad_norm": 11.616580963134766,
      "learning_rate": 2.6234675329697955e-05,
      "loss": 1.7653,
      "step": 737400
    },
    {
      "epoch": 28.522257029044358,
      "grad_norm": 9.126991271972656,
      "learning_rate": 2.623145247579637e-05,
      "loss": 1.9646,
      "step": 737500
    },
    {
      "epoch": 28.526124453726265,
      "grad_norm": 12.872824668884277,
      "learning_rate": 2.6228229621894785e-05,
      "loss": 1.8307,
      "step": 737600
    },
    {
      "epoch": 28.52999187840817,
      "grad_norm": 11.560776710510254,
      "learning_rate": 2.6225006767993193e-05,
      "loss": 1.9412,
      "step": 737700
    },
    {
      "epoch": 28.533859303090072,
      "grad_norm": 12.268928527832031,
      "learning_rate": 2.6221783914091608e-05,
      "loss": 1.8876,
      "step": 737800
    },
    {
      "epoch": 28.537726727771975,
      "grad_norm": 11.091814994812012,
      "learning_rate": 2.6218561060190022e-05,
      "loss": 1.9128,
      "step": 737900
    },
    {
      "epoch": 28.541594152453882,
      "grad_norm": 10.554778099060059,
      "learning_rate": 2.621533820628843e-05,
      "loss": 1.9634,
      "step": 738000
    },
    {
      "epoch": 28.545461577135786,
      "grad_norm": 12.134655952453613,
      "learning_rate": 2.6212115352386845e-05,
      "loss": 1.8955,
      "step": 738100
    },
    {
      "epoch": 28.54932900181769,
      "grad_norm": 8.632810592651367,
      "learning_rate": 2.620889249848526e-05,
      "loss": 1.859,
      "step": 738200
    },
    {
      "epoch": 28.553196426499593,
      "grad_norm": 10.464447975158691,
      "learning_rate": 2.6205669644583674e-05,
      "loss": 1.9142,
      "step": 738300
    },
    {
      "epoch": 28.5570638511815,
      "grad_norm": 12.808353424072266,
      "learning_rate": 2.6202446790682082e-05,
      "loss": 1.8912,
      "step": 738400
    },
    {
      "epoch": 28.560931275863403,
      "grad_norm": 11.320642471313477,
      "learning_rate": 2.6199223936780497e-05,
      "loss": 1.8357,
      "step": 738500
    },
    {
      "epoch": 28.564798700545307,
      "grad_norm": 12.469551086425781,
      "learning_rate": 2.6196001082878912e-05,
      "loss": 1.8722,
      "step": 738600
    },
    {
      "epoch": 28.56866612522721,
      "grad_norm": 9.390947341918945,
      "learning_rate": 2.6192778228977326e-05,
      "loss": 1.9981,
      "step": 738700
    },
    {
      "epoch": 28.572533549909117,
      "grad_norm": 13.99706745147705,
      "learning_rate": 2.6189555375075734e-05,
      "loss": 1.833,
      "step": 738800
    },
    {
      "epoch": 28.57640097459102,
      "grad_norm": 11.40902042388916,
      "learning_rate": 2.618633252117415e-05,
      "loss": 1.9326,
      "step": 738900
    },
    {
      "epoch": 28.580268399272924,
      "grad_norm": 9.341459274291992,
      "learning_rate": 2.6183109667272564e-05,
      "loss": 1.9186,
      "step": 739000
    },
    {
      "epoch": 28.584135823954828,
      "grad_norm": 17.315580368041992,
      "learning_rate": 2.617988681337098e-05,
      "loss": 1.8364,
      "step": 739100
    },
    {
      "epoch": 28.58800324863673,
      "grad_norm": 11.053584098815918,
      "learning_rate": 2.617666395946939e-05,
      "loss": 1.856,
      "step": 739200
    },
    {
      "epoch": 28.59187067331864,
      "grad_norm": 13.679031372070312,
      "learning_rate": 2.6173441105567805e-05,
      "loss": 1.9264,
      "step": 739300
    },
    {
      "epoch": 28.595738098000542,
      "grad_norm": 14.332308769226074,
      "learning_rate": 2.6170218251666216e-05,
      "loss": 1.9765,
      "step": 739400
    },
    {
      "epoch": 28.599605522682445,
      "grad_norm": 10.936898231506348,
      "learning_rate": 2.616699539776463e-05,
      "loss": 1.9015,
      "step": 739500
    },
    {
      "epoch": 28.60347294736435,
      "grad_norm": 10.30637264251709,
      "learning_rate": 2.6163772543863042e-05,
      "loss": 1.907,
      "step": 739600
    },
    {
      "epoch": 28.607340372046256,
      "grad_norm": 11.349364280700684,
      "learning_rate": 2.6160549689961457e-05,
      "loss": 1.9106,
      "step": 739700
    },
    {
      "epoch": 28.61120779672816,
      "grad_norm": 8.288308143615723,
      "learning_rate": 2.615732683605987e-05,
      "loss": 1.8649,
      "step": 739800
    },
    {
      "epoch": 28.615075221410063,
      "grad_norm": 11.190841674804688,
      "learning_rate": 2.6154103982158286e-05,
      "loss": 1.8688,
      "step": 739900
    },
    {
      "epoch": 28.618942646091966,
      "grad_norm": 12.002057075500488,
      "learning_rate": 2.6150881128256694e-05,
      "loss": 1.9968,
      "step": 740000
    },
    {
      "epoch": 28.622810070773873,
      "grad_norm": 10.293441772460938,
      "learning_rate": 2.614765827435511e-05,
      "loss": 1.993,
      "step": 740100
    },
    {
      "epoch": 28.626677495455777,
      "grad_norm": 10.205206871032715,
      "learning_rate": 2.6144435420453524e-05,
      "loss": 1.9858,
      "step": 740200
    },
    {
      "epoch": 28.63054492013768,
      "grad_norm": 11.473687171936035,
      "learning_rate": 2.6141212566551938e-05,
      "loss": 1.8236,
      "step": 740300
    },
    {
      "epoch": 28.634412344819584,
      "grad_norm": 13.148179054260254,
      "learning_rate": 2.6137989712650346e-05,
      "loss": 1.9169,
      "step": 740400
    },
    {
      "epoch": 28.638279769501487,
      "grad_norm": 11.436529159545898,
      "learning_rate": 2.613476685874876e-05,
      "loss": 1.9263,
      "step": 740500
    },
    {
      "epoch": 28.642147194183394,
      "grad_norm": 9.934606552124023,
      "learning_rate": 2.6131544004847176e-05,
      "loss": 1.8958,
      "step": 740600
    },
    {
      "epoch": 28.646014618865298,
      "grad_norm": 11.755931854248047,
      "learning_rate": 2.6128321150945584e-05,
      "loss": 1.8992,
      "step": 740700
    },
    {
      "epoch": 28.6498820435472,
      "grad_norm": 12.775193214416504,
      "learning_rate": 2.6125098297044e-05,
      "loss": 1.8488,
      "step": 740800
    },
    {
      "epoch": 28.653749468229105,
      "grad_norm": 14.86245346069336,
      "learning_rate": 2.6121875443142413e-05,
      "loss": 1.9681,
      "step": 740900
    },
    {
      "epoch": 28.65761689291101,
      "grad_norm": 12.72414779663086,
      "learning_rate": 2.6118652589240828e-05,
      "loss": 2.0026,
      "step": 741000
    },
    {
      "epoch": 28.661484317592915,
      "grad_norm": 14.418965339660645,
      "learning_rate": 2.6115429735339236e-05,
      "loss": 1.9592,
      "step": 741100
    },
    {
      "epoch": 28.66535174227482,
      "grad_norm": 10.638012886047363,
      "learning_rate": 2.611220688143765e-05,
      "loss": 1.949,
      "step": 741200
    },
    {
      "epoch": 28.669219166956722,
      "grad_norm": 14.190070152282715,
      "learning_rate": 2.6108984027536065e-05,
      "loss": 1.9956,
      "step": 741300
    },
    {
      "epoch": 28.67308659163863,
      "grad_norm": 10.6746244430542,
      "learning_rate": 2.610576117363448e-05,
      "loss": 1.8497,
      "step": 741400
    },
    {
      "epoch": 28.676954016320533,
      "grad_norm": 11.18382453918457,
      "learning_rate": 2.6102538319732888e-05,
      "loss": 1.9225,
      "step": 741500
    },
    {
      "epoch": 28.680821441002436,
      "grad_norm": 10.47393798828125,
      "learning_rate": 2.6099315465831303e-05,
      "loss": 1.7954,
      "step": 741600
    },
    {
      "epoch": 28.68468886568434,
      "grad_norm": 13.03612232208252,
      "learning_rate": 2.6096092611929717e-05,
      "loss": 1.8306,
      "step": 741700
    },
    {
      "epoch": 28.688556290366247,
      "grad_norm": 5.665115833282471,
      "learning_rate": 2.6092869758028132e-05,
      "loss": 1.9498,
      "step": 741800
    },
    {
      "epoch": 28.69242371504815,
      "grad_norm": 18.149124145507812,
      "learning_rate": 2.608964690412654e-05,
      "loss": 1.864,
      "step": 741900
    },
    {
      "epoch": 28.696291139730054,
      "grad_norm": 12.757662773132324,
      "learning_rate": 2.6086424050224955e-05,
      "loss": 1.8665,
      "step": 742000
    },
    {
      "epoch": 28.700158564411957,
      "grad_norm": 12.895756721496582,
      "learning_rate": 2.608320119632337e-05,
      "loss": 1.8703,
      "step": 742100
    },
    {
      "epoch": 28.70402598909386,
      "grad_norm": 11.520992279052734,
      "learning_rate": 2.6079978342421784e-05,
      "loss": 1.8236,
      "step": 742200
    },
    {
      "epoch": 28.707893413775768,
      "grad_norm": 12.181529998779297,
      "learning_rate": 2.6076755488520195e-05,
      "loss": 1.9619,
      "step": 742300
    },
    {
      "epoch": 28.71176083845767,
      "grad_norm": 13.770265579223633,
      "learning_rate": 2.6073532634618607e-05,
      "loss": 1.9122,
      "step": 742400
    },
    {
      "epoch": 28.715628263139575,
      "grad_norm": 11.735570907592773,
      "learning_rate": 2.607030978071702e-05,
      "loss": 1.9838,
      "step": 742500
    },
    {
      "epoch": 28.719495687821478,
      "grad_norm": 8.070213317871094,
      "learning_rate": 2.6067086926815436e-05,
      "loss": 1.895,
      "step": 742600
    },
    {
      "epoch": 28.723363112503385,
      "grad_norm": 14.81140422821045,
      "learning_rate": 2.6063864072913847e-05,
      "loss": 1.9573,
      "step": 742700
    },
    {
      "epoch": 28.72723053718529,
      "grad_norm": 15.575996398925781,
      "learning_rate": 2.6060641219012262e-05,
      "loss": 1.8055,
      "step": 742800
    },
    {
      "epoch": 28.731097961867192,
      "grad_norm": 6.518990516662598,
      "learning_rate": 2.6057418365110674e-05,
      "loss": 1.8245,
      "step": 742900
    },
    {
      "epoch": 28.734965386549096,
      "grad_norm": 10.314929008483887,
      "learning_rate": 2.6054195511209088e-05,
      "loss": 1.9507,
      "step": 743000
    },
    {
      "epoch": 28.738832811231003,
      "grad_norm": 11.90543270111084,
      "learning_rate": 2.60509726573075e-05,
      "loss": 1.9547,
      "step": 743100
    },
    {
      "epoch": 28.742700235912906,
      "grad_norm": 11.559356689453125,
      "learning_rate": 2.6047749803405914e-05,
      "loss": 1.8971,
      "step": 743200
    },
    {
      "epoch": 28.74656766059481,
      "grad_norm": 11.979074478149414,
      "learning_rate": 2.604452694950433e-05,
      "loss": 1.8827,
      "step": 743300
    },
    {
      "epoch": 28.750435085276713,
      "grad_norm": 10.497154235839844,
      "learning_rate": 2.6041304095602744e-05,
      "loss": 1.954,
      "step": 743400
    },
    {
      "epoch": 28.75430250995862,
      "grad_norm": 11.626370429992676,
      "learning_rate": 2.603808124170115e-05,
      "loss": 1.8921,
      "step": 743500
    },
    {
      "epoch": 28.758169934640524,
      "grad_norm": 10.131059646606445,
      "learning_rate": 2.6034858387799566e-05,
      "loss": 1.9217,
      "step": 743600
    },
    {
      "epoch": 28.762037359322427,
      "grad_norm": 8.621938705444336,
      "learning_rate": 2.603163553389798e-05,
      "loss": 1.8414,
      "step": 743700
    },
    {
      "epoch": 28.76590478400433,
      "grad_norm": 9.386818885803223,
      "learning_rate": 2.602841267999639e-05,
      "loss": 1.9692,
      "step": 743800
    },
    {
      "epoch": 28.769772208686234,
      "grad_norm": 11.513038635253906,
      "learning_rate": 2.6025189826094804e-05,
      "loss": 1.8881,
      "step": 743900
    },
    {
      "epoch": 28.77363963336814,
      "grad_norm": 11.252602577209473,
      "learning_rate": 2.602196697219322e-05,
      "loss": 1.9057,
      "step": 744000
    },
    {
      "epoch": 28.777507058050045,
      "grad_norm": 11.124344825744629,
      "learning_rate": 2.6018744118291633e-05,
      "loss": 1.8595,
      "step": 744100
    },
    {
      "epoch": 28.781374482731948,
      "grad_norm": 10.122108459472656,
      "learning_rate": 2.601552126439004e-05,
      "loss": 1.8312,
      "step": 744200
    },
    {
      "epoch": 28.78524190741385,
      "grad_norm": 11.260793685913086,
      "learning_rate": 2.6012298410488456e-05,
      "loss": 1.9548,
      "step": 744300
    },
    {
      "epoch": 28.78910933209576,
      "grad_norm": 8.319472312927246,
      "learning_rate": 2.600907555658687e-05,
      "loss": 1.9512,
      "step": 744400
    },
    {
      "epoch": 28.792976756777662,
      "grad_norm": 19.147228240966797,
      "learning_rate": 2.6005852702685285e-05,
      "loss": 1.8377,
      "step": 744500
    },
    {
      "epoch": 28.796844181459566,
      "grad_norm": 10.042545318603516,
      "learning_rate": 2.6002629848783693e-05,
      "loss": 1.8533,
      "step": 744600
    },
    {
      "epoch": 28.80071160614147,
      "grad_norm": 12.954581260681152,
      "learning_rate": 2.5999406994882108e-05,
      "loss": 1.9367,
      "step": 744700
    },
    {
      "epoch": 28.804579030823376,
      "grad_norm": 12.209701538085938,
      "learning_rate": 2.5996184140980523e-05,
      "loss": 1.9581,
      "step": 744800
    },
    {
      "epoch": 28.80844645550528,
      "grad_norm": 12.744948387145996,
      "learning_rate": 2.5992961287078937e-05,
      "loss": 1.8319,
      "step": 744900
    },
    {
      "epoch": 28.812313880187183,
      "grad_norm": 9.715024948120117,
      "learning_rate": 2.5989738433177345e-05,
      "loss": 1.9314,
      "step": 745000
    },
    {
      "epoch": 28.816181304869087,
      "grad_norm": 11.708760261535645,
      "learning_rate": 2.598651557927576e-05,
      "loss": 1.8168,
      "step": 745100
    },
    {
      "epoch": 28.82004872955099,
      "grad_norm": 13.061856269836426,
      "learning_rate": 2.5983292725374175e-05,
      "loss": 1.832,
      "step": 745200
    },
    {
      "epoch": 28.823916154232897,
      "grad_norm": 15.464064598083496,
      "learning_rate": 2.598006987147259e-05,
      "loss": 2.0717,
      "step": 745300
    },
    {
      "epoch": 28.8277835789148,
      "grad_norm": 13.212166786193848,
      "learning_rate": 2.5976847017570997e-05,
      "loss": 1.8142,
      "step": 745400
    },
    {
      "epoch": 28.831651003596704,
      "grad_norm": 11.989644050598145,
      "learning_rate": 2.5973624163669412e-05,
      "loss": 1.8395,
      "step": 745500
    },
    {
      "epoch": 28.835518428278608,
      "grad_norm": 12.5765380859375,
      "learning_rate": 2.5970401309767827e-05,
      "loss": 1.8381,
      "step": 745600
    },
    {
      "epoch": 28.839385852960515,
      "grad_norm": 13.418237686157227,
      "learning_rate": 2.596717845586624e-05,
      "loss": 1.8571,
      "step": 745700
    },
    {
      "epoch": 28.843253277642418,
      "grad_norm": 14.437641143798828,
      "learning_rate": 2.5963955601964653e-05,
      "loss": 1.9408,
      "step": 745800
    },
    {
      "epoch": 28.84712070232432,
      "grad_norm": 10.517013549804688,
      "learning_rate": 2.5960732748063064e-05,
      "loss": 1.8739,
      "step": 745900
    },
    {
      "epoch": 28.850988127006225,
      "grad_norm": 9.954996109008789,
      "learning_rate": 2.595750989416148e-05,
      "loss": 1.8443,
      "step": 746000
    },
    {
      "epoch": 28.854855551688132,
      "grad_norm": 12.411230087280273,
      "learning_rate": 2.5954287040259894e-05,
      "loss": 1.9277,
      "step": 746100
    },
    {
      "epoch": 28.858722976370036,
      "grad_norm": 10.112207412719727,
      "learning_rate": 2.5951064186358305e-05,
      "loss": 1.8771,
      "step": 746200
    },
    {
      "epoch": 28.86259040105194,
      "grad_norm": 10.252641677856445,
      "learning_rate": 2.594784133245672e-05,
      "loss": 2.0003,
      "step": 746300
    },
    {
      "epoch": 28.866457825733843,
      "grad_norm": 12.127426147460938,
      "learning_rate": 2.594461847855513e-05,
      "loss": 1.9136,
      "step": 746400
    },
    {
      "epoch": 28.87032525041575,
      "grad_norm": 11.80297565460205,
      "learning_rate": 2.5941395624653542e-05,
      "loss": 1.8897,
      "step": 746500
    },
    {
      "epoch": 28.874192675097653,
      "grad_norm": 9.883275985717773,
      "learning_rate": 2.5938172770751957e-05,
      "loss": 1.9748,
      "step": 746600
    },
    {
      "epoch": 28.878060099779557,
      "grad_norm": 11.827879905700684,
      "learning_rate": 2.5934949916850372e-05,
      "loss": 1.9024,
      "step": 746700
    },
    {
      "epoch": 28.88192752446146,
      "grad_norm": 10.503739356994629,
      "learning_rate": 2.5931727062948787e-05,
      "loss": 1.8765,
      "step": 746800
    },
    {
      "epoch": 28.885794949143367,
      "grad_norm": 11.151254653930664,
      "learning_rate": 2.5928504209047194e-05,
      "loss": 1.8528,
      "step": 746900
    },
    {
      "epoch": 28.88966237382527,
      "grad_norm": 9.476279258728027,
      "learning_rate": 2.592528135514561e-05,
      "loss": 1.9004,
      "step": 747000
    },
    {
      "epoch": 28.893529798507174,
      "grad_norm": 8.932559967041016,
      "learning_rate": 2.5922058501244024e-05,
      "loss": 1.8729,
      "step": 747100
    },
    {
      "epoch": 28.897397223189078,
      "grad_norm": 12.010683059692383,
      "learning_rate": 2.591883564734244e-05,
      "loss": 1.9195,
      "step": 747200
    },
    {
      "epoch": 28.90126464787098,
      "grad_norm": 11.340840339660645,
      "learning_rate": 2.5915612793440847e-05,
      "loss": 1.9459,
      "step": 747300
    },
    {
      "epoch": 28.905132072552888,
      "grad_norm": 11.338153839111328,
      "learning_rate": 2.591238993953926e-05,
      "loss": 1.8373,
      "step": 747400
    },
    {
      "epoch": 28.90899949723479,
      "grad_norm": 13.993260383605957,
      "learning_rate": 2.5909167085637676e-05,
      "loss": 1.9306,
      "step": 747500
    },
    {
      "epoch": 28.912866921916695,
      "grad_norm": 11.555785179138184,
      "learning_rate": 2.590594423173609e-05,
      "loss": 1.889,
      "step": 747600
    },
    {
      "epoch": 28.9167343465986,
      "grad_norm": 11.090299606323242,
      "learning_rate": 2.59027213778345e-05,
      "loss": 1.8269,
      "step": 747700
    },
    {
      "epoch": 28.920601771280506,
      "grad_norm": 14.014843940734863,
      "learning_rate": 2.5899498523932913e-05,
      "loss": 1.9303,
      "step": 747800
    },
    {
      "epoch": 28.92446919596241,
      "grad_norm": 13.648451805114746,
      "learning_rate": 2.5896275670031328e-05,
      "loss": 1.7746,
      "step": 747900
    },
    {
      "epoch": 28.928336620644313,
      "grad_norm": 11.744500160217285,
      "learning_rate": 2.5893052816129743e-05,
      "loss": 1.8241,
      "step": 748000
    },
    {
      "epoch": 28.932204045326216,
      "grad_norm": 13.087668418884277,
      "learning_rate": 2.588982996222815e-05,
      "loss": 1.9677,
      "step": 748100
    },
    {
      "epoch": 28.936071470008123,
      "grad_norm": 11.495674133300781,
      "learning_rate": 2.5886607108326566e-05,
      "loss": 1.9379,
      "step": 748200
    },
    {
      "epoch": 28.939938894690027,
      "grad_norm": 12.57611083984375,
      "learning_rate": 2.588338425442498e-05,
      "loss": 1.8646,
      "step": 748300
    },
    {
      "epoch": 28.94380631937193,
      "grad_norm": 11.38007640838623,
      "learning_rate": 2.5880161400523395e-05,
      "loss": 1.8128,
      "step": 748400
    },
    {
      "epoch": 28.947673744053834,
      "grad_norm": 12.420844078063965,
      "learning_rate": 2.5876938546621803e-05,
      "loss": 1.886,
      "step": 748500
    },
    {
      "epoch": 28.951541168735737,
      "grad_norm": 14.213432312011719,
      "learning_rate": 2.5873715692720218e-05,
      "loss": 1.9007,
      "step": 748600
    },
    {
      "epoch": 28.955408593417644,
      "grad_norm": 10.233853340148926,
      "learning_rate": 2.5870492838818632e-05,
      "loss": 1.976,
      "step": 748700
    },
    {
      "epoch": 28.959276018099548,
      "grad_norm": 15.054980278015137,
      "learning_rate": 2.5867269984917047e-05,
      "loss": 1.8933,
      "step": 748800
    },
    {
      "epoch": 28.96314344278145,
      "grad_norm": 14.776407241821289,
      "learning_rate": 2.5864047131015455e-05,
      "loss": 1.9243,
      "step": 748900
    },
    {
      "epoch": 28.967010867463355,
      "grad_norm": 9.806100845336914,
      "learning_rate": 2.586082427711387e-05,
      "loss": 1.8888,
      "step": 749000
    },
    {
      "epoch": 28.97087829214526,
      "grad_norm": 11.737406730651855,
      "learning_rate": 2.5857601423212284e-05,
      "loss": 1.9511,
      "step": 749100
    },
    {
      "epoch": 28.974745716827165,
      "grad_norm": 13.09990406036377,
      "learning_rate": 2.58543785693107e-05,
      "loss": 1.9521,
      "step": 749200
    },
    {
      "epoch": 28.97861314150907,
      "grad_norm": 10.292304039001465,
      "learning_rate": 2.585115571540911e-05,
      "loss": 1.99,
      "step": 749300
    },
    {
      "epoch": 28.982480566190972,
      "grad_norm": 14.333130836486816,
      "learning_rate": 2.5847932861507522e-05,
      "loss": 1.9181,
      "step": 749400
    },
    {
      "epoch": 28.98634799087288,
      "grad_norm": 9.371054649353027,
      "learning_rate": 2.5844710007605937e-05,
      "loss": 1.7933,
      "step": 749500
    },
    {
      "epoch": 28.990215415554783,
      "grad_norm": 13.66356372833252,
      "learning_rate": 2.5841487153704348e-05,
      "loss": 1.8334,
      "step": 749600
    },
    {
      "epoch": 28.994082840236686,
      "grad_norm": 11.024852752685547,
      "learning_rate": 2.5838264299802763e-05,
      "loss": 2.0207,
      "step": 749700
    },
    {
      "epoch": 28.99795026491859,
      "grad_norm": 9.490732192993164,
      "learning_rate": 2.5835041445901177e-05,
      "loss": 1.832,
      "step": 749800
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.8068339824676514,
      "eval_runtime": 2.9162,
      "eval_samples_per_second": 466.697,
      "eval_steps_per_second": 466.697,
      "step": 749853
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.6858294010162354,
      "eval_runtime": 56.7102,
      "eval_samples_per_second": 455.95,
      "eval_steps_per_second": 455.95,
      "step": 749853
    },
    {
      "epoch": 29.001817689600497,
      "grad_norm": 14.586992263793945,
      "learning_rate": 2.5831818591999592e-05,
      "loss": 1.8544,
      "step": 749900
    },
    {
      "epoch": 29.0056851142824,
      "grad_norm": 8.642528533935547,
      "learning_rate": 2.5828595738098e-05,
      "loss": 1.8793,
      "step": 750000
    },
    {
      "epoch": 29.009552538964304,
      "grad_norm": 13.758408546447754,
      "learning_rate": 2.5825372884196415e-05,
      "loss": 1.8864,
      "step": 750100
    },
    {
      "epoch": 29.013419963646207,
      "grad_norm": 10.569523811340332,
      "learning_rate": 2.582215003029483e-05,
      "loss": 1.8684,
      "step": 750200
    },
    {
      "epoch": 29.01728738832811,
      "grad_norm": 10.395156860351562,
      "learning_rate": 2.5818927176393244e-05,
      "loss": 1.9308,
      "step": 750300
    },
    {
      "epoch": 29.021154813010018,
      "grad_norm": 8.633947372436523,
      "learning_rate": 2.5815704322491652e-05,
      "loss": 1.9219,
      "step": 750400
    },
    {
      "epoch": 29.02502223769192,
      "grad_norm": 10.409995079040527,
      "learning_rate": 2.5812481468590067e-05,
      "loss": 1.8589,
      "step": 750500
    },
    {
      "epoch": 29.028889662373825,
      "grad_norm": 8.657644271850586,
      "learning_rate": 2.580925861468848e-05,
      "loss": 1.9023,
      "step": 750600
    },
    {
      "epoch": 29.032757087055728,
      "grad_norm": 11.847610473632812,
      "learning_rate": 2.5806035760786896e-05,
      "loss": 1.8992,
      "step": 750700
    },
    {
      "epoch": 29.036624511737635,
      "grad_norm": 9.919234275817871,
      "learning_rate": 2.5802812906885304e-05,
      "loss": 1.875,
      "step": 750800
    },
    {
      "epoch": 29.04049193641954,
      "grad_norm": 12.001312255859375,
      "learning_rate": 2.579959005298372e-05,
      "loss": 1.8708,
      "step": 750900
    },
    {
      "epoch": 29.044359361101442,
      "grad_norm": 11.760733604431152,
      "learning_rate": 2.5796367199082134e-05,
      "loss": 1.8255,
      "step": 751000
    },
    {
      "epoch": 29.048226785783346,
      "grad_norm": 10.08384895324707,
      "learning_rate": 2.5793144345180548e-05,
      "loss": 1.8019,
      "step": 751100
    },
    {
      "epoch": 29.052094210465253,
      "grad_norm": 8.913053512573242,
      "learning_rate": 2.5789921491278956e-05,
      "loss": 1.813,
      "step": 751200
    },
    {
      "epoch": 29.055961635147156,
      "grad_norm": 14.990326881408691,
      "learning_rate": 2.578669863737737e-05,
      "loss": 1.9395,
      "step": 751300
    },
    {
      "epoch": 29.05982905982906,
      "grad_norm": 12.65700912475586,
      "learning_rate": 2.5783475783475786e-05,
      "loss": 1.8901,
      "step": 751400
    },
    {
      "epoch": 29.063696484510963,
      "grad_norm": 4.806570529937744,
      "learning_rate": 2.57802529295742e-05,
      "loss": 1.8552,
      "step": 751500
    },
    {
      "epoch": 29.06756390919287,
      "grad_norm": 14.21717357635498,
      "learning_rate": 2.577703007567261e-05,
      "loss": 1.932,
      "step": 751600
    },
    {
      "epoch": 29.071431333874774,
      "grad_norm": 9.311128616333008,
      "learning_rate": 2.5773807221771023e-05,
      "loss": 1.8809,
      "step": 751700
    },
    {
      "epoch": 29.075298758556677,
      "grad_norm": 9.601179122924805,
      "learning_rate": 2.5770584367869438e-05,
      "loss": 1.8449,
      "step": 751800
    },
    {
      "epoch": 29.07916618323858,
      "grad_norm": 10.871355056762695,
      "learning_rate": 2.5767361513967852e-05,
      "loss": 1.9149,
      "step": 751900
    },
    {
      "epoch": 29.083033607920484,
      "grad_norm": 7.216371536254883,
      "learning_rate": 2.576413866006626e-05,
      "loss": 1.8555,
      "step": 752000
    },
    {
      "epoch": 29.08690103260239,
      "grad_norm": 10.443485260009766,
      "learning_rate": 2.5760915806164675e-05,
      "loss": 1.8041,
      "step": 752100
    },
    {
      "epoch": 29.090768457284295,
      "grad_norm": 10.612222671508789,
      "learning_rate": 2.575769295226309e-05,
      "loss": 1.928,
      "step": 752200
    },
    {
      "epoch": 29.094635881966198,
      "grad_norm": 10.437952041625977,
      "learning_rate": 2.5754470098361505e-05,
      "loss": 1.8585,
      "step": 752300
    },
    {
      "epoch": 29.0985033066481,
      "grad_norm": 13.404390335083008,
      "learning_rate": 2.5751247244459913e-05,
      "loss": 1.9028,
      "step": 752400
    },
    {
      "epoch": 29.10237073133001,
      "grad_norm": 10.894281387329102,
      "learning_rate": 2.5748024390558327e-05,
      "loss": 1.9399,
      "step": 752500
    },
    {
      "epoch": 29.106238156011912,
      "grad_norm": 9.412323951721191,
      "learning_rate": 2.5744801536656742e-05,
      "loss": 1.8637,
      "step": 752600
    },
    {
      "epoch": 29.110105580693816,
      "grad_norm": 10.894940376281738,
      "learning_rate": 2.5741578682755153e-05,
      "loss": 1.9255,
      "step": 752700
    },
    {
      "epoch": 29.11397300537572,
      "grad_norm": 13.932472229003906,
      "learning_rate": 2.5738355828853568e-05,
      "loss": 1.9418,
      "step": 752800
    },
    {
      "epoch": 29.117840430057626,
      "grad_norm": 9.410696029663086,
      "learning_rate": 2.573513297495198e-05,
      "loss": 1.9005,
      "step": 752900
    },
    {
      "epoch": 29.12170785473953,
      "grad_norm": 9.416522979736328,
      "learning_rate": 2.5731910121050394e-05,
      "loss": 1.9644,
      "step": 753000
    },
    {
      "epoch": 29.125575279421433,
      "grad_norm": 10.434297561645508,
      "learning_rate": 2.5728687267148805e-05,
      "loss": 1.8935,
      "step": 753100
    },
    {
      "epoch": 29.129442704103337,
      "grad_norm": 12.254034996032715,
      "learning_rate": 2.572546441324722e-05,
      "loss": 1.909,
      "step": 753200
    },
    {
      "epoch": 29.133310128785244,
      "grad_norm": 12.214967727661133,
      "learning_rate": 2.5722241559345635e-05,
      "loss": 1.775,
      "step": 753300
    },
    {
      "epoch": 29.137177553467147,
      "grad_norm": 12.478164672851562,
      "learning_rate": 2.571901870544405e-05,
      "loss": 1.9002,
      "step": 753400
    },
    {
      "epoch": 29.14104497814905,
      "grad_norm": 13.822369575500488,
      "learning_rate": 2.5715795851542457e-05,
      "loss": 1.9569,
      "step": 753500
    },
    {
      "epoch": 29.144912402830954,
      "grad_norm": 10.663846015930176,
      "learning_rate": 2.5712572997640872e-05,
      "loss": 1.8556,
      "step": 753600
    },
    {
      "epoch": 29.148779827512858,
      "grad_norm": 12.728405952453613,
      "learning_rate": 2.5709350143739287e-05,
      "loss": 1.8369,
      "step": 753700
    },
    {
      "epoch": 29.152647252194765,
      "grad_norm": 11.422375679016113,
      "learning_rate": 2.57061272898377e-05,
      "loss": 1.8006,
      "step": 753800
    },
    {
      "epoch": 29.156514676876668,
      "grad_norm": 10.182780265808105,
      "learning_rate": 2.570290443593611e-05,
      "loss": 1.8715,
      "step": 753900
    },
    {
      "epoch": 29.16038210155857,
      "grad_norm": 18.811437606811523,
      "learning_rate": 2.5699681582034524e-05,
      "loss": 1.9509,
      "step": 754000
    },
    {
      "epoch": 29.164249526240475,
      "grad_norm": 9.808706283569336,
      "learning_rate": 2.569645872813294e-05,
      "loss": 1.8892,
      "step": 754100
    },
    {
      "epoch": 29.168116950922382,
      "grad_norm": 9.322395324707031,
      "learning_rate": 2.5693235874231354e-05,
      "loss": 1.8605,
      "step": 754200
    },
    {
      "epoch": 29.171984375604286,
      "grad_norm": 9.872237205505371,
      "learning_rate": 2.569001302032976e-05,
      "loss": 1.8105,
      "step": 754300
    },
    {
      "epoch": 29.17585180028619,
      "grad_norm": 12.940070152282715,
      "learning_rate": 2.5686790166428176e-05,
      "loss": 1.9543,
      "step": 754400
    },
    {
      "epoch": 29.179719224968093,
      "grad_norm": 12.22903060913086,
      "learning_rate": 2.568356731252659e-05,
      "loss": 1.8714,
      "step": 754500
    },
    {
      "epoch": 29.18358664965,
      "grad_norm": 11.41913890838623,
      "learning_rate": 2.5680344458625006e-05,
      "loss": 1.8779,
      "step": 754600
    },
    {
      "epoch": 29.187454074331903,
      "grad_norm": 12.532670021057129,
      "learning_rate": 2.5677121604723414e-05,
      "loss": 1.9932,
      "step": 754700
    },
    {
      "epoch": 29.191321499013807,
      "grad_norm": 12.065356254577637,
      "learning_rate": 2.567389875082183e-05,
      "loss": 1.8093,
      "step": 754800
    },
    {
      "epoch": 29.19518892369571,
      "grad_norm": 10.972040176391602,
      "learning_rate": 2.5670675896920243e-05,
      "loss": 1.826,
      "step": 754900
    },
    {
      "epoch": 29.199056348377617,
      "grad_norm": 12.347732543945312,
      "learning_rate": 2.5667453043018658e-05,
      "loss": 1.9581,
      "step": 755000
    },
    {
      "epoch": 29.20292377305952,
      "grad_norm": 13.125761985778809,
      "learning_rate": 2.5664230189117066e-05,
      "loss": 1.8505,
      "step": 755100
    },
    {
      "epoch": 29.206791197741424,
      "grad_norm": 10.860662460327148,
      "learning_rate": 2.566100733521548e-05,
      "loss": 1.8395,
      "step": 755200
    },
    {
      "epoch": 29.210658622423328,
      "grad_norm": 10.462450981140137,
      "learning_rate": 2.5657784481313895e-05,
      "loss": 1.9004,
      "step": 755300
    },
    {
      "epoch": 29.21452604710523,
      "grad_norm": 10.97153091430664,
      "learning_rate": 2.5654561627412303e-05,
      "loss": 1.7853,
      "step": 755400
    },
    {
      "epoch": 29.218393471787138,
      "grad_norm": 11.119908332824707,
      "learning_rate": 2.5651338773510718e-05,
      "loss": 1.8234,
      "step": 755500
    },
    {
      "epoch": 29.22226089646904,
      "grad_norm": 8.830317497253418,
      "learning_rate": 2.5648115919609133e-05,
      "loss": 1.941,
      "step": 755600
    },
    {
      "epoch": 29.226128321150945,
      "grad_norm": 13.679909706115723,
      "learning_rate": 2.5644893065707547e-05,
      "loss": 1.9263,
      "step": 755700
    },
    {
      "epoch": 29.22999574583285,
      "grad_norm": 10.777178764343262,
      "learning_rate": 2.564167021180596e-05,
      "loss": 1.9054,
      "step": 755800
    },
    {
      "epoch": 29.233863170514756,
      "grad_norm": 9.471296310424805,
      "learning_rate": 2.563844735790437e-05,
      "loss": 1.9471,
      "step": 755900
    },
    {
      "epoch": 29.23773059519666,
      "grad_norm": 15.948549270629883,
      "learning_rate": 2.5635224504002785e-05,
      "loss": 1.8134,
      "step": 756000
    },
    {
      "epoch": 29.241598019878563,
      "grad_norm": 10.992796897888184,
      "learning_rate": 2.56320016501012e-05,
      "loss": 1.879,
      "step": 756100
    },
    {
      "epoch": 29.245465444560466,
      "grad_norm": 11.573530197143555,
      "learning_rate": 2.562877879619961e-05,
      "loss": 1.9164,
      "step": 756200
    },
    {
      "epoch": 29.249332869242373,
      "grad_norm": 11.779156684875488,
      "learning_rate": 2.5625555942298026e-05,
      "loss": 1.7788,
      "step": 756300
    },
    {
      "epoch": 29.253200293924277,
      "grad_norm": 15.353669166564941,
      "learning_rate": 2.5622333088396437e-05,
      "loss": 1.8828,
      "step": 756400
    },
    {
      "epoch": 29.25706771860618,
      "grad_norm": 14.189915657043457,
      "learning_rate": 2.561911023449485e-05,
      "loss": 1.9306,
      "step": 756500
    },
    {
      "epoch": 29.260935143288084,
      "grad_norm": 11.681595802307129,
      "learning_rate": 2.5615887380593263e-05,
      "loss": 1.8487,
      "step": 756600
    },
    {
      "epoch": 29.264802567969987,
      "grad_norm": 11.970434188842773,
      "learning_rate": 2.5612664526691678e-05,
      "loss": 1.8908,
      "step": 756700
    },
    {
      "epoch": 29.268669992651894,
      "grad_norm": 11.590588569641113,
      "learning_rate": 2.5609441672790092e-05,
      "loss": 1.9675,
      "step": 756800
    },
    {
      "epoch": 29.272537417333798,
      "grad_norm": 10.051609992980957,
      "learning_rate": 2.5606218818888507e-05,
      "loss": 2.0046,
      "step": 756900
    },
    {
      "epoch": 29.2764048420157,
      "grad_norm": 11.08832836151123,
      "learning_rate": 2.5602995964986915e-05,
      "loss": 1.8416,
      "step": 757000
    },
    {
      "epoch": 29.280272266697605,
      "grad_norm": 12.701193809509277,
      "learning_rate": 2.559977311108533e-05,
      "loss": 1.8464,
      "step": 757100
    },
    {
      "epoch": 29.28413969137951,
      "grad_norm": 13.066313743591309,
      "learning_rate": 2.5596550257183744e-05,
      "loss": 1.8245,
      "step": 757200
    },
    {
      "epoch": 29.288007116061415,
      "grad_norm": 11.243268013000488,
      "learning_rate": 2.559332740328216e-05,
      "loss": 1.8303,
      "step": 757300
    },
    {
      "epoch": 29.29187454074332,
      "grad_norm": 10.918642044067383,
      "learning_rate": 2.5590104549380567e-05,
      "loss": 1.8479,
      "step": 757400
    },
    {
      "epoch": 29.295741965425222,
      "grad_norm": 11.634218215942383,
      "learning_rate": 2.5586881695478982e-05,
      "loss": 1.8154,
      "step": 757500
    },
    {
      "epoch": 29.29960939010713,
      "grad_norm": 12.456767082214355,
      "learning_rate": 2.5583658841577397e-05,
      "loss": 1.8837,
      "step": 757600
    },
    {
      "epoch": 29.303476814789033,
      "grad_norm": 10.822872161865234,
      "learning_rate": 2.558043598767581e-05,
      "loss": 1.8425,
      "step": 757700
    },
    {
      "epoch": 29.307344239470936,
      "grad_norm": 8.898042678833008,
      "learning_rate": 2.557721313377422e-05,
      "loss": 1.9937,
      "step": 757800
    },
    {
      "epoch": 29.31121166415284,
      "grad_norm": 11.859484672546387,
      "learning_rate": 2.5573990279872634e-05,
      "loss": 1.8688,
      "step": 757900
    },
    {
      "epoch": 29.315079088834747,
      "grad_norm": 14.377366065979004,
      "learning_rate": 2.557076742597105e-05,
      "loss": 1.9181,
      "step": 758000
    },
    {
      "epoch": 29.31894651351665,
      "grad_norm": 12.640028953552246,
      "learning_rate": 2.5567544572069463e-05,
      "loss": 1.8821,
      "step": 758100
    },
    {
      "epoch": 29.322813938198554,
      "grad_norm": 11.47879409790039,
      "learning_rate": 2.556432171816787e-05,
      "loss": 1.8691,
      "step": 758200
    },
    {
      "epoch": 29.326681362880457,
      "grad_norm": 15.751864433288574,
      "learning_rate": 2.5561098864266286e-05,
      "loss": 1.8126,
      "step": 758300
    },
    {
      "epoch": 29.33054878756236,
      "grad_norm": 12.765546798706055,
      "learning_rate": 2.55578760103647e-05,
      "loss": 1.8101,
      "step": 758400
    },
    {
      "epoch": 29.334416212244268,
      "grad_norm": 10.411439895629883,
      "learning_rate": 2.555465315646311e-05,
      "loss": 1.8869,
      "step": 758500
    },
    {
      "epoch": 29.33828363692617,
      "grad_norm": 13.11306381225586,
      "learning_rate": 2.5551430302561523e-05,
      "loss": 1.8876,
      "step": 758600
    },
    {
      "epoch": 29.342151061608075,
      "grad_norm": 13.908376693725586,
      "learning_rate": 2.5548207448659938e-05,
      "loss": 1.9361,
      "step": 758700
    },
    {
      "epoch": 29.346018486289978,
      "grad_norm": 12.728503227233887,
      "learning_rate": 2.5544984594758353e-05,
      "loss": 1.8684,
      "step": 758800
    },
    {
      "epoch": 29.349885910971885,
      "grad_norm": 13.536267280578613,
      "learning_rate": 2.554176174085676e-05,
      "loss": 1.853,
      "step": 758900
    },
    {
      "epoch": 29.35375333565379,
      "grad_norm": 12.477862358093262,
      "learning_rate": 2.5538538886955176e-05,
      "loss": 1.8578,
      "step": 759000
    },
    {
      "epoch": 29.357620760335692,
      "grad_norm": 10.995770454406738,
      "learning_rate": 2.553531603305359e-05,
      "loss": 1.9335,
      "step": 759100
    },
    {
      "epoch": 29.361488185017595,
      "grad_norm": 14.58991813659668,
      "learning_rate": 2.5532093179152005e-05,
      "loss": 1.9197,
      "step": 759200
    },
    {
      "epoch": 29.365355609699503,
      "grad_norm": 14.685883522033691,
      "learning_rate": 2.5528870325250416e-05,
      "loss": 1.893,
      "step": 759300
    },
    {
      "epoch": 29.369223034381406,
      "grad_norm": 13.654142379760742,
      "learning_rate": 2.5525647471348828e-05,
      "loss": 1.9288,
      "step": 759400
    },
    {
      "epoch": 29.37309045906331,
      "grad_norm": 17.263917922973633,
      "learning_rate": 2.5522424617447242e-05,
      "loss": 1.9134,
      "step": 759500
    },
    {
      "epoch": 29.376957883745213,
      "grad_norm": 8.772479057312012,
      "learning_rate": 2.5519201763545657e-05,
      "loss": 1.8482,
      "step": 759600
    },
    {
      "epoch": 29.38082530842712,
      "grad_norm": 13.953225135803223,
      "learning_rate": 2.551597890964407e-05,
      "loss": 1.9642,
      "step": 759700
    },
    {
      "epoch": 29.384692733109024,
      "grad_norm": 10.5028715133667,
      "learning_rate": 2.5512756055742483e-05,
      "loss": 1.9167,
      "step": 759800
    },
    {
      "epoch": 29.388560157790927,
      "grad_norm": 11.217684745788574,
      "learning_rate": 2.5509533201840898e-05,
      "loss": 1.9076,
      "step": 759900
    },
    {
      "epoch": 29.39242758247283,
      "grad_norm": 11.966804504394531,
      "learning_rate": 2.550631034793931e-05,
      "loss": 1.9654,
      "step": 760000
    },
    {
      "epoch": 29.396295007154734,
      "grad_norm": 15.657394409179688,
      "learning_rate": 2.550308749403772e-05,
      "loss": 1.8282,
      "step": 760100
    },
    {
      "epoch": 29.40016243183664,
      "grad_norm": 9.289633750915527,
      "learning_rate": 2.5499864640136135e-05,
      "loss": 2.0411,
      "step": 760200
    },
    {
      "epoch": 29.404029856518545,
      "grad_norm": 11.028332710266113,
      "learning_rate": 2.549664178623455e-05,
      "loss": 1.8271,
      "step": 760300
    },
    {
      "epoch": 29.407897281200448,
      "grad_norm": 9.763522148132324,
      "learning_rate": 2.5493418932332965e-05,
      "loss": 1.8687,
      "step": 760400
    },
    {
      "epoch": 29.41176470588235,
      "grad_norm": 12.285381317138672,
      "learning_rate": 2.5490196078431373e-05,
      "loss": 1.8136,
      "step": 760500
    },
    {
      "epoch": 29.41563213056426,
      "grad_norm": 11.243399620056152,
      "learning_rate": 2.5486973224529787e-05,
      "loss": 1.8022,
      "step": 760600
    },
    {
      "epoch": 29.419499555246162,
      "grad_norm": 12.99445629119873,
      "learning_rate": 2.5483750370628202e-05,
      "loss": 1.9582,
      "step": 760700
    },
    {
      "epoch": 29.423366979928065,
      "grad_norm": 11.521088600158691,
      "learning_rate": 2.5480527516726617e-05,
      "loss": 1.8621,
      "step": 760800
    },
    {
      "epoch": 29.42723440460997,
      "grad_norm": 11.501054763793945,
      "learning_rate": 2.5477304662825025e-05,
      "loss": 1.887,
      "step": 760900
    },
    {
      "epoch": 29.431101829291876,
      "grad_norm": 9.55115795135498,
      "learning_rate": 2.547408180892344e-05,
      "loss": 1.9232,
      "step": 761000
    },
    {
      "epoch": 29.43496925397378,
      "grad_norm": 10.603423118591309,
      "learning_rate": 2.5470858955021854e-05,
      "loss": 1.8307,
      "step": 761100
    },
    {
      "epoch": 29.438836678655683,
      "grad_norm": 12.20641040802002,
      "learning_rate": 2.546763610112027e-05,
      "loss": 1.8219,
      "step": 761200
    },
    {
      "epoch": 29.442704103337586,
      "grad_norm": 10.808707237243652,
      "learning_rate": 2.5464413247218677e-05,
      "loss": 1.9887,
      "step": 761300
    },
    {
      "epoch": 29.446571528019494,
      "grad_norm": 10.42609977722168,
      "learning_rate": 2.546119039331709e-05,
      "loss": 1.8647,
      "step": 761400
    },
    {
      "epoch": 29.450438952701397,
      "grad_norm": 14.259543418884277,
      "learning_rate": 2.5457967539415506e-05,
      "loss": 1.8499,
      "step": 761500
    },
    {
      "epoch": 29.4543063773833,
      "grad_norm": 12.242183685302734,
      "learning_rate": 2.5454744685513914e-05,
      "loss": 1.9235,
      "step": 761600
    },
    {
      "epoch": 29.458173802065204,
      "grad_norm": 10.504764556884766,
      "learning_rate": 2.545152183161233e-05,
      "loss": 1.8515,
      "step": 761700
    },
    {
      "epoch": 29.462041226747107,
      "grad_norm": 10.251933097839355,
      "learning_rate": 2.5448298977710744e-05,
      "loss": 1.9134,
      "step": 761800
    },
    {
      "epoch": 29.465908651429014,
      "grad_norm": 12.947930335998535,
      "learning_rate": 2.5445076123809158e-05,
      "loss": 1.7969,
      "step": 761900
    },
    {
      "epoch": 29.469776076110918,
      "grad_norm": 9.957148551940918,
      "learning_rate": 2.5441853269907566e-05,
      "loss": 1.8259,
      "step": 762000
    },
    {
      "epoch": 29.47364350079282,
      "grad_norm": 12.691566467285156,
      "learning_rate": 2.543863041600598e-05,
      "loss": 1.8944,
      "step": 762100
    },
    {
      "epoch": 29.477510925474725,
      "grad_norm": 12.399839401245117,
      "learning_rate": 2.5435407562104396e-05,
      "loss": 2.0191,
      "step": 762200
    },
    {
      "epoch": 29.481378350156632,
      "grad_norm": 10.542276382446289,
      "learning_rate": 2.543218470820281e-05,
      "loss": 1.866,
      "step": 762300
    },
    {
      "epoch": 29.485245774838535,
      "grad_norm": 14.45352554321289,
      "learning_rate": 2.542896185430122e-05,
      "loss": 1.9229,
      "step": 762400
    },
    {
      "epoch": 29.48911319952044,
      "grad_norm": 13.144665718078613,
      "learning_rate": 2.5425739000399633e-05,
      "loss": 1.7952,
      "step": 762500
    },
    {
      "epoch": 29.492980624202342,
      "grad_norm": 13.107858657836914,
      "learning_rate": 2.5422516146498048e-05,
      "loss": 1.9293,
      "step": 762600
    },
    {
      "epoch": 29.49684804888425,
      "grad_norm": 10.633896827697754,
      "learning_rate": 2.5419293292596462e-05,
      "loss": 1.916,
      "step": 762700
    },
    {
      "epoch": 29.500715473566153,
      "grad_norm": 10.38435173034668,
      "learning_rate": 2.5416070438694874e-05,
      "loss": 1.852,
      "step": 762800
    },
    {
      "epoch": 29.504582898248056,
      "grad_norm": 11.973222732543945,
      "learning_rate": 2.5412847584793285e-05,
      "loss": 1.8662,
      "step": 762900
    },
    {
      "epoch": 29.50845032292996,
      "grad_norm": 14.905022621154785,
      "learning_rate": 2.54096247308917e-05,
      "loss": 1.9101,
      "step": 763000
    },
    {
      "epoch": 29.512317747611867,
      "grad_norm": 8.59544849395752,
      "learning_rate": 2.5406401876990115e-05,
      "loss": 1.8396,
      "step": 763100
    },
    {
      "epoch": 29.51618517229377,
      "grad_norm": 12.36740493774414,
      "learning_rate": 2.5403179023088526e-05,
      "loss": 1.9324,
      "step": 763200
    },
    {
      "epoch": 29.520052596975674,
      "grad_norm": 9.081822395324707,
      "learning_rate": 2.539995616918694e-05,
      "loss": 1.8528,
      "step": 763300
    },
    {
      "epoch": 29.523920021657577,
      "grad_norm": 10.142788887023926,
      "learning_rate": 2.5396733315285355e-05,
      "loss": 1.796,
      "step": 763400
    },
    {
      "epoch": 29.52778744633948,
      "grad_norm": 9.361889839172363,
      "learning_rate": 2.5393510461383767e-05,
      "loss": 1.7951,
      "step": 763500
    },
    {
      "epoch": 29.531654871021388,
      "grad_norm": 9.31634521484375,
      "learning_rate": 2.5390287607482178e-05,
      "loss": 1.963,
      "step": 763600
    },
    {
      "epoch": 29.53552229570329,
      "grad_norm": 11.261201858520508,
      "learning_rate": 2.5387064753580593e-05,
      "loss": 1.8709,
      "step": 763700
    },
    {
      "epoch": 29.539389720385195,
      "grad_norm": 14.169459342956543,
      "learning_rate": 2.5383841899679007e-05,
      "loss": 1.9077,
      "step": 763800
    },
    {
      "epoch": 29.5432571450671,
      "grad_norm": 16.147823333740234,
      "learning_rate": 2.5380619045777422e-05,
      "loss": 1.9041,
      "step": 763900
    },
    {
      "epoch": 29.547124569749005,
      "grad_norm": 11.440690994262695,
      "learning_rate": 2.537739619187583e-05,
      "loss": 1.8713,
      "step": 764000
    },
    {
      "epoch": 29.55099199443091,
      "grad_norm": 14.96259593963623,
      "learning_rate": 2.5374173337974245e-05,
      "loss": 2.0001,
      "step": 764100
    },
    {
      "epoch": 29.554859419112812,
      "grad_norm": 11.123659133911133,
      "learning_rate": 2.537095048407266e-05,
      "loss": 1.8991,
      "step": 764200
    },
    {
      "epoch": 29.558726843794716,
      "grad_norm": 11.958332061767578,
      "learning_rate": 2.5367727630171067e-05,
      "loss": 1.931,
      "step": 764300
    },
    {
      "epoch": 29.562594268476623,
      "grad_norm": 15.157025337219238,
      "learning_rate": 2.5364504776269482e-05,
      "loss": 1.8104,
      "step": 764400
    },
    {
      "epoch": 29.566461693158526,
      "grad_norm": 10.152265548706055,
      "learning_rate": 2.5361281922367897e-05,
      "loss": 1.9649,
      "step": 764500
    },
    {
      "epoch": 29.57032911784043,
      "grad_norm": 12.040321350097656,
      "learning_rate": 2.535805906846631e-05,
      "loss": 1.8898,
      "step": 764600
    },
    {
      "epoch": 29.574196542522333,
      "grad_norm": 12.759154319763184,
      "learning_rate": 2.535483621456472e-05,
      "loss": 1.931,
      "step": 764700
    },
    {
      "epoch": 29.578063967204237,
      "grad_norm": 13.021563529968262,
      "learning_rate": 2.5351613360663134e-05,
      "loss": 1.9513,
      "step": 764800
    },
    {
      "epoch": 29.581931391886144,
      "grad_norm": 13.617892265319824,
      "learning_rate": 2.534839050676155e-05,
      "loss": 1.8764,
      "step": 764900
    },
    {
      "epoch": 29.585798816568047,
      "grad_norm": 12.636897087097168,
      "learning_rate": 2.5345167652859964e-05,
      "loss": 1.8355,
      "step": 765000
    },
    {
      "epoch": 29.58966624124995,
      "grad_norm": 12.027542114257812,
      "learning_rate": 2.534194479895837e-05,
      "loss": 1.9194,
      "step": 765100
    },
    {
      "epoch": 29.593533665931854,
      "grad_norm": 13.432260513305664,
      "learning_rate": 2.5338721945056786e-05,
      "loss": 1.9167,
      "step": 765200
    },
    {
      "epoch": 29.59740109061376,
      "grad_norm": 11.905500411987305,
      "learning_rate": 2.53354990911552e-05,
      "loss": 1.9246,
      "step": 765300
    },
    {
      "epoch": 29.601268515295665,
      "grad_norm": 10.615588188171387,
      "learning_rate": 2.5332276237253616e-05,
      "loss": 1.8796,
      "step": 765400
    },
    {
      "epoch": 29.60513593997757,
      "grad_norm": 11.957860946655273,
      "learning_rate": 2.5329053383352024e-05,
      "loss": 1.832,
      "step": 765500
    },
    {
      "epoch": 29.609003364659472,
      "grad_norm": 12.345617294311523,
      "learning_rate": 2.532583052945044e-05,
      "loss": 1.8698,
      "step": 765600
    },
    {
      "epoch": 29.61287078934138,
      "grad_norm": 13.806001663208008,
      "learning_rate": 2.5322607675548853e-05,
      "loss": 1.8397,
      "step": 765700
    },
    {
      "epoch": 29.616738214023282,
      "grad_norm": 11.216012001037598,
      "learning_rate": 2.5319384821647268e-05,
      "loss": 1.8658,
      "step": 765800
    },
    {
      "epoch": 29.620605638705186,
      "grad_norm": 10.993099212646484,
      "learning_rate": 2.5316161967745676e-05,
      "loss": 1.8933,
      "step": 765900
    },
    {
      "epoch": 29.62447306338709,
      "grad_norm": 11.403066635131836,
      "learning_rate": 2.531293911384409e-05,
      "loss": 1.9351,
      "step": 766000
    },
    {
      "epoch": 29.628340488068996,
      "grad_norm": 13.627959251403809,
      "learning_rate": 2.5309716259942505e-05,
      "loss": 1.8736,
      "step": 766100
    },
    {
      "epoch": 29.6322079127509,
      "grad_norm": 11.610281944274902,
      "learning_rate": 2.530649340604092e-05,
      "loss": 1.8456,
      "step": 766200
    },
    {
      "epoch": 29.636075337432803,
      "grad_norm": 13.821365356445312,
      "learning_rate": 2.530327055213933e-05,
      "loss": 1.8984,
      "step": 766300
    },
    {
      "epoch": 29.639942762114707,
      "grad_norm": 8.343092918395996,
      "learning_rate": 2.5300047698237743e-05,
      "loss": 1.8896,
      "step": 766400
    },
    {
      "epoch": 29.643810186796614,
      "grad_norm": 13.0255708694458,
      "learning_rate": 2.5296824844336157e-05,
      "loss": 1.8828,
      "step": 766500
    },
    {
      "epoch": 29.647677611478517,
      "grad_norm": 11.803980827331543,
      "learning_rate": 2.5293601990434572e-05,
      "loss": 1.8945,
      "step": 766600
    },
    {
      "epoch": 29.65154503616042,
      "grad_norm": 9.707525253295898,
      "learning_rate": 2.5290379136532983e-05,
      "loss": 1.8645,
      "step": 766700
    },
    {
      "epoch": 29.655412460842324,
      "grad_norm": 12.54444694519043,
      "learning_rate": 2.5287156282631398e-05,
      "loss": 1.8436,
      "step": 766800
    },
    {
      "epoch": 29.659279885524228,
      "grad_norm": 13.686820030212402,
      "learning_rate": 2.5283933428729813e-05,
      "loss": 1.8483,
      "step": 766900
    },
    {
      "epoch": 29.663147310206135,
      "grad_norm": 13.62022590637207,
      "learning_rate": 2.5280710574828224e-05,
      "loss": 1.8618,
      "step": 767000
    },
    {
      "epoch": 29.66701473488804,
      "grad_norm": 9.057915687561035,
      "learning_rate": 2.5277487720926636e-05,
      "loss": 1.9347,
      "step": 767100
    },
    {
      "epoch": 29.670882159569942,
      "grad_norm": 12.730696678161621,
      "learning_rate": 2.527426486702505e-05,
      "loss": 1.8494,
      "step": 767200
    },
    {
      "epoch": 29.674749584251845,
      "grad_norm": 16.359342575073242,
      "learning_rate": 2.5271042013123465e-05,
      "loss": 1.8463,
      "step": 767300
    },
    {
      "epoch": 29.678617008933752,
      "grad_norm": 12.01279067993164,
      "learning_rate": 2.5267819159221873e-05,
      "loss": 1.8649,
      "step": 767400
    },
    {
      "epoch": 29.682484433615656,
      "grad_norm": 13.076650619506836,
      "learning_rate": 2.5264596305320288e-05,
      "loss": 1.8829,
      "step": 767500
    },
    {
      "epoch": 29.68635185829756,
      "grad_norm": 13.24712085723877,
      "learning_rate": 2.5261373451418702e-05,
      "loss": 1.9728,
      "step": 767600
    },
    {
      "epoch": 29.690219282979463,
      "grad_norm": 11.185800552368164,
      "learning_rate": 2.5258150597517117e-05,
      "loss": 1.9358,
      "step": 767700
    },
    {
      "epoch": 29.69408670766137,
      "grad_norm": 12.749594688415527,
      "learning_rate": 2.5254927743615525e-05,
      "loss": 1.9566,
      "step": 767800
    },
    {
      "epoch": 29.697954132343273,
      "grad_norm": 9.614127159118652,
      "learning_rate": 2.525170488971394e-05,
      "loss": 1.9711,
      "step": 767900
    },
    {
      "epoch": 29.701821557025177,
      "grad_norm": 10.532505989074707,
      "learning_rate": 2.5248482035812354e-05,
      "loss": 1.8382,
      "step": 768000
    },
    {
      "epoch": 29.70568898170708,
      "grad_norm": 9.977458000183105,
      "learning_rate": 2.524525918191077e-05,
      "loss": 1.863,
      "step": 768100
    },
    {
      "epoch": 29.709556406388984,
      "grad_norm": 11.6301851272583,
      "learning_rate": 2.5242036328009177e-05,
      "loss": 1.9187,
      "step": 768200
    },
    {
      "epoch": 29.71342383107089,
      "grad_norm": 11.454169273376465,
      "learning_rate": 2.5238813474107592e-05,
      "loss": 1.9521,
      "step": 768300
    },
    {
      "epoch": 29.717291255752794,
      "grad_norm": 10.2667236328125,
      "learning_rate": 2.5235590620206007e-05,
      "loss": 1.8538,
      "step": 768400
    },
    {
      "epoch": 29.721158680434698,
      "grad_norm": 13.010014533996582,
      "learning_rate": 2.523236776630442e-05,
      "loss": 1.8446,
      "step": 768500
    },
    {
      "epoch": 29.7250261051166,
      "grad_norm": 11.825494766235352,
      "learning_rate": 2.522914491240283e-05,
      "loss": 1.8962,
      "step": 768600
    },
    {
      "epoch": 29.72889352979851,
      "grad_norm": 12.073256492614746,
      "learning_rate": 2.5225922058501244e-05,
      "loss": 1.9977,
      "step": 768700
    },
    {
      "epoch": 29.732760954480412,
      "grad_norm": 10.31978988647461,
      "learning_rate": 2.522269920459966e-05,
      "loss": 1.8195,
      "step": 768800
    },
    {
      "epoch": 29.736628379162315,
      "grad_norm": 14.251989364624023,
      "learning_rate": 2.5219476350698073e-05,
      "loss": 1.8719,
      "step": 768900
    },
    {
      "epoch": 29.74049580384422,
      "grad_norm": 13.907022476196289,
      "learning_rate": 2.521625349679648e-05,
      "loss": 1.9122,
      "step": 769000
    },
    {
      "epoch": 29.744363228526126,
      "grad_norm": 11.689019203186035,
      "learning_rate": 2.5213030642894896e-05,
      "loss": 1.9695,
      "step": 769100
    },
    {
      "epoch": 29.74823065320803,
      "grad_norm": 10.827662467956543,
      "learning_rate": 2.520980778899331e-05,
      "loss": 1.8713,
      "step": 769200
    },
    {
      "epoch": 29.752098077889933,
      "grad_norm": 10.73038101196289,
      "learning_rate": 2.5206584935091725e-05,
      "loss": 1.9879,
      "step": 769300
    },
    {
      "epoch": 29.755965502571836,
      "grad_norm": 9.143531799316406,
      "learning_rate": 2.5203362081190133e-05,
      "loss": 1.9112,
      "step": 769400
    },
    {
      "epoch": 29.759832927253743,
      "grad_norm": 9.946249961853027,
      "learning_rate": 2.5200139227288548e-05,
      "loss": 1.8422,
      "step": 769500
    },
    {
      "epoch": 29.763700351935647,
      "grad_norm": 11.90068244934082,
      "learning_rate": 2.5196916373386963e-05,
      "loss": 1.9287,
      "step": 769600
    },
    {
      "epoch": 29.76756777661755,
      "grad_norm": 14.341897964477539,
      "learning_rate": 2.5193693519485378e-05,
      "loss": 1.948,
      "step": 769700
    },
    {
      "epoch": 29.771435201299454,
      "grad_norm": 11.450947761535645,
      "learning_rate": 2.519047066558379e-05,
      "loss": 1.8297,
      "step": 769800
    },
    {
      "epoch": 29.775302625981357,
      "grad_norm": 8.249503135681152,
      "learning_rate": 2.51872478116822e-05,
      "loss": 1.8392,
      "step": 769900
    },
    {
      "epoch": 29.779170050663264,
      "grad_norm": 9.934014320373535,
      "learning_rate": 2.5184024957780615e-05,
      "loss": 1.9563,
      "step": 770000
    },
    {
      "epoch": 29.783037475345168,
      "grad_norm": 13.380678176879883,
      "learning_rate": 2.518080210387903e-05,
      "loss": 1.9204,
      "step": 770100
    },
    {
      "epoch": 29.78690490002707,
      "grad_norm": 11.205830574035645,
      "learning_rate": 2.517757924997744e-05,
      "loss": 1.8669,
      "step": 770200
    },
    {
      "epoch": 29.790772324708975,
      "grad_norm": 12.408878326416016,
      "learning_rate": 2.5174356396075856e-05,
      "loss": 1.7864,
      "step": 770300
    },
    {
      "epoch": 29.794639749390882,
      "grad_norm": 8.956559181213379,
      "learning_rate": 2.517113354217427e-05,
      "loss": 1.9299,
      "step": 770400
    },
    {
      "epoch": 29.798507174072785,
      "grad_norm": 11.608892440795898,
      "learning_rate": 2.516791068827268e-05,
      "loss": 1.9091,
      "step": 770500
    },
    {
      "epoch": 29.80237459875469,
      "grad_norm": 11.922380447387695,
      "learning_rate": 2.5164687834371093e-05,
      "loss": 1.8748,
      "step": 770600
    },
    {
      "epoch": 29.806242023436592,
      "grad_norm": 14.749185562133789,
      "learning_rate": 2.5161464980469508e-05,
      "loss": 1.9132,
      "step": 770700
    },
    {
      "epoch": 29.8101094481185,
      "grad_norm": 12.994531631469727,
      "learning_rate": 2.5158242126567923e-05,
      "loss": 1.8351,
      "step": 770800
    },
    {
      "epoch": 29.813976872800403,
      "grad_norm": 10.033988952636719,
      "learning_rate": 2.515501927266633e-05,
      "loss": 1.8769,
      "step": 770900
    },
    {
      "epoch": 29.817844297482306,
      "grad_norm": 11.317933082580566,
      "learning_rate": 2.5151796418764745e-05,
      "loss": 1.9074,
      "step": 771000
    },
    {
      "epoch": 29.82171172216421,
      "grad_norm": 10.661246299743652,
      "learning_rate": 2.514857356486316e-05,
      "loss": 1.9301,
      "step": 771100
    },
    {
      "epoch": 29.825579146846117,
      "grad_norm": 13.516725540161133,
      "learning_rate": 2.5145350710961575e-05,
      "loss": 1.8873,
      "step": 771200
    },
    {
      "epoch": 29.82944657152802,
      "grad_norm": 12.107053756713867,
      "learning_rate": 2.5142127857059983e-05,
      "loss": 1.9164,
      "step": 771300
    },
    {
      "epoch": 29.833313996209924,
      "grad_norm": 17.8538761138916,
      "learning_rate": 2.5138905003158397e-05,
      "loss": 1.8386,
      "step": 771400
    },
    {
      "epoch": 29.837181420891827,
      "grad_norm": 10.779780387878418,
      "learning_rate": 2.5135682149256812e-05,
      "loss": 1.9723,
      "step": 771500
    },
    {
      "epoch": 29.84104884557373,
      "grad_norm": 10.86677360534668,
      "learning_rate": 2.5132459295355227e-05,
      "loss": 1.8374,
      "step": 771600
    },
    {
      "epoch": 29.844916270255638,
      "grad_norm": 9.642804145812988,
      "learning_rate": 2.5129236441453635e-05,
      "loss": 1.8666,
      "step": 771700
    },
    {
      "epoch": 29.84878369493754,
      "grad_norm": 14.063361167907715,
      "learning_rate": 2.512601358755205e-05,
      "loss": 1.8701,
      "step": 771800
    },
    {
      "epoch": 29.852651119619445,
      "grad_norm": 10.878743171691895,
      "learning_rate": 2.5122790733650464e-05,
      "loss": 1.8149,
      "step": 771900
    },
    {
      "epoch": 29.85651854430135,
      "grad_norm": 13.605846405029297,
      "learning_rate": 2.511956787974888e-05,
      "loss": 1.8533,
      "step": 772000
    },
    {
      "epoch": 29.860385968983255,
      "grad_norm": 7.50339412689209,
      "learning_rate": 2.5116345025847287e-05,
      "loss": 1.8507,
      "step": 772100
    },
    {
      "epoch": 29.86425339366516,
      "grad_norm": 11.649299621582031,
      "learning_rate": 2.51131221719457e-05,
      "loss": 1.9136,
      "step": 772200
    },
    {
      "epoch": 29.868120818347062,
      "grad_norm": 15.830639839172363,
      "learning_rate": 2.5109899318044116e-05,
      "loss": 1.9032,
      "step": 772300
    },
    {
      "epoch": 29.871988243028966,
      "grad_norm": 8.19005012512207,
      "learning_rate": 2.510667646414253e-05,
      "loss": 1.9433,
      "step": 772400
    },
    {
      "epoch": 29.875855667710873,
      "grad_norm": 13.599492073059082,
      "learning_rate": 2.510345361024094e-05,
      "loss": 1.8933,
      "step": 772500
    },
    {
      "epoch": 29.879723092392776,
      "grad_norm": 12.19730281829834,
      "learning_rate": 2.5100230756339354e-05,
      "loss": 1.8977,
      "step": 772600
    },
    {
      "epoch": 29.88359051707468,
      "grad_norm": 10.830413818359375,
      "learning_rate": 2.509700790243777e-05,
      "loss": 1.9043,
      "step": 772700
    },
    {
      "epoch": 29.887457941756583,
      "grad_norm": 6.6102824211120605,
      "learning_rate": 2.5093785048536183e-05,
      "loss": 1.9188,
      "step": 772800
    },
    {
      "epoch": 29.891325366438487,
      "grad_norm": 9.29171085357666,
      "learning_rate": 2.509056219463459e-05,
      "loss": 1.8972,
      "step": 772900
    },
    {
      "epoch": 29.895192791120394,
      "grad_norm": 11.277396202087402,
      "learning_rate": 2.5087339340733006e-05,
      "loss": 1.8745,
      "step": 773000
    },
    {
      "epoch": 29.899060215802297,
      "grad_norm": 13.104392051696777,
      "learning_rate": 2.508411648683142e-05,
      "loss": 1.8415,
      "step": 773100
    },
    {
      "epoch": 29.9029276404842,
      "grad_norm": 10.30672550201416,
      "learning_rate": 2.5080893632929832e-05,
      "loss": 1.8598,
      "step": 773200
    },
    {
      "epoch": 29.906795065166104,
      "grad_norm": 17.00042152404785,
      "learning_rate": 2.5077670779028246e-05,
      "loss": 1.8931,
      "step": 773300
    },
    {
      "epoch": 29.91066248984801,
      "grad_norm": 14.810334205627441,
      "learning_rate": 2.507444792512666e-05,
      "loss": 1.9442,
      "step": 773400
    },
    {
      "epoch": 29.914529914529915,
      "grad_norm": 10.969456672668457,
      "learning_rate": 2.5071225071225073e-05,
      "loss": 1.953,
      "step": 773500
    },
    {
      "epoch": 29.91839733921182,
      "grad_norm": 16.626224517822266,
      "learning_rate": 2.5068002217323484e-05,
      "loss": 1.8976,
      "step": 773600
    },
    {
      "epoch": 29.922264763893722,
      "grad_norm": 17.405742645263672,
      "learning_rate": 2.50647793634219e-05,
      "loss": 1.8205,
      "step": 773700
    },
    {
      "epoch": 29.92613218857563,
      "grad_norm": 12.063638687133789,
      "learning_rate": 2.5061556509520313e-05,
      "loss": 1.8659,
      "step": 773800
    },
    {
      "epoch": 29.929999613257532,
      "grad_norm": 11.90373420715332,
      "learning_rate": 2.5058333655618728e-05,
      "loss": 1.9255,
      "step": 773900
    },
    {
      "epoch": 29.933867037939436,
      "grad_norm": 9.819385528564453,
      "learning_rate": 2.5055110801717136e-05,
      "loss": 1.8525,
      "step": 774000
    },
    {
      "epoch": 29.93773446262134,
      "grad_norm": 11.50913143157959,
      "learning_rate": 2.505188794781555e-05,
      "loss": 1.8867,
      "step": 774100
    },
    {
      "epoch": 29.941601887303246,
      "grad_norm": 13.06889533996582,
      "learning_rate": 2.5048665093913965e-05,
      "loss": 1.8914,
      "step": 774200
    },
    {
      "epoch": 29.94546931198515,
      "grad_norm": 10.81475830078125,
      "learning_rate": 2.504544224001238e-05,
      "loss": 1.798,
      "step": 774300
    },
    {
      "epoch": 29.949336736667053,
      "grad_norm": 12.544018745422363,
      "learning_rate": 2.5042219386110788e-05,
      "loss": 1.9038,
      "step": 774400
    },
    {
      "epoch": 29.953204161348957,
      "grad_norm": 10.618620872497559,
      "learning_rate": 2.5038996532209203e-05,
      "loss": 1.8202,
      "step": 774500
    },
    {
      "epoch": 29.957071586030864,
      "grad_norm": 13.711278915405273,
      "learning_rate": 2.5035773678307617e-05,
      "loss": 1.8907,
      "step": 774600
    },
    {
      "epoch": 29.960939010712767,
      "grad_norm": 14.39186954498291,
      "learning_rate": 2.5032550824406032e-05,
      "loss": 1.8673,
      "step": 774700
    },
    {
      "epoch": 29.96480643539467,
      "grad_norm": 10.738777160644531,
      "learning_rate": 2.502932797050444e-05,
      "loss": 1.9005,
      "step": 774800
    },
    {
      "epoch": 29.968673860076574,
      "grad_norm": 10.07501220703125,
      "learning_rate": 2.5026105116602855e-05,
      "loss": 1.8156,
      "step": 774900
    },
    {
      "epoch": 29.972541284758478,
      "grad_norm": 11.774922370910645,
      "learning_rate": 2.502288226270127e-05,
      "loss": 1.8947,
      "step": 775000
    },
    {
      "epoch": 29.976408709440385,
      "grad_norm": 11.410859107971191,
      "learning_rate": 2.5019659408799684e-05,
      "loss": 1.9791,
      "step": 775100
    },
    {
      "epoch": 29.98027613412229,
      "grad_norm": 16.889646530151367,
      "learning_rate": 2.5016436554898092e-05,
      "loss": 1.9062,
      "step": 775200
    },
    {
      "epoch": 29.984143558804192,
      "grad_norm": 12.916385650634766,
      "learning_rate": 2.5013213700996507e-05,
      "loss": 1.9768,
      "step": 775300
    },
    {
      "epoch": 29.988010983486095,
      "grad_norm": 13.230262756347656,
      "learning_rate": 2.500999084709492e-05,
      "loss": 1.8937,
      "step": 775400
    },
    {
      "epoch": 29.991878408168002,
      "grad_norm": 9.620269775390625,
      "learning_rate": 2.5006767993193336e-05,
      "loss": 1.8028,
      "step": 775500
    },
    {
      "epoch": 29.995745832849906,
      "grad_norm": 10.169715881347656,
      "learning_rate": 2.5003545139291744e-05,
      "loss": 1.934,
      "step": 775600
    },
    {
      "epoch": 29.99961325753181,
      "grad_norm": 10.192645072937012,
      "learning_rate": 2.500032228539016e-05,
      "loss": 1.7922,
      "step": 775700
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.8083165884017944,
      "eval_runtime": 3.0391,
      "eval_samples_per_second": 447.835,
      "eval_steps_per_second": 447.835,
      "step": 775710
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.6828826665878296,
      "eval_runtime": 56.0312,
      "eval_samples_per_second": 461.475,
      "eval_steps_per_second": 461.475,
      "step": 775710
    },
    {
      "epoch": 30.003480682213713,
      "grad_norm": 11.881258010864258,
      "learning_rate": 2.4997099431488574e-05,
      "loss": 1.8513,
      "step": 775800
    },
    {
      "epoch": 30.00734810689562,
      "grad_norm": 9.202167510986328,
      "learning_rate": 2.4993876577586985e-05,
      "loss": 1.9012,
      "step": 775900
    },
    {
      "epoch": 30.011215531577523,
      "grad_norm": 8.864812850952148,
      "learning_rate": 2.49906537236854e-05,
      "loss": 1.8459,
      "step": 776000
    },
    {
      "epoch": 30.015082956259427,
      "grad_norm": 13.353060722351074,
      "learning_rate": 2.498743086978381e-05,
      "loss": 1.8664,
      "step": 776100
    },
    {
      "epoch": 30.01895038094133,
      "grad_norm": 11.340520858764648,
      "learning_rate": 2.4984208015882222e-05,
      "loss": 1.8697,
      "step": 776200
    },
    {
      "epoch": 30.022817805623234,
      "grad_norm": 11.031219482421875,
      "learning_rate": 2.4980985161980637e-05,
      "loss": 1.8449,
      "step": 776300
    },
    {
      "epoch": 30.02668523030514,
      "grad_norm": 8.808843612670898,
      "learning_rate": 2.497776230807905e-05,
      "loss": 1.8083,
      "step": 776400
    },
    {
      "epoch": 30.030552654987044,
      "grad_norm": 10.50241470336914,
      "learning_rate": 2.4974539454177463e-05,
      "loss": 1.9258,
      "step": 776500
    },
    {
      "epoch": 30.034420079668948,
      "grad_norm": 13.858226776123047,
      "learning_rate": 2.4971316600275878e-05,
      "loss": 1.8601,
      "step": 776600
    },
    {
      "epoch": 30.03828750435085,
      "grad_norm": 11.079885482788086,
      "learning_rate": 2.496809374637429e-05,
      "loss": 1.8641,
      "step": 776700
    },
    {
      "epoch": 30.04215492903276,
      "grad_norm": 8.56672191619873,
      "learning_rate": 2.4964870892472704e-05,
      "loss": 1.8897,
      "step": 776800
    },
    {
      "epoch": 30.046022353714662,
      "grad_norm": 11.308876037597656,
      "learning_rate": 2.496164803857112e-05,
      "loss": 1.962,
      "step": 776900
    },
    {
      "epoch": 30.049889778396565,
      "grad_norm": 10.651298522949219,
      "learning_rate": 2.495842518466953e-05,
      "loss": 1.7216,
      "step": 777000
    },
    {
      "epoch": 30.05375720307847,
      "grad_norm": 11.910072326660156,
      "learning_rate": 2.4955202330767945e-05,
      "loss": 1.9815,
      "step": 777100
    },
    {
      "epoch": 30.057624627760376,
      "grad_norm": 12.885533332824707,
      "learning_rate": 2.4951979476866356e-05,
      "loss": 1.8827,
      "step": 777200
    },
    {
      "epoch": 30.06149205244228,
      "grad_norm": 11.42288875579834,
      "learning_rate": 2.494875662296477e-05,
      "loss": 1.8041,
      "step": 777300
    },
    {
      "epoch": 30.065359477124183,
      "grad_norm": 10.279787063598633,
      "learning_rate": 2.4945533769063182e-05,
      "loss": 1.842,
      "step": 777400
    },
    {
      "epoch": 30.069226901806086,
      "grad_norm": 13.49676513671875,
      "learning_rate": 2.4942310915161597e-05,
      "loss": 1.9101,
      "step": 777500
    },
    {
      "epoch": 30.073094326487993,
      "grad_norm": 14.374181747436523,
      "learning_rate": 2.4939088061260008e-05,
      "loss": 1.8611,
      "step": 777600
    },
    {
      "epoch": 30.076961751169897,
      "grad_norm": 13.660774230957031,
      "learning_rate": 2.4935865207358423e-05,
      "loss": 1.873,
      "step": 777700
    },
    {
      "epoch": 30.0808291758518,
      "grad_norm": 12.115813255310059,
      "learning_rate": 2.4932642353456834e-05,
      "loss": 1.8504,
      "step": 777800
    },
    {
      "epoch": 30.084696600533704,
      "grad_norm": 9.731037139892578,
      "learning_rate": 2.492941949955525e-05,
      "loss": 1.8352,
      "step": 777900
    },
    {
      "epoch": 30.088564025215607,
      "grad_norm": 12.487135887145996,
      "learning_rate": 2.492619664565366e-05,
      "loss": 1.9085,
      "step": 778000
    },
    {
      "epoch": 30.092431449897514,
      "grad_norm": 11.083396911621094,
      "learning_rate": 2.4922973791752075e-05,
      "loss": 1.7972,
      "step": 778100
    },
    {
      "epoch": 30.096298874579418,
      "grad_norm": 9.964337348937988,
      "learning_rate": 2.4919750937850486e-05,
      "loss": 1.8949,
      "step": 778200
    },
    {
      "epoch": 30.10016629926132,
      "grad_norm": 9.684855461120605,
      "learning_rate": 2.49165280839489e-05,
      "loss": 1.825,
      "step": 778300
    },
    {
      "epoch": 30.104033723943225,
      "grad_norm": 13.369305610656738,
      "learning_rate": 2.4913305230047312e-05,
      "loss": 1.9053,
      "step": 778400
    },
    {
      "epoch": 30.107901148625132,
      "grad_norm": 11.809447288513184,
      "learning_rate": 2.4910082376145727e-05,
      "loss": 1.9713,
      "step": 778500
    },
    {
      "epoch": 30.111768573307035,
      "grad_norm": 8.469099998474121,
      "learning_rate": 2.490685952224414e-05,
      "loss": 1.8117,
      "step": 778600
    },
    {
      "epoch": 30.11563599798894,
      "grad_norm": 9.854469299316406,
      "learning_rate": 2.4903636668342553e-05,
      "loss": 1.8529,
      "step": 778700
    },
    {
      "epoch": 30.119503422670842,
      "grad_norm": 13.843561172485352,
      "learning_rate": 2.4900413814440964e-05,
      "loss": 1.8307,
      "step": 778800
    },
    {
      "epoch": 30.12337084735275,
      "grad_norm": 10.976208686828613,
      "learning_rate": 2.489719096053938e-05,
      "loss": 1.8397,
      "step": 778900
    },
    {
      "epoch": 30.127238272034653,
      "grad_norm": 14.895941734313965,
      "learning_rate": 2.489396810663779e-05,
      "loss": 1.8311,
      "step": 779000
    },
    {
      "epoch": 30.131105696716556,
      "grad_norm": 14.328227043151855,
      "learning_rate": 2.4890745252736202e-05,
      "loss": 1.8583,
      "step": 779100
    },
    {
      "epoch": 30.13497312139846,
      "grad_norm": 10.125642776489258,
      "learning_rate": 2.4887522398834617e-05,
      "loss": 1.9974,
      "step": 779200
    },
    {
      "epoch": 30.138840546080367,
      "grad_norm": 12.818233489990234,
      "learning_rate": 2.4884299544933028e-05,
      "loss": 1.9255,
      "step": 779300
    },
    {
      "epoch": 30.14270797076227,
      "grad_norm": 10.8829984664917,
      "learning_rate": 2.4881076691031443e-05,
      "loss": 1.9705,
      "step": 779400
    },
    {
      "epoch": 30.146575395444174,
      "grad_norm": 13.729498863220215,
      "learning_rate": 2.4877853837129854e-05,
      "loss": 1.8702,
      "step": 779500
    },
    {
      "epoch": 30.150442820126077,
      "grad_norm": 14.608952522277832,
      "learning_rate": 2.487463098322827e-05,
      "loss": 1.9184,
      "step": 779600
    },
    {
      "epoch": 30.15431024480798,
      "grad_norm": 14.36165714263916,
      "learning_rate": 2.487140812932668e-05,
      "loss": 1.9039,
      "step": 779700
    },
    {
      "epoch": 30.158177669489888,
      "grad_norm": 10.648160934448242,
      "learning_rate": 2.4868185275425095e-05,
      "loss": 1.843,
      "step": 779800
    },
    {
      "epoch": 30.16204509417179,
      "grad_norm": 10.162741661071777,
      "learning_rate": 2.4864962421523506e-05,
      "loss": 1.8574,
      "step": 779900
    },
    {
      "epoch": 30.165912518853695,
      "grad_norm": 10.943995475769043,
      "learning_rate": 2.486173956762192e-05,
      "loss": 1.8872,
      "step": 780000
    },
    {
      "epoch": 30.169779943535598,
      "grad_norm": 11.610679626464844,
      "learning_rate": 2.4858516713720336e-05,
      "loss": 1.7965,
      "step": 780100
    },
    {
      "epoch": 30.173647368217505,
      "grad_norm": 12.407299995422363,
      "learning_rate": 2.4855293859818747e-05,
      "loss": 1.8019,
      "step": 780200
    },
    {
      "epoch": 30.17751479289941,
      "grad_norm": 11.622715950012207,
      "learning_rate": 2.485207100591716e-05,
      "loss": 1.9177,
      "step": 780300
    },
    {
      "epoch": 30.181382217581312,
      "grad_norm": 13.090559959411621,
      "learning_rate": 2.4848848152015576e-05,
      "loss": 1.922,
      "step": 780400
    },
    {
      "epoch": 30.185249642263216,
      "grad_norm": 12.537091255187988,
      "learning_rate": 2.4845625298113988e-05,
      "loss": 1.8186,
      "step": 780500
    },
    {
      "epoch": 30.189117066945123,
      "grad_norm": 10.523038864135742,
      "learning_rate": 2.4842402444212402e-05,
      "loss": 1.8741,
      "step": 780600
    },
    {
      "epoch": 30.192984491627026,
      "grad_norm": 16.118019104003906,
      "learning_rate": 2.4839179590310814e-05,
      "loss": 1.9161,
      "step": 780700
    },
    {
      "epoch": 30.19685191630893,
      "grad_norm": 12.801981925964355,
      "learning_rate": 2.483595673640923e-05,
      "loss": 1.8542,
      "step": 780800
    },
    {
      "epoch": 30.200719340990833,
      "grad_norm": 11.317617416381836,
      "learning_rate": 2.483273388250764e-05,
      "loss": 1.8128,
      "step": 780900
    },
    {
      "epoch": 30.20458676567274,
      "grad_norm": 10.537385940551758,
      "learning_rate": 2.4829511028606054e-05,
      "loss": 1.9013,
      "step": 781000
    },
    {
      "epoch": 30.208454190354644,
      "grad_norm": 10.31912612915039,
      "learning_rate": 2.4826288174704466e-05,
      "loss": 1.9801,
      "step": 781100
    },
    {
      "epoch": 30.212321615036547,
      "grad_norm": 13.234135627746582,
      "learning_rate": 2.482306532080288e-05,
      "loss": 1.842,
      "step": 781200
    },
    {
      "epoch": 30.21618903971845,
      "grad_norm": 11.8065824508667,
      "learning_rate": 2.4819842466901292e-05,
      "loss": 1.7715,
      "step": 781300
    },
    {
      "epoch": 30.220056464400354,
      "grad_norm": 14.723872184753418,
      "learning_rate": 2.4816619612999707e-05,
      "loss": 1.8693,
      "step": 781400
    },
    {
      "epoch": 30.22392388908226,
      "grad_norm": 13.550588607788086,
      "learning_rate": 2.4813396759098118e-05,
      "loss": 1.8489,
      "step": 781500
    },
    {
      "epoch": 30.227791313764165,
      "grad_norm": 11.597648620605469,
      "learning_rate": 2.4810173905196533e-05,
      "loss": 1.9539,
      "step": 781600
    },
    {
      "epoch": 30.231658738446068,
      "grad_norm": 13.146438598632812,
      "learning_rate": 2.4806951051294944e-05,
      "loss": 1.8146,
      "step": 781700
    },
    {
      "epoch": 30.23552616312797,
      "grad_norm": 12.600093841552734,
      "learning_rate": 2.480372819739336e-05,
      "loss": 1.9654,
      "step": 781800
    },
    {
      "epoch": 30.23939358780988,
      "grad_norm": 13.228296279907227,
      "learning_rate": 2.480050534349177e-05,
      "loss": 1.8521,
      "step": 781900
    },
    {
      "epoch": 30.243261012491782,
      "grad_norm": 12.56653881072998,
      "learning_rate": 2.479728248959018e-05,
      "loss": 1.8042,
      "step": 782000
    },
    {
      "epoch": 30.247128437173686,
      "grad_norm": 12.888704299926758,
      "learning_rate": 2.4794059635688596e-05,
      "loss": 1.8241,
      "step": 782100
    },
    {
      "epoch": 30.25099586185559,
      "grad_norm": 12.581583976745605,
      "learning_rate": 2.4790836781787007e-05,
      "loss": 1.9182,
      "step": 782200
    },
    {
      "epoch": 30.254863286537496,
      "grad_norm": 10.711907386779785,
      "learning_rate": 2.4787613927885422e-05,
      "loss": 1.8489,
      "step": 782300
    },
    {
      "epoch": 30.2587307112194,
      "grad_norm": 10.372552871704102,
      "learning_rate": 2.4784391073983833e-05,
      "loss": 1.8831,
      "step": 782400
    },
    {
      "epoch": 30.262598135901303,
      "grad_norm": 11.357577323913574,
      "learning_rate": 2.4781168220082248e-05,
      "loss": 1.9604,
      "step": 782500
    },
    {
      "epoch": 30.266465560583207,
      "grad_norm": 12.811178207397461,
      "learning_rate": 2.477794536618066e-05,
      "loss": 1.895,
      "step": 782600
    },
    {
      "epoch": 30.270332985265114,
      "grad_norm": 13.372196197509766,
      "learning_rate": 2.4774722512279074e-05,
      "loss": 1.9396,
      "step": 782700
    },
    {
      "epoch": 30.274200409947017,
      "grad_norm": 12.978015899658203,
      "learning_rate": 2.4771499658377485e-05,
      "loss": 1.9893,
      "step": 782800
    },
    {
      "epoch": 30.27806783462892,
      "grad_norm": 12.206502914428711,
      "learning_rate": 2.47682768044759e-05,
      "loss": 1.8197,
      "step": 782900
    },
    {
      "epoch": 30.281935259310824,
      "grad_norm": 11.000778198242188,
      "learning_rate": 2.476505395057431e-05,
      "loss": 1.9612,
      "step": 783000
    },
    {
      "epoch": 30.285802683992728,
      "grad_norm": 14.628094673156738,
      "learning_rate": 2.4761831096672726e-05,
      "loss": 1.8778,
      "step": 783100
    },
    {
      "epoch": 30.289670108674635,
      "grad_norm": 11.75671672821045,
      "learning_rate": 2.4758608242771138e-05,
      "loss": 1.9148,
      "step": 783200
    },
    {
      "epoch": 30.293537533356538,
      "grad_norm": 15.127947807312012,
      "learning_rate": 2.4755385388869552e-05,
      "loss": 1.9511,
      "step": 783300
    },
    {
      "epoch": 30.29740495803844,
      "grad_norm": 15.509913444519043,
      "learning_rate": 2.4752162534967967e-05,
      "loss": 1.9412,
      "step": 783400
    },
    {
      "epoch": 30.301272382720345,
      "grad_norm": 11.8911771774292,
      "learning_rate": 2.474893968106638e-05,
      "loss": 1.9003,
      "step": 783500
    },
    {
      "epoch": 30.305139807402252,
      "grad_norm": 11.08275032043457,
      "learning_rate": 2.4745716827164793e-05,
      "loss": 1.9085,
      "step": 783600
    },
    {
      "epoch": 30.309007232084156,
      "grad_norm": 12.132538795471191,
      "learning_rate": 2.4742493973263204e-05,
      "loss": 1.9326,
      "step": 783700
    },
    {
      "epoch": 30.31287465676606,
      "grad_norm": 7.3165154457092285,
      "learning_rate": 2.473927111936162e-05,
      "loss": 1.863,
      "step": 783800
    },
    {
      "epoch": 30.316742081447963,
      "grad_norm": 10.56784725189209,
      "learning_rate": 2.4736048265460034e-05,
      "loss": 1.8687,
      "step": 783900
    },
    {
      "epoch": 30.32060950612987,
      "grad_norm": 12.20505428314209,
      "learning_rate": 2.4732825411558445e-05,
      "loss": 1.7822,
      "step": 784000
    },
    {
      "epoch": 30.324476930811773,
      "grad_norm": 11.862279891967773,
      "learning_rate": 2.472960255765686e-05,
      "loss": 1.875,
      "step": 784100
    },
    {
      "epoch": 30.328344355493677,
      "grad_norm": 11.24166202545166,
      "learning_rate": 2.472637970375527e-05,
      "loss": 1.8668,
      "step": 784200
    },
    {
      "epoch": 30.33221178017558,
      "grad_norm": 10.510512351989746,
      "learning_rate": 2.4723156849853686e-05,
      "loss": 1.8937,
      "step": 784300
    },
    {
      "epoch": 30.336079204857484,
      "grad_norm": 9.419478416442871,
      "learning_rate": 2.4719933995952097e-05,
      "loss": 1.8642,
      "step": 784400
    },
    {
      "epoch": 30.33994662953939,
      "grad_norm": 13.581061363220215,
      "learning_rate": 2.4716711142050512e-05,
      "loss": 1.8033,
      "step": 784500
    },
    {
      "epoch": 30.343814054221294,
      "grad_norm": 14.225284576416016,
      "learning_rate": 2.4713488288148923e-05,
      "loss": 1.8922,
      "step": 784600
    },
    {
      "epoch": 30.347681478903198,
      "grad_norm": 9.913573265075684,
      "learning_rate": 2.4710265434247338e-05,
      "loss": 1.8537,
      "step": 784700
    },
    {
      "epoch": 30.3515489035851,
      "grad_norm": 11.662924766540527,
      "learning_rate": 2.470704258034575e-05,
      "loss": 1.8741,
      "step": 784800
    },
    {
      "epoch": 30.355416328267008,
      "grad_norm": 10.453567504882812,
      "learning_rate": 2.470381972644416e-05,
      "loss": 1.9193,
      "step": 784900
    },
    {
      "epoch": 30.35928375294891,
      "grad_norm": 11.212651252746582,
      "learning_rate": 2.4700596872542575e-05,
      "loss": 1.8949,
      "step": 785000
    },
    {
      "epoch": 30.363151177630815,
      "grad_norm": 12.548419952392578,
      "learning_rate": 2.4697374018640987e-05,
      "loss": 1.8151,
      "step": 785100
    },
    {
      "epoch": 30.36701860231272,
      "grad_norm": 16.61321258544922,
      "learning_rate": 2.46941511647394e-05,
      "loss": 1.8889,
      "step": 785200
    },
    {
      "epoch": 30.370886026994626,
      "grad_norm": 14.365127563476562,
      "learning_rate": 2.4690928310837813e-05,
      "loss": 1.9077,
      "step": 785300
    },
    {
      "epoch": 30.37475345167653,
      "grad_norm": 17.09502410888672,
      "learning_rate": 2.4687705456936227e-05,
      "loss": 2.0114,
      "step": 785400
    },
    {
      "epoch": 30.378620876358433,
      "grad_norm": 11.65549087524414,
      "learning_rate": 2.468448260303464e-05,
      "loss": 1.985,
      "step": 785500
    },
    {
      "epoch": 30.382488301040336,
      "grad_norm": 11.772787094116211,
      "learning_rate": 2.4681259749133054e-05,
      "loss": 1.9535,
      "step": 785600
    },
    {
      "epoch": 30.386355725722243,
      "grad_norm": 11.767624855041504,
      "learning_rate": 2.4678036895231465e-05,
      "loss": 1.8764,
      "step": 785700
    },
    {
      "epoch": 30.390223150404147,
      "grad_norm": 12.478252410888672,
      "learning_rate": 2.467481404132988e-05,
      "loss": 1.9105,
      "step": 785800
    },
    {
      "epoch": 30.39409057508605,
      "grad_norm": 11.62112808227539,
      "learning_rate": 2.467159118742829e-05,
      "loss": 1.8288,
      "step": 785900
    },
    {
      "epoch": 30.397957999767954,
      "grad_norm": 14.150322914123535,
      "learning_rate": 2.4668368333526706e-05,
      "loss": 1.9195,
      "step": 786000
    },
    {
      "epoch": 30.401825424449857,
      "grad_norm": 14.197171211242676,
      "learning_rate": 2.4665145479625117e-05,
      "loss": 1.8449,
      "step": 786100
    },
    {
      "epoch": 30.405692849131764,
      "grad_norm": 9.681513786315918,
      "learning_rate": 2.466192262572353e-05,
      "loss": 1.8318,
      "step": 786200
    },
    {
      "epoch": 30.409560273813668,
      "grad_norm": 9.691669464111328,
      "learning_rate": 2.4658699771821943e-05,
      "loss": 1.8664,
      "step": 786300
    },
    {
      "epoch": 30.41342769849557,
      "grad_norm": 10.963160514831543,
      "learning_rate": 2.4655476917920358e-05,
      "loss": 1.9079,
      "step": 786400
    },
    {
      "epoch": 30.417295123177475,
      "grad_norm": 11.712272644042969,
      "learning_rate": 2.465225406401877e-05,
      "loss": 1.9324,
      "step": 786500
    },
    {
      "epoch": 30.42116254785938,
      "grad_norm": 13.043624877929688,
      "learning_rate": 2.4649031210117184e-05,
      "loss": 1.852,
      "step": 786600
    },
    {
      "epoch": 30.425029972541285,
      "grad_norm": 12.220145225524902,
      "learning_rate": 2.4645808356215595e-05,
      "loss": 1.9528,
      "step": 786700
    },
    {
      "epoch": 30.42889739722319,
      "grad_norm": 13.710179328918457,
      "learning_rate": 2.464258550231401e-05,
      "loss": 1.9892,
      "step": 786800
    },
    {
      "epoch": 30.432764821905092,
      "grad_norm": 10.996265411376953,
      "learning_rate": 2.4639362648412425e-05,
      "loss": 1.8444,
      "step": 786900
    },
    {
      "epoch": 30.436632246587,
      "grad_norm": 10.838544845581055,
      "learning_rate": 2.4636139794510836e-05,
      "loss": 1.8634,
      "step": 787000
    },
    {
      "epoch": 30.440499671268903,
      "grad_norm": 15.253308296203613,
      "learning_rate": 2.463291694060925e-05,
      "loss": 1.8896,
      "step": 787100
    },
    {
      "epoch": 30.444367095950806,
      "grad_norm": 8.575762748718262,
      "learning_rate": 2.4629694086707665e-05,
      "loss": 1.9266,
      "step": 787200
    },
    {
      "epoch": 30.44823452063271,
      "grad_norm": 12.222434997558594,
      "learning_rate": 2.4626471232806077e-05,
      "loss": 1.9412,
      "step": 787300
    },
    {
      "epoch": 30.452101945314617,
      "grad_norm": 12.2092866897583,
      "learning_rate": 2.462324837890449e-05,
      "loss": 1.9821,
      "step": 787400
    },
    {
      "epoch": 30.45596936999652,
      "grad_norm": 10.727083206176758,
      "learning_rate": 2.4620025525002903e-05,
      "loss": 1.9129,
      "step": 787500
    },
    {
      "epoch": 30.459836794678424,
      "grad_norm": 16.292194366455078,
      "learning_rate": 2.4616802671101317e-05,
      "loss": 1.8563,
      "step": 787600
    },
    {
      "epoch": 30.463704219360327,
      "grad_norm": 11.772163391113281,
      "learning_rate": 2.461357981719973e-05,
      "loss": 1.9442,
      "step": 787700
    },
    {
      "epoch": 30.46757164404223,
      "grad_norm": 10.301581382751465,
      "learning_rate": 2.4610356963298143e-05,
      "loss": 1.7906,
      "step": 787800
    },
    {
      "epoch": 30.471439068724138,
      "grad_norm": 9.217280387878418,
      "learning_rate": 2.4607134109396555e-05,
      "loss": 1.8535,
      "step": 787900
    },
    {
      "epoch": 30.47530649340604,
      "grad_norm": 12.740548133850098,
      "learning_rate": 2.4603911255494966e-05,
      "loss": 1.8739,
      "step": 788000
    },
    {
      "epoch": 30.479173918087945,
      "grad_norm": 12.360543251037598,
      "learning_rate": 2.460068840159338e-05,
      "loss": 1.8882,
      "step": 788100
    },
    {
      "epoch": 30.483041342769848,
      "grad_norm": 10.661444664001465,
      "learning_rate": 2.4597465547691792e-05,
      "loss": 1.8399,
      "step": 788200
    },
    {
      "epoch": 30.486908767451755,
      "grad_norm": 11.355597496032715,
      "learning_rate": 2.4594242693790207e-05,
      "loss": 1.7841,
      "step": 788300
    },
    {
      "epoch": 30.49077619213366,
      "grad_norm": 9.499527931213379,
      "learning_rate": 2.4591019839888618e-05,
      "loss": 1.9498,
      "step": 788400
    },
    {
      "epoch": 30.494643616815562,
      "grad_norm": 11.731335639953613,
      "learning_rate": 2.4587796985987033e-05,
      "loss": 1.9023,
      "step": 788500
    },
    {
      "epoch": 30.498511041497466,
      "grad_norm": 8.94040298461914,
      "learning_rate": 2.4584574132085444e-05,
      "loss": 1.929,
      "step": 788600
    },
    {
      "epoch": 30.502378466179373,
      "grad_norm": 15.489470481872559,
      "learning_rate": 2.458135127818386e-05,
      "loss": 1.9197,
      "step": 788700
    },
    {
      "epoch": 30.506245890861276,
      "grad_norm": 10.980182647705078,
      "learning_rate": 2.457812842428227e-05,
      "loss": 1.9054,
      "step": 788800
    },
    {
      "epoch": 30.51011331554318,
      "grad_norm": 10.9376802444458,
      "learning_rate": 2.4574905570380685e-05,
      "loss": 1.754,
      "step": 788900
    },
    {
      "epoch": 30.513980740225083,
      "grad_norm": 12.104264259338379,
      "learning_rate": 2.4571682716479096e-05,
      "loss": 1.8842,
      "step": 789000
    },
    {
      "epoch": 30.51784816490699,
      "grad_norm": 11.433716773986816,
      "learning_rate": 2.456845986257751e-05,
      "loss": 1.8982,
      "step": 789100
    },
    {
      "epoch": 30.521715589588894,
      "grad_norm": 12.182771682739258,
      "learning_rate": 2.4565237008675922e-05,
      "loss": 1.8951,
      "step": 789200
    },
    {
      "epoch": 30.525583014270797,
      "grad_norm": 9.608952522277832,
      "learning_rate": 2.4562014154774337e-05,
      "loss": 1.8361,
      "step": 789300
    },
    {
      "epoch": 30.5294504389527,
      "grad_norm": 12.756014823913574,
      "learning_rate": 2.455879130087275e-05,
      "loss": 1.9312,
      "step": 789400
    },
    {
      "epoch": 30.533317863634604,
      "grad_norm": 16.760698318481445,
      "learning_rate": 2.4555568446971163e-05,
      "loss": 1.9789,
      "step": 789500
    },
    {
      "epoch": 30.53718528831651,
      "grad_norm": 10.985673904418945,
      "learning_rate": 2.4552345593069575e-05,
      "loss": 1.9057,
      "step": 789600
    },
    {
      "epoch": 30.541052712998415,
      "grad_norm": 10.767552375793457,
      "learning_rate": 2.454912273916799e-05,
      "loss": 1.9422,
      "step": 789700
    },
    {
      "epoch": 30.544920137680318,
      "grad_norm": 10.141597747802734,
      "learning_rate": 2.45458998852664e-05,
      "loss": 1.8431,
      "step": 789800
    },
    {
      "epoch": 30.54878756236222,
      "grad_norm": 10.742233276367188,
      "learning_rate": 2.4542677031364815e-05,
      "loss": 1.9284,
      "step": 789900
    },
    {
      "epoch": 30.55265498704413,
      "grad_norm": 11.752287864685059,
      "learning_rate": 2.4539454177463227e-05,
      "loss": 1.8221,
      "step": 790000
    },
    {
      "epoch": 30.556522411726032,
      "grad_norm": 12.631125450134277,
      "learning_rate": 2.453623132356164e-05,
      "loss": 1.8617,
      "step": 790100
    },
    {
      "epoch": 30.560389836407936,
      "grad_norm": 14.720258712768555,
      "learning_rate": 2.4533008469660053e-05,
      "loss": 1.8161,
      "step": 790200
    },
    {
      "epoch": 30.56425726108984,
      "grad_norm": 9.66862964630127,
      "learning_rate": 2.4529785615758467e-05,
      "loss": 1.9385,
      "step": 790300
    },
    {
      "epoch": 30.568124685771746,
      "grad_norm": 12.086211204528809,
      "learning_rate": 2.4526562761856882e-05,
      "loss": 1.8424,
      "step": 790400
    },
    {
      "epoch": 30.57199211045365,
      "grad_norm": 11.53652286529541,
      "learning_rate": 2.4523339907955293e-05,
      "loss": 1.8724,
      "step": 790500
    },
    {
      "epoch": 30.575859535135553,
      "grad_norm": 12.06863784790039,
      "learning_rate": 2.4520117054053708e-05,
      "loss": 1.8752,
      "step": 790600
    },
    {
      "epoch": 30.579726959817457,
      "grad_norm": 9.922478675842285,
      "learning_rate": 2.4516894200152123e-05,
      "loss": 1.863,
      "step": 790700
    },
    {
      "epoch": 30.583594384499364,
      "grad_norm": 9.886664390563965,
      "learning_rate": 2.4513671346250534e-05,
      "loss": 1.8712,
      "step": 790800
    },
    {
      "epoch": 30.587461809181267,
      "grad_norm": 13.383930206298828,
      "learning_rate": 2.4510448492348946e-05,
      "loss": 1.8791,
      "step": 790900
    },
    {
      "epoch": 30.59132923386317,
      "grad_norm": 10.906046867370605,
      "learning_rate": 2.450722563844736e-05,
      "loss": 1.8906,
      "step": 791000
    },
    {
      "epoch": 30.595196658545074,
      "grad_norm": 6.718664646148682,
      "learning_rate": 2.450400278454577e-05,
      "loss": 1.8824,
      "step": 791100
    },
    {
      "epoch": 30.599064083226978,
      "grad_norm": 11.214651107788086,
      "learning_rate": 2.4500779930644186e-05,
      "loss": 1.8498,
      "step": 791200
    },
    {
      "epoch": 30.602931507908885,
      "grad_norm": 8.815808296203613,
      "learning_rate": 2.4497557076742598e-05,
      "loss": 1.9194,
      "step": 791300
    },
    {
      "epoch": 30.606798932590788,
      "grad_norm": 10.772720336914062,
      "learning_rate": 2.4494334222841012e-05,
      "loss": 1.8499,
      "step": 791400
    },
    {
      "epoch": 30.61066635727269,
      "grad_norm": 8.026177406311035,
      "learning_rate": 2.4491111368939424e-05,
      "loss": 1.7792,
      "step": 791500
    },
    {
      "epoch": 30.614533781954595,
      "grad_norm": 13.503008842468262,
      "learning_rate": 2.448788851503784e-05,
      "loss": 1.7225,
      "step": 791600
    },
    {
      "epoch": 30.618401206636502,
      "grad_norm": 11.30703353881836,
      "learning_rate": 2.448466566113625e-05,
      "loss": 1.9098,
      "step": 791700
    },
    {
      "epoch": 30.622268631318406,
      "grad_norm": 13.431660652160645,
      "learning_rate": 2.4481442807234664e-05,
      "loss": 1.8299,
      "step": 791800
    },
    {
      "epoch": 30.62613605600031,
      "grad_norm": 10.393735885620117,
      "learning_rate": 2.4478219953333076e-05,
      "loss": 1.8161,
      "step": 791900
    },
    {
      "epoch": 30.630003480682213,
      "grad_norm": 13.533125877380371,
      "learning_rate": 2.447499709943149e-05,
      "loss": 1.8865,
      "step": 792000
    },
    {
      "epoch": 30.63387090536412,
      "grad_norm": 13.752667427062988,
      "learning_rate": 2.4471774245529902e-05,
      "loss": 1.9038,
      "step": 792100
    },
    {
      "epoch": 30.637738330046023,
      "grad_norm": 11.648042678833008,
      "learning_rate": 2.4468551391628317e-05,
      "loss": 1.8555,
      "step": 792200
    },
    {
      "epoch": 30.641605754727927,
      "grad_norm": 12.516372680664062,
      "learning_rate": 2.4465328537726728e-05,
      "loss": 1.8307,
      "step": 792300
    },
    {
      "epoch": 30.64547317940983,
      "grad_norm": 12.913732528686523,
      "learning_rate": 2.4462105683825143e-05,
      "loss": 1.8692,
      "step": 792400
    },
    {
      "epoch": 30.649340604091734,
      "grad_norm": 10.306466102600098,
      "learning_rate": 2.4458882829923554e-05,
      "loss": 1.8536,
      "step": 792500
    },
    {
      "epoch": 30.65320802877364,
      "grad_norm": 12.559536933898926,
      "learning_rate": 2.445565997602197e-05,
      "loss": 1.8986,
      "step": 792600
    },
    {
      "epoch": 30.657075453455544,
      "grad_norm": 10.223766326904297,
      "learning_rate": 2.445243712212038e-05,
      "loss": 1.7624,
      "step": 792700
    },
    {
      "epoch": 30.660942878137448,
      "grad_norm": 8.858316421508789,
      "learning_rate": 2.4449214268218795e-05,
      "loss": 1.8029,
      "step": 792800
    },
    {
      "epoch": 30.66481030281935,
      "grad_norm": 11.633999824523926,
      "learning_rate": 2.4445991414317206e-05,
      "loss": 1.8267,
      "step": 792900
    },
    {
      "epoch": 30.668677727501258,
      "grad_norm": 10.317354202270508,
      "learning_rate": 2.444276856041562e-05,
      "loss": 1.9291,
      "step": 793000
    },
    {
      "epoch": 30.67254515218316,
      "grad_norm": 10.63836669921875,
      "learning_rate": 2.4439545706514032e-05,
      "loss": 1.9593,
      "step": 793100
    },
    {
      "epoch": 30.676412576865065,
      "grad_norm": 10.674242973327637,
      "learning_rate": 2.4436322852612447e-05,
      "loss": 1.874,
      "step": 793200
    },
    {
      "epoch": 30.68028000154697,
      "grad_norm": 10.33193302154541,
      "learning_rate": 2.4433099998710858e-05,
      "loss": 1.7825,
      "step": 793300
    },
    {
      "epoch": 30.684147426228876,
      "grad_norm": 13.445260047912598,
      "learning_rate": 2.4429877144809273e-05,
      "loss": 1.9023,
      "step": 793400
    },
    {
      "epoch": 30.68801485091078,
      "grad_norm": 10.474781036376953,
      "learning_rate": 2.4426654290907684e-05,
      "loss": 1.8831,
      "step": 793500
    },
    {
      "epoch": 30.691882275592683,
      "grad_norm": 11.508241653442383,
      "learning_rate": 2.44234314370061e-05,
      "loss": 1.9121,
      "step": 793600
    },
    {
      "epoch": 30.695749700274586,
      "grad_norm": 11.004678726196289,
      "learning_rate": 2.442020858310451e-05,
      "loss": 1.9253,
      "step": 793700
    },
    {
      "epoch": 30.699617124956493,
      "grad_norm": 15.270855903625488,
      "learning_rate": 2.4416985729202925e-05,
      "loss": 1.9707,
      "step": 793800
    },
    {
      "epoch": 30.703484549638397,
      "grad_norm": 12.459253311157227,
      "learning_rate": 2.441376287530134e-05,
      "loss": 1.8381,
      "step": 793900
    },
    {
      "epoch": 30.7073519743203,
      "grad_norm": 8.538511276245117,
      "learning_rate": 2.441054002139975e-05,
      "loss": 1.9498,
      "step": 794000
    },
    {
      "epoch": 30.711219399002204,
      "grad_norm": 10.996685981750488,
      "learning_rate": 2.4407317167498166e-05,
      "loss": 1.8493,
      "step": 794100
    },
    {
      "epoch": 30.715086823684107,
      "grad_norm": 9.484733581542969,
      "learning_rate": 2.4404094313596577e-05,
      "loss": 1.8354,
      "step": 794200
    },
    {
      "epoch": 30.718954248366014,
      "grad_norm": 11.021178245544434,
      "learning_rate": 2.4400871459694992e-05,
      "loss": 1.9199,
      "step": 794300
    },
    {
      "epoch": 30.722821673047918,
      "grad_norm": 13.7583589553833,
      "learning_rate": 2.4397648605793403e-05,
      "loss": 1.9527,
      "step": 794400
    },
    {
      "epoch": 30.72668909772982,
      "grad_norm": 11.78460693359375,
      "learning_rate": 2.4394425751891818e-05,
      "loss": 1.8922,
      "step": 794500
    },
    {
      "epoch": 30.730556522411725,
      "grad_norm": 11.64128303527832,
      "learning_rate": 2.439120289799023e-05,
      "loss": 1.8058,
      "step": 794600
    },
    {
      "epoch": 30.73442394709363,
      "grad_norm": 10.990826606750488,
      "learning_rate": 2.4387980044088644e-05,
      "loss": 1.9265,
      "step": 794700
    },
    {
      "epoch": 30.738291371775535,
      "grad_norm": 10.162632942199707,
      "learning_rate": 2.4384757190187055e-05,
      "loss": 1.8247,
      "step": 794800
    },
    {
      "epoch": 30.74215879645744,
      "grad_norm": 12.576142311096191,
      "learning_rate": 2.438153433628547e-05,
      "loss": 1.842,
      "step": 794900
    },
    {
      "epoch": 30.746026221139342,
      "grad_norm": 12.149320602416992,
      "learning_rate": 2.437831148238388e-05,
      "loss": 1.8566,
      "step": 795000
    },
    {
      "epoch": 30.74989364582125,
      "grad_norm": 10.550862312316895,
      "learning_rate": 2.4375088628482296e-05,
      "loss": 1.8219,
      "step": 795100
    },
    {
      "epoch": 30.753761070503153,
      "grad_norm": 9.279107093811035,
      "learning_rate": 2.4371865774580707e-05,
      "loss": 1.8929,
      "step": 795200
    },
    {
      "epoch": 30.757628495185056,
      "grad_norm": 11.86976146697998,
      "learning_rate": 2.4368642920679122e-05,
      "loss": 1.9436,
      "step": 795300
    },
    {
      "epoch": 30.76149591986696,
      "grad_norm": 9.522278785705566,
      "learning_rate": 2.4365420066777533e-05,
      "loss": 1.8976,
      "step": 795400
    },
    {
      "epoch": 30.765363344548867,
      "grad_norm": 12.034829139709473,
      "learning_rate": 2.4362197212875948e-05,
      "loss": 1.8926,
      "step": 795500
    },
    {
      "epoch": 30.76923076923077,
      "grad_norm": 11.083921432495117,
      "learning_rate": 2.435897435897436e-05,
      "loss": 1.7813,
      "step": 795600
    },
    {
      "epoch": 30.773098193912674,
      "grad_norm": 12.464536666870117,
      "learning_rate": 2.4355751505072774e-05,
      "loss": 1.8615,
      "step": 795700
    },
    {
      "epoch": 30.776965618594577,
      "grad_norm": 16.725841522216797,
      "learning_rate": 2.4352528651171185e-05,
      "loss": 1.9191,
      "step": 795800
    },
    {
      "epoch": 30.78083304327648,
      "grad_norm": 12.9192533493042,
      "learning_rate": 2.43493057972696e-05,
      "loss": 1.8984,
      "step": 795900
    },
    {
      "epoch": 30.784700467958388,
      "grad_norm": 16.742855072021484,
      "learning_rate": 2.434608294336801e-05,
      "loss": 1.8825,
      "step": 796000
    },
    {
      "epoch": 30.78856789264029,
      "grad_norm": 13.739065170288086,
      "learning_rate": 2.4342860089466426e-05,
      "loss": 1.9296,
      "step": 796100
    },
    {
      "epoch": 30.792435317322195,
      "grad_norm": 11.415945053100586,
      "learning_rate": 2.4339637235564838e-05,
      "loss": 1.8548,
      "step": 796200
    },
    {
      "epoch": 30.796302742004098,
      "grad_norm": 9.949090003967285,
      "learning_rate": 2.4336414381663252e-05,
      "loss": 1.9225,
      "step": 796300
    },
    {
      "epoch": 30.800170166686005,
      "grad_norm": 11.474306106567383,
      "learning_rate": 2.4333191527761664e-05,
      "loss": 1.8557,
      "step": 796400
    },
    {
      "epoch": 30.80403759136791,
      "grad_norm": 13.116493225097656,
      "learning_rate": 2.4329968673860078e-05,
      "loss": 1.8531,
      "step": 796500
    },
    {
      "epoch": 30.807905016049812,
      "grad_norm": 12.357745170593262,
      "learning_rate": 2.432674581995849e-05,
      "loss": 1.9049,
      "step": 796600
    },
    {
      "epoch": 30.811772440731716,
      "grad_norm": 12.026707649230957,
      "learning_rate": 2.43235229660569e-05,
      "loss": 1.9312,
      "step": 796700
    },
    {
      "epoch": 30.815639865413623,
      "grad_norm": 13.006939888000488,
      "learning_rate": 2.4320300112155316e-05,
      "loss": 1.8486,
      "step": 796800
    },
    {
      "epoch": 30.819507290095526,
      "grad_norm": 13.208823204040527,
      "learning_rate": 2.431707725825373e-05,
      "loss": 1.8293,
      "step": 796900
    },
    {
      "epoch": 30.82337471477743,
      "grad_norm": 10.297131538391113,
      "learning_rate": 2.4313854404352142e-05,
      "loss": 1.8587,
      "step": 797000
    },
    {
      "epoch": 30.827242139459333,
      "grad_norm": 14.01452350616455,
      "learning_rate": 2.4310631550450556e-05,
      "loss": 1.891,
      "step": 797100
    },
    {
      "epoch": 30.83110956414124,
      "grad_norm": 10.142295837402344,
      "learning_rate": 2.430740869654897e-05,
      "loss": 1.9114,
      "step": 797200
    },
    {
      "epoch": 30.834976988823144,
      "grad_norm": 11.103930473327637,
      "learning_rate": 2.4304185842647382e-05,
      "loss": 1.8437,
      "step": 797300
    },
    {
      "epoch": 30.838844413505047,
      "grad_norm": 12.98091983795166,
      "learning_rate": 2.4300962988745797e-05,
      "loss": 1.9299,
      "step": 797400
    },
    {
      "epoch": 30.84271183818695,
      "grad_norm": 11.982022285461426,
      "learning_rate": 2.429774013484421e-05,
      "loss": 1.8364,
      "step": 797500
    },
    {
      "epoch": 30.846579262868854,
      "grad_norm": 13.464385032653809,
      "learning_rate": 2.4294517280942623e-05,
      "loss": 1.8376,
      "step": 797600
    },
    {
      "epoch": 30.85044668755076,
      "grad_norm": 9.585793495178223,
      "learning_rate": 2.4291294427041035e-05,
      "loss": 1.8827,
      "step": 797700
    },
    {
      "epoch": 30.854314112232665,
      "grad_norm": 9.075935363769531,
      "learning_rate": 2.428807157313945e-05,
      "loss": 1.8709,
      "step": 797800
    },
    {
      "epoch": 30.858181536914568,
      "grad_norm": 11.488506317138672,
      "learning_rate": 2.428484871923786e-05,
      "loss": 1.8945,
      "step": 797900
    },
    {
      "epoch": 30.86204896159647,
      "grad_norm": 7.0857672691345215,
      "learning_rate": 2.4281625865336275e-05,
      "loss": 1.8772,
      "step": 798000
    },
    {
      "epoch": 30.86591638627838,
      "grad_norm": 14.479410171508789,
      "learning_rate": 2.4278403011434687e-05,
      "loss": 1.8802,
      "step": 798100
    },
    {
      "epoch": 30.869783810960282,
      "grad_norm": 8.839705467224121,
      "learning_rate": 2.42751801575331e-05,
      "loss": 1.787,
      "step": 798200
    },
    {
      "epoch": 30.873651235642185,
      "grad_norm": 13.176026344299316,
      "learning_rate": 2.4271957303631513e-05,
      "loss": 1.8995,
      "step": 798300
    },
    {
      "epoch": 30.87751866032409,
      "grad_norm": 10.882879257202148,
      "learning_rate": 2.4268734449729927e-05,
      "loss": 1.9055,
      "step": 798400
    },
    {
      "epoch": 30.881386085005996,
      "grad_norm": 14.161328315734863,
      "learning_rate": 2.426551159582834e-05,
      "loss": 1.8366,
      "step": 798500
    },
    {
      "epoch": 30.8852535096879,
      "grad_norm": 11.143725395202637,
      "learning_rate": 2.4262288741926753e-05,
      "loss": 1.9011,
      "step": 798600
    },
    {
      "epoch": 30.889120934369803,
      "grad_norm": 12.378478050231934,
      "learning_rate": 2.4259065888025165e-05,
      "loss": 1.9397,
      "step": 798700
    },
    {
      "epoch": 30.892988359051706,
      "grad_norm": 10.599571228027344,
      "learning_rate": 2.425584303412358e-05,
      "loss": 1.7888,
      "step": 798800
    },
    {
      "epoch": 30.896855783733614,
      "grad_norm": 14.39037799835205,
      "learning_rate": 2.425262018022199e-05,
      "loss": 1.9403,
      "step": 798900
    },
    {
      "epoch": 30.900723208415517,
      "grad_norm": 11.575796127319336,
      "learning_rate": 2.4249397326320406e-05,
      "loss": 1.8336,
      "step": 799000
    },
    {
      "epoch": 30.90459063309742,
      "grad_norm": 10.324549674987793,
      "learning_rate": 2.4246174472418817e-05,
      "loss": 1.8904,
      "step": 799100
    },
    {
      "epoch": 30.908458057779324,
      "grad_norm": 11.398375511169434,
      "learning_rate": 2.424295161851723e-05,
      "loss": 1.8879,
      "step": 799200
    },
    {
      "epoch": 30.912325482461227,
      "grad_norm": 11.175383567810059,
      "learning_rate": 2.4239728764615643e-05,
      "loss": 1.8622,
      "step": 799300
    },
    {
      "epoch": 30.916192907143135,
      "grad_norm": 13.90841293334961,
      "learning_rate": 2.4236505910714058e-05,
      "loss": 1.9468,
      "step": 799400
    },
    {
      "epoch": 30.920060331825038,
      "grad_norm": 11.329973220825195,
      "learning_rate": 2.423328305681247e-05,
      "loss": 1.9621,
      "step": 799500
    },
    {
      "epoch": 30.92392775650694,
      "grad_norm": 11.955321311950684,
      "learning_rate": 2.4230060202910884e-05,
      "loss": 1.8387,
      "step": 799600
    },
    {
      "epoch": 30.927795181188845,
      "grad_norm": 12.21646785736084,
      "learning_rate": 2.4226837349009295e-05,
      "loss": 1.9283,
      "step": 799700
    },
    {
      "epoch": 30.931662605870752,
      "grad_norm": 8.083488464355469,
      "learning_rate": 2.4223614495107706e-05,
      "loss": 1.8642,
      "step": 799800
    },
    {
      "epoch": 30.935530030552655,
      "grad_norm": 16.681249618530273,
      "learning_rate": 2.422039164120612e-05,
      "loss": 1.9217,
      "step": 799900
    },
    {
      "epoch": 30.93939745523456,
      "grad_norm": 12.403644561767578,
      "learning_rate": 2.4217168787304532e-05,
      "loss": 1.8428,
      "step": 800000
    },
    {
      "epoch": 30.943264879916462,
      "grad_norm": 12.555267333984375,
      "learning_rate": 2.4213945933402947e-05,
      "loss": 1.9341,
      "step": 800100
    },
    {
      "epoch": 30.94713230459837,
      "grad_norm": 12.172636032104492,
      "learning_rate": 2.421072307950136e-05,
      "loss": 1.901,
      "step": 800200
    },
    {
      "epoch": 30.950999729280273,
      "grad_norm": 13.223986625671387,
      "learning_rate": 2.4207500225599773e-05,
      "loss": 1.8346,
      "step": 800300
    },
    {
      "epoch": 30.954867153962176,
      "grad_norm": 9.85376262664795,
      "learning_rate": 2.4204277371698188e-05,
      "loss": 1.8301,
      "step": 800400
    },
    {
      "epoch": 30.95873457864408,
      "grad_norm": 10.44340705871582,
      "learning_rate": 2.42010545177966e-05,
      "loss": 1.9008,
      "step": 800500
    },
    {
      "epoch": 30.962602003325983,
      "grad_norm": 12.985119819641113,
      "learning_rate": 2.4197831663895014e-05,
      "loss": 1.8828,
      "step": 800600
    },
    {
      "epoch": 30.96646942800789,
      "grad_norm": 13.970672607421875,
      "learning_rate": 2.419460880999343e-05,
      "loss": 1.828,
      "step": 800700
    },
    {
      "epoch": 30.970336852689794,
      "grad_norm": 9.519853591918945,
      "learning_rate": 2.419138595609184e-05,
      "loss": 1.9234,
      "step": 800800
    },
    {
      "epoch": 30.974204277371697,
      "grad_norm": 14.295814514160156,
      "learning_rate": 2.4188163102190255e-05,
      "loss": 1.8605,
      "step": 800900
    },
    {
      "epoch": 30.9780717020536,
      "grad_norm": 13.834441184997559,
      "learning_rate": 2.4184940248288666e-05,
      "loss": 1.8519,
      "step": 801000
    },
    {
      "epoch": 30.981939126735508,
      "grad_norm": 10.769689559936523,
      "learning_rate": 2.418171739438708e-05,
      "loss": 1.868,
      "step": 801100
    },
    {
      "epoch": 30.98580655141741,
      "grad_norm": 11.343149185180664,
      "learning_rate": 2.4178494540485492e-05,
      "loss": 1.8654,
      "step": 801200
    },
    {
      "epoch": 30.989673976099315,
      "grad_norm": 9.948407173156738,
      "learning_rate": 2.4175271686583907e-05,
      "loss": 1.8662,
      "step": 801300
    },
    {
      "epoch": 30.99354140078122,
      "grad_norm": 12.495434761047363,
      "learning_rate": 2.4172048832682318e-05,
      "loss": 1.9174,
      "step": 801400
    },
    {
      "epoch": 30.997408825463125,
      "grad_norm": 12.9399995803833,
      "learning_rate": 2.4168825978780733e-05,
      "loss": 1.7884,
      "step": 801500
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.7984675168991089,
      "eval_runtime": 2.9297,
      "eval_samples_per_second": 464.551,
      "eval_steps_per_second": 464.551,
      "step": 801567
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.672106146812439,
      "eval_runtime": 55.7535,
      "eval_samples_per_second": 463.774,
      "eval_steps_per_second": 463.774,
      "step": 801567
    },
    {
      "epoch": 31.00127625014503,
      "grad_norm": 14.522664070129395,
      "learning_rate": 2.4165603124879144e-05,
      "loss": 1.8801,
      "step": 801600
    },
    {
      "epoch": 31.005143674826932,
      "grad_norm": 13.829940795898438,
      "learning_rate": 2.416238027097756e-05,
      "loss": 1.8778,
      "step": 801700
    },
    {
      "epoch": 31.009011099508836,
      "grad_norm": 9.925050735473633,
      "learning_rate": 2.415915741707597e-05,
      "loss": 1.8755,
      "step": 801800
    },
    {
      "epoch": 31.012878524190743,
      "grad_norm": 11.898107528686523,
      "learning_rate": 2.4155934563174385e-05,
      "loss": 1.9227,
      "step": 801900
    },
    {
      "epoch": 31.016745948872646,
      "grad_norm": 11.224355697631836,
      "learning_rate": 2.4152711709272796e-05,
      "loss": 1.8537,
      "step": 802000
    },
    {
      "epoch": 31.02061337355455,
      "grad_norm": 14.158147811889648,
      "learning_rate": 2.414948885537121e-05,
      "loss": 1.8134,
      "step": 802100
    },
    {
      "epoch": 31.024480798236453,
      "grad_norm": 9.606160163879395,
      "learning_rate": 2.4146266001469622e-05,
      "loss": 1.8761,
      "step": 802200
    },
    {
      "epoch": 31.028348222918357,
      "grad_norm": 12.067398071289062,
      "learning_rate": 2.4143043147568037e-05,
      "loss": 1.8473,
      "step": 802300
    },
    {
      "epoch": 31.032215647600264,
      "grad_norm": 10.96348762512207,
      "learning_rate": 2.413982029366645e-05,
      "loss": 1.7657,
      "step": 802400
    },
    {
      "epoch": 31.036083072282167,
      "grad_norm": 11.398584365844727,
      "learning_rate": 2.4136597439764863e-05,
      "loss": 1.9005,
      "step": 802500
    },
    {
      "epoch": 31.03995049696407,
      "grad_norm": 9.860628128051758,
      "learning_rate": 2.4133374585863274e-05,
      "loss": 1.8782,
      "step": 802600
    },
    {
      "epoch": 31.043817921645974,
      "grad_norm": 11.93994140625,
      "learning_rate": 2.4130151731961686e-05,
      "loss": 1.908,
      "step": 802700
    },
    {
      "epoch": 31.04768534632788,
      "grad_norm": 9.971210479736328,
      "learning_rate": 2.41269288780601e-05,
      "loss": 1.9051,
      "step": 802800
    },
    {
      "epoch": 31.051552771009785,
      "grad_norm": 10.925921440124512,
      "learning_rate": 2.4123706024158512e-05,
      "loss": 1.8814,
      "step": 802900
    },
    {
      "epoch": 31.05542019569169,
      "grad_norm": 17.640390396118164,
      "learning_rate": 2.4120483170256927e-05,
      "loss": 1.8251,
      "step": 803000
    },
    {
      "epoch": 31.059287620373592,
      "grad_norm": 11.18612289428711,
      "learning_rate": 2.4117260316355338e-05,
      "loss": 1.8489,
      "step": 803100
    },
    {
      "epoch": 31.0631550450555,
      "grad_norm": 12.908428192138672,
      "learning_rate": 2.4114037462453753e-05,
      "loss": 1.8114,
      "step": 803200
    },
    {
      "epoch": 31.067022469737402,
      "grad_norm": 12.807088851928711,
      "learning_rate": 2.4110814608552164e-05,
      "loss": 1.8657,
      "step": 803300
    },
    {
      "epoch": 31.070889894419306,
      "grad_norm": 11.377483367919922,
      "learning_rate": 2.410759175465058e-05,
      "loss": 1.8925,
      "step": 803400
    },
    {
      "epoch": 31.07475731910121,
      "grad_norm": 9.679740905761719,
      "learning_rate": 2.410436890074899e-05,
      "loss": 1.8266,
      "step": 803500
    },
    {
      "epoch": 31.078624743783116,
      "grad_norm": 13.504904747009277,
      "learning_rate": 2.4101146046847405e-05,
      "loss": 1.9023,
      "step": 803600
    },
    {
      "epoch": 31.08249216846502,
      "grad_norm": 11.051215171813965,
      "learning_rate": 2.4097923192945816e-05,
      "loss": 1.8677,
      "step": 803700
    },
    {
      "epoch": 31.086359593146923,
      "grad_norm": 13.325789451599121,
      "learning_rate": 2.409470033904423e-05,
      "loss": 1.8484,
      "step": 803800
    },
    {
      "epoch": 31.090227017828827,
      "grad_norm": 18.230993270874023,
      "learning_rate": 2.4091477485142645e-05,
      "loss": 1.936,
      "step": 803900
    },
    {
      "epoch": 31.09409444251073,
      "grad_norm": 17.847536087036133,
      "learning_rate": 2.4088254631241057e-05,
      "loss": 1.9322,
      "step": 804000
    },
    {
      "epoch": 31.097961867192637,
      "grad_norm": 12.407646179199219,
      "learning_rate": 2.408503177733947e-05,
      "loss": 1.9519,
      "step": 804100
    },
    {
      "epoch": 31.10182929187454,
      "grad_norm": 16.664955139160156,
      "learning_rate": 2.4081808923437886e-05,
      "loss": 1.8137,
      "step": 804200
    },
    {
      "epoch": 31.105696716556444,
      "grad_norm": 14.592790603637695,
      "learning_rate": 2.4078586069536298e-05,
      "loss": 1.858,
      "step": 804300
    },
    {
      "epoch": 31.109564141238348,
      "grad_norm": 11.09090805053711,
      "learning_rate": 2.4075363215634712e-05,
      "loss": 1.9042,
      "step": 804400
    },
    {
      "epoch": 31.113431565920255,
      "grad_norm": 12.037385940551758,
      "learning_rate": 2.4072140361733124e-05,
      "loss": 1.8719,
      "step": 804500
    },
    {
      "epoch": 31.11729899060216,
      "grad_norm": 11.7892427444458,
      "learning_rate": 2.406891750783154e-05,
      "loss": 1.91,
      "step": 804600
    },
    {
      "epoch": 31.121166415284062,
      "grad_norm": 11.05590534210205,
      "learning_rate": 2.406569465392995e-05,
      "loss": 1.7915,
      "step": 804700
    },
    {
      "epoch": 31.125033839965965,
      "grad_norm": 9.354312896728516,
      "learning_rate": 2.4062471800028364e-05,
      "loss": 1.973,
      "step": 804800
    },
    {
      "epoch": 31.128901264647872,
      "grad_norm": 12.364299774169922,
      "learning_rate": 2.4059248946126776e-05,
      "loss": 1.806,
      "step": 804900
    },
    {
      "epoch": 31.132768689329776,
      "grad_norm": 9.420957565307617,
      "learning_rate": 2.405602609222519e-05,
      "loss": 1.8447,
      "step": 805000
    },
    {
      "epoch": 31.13663611401168,
      "grad_norm": 10.15322208404541,
      "learning_rate": 2.4052803238323602e-05,
      "loss": 1.8194,
      "step": 805100
    },
    {
      "epoch": 31.140503538693583,
      "grad_norm": 13.974802017211914,
      "learning_rate": 2.4049580384422016e-05,
      "loss": 1.8069,
      "step": 805200
    },
    {
      "epoch": 31.14437096337549,
      "grad_norm": 12.687832832336426,
      "learning_rate": 2.4046357530520428e-05,
      "loss": 1.8367,
      "step": 805300
    },
    {
      "epoch": 31.148238388057393,
      "grad_norm": 16.008975982666016,
      "learning_rate": 2.4043134676618843e-05,
      "loss": 1.9113,
      "step": 805400
    },
    {
      "epoch": 31.152105812739297,
      "grad_norm": 15.4850492477417,
      "learning_rate": 2.4039911822717254e-05,
      "loss": 1.7882,
      "step": 805500
    },
    {
      "epoch": 31.1559732374212,
      "grad_norm": 13.212099075317383,
      "learning_rate": 2.4036688968815665e-05,
      "loss": 1.9119,
      "step": 805600
    },
    {
      "epoch": 31.159840662103104,
      "grad_norm": 13.394636154174805,
      "learning_rate": 2.403346611491408e-05,
      "loss": 1.8442,
      "step": 805700
    },
    {
      "epoch": 31.16370808678501,
      "grad_norm": 11.276331901550293,
      "learning_rate": 2.403024326101249e-05,
      "loss": 1.8923,
      "step": 805800
    },
    {
      "epoch": 31.167575511466914,
      "grad_norm": 14.211122512817383,
      "learning_rate": 2.4027020407110906e-05,
      "loss": 1.8762,
      "step": 805900
    },
    {
      "epoch": 31.171442936148818,
      "grad_norm": 10.489800453186035,
      "learning_rate": 2.4023797553209317e-05,
      "loss": 1.7524,
      "step": 806000
    },
    {
      "epoch": 31.17531036083072,
      "grad_norm": 11.920512199401855,
      "learning_rate": 2.4020574699307732e-05,
      "loss": 1.845,
      "step": 806100
    },
    {
      "epoch": 31.17917778551263,
      "grad_norm": 11.86905288696289,
      "learning_rate": 2.4017351845406143e-05,
      "loss": 1.8451,
      "step": 806200
    },
    {
      "epoch": 31.183045210194532,
      "grad_norm": 12.299144744873047,
      "learning_rate": 2.4014128991504558e-05,
      "loss": 1.8941,
      "step": 806300
    },
    {
      "epoch": 31.186912634876435,
      "grad_norm": 12.772418022155762,
      "learning_rate": 2.401090613760297e-05,
      "loss": 1.8881,
      "step": 806400
    },
    {
      "epoch": 31.19078005955834,
      "grad_norm": 11.61970329284668,
      "learning_rate": 2.4007683283701384e-05,
      "loss": 1.9139,
      "step": 806500
    },
    {
      "epoch": 31.194647484240246,
      "grad_norm": 15.096035957336426,
      "learning_rate": 2.4004460429799795e-05,
      "loss": 1.9138,
      "step": 806600
    },
    {
      "epoch": 31.19851490892215,
      "grad_norm": 12.474760055541992,
      "learning_rate": 2.400123757589821e-05,
      "loss": 1.8456,
      "step": 806700
    },
    {
      "epoch": 31.202382333604053,
      "grad_norm": 10.748876571655273,
      "learning_rate": 2.399801472199662e-05,
      "loss": 1.7618,
      "step": 806800
    },
    {
      "epoch": 31.206249758285956,
      "grad_norm": 12.502808570861816,
      "learning_rate": 2.3994791868095036e-05,
      "loss": 1.8501,
      "step": 806900
    },
    {
      "epoch": 31.210117182967863,
      "grad_norm": 9.498605728149414,
      "learning_rate": 2.3991569014193448e-05,
      "loss": 1.8701,
      "step": 807000
    },
    {
      "epoch": 31.213984607649767,
      "grad_norm": 11.65489673614502,
      "learning_rate": 2.3988346160291862e-05,
      "loss": 1.854,
      "step": 807100
    },
    {
      "epoch": 31.21785203233167,
      "grad_norm": 10.563302040100098,
      "learning_rate": 2.3985123306390277e-05,
      "loss": 1.938,
      "step": 807200
    },
    {
      "epoch": 31.221719457013574,
      "grad_norm": 11.854363441467285,
      "learning_rate": 2.3981900452488688e-05,
      "loss": 1.8561,
      "step": 807300
    },
    {
      "epoch": 31.225586881695477,
      "grad_norm": 12.615581512451172,
      "learning_rate": 2.3978677598587103e-05,
      "loss": 1.8282,
      "step": 807400
    },
    {
      "epoch": 31.229454306377384,
      "grad_norm": 10.385832786560059,
      "learning_rate": 2.3975454744685514e-05,
      "loss": 1.8941,
      "step": 807500
    },
    {
      "epoch": 31.233321731059288,
      "grad_norm": 11.078028678894043,
      "learning_rate": 2.397223189078393e-05,
      "loss": 1.9241,
      "step": 807600
    },
    {
      "epoch": 31.23718915574119,
      "grad_norm": 13.79161548614502,
      "learning_rate": 2.3969009036882344e-05,
      "loss": 1.9138,
      "step": 807700
    },
    {
      "epoch": 31.241056580423095,
      "grad_norm": 11.477804183959961,
      "learning_rate": 2.3965786182980755e-05,
      "loss": 1.9295,
      "step": 807800
    },
    {
      "epoch": 31.244924005105002,
      "grad_norm": 10.787004470825195,
      "learning_rate": 2.396256332907917e-05,
      "loss": 1.8365,
      "step": 807900
    },
    {
      "epoch": 31.248791429786905,
      "grad_norm": 9.923709869384766,
      "learning_rate": 2.395934047517758e-05,
      "loss": 1.8113,
      "step": 808000
    },
    {
      "epoch": 31.25265885446881,
      "grad_norm": 9.487430572509766,
      "learning_rate": 2.3956117621275996e-05,
      "loss": 1.8903,
      "step": 808100
    },
    {
      "epoch": 31.256526279150712,
      "grad_norm": 14.223031044006348,
      "learning_rate": 2.3952894767374407e-05,
      "loss": 1.9076,
      "step": 808200
    },
    {
      "epoch": 31.26039370383262,
      "grad_norm": 11.282275199890137,
      "learning_rate": 2.3949671913472822e-05,
      "loss": 1.8518,
      "step": 808300
    },
    {
      "epoch": 31.264261128514523,
      "grad_norm": 12.022457122802734,
      "learning_rate": 2.3946449059571233e-05,
      "loss": 1.9291,
      "step": 808400
    },
    {
      "epoch": 31.268128553196426,
      "grad_norm": 12.076225280761719,
      "learning_rate": 2.3943226205669645e-05,
      "loss": 1.8693,
      "step": 808500
    },
    {
      "epoch": 31.27199597787833,
      "grad_norm": 11.959653854370117,
      "learning_rate": 2.394000335176806e-05,
      "loss": 1.8738,
      "step": 808600
    },
    {
      "epoch": 31.275863402560233,
      "grad_norm": 13.193835258483887,
      "learning_rate": 2.393678049786647e-05,
      "loss": 1.8814,
      "step": 808700
    },
    {
      "epoch": 31.27973082724214,
      "grad_norm": 9.36750316619873,
      "learning_rate": 2.3933557643964885e-05,
      "loss": 1.8848,
      "step": 808800
    },
    {
      "epoch": 31.283598251924044,
      "grad_norm": 13.44739055633545,
      "learning_rate": 2.3930334790063297e-05,
      "loss": 1.8089,
      "step": 808900
    },
    {
      "epoch": 31.287465676605947,
      "grad_norm": 12.84676742553711,
      "learning_rate": 2.392711193616171e-05,
      "loss": 1.905,
      "step": 809000
    },
    {
      "epoch": 31.29133310128785,
      "grad_norm": 10.717803001403809,
      "learning_rate": 2.3923889082260123e-05,
      "loss": 1.9044,
      "step": 809100
    },
    {
      "epoch": 31.295200525969758,
      "grad_norm": 11.497678756713867,
      "learning_rate": 2.3920666228358537e-05,
      "loss": 1.9507,
      "step": 809200
    },
    {
      "epoch": 31.29906795065166,
      "grad_norm": 12.757885932922363,
      "learning_rate": 2.391744337445695e-05,
      "loss": 1.8354,
      "step": 809300
    },
    {
      "epoch": 31.302935375333565,
      "grad_norm": 13.171992301940918,
      "learning_rate": 2.3914220520555363e-05,
      "loss": 1.9403,
      "step": 809400
    },
    {
      "epoch": 31.30680280001547,
      "grad_norm": 9.728242874145508,
      "learning_rate": 2.3910997666653775e-05,
      "loss": 1.8296,
      "step": 809500
    },
    {
      "epoch": 31.310670224697375,
      "grad_norm": 16.98564338684082,
      "learning_rate": 2.390777481275219e-05,
      "loss": 1.9312,
      "step": 809600
    },
    {
      "epoch": 31.31453764937928,
      "grad_norm": 10.522308349609375,
      "learning_rate": 2.39045519588506e-05,
      "loss": 1.8276,
      "step": 809700
    },
    {
      "epoch": 31.318405074061182,
      "grad_norm": 10.239575386047363,
      "learning_rate": 2.3901329104949016e-05,
      "loss": 1.8346,
      "step": 809800
    },
    {
      "epoch": 31.322272498743086,
      "grad_norm": 12.339566230773926,
      "learning_rate": 2.3898106251047427e-05,
      "loss": 1.8918,
      "step": 809900
    },
    {
      "epoch": 31.326139923424993,
      "grad_norm": 9.2813720703125,
      "learning_rate": 2.389488339714584e-05,
      "loss": 1.8854,
      "step": 810000
    },
    {
      "epoch": 31.330007348106896,
      "grad_norm": 15.60693359375,
      "learning_rate": 2.3891660543244253e-05,
      "loss": 1.9718,
      "step": 810100
    },
    {
      "epoch": 31.3338747727888,
      "grad_norm": 10.694146156311035,
      "learning_rate": 2.3888437689342668e-05,
      "loss": 1.9007,
      "step": 810200
    },
    {
      "epoch": 31.337742197470703,
      "grad_norm": 15.41318416595459,
      "learning_rate": 2.388521483544108e-05,
      "loss": 1.8756,
      "step": 810300
    },
    {
      "epoch": 31.34160962215261,
      "grad_norm": 12.009604454040527,
      "learning_rate": 2.3881991981539494e-05,
      "loss": 1.8308,
      "step": 810400
    },
    {
      "epoch": 31.345477046834514,
      "grad_norm": 11.727923393249512,
      "learning_rate": 2.3878769127637905e-05,
      "loss": 1.8156,
      "step": 810500
    },
    {
      "epoch": 31.349344471516417,
      "grad_norm": 13.749309539794922,
      "learning_rate": 2.387554627373632e-05,
      "loss": 1.7944,
      "step": 810600
    },
    {
      "epoch": 31.35321189619832,
      "grad_norm": 10.848372459411621,
      "learning_rate": 2.3872323419834735e-05,
      "loss": 1.8795,
      "step": 810700
    },
    {
      "epoch": 31.357079320880224,
      "grad_norm": 12.658764839172363,
      "learning_rate": 2.3869100565933146e-05,
      "loss": 1.8671,
      "step": 810800
    },
    {
      "epoch": 31.36094674556213,
      "grad_norm": 9.829392433166504,
      "learning_rate": 2.386587771203156e-05,
      "loss": 1.8856,
      "step": 810900
    },
    {
      "epoch": 31.364814170244035,
      "grad_norm": 12.947545051574707,
      "learning_rate": 2.3862654858129975e-05,
      "loss": 1.8324,
      "step": 811000
    },
    {
      "epoch": 31.36868159492594,
      "grad_norm": 14.22965145111084,
      "learning_rate": 2.3859432004228387e-05,
      "loss": 1.8009,
      "step": 811100
    },
    {
      "epoch": 31.372549019607842,
      "grad_norm": 9.56387710571289,
      "learning_rate": 2.38562091503268e-05,
      "loss": 1.9424,
      "step": 811200
    },
    {
      "epoch": 31.37641644428975,
      "grad_norm": 12.10966968536377,
      "learning_rate": 2.3852986296425213e-05,
      "loss": 1.877,
      "step": 811300
    },
    {
      "epoch": 31.380283868971652,
      "grad_norm": 13.560079574584961,
      "learning_rate": 2.3849763442523627e-05,
      "loss": 1.8874,
      "step": 811400
    },
    {
      "epoch": 31.384151293653556,
      "grad_norm": 9.534876823425293,
      "learning_rate": 2.384654058862204e-05,
      "loss": 1.8331,
      "step": 811500
    },
    {
      "epoch": 31.38801871833546,
      "grad_norm": 12.05664348602295,
      "learning_rate": 2.384331773472045e-05,
      "loss": 1.87,
      "step": 811600
    },
    {
      "epoch": 31.391886143017366,
      "grad_norm": 12.9849853515625,
      "learning_rate": 2.3840094880818865e-05,
      "loss": 1.8277,
      "step": 811700
    },
    {
      "epoch": 31.39575356769927,
      "grad_norm": 12.258965492248535,
      "learning_rate": 2.3836872026917276e-05,
      "loss": 1.8955,
      "step": 811800
    },
    {
      "epoch": 31.399620992381173,
      "grad_norm": 9.181825637817383,
      "learning_rate": 2.383364917301569e-05,
      "loss": 1.9789,
      "step": 811900
    },
    {
      "epoch": 31.403488417063077,
      "grad_norm": 10.756048202514648,
      "learning_rate": 2.3830426319114102e-05,
      "loss": 1.8859,
      "step": 812000
    },
    {
      "epoch": 31.40735584174498,
      "grad_norm": 7.892699241638184,
      "learning_rate": 2.3827203465212517e-05,
      "loss": 1.8968,
      "step": 812100
    },
    {
      "epoch": 31.411223266426887,
      "grad_norm": 12.898612976074219,
      "learning_rate": 2.3823980611310928e-05,
      "loss": 1.9061,
      "step": 812200
    },
    {
      "epoch": 31.41509069110879,
      "grad_norm": 11.502510070800781,
      "learning_rate": 2.3820757757409343e-05,
      "loss": 1.8828,
      "step": 812300
    },
    {
      "epoch": 31.418958115790694,
      "grad_norm": 15.396576881408691,
      "learning_rate": 2.3817534903507754e-05,
      "loss": 1.8578,
      "step": 812400
    },
    {
      "epoch": 31.422825540472598,
      "grad_norm": 16.959230422973633,
      "learning_rate": 2.381431204960617e-05,
      "loss": 1.8828,
      "step": 812500
    },
    {
      "epoch": 31.426692965154505,
      "grad_norm": 14.953933715820312,
      "learning_rate": 2.381108919570458e-05,
      "loss": 1.9604,
      "step": 812600
    },
    {
      "epoch": 31.43056038983641,
      "grad_norm": 15.152195930480957,
      "learning_rate": 2.3807866341802995e-05,
      "loss": 1.859,
      "step": 812700
    },
    {
      "epoch": 31.434427814518312,
      "grad_norm": 9.944464683532715,
      "learning_rate": 2.3804643487901406e-05,
      "loss": 1.8608,
      "step": 812800
    },
    {
      "epoch": 31.438295239200215,
      "grad_norm": 9.445880889892578,
      "learning_rate": 2.380142063399982e-05,
      "loss": 1.8561,
      "step": 812900
    },
    {
      "epoch": 31.442162663882122,
      "grad_norm": 12.818411827087402,
      "learning_rate": 2.3798197780098232e-05,
      "loss": 1.8192,
      "step": 813000
    },
    {
      "epoch": 31.446030088564026,
      "grad_norm": 13.927213668823242,
      "learning_rate": 2.3794974926196647e-05,
      "loss": 1.9332,
      "step": 813100
    },
    {
      "epoch": 31.44989751324593,
      "grad_norm": 10.39979362487793,
      "learning_rate": 2.379175207229506e-05,
      "loss": 1.786,
      "step": 813200
    },
    {
      "epoch": 31.453764937927833,
      "grad_norm": 11.957233428955078,
      "learning_rate": 2.3788529218393473e-05,
      "loss": 1.894,
      "step": 813300
    },
    {
      "epoch": 31.45763236260974,
      "grad_norm": 11.253726959228516,
      "learning_rate": 2.3785306364491884e-05,
      "loss": 1.8929,
      "step": 813400
    },
    {
      "epoch": 31.461499787291643,
      "grad_norm": 10.07193374633789,
      "learning_rate": 2.37820835105903e-05,
      "loss": 1.8909,
      "step": 813500
    },
    {
      "epoch": 31.465367211973547,
      "grad_norm": 10.579767227172852,
      "learning_rate": 2.377886065668871e-05,
      "loss": 1.8645,
      "step": 813600
    },
    {
      "epoch": 31.46923463665545,
      "grad_norm": 12.916707992553711,
      "learning_rate": 2.3775637802787125e-05,
      "loss": 1.7881,
      "step": 813700
    },
    {
      "epoch": 31.473102061337354,
      "grad_norm": 10.654026985168457,
      "learning_rate": 2.3772414948885537e-05,
      "loss": 1.8513,
      "step": 813800
    },
    {
      "epoch": 31.47696948601926,
      "grad_norm": 12.154528617858887,
      "learning_rate": 2.376919209498395e-05,
      "loss": 1.8428,
      "step": 813900
    },
    {
      "epoch": 31.480836910701164,
      "grad_norm": 11.482630729675293,
      "learning_rate": 2.3765969241082363e-05,
      "loss": 1.8833,
      "step": 814000
    },
    {
      "epoch": 31.484704335383068,
      "grad_norm": 12.04455280303955,
      "learning_rate": 2.3762746387180777e-05,
      "loss": 1.8378,
      "step": 814100
    },
    {
      "epoch": 31.48857176006497,
      "grad_norm": 13.259551048278809,
      "learning_rate": 2.3759523533279192e-05,
      "loss": 1.909,
      "step": 814200
    },
    {
      "epoch": 31.49243918474688,
      "grad_norm": 9.874835014343262,
      "learning_rate": 2.3756300679377603e-05,
      "loss": 1.8204,
      "step": 814300
    },
    {
      "epoch": 31.496306609428782,
      "grad_norm": 10.024530410766602,
      "learning_rate": 2.3753077825476018e-05,
      "loss": 1.8891,
      "step": 814400
    },
    {
      "epoch": 31.500174034110685,
      "grad_norm": 14.685810089111328,
      "learning_rate": 2.374985497157443e-05,
      "loss": 1.9347,
      "step": 814500
    },
    {
      "epoch": 31.50404145879259,
      "grad_norm": 12.159570693969727,
      "learning_rate": 2.3746632117672844e-05,
      "loss": 1.9289,
      "step": 814600
    },
    {
      "epoch": 31.507908883474496,
      "grad_norm": 13.78617000579834,
      "learning_rate": 2.3743409263771255e-05,
      "loss": 1.8121,
      "step": 814700
    },
    {
      "epoch": 31.5117763081564,
      "grad_norm": 15.119437217712402,
      "learning_rate": 2.374018640986967e-05,
      "loss": 1.8521,
      "step": 814800
    },
    {
      "epoch": 31.515643732838303,
      "grad_norm": 11.514050483703613,
      "learning_rate": 2.373696355596808e-05,
      "loss": 1.9133,
      "step": 814900
    },
    {
      "epoch": 31.519511157520206,
      "grad_norm": 10.512372970581055,
      "learning_rate": 2.3733740702066496e-05,
      "loss": 1.9048,
      "step": 815000
    },
    {
      "epoch": 31.523378582202113,
      "grad_norm": 10.196155548095703,
      "learning_rate": 2.3730517848164908e-05,
      "loss": 2.0224,
      "step": 815100
    },
    {
      "epoch": 31.527246006884017,
      "grad_norm": 9.811715126037598,
      "learning_rate": 2.3727294994263322e-05,
      "loss": 1.7981,
      "step": 815200
    },
    {
      "epoch": 31.53111343156592,
      "grad_norm": 8.144623756408691,
      "learning_rate": 2.3724072140361734e-05,
      "loss": 1.8581,
      "step": 815300
    },
    {
      "epoch": 31.534980856247824,
      "grad_norm": 9.809699058532715,
      "learning_rate": 2.372084928646015e-05,
      "loss": 1.8323,
      "step": 815400
    },
    {
      "epoch": 31.538848280929727,
      "grad_norm": 12.996225357055664,
      "learning_rate": 2.371762643255856e-05,
      "loss": 1.8509,
      "step": 815500
    },
    {
      "epoch": 31.542715705611634,
      "grad_norm": 7.468374729156494,
      "learning_rate": 2.3714403578656974e-05,
      "loss": 1.7787,
      "step": 815600
    },
    {
      "epoch": 31.546583130293538,
      "grad_norm": 11.682581901550293,
      "learning_rate": 2.3711180724755386e-05,
      "loss": 1.9543,
      "step": 815700
    },
    {
      "epoch": 31.55045055497544,
      "grad_norm": 13.525092124938965,
      "learning_rate": 2.37079578708538e-05,
      "loss": 1.8133,
      "step": 815800
    },
    {
      "epoch": 31.554317979657345,
      "grad_norm": 14.921712875366211,
      "learning_rate": 2.3704735016952212e-05,
      "loss": 1.902,
      "step": 815900
    },
    {
      "epoch": 31.558185404339252,
      "grad_norm": 14.482645034790039,
      "learning_rate": 2.3701512163050626e-05,
      "loss": 1.8991,
      "step": 816000
    },
    {
      "epoch": 31.562052829021155,
      "grad_norm": 12.461037635803223,
      "learning_rate": 2.3698289309149038e-05,
      "loss": 1.8467,
      "step": 816100
    },
    {
      "epoch": 31.56592025370306,
      "grad_norm": 11.16553783416748,
      "learning_rate": 2.3695066455247453e-05,
      "loss": 1.8828,
      "step": 816200
    },
    {
      "epoch": 31.569787678384962,
      "grad_norm": 13.653789520263672,
      "learning_rate": 2.3691843601345864e-05,
      "loss": 1.8962,
      "step": 816300
    },
    {
      "epoch": 31.57365510306687,
      "grad_norm": 10.125777244567871,
      "learning_rate": 2.368862074744428e-05,
      "loss": 1.8612,
      "step": 816400
    },
    {
      "epoch": 31.577522527748773,
      "grad_norm": 11.424972534179688,
      "learning_rate": 2.368539789354269e-05,
      "loss": 1.9271,
      "step": 816500
    },
    {
      "epoch": 31.581389952430676,
      "grad_norm": 10.872617721557617,
      "learning_rate": 2.3682175039641105e-05,
      "loss": 1.7827,
      "step": 816600
    },
    {
      "epoch": 31.58525737711258,
      "grad_norm": 13.45716381072998,
      "learning_rate": 2.3678952185739516e-05,
      "loss": 1.8783,
      "step": 816700
    },
    {
      "epoch": 31.589124801794483,
      "grad_norm": 16.437850952148438,
      "learning_rate": 2.367572933183793e-05,
      "loss": 1.8834,
      "step": 816800
    },
    {
      "epoch": 31.59299222647639,
      "grad_norm": 15.864373207092285,
      "learning_rate": 2.3672506477936342e-05,
      "loss": 1.9785,
      "step": 816900
    },
    {
      "epoch": 31.596859651158294,
      "grad_norm": 14.87672233581543,
      "learning_rate": 2.3669283624034757e-05,
      "loss": 1.8144,
      "step": 817000
    },
    {
      "epoch": 31.600727075840197,
      "grad_norm": 11.39738655090332,
      "learning_rate": 2.3666060770133168e-05,
      "loss": 1.8918,
      "step": 817100
    },
    {
      "epoch": 31.6045945005221,
      "grad_norm": 7.4808549880981445,
      "learning_rate": 2.3662837916231583e-05,
      "loss": 1.8515,
      "step": 817200
    },
    {
      "epoch": 31.608461925204008,
      "grad_norm": 11.922178268432617,
      "learning_rate": 2.3659615062329994e-05,
      "loss": 1.8979,
      "step": 817300
    },
    {
      "epoch": 31.61232934988591,
      "grad_norm": 11.318243026733398,
      "learning_rate": 2.365639220842841e-05,
      "loss": 1.8646,
      "step": 817400
    },
    {
      "epoch": 31.616196774567815,
      "grad_norm": 14.831446647644043,
      "learning_rate": 2.365316935452682e-05,
      "loss": 1.8795,
      "step": 817500
    },
    {
      "epoch": 31.62006419924972,
      "grad_norm": 13.46158218383789,
      "learning_rate": 2.3649946500625235e-05,
      "loss": 1.9065,
      "step": 817600
    },
    {
      "epoch": 31.623931623931625,
      "grad_norm": 10.040578842163086,
      "learning_rate": 2.364672364672365e-05,
      "loss": 1.9228,
      "step": 817700
    },
    {
      "epoch": 31.62779904861353,
      "grad_norm": 10.416691780090332,
      "learning_rate": 2.364350079282206e-05,
      "loss": 1.8632,
      "step": 817800
    },
    {
      "epoch": 31.631666473295432,
      "grad_norm": 12.329117774963379,
      "learning_rate": 2.3640277938920476e-05,
      "loss": 1.9912,
      "step": 817900
    },
    {
      "epoch": 31.635533897977336,
      "grad_norm": 9.428499221801758,
      "learning_rate": 2.3637055085018887e-05,
      "loss": 1.9132,
      "step": 818000
    },
    {
      "epoch": 31.639401322659243,
      "grad_norm": 15.120638847351074,
      "learning_rate": 2.36338322311173e-05,
      "loss": 1.8927,
      "step": 818100
    },
    {
      "epoch": 31.643268747341146,
      "grad_norm": 11.21461009979248,
      "learning_rate": 2.3630609377215713e-05,
      "loss": 1.8125,
      "step": 818200
    },
    {
      "epoch": 31.64713617202305,
      "grad_norm": 11.741189956665039,
      "learning_rate": 2.3627386523314128e-05,
      "loss": 1.8387,
      "step": 818300
    },
    {
      "epoch": 31.651003596704953,
      "grad_norm": 12.197957038879395,
      "learning_rate": 2.362416366941254e-05,
      "loss": 1.8184,
      "step": 818400
    },
    {
      "epoch": 31.65487102138686,
      "grad_norm": 10.296464920043945,
      "learning_rate": 2.3620940815510954e-05,
      "loss": 1.8103,
      "step": 818500
    },
    {
      "epoch": 31.658738446068764,
      "grad_norm": 12.629805564880371,
      "learning_rate": 2.3617717961609365e-05,
      "loss": 1.928,
      "step": 818600
    },
    {
      "epoch": 31.662605870750667,
      "grad_norm": 13.855791091918945,
      "learning_rate": 2.361449510770778e-05,
      "loss": 1.9626,
      "step": 818700
    },
    {
      "epoch": 31.66647329543257,
      "grad_norm": 11.47337532043457,
      "learning_rate": 2.361127225380619e-05,
      "loss": 1.9309,
      "step": 818800
    },
    {
      "epoch": 31.670340720114474,
      "grad_norm": 12.382716178894043,
      "learning_rate": 2.3608049399904606e-05,
      "loss": 1.895,
      "step": 818900
    },
    {
      "epoch": 31.67420814479638,
      "grad_norm": 10.907453536987305,
      "learning_rate": 2.3604826546003017e-05,
      "loss": 1.7674,
      "step": 819000
    },
    {
      "epoch": 31.678075569478285,
      "grad_norm": 9.576443672180176,
      "learning_rate": 2.3601603692101432e-05,
      "loss": 1.9125,
      "step": 819100
    },
    {
      "epoch": 31.681942994160188,
      "grad_norm": 10.435236930847168,
      "learning_rate": 2.3598380838199843e-05,
      "loss": 1.8789,
      "step": 819200
    },
    {
      "epoch": 31.68581041884209,
      "grad_norm": 12.95036506652832,
      "learning_rate": 2.3595157984298258e-05,
      "loss": 1.8296,
      "step": 819300
    },
    {
      "epoch": 31.689677843524,
      "grad_norm": 12.131550788879395,
      "learning_rate": 2.359193513039667e-05,
      "loss": 1.8756,
      "step": 819400
    },
    {
      "epoch": 31.693545268205902,
      "grad_norm": 17.124441146850586,
      "learning_rate": 2.3588712276495084e-05,
      "loss": 1.872,
      "step": 819500
    },
    {
      "epoch": 31.697412692887806,
      "grad_norm": 12.001486778259277,
      "learning_rate": 2.3585489422593495e-05,
      "loss": 1.8814,
      "step": 819600
    },
    {
      "epoch": 31.70128011756971,
      "grad_norm": 11.548203468322754,
      "learning_rate": 2.358226656869191e-05,
      "loss": 1.8468,
      "step": 819700
    },
    {
      "epoch": 31.705147542251616,
      "grad_norm": 8.548239707946777,
      "learning_rate": 2.357904371479032e-05,
      "loss": 1.8734,
      "step": 819800
    },
    {
      "epoch": 31.70901496693352,
      "grad_norm": 9.069168090820312,
      "learning_rate": 2.3575820860888736e-05,
      "loss": 1.8631,
      "step": 819900
    },
    {
      "epoch": 31.712882391615423,
      "grad_norm": 10.018830299377441,
      "learning_rate": 2.3572598006987147e-05,
      "loss": 1.8367,
      "step": 820000
    },
    {
      "epoch": 31.716749816297327,
      "grad_norm": 12.226893424987793,
      "learning_rate": 2.3569375153085562e-05,
      "loss": 1.8417,
      "step": 820100
    },
    {
      "epoch": 31.72061724097923,
      "grad_norm": 10.775609970092773,
      "learning_rate": 2.3566152299183974e-05,
      "loss": 1.9292,
      "step": 820200
    },
    {
      "epoch": 31.724484665661137,
      "grad_norm": 9.191611289978027,
      "learning_rate": 2.3562929445282385e-05,
      "loss": 1.8092,
      "step": 820300
    },
    {
      "epoch": 31.72835209034304,
      "grad_norm": 11.403095245361328,
      "learning_rate": 2.35597065913808e-05,
      "loss": 1.7766,
      "step": 820400
    },
    {
      "epoch": 31.732219515024944,
      "grad_norm": 11.288564682006836,
      "learning_rate": 2.355648373747921e-05,
      "loss": 1.8398,
      "step": 820500
    },
    {
      "epoch": 31.736086939706848,
      "grad_norm": 13.09632396697998,
      "learning_rate": 2.3553260883577626e-05,
      "loss": 1.9486,
      "step": 820600
    },
    {
      "epoch": 31.739954364388755,
      "grad_norm": 10.071894645690918,
      "learning_rate": 2.355003802967604e-05,
      "loss": 1.8026,
      "step": 820700
    },
    {
      "epoch": 31.743821789070658,
      "grad_norm": 8.891894340515137,
      "learning_rate": 2.354681517577445e-05,
      "loss": 1.8956,
      "step": 820800
    },
    {
      "epoch": 31.74768921375256,
      "grad_norm": 10.401684761047363,
      "learning_rate": 2.3543592321872866e-05,
      "loss": 1.8452,
      "step": 820900
    },
    {
      "epoch": 31.751556638434465,
      "grad_norm": 13.358853340148926,
      "learning_rate": 2.354036946797128e-05,
      "loss": 1.8722,
      "step": 821000
    },
    {
      "epoch": 31.755424063116372,
      "grad_norm": 12.53187084197998,
      "learning_rate": 2.3537146614069692e-05,
      "loss": 1.8845,
      "step": 821100
    },
    {
      "epoch": 31.759291487798276,
      "grad_norm": 13.598798751831055,
      "learning_rate": 2.3533923760168107e-05,
      "loss": 1.8555,
      "step": 821200
    },
    {
      "epoch": 31.76315891248018,
      "grad_norm": 11.633246421813965,
      "learning_rate": 2.353070090626652e-05,
      "loss": 1.9607,
      "step": 821300
    },
    {
      "epoch": 31.767026337162083,
      "grad_norm": 12.220755577087402,
      "learning_rate": 2.3527478052364933e-05,
      "loss": 1.8208,
      "step": 821400
    },
    {
      "epoch": 31.77089376184399,
      "grad_norm": 9.236249923706055,
      "learning_rate": 2.3524255198463345e-05,
      "loss": 1.8586,
      "step": 821500
    },
    {
      "epoch": 31.774761186525893,
      "grad_norm": 10.353845596313477,
      "learning_rate": 2.352103234456176e-05,
      "loss": 1.8019,
      "step": 821600
    },
    {
      "epoch": 31.778628611207797,
      "grad_norm": 11.295140266418457,
      "learning_rate": 2.351780949066017e-05,
      "loss": 1.8716,
      "step": 821700
    },
    {
      "epoch": 31.7824960358897,
      "grad_norm": 12.038768768310547,
      "learning_rate": 2.3514586636758585e-05,
      "loss": 1.8432,
      "step": 821800
    },
    {
      "epoch": 31.786363460571604,
      "grad_norm": 14.040389060974121,
      "learning_rate": 2.3511363782856997e-05,
      "loss": 1.8915,
      "step": 821900
    },
    {
      "epoch": 31.79023088525351,
      "grad_norm": 11.11733627319336,
      "learning_rate": 2.350814092895541e-05,
      "loss": 1.89,
      "step": 822000
    },
    {
      "epoch": 31.794098309935414,
      "grad_norm": 16.39969825744629,
      "learning_rate": 2.3504918075053823e-05,
      "loss": 1.9495,
      "step": 822100
    },
    {
      "epoch": 31.797965734617318,
      "grad_norm": 9.832734107971191,
      "learning_rate": 2.3501695221152237e-05,
      "loss": 1.8515,
      "step": 822200
    },
    {
      "epoch": 31.80183315929922,
      "grad_norm": 11.143571853637695,
      "learning_rate": 2.349847236725065e-05,
      "loss": 1.9508,
      "step": 822300
    },
    {
      "epoch": 31.805700583981128,
      "grad_norm": 10.403650283813477,
      "learning_rate": 2.3495249513349063e-05,
      "loss": 1.8144,
      "step": 822400
    },
    {
      "epoch": 31.80956800866303,
      "grad_norm": 10.09223461151123,
      "learning_rate": 2.3492026659447475e-05,
      "loss": 1.7958,
      "step": 822500
    },
    {
      "epoch": 31.813435433344935,
      "grad_norm": 7.776689052581787,
      "learning_rate": 2.348880380554589e-05,
      "loss": 1.8828,
      "step": 822600
    },
    {
      "epoch": 31.81730285802684,
      "grad_norm": 13.28451919555664,
      "learning_rate": 2.34855809516443e-05,
      "loss": 1.9568,
      "step": 822700
    },
    {
      "epoch": 31.821170282708746,
      "grad_norm": 12.163119316101074,
      "learning_rate": 2.3482358097742716e-05,
      "loss": 1.8711,
      "step": 822800
    },
    {
      "epoch": 31.82503770739065,
      "grad_norm": 18.22934913635254,
      "learning_rate": 2.3479135243841127e-05,
      "loss": 1.8553,
      "step": 822900
    },
    {
      "epoch": 31.828905132072553,
      "grad_norm": 11.223222732543945,
      "learning_rate": 2.347591238993954e-05,
      "loss": 1.8417,
      "step": 823000
    },
    {
      "epoch": 31.832772556754456,
      "grad_norm": 11.019709587097168,
      "learning_rate": 2.3472689536037953e-05,
      "loss": 1.8363,
      "step": 823100
    },
    {
      "epoch": 31.836639981436363,
      "grad_norm": 12.559340476989746,
      "learning_rate": 2.3469466682136368e-05,
      "loss": 1.8566,
      "step": 823200
    },
    {
      "epoch": 31.840507406118267,
      "grad_norm": 10.325337409973145,
      "learning_rate": 2.346624382823478e-05,
      "loss": 1.9954,
      "step": 823300
    },
    {
      "epoch": 31.84437483080017,
      "grad_norm": 10.051192283630371,
      "learning_rate": 2.346302097433319e-05,
      "loss": 1.8684,
      "step": 823400
    },
    {
      "epoch": 31.848242255482074,
      "grad_norm": 10.704861640930176,
      "learning_rate": 2.3459798120431605e-05,
      "loss": 1.9262,
      "step": 823500
    },
    {
      "epoch": 31.852109680163977,
      "grad_norm": 11.17425537109375,
      "learning_rate": 2.3456575266530016e-05,
      "loss": 1.7693,
      "step": 823600
    },
    {
      "epoch": 31.855977104845884,
      "grad_norm": 12.09885025024414,
      "learning_rate": 2.345335241262843e-05,
      "loss": 1.8742,
      "step": 823700
    },
    {
      "epoch": 31.859844529527788,
      "grad_norm": 9.708191871643066,
      "learning_rate": 2.3450129558726842e-05,
      "loss": 1.9949,
      "step": 823800
    },
    {
      "epoch": 31.86371195420969,
      "grad_norm": 11.062630653381348,
      "learning_rate": 2.3446906704825257e-05,
      "loss": 1.8581,
      "step": 823900
    },
    {
      "epoch": 31.867579378891595,
      "grad_norm": 9.747878074645996,
      "learning_rate": 2.344368385092367e-05,
      "loss": 1.9007,
      "step": 824000
    },
    {
      "epoch": 31.8714468035735,
      "grad_norm": 12.580127716064453,
      "learning_rate": 2.3440460997022083e-05,
      "loss": 1.8711,
      "step": 824100
    },
    {
      "epoch": 31.875314228255405,
      "grad_norm": 13.484322547912598,
      "learning_rate": 2.3437238143120498e-05,
      "loss": 1.859,
      "step": 824200
    },
    {
      "epoch": 31.87918165293731,
      "grad_norm": 13.658970832824707,
      "learning_rate": 2.343401528921891e-05,
      "loss": 1.8466,
      "step": 824300
    },
    {
      "epoch": 31.883049077619212,
      "grad_norm": 8.7711181640625,
      "learning_rate": 2.3430792435317324e-05,
      "loss": 1.8924,
      "step": 824400
    },
    {
      "epoch": 31.88691650230112,
      "grad_norm": 14.070721626281738,
      "learning_rate": 2.342756958141574e-05,
      "loss": 1.8985,
      "step": 824500
    },
    {
      "epoch": 31.890783926983023,
      "grad_norm": 13.238227844238281,
      "learning_rate": 2.342434672751415e-05,
      "loss": 1.8772,
      "step": 824600
    },
    {
      "epoch": 31.894651351664926,
      "grad_norm": 12.911147117614746,
      "learning_rate": 2.3421123873612565e-05,
      "loss": 1.79,
      "step": 824700
    },
    {
      "epoch": 31.89851877634683,
      "grad_norm": 12.922606468200684,
      "learning_rate": 2.3417901019710976e-05,
      "loss": 1.8668,
      "step": 824800
    },
    {
      "epoch": 31.902386201028737,
      "grad_norm": 11.382499694824219,
      "learning_rate": 2.341467816580939e-05,
      "loss": 1.8744,
      "step": 824900
    },
    {
      "epoch": 31.90625362571064,
      "grad_norm": 15.145915985107422,
      "learning_rate": 2.3411455311907802e-05,
      "loss": 1.8162,
      "step": 825000
    },
    {
      "epoch": 31.910121050392544,
      "grad_norm": 8.691760063171387,
      "learning_rate": 2.3408232458006217e-05,
      "loss": 1.8459,
      "step": 825100
    },
    {
      "epoch": 31.913988475074447,
      "grad_norm": 12.230369567871094,
      "learning_rate": 2.3405009604104628e-05,
      "loss": 1.8369,
      "step": 825200
    },
    {
      "epoch": 31.91785589975635,
      "grad_norm": 8.283026695251465,
      "learning_rate": 2.3401786750203043e-05,
      "loss": 1.8312,
      "step": 825300
    },
    {
      "epoch": 31.921723324438258,
      "grad_norm": 10.980323791503906,
      "learning_rate": 2.3398563896301454e-05,
      "loss": 1.9561,
      "step": 825400
    },
    {
      "epoch": 31.92559074912016,
      "grad_norm": 13.084311485290527,
      "learning_rate": 2.339534104239987e-05,
      "loss": 1.8798,
      "step": 825500
    },
    {
      "epoch": 31.929458173802065,
      "grad_norm": 14.45781421661377,
      "learning_rate": 2.339211818849828e-05,
      "loss": 1.8432,
      "step": 825600
    },
    {
      "epoch": 31.933325598483968,
      "grad_norm": 12.96198844909668,
      "learning_rate": 2.3388895334596695e-05,
      "loss": 1.9721,
      "step": 825700
    },
    {
      "epoch": 31.937193023165875,
      "grad_norm": 11.388672828674316,
      "learning_rate": 2.3385672480695106e-05,
      "loss": 1.8663,
      "step": 825800
    },
    {
      "epoch": 31.94106044784778,
      "grad_norm": 12.951693534851074,
      "learning_rate": 2.338244962679352e-05,
      "loss": 1.9412,
      "step": 825900
    },
    {
      "epoch": 31.944927872529682,
      "grad_norm": 8.453426361083984,
      "learning_rate": 2.3379226772891932e-05,
      "loss": 1.8357,
      "step": 826000
    },
    {
      "epoch": 31.948795297211586,
      "grad_norm": 11.817937850952148,
      "learning_rate": 2.3376003918990347e-05,
      "loss": 1.9868,
      "step": 826100
    },
    {
      "epoch": 31.952662721893493,
      "grad_norm": 8.874822616577148,
      "learning_rate": 2.337278106508876e-05,
      "loss": 1.8957,
      "step": 826200
    },
    {
      "epoch": 31.956530146575396,
      "grad_norm": 11.435918807983398,
      "learning_rate": 2.336955821118717e-05,
      "loss": 1.8717,
      "step": 826300
    },
    {
      "epoch": 31.9603975712573,
      "grad_norm": 13.982110023498535,
      "learning_rate": 2.3366335357285584e-05,
      "loss": 1.9694,
      "step": 826400
    },
    {
      "epoch": 31.964264995939203,
      "grad_norm": 12.38277530670166,
      "learning_rate": 2.3363112503383996e-05,
      "loss": 1.8625,
      "step": 826500
    },
    {
      "epoch": 31.96813242062111,
      "grad_norm": 12.685659408569336,
      "learning_rate": 2.335988964948241e-05,
      "loss": 1.8881,
      "step": 826600
    },
    {
      "epoch": 31.971999845303014,
      "grad_norm": 9.185516357421875,
      "learning_rate": 2.3356666795580822e-05,
      "loss": 1.8849,
      "step": 826700
    },
    {
      "epoch": 31.975867269984917,
      "grad_norm": 11.19779109954834,
      "learning_rate": 2.3353443941679237e-05,
      "loss": 1.7777,
      "step": 826800
    },
    {
      "epoch": 31.97973469466682,
      "grad_norm": 15.82250690460205,
      "learning_rate": 2.3350221087777648e-05,
      "loss": 1.8371,
      "step": 826900
    },
    {
      "epoch": 31.983602119348724,
      "grad_norm": 13.562792778015137,
      "learning_rate": 2.3346998233876063e-05,
      "loss": 1.8477,
      "step": 827000
    },
    {
      "epoch": 31.98746954403063,
      "grad_norm": 12.265989303588867,
      "learning_rate": 2.3343775379974474e-05,
      "loss": 1.8263,
      "step": 827100
    },
    {
      "epoch": 31.991336968712535,
      "grad_norm": 11.43093490600586,
      "learning_rate": 2.334055252607289e-05,
      "loss": 2.0116,
      "step": 827200
    },
    {
      "epoch": 31.995204393394438,
      "grad_norm": 7.482044219970703,
      "learning_rate": 2.33373296721713e-05,
      "loss": 1.8343,
      "step": 827300
    },
    {
      "epoch": 31.99907181807634,
      "grad_norm": 12.909907341003418,
      "learning_rate": 2.3334106818269715e-05,
      "loss": 1.8713,
      "step": 827400
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.8005527257919312,
      "eval_runtime": 2.9437,
      "eval_samples_per_second": 462.349,
      "eval_steps_per_second": 462.349,
      "step": 827424
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.6701411008834839,
      "eval_runtime": 56.2263,
      "eval_samples_per_second": 459.873,
      "eval_steps_per_second": 459.873,
      "step": 827424
    },
    {
      "epoch": 32.002939242758245,
      "grad_norm": 10.636922836303711,
      "learning_rate": 2.3330883964368126e-05,
      "loss": 1.8351,
      "step": 827500
    },
    {
      "epoch": 32.00680666744015,
      "grad_norm": 12.83030891418457,
      "learning_rate": 2.332766111046654e-05,
      "loss": 1.844,
      "step": 827600
    },
    {
      "epoch": 32.01067409212206,
      "grad_norm": 10.263361930847168,
      "learning_rate": 2.3324438256564955e-05,
      "loss": 1.8757,
      "step": 827700
    },
    {
      "epoch": 32.01454151680396,
      "grad_norm": 11.84481430053711,
      "learning_rate": 2.3321215402663367e-05,
      "loss": 1.8498,
      "step": 827800
    },
    {
      "epoch": 32.018408941485866,
      "grad_norm": 11.924712181091309,
      "learning_rate": 2.331799254876178e-05,
      "loss": 1.7838,
      "step": 827900
    },
    {
      "epoch": 32.022276366167766,
      "grad_norm": 8.686232566833496,
      "learning_rate": 2.3314769694860196e-05,
      "loss": 1.7525,
      "step": 828000
    },
    {
      "epoch": 32.02614379084967,
      "grad_norm": 9.402853965759277,
      "learning_rate": 2.3311546840958608e-05,
      "loss": 1.7793,
      "step": 828100
    },
    {
      "epoch": 32.03001121553158,
      "grad_norm": 10.34945011138916,
      "learning_rate": 2.3308323987057022e-05,
      "loss": 1.8757,
      "step": 828200
    },
    {
      "epoch": 32.03387864021348,
      "grad_norm": 8.67728042602539,
      "learning_rate": 2.3305101133155434e-05,
      "loss": 1.7798,
      "step": 828300
    },
    {
      "epoch": 32.03774606489539,
      "grad_norm": 16.90923500061035,
      "learning_rate": 2.3301878279253848e-05,
      "loss": 1.8639,
      "step": 828400
    },
    {
      "epoch": 32.04161348957729,
      "grad_norm": 11.715524673461914,
      "learning_rate": 2.329865542535226e-05,
      "loss": 1.8528,
      "step": 828500
    },
    {
      "epoch": 32.045480914259194,
      "grad_norm": 14.047759056091309,
      "learning_rate": 2.3295432571450674e-05,
      "loss": 1.8979,
      "step": 828600
    },
    {
      "epoch": 32.0493483389411,
      "grad_norm": 7.028289794921875,
      "learning_rate": 2.3292209717549086e-05,
      "loss": 1.8894,
      "step": 828700
    },
    {
      "epoch": 32.053215763623,
      "grad_norm": 11.232654571533203,
      "learning_rate": 2.32889868636475e-05,
      "loss": 1.8199,
      "step": 828800
    },
    {
      "epoch": 32.05708318830491,
      "grad_norm": 14.329059600830078,
      "learning_rate": 2.3285764009745912e-05,
      "loss": 1.8449,
      "step": 828900
    },
    {
      "epoch": 32.060950612986815,
      "grad_norm": 10.23479175567627,
      "learning_rate": 2.3282541155844326e-05,
      "loss": 1.7897,
      "step": 829000
    },
    {
      "epoch": 32.064818037668715,
      "grad_norm": 13.536309242248535,
      "learning_rate": 2.3279318301942738e-05,
      "loss": 1.8363,
      "step": 829100
    },
    {
      "epoch": 32.06868546235062,
      "grad_norm": 13.966443061828613,
      "learning_rate": 2.327609544804115e-05,
      "loss": 1.816,
      "step": 829200
    },
    {
      "epoch": 32.07255288703252,
      "grad_norm": 10.023530960083008,
      "learning_rate": 2.3272872594139564e-05,
      "loss": 1.8473,
      "step": 829300
    },
    {
      "epoch": 32.07642031171443,
      "grad_norm": 12.97720718383789,
      "learning_rate": 2.3269649740237975e-05,
      "loss": 1.9323,
      "step": 829400
    },
    {
      "epoch": 32.080287736396336,
      "grad_norm": 11.25750732421875,
      "learning_rate": 2.326642688633639e-05,
      "loss": 1.8873,
      "step": 829500
    },
    {
      "epoch": 32.084155161078236,
      "grad_norm": 12.193314552307129,
      "learning_rate": 2.32632040324348e-05,
      "loss": 1.7081,
      "step": 829600
    },
    {
      "epoch": 32.08802258576014,
      "grad_norm": 10.644776344299316,
      "learning_rate": 2.3259981178533216e-05,
      "loss": 1.8924,
      "step": 829700
    },
    {
      "epoch": 32.09189001044205,
      "grad_norm": 11.16415023803711,
      "learning_rate": 2.3256758324631627e-05,
      "loss": 1.8491,
      "step": 829800
    },
    {
      "epoch": 32.09575743512395,
      "grad_norm": 6.997364521026611,
      "learning_rate": 2.3253535470730042e-05,
      "loss": 1.8185,
      "step": 829900
    },
    {
      "epoch": 32.09962485980586,
      "grad_norm": 9.157095909118652,
      "learning_rate": 2.3250312616828453e-05,
      "loss": 1.8995,
      "step": 830000
    },
    {
      "epoch": 32.10349228448776,
      "grad_norm": 13.728055000305176,
      "learning_rate": 2.3247089762926868e-05,
      "loss": 1.8384,
      "step": 830100
    },
    {
      "epoch": 32.107359709169664,
      "grad_norm": 11.834864616394043,
      "learning_rate": 2.324386690902528e-05,
      "loss": 1.8956,
      "step": 830200
    },
    {
      "epoch": 32.11122713385157,
      "grad_norm": 10.697121620178223,
      "learning_rate": 2.3240644055123694e-05,
      "loss": 1.854,
      "step": 830300
    },
    {
      "epoch": 32.11509455853347,
      "grad_norm": 14.122329711914062,
      "learning_rate": 2.3237421201222105e-05,
      "loss": 1.9095,
      "step": 830400
    },
    {
      "epoch": 32.11896198321538,
      "grad_norm": 12.668363571166992,
      "learning_rate": 2.323419834732052e-05,
      "loss": 1.8536,
      "step": 830500
    },
    {
      "epoch": 32.12282940789728,
      "grad_norm": 11.262716293334961,
      "learning_rate": 2.323097549341893e-05,
      "loss": 1.8315,
      "step": 830600
    },
    {
      "epoch": 32.126696832579185,
      "grad_norm": 8.509013175964355,
      "learning_rate": 2.3227752639517346e-05,
      "loss": 1.898,
      "step": 830700
    },
    {
      "epoch": 32.13056425726109,
      "grad_norm": 11.298421859741211,
      "learning_rate": 2.3224529785615757e-05,
      "loss": 1.8511,
      "step": 830800
    },
    {
      "epoch": 32.13443168194299,
      "grad_norm": 12.70158863067627,
      "learning_rate": 2.3221306931714172e-05,
      "loss": 1.9475,
      "step": 830900
    },
    {
      "epoch": 32.1382991066249,
      "grad_norm": 17.534549713134766,
      "learning_rate": 2.3218084077812584e-05,
      "loss": 1.9239,
      "step": 831000
    },
    {
      "epoch": 32.142166531306806,
      "grad_norm": 15.219321250915527,
      "learning_rate": 2.3214861223910998e-05,
      "loss": 1.8143,
      "step": 831100
    },
    {
      "epoch": 32.146033955988706,
      "grad_norm": 11.933598518371582,
      "learning_rate": 2.3211638370009413e-05,
      "loss": 1.8243,
      "step": 831200
    },
    {
      "epoch": 32.14990138067061,
      "grad_norm": 10.7256498336792,
      "learning_rate": 2.3208415516107824e-05,
      "loss": 1.8433,
      "step": 831300
    },
    {
      "epoch": 32.15376880535251,
      "grad_norm": 12.860156059265137,
      "learning_rate": 2.320519266220624e-05,
      "loss": 1.9792,
      "step": 831400
    },
    {
      "epoch": 32.15763623003442,
      "grad_norm": 12.621528625488281,
      "learning_rate": 2.3201969808304654e-05,
      "loss": 1.8091,
      "step": 831500
    },
    {
      "epoch": 32.16150365471633,
      "grad_norm": 13.339213371276855,
      "learning_rate": 2.3198746954403065e-05,
      "loss": 1.9235,
      "step": 831600
    },
    {
      "epoch": 32.16537107939823,
      "grad_norm": 10.003833770751953,
      "learning_rate": 2.319552410050148e-05,
      "loss": 1.7534,
      "step": 831700
    },
    {
      "epoch": 32.169238504080134,
      "grad_norm": 15.924288749694824,
      "learning_rate": 2.319230124659989e-05,
      "loss": 1.8371,
      "step": 831800
    },
    {
      "epoch": 32.173105928762034,
      "grad_norm": 12.57334041595459,
      "learning_rate": 2.3189078392698306e-05,
      "loss": 1.8971,
      "step": 831900
    },
    {
      "epoch": 32.17697335344394,
      "grad_norm": 11.16114616394043,
      "learning_rate": 2.3185855538796717e-05,
      "loss": 1.8685,
      "step": 832000
    },
    {
      "epoch": 32.18084077812585,
      "grad_norm": 15.531947135925293,
      "learning_rate": 2.318263268489513e-05,
      "loss": 1.9252,
      "step": 832100
    },
    {
      "epoch": 32.18470820280775,
      "grad_norm": 10.74590015411377,
      "learning_rate": 2.3179409830993543e-05,
      "loss": 1.8602,
      "step": 832200
    },
    {
      "epoch": 32.188575627489655,
      "grad_norm": 12.11031436920166,
      "learning_rate": 2.3176186977091955e-05,
      "loss": 1.8846,
      "step": 832300
    },
    {
      "epoch": 32.19244305217156,
      "grad_norm": 12.81461238861084,
      "learning_rate": 2.317296412319037e-05,
      "loss": 1.9404,
      "step": 832400
    },
    {
      "epoch": 32.19631047685346,
      "grad_norm": 13.587401390075684,
      "learning_rate": 2.316974126928878e-05,
      "loss": 1.8727,
      "step": 832500
    },
    {
      "epoch": 32.20017790153537,
      "grad_norm": 12.081892967224121,
      "learning_rate": 2.3166518415387195e-05,
      "loss": 1.9237,
      "step": 832600
    },
    {
      "epoch": 32.20404532621727,
      "grad_norm": 9.455099105834961,
      "learning_rate": 2.3163295561485607e-05,
      "loss": 1.9449,
      "step": 832700
    },
    {
      "epoch": 32.207912750899176,
      "grad_norm": 11.460746765136719,
      "learning_rate": 2.316007270758402e-05,
      "loss": 1.8464,
      "step": 832800
    },
    {
      "epoch": 32.21178017558108,
      "grad_norm": 10.090664863586426,
      "learning_rate": 2.3156849853682433e-05,
      "loss": 1.8792,
      "step": 832900
    },
    {
      "epoch": 32.21564760026298,
      "grad_norm": 9.673859596252441,
      "learning_rate": 2.3153626999780847e-05,
      "loss": 1.9324,
      "step": 833000
    },
    {
      "epoch": 32.21951502494489,
      "grad_norm": 13.666597366333008,
      "learning_rate": 2.315040414587926e-05,
      "loss": 1.8203,
      "step": 833100
    },
    {
      "epoch": 32.22338244962679,
      "grad_norm": 13.092303276062012,
      "learning_rate": 2.3147181291977673e-05,
      "loss": 1.9189,
      "step": 833200
    },
    {
      "epoch": 32.2272498743087,
      "grad_norm": 11.7354154586792,
      "learning_rate": 2.3143958438076085e-05,
      "loss": 1.8958,
      "step": 833300
    },
    {
      "epoch": 32.231117298990604,
      "grad_norm": 10.949602127075195,
      "learning_rate": 2.31407355841745e-05,
      "loss": 1.8836,
      "step": 833400
    },
    {
      "epoch": 32.234984723672504,
      "grad_norm": 12.053832054138184,
      "learning_rate": 2.313751273027291e-05,
      "loss": 1.929,
      "step": 833500
    },
    {
      "epoch": 32.23885214835441,
      "grad_norm": 9.892394065856934,
      "learning_rate": 2.3134289876371326e-05,
      "loss": 1.8137,
      "step": 833600
    },
    {
      "epoch": 32.24271957303632,
      "grad_norm": 10.012454986572266,
      "learning_rate": 2.3131067022469737e-05,
      "loss": 1.8663,
      "step": 833700
    },
    {
      "epoch": 32.24658699771822,
      "grad_norm": 12.46107292175293,
      "learning_rate": 2.312784416856815e-05,
      "loss": 1.8417,
      "step": 833800
    },
    {
      "epoch": 32.250454422400125,
      "grad_norm": 8.975439071655273,
      "learning_rate": 2.3124621314666563e-05,
      "loss": 1.8474,
      "step": 833900
    },
    {
      "epoch": 32.254321847082025,
      "grad_norm": 13.384611129760742,
      "learning_rate": 2.3121398460764978e-05,
      "loss": 1.8762,
      "step": 834000
    },
    {
      "epoch": 32.25818927176393,
      "grad_norm": 8.947627067565918,
      "learning_rate": 2.311817560686339e-05,
      "loss": 1.8303,
      "step": 834100
    },
    {
      "epoch": 32.26205669644584,
      "grad_norm": 11.119504928588867,
      "learning_rate": 2.3114952752961804e-05,
      "loss": 1.9008,
      "step": 834200
    },
    {
      "epoch": 32.26592412112774,
      "grad_norm": 9.854965209960938,
      "learning_rate": 2.3111729899060215e-05,
      "loss": 1.8372,
      "step": 834300
    },
    {
      "epoch": 32.269791545809646,
      "grad_norm": 9.59575080871582,
      "learning_rate": 2.310850704515863e-05,
      "loss": 1.8812,
      "step": 834400
    },
    {
      "epoch": 32.27365897049155,
      "grad_norm": 11.101558685302734,
      "learning_rate": 2.3105284191257044e-05,
      "loss": 1.8434,
      "step": 834500
    },
    {
      "epoch": 32.27752639517345,
      "grad_norm": 13.811932563781738,
      "learning_rate": 2.3102061337355456e-05,
      "loss": 1.8836,
      "step": 834600
    },
    {
      "epoch": 32.28139381985536,
      "grad_norm": 9.690264701843262,
      "learning_rate": 2.309883848345387e-05,
      "loss": 1.883,
      "step": 834700
    },
    {
      "epoch": 32.28526124453726,
      "grad_norm": 12.424785614013672,
      "learning_rate": 2.3095615629552282e-05,
      "loss": 1.9119,
      "step": 834800
    },
    {
      "epoch": 32.28912866921917,
      "grad_norm": 12.468435287475586,
      "learning_rate": 2.3092392775650697e-05,
      "loss": 1.8127,
      "step": 834900
    },
    {
      "epoch": 32.292996093901074,
      "grad_norm": 14.350744247436523,
      "learning_rate": 2.308916992174911e-05,
      "loss": 1.721,
      "step": 835000
    },
    {
      "epoch": 32.296863518582974,
      "grad_norm": 13.502809524536133,
      "learning_rate": 2.3085947067847523e-05,
      "loss": 1.9355,
      "step": 835100
    },
    {
      "epoch": 32.30073094326488,
      "grad_norm": 12.391829490661621,
      "learning_rate": 2.3082724213945934e-05,
      "loss": 1.8959,
      "step": 835200
    },
    {
      "epoch": 32.30459836794678,
      "grad_norm": 10.521050453186035,
      "learning_rate": 2.307950136004435e-05,
      "loss": 1.9504,
      "step": 835300
    },
    {
      "epoch": 32.30846579262869,
      "grad_norm": 12.342945098876953,
      "learning_rate": 2.307627850614276e-05,
      "loss": 1.8086,
      "step": 835400
    },
    {
      "epoch": 32.312333217310595,
      "grad_norm": 11.247777938842773,
      "learning_rate": 2.3073055652241175e-05,
      "loss": 1.8408,
      "step": 835500
    },
    {
      "epoch": 32.316200641992495,
      "grad_norm": 12.589733123779297,
      "learning_rate": 2.3069832798339586e-05,
      "loss": 1.9231,
      "step": 835600
    },
    {
      "epoch": 32.3200680666744,
      "grad_norm": 14.18217658996582,
      "learning_rate": 2.3066609944438e-05,
      "loss": 1.9281,
      "step": 835700
    },
    {
      "epoch": 32.32393549135631,
      "grad_norm": 15.225773811340332,
      "learning_rate": 2.3063387090536412e-05,
      "loss": 1.8802,
      "step": 835800
    },
    {
      "epoch": 32.32780291603821,
      "grad_norm": 11.392425537109375,
      "learning_rate": 2.3060164236634827e-05,
      "loss": 1.8102,
      "step": 835900
    },
    {
      "epoch": 32.331670340720116,
      "grad_norm": 10.25472640991211,
      "learning_rate": 2.3056941382733238e-05,
      "loss": 1.8453,
      "step": 836000
    },
    {
      "epoch": 32.335537765402016,
      "grad_norm": 13.683631896972656,
      "learning_rate": 2.3053718528831653e-05,
      "loss": 1.7148,
      "step": 836100
    },
    {
      "epoch": 32.33940519008392,
      "grad_norm": 11.525758743286133,
      "learning_rate": 2.3050495674930064e-05,
      "loss": 1.8552,
      "step": 836200
    },
    {
      "epoch": 32.34327261476583,
      "grad_norm": 11.200461387634277,
      "learning_rate": 2.304727282102848e-05,
      "loss": 1.8493,
      "step": 836300
    },
    {
      "epoch": 32.34714003944773,
      "grad_norm": 15.34284496307373,
      "learning_rate": 2.304404996712689e-05,
      "loss": 1.9339,
      "step": 836400
    },
    {
      "epoch": 32.35100746412964,
      "grad_norm": 10.772550582885742,
      "learning_rate": 2.3040827113225305e-05,
      "loss": 1.7711,
      "step": 836500
    },
    {
      "epoch": 32.35487488881154,
      "grad_norm": 9.96849250793457,
      "learning_rate": 2.3037604259323716e-05,
      "loss": 1.8984,
      "step": 836600
    },
    {
      "epoch": 32.358742313493444,
      "grad_norm": 11.442277908325195,
      "learning_rate": 2.303438140542213e-05,
      "loss": 1.8004,
      "step": 836700
    },
    {
      "epoch": 32.36260973817535,
      "grad_norm": 12.39799690246582,
      "learning_rate": 2.3031158551520542e-05,
      "loss": 1.9512,
      "step": 836800
    },
    {
      "epoch": 32.36647716285725,
      "grad_norm": 11.65983772277832,
      "learning_rate": 2.3027935697618957e-05,
      "loss": 1.8369,
      "step": 836900
    },
    {
      "epoch": 32.37034458753916,
      "grad_norm": 14.013049125671387,
      "learning_rate": 2.302471284371737e-05,
      "loss": 1.8475,
      "step": 837000
    },
    {
      "epoch": 32.374212012221065,
      "grad_norm": 16.509910583496094,
      "learning_rate": 2.3021489989815783e-05,
      "loss": 1.911,
      "step": 837100
    },
    {
      "epoch": 32.378079436902965,
      "grad_norm": 10.367547035217285,
      "learning_rate": 2.3018267135914194e-05,
      "loss": 1.8018,
      "step": 837200
    },
    {
      "epoch": 32.38194686158487,
      "grad_norm": 9.742972373962402,
      "learning_rate": 2.301504428201261e-05,
      "loss": 1.8449,
      "step": 837300
    },
    {
      "epoch": 32.38581428626677,
      "grad_norm": 16.22832489013672,
      "learning_rate": 2.301182142811102e-05,
      "loss": 1.8915,
      "step": 837400
    },
    {
      "epoch": 32.38968171094868,
      "grad_norm": 11.30766773223877,
      "learning_rate": 2.3008598574209435e-05,
      "loss": 1.8165,
      "step": 837500
    },
    {
      "epoch": 32.393549135630586,
      "grad_norm": 11.126116752624512,
      "learning_rate": 2.3005375720307847e-05,
      "loss": 1.8845,
      "step": 837600
    },
    {
      "epoch": 32.397416560312486,
      "grad_norm": 11.091574668884277,
      "learning_rate": 2.300215286640626e-05,
      "loss": 1.9003,
      "step": 837700
    },
    {
      "epoch": 32.40128398499439,
      "grad_norm": 11.366333961486816,
      "learning_rate": 2.2998930012504673e-05,
      "loss": 1.8433,
      "step": 837800
    },
    {
      "epoch": 32.4051514096763,
      "grad_norm": 9.674129486083984,
      "learning_rate": 2.2995707158603087e-05,
      "loss": 1.7932,
      "step": 837900
    },
    {
      "epoch": 32.4090188343582,
      "grad_norm": 13.960700035095215,
      "learning_rate": 2.2992484304701502e-05,
      "loss": 1.7882,
      "step": 838000
    },
    {
      "epoch": 32.41288625904011,
      "grad_norm": 13.052565574645996,
      "learning_rate": 2.2989261450799913e-05,
      "loss": 1.9234,
      "step": 838100
    },
    {
      "epoch": 32.41675368372201,
      "grad_norm": 8.514482498168945,
      "learning_rate": 2.2986038596898328e-05,
      "loss": 1.85,
      "step": 838200
    },
    {
      "epoch": 32.420621108403914,
      "grad_norm": 12.953579902648926,
      "learning_rate": 2.298281574299674e-05,
      "loss": 1.7665,
      "step": 838300
    },
    {
      "epoch": 32.42448853308582,
      "grad_norm": 11.439287185668945,
      "learning_rate": 2.2979592889095154e-05,
      "loss": 1.8075,
      "step": 838400
    },
    {
      "epoch": 32.42835595776772,
      "grad_norm": 10.506226539611816,
      "learning_rate": 2.2976370035193565e-05,
      "loss": 1.8557,
      "step": 838500
    },
    {
      "epoch": 32.43222338244963,
      "grad_norm": 8.865123748779297,
      "learning_rate": 2.297314718129198e-05,
      "loss": 1.8809,
      "step": 838600
    },
    {
      "epoch": 32.43609080713153,
      "grad_norm": 11.860389709472656,
      "learning_rate": 2.296992432739039e-05,
      "loss": 1.8665,
      "step": 838700
    },
    {
      "epoch": 32.439958231813435,
      "grad_norm": 14.117372512817383,
      "learning_rate": 2.2966701473488806e-05,
      "loss": 1.9239,
      "step": 838800
    },
    {
      "epoch": 32.44382565649534,
      "grad_norm": 12.05704116821289,
      "learning_rate": 2.2963478619587218e-05,
      "loss": 1.8439,
      "step": 838900
    },
    {
      "epoch": 32.44769308117724,
      "grad_norm": 13.606451034545898,
      "learning_rate": 2.2960255765685632e-05,
      "loss": 1.7948,
      "step": 839000
    },
    {
      "epoch": 32.45156050585915,
      "grad_norm": 10.506937980651855,
      "learning_rate": 2.2957032911784044e-05,
      "loss": 1.9023,
      "step": 839100
    },
    {
      "epoch": 32.455427930541056,
      "grad_norm": 15.998688697814941,
      "learning_rate": 2.2953810057882458e-05,
      "loss": 1.8533,
      "step": 839200
    },
    {
      "epoch": 32.459295355222956,
      "grad_norm": 9.341787338256836,
      "learning_rate": 2.295058720398087e-05,
      "loss": 1.7949,
      "step": 839300
    },
    {
      "epoch": 32.46316277990486,
      "grad_norm": 11.182523727416992,
      "learning_rate": 2.2947364350079284e-05,
      "loss": 1.9478,
      "step": 839400
    },
    {
      "epoch": 32.46703020458676,
      "grad_norm": 10.25255298614502,
      "learning_rate": 2.2944141496177696e-05,
      "loss": 1.8669,
      "step": 839500
    },
    {
      "epoch": 32.47089762926867,
      "grad_norm": 10.048027992248535,
      "learning_rate": 2.294091864227611e-05,
      "loss": 1.9096,
      "step": 839600
    },
    {
      "epoch": 32.47476505395058,
      "grad_norm": 12.322187423706055,
      "learning_rate": 2.2937695788374522e-05,
      "loss": 1.8761,
      "step": 839700
    },
    {
      "epoch": 32.47863247863248,
      "grad_norm": 9.909664154052734,
      "learning_rate": 2.2934472934472936e-05,
      "loss": 1.8077,
      "step": 839800
    },
    {
      "epoch": 32.482499903314384,
      "grad_norm": 12.619953155517578,
      "learning_rate": 2.2931250080571348e-05,
      "loss": 1.8854,
      "step": 839900
    },
    {
      "epoch": 32.486367327996284,
      "grad_norm": 14.412032127380371,
      "learning_rate": 2.2928027226669762e-05,
      "loss": 1.8614,
      "step": 840000
    },
    {
      "epoch": 32.49023475267819,
      "grad_norm": 11.140535354614258,
      "learning_rate": 2.2924804372768174e-05,
      "loss": 1.8691,
      "step": 840100
    },
    {
      "epoch": 32.4941021773601,
      "grad_norm": 12.274312973022461,
      "learning_rate": 2.292158151886659e-05,
      "loss": 1.8954,
      "step": 840200
    },
    {
      "epoch": 32.497969602042,
      "grad_norm": 10.260374069213867,
      "learning_rate": 2.2918358664965e-05,
      "loss": 1.8877,
      "step": 840300
    },
    {
      "epoch": 32.501837026723905,
      "grad_norm": 11.124354362487793,
      "learning_rate": 2.2915135811063415e-05,
      "loss": 1.9488,
      "step": 840400
    },
    {
      "epoch": 32.50570445140581,
      "grad_norm": 13.6845121383667,
      "learning_rate": 2.2911912957161826e-05,
      "loss": 1.8649,
      "step": 840500
    },
    {
      "epoch": 32.50957187608771,
      "grad_norm": 11.7493257522583,
      "learning_rate": 2.290869010326024e-05,
      "loss": 1.9716,
      "step": 840600
    },
    {
      "epoch": 32.51343930076962,
      "grad_norm": 11.579522132873535,
      "learning_rate": 2.2905467249358652e-05,
      "loss": 1.8816,
      "step": 840700
    },
    {
      "epoch": 32.51730672545152,
      "grad_norm": 8.199678421020508,
      "learning_rate": 2.2902244395457067e-05,
      "loss": 1.8699,
      "step": 840800
    },
    {
      "epoch": 32.521174150133426,
      "grad_norm": 10.874749183654785,
      "learning_rate": 2.2899021541555478e-05,
      "loss": 1.899,
      "step": 840900
    },
    {
      "epoch": 32.52504157481533,
      "grad_norm": 9.982124328613281,
      "learning_rate": 2.289579868765389e-05,
      "loss": 1.8448,
      "step": 841000
    },
    {
      "epoch": 32.52890899949723,
      "grad_norm": 10.11673355102539,
      "learning_rate": 2.2892575833752304e-05,
      "loss": 1.9129,
      "step": 841100
    },
    {
      "epoch": 32.53277642417914,
      "grad_norm": 13.801288604736328,
      "learning_rate": 2.288935297985072e-05,
      "loss": 1.8084,
      "step": 841200
    },
    {
      "epoch": 32.53664384886105,
      "grad_norm": 14.598113059997559,
      "learning_rate": 2.288613012594913e-05,
      "loss": 1.8695,
      "step": 841300
    },
    {
      "epoch": 32.54051127354295,
      "grad_norm": 12.6762113571167,
      "learning_rate": 2.2882907272047545e-05,
      "loss": 1.8437,
      "step": 841400
    },
    {
      "epoch": 32.544378698224854,
      "grad_norm": 15.22666072845459,
      "learning_rate": 2.287968441814596e-05,
      "loss": 1.8512,
      "step": 841500
    },
    {
      "epoch": 32.548246122906754,
      "grad_norm": 12.325125694274902,
      "learning_rate": 2.287646156424437e-05,
      "loss": 1.8512,
      "step": 841600
    },
    {
      "epoch": 32.55211354758866,
      "grad_norm": 11.834030151367188,
      "learning_rate": 2.2873238710342786e-05,
      "loss": 1.7822,
      "step": 841700
    },
    {
      "epoch": 32.55598097227057,
      "grad_norm": 12.562090873718262,
      "learning_rate": 2.2870015856441197e-05,
      "loss": 1.8592,
      "step": 841800
    },
    {
      "epoch": 32.55984839695247,
      "grad_norm": 11.648163795471191,
      "learning_rate": 2.286679300253961e-05,
      "loss": 1.9294,
      "step": 841900
    },
    {
      "epoch": 32.563715821634375,
      "grad_norm": 10.100183486938477,
      "learning_rate": 2.2863570148638023e-05,
      "loss": 1.9023,
      "step": 842000
    },
    {
      "epoch": 32.567583246316275,
      "grad_norm": 12.646406173706055,
      "learning_rate": 2.2860347294736438e-05,
      "loss": 1.847,
      "step": 842100
    },
    {
      "epoch": 32.57145067099818,
      "grad_norm": 10.281938552856445,
      "learning_rate": 2.285712444083485e-05,
      "loss": 1.7701,
      "step": 842200
    },
    {
      "epoch": 32.57531809568009,
      "grad_norm": 12.561716079711914,
      "learning_rate": 2.2853901586933264e-05,
      "loss": 1.8023,
      "step": 842300
    },
    {
      "epoch": 32.57918552036199,
      "grad_norm": 13.718790054321289,
      "learning_rate": 2.2850678733031675e-05,
      "loss": 1.9073,
      "step": 842400
    },
    {
      "epoch": 32.583052945043896,
      "grad_norm": 9.854784965515137,
      "learning_rate": 2.284745587913009e-05,
      "loss": 1.9264,
      "step": 842500
    },
    {
      "epoch": 32.5869203697258,
      "grad_norm": 15.300057411193848,
      "learning_rate": 2.28442330252285e-05,
      "loss": 1.9355,
      "step": 842600
    },
    {
      "epoch": 32.5907877944077,
      "grad_norm": 19.91330909729004,
      "learning_rate": 2.2841010171326916e-05,
      "loss": 1.8985,
      "step": 842700
    },
    {
      "epoch": 32.59465521908961,
      "grad_norm": 13.471423149108887,
      "learning_rate": 2.2837787317425327e-05,
      "loss": 1.8686,
      "step": 842800
    },
    {
      "epoch": 32.59852264377151,
      "grad_norm": 12.469968795776367,
      "learning_rate": 2.2834564463523742e-05,
      "loss": 1.9155,
      "step": 842900
    },
    {
      "epoch": 32.60239006845342,
      "grad_norm": 10.765732765197754,
      "learning_rate": 2.2831341609622153e-05,
      "loss": 1.9371,
      "step": 843000
    },
    {
      "epoch": 32.606257493135324,
      "grad_norm": 11.251568794250488,
      "learning_rate": 2.2828118755720568e-05,
      "loss": 1.7966,
      "step": 843100
    },
    {
      "epoch": 32.610124917817224,
      "grad_norm": 11.544642448425293,
      "learning_rate": 2.282489590181898e-05,
      "loss": 2.0129,
      "step": 843200
    },
    {
      "epoch": 32.61399234249913,
      "grad_norm": 12.405427932739258,
      "learning_rate": 2.2821673047917394e-05,
      "loss": 1.9318,
      "step": 843300
    },
    {
      "epoch": 32.61785976718103,
      "grad_norm": 9.935432434082031,
      "learning_rate": 2.2818450194015805e-05,
      "loss": 1.8917,
      "step": 843400
    },
    {
      "epoch": 32.62172719186294,
      "grad_norm": 11.49228286743164,
      "learning_rate": 2.281522734011422e-05,
      "loss": 1.8498,
      "step": 843500
    },
    {
      "epoch": 32.625594616544845,
      "grad_norm": 13.497733116149902,
      "learning_rate": 2.281200448621263e-05,
      "loss": 1.8614,
      "step": 843600
    },
    {
      "epoch": 32.629462041226745,
      "grad_norm": 8.325512886047363,
      "learning_rate": 2.2808781632311046e-05,
      "loss": 1.8602,
      "step": 843700
    },
    {
      "epoch": 32.63332946590865,
      "grad_norm": 8.513794898986816,
      "learning_rate": 2.2805558778409457e-05,
      "loss": 1.9526,
      "step": 843800
    },
    {
      "epoch": 32.63719689059056,
      "grad_norm": 10.948893547058105,
      "learning_rate": 2.280233592450787e-05,
      "loss": 1.8954,
      "step": 843900
    },
    {
      "epoch": 32.64106431527246,
      "grad_norm": 12.763595581054688,
      "learning_rate": 2.2799113070606283e-05,
      "loss": 1.8436,
      "step": 844000
    },
    {
      "epoch": 32.644931739954366,
      "grad_norm": 12.366209983825684,
      "learning_rate": 2.2795890216704695e-05,
      "loss": 1.8764,
      "step": 844100
    },
    {
      "epoch": 32.648799164636266,
      "grad_norm": 9.500744819641113,
      "learning_rate": 2.279266736280311e-05,
      "loss": 1.8089,
      "step": 844200
    },
    {
      "epoch": 32.65266658931817,
      "grad_norm": 11.58574104309082,
      "learning_rate": 2.278944450890152e-05,
      "loss": 1.8452,
      "step": 844300
    },
    {
      "epoch": 32.65653401400008,
      "grad_norm": 12.022209167480469,
      "learning_rate": 2.2786221654999936e-05,
      "loss": 1.8023,
      "step": 844400
    },
    {
      "epoch": 32.66040143868198,
      "grad_norm": 12.461281776428223,
      "learning_rate": 2.278299880109835e-05,
      "loss": 1.8716,
      "step": 844500
    },
    {
      "epoch": 32.66426886336389,
      "grad_norm": 10.998746871948242,
      "learning_rate": 2.277977594719676e-05,
      "loss": 1.8503,
      "step": 844600
    },
    {
      "epoch": 32.66813628804579,
      "grad_norm": 11.871001243591309,
      "learning_rate": 2.2776553093295176e-05,
      "loss": 1.8697,
      "step": 844700
    },
    {
      "epoch": 32.672003712727694,
      "grad_norm": 11.562044143676758,
      "learning_rate": 2.2773330239393588e-05,
      "loss": 1.8436,
      "step": 844800
    },
    {
      "epoch": 32.6758711374096,
      "grad_norm": 11.509051322937012,
      "learning_rate": 2.2770107385492002e-05,
      "loss": 1.781,
      "step": 844900
    },
    {
      "epoch": 32.6797385620915,
      "grad_norm": 11.449445724487305,
      "learning_rate": 2.2766884531590417e-05,
      "loss": 1.8796,
      "step": 845000
    },
    {
      "epoch": 32.68360598677341,
      "grad_norm": 10.711469650268555,
      "learning_rate": 2.276366167768883e-05,
      "loss": 1.8636,
      "step": 845100
    },
    {
      "epoch": 32.687473411455315,
      "grad_norm": 9.54288101196289,
      "learning_rate": 2.2760438823787243e-05,
      "loss": 1.8942,
      "step": 845200
    },
    {
      "epoch": 32.691340836137215,
      "grad_norm": 11.747674942016602,
      "learning_rate": 2.2757215969885654e-05,
      "loss": 1.8497,
      "step": 845300
    },
    {
      "epoch": 32.69520826081912,
      "grad_norm": 13.350400924682617,
      "learning_rate": 2.275399311598407e-05,
      "loss": 1.8283,
      "step": 845400
    },
    {
      "epoch": 32.69907568550102,
      "grad_norm": 9.64760684967041,
      "learning_rate": 2.275077026208248e-05,
      "loss": 1.9056,
      "step": 845500
    },
    {
      "epoch": 32.70294311018293,
      "grad_norm": 10.148807525634766,
      "learning_rate": 2.2747547408180895e-05,
      "loss": 1.9241,
      "step": 845600
    },
    {
      "epoch": 32.706810534864836,
      "grad_norm": 10.58222770690918,
      "learning_rate": 2.2744324554279307e-05,
      "loss": 1.9113,
      "step": 845700
    },
    {
      "epoch": 32.710677959546736,
      "grad_norm": 11.88998031616211,
      "learning_rate": 2.274110170037772e-05,
      "loss": 1.8258,
      "step": 845800
    },
    {
      "epoch": 32.71454538422864,
      "grad_norm": 11.26317310333252,
      "learning_rate": 2.2737878846476133e-05,
      "loss": 2.0171,
      "step": 845900
    },
    {
      "epoch": 32.71841280891055,
      "grad_norm": 11.289057731628418,
      "learning_rate": 2.2734655992574547e-05,
      "loss": 1.862,
      "step": 846000
    },
    {
      "epoch": 32.72228023359245,
      "grad_norm": 14.73732852935791,
      "learning_rate": 2.273143313867296e-05,
      "loss": 1.8022,
      "step": 846100
    },
    {
      "epoch": 32.72614765827436,
      "grad_norm": 14.973440170288086,
      "learning_rate": 2.2728210284771373e-05,
      "loss": 1.9123,
      "step": 846200
    },
    {
      "epoch": 32.73001508295626,
      "grad_norm": 11.12204360961914,
      "learning_rate": 2.2724987430869785e-05,
      "loss": 1.8698,
      "step": 846300
    },
    {
      "epoch": 32.733882507638164,
      "grad_norm": 9.613665580749512,
      "learning_rate": 2.27217645769682e-05,
      "loss": 1.7888,
      "step": 846400
    },
    {
      "epoch": 32.73774993232007,
      "grad_norm": 10.837745666503906,
      "learning_rate": 2.271854172306661e-05,
      "loss": 1.8966,
      "step": 846500
    },
    {
      "epoch": 32.74161735700197,
      "grad_norm": 14.952293395996094,
      "learning_rate": 2.2715318869165025e-05,
      "loss": 1.9215,
      "step": 846600
    },
    {
      "epoch": 32.74548478168388,
      "grad_norm": 10.714447021484375,
      "learning_rate": 2.2712096015263437e-05,
      "loss": 1.9044,
      "step": 846700
    },
    {
      "epoch": 32.74935220636578,
      "grad_norm": 9.916633605957031,
      "learning_rate": 2.270887316136185e-05,
      "loss": 1.8889,
      "step": 846800
    },
    {
      "epoch": 32.753219631047685,
      "grad_norm": 10.11024284362793,
      "learning_rate": 2.2705650307460263e-05,
      "loss": 1.9381,
      "step": 846900
    },
    {
      "epoch": 32.75708705572959,
      "grad_norm": 12.72457218170166,
      "learning_rate": 2.2702427453558674e-05,
      "loss": 1.8688,
      "step": 847000
    },
    {
      "epoch": 32.76095448041149,
      "grad_norm": 10.0407133102417,
      "learning_rate": 2.269920459965709e-05,
      "loss": 1.793,
      "step": 847100
    },
    {
      "epoch": 32.7648219050934,
      "grad_norm": 11.203879356384277,
      "learning_rate": 2.26959817457555e-05,
      "loss": 1.8342,
      "step": 847200
    },
    {
      "epoch": 32.768689329775306,
      "grad_norm": 13.000535011291504,
      "learning_rate": 2.2692758891853915e-05,
      "loss": 1.8685,
      "step": 847300
    },
    {
      "epoch": 32.772556754457206,
      "grad_norm": 9.758515357971191,
      "learning_rate": 2.2689536037952326e-05,
      "loss": 1.8578,
      "step": 847400
    },
    {
      "epoch": 32.77642417913911,
      "grad_norm": 9.77137565612793,
      "learning_rate": 2.268631318405074e-05,
      "loss": 1.8939,
      "step": 847500
    },
    {
      "epoch": 32.78029160382101,
      "grad_norm": 12.439739227294922,
      "learning_rate": 2.2683090330149152e-05,
      "loss": 1.8558,
      "step": 847600
    },
    {
      "epoch": 32.78415902850292,
      "grad_norm": 10.670572280883789,
      "learning_rate": 2.2679867476247567e-05,
      "loss": 1.8717,
      "step": 847700
    },
    {
      "epoch": 32.78802645318483,
      "grad_norm": 10.268625259399414,
      "learning_rate": 2.267664462234598e-05,
      "loss": 1.9592,
      "step": 847800
    },
    {
      "epoch": 32.79189387786673,
      "grad_norm": 10.185157775878906,
      "learning_rate": 2.2673421768444393e-05,
      "loss": 1.8774,
      "step": 847900
    },
    {
      "epoch": 32.795761302548634,
      "grad_norm": 13.17377758026123,
      "learning_rate": 2.2670198914542808e-05,
      "loss": 1.8221,
      "step": 848000
    },
    {
      "epoch": 32.799628727230534,
      "grad_norm": 10.41627311706543,
      "learning_rate": 2.266697606064122e-05,
      "loss": 2.0146,
      "step": 848100
    },
    {
      "epoch": 32.80349615191244,
      "grad_norm": 12.374605178833008,
      "learning_rate": 2.2663753206739634e-05,
      "loss": 1.8743,
      "step": 848200
    },
    {
      "epoch": 32.80736357659435,
      "grad_norm": 6.409089088439941,
      "learning_rate": 2.266053035283805e-05,
      "loss": 1.9635,
      "step": 848300
    },
    {
      "epoch": 32.81123100127625,
      "grad_norm": 9.355268478393555,
      "learning_rate": 2.265730749893646e-05,
      "loss": 1.8143,
      "step": 848400
    },
    {
      "epoch": 32.815098425958155,
      "grad_norm": 11.402078628540039,
      "learning_rate": 2.2654084645034875e-05,
      "loss": 1.8579,
      "step": 848500
    },
    {
      "epoch": 32.81896585064006,
      "grad_norm": 15.076912879943848,
      "learning_rate": 2.2650861791133286e-05,
      "loss": 1.8561,
      "step": 848600
    },
    {
      "epoch": 32.82283327532196,
      "grad_norm": 12.41831111907959,
      "learning_rate": 2.26476389372317e-05,
      "loss": 1.8782,
      "step": 848700
    },
    {
      "epoch": 32.82670070000387,
      "grad_norm": 12.057472229003906,
      "learning_rate": 2.2644416083330112e-05,
      "loss": 1.8815,
      "step": 848800
    },
    {
      "epoch": 32.83056812468577,
      "grad_norm": 9.590925216674805,
      "learning_rate": 2.2641193229428527e-05,
      "loss": 1.8198,
      "step": 848900
    },
    {
      "epoch": 32.834435549367676,
      "grad_norm": 11.040060043334961,
      "learning_rate": 2.2637970375526938e-05,
      "loss": 1.8423,
      "step": 849000
    },
    {
      "epoch": 32.83830297404958,
      "grad_norm": 14.676335334777832,
      "learning_rate": 2.2634747521625353e-05,
      "loss": 1.8965,
      "step": 849100
    },
    {
      "epoch": 32.84217039873148,
      "grad_norm": 8.737582206726074,
      "learning_rate": 2.2631524667723764e-05,
      "loss": 1.8358,
      "step": 849200
    },
    {
      "epoch": 32.84603782341339,
      "grad_norm": 11.438800811767578,
      "learning_rate": 2.262830181382218e-05,
      "loss": 1.8002,
      "step": 849300
    },
    {
      "epoch": 32.8499052480953,
      "grad_norm": 13.615422248840332,
      "learning_rate": 2.262507895992059e-05,
      "loss": 1.7969,
      "step": 849400
    },
    {
      "epoch": 32.8537726727772,
      "grad_norm": 13.52750301361084,
      "learning_rate": 2.2621856106019005e-05,
      "loss": 1.9527,
      "step": 849500
    },
    {
      "epoch": 32.857640097459104,
      "grad_norm": 13.976609230041504,
      "learning_rate": 2.2618633252117416e-05,
      "loss": 1.8147,
      "step": 849600
    },
    {
      "epoch": 32.861507522141004,
      "grad_norm": 9.989731788635254,
      "learning_rate": 2.261541039821583e-05,
      "loss": 1.9951,
      "step": 849700
    },
    {
      "epoch": 32.86537494682291,
      "grad_norm": 10.916569709777832,
      "learning_rate": 2.2612187544314242e-05,
      "loss": 1.8155,
      "step": 849800
    },
    {
      "epoch": 32.86924237150482,
      "grad_norm": 12.448516845703125,
      "learning_rate": 2.2608964690412654e-05,
      "loss": 1.8571,
      "step": 849900
    },
    {
      "epoch": 32.87310979618672,
      "grad_norm": 11.694762229919434,
      "learning_rate": 2.260574183651107e-05,
      "loss": 1.8783,
      "step": 850000
    },
    {
      "epoch": 32.876977220868625,
      "grad_norm": 14.393253326416016,
      "learning_rate": 2.260251898260948e-05,
      "loss": 1.9387,
      "step": 850100
    },
    {
      "epoch": 32.880844645550525,
      "grad_norm": 5.167255401611328,
      "learning_rate": 2.2599296128707894e-05,
      "loss": 1.8144,
      "step": 850200
    },
    {
      "epoch": 32.88471207023243,
      "grad_norm": 14.2907075881958,
      "learning_rate": 2.2596073274806306e-05,
      "loss": 1.732,
      "step": 850300
    },
    {
      "epoch": 32.88857949491434,
      "grad_norm": 14.725159645080566,
      "learning_rate": 2.259285042090472e-05,
      "loss": 1.8507,
      "step": 850400
    },
    {
      "epoch": 32.89244691959624,
      "grad_norm": 11.66305923461914,
      "learning_rate": 2.2589627567003132e-05,
      "loss": 1.8723,
      "step": 850500
    },
    {
      "epoch": 32.896314344278146,
      "grad_norm": 10.456241607666016,
      "learning_rate": 2.2586404713101546e-05,
      "loss": 1.8449,
      "step": 850600
    },
    {
      "epoch": 32.90018176896005,
      "grad_norm": 13.038768768310547,
      "learning_rate": 2.2583181859199958e-05,
      "loss": 1.8599,
      "step": 850700
    },
    {
      "epoch": 32.90404919364195,
      "grad_norm": 12.065632820129395,
      "learning_rate": 2.2579959005298373e-05,
      "loss": 1.9573,
      "step": 850800
    },
    {
      "epoch": 32.90791661832386,
      "grad_norm": 11.544306755065918,
      "learning_rate": 2.2576736151396784e-05,
      "loss": 1.8821,
      "step": 850900
    },
    {
      "epoch": 32.91178404300576,
      "grad_norm": 11.80759048461914,
      "learning_rate": 2.25735132974952e-05,
      "loss": 1.8777,
      "step": 851000
    },
    {
      "epoch": 32.91565146768767,
      "grad_norm": 12.521448135375977,
      "learning_rate": 2.257029044359361e-05,
      "loss": 1.8756,
      "step": 851100
    },
    {
      "epoch": 32.919518892369574,
      "grad_norm": 8.928942680358887,
      "learning_rate": 2.2567067589692025e-05,
      "loss": 1.8658,
      "step": 851200
    },
    {
      "epoch": 32.923386317051474,
      "grad_norm": 11.7952299118042,
      "learning_rate": 2.2563844735790436e-05,
      "loss": 1.8783,
      "step": 851300
    },
    {
      "epoch": 32.92725374173338,
      "grad_norm": 12.998623847961426,
      "learning_rate": 2.256062188188885e-05,
      "loss": 1.8725,
      "step": 851400
    },
    {
      "epoch": 32.93112116641528,
      "grad_norm": 12.845612525939941,
      "learning_rate": 2.2557399027987265e-05,
      "loss": 1.8641,
      "step": 851500
    },
    {
      "epoch": 32.93498859109719,
      "grad_norm": 11.02394962310791,
      "learning_rate": 2.2554176174085677e-05,
      "loss": 1.8391,
      "step": 851600
    },
    {
      "epoch": 32.938856015779095,
      "grad_norm": 11.253994941711426,
      "learning_rate": 2.255095332018409e-05,
      "loss": 1.849,
      "step": 851700
    },
    {
      "epoch": 32.942723440460995,
      "grad_norm": 12.260356903076172,
      "learning_rate": 2.2547730466282506e-05,
      "loss": 1.8747,
      "step": 851800
    },
    {
      "epoch": 32.9465908651429,
      "grad_norm": 12.658052444458008,
      "learning_rate": 2.2544507612380917e-05,
      "loss": 1.8161,
      "step": 851900
    },
    {
      "epoch": 32.95045828982481,
      "grad_norm": 11.128347396850586,
      "learning_rate": 2.2541284758479332e-05,
      "loss": 1.8035,
      "step": 852000
    },
    {
      "epoch": 32.95432571450671,
      "grad_norm": 11.417159080505371,
      "learning_rate": 2.2538061904577744e-05,
      "loss": 1.9076,
      "step": 852100
    },
    {
      "epoch": 32.958193139188616,
      "grad_norm": 11.308992385864258,
      "learning_rate": 2.2534839050676158e-05,
      "loss": 2.0119,
      "step": 852200
    },
    {
      "epoch": 32.962060563870516,
      "grad_norm": 11.39600944519043,
      "learning_rate": 2.253161619677457e-05,
      "loss": 1.8248,
      "step": 852300
    },
    {
      "epoch": 32.96592798855242,
      "grad_norm": 12.081750869750977,
      "learning_rate": 2.2528393342872984e-05,
      "loss": 1.9527,
      "step": 852400
    },
    {
      "epoch": 32.96979541323433,
      "grad_norm": 13.429429054260254,
      "learning_rate": 2.2525170488971396e-05,
      "loss": 1.8524,
      "step": 852500
    },
    {
      "epoch": 32.97366283791623,
      "grad_norm": 13.408370018005371,
      "learning_rate": 2.252194763506981e-05,
      "loss": 1.8172,
      "step": 852600
    },
    {
      "epoch": 32.97753026259814,
      "grad_norm": 12.343297958374023,
      "learning_rate": 2.251872478116822e-05,
      "loss": 1.9047,
      "step": 852700
    },
    {
      "epoch": 32.98139768728004,
      "grad_norm": 12.281431198120117,
      "learning_rate": 2.2515501927266633e-05,
      "loss": 1.946,
      "step": 852800
    },
    {
      "epoch": 32.985265111961944,
      "grad_norm": 16.97053337097168,
      "learning_rate": 2.2512279073365048e-05,
      "loss": 1.8325,
      "step": 852900
    },
    {
      "epoch": 32.98913253664385,
      "grad_norm": 12.237907409667969,
      "learning_rate": 2.250905621946346e-05,
      "loss": 1.8736,
      "step": 853000
    },
    {
      "epoch": 32.99299996132575,
      "grad_norm": 10.593847274780273,
      "learning_rate": 2.2505833365561874e-05,
      "loss": 1.9305,
      "step": 853100
    },
    {
      "epoch": 32.99686738600766,
      "grad_norm": 11.487992286682129,
      "learning_rate": 2.2502610511660285e-05,
      "loss": 1.8582,
      "step": 853200
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.7975032329559326,
      "eval_runtime": 2.8913,
      "eval_samples_per_second": 470.719,
      "eval_steps_per_second": 470.719,
      "step": 853281
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.6635295152664185,
      "eval_runtime": 55.6569,
      "eval_samples_per_second": 464.578,
      "eval_steps_per_second": 464.578,
      "step": 853281
    },
    {
      "epoch": 33.000734810689565,
      "grad_norm": 12.575692176818848,
      "learning_rate": 2.24993876577587e-05,
      "loss": 1.9559,
      "step": 853300
    },
    {
      "epoch": 33.004602235371465,
      "grad_norm": 16.746612548828125,
      "learning_rate": 2.249616480385711e-05,
      "loss": 1.855,
      "step": 853400
    },
    {
      "epoch": 33.00846966005337,
      "grad_norm": 11.619091033935547,
      "learning_rate": 2.2492941949955526e-05,
      "loss": 2.0221,
      "step": 853500
    },
    {
      "epoch": 33.01233708473527,
      "grad_norm": 7.714592933654785,
      "learning_rate": 2.2489719096053937e-05,
      "loss": 1.9239,
      "step": 853600
    },
    {
      "epoch": 33.01620450941718,
      "grad_norm": 10.352458953857422,
      "learning_rate": 2.2486496242152352e-05,
      "loss": 1.9508,
      "step": 853700
    },
    {
      "epoch": 33.020071934099086,
      "grad_norm": 11.938828468322754,
      "learning_rate": 2.2483273388250763e-05,
      "loss": 1.8283,
      "step": 853800
    },
    {
      "epoch": 33.023939358780986,
      "grad_norm": 11.122705459594727,
      "learning_rate": 2.2480050534349178e-05,
      "loss": 1.7382,
      "step": 853900
    },
    {
      "epoch": 33.02780678346289,
      "grad_norm": 13.272643089294434,
      "learning_rate": 2.247682768044759e-05,
      "loss": 1.9074,
      "step": 854000
    },
    {
      "epoch": 33.0316742081448,
      "grad_norm": 10.482690811157227,
      "learning_rate": 2.2473604826546004e-05,
      "loss": 1.8553,
      "step": 854100
    },
    {
      "epoch": 33.0355416328267,
      "grad_norm": 10.786133766174316,
      "learning_rate": 2.2470381972644415e-05,
      "loss": 1.8872,
      "step": 854200
    },
    {
      "epoch": 33.03940905750861,
      "grad_norm": 11.249040603637695,
      "learning_rate": 2.246715911874283e-05,
      "loss": 1.8776,
      "step": 854300
    },
    {
      "epoch": 33.04327648219051,
      "grad_norm": 12.736716270446777,
      "learning_rate": 2.246393626484124e-05,
      "loss": 1.872,
      "step": 854400
    },
    {
      "epoch": 33.047143906872414,
      "grad_norm": 9.797727584838867,
      "learning_rate": 2.2460713410939656e-05,
      "loss": 1.7983,
      "step": 854500
    },
    {
      "epoch": 33.05101133155432,
      "grad_norm": 14.664770126342773,
      "learning_rate": 2.2457490557038067e-05,
      "loss": 1.7974,
      "step": 854600
    },
    {
      "epoch": 33.05487875623622,
      "grad_norm": 11.295516967773438,
      "learning_rate": 2.2454267703136482e-05,
      "loss": 1.8717,
      "step": 854700
    },
    {
      "epoch": 33.05874618091813,
      "grad_norm": 10.796195983886719,
      "learning_rate": 2.2451044849234893e-05,
      "loss": 1.8345,
      "step": 854800
    },
    {
      "epoch": 33.06261360560003,
      "grad_norm": 14.317961692810059,
      "learning_rate": 2.2447821995333308e-05,
      "loss": 1.7586,
      "step": 854900
    },
    {
      "epoch": 33.066481030281935,
      "grad_norm": 12.55612850189209,
      "learning_rate": 2.2444599141431723e-05,
      "loss": 1.8159,
      "step": 855000
    },
    {
      "epoch": 33.07034845496384,
      "grad_norm": 9.329404830932617,
      "learning_rate": 2.2441376287530134e-05,
      "loss": 1.7756,
      "step": 855100
    },
    {
      "epoch": 33.07421587964574,
      "grad_norm": 10.377363204956055,
      "learning_rate": 2.243815343362855e-05,
      "loss": 1.8994,
      "step": 855200
    },
    {
      "epoch": 33.07808330432765,
      "grad_norm": 14.797775268554688,
      "learning_rate": 2.2434930579726964e-05,
      "loss": 1.9163,
      "step": 855300
    },
    {
      "epoch": 33.081950729009556,
      "grad_norm": 9.606732368469238,
      "learning_rate": 2.2431707725825375e-05,
      "loss": 1.7812,
      "step": 855400
    },
    {
      "epoch": 33.085818153691456,
      "grad_norm": 11.701272964477539,
      "learning_rate": 2.242848487192379e-05,
      "loss": 1.8163,
      "step": 855500
    },
    {
      "epoch": 33.08968557837336,
      "grad_norm": 8.99434757232666,
      "learning_rate": 2.24252620180222e-05,
      "loss": 1.7786,
      "step": 855600
    },
    {
      "epoch": 33.09355300305526,
      "grad_norm": 9.679764747619629,
      "learning_rate": 2.2422039164120612e-05,
      "loss": 1.8356,
      "step": 855700
    },
    {
      "epoch": 33.09742042773717,
      "grad_norm": 14.203352928161621,
      "learning_rate": 2.2418816310219027e-05,
      "loss": 1.8246,
      "step": 855800
    },
    {
      "epoch": 33.10128785241908,
      "grad_norm": 12.313100814819336,
      "learning_rate": 2.241559345631744e-05,
      "loss": 1.8499,
      "step": 855900
    },
    {
      "epoch": 33.10515527710098,
      "grad_norm": 14.000387191772461,
      "learning_rate": 2.2412370602415853e-05,
      "loss": 1.8466,
      "step": 856000
    },
    {
      "epoch": 33.109022701782884,
      "grad_norm": 10.646796226501465,
      "learning_rate": 2.2409147748514264e-05,
      "loss": 1.9219,
      "step": 856100
    },
    {
      "epoch": 33.112890126464784,
      "grad_norm": 13.674175262451172,
      "learning_rate": 2.240592489461268e-05,
      "loss": 1.8524,
      "step": 856200
    },
    {
      "epoch": 33.11675755114669,
      "grad_norm": 12.629035949707031,
      "learning_rate": 2.240270204071109e-05,
      "loss": 1.8894,
      "step": 856300
    },
    {
      "epoch": 33.1206249758286,
      "grad_norm": 9.310312271118164,
      "learning_rate": 2.2399479186809505e-05,
      "loss": 1.8854,
      "step": 856400
    },
    {
      "epoch": 33.1244924005105,
      "grad_norm": 14.497710227966309,
      "learning_rate": 2.2396256332907917e-05,
      "loss": 1.8963,
      "step": 856500
    },
    {
      "epoch": 33.128359825192405,
      "grad_norm": 10.558005332946777,
      "learning_rate": 2.239303347900633e-05,
      "loss": 1.9294,
      "step": 856600
    },
    {
      "epoch": 33.13222724987431,
      "grad_norm": 14.267230033874512,
      "learning_rate": 2.2389810625104743e-05,
      "loss": 1.903,
      "step": 856700
    },
    {
      "epoch": 33.13609467455621,
      "grad_norm": 12.847127914428711,
      "learning_rate": 2.2386587771203157e-05,
      "loss": 1.8563,
      "step": 856800
    },
    {
      "epoch": 33.13996209923812,
      "grad_norm": 9.864863395690918,
      "learning_rate": 2.238336491730157e-05,
      "loss": 1.8349,
      "step": 856900
    },
    {
      "epoch": 33.14382952392002,
      "grad_norm": 12.737075805664062,
      "learning_rate": 2.2380142063399983e-05,
      "loss": 1.7978,
      "step": 857000
    },
    {
      "epoch": 33.147696948601926,
      "grad_norm": 9.498811721801758,
      "learning_rate": 2.2376919209498395e-05,
      "loss": 1.8479,
      "step": 857100
    },
    {
      "epoch": 33.15156437328383,
      "grad_norm": 12.541888236999512,
      "learning_rate": 2.237369635559681e-05,
      "loss": 1.9112,
      "step": 857200
    },
    {
      "epoch": 33.15543179796573,
      "grad_norm": 11.81589126586914,
      "learning_rate": 2.237047350169522e-05,
      "loss": 1.8519,
      "step": 857300
    },
    {
      "epoch": 33.15929922264764,
      "grad_norm": 9.621159553527832,
      "learning_rate": 2.2367250647793635e-05,
      "loss": 1.9552,
      "step": 857400
    },
    {
      "epoch": 33.16316664732955,
      "grad_norm": 10.841425895690918,
      "learning_rate": 2.2364027793892047e-05,
      "loss": 1.8644,
      "step": 857500
    },
    {
      "epoch": 33.16703407201145,
      "grad_norm": 9.816450119018555,
      "learning_rate": 2.236080493999046e-05,
      "loss": 1.9006,
      "step": 857600
    },
    {
      "epoch": 33.170901496693354,
      "grad_norm": 11.851835250854492,
      "learning_rate": 2.2357582086088873e-05,
      "loss": 1.832,
      "step": 857700
    },
    {
      "epoch": 33.174768921375254,
      "grad_norm": 12.490407943725586,
      "learning_rate": 2.2354359232187288e-05,
      "loss": 1.931,
      "step": 857800
    },
    {
      "epoch": 33.17863634605716,
      "grad_norm": 14.363651275634766,
      "learning_rate": 2.23511363782857e-05,
      "loss": 1.8614,
      "step": 857900
    },
    {
      "epoch": 33.18250377073907,
      "grad_norm": 12.2616605758667,
      "learning_rate": 2.2347913524384114e-05,
      "loss": 1.9089,
      "step": 858000
    },
    {
      "epoch": 33.18637119542097,
      "grad_norm": 12.067200660705566,
      "learning_rate": 2.2344690670482525e-05,
      "loss": 1.8507,
      "step": 858100
    },
    {
      "epoch": 33.190238620102875,
      "grad_norm": 11.002986907958984,
      "learning_rate": 2.234146781658094e-05,
      "loss": 1.7966,
      "step": 858200
    },
    {
      "epoch": 33.194106044784775,
      "grad_norm": 15.036032676696777,
      "learning_rate": 2.2338244962679354e-05,
      "loss": 1.8392,
      "step": 858300
    },
    {
      "epoch": 33.19797346946668,
      "grad_norm": 11.118303298950195,
      "learning_rate": 2.2335022108777766e-05,
      "loss": 1.8706,
      "step": 858400
    },
    {
      "epoch": 33.20184089414859,
      "grad_norm": 13.26802921295166,
      "learning_rate": 2.233179925487618e-05,
      "loss": 1.8471,
      "step": 858500
    },
    {
      "epoch": 33.20570831883049,
      "grad_norm": 12.219799995422363,
      "learning_rate": 2.2328576400974592e-05,
      "loss": 1.7686,
      "step": 858600
    },
    {
      "epoch": 33.209575743512396,
      "grad_norm": 13.474349975585938,
      "learning_rate": 2.2325353547073007e-05,
      "loss": 1.8692,
      "step": 858700
    },
    {
      "epoch": 33.2134431681943,
      "grad_norm": 13.279043197631836,
      "learning_rate": 2.2322130693171418e-05,
      "loss": 1.8637,
      "step": 858800
    },
    {
      "epoch": 33.2173105928762,
      "grad_norm": 11.567763328552246,
      "learning_rate": 2.2318907839269833e-05,
      "loss": 1.8793,
      "step": 858900
    },
    {
      "epoch": 33.22117801755811,
      "grad_norm": 12.74454116821289,
      "learning_rate": 2.2315684985368244e-05,
      "loss": 1.7699,
      "step": 859000
    },
    {
      "epoch": 33.22504544224001,
      "grad_norm": 10.555370330810547,
      "learning_rate": 2.231246213146666e-05,
      "loss": 1.8202,
      "step": 859100
    },
    {
      "epoch": 33.22891286692192,
      "grad_norm": 11.323514938354492,
      "learning_rate": 2.230923927756507e-05,
      "loss": 1.8318,
      "step": 859200
    },
    {
      "epoch": 33.232780291603824,
      "grad_norm": 14.100374221801758,
      "learning_rate": 2.2306016423663485e-05,
      "loss": 1.9015,
      "step": 859300
    },
    {
      "epoch": 33.236647716285724,
      "grad_norm": 11.430225372314453,
      "learning_rate": 2.2302793569761896e-05,
      "loss": 1.938,
      "step": 859400
    },
    {
      "epoch": 33.24051514096763,
      "grad_norm": 9.617488861083984,
      "learning_rate": 2.229957071586031e-05,
      "loss": 1.8451,
      "step": 859500
    },
    {
      "epoch": 33.24438256564953,
      "grad_norm": 9.980051040649414,
      "learning_rate": 2.2296347861958722e-05,
      "loss": 1.8139,
      "step": 859600
    },
    {
      "epoch": 33.24824999033144,
      "grad_norm": 19.170291900634766,
      "learning_rate": 2.2293125008057137e-05,
      "loss": 1.8282,
      "step": 859700
    },
    {
      "epoch": 33.252117415013345,
      "grad_norm": 11.051315307617188,
      "learning_rate": 2.2289902154155548e-05,
      "loss": 1.7274,
      "step": 859800
    },
    {
      "epoch": 33.255984839695245,
      "grad_norm": 19.365089416503906,
      "learning_rate": 2.2286679300253963e-05,
      "loss": 1.814,
      "step": 859900
    },
    {
      "epoch": 33.25985226437715,
      "grad_norm": 11.038067817687988,
      "learning_rate": 2.2283456446352374e-05,
      "loss": 1.9025,
      "step": 860000
    },
    {
      "epoch": 33.26371968905906,
      "grad_norm": 11.026026725769043,
      "learning_rate": 2.228023359245079e-05,
      "loss": 1.9785,
      "step": 860100
    },
    {
      "epoch": 33.26758711374096,
      "grad_norm": 11.250690460205078,
      "learning_rate": 2.22770107385492e-05,
      "loss": 1.9508,
      "step": 860200
    },
    {
      "epoch": 33.271454538422866,
      "grad_norm": 11.360442161560059,
      "learning_rate": 2.2273787884647615e-05,
      "loss": 1.9068,
      "step": 860300
    },
    {
      "epoch": 33.275321963104766,
      "grad_norm": 18.34048843383789,
      "learning_rate": 2.2270565030746026e-05,
      "loss": 1.7287,
      "step": 860400
    },
    {
      "epoch": 33.27918938778667,
      "grad_norm": 12.032814025878906,
      "learning_rate": 2.226734217684444e-05,
      "loss": 1.8593,
      "step": 860500
    },
    {
      "epoch": 33.28305681246858,
      "grad_norm": 10.544261932373047,
      "learning_rate": 2.2264119322942852e-05,
      "loss": 1.8329,
      "step": 860600
    },
    {
      "epoch": 33.28692423715048,
      "grad_norm": 13.414013862609863,
      "learning_rate": 2.2260896469041267e-05,
      "loss": 1.9204,
      "step": 860700
    },
    {
      "epoch": 33.29079166183239,
      "grad_norm": 10.492585182189941,
      "learning_rate": 2.225767361513968e-05,
      "loss": 1.7578,
      "step": 860800
    },
    {
      "epoch": 33.29465908651429,
      "grad_norm": 9.81793212890625,
      "learning_rate": 2.2254450761238093e-05,
      "loss": 1.9244,
      "step": 860900
    },
    {
      "epoch": 33.298526511196194,
      "grad_norm": 13.357290267944336,
      "learning_rate": 2.2251227907336504e-05,
      "loss": 1.7818,
      "step": 861000
    },
    {
      "epoch": 33.3023939358781,
      "grad_norm": 12.967463493347168,
      "learning_rate": 2.224800505343492e-05,
      "loss": 1.8286,
      "step": 861100
    },
    {
      "epoch": 33.30626136056,
      "grad_norm": 10.970437049865723,
      "learning_rate": 2.224478219953333e-05,
      "loss": 1.8592,
      "step": 861200
    },
    {
      "epoch": 33.31012878524191,
      "grad_norm": 15.015172004699707,
      "learning_rate": 2.2241559345631745e-05,
      "loss": 1.8456,
      "step": 861300
    },
    {
      "epoch": 33.313996209923815,
      "grad_norm": 14.595611572265625,
      "learning_rate": 2.2238336491730156e-05,
      "loss": 1.9334,
      "step": 861400
    },
    {
      "epoch": 33.317863634605715,
      "grad_norm": 12.600046157836914,
      "learning_rate": 2.223511363782857e-05,
      "loss": 1.8137,
      "step": 861500
    },
    {
      "epoch": 33.32173105928762,
      "grad_norm": 10.367110252380371,
      "learning_rate": 2.2231890783926983e-05,
      "loss": 1.8788,
      "step": 861600
    },
    {
      "epoch": 33.32559848396952,
      "grad_norm": 11.505471229553223,
      "learning_rate": 2.2228667930025397e-05,
      "loss": 1.7494,
      "step": 861700
    },
    {
      "epoch": 33.32946590865143,
      "grad_norm": 15.088942527770996,
      "learning_rate": 2.2225445076123812e-05,
      "loss": 1.7905,
      "step": 861800
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 9.346731185913086,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 1.9031,
      "step": 861900
    },
    {
      "epoch": 33.337200758015236,
      "grad_norm": 10.539432525634766,
      "learning_rate": 2.2218999368320638e-05,
      "loss": 1.8671,
      "step": 862000
    },
    {
      "epoch": 33.34106818269714,
      "grad_norm": 10.69544506072998,
      "learning_rate": 2.221577651441905e-05,
      "loss": 1.8119,
      "step": 862100
    },
    {
      "epoch": 33.34493560737905,
      "grad_norm": 15.882070541381836,
      "learning_rate": 2.2212553660517464e-05,
      "loss": 1.7719,
      "step": 862200
    },
    {
      "epoch": 33.34880303206095,
      "grad_norm": 11.768976211547852,
      "learning_rate": 2.2209330806615875e-05,
      "loss": 1.8529,
      "step": 862300
    },
    {
      "epoch": 33.35267045674286,
      "grad_norm": 12.147930145263672,
      "learning_rate": 2.220610795271429e-05,
      "loss": 1.9507,
      "step": 862400
    },
    {
      "epoch": 33.35653788142476,
      "grad_norm": 12.320168495178223,
      "learning_rate": 2.22028850988127e-05,
      "loss": 1.8922,
      "step": 862500
    },
    {
      "epoch": 33.360405306106664,
      "grad_norm": 11.000758171081543,
      "learning_rate": 2.2199662244911116e-05,
      "loss": 1.8869,
      "step": 862600
    },
    {
      "epoch": 33.36427273078857,
      "grad_norm": 10.559410095214844,
      "learning_rate": 2.2196439391009527e-05,
      "loss": 1.8899,
      "step": 862700
    },
    {
      "epoch": 33.36814015547047,
      "grad_norm": 10.77810287475586,
      "learning_rate": 2.2193216537107942e-05,
      "loss": 1.8408,
      "step": 862800
    },
    {
      "epoch": 33.37200758015238,
      "grad_norm": 9.4273099899292,
      "learning_rate": 2.2189993683206354e-05,
      "loss": 1.862,
      "step": 862900
    },
    {
      "epoch": 33.37587500483428,
      "grad_norm": 13.533407211303711,
      "learning_rate": 2.2186770829304768e-05,
      "loss": 1.7111,
      "step": 863000
    },
    {
      "epoch": 33.379742429516185,
      "grad_norm": 11.80001163482666,
      "learning_rate": 2.218354797540318e-05,
      "loss": 1.8552,
      "step": 863100
    },
    {
      "epoch": 33.38360985419809,
      "grad_norm": 12.850044250488281,
      "learning_rate": 2.2180325121501594e-05,
      "loss": 1.8542,
      "step": 863200
    },
    {
      "epoch": 33.38747727887999,
      "grad_norm": 9.213116645812988,
      "learning_rate": 2.2177102267600006e-05,
      "loss": 1.8656,
      "step": 863300
    },
    {
      "epoch": 33.3913447035619,
      "grad_norm": 11.356931686401367,
      "learning_rate": 2.217387941369842e-05,
      "loss": 1.881,
      "step": 863400
    },
    {
      "epoch": 33.395212128243806,
      "grad_norm": 8.725515365600586,
      "learning_rate": 2.217065655979683e-05,
      "loss": 1.8739,
      "step": 863500
    },
    {
      "epoch": 33.399079552925706,
      "grad_norm": 8.593168258666992,
      "learning_rate": 2.2167433705895246e-05,
      "loss": 1.9616,
      "step": 863600
    },
    {
      "epoch": 33.40294697760761,
      "grad_norm": 11.972530364990234,
      "learning_rate": 2.2164210851993658e-05,
      "loss": 1.8634,
      "step": 863700
    },
    {
      "epoch": 33.40681440228951,
      "grad_norm": 9.960302352905273,
      "learning_rate": 2.2160987998092072e-05,
      "loss": 1.8966,
      "step": 863800
    },
    {
      "epoch": 33.41068182697142,
      "grad_norm": 12.668540000915527,
      "learning_rate": 2.2157765144190484e-05,
      "loss": 1.8827,
      "step": 863900
    },
    {
      "epoch": 33.41454925165333,
      "grad_norm": 10.167000770568848,
      "learning_rate": 2.21545422902889e-05,
      "loss": 1.8382,
      "step": 864000
    },
    {
      "epoch": 33.41841667633523,
      "grad_norm": 9.18332290649414,
      "learning_rate": 2.215131943638731e-05,
      "loss": 1.7909,
      "step": 864100
    },
    {
      "epoch": 33.422284101017134,
      "grad_norm": 12.816841125488281,
      "learning_rate": 2.2148096582485725e-05,
      "loss": 1.8566,
      "step": 864200
    },
    {
      "epoch": 33.426151525699034,
      "grad_norm": 13.340557098388672,
      "learning_rate": 2.2144873728584136e-05,
      "loss": 1.8033,
      "step": 864300
    },
    {
      "epoch": 33.43001895038094,
      "grad_norm": 14.141283988952637,
      "learning_rate": 2.214165087468255e-05,
      "loss": 1.9312,
      "step": 864400
    },
    {
      "epoch": 33.43388637506285,
      "grad_norm": 11.63794231414795,
      "learning_rate": 2.2138428020780962e-05,
      "loss": 1.8485,
      "step": 864500
    },
    {
      "epoch": 33.43775379974475,
      "grad_norm": 16.2039794921875,
      "learning_rate": 2.2135205166879373e-05,
      "loss": 1.8986,
      "step": 864600
    },
    {
      "epoch": 33.441621224426655,
      "grad_norm": 12.334734916687012,
      "learning_rate": 2.2131982312977788e-05,
      "loss": 1.8352,
      "step": 864700
    },
    {
      "epoch": 33.44548864910856,
      "grad_norm": 8.932344436645508,
      "learning_rate": 2.21287594590762e-05,
      "loss": 1.9064,
      "step": 864800
    },
    {
      "epoch": 33.44935607379046,
      "grad_norm": 12.7672119140625,
      "learning_rate": 2.2125536605174614e-05,
      "loss": 1.9139,
      "step": 864900
    },
    {
      "epoch": 33.45322349847237,
      "grad_norm": 13.642722129821777,
      "learning_rate": 2.212231375127303e-05,
      "loss": 1.7564,
      "step": 865000
    },
    {
      "epoch": 33.45709092315427,
      "grad_norm": 13.99402904510498,
      "learning_rate": 2.211909089737144e-05,
      "loss": 1.8263,
      "step": 865100
    },
    {
      "epoch": 33.460958347836176,
      "grad_norm": 11.047402381896973,
      "learning_rate": 2.2115868043469855e-05,
      "loss": 1.804,
      "step": 865200
    },
    {
      "epoch": 33.46482577251808,
      "grad_norm": 7.478945732116699,
      "learning_rate": 2.211264518956827e-05,
      "loss": 1.8455,
      "step": 865300
    },
    {
      "epoch": 33.46869319719998,
      "grad_norm": 10.546419143676758,
      "learning_rate": 2.210942233566668e-05,
      "loss": 1.8841,
      "step": 865400
    },
    {
      "epoch": 33.47256062188189,
      "grad_norm": 12.339096069335938,
      "learning_rate": 2.2106199481765096e-05,
      "loss": 1.8089,
      "step": 865500
    },
    {
      "epoch": 33.4764280465638,
      "grad_norm": 13.703678131103516,
      "learning_rate": 2.2102976627863507e-05,
      "loss": 1.9022,
      "step": 865600
    },
    {
      "epoch": 33.4802954712457,
      "grad_norm": 11.327245712280273,
      "learning_rate": 2.209975377396192e-05,
      "loss": 1.8173,
      "step": 865700
    },
    {
      "epoch": 33.484162895927604,
      "grad_norm": 10.097602844238281,
      "learning_rate": 2.2096530920060333e-05,
      "loss": 1.849,
      "step": 865800
    },
    {
      "epoch": 33.488030320609504,
      "grad_norm": 10.906631469726562,
      "learning_rate": 2.2093308066158748e-05,
      "loss": 1.831,
      "step": 865900
    },
    {
      "epoch": 33.49189774529141,
      "grad_norm": 17.266315460205078,
      "learning_rate": 2.209008521225716e-05,
      "loss": 1.8482,
      "step": 866000
    },
    {
      "epoch": 33.49576516997332,
      "grad_norm": 9.6152925491333,
      "learning_rate": 2.2086862358355574e-05,
      "loss": 1.7487,
      "step": 866100
    },
    {
      "epoch": 33.49963259465522,
      "grad_norm": 10.935569763183594,
      "learning_rate": 2.2083639504453985e-05,
      "loss": 1.9854,
      "step": 866200
    },
    {
      "epoch": 33.503500019337125,
      "grad_norm": 11.153402328491211,
      "learning_rate": 2.20804166505524e-05,
      "loss": 1.8379,
      "step": 866300
    },
    {
      "epoch": 33.507367444019025,
      "grad_norm": 10.444140434265137,
      "learning_rate": 2.207719379665081e-05,
      "loss": 1.8515,
      "step": 866400
    },
    {
      "epoch": 33.51123486870093,
      "grad_norm": 14.494661331176758,
      "learning_rate": 2.2073970942749226e-05,
      "loss": 1.8524,
      "step": 866500
    },
    {
      "epoch": 33.51510229338284,
      "grad_norm": 10.160222053527832,
      "learning_rate": 2.2070748088847637e-05,
      "loss": 1.9162,
      "step": 866600
    },
    {
      "epoch": 33.51896971806474,
      "grad_norm": 10.525588989257812,
      "learning_rate": 2.2067525234946052e-05,
      "loss": 1.8444,
      "step": 866700
    },
    {
      "epoch": 33.522837142746646,
      "grad_norm": 15.057894706726074,
      "learning_rate": 2.2064302381044463e-05,
      "loss": 1.9039,
      "step": 866800
    },
    {
      "epoch": 33.52670456742855,
      "grad_norm": 12.220562934875488,
      "learning_rate": 2.2061079527142878e-05,
      "loss": 1.8355,
      "step": 866900
    },
    {
      "epoch": 33.53057199211045,
      "grad_norm": 12.974777221679688,
      "learning_rate": 2.205785667324129e-05,
      "loss": 1.8661,
      "step": 867000
    },
    {
      "epoch": 33.53443941679236,
      "grad_norm": 12.84245491027832,
      "learning_rate": 2.2054633819339704e-05,
      "loss": 1.8546,
      "step": 867100
    },
    {
      "epoch": 33.53830684147426,
      "grad_norm": 12.166036605834961,
      "learning_rate": 2.2051410965438115e-05,
      "loss": 1.8382,
      "step": 867200
    },
    {
      "epoch": 33.54217426615617,
      "grad_norm": 10.811734199523926,
      "learning_rate": 2.204818811153653e-05,
      "loss": 1.8856,
      "step": 867300
    },
    {
      "epoch": 33.546041690838074,
      "grad_norm": 11.1364164352417,
      "learning_rate": 2.204496525763494e-05,
      "loss": 1.785,
      "step": 867400
    },
    {
      "epoch": 33.549909115519974,
      "grad_norm": 12.84260368347168,
      "learning_rate": 2.2041742403733353e-05,
      "loss": 1.7912,
      "step": 867500
    },
    {
      "epoch": 33.55377654020188,
      "grad_norm": 19.260923385620117,
      "learning_rate": 2.2038519549831767e-05,
      "loss": 1.8262,
      "step": 867600
    },
    {
      "epoch": 33.55764396488378,
      "grad_norm": 11.312217712402344,
      "learning_rate": 2.203529669593018e-05,
      "loss": 1.9111,
      "step": 867700
    },
    {
      "epoch": 33.56151138956569,
      "grad_norm": 10.841410636901855,
      "learning_rate": 2.2032073842028593e-05,
      "loss": 1.969,
      "step": 867800
    },
    {
      "epoch": 33.565378814247595,
      "grad_norm": 11.426132202148438,
      "learning_rate": 2.2028850988127005e-05,
      "loss": 1.8647,
      "step": 867900
    },
    {
      "epoch": 33.569246238929495,
      "grad_norm": 11.041176795959473,
      "learning_rate": 2.202562813422542e-05,
      "loss": 1.9233,
      "step": 868000
    },
    {
      "epoch": 33.5731136636114,
      "grad_norm": 13.979291915893555,
      "learning_rate": 2.202240528032383e-05,
      "loss": 1.8556,
      "step": 868100
    },
    {
      "epoch": 33.57698108829331,
      "grad_norm": 14.314728736877441,
      "learning_rate": 2.2019182426422246e-05,
      "loss": 1.8988,
      "step": 868200
    },
    {
      "epoch": 33.58084851297521,
      "grad_norm": 11.902193069458008,
      "learning_rate": 2.201595957252066e-05,
      "loss": 1.8496,
      "step": 868300
    },
    {
      "epoch": 33.584715937657116,
      "grad_norm": 10.74188232421875,
      "learning_rate": 2.201273671861907e-05,
      "loss": 1.9464,
      "step": 868400
    },
    {
      "epoch": 33.588583362339016,
      "grad_norm": 12.772268295288086,
      "learning_rate": 2.2009513864717486e-05,
      "loss": 1.963,
      "step": 868500
    },
    {
      "epoch": 33.59245078702092,
      "grad_norm": 10.456335067749023,
      "learning_rate": 2.2006291010815898e-05,
      "loss": 1.8744,
      "step": 868600
    },
    {
      "epoch": 33.59631821170283,
      "grad_norm": 12.696611404418945,
      "learning_rate": 2.2003068156914312e-05,
      "loss": 1.915,
      "step": 868700
    },
    {
      "epoch": 33.60018563638473,
      "grad_norm": 10.225971221923828,
      "learning_rate": 2.1999845303012727e-05,
      "loss": 1.8137,
      "step": 868800
    },
    {
      "epoch": 33.60405306106664,
      "grad_norm": 9.438130378723145,
      "learning_rate": 2.199662244911114e-05,
      "loss": 1.8685,
      "step": 868900
    },
    {
      "epoch": 33.60792048574854,
      "grad_norm": 14.267736434936523,
      "learning_rate": 2.1993399595209553e-05,
      "loss": 1.9746,
      "step": 869000
    },
    {
      "epoch": 33.611787910430444,
      "grad_norm": 10.497809410095215,
      "learning_rate": 2.1990176741307964e-05,
      "loss": 1.8891,
      "step": 869100
    },
    {
      "epoch": 33.61565533511235,
      "grad_norm": 12.984041213989258,
      "learning_rate": 2.198695388740638e-05,
      "loss": 1.7825,
      "step": 869200
    },
    {
      "epoch": 33.61952275979425,
      "grad_norm": 14.041789054870605,
      "learning_rate": 2.198373103350479e-05,
      "loss": 1.8377,
      "step": 869300
    },
    {
      "epoch": 33.62339018447616,
      "grad_norm": 15.68082332611084,
      "learning_rate": 2.1980508179603205e-05,
      "loss": 1.8812,
      "step": 869400
    },
    {
      "epoch": 33.627257609158065,
      "grad_norm": 13.496833801269531,
      "learning_rate": 2.1977285325701617e-05,
      "loss": 1.8768,
      "step": 869500
    },
    {
      "epoch": 33.631125033839965,
      "grad_norm": 8.027429580688477,
      "learning_rate": 2.197406247180003e-05,
      "loss": 1.8704,
      "step": 869600
    },
    {
      "epoch": 33.63499245852187,
      "grad_norm": 10.907512664794922,
      "learning_rate": 2.1970839617898443e-05,
      "loss": 1.7962,
      "step": 869700
    },
    {
      "epoch": 33.63885988320377,
      "grad_norm": 8.795303344726562,
      "learning_rate": 2.1967616763996857e-05,
      "loss": 1.8822,
      "step": 869800
    },
    {
      "epoch": 33.64272730788568,
      "grad_norm": 10.332427978515625,
      "learning_rate": 2.196439391009527e-05,
      "loss": 1.8347,
      "step": 869900
    },
    {
      "epoch": 33.646594732567586,
      "grad_norm": 12.855438232421875,
      "learning_rate": 2.1961171056193683e-05,
      "loss": 1.8677,
      "step": 870000
    },
    {
      "epoch": 33.650462157249486,
      "grad_norm": 9.220943450927734,
      "learning_rate": 2.1957948202292095e-05,
      "loss": 1.8542,
      "step": 870100
    },
    {
      "epoch": 33.65432958193139,
      "grad_norm": 14.952641487121582,
      "learning_rate": 2.195472534839051e-05,
      "loss": 1.8502,
      "step": 870200
    },
    {
      "epoch": 33.6581970066133,
      "grad_norm": 11.147377967834473,
      "learning_rate": 2.195150249448892e-05,
      "loss": 1.9171,
      "step": 870300
    },
    {
      "epoch": 33.6620644312952,
      "grad_norm": 11.014644622802734,
      "learning_rate": 2.1948279640587335e-05,
      "loss": 1.9624,
      "step": 870400
    },
    {
      "epoch": 33.66593185597711,
      "grad_norm": 12.053112983703613,
      "learning_rate": 2.1945056786685747e-05,
      "loss": 1.8553,
      "step": 870500
    },
    {
      "epoch": 33.66979928065901,
      "grad_norm": 9.728918075561523,
      "learning_rate": 2.1941833932784158e-05,
      "loss": 1.8646,
      "step": 870600
    },
    {
      "epoch": 33.67366670534091,
      "grad_norm": 14.258893966674805,
      "learning_rate": 2.1938611078882573e-05,
      "loss": 1.8324,
      "step": 870700
    },
    {
      "epoch": 33.67753413002282,
      "grad_norm": 12.364733695983887,
      "learning_rate": 2.1935388224980984e-05,
      "loss": 1.8357,
      "step": 870800
    },
    {
      "epoch": 33.68140155470472,
      "grad_norm": 11.374072074890137,
      "learning_rate": 2.19321653710794e-05,
      "loss": 1.8656,
      "step": 870900
    },
    {
      "epoch": 33.68526897938663,
      "grad_norm": 9.894594192504883,
      "learning_rate": 2.192894251717781e-05,
      "loss": 1.8729,
      "step": 871000
    },
    {
      "epoch": 33.68913640406853,
      "grad_norm": 14.849161148071289,
      "learning_rate": 2.1925719663276225e-05,
      "loss": 1.8946,
      "step": 871100
    },
    {
      "epoch": 33.693003828750435,
      "grad_norm": 10.092083930969238,
      "learning_rate": 2.1922496809374636e-05,
      "loss": 1.8336,
      "step": 871200
    },
    {
      "epoch": 33.69687125343234,
      "grad_norm": 10.974486351013184,
      "learning_rate": 2.191927395547305e-05,
      "loss": 1.8317,
      "step": 871300
    },
    {
      "epoch": 33.70073867811424,
      "grad_norm": 13.43764877319336,
      "learning_rate": 2.1916051101571462e-05,
      "loss": 1.9184,
      "step": 871400
    },
    {
      "epoch": 33.70460610279615,
      "grad_norm": 15.920909881591797,
      "learning_rate": 2.1912828247669877e-05,
      "loss": 1.8238,
      "step": 871500
    },
    {
      "epoch": 33.708473527478056,
      "grad_norm": 16.485450744628906,
      "learning_rate": 2.190960539376829e-05,
      "loss": 1.8125,
      "step": 871600
    },
    {
      "epoch": 33.712340952159956,
      "grad_norm": 11.68247127532959,
      "learning_rate": 2.1906382539866703e-05,
      "loss": 1.8675,
      "step": 871700
    },
    {
      "epoch": 33.71620837684186,
      "grad_norm": 15.471977233886719,
      "learning_rate": 2.1903159685965118e-05,
      "loss": 1.8568,
      "step": 871800
    },
    {
      "epoch": 33.72007580152376,
      "grad_norm": 10.562749862670898,
      "learning_rate": 2.189993683206353e-05,
      "loss": 1.8085,
      "step": 871900
    },
    {
      "epoch": 33.72394322620567,
      "grad_norm": 11.799083709716797,
      "learning_rate": 2.1896713978161944e-05,
      "loss": 1.8599,
      "step": 872000
    },
    {
      "epoch": 33.72781065088758,
      "grad_norm": 14.84526538848877,
      "learning_rate": 2.189349112426036e-05,
      "loss": 1.8578,
      "step": 872100
    },
    {
      "epoch": 33.73167807556948,
      "grad_norm": 11.897785186767578,
      "learning_rate": 2.189026827035877e-05,
      "loss": 1.7871,
      "step": 872200
    },
    {
      "epoch": 33.73554550025138,
      "grad_norm": 13.043200492858887,
      "learning_rate": 2.1887045416457185e-05,
      "loss": 1.8558,
      "step": 872300
    },
    {
      "epoch": 33.73941292493328,
      "grad_norm": 10.420439720153809,
      "learning_rate": 2.1883822562555596e-05,
      "loss": 1.8375,
      "step": 872400
    },
    {
      "epoch": 33.74328034961519,
      "grad_norm": 11.173619270324707,
      "learning_rate": 2.188059970865401e-05,
      "loss": 1.8705,
      "step": 872500
    },
    {
      "epoch": 33.7471477742971,
      "grad_norm": 11.854043006896973,
      "learning_rate": 2.1877376854752422e-05,
      "loss": 1.9387,
      "step": 872600
    },
    {
      "epoch": 33.751015198979,
      "grad_norm": 10.363155364990234,
      "learning_rate": 2.1874154000850837e-05,
      "loss": 1.901,
      "step": 872700
    },
    {
      "epoch": 33.754882623660905,
      "grad_norm": 13.893210411071777,
      "learning_rate": 2.1870931146949248e-05,
      "loss": 1.7779,
      "step": 872800
    },
    {
      "epoch": 33.75875004834281,
      "grad_norm": 10.006935119628906,
      "learning_rate": 2.1867708293047663e-05,
      "loss": 1.8297,
      "step": 872900
    },
    {
      "epoch": 33.76261747302471,
      "grad_norm": 9.387866020202637,
      "learning_rate": 2.1864485439146074e-05,
      "loss": 1.8872,
      "step": 873000
    },
    {
      "epoch": 33.76648489770662,
      "grad_norm": 12.44163990020752,
      "learning_rate": 2.186126258524449e-05,
      "loss": 1.8639,
      "step": 873100
    },
    {
      "epoch": 33.77035232238852,
      "grad_norm": 6.398244857788086,
      "learning_rate": 2.18580397313429e-05,
      "loss": 1.8498,
      "step": 873200
    },
    {
      "epoch": 33.774219747070426,
      "grad_norm": 10.193658828735352,
      "learning_rate": 2.1854816877441315e-05,
      "loss": 1.9383,
      "step": 873300
    },
    {
      "epoch": 33.77808717175233,
      "grad_norm": 14.524178504943848,
      "learning_rate": 2.1851594023539726e-05,
      "loss": 1.8694,
      "step": 873400
    },
    {
      "epoch": 33.78195459643423,
      "grad_norm": 14.646552085876465,
      "learning_rate": 2.1848371169638137e-05,
      "loss": 1.9187,
      "step": 873500
    },
    {
      "epoch": 33.78582202111614,
      "grad_norm": 14.231000900268555,
      "learning_rate": 2.1845148315736552e-05,
      "loss": 1.8664,
      "step": 873600
    },
    {
      "epoch": 33.78968944579805,
      "grad_norm": 9.099173545837402,
      "learning_rate": 2.1841925461834964e-05,
      "loss": 1.8702,
      "step": 873700
    },
    {
      "epoch": 33.79355687047995,
      "grad_norm": 13.441252708435059,
      "learning_rate": 2.1838702607933378e-05,
      "loss": 1.8233,
      "step": 873800
    },
    {
      "epoch": 33.79742429516185,
      "grad_norm": 7.039011001586914,
      "learning_rate": 2.183547975403179e-05,
      "loss": 1.8554,
      "step": 873900
    },
    {
      "epoch": 33.80129171984375,
      "grad_norm": 16.613679885864258,
      "learning_rate": 2.1832256900130204e-05,
      "loss": 1.9529,
      "step": 874000
    },
    {
      "epoch": 33.80515914452566,
      "grad_norm": 10.898700714111328,
      "learning_rate": 2.1829034046228616e-05,
      "loss": 1.8075,
      "step": 874100
    },
    {
      "epoch": 33.80902656920757,
      "grad_norm": 12.918353080749512,
      "learning_rate": 2.182581119232703e-05,
      "loss": 1.8689,
      "step": 874200
    },
    {
      "epoch": 33.81289399388947,
      "grad_norm": 12.83189582824707,
      "learning_rate": 2.182258833842544e-05,
      "loss": 1.8745,
      "step": 874300
    },
    {
      "epoch": 33.816761418571375,
      "grad_norm": 10.943958282470703,
      "learning_rate": 2.1819365484523856e-05,
      "loss": 1.9109,
      "step": 874400
    },
    {
      "epoch": 33.820628843253274,
      "grad_norm": 9.70170783996582,
      "learning_rate": 2.1816142630622268e-05,
      "loss": 1.907,
      "step": 874500
    },
    {
      "epoch": 33.82449626793518,
      "grad_norm": 9.882271766662598,
      "learning_rate": 2.1812919776720682e-05,
      "loss": 1.8343,
      "step": 874600
    },
    {
      "epoch": 33.82836369261709,
      "grad_norm": 9.636552810668945,
      "learning_rate": 2.1809696922819094e-05,
      "loss": 1.9427,
      "step": 874700
    },
    {
      "epoch": 33.83223111729899,
      "grad_norm": 13.212322235107422,
      "learning_rate": 2.180647406891751e-05,
      "loss": 1.9474,
      "step": 874800
    },
    {
      "epoch": 33.836098541980896,
      "grad_norm": 13.99539852142334,
      "learning_rate": 2.180325121501592e-05,
      "loss": 1.7875,
      "step": 874900
    },
    {
      "epoch": 33.8399659666628,
      "grad_norm": 10.811575889587402,
      "learning_rate": 2.1800028361114335e-05,
      "loss": 1.771,
      "step": 875000
    },
    {
      "epoch": 33.8438333913447,
      "grad_norm": 13.886582374572754,
      "learning_rate": 2.1796805507212746e-05,
      "loss": 1.8447,
      "step": 875100
    },
    {
      "epoch": 33.84770081602661,
      "grad_norm": 12.478729248046875,
      "learning_rate": 2.179358265331116e-05,
      "loss": 1.8991,
      "step": 875200
    },
    {
      "epoch": 33.85156824070851,
      "grad_norm": 12.87514591217041,
      "learning_rate": 2.1790359799409575e-05,
      "loss": 1.8771,
      "step": 875300
    },
    {
      "epoch": 33.85543566539042,
      "grad_norm": 8.960894584655762,
      "learning_rate": 2.1787136945507987e-05,
      "loss": 1.8856,
      "step": 875400
    },
    {
      "epoch": 33.85930309007232,
      "grad_norm": 11.283452033996582,
      "learning_rate": 2.17839140916064e-05,
      "loss": 1.7996,
      "step": 875500
    },
    {
      "epoch": 33.86317051475422,
      "grad_norm": 11.589051246643066,
      "learning_rate": 2.1780691237704816e-05,
      "loss": 1.9741,
      "step": 875600
    },
    {
      "epoch": 33.86703793943613,
      "grad_norm": 14.643779754638672,
      "learning_rate": 2.1777468383803227e-05,
      "loss": 1.7656,
      "step": 875700
    },
    {
      "epoch": 33.87090536411803,
      "grad_norm": 15.672286987304688,
      "learning_rate": 2.1774245529901642e-05,
      "loss": 1.9187,
      "step": 875800
    },
    {
      "epoch": 33.87477278879994,
      "grad_norm": 10.03065013885498,
      "learning_rate": 2.1771022676000053e-05,
      "loss": 1.8949,
      "step": 875900
    },
    {
      "epoch": 33.878640213481845,
      "grad_norm": 14.08967113494873,
      "learning_rate": 2.1767799822098468e-05,
      "loss": 1.8355,
      "step": 876000
    },
    {
      "epoch": 33.882507638163744,
      "grad_norm": 11.202208518981934,
      "learning_rate": 2.176457696819688e-05,
      "loss": 1.8582,
      "step": 876100
    },
    {
      "epoch": 33.88637506284565,
      "grad_norm": 13.546449661254883,
      "learning_rate": 2.1761354114295294e-05,
      "loss": 1.8967,
      "step": 876200
    },
    {
      "epoch": 33.89024248752756,
      "grad_norm": 11.235045433044434,
      "learning_rate": 2.1758131260393706e-05,
      "loss": 1.8119,
      "step": 876300
    },
    {
      "epoch": 33.89410991220946,
      "grad_norm": 11.144192695617676,
      "learning_rate": 2.1754908406492117e-05,
      "loss": 1.8958,
      "step": 876400
    },
    {
      "epoch": 33.897977336891365,
      "grad_norm": 9.131307601928711,
      "learning_rate": 2.175168555259053e-05,
      "loss": 1.8731,
      "step": 876500
    },
    {
      "epoch": 33.901844761573265,
      "grad_norm": 12.721006393432617,
      "learning_rate": 2.1748462698688943e-05,
      "loss": 1.8432,
      "step": 876600
    },
    {
      "epoch": 33.90571218625517,
      "grad_norm": 13.585000991821289,
      "learning_rate": 2.1745239844787358e-05,
      "loss": 1.7929,
      "step": 876700
    },
    {
      "epoch": 33.90957961093708,
      "grad_norm": 13.586403846740723,
      "learning_rate": 2.174201699088577e-05,
      "loss": 1.926,
      "step": 876800
    },
    {
      "epoch": 33.91344703561898,
      "grad_norm": 14.473483085632324,
      "learning_rate": 2.1738794136984184e-05,
      "loss": 1.9261,
      "step": 876900
    },
    {
      "epoch": 33.91731446030089,
      "grad_norm": 8.076902389526367,
      "learning_rate": 2.1735571283082595e-05,
      "loss": 1.8121,
      "step": 877000
    },
    {
      "epoch": 33.921181884982786,
      "grad_norm": 11.186830520629883,
      "learning_rate": 2.173234842918101e-05,
      "loss": 1.8789,
      "step": 877100
    },
    {
      "epoch": 33.92504930966469,
      "grad_norm": 11.523950576782227,
      "learning_rate": 2.172912557527942e-05,
      "loss": 1.841,
      "step": 877200
    },
    {
      "epoch": 33.9289167343466,
      "grad_norm": 11.749128341674805,
      "learning_rate": 2.1725902721377836e-05,
      "loss": 1.8777,
      "step": 877300
    },
    {
      "epoch": 33.9327841590285,
      "grad_norm": 13.455962181091309,
      "learning_rate": 2.1722679867476247e-05,
      "loss": 1.8015,
      "step": 877400
    },
    {
      "epoch": 33.93665158371041,
      "grad_norm": 10.67768383026123,
      "learning_rate": 2.1719457013574662e-05,
      "loss": 1.9095,
      "step": 877500
    },
    {
      "epoch": 33.940519008392315,
      "grad_norm": 11.214560508728027,
      "learning_rate": 2.1716234159673073e-05,
      "loss": 1.8649,
      "step": 877600
    },
    {
      "epoch": 33.944386433074214,
      "grad_norm": 13.343209266662598,
      "learning_rate": 2.1713011305771488e-05,
      "loss": 1.9389,
      "step": 877700
    },
    {
      "epoch": 33.94825385775612,
      "grad_norm": 11.642841339111328,
      "learning_rate": 2.17097884518699e-05,
      "loss": 1.831,
      "step": 877800
    },
    {
      "epoch": 33.95212128243802,
      "grad_norm": 11.909854888916016,
      "learning_rate": 2.1706565597968314e-05,
      "loss": 1.8096,
      "step": 877900
    },
    {
      "epoch": 33.95598870711993,
      "grad_norm": 13.658975601196289,
      "learning_rate": 2.1703342744066725e-05,
      "loss": 1.8759,
      "step": 878000
    },
    {
      "epoch": 33.959856131801835,
      "grad_norm": 13.356520652770996,
      "learning_rate": 2.170011989016514e-05,
      "loss": 1.8704,
      "step": 878100
    },
    {
      "epoch": 33.963723556483735,
      "grad_norm": 12.515336990356445,
      "learning_rate": 2.169689703626355e-05,
      "loss": 1.8902,
      "step": 878200
    },
    {
      "epoch": 33.96759098116564,
      "grad_norm": 13.322526931762695,
      "learning_rate": 2.1693674182361966e-05,
      "loss": 1.935,
      "step": 878300
    },
    {
      "epoch": 33.97145840584755,
      "grad_norm": 13.210334777832031,
      "learning_rate": 2.1690451328460377e-05,
      "loss": 1.8783,
      "step": 878400
    },
    {
      "epoch": 33.97532583052945,
      "grad_norm": 14.878948211669922,
      "learning_rate": 2.1687228474558792e-05,
      "loss": 1.8399,
      "step": 878500
    },
    {
      "epoch": 33.97919325521136,
      "grad_norm": 10.310660362243652,
      "learning_rate": 2.1684005620657203e-05,
      "loss": 1.8711,
      "step": 878600
    },
    {
      "epoch": 33.983060679893256,
      "grad_norm": 12.701578140258789,
      "learning_rate": 2.1680782766755618e-05,
      "loss": 1.8821,
      "step": 878700
    },
    {
      "epoch": 33.98692810457516,
      "grad_norm": 11.439940452575684,
      "learning_rate": 2.1677559912854033e-05,
      "loss": 1.8291,
      "step": 878800
    },
    {
      "epoch": 33.99079552925707,
      "grad_norm": 10.484230995178223,
      "learning_rate": 2.1674337058952444e-05,
      "loss": 1.8311,
      "step": 878900
    },
    {
      "epoch": 33.99466295393897,
      "grad_norm": 11.292746543884277,
      "learning_rate": 2.167111420505086e-05,
      "loss": 1.9125,
      "step": 879000
    },
    {
      "epoch": 33.99853037862088,
      "grad_norm": 9.028709411621094,
      "learning_rate": 2.1667891351149274e-05,
      "loss": 1.8528,
      "step": 879100
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.7918561697006226,
      "eval_runtime": 2.9442,
      "eval_samples_per_second": 462.27,
      "eval_steps_per_second": 462.27,
      "step": 879138
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.6582454442977905,
      "eval_runtime": 56.6047,
      "eval_samples_per_second": 456.8,
      "eval_steps_per_second": 456.8,
      "step": 879138
    },
    {
      "epoch": 34.00239780330278,
      "grad_norm": 12.748333930969238,
      "learning_rate": 2.1664668497247685e-05,
      "loss": 1.8893,
      "step": 879200
    },
    {
      "epoch": 34.006265227984684,
      "grad_norm": 11.75358772277832,
      "learning_rate": 2.1661445643346096e-05,
      "loss": 1.7965,
      "step": 879300
    },
    {
      "epoch": 34.01013265266659,
      "grad_norm": 11.471301078796387,
      "learning_rate": 2.165822278944451e-05,
      "loss": 1.8224,
      "step": 879400
    },
    {
      "epoch": 34.01400007734849,
      "grad_norm": 12.242043495178223,
      "learning_rate": 2.1654999935542922e-05,
      "loss": 1.9568,
      "step": 879500
    },
    {
      "epoch": 34.0178675020304,
      "grad_norm": 9.787327766418457,
      "learning_rate": 2.1651777081641337e-05,
      "loss": 1.8161,
      "step": 879600
    },
    {
      "epoch": 34.021734926712305,
      "grad_norm": 13.332983016967773,
      "learning_rate": 2.164855422773975e-05,
      "loss": 1.8591,
      "step": 879700
    },
    {
      "epoch": 34.025602351394205,
      "grad_norm": 10.701730728149414,
      "learning_rate": 2.1645331373838163e-05,
      "loss": 1.8678,
      "step": 879800
    },
    {
      "epoch": 34.02946977607611,
      "grad_norm": 14.447824478149414,
      "learning_rate": 2.1642108519936574e-05,
      "loss": 1.862,
      "step": 879900
    },
    {
      "epoch": 34.03333720075801,
      "grad_norm": 10.744793891906738,
      "learning_rate": 2.163888566603499e-05,
      "loss": 1.8509,
      "step": 880000
    },
    {
      "epoch": 34.03720462543992,
      "grad_norm": 13.198995590209961,
      "learning_rate": 2.16356628121334e-05,
      "loss": 1.8048,
      "step": 880100
    },
    {
      "epoch": 34.04107205012183,
      "grad_norm": 11.706539154052734,
      "learning_rate": 2.1632439958231815e-05,
      "loss": 1.8614,
      "step": 880200
    },
    {
      "epoch": 34.044939474803726,
      "grad_norm": 11.665460586547852,
      "learning_rate": 2.1629217104330227e-05,
      "loss": 1.8115,
      "step": 880300
    },
    {
      "epoch": 34.04880689948563,
      "grad_norm": 12.731928825378418,
      "learning_rate": 2.162599425042864e-05,
      "loss": 1.8892,
      "step": 880400
    },
    {
      "epoch": 34.05267432416753,
      "grad_norm": 9.721329689025879,
      "learning_rate": 2.1622771396527053e-05,
      "loss": 1.8437,
      "step": 880500
    },
    {
      "epoch": 34.05654174884944,
      "grad_norm": 13.304679870605469,
      "learning_rate": 2.1619548542625467e-05,
      "loss": 1.7759,
      "step": 880600
    },
    {
      "epoch": 34.06040917353135,
      "grad_norm": 11.94704532623291,
      "learning_rate": 2.161632568872388e-05,
      "loss": 1.8588,
      "step": 880700
    },
    {
      "epoch": 34.06427659821325,
      "grad_norm": 16.75301170349121,
      "learning_rate": 2.1613102834822293e-05,
      "loss": 1.7874,
      "step": 880800
    },
    {
      "epoch": 34.068144022895154,
      "grad_norm": 11.025190353393555,
      "learning_rate": 2.1609879980920705e-05,
      "loss": 1.7927,
      "step": 880900
    },
    {
      "epoch": 34.07201144757706,
      "grad_norm": 13.58325481414795,
      "learning_rate": 2.160665712701912e-05,
      "loss": 1.7952,
      "step": 881000
    },
    {
      "epoch": 34.07587887225896,
      "grad_norm": 8.617873191833496,
      "learning_rate": 2.160343427311753e-05,
      "loss": 1.7938,
      "step": 881100
    },
    {
      "epoch": 34.07974629694087,
      "grad_norm": 12.534563064575195,
      "learning_rate": 2.1600211419215945e-05,
      "loss": 1.8738,
      "step": 881200
    },
    {
      "epoch": 34.08361372162277,
      "grad_norm": 13.268811225891113,
      "learning_rate": 2.1596988565314357e-05,
      "loss": 1.9134,
      "step": 881300
    },
    {
      "epoch": 34.087481146304675,
      "grad_norm": 11.724935531616211,
      "learning_rate": 2.159376571141277e-05,
      "loss": 1.8241,
      "step": 881400
    },
    {
      "epoch": 34.09134857098658,
      "grad_norm": 15.186304092407227,
      "learning_rate": 2.1590542857511183e-05,
      "loss": 1.899,
      "step": 881500
    },
    {
      "epoch": 34.09521599566848,
      "grad_norm": 12.5064115524292,
      "learning_rate": 2.1587320003609598e-05,
      "loss": 1.8715,
      "step": 881600
    },
    {
      "epoch": 34.09908342035039,
      "grad_norm": 12.800091743469238,
      "learning_rate": 2.158409714970801e-05,
      "loss": 1.8788,
      "step": 881700
    },
    {
      "epoch": 34.1029508450323,
      "grad_norm": 10.739048957824707,
      "learning_rate": 2.1580874295806424e-05,
      "loss": 1.823,
      "step": 881800
    },
    {
      "epoch": 34.106818269714196,
      "grad_norm": 10.992119789123535,
      "learning_rate": 2.1577651441904835e-05,
      "loss": 1.8812,
      "step": 881900
    },
    {
      "epoch": 34.1106856943961,
      "grad_norm": 9.407869338989258,
      "learning_rate": 2.157442858800325e-05,
      "loss": 1.8986,
      "step": 882000
    },
    {
      "epoch": 34.114553119078,
      "grad_norm": 8.901028633117676,
      "learning_rate": 2.1571205734101664e-05,
      "loss": 1.7992,
      "step": 882100
    },
    {
      "epoch": 34.11842054375991,
      "grad_norm": 11.699344635009766,
      "learning_rate": 2.1567982880200076e-05,
      "loss": 1.8524,
      "step": 882200
    },
    {
      "epoch": 34.12228796844182,
      "grad_norm": 13.794301986694336,
      "learning_rate": 2.156476002629849e-05,
      "loss": 1.7813,
      "step": 882300
    },
    {
      "epoch": 34.12615539312372,
      "grad_norm": 12.347572326660156,
      "learning_rate": 2.1561537172396902e-05,
      "loss": 1.7696,
      "step": 882400
    },
    {
      "epoch": 34.130022817805624,
      "grad_norm": 10.776472091674805,
      "learning_rate": 2.1558314318495316e-05,
      "loss": 1.8774,
      "step": 882500
    },
    {
      "epoch": 34.133890242487524,
      "grad_norm": 9.013529777526855,
      "learning_rate": 2.1555091464593728e-05,
      "loss": 1.8216,
      "step": 882600
    },
    {
      "epoch": 34.13775766716943,
      "grad_norm": 9.105517387390137,
      "learning_rate": 2.1551868610692143e-05,
      "loss": 1.8317,
      "step": 882700
    },
    {
      "epoch": 34.14162509185134,
      "grad_norm": 14.071739196777344,
      "learning_rate": 2.1548645756790554e-05,
      "loss": 1.8364,
      "step": 882800
    },
    {
      "epoch": 34.14549251653324,
      "grad_norm": 13.132004737854004,
      "learning_rate": 2.154542290288897e-05,
      "loss": 1.9012,
      "step": 882900
    },
    {
      "epoch": 34.149359941215145,
      "grad_norm": 13.357023239135742,
      "learning_rate": 2.154220004898738e-05,
      "loss": 1.8496,
      "step": 883000
    },
    {
      "epoch": 34.15322736589705,
      "grad_norm": 13.4417142868042,
      "learning_rate": 2.1538977195085795e-05,
      "loss": 1.8652,
      "step": 883100
    },
    {
      "epoch": 34.15709479057895,
      "grad_norm": 11.529383659362793,
      "learning_rate": 2.1535754341184206e-05,
      "loss": 1.8165,
      "step": 883200
    },
    {
      "epoch": 34.16096221526086,
      "grad_norm": 20.2513484954834,
      "learning_rate": 2.153253148728262e-05,
      "loss": 1.8936,
      "step": 883300
    },
    {
      "epoch": 34.16482963994276,
      "grad_norm": 10.556097984313965,
      "learning_rate": 2.1529308633381032e-05,
      "loss": 1.8472,
      "step": 883400
    },
    {
      "epoch": 34.168697064624666,
      "grad_norm": 14.423701286315918,
      "learning_rate": 2.1526085779479447e-05,
      "loss": 1.8848,
      "step": 883500
    },
    {
      "epoch": 34.17256448930657,
      "grad_norm": 11.5947265625,
      "learning_rate": 2.1522862925577858e-05,
      "loss": 1.7909,
      "step": 883600
    },
    {
      "epoch": 34.17643191398847,
      "grad_norm": 12.661153793334961,
      "learning_rate": 2.1519640071676273e-05,
      "loss": 1.7974,
      "step": 883700
    },
    {
      "epoch": 34.18029933867038,
      "grad_norm": 15.770054817199707,
      "learning_rate": 2.1516417217774684e-05,
      "loss": 1.8069,
      "step": 883800
    },
    {
      "epoch": 34.18416676335228,
      "grad_norm": 17.27094841003418,
      "learning_rate": 2.15131943638731e-05,
      "loss": 1.8121,
      "step": 883900
    },
    {
      "epoch": 34.18803418803419,
      "grad_norm": 13.081892013549805,
      "learning_rate": 2.150997150997151e-05,
      "loss": 1.7573,
      "step": 884000
    },
    {
      "epoch": 34.191901612716094,
      "grad_norm": 14.662689208984375,
      "learning_rate": 2.1506748656069925e-05,
      "loss": 1.8129,
      "step": 884100
    },
    {
      "epoch": 34.195769037397994,
      "grad_norm": 13.2355318069458,
      "learning_rate": 2.1503525802168336e-05,
      "loss": 1.9228,
      "step": 884200
    },
    {
      "epoch": 34.1996364620799,
      "grad_norm": 14.042104721069336,
      "learning_rate": 2.150030294826675e-05,
      "loss": 1.7777,
      "step": 884300
    },
    {
      "epoch": 34.20350388676181,
      "grad_norm": 14.914918899536133,
      "learning_rate": 2.1497080094365162e-05,
      "loss": 1.7994,
      "step": 884400
    },
    {
      "epoch": 34.20737131144371,
      "grad_norm": 11.230838775634766,
      "learning_rate": 2.1493857240463577e-05,
      "loss": 1.9403,
      "step": 884500
    },
    {
      "epoch": 34.211238736125615,
      "grad_norm": 12.590934753417969,
      "learning_rate": 2.1490634386561988e-05,
      "loss": 1.8975,
      "step": 884600
    },
    {
      "epoch": 34.215106160807515,
      "grad_norm": 11.654351234436035,
      "learning_rate": 2.1487411532660403e-05,
      "loss": 1.9255,
      "step": 884700
    },
    {
      "epoch": 34.21897358548942,
      "grad_norm": 11.592373847961426,
      "learning_rate": 2.1484188678758814e-05,
      "loss": 1.8229,
      "step": 884800
    },
    {
      "epoch": 34.22284101017133,
      "grad_norm": 3.3948886394500732,
      "learning_rate": 2.148096582485723e-05,
      "loss": 1.8769,
      "step": 884900
    },
    {
      "epoch": 34.22670843485323,
      "grad_norm": 12.445255279541016,
      "learning_rate": 2.147774297095564e-05,
      "loss": 1.7931,
      "step": 885000
    },
    {
      "epoch": 34.230575859535136,
      "grad_norm": 11.608912467956543,
      "learning_rate": 2.1474520117054055e-05,
      "loss": 1.865,
      "step": 885100
    },
    {
      "epoch": 34.234443284217036,
      "grad_norm": 10.532273292541504,
      "learning_rate": 2.1471297263152466e-05,
      "loss": 1.882,
      "step": 885200
    },
    {
      "epoch": 34.23831070889894,
      "grad_norm": 11.770637512207031,
      "learning_rate": 2.146807440925088e-05,
      "loss": 1.8321,
      "step": 885300
    },
    {
      "epoch": 34.24217813358085,
      "grad_norm": 15.37556266784668,
      "learning_rate": 2.1464851555349292e-05,
      "loss": 1.8673,
      "step": 885400
    },
    {
      "epoch": 34.24604555826275,
      "grad_norm": 12.399903297424316,
      "learning_rate": 2.1461628701447707e-05,
      "loss": 1.8582,
      "step": 885500
    },
    {
      "epoch": 34.24991298294466,
      "grad_norm": 13.45764446258545,
      "learning_rate": 2.1458405847546122e-05,
      "loss": 1.9487,
      "step": 885600
    },
    {
      "epoch": 34.253780407626564,
      "grad_norm": 12.469876289367676,
      "learning_rate": 2.1455182993644533e-05,
      "loss": 1.8515,
      "step": 885700
    },
    {
      "epoch": 34.257647832308464,
      "grad_norm": 7.476150035858154,
      "learning_rate": 2.1451960139742948e-05,
      "loss": 1.7866,
      "step": 885800
    },
    {
      "epoch": 34.26151525699037,
      "grad_norm": 9.363073348999023,
      "learning_rate": 2.144873728584136e-05,
      "loss": 1.7653,
      "step": 885900
    },
    {
      "epoch": 34.26538268167227,
      "grad_norm": 8.611127853393555,
      "learning_rate": 2.1445514431939774e-05,
      "loss": 1.8989,
      "step": 886000
    },
    {
      "epoch": 34.26925010635418,
      "grad_norm": 11.98548412322998,
      "learning_rate": 2.1442291578038185e-05,
      "loss": 1.8255,
      "step": 886100
    },
    {
      "epoch": 34.273117531036085,
      "grad_norm": 11.540854454040527,
      "learning_rate": 2.14390687241366e-05,
      "loss": 1.7897,
      "step": 886200
    },
    {
      "epoch": 34.276984955717985,
      "grad_norm": 12.910347938537598,
      "learning_rate": 2.143584587023501e-05,
      "loss": 1.7718,
      "step": 886300
    },
    {
      "epoch": 34.28085238039989,
      "grad_norm": 11.197221755981445,
      "learning_rate": 2.1432623016333426e-05,
      "loss": 1.8749,
      "step": 886400
    },
    {
      "epoch": 34.2847198050818,
      "grad_norm": 12.60522747039795,
      "learning_rate": 2.1429400162431837e-05,
      "loss": 1.8361,
      "step": 886500
    },
    {
      "epoch": 34.2885872297637,
      "grad_norm": 10.485525131225586,
      "learning_rate": 2.1426177308530252e-05,
      "loss": 1.7697,
      "step": 886600
    },
    {
      "epoch": 34.292454654445606,
      "grad_norm": 11.319663047790527,
      "learning_rate": 2.1422954454628663e-05,
      "loss": 1.8305,
      "step": 886700
    },
    {
      "epoch": 34.296322079127506,
      "grad_norm": 10.873966217041016,
      "learning_rate": 2.1419731600727078e-05,
      "loss": 1.8701,
      "step": 886800
    },
    {
      "epoch": 34.30018950380941,
      "grad_norm": 13.901390075683594,
      "learning_rate": 2.141650874682549e-05,
      "loss": 1.9081,
      "step": 886900
    },
    {
      "epoch": 34.30405692849132,
      "grad_norm": 15.955731391906738,
      "learning_rate": 2.1413285892923904e-05,
      "loss": 1.9076,
      "step": 887000
    },
    {
      "epoch": 34.30792435317322,
      "grad_norm": 12.627553939819336,
      "learning_rate": 2.1410063039022316e-05,
      "loss": 1.8521,
      "step": 887100
    },
    {
      "epoch": 34.31179177785513,
      "grad_norm": 8.963239669799805,
      "learning_rate": 2.140684018512073e-05,
      "loss": 1.8005,
      "step": 887200
    },
    {
      "epoch": 34.31565920253703,
      "grad_norm": 11.589953422546387,
      "learning_rate": 2.140361733121914e-05,
      "loss": 1.8586,
      "step": 887300
    },
    {
      "epoch": 34.319526627218934,
      "grad_norm": 15.586408615112305,
      "learning_rate": 2.1400394477317556e-05,
      "loss": 1.8021,
      "step": 887400
    },
    {
      "epoch": 34.32339405190084,
      "grad_norm": 12.073806762695312,
      "learning_rate": 2.1397171623415968e-05,
      "loss": 1.8124,
      "step": 887500
    },
    {
      "epoch": 34.32726147658274,
      "grad_norm": 7.841395378112793,
      "learning_rate": 2.1393948769514382e-05,
      "loss": 1.7768,
      "step": 887600
    },
    {
      "epoch": 34.33112890126465,
      "grad_norm": 10.726304054260254,
      "learning_rate": 2.1390725915612794e-05,
      "loss": 1.8703,
      "step": 887700
    },
    {
      "epoch": 34.334996325946555,
      "grad_norm": 10.596503257751465,
      "learning_rate": 2.138750306171121e-05,
      "loss": 1.8497,
      "step": 887800
    },
    {
      "epoch": 34.338863750628455,
      "grad_norm": 10.764613151550293,
      "learning_rate": 2.138428020780962e-05,
      "loss": 1.8067,
      "step": 887900
    },
    {
      "epoch": 34.34273117531036,
      "grad_norm": 13.141558647155762,
      "learning_rate": 2.1381057353908034e-05,
      "loss": 1.8327,
      "step": 888000
    },
    {
      "epoch": 34.34659859999226,
      "grad_norm": 9.52138614654541,
      "learning_rate": 2.1377834500006446e-05,
      "loss": 1.8456,
      "step": 888100
    },
    {
      "epoch": 34.35046602467417,
      "grad_norm": 12.390149116516113,
      "learning_rate": 2.1374611646104857e-05,
      "loss": 1.8522,
      "step": 888200
    },
    {
      "epoch": 34.354333449356076,
      "grad_norm": 13.032105445861816,
      "learning_rate": 2.1371388792203272e-05,
      "loss": 1.9089,
      "step": 888300
    },
    {
      "epoch": 34.358200874037976,
      "grad_norm": 11.79245662689209,
      "learning_rate": 2.1368165938301683e-05,
      "loss": 1.9429,
      "step": 888400
    },
    {
      "epoch": 34.36206829871988,
      "grad_norm": 12.928948402404785,
      "learning_rate": 2.1364943084400098e-05,
      "loss": 1.839,
      "step": 888500
    },
    {
      "epoch": 34.36593572340178,
      "grad_norm": 12.681113243103027,
      "learning_rate": 2.136172023049851e-05,
      "loss": 1.8695,
      "step": 888600
    },
    {
      "epoch": 34.36980314808369,
      "grad_norm": 12.949625968933105,
      "learning_rate": 2.1358497376596924e-05,
      "loss": 1.8735,
      "step": 888700
    },
    {
      "epoch": 34.3736705727656,
      "grad_norm": 11.523165702819824,
      "learning_rate": 2.135527452269534e-05,
      "loss": 2.0131,
      "step": 888800
    },
    {
      "epoch": 34.3775379974475,
      "grad_norm": 11.393880844116211,
      "learning_rate": 2.135205166879375e-05,
      "loss": 1.8523,
      "step": 888900
    },
    {
      "epoch": 34.381405422129404,
      "grad_norm": 16.282129287719727,
      "learning_rate": 2.1348828814892165e-05,
      "loss": 1.7916,
      "step": 889000
    },
    {
      "epoch": 34.38527284681131,
      "grad_norm": 12.808265686035156,
      "learning_rate": 2.134560596099058e-05,
      "loss": 1.9083,
      "step": 889100
    },
    {
      "epoch": 34.38914027149321,
      "grad_norm": 11.266343116760254,
      "learning_rate": 2.134238310708899e-05,
      "loss": 1.8342,
      "step": 889200
    },
    {
      "epoch": 34.39300769617512,
      "grad_norm": 12.25683879852295,
      "learning_rate": 2.1339160253187406e-05,
      "loss": 1.8899,
      "step": 889300
    },
    {
      "epoch": 34.39687512085702,
      "grad_norm": 11.685394287109375,
      "learning_rate": 2.1335937399285817e-05,
      "loss": 1.8874,
      "step": 889400
    },
    {
      "epoch": 34.400742545538925,
      "grad_norm": 12.743123054504395,
      "learning_rate": 2.133271454538423e-05,
      "loss": 1.8939,
      "step": 889500
    },
    {
      "epoch": 34.40460997022083,
      "grad_norm": 10.755265235900879,
      "learning_rate": 2.1329491691482643e-05,
      "loss": 1.8364,
      "step": 889600
    },
    {
      "epoch": 34.40847739490273,
      "grad_norm": 11.07827091217041,
      "learning_rate": 2.1326268837581058e-05,
      "loss": 1.941,
      "step": 889700
    },
    {
      "epoch": 34.41234481958464,
      "grad_norm": 10.629022598266602,
      "learning_rate": 2.132304598367947e-05,
      "loss": 1.8419,
      "step": 889800
    },
    {
      "epoch": 34.416212244266546,
      "grad_norm": 11.545062065124512,
      "learning_rate": 2.1319823129777884e-05,
      "loss": 1.8407,
      "step": 889900
    },
    {
      "epoch": 34.420079668948446,
      "grad_norm": 13.179224014282227,
      "learning_rate": 2.1316600275876295e-05,
      "loss": 1.8671,
      "step": 890000
    },
    {
      "epoch": 34.42394709363035,
      "grad_norm": 14.564210891723633,
      "learning_rate": 2.131337742197471e-05,
      "loss": 1.9007,
      "step": 890100
    },
    {
      "epoch": 34.42781451831225,
      "grad_norm": 10.507513999938965,
      "learning_rate": 2.131015456807312e-05,
      "loss": 1.9245,
      "step": 890200
    },
    {
      "epoch": 34.43168194299416,
      "grad_norm": 11.755971908569336,
      "learning_rate": 2.1306931714171536e-05,
      "loss": 1.9261,
      "step": 890300
    },
    {
      "epoch": 34.43554936767607,
      "grad_norm": 13.41930103302002,
      "learning_rate": 2.1303708860269947e-05,
      "loss": 1.8927,
      "step": 890400
    },
    {
      "epoch": 34.43941679235797,
      "grad_norm": 9.29410457611084,
      "learning_rate": 2.1300486006368362e-05,
      "loss": 1.8327,
      "step": 890500
    },
    {
      "epoch": 34.443284217039874,
      "grad_norm": 14.399855613708496,
      "learning_rate": 2.1297263152466773e-05,
      "loss": 1.8419,
      "step": 890600
    },
    {
      "epoch": 34.447151641721774,
      "grad_norm": 10.669639587402344,
      "learning_rate": 2.1294040298565188e-05,
      "loss": 1.835,
      "step": 890700
    },
    {
      "epoch": 34.45101906640368,
      "grad_norm": 14.565556526184082,
      "learning_rate": 2.12908174446636e-05,
      "loss": 1.7785,
      "step": 890800
    },
    {
      "epoch": 34.45488649108559,
      "grad_norm": 12.569284439086914,
      "learning_rate": 2.1287594590762014e-05,
      "loss": 1.8573,
      "step": 890900
    },
    {
      "epoch": 34.45875391576749,
      "grad_norm": 12.620549201965332,
      "learning_rate": 2.1284371736860425e-05,
      "loss": 1.8929,
      "step": 891000
    },
    {
      "epoch": 34.462621340449395,
      "grad_norm": 11.200502395629883,
      "learning_rate": 2.1281148882958837e-05,
      "loss": 1.7166,
      "step": 891100
    },
    {
      "epoch": 34.4664887651313,
      "grad_norm": 12.372668266296387,
      "learning_rate": 2.127792602905725e-05,
      "loss": 1.8237,
      "step": 891200
    },
    {
      "epoch": 34.4703561898132,
      "grad_norm": 12.671951293945312,
      "learning_rate": 2.1274703175155663e-05,
      "loss": 1.9004,
      "step": 891300
    },
    {
      "epoch": 34.47422361449511,
      "grad_norm": 9.526155471801758,
      "learning_rate": 2.1271480321254077e-05,
      "loss": 1.8047,
      "step": 891400
    },
    {
      "epoch": 34.47809103917701,
      "grad_norm": 12.741900444030762,
      "learning_rate": 2.126825746735249e-05,
      "loss": 1.8681,
      "step": 891500
    },
    {
      "epoch": 34.481958463858916,
      "grad_norm": 10.587803840637207,
      "learning_rate": 2.1265034613450903e-05,
      "loss": 1.7925,
      "step": 891600
    },
    {
      "epoch": 34.48582588854082,
      "grad_norm": 12.848578453063965,
      "learning_rate": 2.1261811759549315e-05,
      "loss": 1.929,
      "step": 891700
    },
    {
      "epoch": 34.48969331322272,
      "grad_norm": 14.28104019165039,
      "learning_rate": 2.125858890564773e-05,
      "loss": 1.8853,
      "step": 891800
    },
    {
      "epoch": 34.49356073790463,
      "grad_norm": 12.924786567687988,
      "learning_rate": 2.125536605174614e-05,
      "loss": 1.8918,
      "step": 891900
    },
    {
      "epoch": 34.49742816258653,
      "grad_norm": 11.812382698059082,
      "learning_rate": 2.1252143197844555e-05,
      "loss": 1.8172,
      "step": 892000
    },
    {
      "epoch": 34.50129558726844,
      "grad_norm": 9.856989860534668,
      "learning_rate": 2.1248920343942967e-05,
      "loss": 1.8121,
      "step": 892100
    },
    {
      "epoch": 34.505163011950344,
      "grad_norm": 12.200031280517578,
      "learning_rate": 2.124569749004138e-05,
      "loss": 1.9033,
      "step": 892200
    },
    {
      "epoch": 34.509030436632244,
      "grad_norm": 11.352239608764648,
      "learning_rate": 2.1242474636139796e-05,
      "loss": 1.7873,
      "step": 892300
    },
    {
      "epoch": 34.51289786131415,
      "grad_norm": 12.265771865844727,
      "learning_rate": 2.1239251782238208e-05,
      "loss": 1.8449,
      "step": 892400
    },
    {
      "epoch": 34.51676528599606,
      "grad_norm": 10.196495056152344,
      "learning_rate": 2.1236028928336622e-05,
      "loss": 1.8137,
      "step": 892500
    },
    {
      "epoch": 34.52063271067796,
      "grad_norm": 12.737278938293457,
      "learning_rate": 2.1232806074435037e-05,
      "loss": 1.854,
      "step": 892600
    },
    {
      "epoch": 34.524500135359865,
      "grad_norm": 15.98408031463623,
      "learning_rate": 2.122958322053345e-05,
      "loss": 1.8803,
      "step": 892700
    },
    {
      "epoch": 34.528367560041765,
      "grad_norm": 10.61662769317627,
      "learning_rate": 2.1226360366631863e-05,
      "loss": 1.8628,
      "step": 892800
    },
    {
      "epoch": 34.53223498472367,
      "grad_norm": 11.996103286743164,
      "learning_rate": 2.1223137512730274e-05,
      "loss": 1.9081,
      "step": 892900
    },
    {
      "epoch": 34.53610240940558,
      "grad_norm": 13.258456230163574,
      "learning_rate": 2.121991465882869e-05,
      "loss": 1.8622,
      "step": 893000
    },
    {
      "epoch": 34.53996983408748,
      "grad_norm": 16.40751075744629,
      "learning_rate": 2.12166918049271e-05,
      "loss": 1.8731,
      "step": 893100
    },
    {
      "epoch": 34.543837258769386,
      "grad_norm": 14.383007049560547,
      "learning_rate": 2.1213468951025515e-05,
      "loss": 1.8356,
      "step": 893200
    },
    {
      "epoch": 34.547704683451286,
      "grad_norm": 12.513609886169434,
      "learning_rate": 2.1210246097123926e-05,
      "loss": 1.8709,
      "step": 893300
    },
    {
      "epoch": 34.55157210813319,
      "grad_norm": 12.74951171875,
      "learning_rate": 2.120702324322234e-05,
      "loss": 1.7564,
      "step": 893400
    },
    {
      "epoch": 34.5554395328151,
      "grad_norm": 13.126206398010254,
      "learning_rate": 2.1203800389320753e-05,
      "loss": 1.8896,
      "step": 893500
    },
    {
      "epoch": 34.559306957497,
      "grad_norm": 11.345687866210938,
      "learning_rate": 2.1200577535419167e-05,
      "loss": 1.8096,
      "step": 893600
    },
    {
      "epoch": 34.56317438217891,
      "grad_norm": 8.634393692016602,
      "learning_rate": 2.119735468151758e-05,
      "loss": 1.8707,
      "step": 893700
    },
    {
      "epoch": 34.567041806860814,
      "grad_norm": 11.791704177856445,
      "learning_rate": 2.1194131827615993e-05,
      "loss": 1.7795,
      "step": 893800
    },
    {
      "epoch": 34.570909231542714,
      "grad_norm": 16.67159652709961,
      "learning_rate": 2.1190908973714405e-05,
      "loss": 1.8989,
      "step": 893900
    },
    {
      "epoch": 34.57477665622462,
      "grad_norm": 10.81733226776123,
      "learning_rate": 2.118768611981282e-05,
      "loss": 1.9399,
      "step": 894000
    },
    {
      "epoch": 34.57864408090652,
      "grad_norm": 12.06737995147705,
      "learning_rate": 2.118446326591123e-05,
      "loss": 1.8752,
      "step": 894100
    },
    {
      "epoch": 34.58251150558843,
      "grad_norm": 14.71403980255127,
      "learning_rate": 2.1181240412009642e-05,
      "loss": 1.7689,
      "step": 894200
    },
    {
      "epoch": 34.586378930270335,
      "grad_norm": 11.129076957702637,
      "learning_rate": 2.1178017558108057e-05,
      "loss": 1.8024,
      "step": 894300
    },
    {
      "epoch": 34.590246354952235,
      "grad_norm": 11.68184757232666,
      "learning_rate": 2.1174794704206468e-05,
      "loss": 1.9073,
      "step": 894400
    },
    {
      "epoch": 34.59411377963414,
      "grad_norm": 14.396038055419922,
      "learning_rate": 2.1171571850304883e-05,
      "loss": 1.8791,
      "step": 894500
    },
    {
      "epoch": 34.59798120431605,
      "grad_norm": 11.04931640625,
      "learning_rate": 2.1168348996403294e-05,
      "loss": 1.8462,
      "step": 894600
    },
    {
      "epoch": 34.60184862899795,
      "grad_norm": 9.362512588500977,
      "learning_rate": 2.116512614250171e-05,
      "loss": 1.8706,
      "step": 894700
    },
    {
      "epoch": 34.605716053679856,
      "grad_norm": 13.441329956054688,
      "learning_rate": 2.116190328860012e-05,
      "loss": 1.8749,
      "step": 894800
    },
    {
      "epoch": 34.609583478361756,
      "grad_norm": 12.04774284362793,
      "learning_rate": 2.1158680434698535e-05,
      "loss": 1.896,
      "step": 894900
    },
    {
      "epoch": 34.61345090304366,
      "grad_norm": 12.137101173400879,
      "learning_rate": 2.1155457580796946e-05,
      "loss": 1.8895,
      "step": 895000
    },
    {
      "epoch": 34.61731832772557,
      "grad_norm": 11.198046684265137,
      "learning_rate": 2.115223472689536e-05,
      "loss": 1.9345,
      "step": 895100
    },
    {
      "epoch": 34.62118575240747,
      "grad_norm": 14.226399421691895,
      "learning_rate": 2.1149011872993772e-05,
      "loss": 1.8889,
      "step": 895200
    },
    {
      "epoch": 34.62505317708938,
      "grad_norm": 12.610930442810059,
      "learning_rate": 2.1145789019092187e-05,
      "loss": 1.9281,
      "step": 895300
    },
    {
      "epoch": 34.62892060177128,
      "grad_norm": 11.500553131103516,
      "learning_rate": 2.1142566165190598e-05,
      "loss": 1.886,
      "step": 895400
    },
    {
      "epoch": 34.632788026453184,
      "grad_norm": 12.152634620666504,
      "learning_rate": 2.1139343311289013e-05,
      "loss": 1.8366,
      "step": 895500
    },
    {
      "epoch": 34.63665545113509,
      "grad_norm": 12.656270027160645,
      "learning_rate": 2.1136120457387428e-05,
      "loss": 1.8396,
      "step": 895600
    },
    {
      "epoch": 34.64052287581699,
      "grad_norm": 14.981378555297852,
      "learning_rate": 2.113289760348584e-05,
      "loss": 1.8315,
      "step": 895700
    },
    {
      "epoch": 34.6443903004989,
      "grad_norm": 10.850461959838867,
      "learning_rate": 2.1129674749584254e-05,
      "loss": 1.9267,
      "step": 895800
    },
    {
      "epoch": 34.648257725180805,
      "grad_norm": 11.452320098876953,
      "learning_rate": 2.112645189568267e-05,
      "loss": 1.7524,
      "step": 895900
    },
    {
      "epoch": 34.652125149862705,
      "grad_norm": 10.960841178894043,
      "learning_rate": 2.112322904178108e-05,
      "loss": 1.8447,
      "step": 896000
    },
    {
      "epoch": 34.65599257454461,
      "grad_norm": 9.48932933807373,
      "learning_rate": 2.1120006187879495e-05,
      "loss": 1.8792,
      "step": 896100
    },
    {
      "epoch": 34.65985999922651,
      "grad_norm": 18.65190887451172,
      "learning_rate": 2.1116783333977906e-05,
      "loss": 1.8821,
      "step": 896200
    },
    {
      "epoch": 34.66372742390842,
      "grad_norm": 11.9186429977417,
      "learning_rate": 2.111356048007632e-05,
      "loss": 1.8543,
      "step": 896300
    },
    {
      "epoch": 34.667594848590326,
      "grad_norm": 12.744434356689453,
      "learning_rate": 2.1110337626174732e-05,
      "loss": 1.9017,
      "step": 896400
    },
    {
      "epoch": 34.671462273272226,
      "grad_norm": 8.5933198928833,
      "learning_rate": 2.1107114772273147e-05,
      "loss": 1.8419,
      "step": 896500
    },
    {
      "epoch": 34.67532969795413,
      "grad_norm": 14.655815124511719,
      "learning_rate": 2.1103891918371558e-05,
      "loss": 1.9142,
      "step": 896600
    },
    {
      "epoch": 34.67919712263604,
      "grad_norm": 9.309585571289062,
      "learning_rate": 2.1100669064469973e-05,
      "loss": 1.8695,
      "step": 896700
    },
    {
      "epoch": 34.68306454731794,
      "grad_norm": 11.280672073364258,
      "learning_rate": 2.1097446210568384e-05,
      "loss": 1.8248,
      "step": 896800
    },
    {
      "epoch": 34.68693197199985,
      "grad_norm": 11.49501895904541,
      "learning_rate": 2.10942233566668e-05,
      "loss": 1.8187,
      "step": 896900
    },
    {
      "epoch": 34.69079939668175,
      "grad_norm": 14.37071418762207,
      "learning_rate": 2.109100050276521e-05,
      "loss": 1.839,
      "step": 897000
    },
    {
      "epoch": 34.694666821363654,
      "grad_norm": 12.470919609069824,
      "learning_rate": 2.108777764886362e-05,
      "loss": 1.844,
      "step": 897100
    },
    {
      "epoch": 34.69853424604556,
      "grad_norm": 13.939985275268555,
      "learning_rate": 2.1084554794962036e-05,
      "loss": 1.8261,
      "step": 897200
    },
    {
      "epoch": 34.70240167072746,
      "grad_norm": 12.258957862854004,
      "learning_rate": 2.1081331941060447e-05,
      "loss": 1.9003,
      "step": 897300
    },
    {
      "epoch": 34.70626909540937,
      "grad_norm": 12.72283935546875,
      "learning_rate": 2.1078109087158862e-05,
      "loss": 1.9934,
      "step": 897400
    },
    {
      "epoch": 34.71013652009127,
      "grad_norm": 9.45971393585205,
      "learning_rate": 2.1074886233257273e-05,
      "loss": 1.8011,
      "step": 897500
    },
    {
      "epoch": 34.714003944773175,
      "grad_norm": 11.463045120239258,
      "learning_rate": 2.1071663379355688e-05,
      "loss": 1.8037,
      "step": 897600
    },
    {
      "epoch": 34.71787136945508,
      "grad_norm": 11.840057373046875,
      "learning_rate": 2.10684405254541e-05,
      "loss": 1.8633,
      "step": 897700
    },
    {
      "epoch": 34.72173879413698,
      "grad_norm": 12.296846389770508,
      "learning_rate": 2.1065217671552514e-05,
      "loss": 1.7562,
      "step": 897800
    },
    {
      "epoch": 34.72560621881889,
      "grad_norm": 12.879793167114258,
      "learning_rate": 2.1061994817650926e-05,
      "loss": 1.8148,
      "step": 897900
    },
    {
      "epoch": 34.729473643500796,
      "grad_norm": 11.289918899536133,
      "learning_rate": 2.105877196374934e-05,
      "loss": 1.9099,
      "step": 898000
    },
    {
      "epoch": 34.733341068182696,
      "grad_norm": 11.389599800109863,
      "learning_rate": 2.105554910984775e-05,
      "loss": 1.9412,
      "step": 898100
    },
    {
      "epoch": 34.7372084928646,
      "grad_norm": 11.241654396057129,
      "learning_rate": 2.1052326255946166e-05,
      "loss": 1.8851,
      "step": 898200
    },
    {
      "epoch": 34.7410759175465,
      "grad_norm": 10.648027420043945,
      "learning_rate": 2.1049103402044578e-05,
      "loss": 1.8747,
      "step": 898300
    },
    {
      "epoch": 34.74494334222841,
      "grad_norm": 11.442402839660645,
      "learning_rate": 2.1045880548142992e-05,
      "loss": 1.8742,
      "step": 898400
    },
    {
      "epoch": 34.74881076691032,
      "grad_norm": 8.408074378967285,
      "learning_rate": 2.1042657694241404e-05,
      "loss": 1.963,
      "step": 898500
    },
    {
      "epoch": 34.75267819159222,
      "grad_norm": 8.94657039642334,
      "learning_rate": 2.103943484033982e-05,
      "loss": 1.8748,
      "step": 898600
    },
    {
      "epoch": 34.756545616274124,
      "grad_norm": 15.48625373840332,
      "learning_rate": 2.103621198643823e-05,
      "loss": 1.8957,
      "step": 898700
    },
    {
      "epoch": 34.760413040956024,
      "grad_norm": 10.238036155700684,
      "learning_rate": 2.1032989132536645e-05,
      "loss": 1.8028,
      "step": 898800
    },
    {
      "epoch": 34.76428046563793,
      "grad_norm": 11.822379112243652,
      "learning_rate": 2.1029766278635056e-05,
      "loss": 1.8741,
      "step": 898900
    },
    {
      "epoch": 34.76814789031984,
      "grad_norm": 13.170117378234863,
      "learning_rate": 2.102654342473347e-05,
      "loss": 1.7688,
      "step": 899000
    },
    {
      "epoch": 34.77201531500174,
      "grad_norm": 11.232383728027344,
      "learning_rate": 2.1023320570831885e-05,
      "loss": 1.8655,
      "step": 899100
    },
    {
      "epoch": 34.775882739683645,
      "grad_norm": 12.256250381469727,
      "learning_rate": 2.1020097716930297e-05,
      "loss": 1.8537,
      "step": 899200
    },
    {
      "epoch": 34.77975016436555,
      "grad_norm": 10.866211891174316,
      "learning_rate": 2.101687486302871e-05,
      "loss": 1.9701,
      "step": 899300
    },
    {
      "epoch": 34.78361758904745,
      "grad_norm": 10.601166725158691,
      "learning_rate": 2.1013652009127126e-05,
      "loss": 2.0043,
      "step": 899400
    },
    {
      "epoch": 34.78748501372936,
      "grad_norm": 11.837288856506348,
      "learning_rate": 2.1010429155225537e-05,
      "loss": 1.8431,
      "step": 899500
    },
    {
      "epoch": 34.79135243841126,
      "grad_norm": 12.408191680908203,
      "learning_rate": 2.1007206301323952e-05,
      "loss": 1.9261,
      "step": 899600
    },
    {
      "epoch": 34.795219863093166,
      "grad_norm": 11.373583793640137,
      "learning_rate": 2.1003983447422363e-05,
      "loss": 1.8103,
      "step": 899700
    },
    {
      "epoch": 34.79908728777507,
      "grad_norm": 9.966577529907227,
      "learning_rate": 2.1000760593520778e-05,
      "loss": 1.8324,
      "step": 899800
    },
    {
      "epoch": 34.80295471245697,
      "grad_norm": 11.924872398376465,
      "learning_rate": 2.099753773961919e-05,
      "loss": 1.8241,
      "step": 899900
    },
    {
      "epoch": 34.80682213713888,
      "grad_norm": 13.919352531433105,
      "learning_rate": 2.09943148857176e-05,
      "loss": 1.8668,
      "step": 900000
    },
    {
      "epoch": 34.81068956182078,
      "grad_norm": 11.708209037780762,
      "learning_rate": 2.0991092031816016e-05,
      "loss": 1.8796,
      "step": 900100
    },
    {
      "epoch": 34.81455698650269,
      "grad_norm": 13.040046691894531,
      "learning_rate": 2.0987869177914427e-05,
      "loss": 1.9872,
      "step": 900200
    },
    {
      "epoch": 34.818424411184594,
      "grad_norm": 14.70474624633789,
      "learning_rate": 2.098464632401284e-05,
      "loss": 1.898,
      "step": 900300
    },
    {
      "epoch": 34.822291835866494,
      "grad_norm": 11.786099433898926,
      "learning_rate": 2.0981423470111253e-05,
      "loss": 1.9119,
      "step": 900400
    },
    {
      "epoch": 34.8261592605484,
      "grad_norm": 12.07531452178955,
      "learning_rate": 2.0978200616209668e-05,
      "loss": 1.8496,
      "step": 900500
    },
    {
      "epoch": 34.83002668523031,
      "grad_norm": 10.382007598876953,
      "learning_rate": 2.097497776230808e-05,
      "loss": 1.8493,
      "step": 900600
    },
    {
      "epoch": 34.83389410991221,
      "grad_norm": 12.441362380981445,
      "learning_rate": 2.0971754908406494e-05,
      "loss": 1.8888,
      "step": 900700
    },
    {
      "epoch": 34.837761534594115,
      "grad_norm": 8.874640464782715,
      "learning_rate": 2.0968532054504905e-05,
      "loss": 1.7896,
      "step": 900800
    },
    {
      "epoch": 34.841628959276015,
      "grad_norm": 11.241267204284668,
      "learning_rate": 2.096530920060332e-05,
      "loss": 1.9111,
      "step": 900900
    },
    {
      "epoch": 34.84549638395792,
      "grad_norm": 12.585489273071289,
      "learning_rate": 2.096208634670173e-05,
      "loss": 1.8857,
      "step": 901000
    },
    {
      "epoch": 34.84936380863983,
      "grad_norm": 10.252147674560547,
      "learning_rate": 2.0958863492800146e-05,
      "loss": 1.7972,
      "step": 901100
    },
    {
      "epoch": 34.85323123332173,
      "grad_norm": 11.910512924194336,
      "learning_rate": 2.0955640638898557e-05,
      "loss": 1.8734,
      "step": 901200
    },
    {
      "epoch": 34.857098658003636,
      "grad_norm": 12.903721809387207,
      "learning_rate": 2.0952417784996972e-05,
      "loss": 1.9072,
      "step": 901300
    },
    {
      "epoch": 34.860966082685536,
      "grad_norm": 8.801505088806152,
      "learning_rate": 2.0949194931095383e-05,
      "loss": 1.9005,
      "step": 901400
    },
    {
      "epoch": 34.86483350736744,
      "grad_norm": 12.745096206665039,
      "learning_rate": 2.0945972077193798e-05,
      "loss": 1.819,
      "step": 901500
    },
    {
      "epoch": 34.86870093204935,
      "grad_norm": 13.12971019744873,
      "learning_rate": 2.094274922329221e-05,
      "loss": 1.8644,
      "step": 901600
    },
    {
      "epoch": 34.87256835673125,
      "grad_norm": 10.948667526245117,
      "learning_rate": 2.0939526369390624e-05,
      "loss": 1.7936,
      "step": 901700
    },
    {
      "epoch": 34.87643578141316,
      "grad_norm": 11.383755683898926,
      "learning_rate": 2.0936303515489035e-05,
      "loss": 1.9493,
      "step": 901800
    },
    {
      "epoch": 34.880303206095064,
      "grad_norm": 13.20284652709961,
      "learning_rate": 2.093308066158745e-05,
      "loss": 1.8327,
      "step": 901900
    },
    {
      "epoch": 34.884170630776964,
      "grad_norm": 11.53077507019043,
      "learning_rate": 2.092985780768586e-05,
      "loss": 1.9113,
      "step": 902000
    },
    {
      "epoch": 34.88803805545887,
      "grad_norm": 10.710159301757812,
      "learning_rate": 2.0926634953784276e-05,
      "loss": 1.8048,
      "step": 902100
    },
    {
      "epoch": 34.89190548014077,
      "grad_norm": 9.700608253479004,
      "learning_rate": 2.0923412099882687e-05,
      "loss": 1.8416,
      "step": 902200
    },
    {
      "epoch": 34.89577290482268,
      "grad_norm": 11.976089477539062,
      "learning_rate": 2.0920189245981102e-05,
      "loss": 1.8408,
      "step": 902300
    },
    {
      "epoch": 34.899640329504585,
      "grad_norm": 11.289470672607422,
      "learning_rate": 2.0916966392079513e-05,
      "loss": 1.9387,
      "step": 902400
    },
    {
      "epoch": 34.903507754186485,
      "grad_norm": 14.858378410339355,
      "learning_rate": 2.0913743538177928e-05,
      "loss": 1.9054,
      "step": 902500
    },
    {
      "epoch": 34.90737517886839,
      "grad_norm": 11.743130683898926,
      "learning_rate": 2.0910520684276343e-05,
      "loss": 1.8512,
      "step": 902600
    },
    {
      "epoch": 34.9112426035503,
      "grad_norm": 20.102067947387695,
      "learning_rate": 2.0907297830374754e-05,
      "loss": 1.8887,
      "step": 902700
    },
    {
      "epoch": 34.9151100282322,
      "grad_norm": 11.16701889038086,
      "learning_rate": 2.090407497647317e-05,
      "loss": 1.7741,
      "step": 902800
    },
    {
      "epoch": 34.918977452914106,
      "grad_norm": 10.903538703918457,
      "learning_rate": 2.090085212257158e-05,
      "loss": 1.8604,
      "step": 902900
    },
    {
      "epoch": 34.922844877596006,
      "grad_norm": 15.693081855773926,
      "learning_rate": 2.0897629268669995e-05,
      "loss": 1.8959,
      "step": 903000
    },
    {
      "epoch": 34.92671230227791,
      "grad_norm": 11.797721862792969,
      "learning_rate": 2.0894406414768406e-05,
      "loss": 1.8221,
      "step": 903100
    },
    {
      "epoch": 34.93057972695982,
      "grad_norm": 7.846118927001953,
      "learning_rate": 2.089118356086682e-05,
      "loss": 1.8565,
      "step": 903200
    },
    {
      "epoch": 34.93444715164172,
      "grad_norm": 10.500659942626953,
      "learning_rate": 2.0887960706965232e-05,
      "loss": 1.8122,
      "step": 903300
    },
    {
      "epoch": 34.93831457632363,
      "grad_norm": 12.786518096923828,
      "learning_rate": 2.0884737853063647e-05,
      "loss": 1.8015,
      "step": 903400
    },
    {
      "epoch": 34.94218200100553,
      "grad_norm": 11.549796104431152,
      "learning_rate": 2.088151499916206e-05,
      "loss": 1.8676,
      "step": 903500
    },
    {
      "epoch": 34.946049425687434,
      "grad_norm": 10.591511726379395,
      "learning_rate": 2.0878292145260473e-05,
      "loss": 1.8003,
      "step": 903600
    },
    {
      "epoch": 34.94991685036934,
      "grad_norm": 13.262486457824707,
      "learning_rate": 2.0875069291358884e-05,
      "loss": 1.8227,
      "step": 903700
    },
    {
      "epoch": 34.95378427505124,
      "grad_norm": 12.04865550994873,
      "learning_rate": 2.08718464374573e-05,
      "loss": 1.8423,
      "step": 903800
    },
    {
      "epoch": 34.95765169973315,
      "grad_norm": 11.010114669799805,
      "learning_rate": 2.086862358355571e-05,
      "loss": 1.9693,
      "step": 903900
    },
    {
      "epoch": 34.961519124415055,
      "grad_norm": 12.917539596557617,
      "learning_rate": 2.0865400729654125e-05,
      "loss": 1.8973,
      "step": 904000
    },
    {
      "epoch": 34.965386549096955,
      "grad_norm": 13.312713623046875,
      "learning_rate": 2.0862177875752536e-05,
      "loss": 1.8781,
      "step": 904100
    },
    {
      "epoch": 34.96925397377886,
      "grad_norm": 11.120646476745605,
      "learning_rate": 2.085895502185095e-05,
      "loss": 1.8085,
      "step": 904200
    },
    {
      "epoch": 34.97312139846076,
      "grad_norm": 11.468902587890625,
      "learning_rate": 2.0855732167949363e-05,
      "loss": 1.8619,
      "step": 904300
    },
    {
      "epoch": 34.97698882314267,
      "grad_norm": 11.734063148498535,
      "learning_rate": 2.0852509314047777e-05,
      "loss": 1.8154,
      "step": 904400
    },
    {
      "epoch": 34.980856247824576,
      "grad_norm": 8.623194694519043,
      "learning_rate": 2.084928646014619e-05,
      "loss": 1.8737,
      "step": 904500
    },
    {
      "epoch": 34.984723672506476,
      "grad_norm": 11.482773780822754,
      "learning_rate": 2.0846063606244603e-05,
      "loss": 1.8795,
      "step": 904600
    },
    {
      "epoch": 34.98859109718838,
      "grad_norm": 7.3462114334106445,
      "learning_rate": 2.0842840752343015e-05,
      "loss": 1.7564,
      "step": 904700
    },
    {
      "epoch": 34.99245852187029,
      "grad_norm": 11.619622230529785,
      "learning_rate": 2.083961789844143e-05,
      "loss": 1.7998,
      "step": 904800
    },
    {
      "epoch": 34.99632594655219,
      "grad_norm": 12.717671394348145,
      "learning_rate": 2.083639504453984e-05,
      "loss": 1.8226,
      "step": 904900
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.79128897190094,
      "eval_runtime": 3.1689,
      "eval_samples_per_second": 429.485,
      "eval_steps_per_second": 429.485,
      "step": 904995
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.6537117958068848,
      "eval_runtime": 56.2446,
      "eval_samples_per_second": 459.724,
      "eval_steps_per_second": 459.724,
      "step": 904995
    },
    {
      "epoch": 35.0001933712341,
      "grad_norm": 14.951298713684082,
      "learning_rate": 2.0833172190638255e-05,
      "loss": 1.8682,
      "step": 905000
    },
    {
      "epoch": 35.004060795916,
      "grad_norm": 11.20651626586914,
      "learning_rate": 2.0829949336736667e-05,
      "loss": 1.8119,
      "step": 905100
    },
    {
      "epoch": 35.007928220597904,
      "grad_norm": 13.005334854125977,
      "learning_rate": 2.082672648283508e-05,
      "loss": 1.8418,
      "step": 905200
    },
    {
      "epoch": 35.01179564527981,
      "grad_norm": 11.888676643371582,
      "learning_rate": 2.0823503628933493e-05,
      "loss": 1.8924,
      "step": 905300
    },
    {
      "epoch": 35.01566306996171,
      "grad_norm": 9.95934772491455,
      "learning_rate": 2.0820280775031908e-05,
      "loss": 1.7599,
      "step": 905400
    },
    {
      "epoch": 35.01953049464362,
      "grad_norm": 12.828692436218262,
      "learning_rate": 2.081705792113032e-05,
      "loss": 1.8367,
      "step": 905500
    },
    {
      "epoch": 35.02339791932552,
      "grad_norm": 12.533626556396484,
      "learning_rate": 2.0813835067228734e-05,
      "loss": 1.8742,
      "step": 905600
    },
    {
      "epoch": 35.027265344007425,
      "grad_norm": 11.051302909851074,
      "learning_rate": 2.0810612213327145e-05,
      "loss": 1.7882,
      "step": 905700
    },
    {
      "epoch": 35.03113276868933,
      "grad_norm": 13.481517791748047,
      "learning_rate": 2.080738935942556e-05,
      "loss": 1.8749,
      "step": 905800
    },
    {
      "epoch": 35.03500019337123,
      "grad_norm": 12.305184364318848,
      "learning_rate": 2.080416650552397e-05,
      "loss": 1.8514,
      "step": 905900
    },
    {
      "epoch": 35.03886761805314,
      "grad_norm": 10.881930351257324,
      "learning_rate": 2.0800943651622386e-05,
      "loss": 1.9397,
      "step": 906000
    },
    {
      "epoch": 35.042735042735046,
      "grad_norm": 13.675076484680176,
      "learning_rate": 2.07977207977208e-05,
      "loss": 1.778,
      "step": 906100
    },
    {
      "epoch": 35.046602467416946,
      "grad_norm": 11.252073287963867,
      "learning_rate": 2.0794497943819212e-05,
      "loss": 1.8812,
      "step": 906200
    },
    {
      "epoch": 35.05046989209885,
      "grad_norm": 14.715551376342773,
      "learning_rate": 2.0791275089917626e-05,
      "loss": 1.8286,
      "step": 906300
    },
    {
      "epoch": 35.05433731678075,
      "grad_norm": 15.591425895690918,
      "learning_rate": 2.0788052236016038e-05,
      "loss": 1.8298,
      "step": 906400
    },
    {
      "epoch": 35.05820474146266,
      "grad_norm": 8.32120418548584,
      "learning_rate": 2.0784829382114452e-05,
      "loss": 1.7696,
      "step": 906500
    },
    {
      "epoch": 35.06207216614457,
      "grad_norm": 10.765446662902832,
      "learning_rate": 2.0781606528212864e-05,
      "loss": 1.8228,
      "step": 906600
    },
    {
      "epoch": 35.06593959082647,
      "grad_norm": 14.152567863464355,
      "learning_rate": 2.077838367431128e-05,
      "loss": 1.8819,
      "step": 906700
    },
    {
      "epoch": 35.069807015508374,
      "grad_norm": 12.512027740478516,
      "learning_rate": 2.077516082040969e-05,
      "loss": 1.8482,
      "step": 906800
    },
    {
      "epoch": 35.073674440190274,
      "grad_norm": 13.226423263549805,
      "learning_rate": 2.0771937966508105e-05,
      "loss": 1.8637,
      "step": 906900
    },
    {
      "epoch": 35.07754186487218,
      "grad_norm": 13.347997665405273,
      "learning_rate": 2.0768715112606516e-05,
      "loss": 1.8001,
      "step": 907000
    },
    {
      "epoch": 35.08140928955409,
      "grad_norm": 13.401433944702148,
      "learning_rate": 2.076549225870493e-05,
      "loss": 1.8558,
      "step": 907100
    },
    {
      "epoch": 35.08527671423599,
      "grad_norm": 9.815523147583008,
      "learning_rate": 2.0762269404803342e-05,
      "loss": 1.8321,
      "step": 907200
    },
    {
      "epoch": 35.089144138917895,
      "grad_norm": 10.203033447265625,
      "learning_rate": 2.0759046550901757e-05,
      "loss": 1.8649,
      "step": 907300
    },
    {
      "epoch": 35.0930115635998,
      "grad_norm": 14.684636116027832,
      "learning_rate": 2.0755823697000168e-05,
      "loss": 1.9106,
      "step": 907400
    },
    {
      "epoch": 35.0968789882817,
      "grad_norm": 11.887838363647461,
      "learning_rate": 2.0752600843098583e-05,
      "loss": 1.8458,
      "step": 907500
    },
    {
      "epoch": 35.10074641296361,
      "grad_norm": 10.149158477783203,
      "learning_rate": 2.0749377989196994e-05,
      "loss": 1.8628,
      "step": 907600
    },
    {
      "epoch": 35.10461383764551,
      "grad_norm": 10.539535522460938,
      "learning_rate": 2.074615513529541e-05,
      "loss": 1.8034,
      "step": 907700
    },
    {
      "epoch": 35.108481262327416,
      "grad_norm": 10.363360404968262,
      "learning_rate": 2.074293228139382e-05,
      "loss": 1.7769,
      "step": 907800
    },
    {
      "epoch": 35.11234868700932,
      "grad_norm": 10.609685897827148,
      "learning_rate": 2.0739709427492235e-05,
      "loss": 1.9094,
      "step": 907900
    },
    {
      "epoch": 35.11621611169122,
      "grad_norm": 17.27149772644043,
      "learning_rate": 2.0736486573590646e-05,
      "loss": 1.9286,
      "step": 908000
    },
    {
      "epoch": 35.12008353637313,
      "grad_norm": 13.833175659179688,
      "learning_rate": 2.073326371968906e-05,
      "loss": 1.8846,
      "step": 908100
    },
    {
      "epoch": 35.12395096105503,
      "grad_norm": 13.606569290161133,
      "learning_rate": 2.0730040865787472e-05,
      "loss": 1.81,
      "step": 908200
    },
    {
      "epoch": 35.12781838573694,
      "grad_norm": 8.546122550964355,
      "learning_rate": 2.0726818011885887e-05,
      "loss": 1.8116,
      "step": 908300
    },
    {
      "epoch": 35.131685810418844,
      "grad_norm": 14.247401237487793,
      "learning_rate": 2.0723595157984298e-05,
      "loss": 1.8442,
      "step": 908400
    },
    {
      "epoch": 35.135553235100744,
      "grad_norm": 11.328330993652344,
      "learning_rate": 2.0720372304082713e-05,
      "loss": 1.8606,
      "step": 908500
    },
    {
      "epoch": 35.13942065978265,
      "grad_norm": 12.305187225341797,
      "learning_rate": 2.0717149450181124e-05,
      "loss": 1.897,
      "step": 908600
    },
    {
      "epoch": 35.14328808446456,
      "grad_norm": 10.902958869934082,
      "learning_rate": 2.071392659627954e-05,
      "loss": 1.8589,
      "step": 908700
    },
    {
      "epoch": 35.14715550914646,
      "grad_norm": 11.412261962890625,
      "learning_rate": 2.071070374237795e-05,
      "loss": 1.8638,
      "step": 908800
    },
    {
      "epoch": 35.151022933828365,
      "grad_norm": 14.21062183380127,
      "learning_rate": 2.070748088847636e-05,
      "loss": 1.8342,
      "step": 908900
    },
    {
      "epoch": 35.154890358510265,
      "grad_norm": 14.005289077758789,
      "learning_rate": 2.0704258034574776e-05,
      "loss": 1.7984,
      "step": 909000
    },
    {
      "epoch": 35.15875778319217,
      "grad_norm": 13.649728775024414,
      "learning_rate": 2.070103518067319e-05,
      "loss": 1.8237,
      "step": 909100
    },
    {
      "epoch": 35.16262520787408,
      "grad_norm": 12.412506103515625,
      "learning_rate": 2.0697812326771602e-05,
      "loss": 1.7686,
      "step": 909200
    },
    {
      "epoch": 35.16649263255598,
      "grad_norm": 11.649900436401367,
      "learning_rate": 2.0694589472870017e-05,
      "loss": 1.7897,
      "step": 909300
    },
    {
      "epoch": 35.170360057237886,
      "grad_norm": 10.308920860290527,
      "learning_rate": 2.0691366618968432e-05,
      "loss": 1.818,
      "step": 909400
    },
    {
      "epoch": 35.17422748191979,
      "grad_norm": 10.2909574508667,
      "learning_rate": 2.0688143765066843e-05,
      "loss": 1.8312,
      "step": 909500
    },
    {
      "epoch": 35.17809490660169,
      "grad_norm": 13.960421562194824,
      "learning_rate": 2.0684920911165258e-05,
      "loss": 1.9079,
      "step": 909600
    },
    {
      "epoch": 35.1819623312836,
      "grad_norm": 13.640406608581543,
      "learning_rate": 2.068169805726367e-05,
      "loss": 1.8446,
      "step": 909700
    },
    {
      "epoch": 35.1858297559655,
      "grad_norm": 12.375102043151855,
      "learning_rate": 2.0678475203362084e-05,
      "loss": 1.8433,
      "step": 909800
    },
    {
      "epoch": 35.18969718064741,
      "grad_norm": 10.449235916137695,
      "learning_rate": 2.0675252349460495e-05,
      "loss": 1.8462,
      "step": 909900
    },
    {
      "epoch": 35.193564605329314,
      "grad_norm": 13.876570701599121,
      "learning_rate": 2.067202949555891e-05,
      "loss": 1.8639,
      "step": 910000
    },
    {
      "epoch": 35.197432030011214,
      "grad_norm": 11.314579963684082,
      "learning_rate": 2.066880664165732e-05,
      "loss": 1.8132,
      "step": 910100
    },
    {
      "epoch": 35.20129945469312,
      "grad_norm": 14.90810775756836,
      "learning_rate": 2.0665583787755736e-05,
      "loss": 1.9429,
      "step": 910200
    },
    {
      "epoch": 35.20516687937502,
      "grad_norm": 11.35757064819336,
      "learning_rate": 2.0662360933854147e-05,
      "loss": 1.7854,
      "step": 910300
    },
    {
      "epoch": 35.20903430405693,
      "grad_norm": 11.89603042602539,
      "learning_rate": 2.0659138079952562e-05,
      "loss": 1.8609,
      "step": 910400
    },
    {
      "epoch": 35.212901728738835,
      "grad_norm": 11.087111473083496,
      "learning_rate": 2.0655915226050973e-05,
      "loss": 1.884,
      "step": 910500
    },
    {
      "epoch": 35.216769153420735,
      "grad_norm": 13.520895004272461,
      "learning_rate": 2.0652692372149388e-05,
      "loss": 1.8097,
      "step": 910600
    },
    {
      "epoch": 35.22063657810264,
      "grad_norm": 12.721626281738281,
      "learning_rate": 2.06494695182478e-05,
      "loss": 1.9293,
      "step": 910700
    },
    {
      "epoch": 35.22450400278455,
      "grad_norm": 12.408842086791992,
      "learning_rate": 2.0646246664346214e-05,
      "loss": 1.8427,
      "step": 910800
    },
    {
      "epoch": 35.22837142746645,
      "grad_norm": 11.423637390136719,
      "learning_rate": 2.0643023810444626e-05,
      "loss": 1.8459,
      "step": 910900
    },
    {
      "epoch": 35.232238852148356,
      "grad_norm": 12.239592552185059,
      "learning_rate": 2.063980095654304e-05,
      "loss": 1.8767,
      "step": 911000
    },
    {
      "epoch": 35.236106276830256,
      "grad_norm": 9.928285598754883,
      "learning_rate": 2.063657810264145e-05,
      "loss": 1.8511,
      "step": 911100
    },
    {
      "epoch": 35.23997370151216,
      "grad_norm": 12.484641075134277,
      "learning_rate": 2.0633355248739866e-05,
      "loss": 1.8358,
      "step": 911200
    },
    {
      "epoch": 35.24384112619407,
      "grad_norm": 11.879740715026855,
      "learning_rate": 2.0630132394838278e-05,
      "loss": 1.9459,
      "step": 911300
    },
    {
      "epoch": 35.24770855087597,
      "grad_norm": 13.504253387451172,
      "learning_rate": 2.0626909540936692e-05,
      "loss": 1.9378,
      "step": 911400
    },
    {
      "epoch": 35.25157597555788,
      "grad_norm": 11.465677261352539,
      "learning_rate": 2.0623686687035104e-05,
      "loss": 1.8742,
      "step": 911500
    },
    {
      "epoch": 35.25544340023978,
      "grad_norm": 9.447924613952637,
      "learning_rate": 2.062046383313352e-05,
      "loss": 1.8474,
      "step": 911600
    },
    {
      "epoch": 35.259310824921684,
      "grad_norm": 10.780352592468262,
      "learning_rate": 2.061724097923193e-05,
      "loss": 1.8997,
      "step": 911700
    },
    {
      "epoch": 35.26317824960359,
      "grad_norm": 17.363685607910156,
      "learning_rate": 2.061401812533034e-05,
      "loss": 1.8663,
      "step": 911800
    },
    {
      "epoch": 35.26704567428549,
      "grad_norm": 10.97024154663086,
      "learning_rate": 2.0610795271428756e-05,
      "loss": 1.8996,
      "step": 911900
    },
    {
      "epoch": 35.2709130989674,
      "grad_norm": 14.886619567871094,
      "learning_rate": 2.0607572417527167e-05,
      "loss": 1.8995,
      "step": 912000
    },
    {
      "epoch": 35.274780523649305,
      "grad_norm": 12.001486778259277,
      "learning_rate": 2.0604349563625582e-05,
      "loss": 1.87,
      "step": 912100
    },
    {
      "epoch": 35.278647948331205,
      "grad_norm": 13.82396125793457,
      "learning_rate": 2.0601126709723993e-05,
      "loss": 1.8788,
      "step": 912200
    },
    {
      "epoch": 35.28251537301311,
      "grad_norm": 10.045964241027832,
      "learning_rate": 2.0597903855822408e-05,
      "loss": 1.8532,
      "step": 912300
    },
    {
      "epoch": 35.28638279769501,
      "grad_norm": 14.72332763671875,
      "learning_rate": 2.059468100192082e-05,
      "loss": 1.7785,
      "step": 912400
    },
    {
      "epoch": 35.29025022237692,
      "grad_norm": 8.435323715209961,
      "learning_rate": 2.0591458148019234e-05,
      "loss": 1.8432,
      "step": 912500
    },
    {
      "epoch": 35.294117647058826,
      "grad_norm": 13.763896942138672,
      "learning_rate": 2.058823529411765e-05,
      "loss": 1.8184,
      "step": 912600
    },
    {
      "epoch": 35.297985071740726,
      "grad_norm": 13.697606086730957,
      "learning_rate": 2.058501244021606e-05,
      "loss": 1.8511,
      "step": 912700
    },
    {
      "epoch": 35.30185249642263,
      "grad_norm": 12.641736030578613,
      "learning_rate": 2.0581789586314475e-05,
      "loss": 1.8847,
      "step": 912800
    },
    {
      "epoch": 35.30571992110454,
      "grad_norm": 11.700387001037598,
      "learning_rate": 2.057856673241289e-05,
      "loss": 1.8443,
      "step": 912900
    },
    {
      "epoch": 35.30958734578644,
      "grad_norm": 12.142595291137695,
      "learning_rate": 2.05753438785113e-05,
      "loss": 1.8633,
      "step": 913000
    },
    {
      "epoch": 35.31345477046835,
      "grad_norm": 13.958683967590332,
      "learning_rate": 2.0572121024609715e-05,
      "loss": 1.855,
      "step": 913100
    },
    {
      "epoch": 35.31732219515025,
      "grad_norm": 13.013786315917969,
      "learning_rate": 2.0568898170708127e-05,
      "loss": 1.8291,
      "step": 913200
    },
    {
      "epoch": 35.321189619832154,
      "grad_norm": 16.123125076293945,
      "learning_rate": 2.056567531680654e-05,
      "loss": 1.8452,
      "step": 913300
    },
    {
      "epoch": 35.32505704451406,
      "grad_norm": 11.02022647857666,
      "learning_rate": 2.0562452462904953e-05,
      "loss": 1.9032,
      "step": 913400
    },
    {
      "epoch": 35.32892446919596,
      "grad_norm": 11.916372299194336,
      "learning_rate": 2.0559229609003368e-05,
      "loss": 1.8052,
      "step": 913500
    },
    {
      "epoch": 35.33279189387787,
      "grad_norm": 12.928509712219238,
      "learning_rate": 2.055600675510178e-05,
      "loss": 1.8746,
      "step": 913600
    },
    {
      "epoch": 35.33665931855977,
      "grad_norm": 11.472658157348633,
      "learning_rate": 2.0552783901200194e-05,
      "loss": 1.9182,
      "step": 913700
    },
    {
      "epoch": 35.340526743241675,
      "grad_norm": 9.601164817810059,
      "learning_rate": 2.0549561047298605e-05,
      "loss": 1.8809,
      "step": 913800
    },
    {
      "epoch": 35.34439416792358,
      "grad_norm": 13.038297653198242,
      "learning_rate": 2.054633819339702e-05,
      "loss": 1.8479,
      "step": 913900
    },
    {
      "epoch": 35.34826159260548,
      "grad_norm": 14.133437156677246,
      "learning_rate": 2.054311533949543e-05,
      "loss": 1.9821,
      "step": 914000
    },
    {
      "epoch": 35.35212901728739,
      "grad_norm": 10.514941215515137,
      "learning_rate": 2.0539892485593846e-05,
      "loss": 1.8352,
      "step": 914100
    },
    {
      "epoch": 35.355996441969296,
      "grad_norm": 11.851628303527832,
      "learning_rate": 2.0536669631692257e-05,
      "loss": 1.8828,
      "step": 914200
    },
    {
      "epoch": 35.359863866651196,
      "grad_norm": 11.288623809814453,
      "learning_rate": 2.0533446777790672e-05,
      "loss": 1.8944,
      "step": 914300
    },
    {
      "epoch": 35.3637312913331,
      "grad_norm": 11.603809356689453,
      "learning_rate": 2.0530223923889083e-05,
      "loss": 1.9683,
      "step": 914400
    },
    {
      "epoch": 35.367598716015,
      "grad_norm": 9.191879272460938,
      "learning_rate": 2.0527001069987498e-05,
      "loss": 1.878,
      "step": 914500
    },
    {
      "epoch": 35.37146614069691,
      "grad_norm": 11.635171890258789,
      "learning_rate": 2.052377821608591e-05,
      "loss": 1.8134,
      "step": 914600
    },
    {
      "epoch": 35.37533356537882,
      "grad_norm": 9.237539291381836,
      "learning_rate": 2.052055536218432e-05,
      "loss": 1.8249,
      "step": 914700
    },
    {
      "epoch": 35.37920099006072,
      "grad_norm": 12.983970642089844,
      "learning_rate": 2.0517332508282735e-05,
      "loss": 1.8485,
      "step": 914800
    },
    {
      "epoch": 35.383068414742624,
      "grad_norm": 10.768985748291016,
      "learning_rate": 2.0514109654381147e-05,
      "loss": 1.8964,
      "step": 914900
    },
    {
      "epoch": 35.386935839424524,
      "grad_norm": 12.302746772766113,
      "learning_rate": 2.051088680047956e-05,
      "loss": 1.8284,
      "step": 915000
    },
    {
      "epoch": 35.39080326410643,
      "grad_norm": 10.086318016052246,
      "learning_rate": 2.0507663946577973e-05,
      "loss": 1.8132,
      "step": 915100
    },
    {
      "epoch": 35.39467068878834,
      "grad_norm": 11.636884689331055,
      "learning_rate": 2.0504441092676387e-05,
      "loss": 1.8698,
      "step": 915200
    },
    {
      "epoch": 35.39853811347024,
      "grad_norm": 13.977416038513184,
      "learning_rate": 2.05012182387748e-05,
      "loss": 1.9453,
      "step": 915300
    },
    {
      "epoch": 35.402405538152145,
      "grad_norm": 13.190413475036621,
      "learning_rate": 2.0497995384873213e-05,
      "loss": 1.876,
      "step": 915400
    },
    {
      "epoch": 35.40627296283405,
      "grad_norm": 11.472677230834961,
      "learning_rate": 2.0494772530971625e-05,
      "loss": 1.8548,
      "step": 915500
    },
    {
      "epoch": 35.41014038751595,
      "grad_norm": 11.9122896194458,
      "learning_rate": 2.049154967707004e-05,
      "loss": 1.8666,
      "step": 915600
    },
    {
      "epoch": 35.41400781219786,
      "grad_norm": 10.792607307434082,
      "learning_rate": 2.048832682316845e-05,
      "loss": 1.8579,
      "step": 915700
    },
    {
      "epoch": 35.41787523687976,
      "grad_norm": 13.509923934936523,
      "learning_rate": 2.0485103969266865e-05,
      "loss": 1.8762,
      "step": 915800
    },
    {
      "epoch": 35.421742661561666,
      "grad_norm": 9.529102325439453,
      "learning_rate": 2.0481881115365277e-05,
      "loss": 1.8146,
      "step": 915900
    },
    {
      "epoch": 35.42561008624357,
      "grad_norm": 14.265010833740234,
      "learning_rate": 2.047865826146369e-05,
      "loss": 1.9521,
      "step": 916000
    },
    {
      "epoch": 35.42947751092547,
      "grad_norm": 8.277860641479492,
      "learning_rate": 2.0475435407562106e-05,
      "loss": 1.8856,
      "step": 916100
    },
    {
      "epoch": 35.43334493560738,
      "grad_norm": 13.858261108398438,
      "learning_rate": 2.0472212553660518e-05,
      "loss": 1.9365,
      "step": 916200
    },
    {
      "epoch": 35.43721236028928,
      "grad_norm": 13.764005661010742,
      "learning_rate": 2.0468989699758932e-05,
      "loss": 1.8553,
      "step": 916300
    },
    {
      "epoch": 35.44107978497119,
      "grad_norm": 10.618990898132324,
      "learning_rate": 2.0465766845857347e-05,
      "loss": 1.8392,
      "step": 916400
    },
    {
      "epoch": 35.444947209653094,
      "grad_norm": 10.868117332458496,
      "learning_rate": 2.0462543991955758e-05,
      "loss": 1.8255,
      "step": 916500
    },
    {
      "epoch": 35.448814634334994,
      "grad_norm": 11.132881164550781,
      "learning_rate": 2.0459321138054173e-05,
      "loss": 1.884,
      "step": 916600
    },
    {
      "epoch": 35.4526820590169,
      "grad_norm": 12.47696590423584,
      "learning_rate": 2.0456098284152584e-05,
      "loss": 1.8885,
      "step": 916700
    },
    {
      "epoch": 35.45654948369881,
      "grad_norm": 12.793209075927734,
      "learning_rate": 2.0452875430251e-05,
      "loss": 1.8456,
      "step": 916800
    },
    {
      "epoch": 35.46041690838071,
      "grad_norm": 11.924402236938477,
      "learning_rate": 2.044965257634941e-05,
      "loss": 1.8255,
      "step": 916900
    },
    {
      "epoch": 35.464284333062615,
      "grad_norm": 9.176756858825684,
      "learning_rate": 2.0446429722447825e-05,
      "loss": 1.9159,
      "step": 917000
    },
    {
      "epoch": 35.468151757744515,
      "grad_norm": 8.243681907653809,
      "learning_rate": 2.0443206868546236e-05,
      "loss": 1.8607,
      "step": 917100
    },
    {
      "epoch": 35.47201918242642,
      "grad_norm": 16.005077362060547,
      "learning_rate": 2.043998401464465e-05,
      "loss": 1.813,
      "step": 917200
    },
    {
      "epoch": 35.47588660710833,
      "grad_norm": 12.32227897644043,
      "learning_rate": 2.0436761160743062e-05,
      "loss": 1.7845,
      "step": 917300
    },
    {
      "epoch": 35.47975403179023,
      "grad_norm": 14.062034606933594,
      "learning_rate": 2.0433538306841477e-05,
      "loss": 1.8347,
      "step": 917400
    },
    {
      "epoch": 35.483621456472136,
      "grad_norm": 14.198702812194824,
      "learning_rate": 2.043031545293989e-05,
      "loss": 1.7655,
      "step": 917500
    },
    {
      "epoch": 35.48748888115404,
      "grad_norm": 13.369564056396484,
      "learning_rate": 2.0427092599038303e-05,
      "loss": 1.8569,
      "step": 917600
    },
    {
      "epoch": 35.49135630583594,
      "grad_norm": 12.518683433532715,
      "learning_rate": 2.0423869745136715e-05,
      "loss": 1.8472,
      "step": 917700
    },
    {
      "epoch": 35.49522373051785,
      "grad_norm": 11.093194961547852,
      "learning_rate": 2.0420646891235126e-05,
      "loss": 1.9068,
      "step": 917800
    },
    {
      "epoch": 35.49909115519975,
      "grad_norm": 12.253087043762207,
      "learning_rate": 2.041742403733354e-05,
      "loss": 1.8685,
      "step": 917900
    },
    {
      "epoch": 35.50295857988166,
      "grad_norm": 13.71226692199707,
      "learning_rate": 2.0414201183431952e-05,
      "loss": 1.8639,
      "step": 918000
    },
    {
      "epoch": 35.506826004563564,
      "grad_norm": 16.023048400878906,
      "learning_rate": 2.0410978329530367e-05,
      "loss": 1.8758,
      "step": 918100
    },
    {
      "epoch": 35.510693429245464,
      "grad_norm": 15.28683853149414,
      "learning_rate": 2.0407755475628778e-05,
      "loss": 1.83,
      "step": 918200
    },
    {
      "epoch": 35.51456085392737,
      "grad_norm": 11.044995307922363,
      "learning_rate": 2.0404532621727193e-05,
      "loss": 1.887,
      "step": 918300
    },
    {
      "epoch": 35.51842827860927,
      "grad_norm": 14.103194236755371,
      "learning_rate": 2.0401309767825604e-05,
      "loss": 1.765,
      "step": 918400
    },
    {
      "epoch": 35.52229570329118,
      "grad_norm": 11.992616653442383,
      "learning_rate": 2.039808691392402e-05,
      "loss": 1.8198,
      "step": 918500
    },
    {
      "epoch": 35.526163127973085,
      "grad_norm": 12.187721252441406,
      "learning_rate": 2.039486406002243e-05,
      "loss": 1.8412,
      "step": 918600
    },
    {
      "epoch": 35.530030552654985,
      "grad_norm": 12.690452575683594,
      "learning_rate": 2.0391641206120845e-05,
      "loss": 1.8423,
      "step": 918700
    },
    {
      "epoch": 35.53389797733689,
      "grad_norm": 10.74512004852295,
      "learning_rate": 2.0388418352219256e-05,
      "loss": 1.8058,
      "step": 918800
    },
    {
      "epoch": 35.5377654020188,
      "grad_norm": 13.231966972351074,
      "learning_rate": 2.038519549831767e-05,
      "loss": 1.8901,
      "step": 918900
    },
    {
      "epoch": 35.5416328267007,
      "grad_norm": 11.36391544342041,
      "learning_rate": 2.0381972644416082e-05,
      "loss": 1.8645,
      "step": 919000
    },
    {
      "epoch": 35.545500251382606,
      "grad_norm": 9.083229064941406,
      "learning_rate": 2.0378749790514497e-05,
      "loss": 1.7834,
      "step": 919100
    },
    {
      "epoch": 35.549367676064506,
      "grad_norm": 10.125862121582031,
      "learning_rate": 2.0375526936612908e-05,
      "loss": 1.754,
      "step": 919200
    },
    {
      "epoch": 35.55323510074641,
      "grad_norm": 11.584052085876465,
      "learning_rate": 2.0372304082711323e-05,
      "loss": 1.8281,
      "step": 919300
    },
    {
      "epoch": 35.55710252542832,
      "grad_norm": 12.537476539611816,
      "learning_rate": 2.0369081228809738e-05,
      "loss": 1.784,
      "step": 919400
    },
    {
      "epoch": 35.56096995011022,
      "grad_norm": 12.431449890136719,
      "learning_rate": 2.036585837490815e-05,
      "loss": 1.7764,
      "step": 919500
    },
    {
      "epoch": 35.56483737479213,
      "grad_norm": 14.590300559997559,
      "learning_rate": 2.0362635521006564e-05,
      "loss": 1.9063,
      "step": 919600
    },
    {
      "epoch": 35.56870479947403,
      "grad_norm": 11.11299991607666,
      "learning_rate": 2.0359412667104975e-05,
      "loss": 1.8318,
      "step": 919700
    },
    {
      "epoch": 35.572572224155934,
      "grad_norm": 13.340275764465332,
      "learning_rate": 2.035618981320339e-05,
      "loss": 1.9158,
      "step": 919800
    },
    {
      "epoch": 35.57643964883784,
      "grad_norm": 12.170660018920898,
      "learning_rate": 2.0352966959301805e-05,
      "loss": 1.836,
      "step": 919900
    },
    {
      "epoch": 35.58030707351974,
      "grad_norm": 9.844132423400879,
      "learning_rate": 2.0349744105400216e-05,
      "loss": 1.789,
      "step": 920000
    },
    {
      "epoch": 35.58417449820165,
      "grad_norm": 11.085965156555176,
      "learning_rate": 2.034652125149863e-05,
      "loss": 1.9045,
      "step": 920100
    },
    {
      "epoch": 35.588041922883555,
      "grad_norm": 13.302108764648438,
      "learning_rate": 2.0343298397597042e-05,
      "loss": 1.8171,
      "step": 920200
    },
    {
      "epoch": 35.591909347565455,
      "grad_norm": 13.24267578125,
      "learning_rate": 2.0340075543695457e-05,
      "loss": 1.8966,
      "step": 920300
    },
    {
      "epoch": 35.59577677224736,
      "grad_norm": 11.833512306213379,
      "learning_rate": 2.0336852689793868e-05,
      "loss": 1.7892,
      "step": 920400
    },
    {
      "epoch": 35.59964419692926,
      "grad_norm": 12.699031829833984,
      "learning_rate": 2.0333629835892283e-05,
      "loss": 1.8487,
      "step": 920500
    },
    {
      "epoch": 35.60351162161117,
      "grad_norm": 13.003890037536621,
      "learning_rate": 2.0330406981990694e-05,
      "loss": 1.9075,
      "step": 920600
    },
    {
      "epoch": 35.607379046293076,
      "grad_norm": 10.541341781616211,
      "learning_rate": 2.0327184128089105e-05,
      "loss": 1.8001,
      "step": 920700
    },
    {
      "epoch": 35.611246470974976,
      "grad_norm": 11.654650688171387,
      "learning_rate": 2.032396127418752e-05,
      "loss": 1.8295,
      "step": 920800
    },
    {
      "epoch": 35.61511389565688,
      "grad_norm": 13.541051864624023,
      "learning_rate": 2.032073842028593e-05,
      "loss": 1.8525,
      "step": 920900
    },
    {
      "epoch": 35.61898132033879,
      "grad_norm": 14.598859786987305,
      "learning_rate": 2.0317515566384346e-05,
      "loss": 1.9058,
      "step": 921000
    },
    {
      "epoch": 35.62284874502069,
      "grad_norm": 14.877943992614746,
      "learning_rate": 2.0314292712482757e-05,
      "loss": 1.8794,
      "step": 921100
    },
    {
      "epoch": 35.6267161697026,
      "grad_norm": 14.49198055267334,
      "learning_rate": 2.0311069858581172e-05,
      "loss": 1.8317,
      "step": 921200
    },
    {
      "epoch": 35.6305835943845,
      "grad_norm": 12.368794441223145,
      "learning_rate": 2.0307847004679583e-05,
      "loss": 1.8805,
      "step": 921300
    },
    {
      "epoch": 35.634451019066404,
      "grad_norm": 13.116061210632324,
      "learning_rate": 2.0304624150777998e-05,
      "loss": 1.9387,
      "step": 921400
    },
    {
      "epoch": 35.63831844374831,
      "grad_norm": 12.608227729797363,
      "learning_rate": 2.030140129687641e-05,
      "loss": 1.8376,
      "step": 921500
    },
    {
      "epoch": 35.64218586843021,
      "grad_norm": 12.102368354797363,
      "learning_rate": 2.0298178442974824e-05,
      "loss": 1.8669,
      "step": 921600
    },
    {
      "epoch": 35.64605329311212,
      "grad_norm": 12.638453483581543,
      "learning_rate": 2.0294955589073236e-05,
      "loss": 1.8625,
      "step": 921700
    },
    {
      "epoch": 35.64992071779402,
      "grad_norm": 12.855938911437988,
      "learning_rate": 2.029173273517165e-05,
      "loss": 1.7754,
      "step": 921800
    },
    {
      "epoch": 35.653788142475925,
      "grad_norm": 15.203673362731934,
      "learning_rate": 2.028850988127006e-05,
      "loss": 1.7863,
      "step": 921900
    },
    {
      "epoch": 35.65765556715783,
      "grad_norm": 10.900955200195312,
      "learning_rate": 2.0285287027368476e-05,
      "loss": 1.9245,
      "step": 922000
    },
    {
      "epoch": 35.66152299183973,
      "grad_norm": 9.871295928955078,
      "learning_rate": 2.0282064173466888e-05,
      "loss": 1.8733,
      "step": 922100
    },
    {
      "epoch": 35.66539041652164,
      "grad_norm": 14.129347801208496,
      "learning_rate": 2.0278841319565302e-05,
      "loss": 1.7512,
      "step": 922200
    },
    {
      "epoch": 35.669257841203546,
      "grad_norm": 10.77399730682373,
      "learning_rate": 2.0275618465663714e-05,
      "loss": 1.7987,
      "step": 922300
    },
    {
      "epoch": 35.673125265885446,
      "grad_norm": 15.129782676696777,
      "learning_rate": 2.027239561176213e-05,
      "loss": 1.8442,
      "step": 922400
    },
    {
      "epoch": 35.67699269056735,
      "grad_norm": 10.417698860168457,
      "learning_rate": 2.026917275786054e-05,
      "loss": 1.8485,
      "step": 922500
    },
    {
      "epoch": 35.68086011524925,
      "grad_norm": 11.019693374633789,
      "learning_rate": 2.0265949903958954e-05,
      "loss": 1.8466,
      "step": 922600
    },
    {
      "epoch": 35.68472753993116,
      "grad_norm": 11.99123477935791,
      "learning_rate": 2.0262727050057366e-05,
      "loss": 1.7904,
      "step": 922700
    },
    {
      "epoch": 35.68859496461307,
      "grad_norm": 10.267935752868652,
      "learning_rate": 2.025950419615578e-05,
      "loss": 1.8774,
      "step": 922800
    },
    {
      "epoch": 35.69246238929497,
      "grad_norm": 15.329498291015625,
      "learning_rate": 2.0256281342254195e-05,
      "loss": 1.869,
      "step": 922900
    },
    {
      "epoch": 35.696329813976874,
      "grad_norm": 15.508536338806152,
      "learning_rate": 2.0253058488352607e-05,
      "loss": 1.7879,
      "step": 923000
    },
    {
      "epoch": 35.700197238658774,
      "grad_norm": 12.694683074951172,
      "learning_rate": 2.024983563445102e-05,
      "loss": 1.8033,
      "step": 923100
    },
    {
      "epoch": 35.70406466334068,
      "grad_norm": 12.900975227355957,
      "learning_rate": 2.0246612780549436e-05,
      "loss": 1.8258,
      "step": 923200
    },
    {
      "epoch": 35.70793208802259,
      "grad_norm": 15.899958610534668,
      "learning_rate": 2.0243389926647847e-05,
      "loss": 1.8226,
      "step": 923300
    },
    {
      "epoch": 35.71179951270449,
      "grad_norm": 10.045565605163574,
      "learning_rate": 2.0240167072746262e-05,
      "loss": 1.8725,
      "step": 923400
    },
    {
      "epoch": 35.715666937386395,
      "grad_norm": 11.375630378723145,
      "learning_rate": 2.0236944218844673e-05,
      "loss": 1.9162,
      "step": 923500
    },
    {
      "epoch": 35.7195343620683,
      "grad_norm": 13.526897430419922,
      "learning_rate": 2.0233721364943085e-05,
      "loss": 1.9774,
      "step": 923600
    },
    {
      "epoch": 35.7234017867502,
      "grad_norm": 12.914379119873047,
      "learning_rate": 2.02304985110415e-05,
      "loss": 1.887,
      "step": 923700
    },
    {
      "epoch": 35.72726921143211,
      "grad_norm": 9.640271186828613,
      "learning_rate": 2.022727565713991e-05,
      "loss": 1.8836,
      "step": 923800
    },
    {
      "epoch": 35.73113663611401,
      "grad_norm": 10.549599647521973,
      "learning_rate": 2.0224052803238325e-05,
      "loss": 1.8231,
      "step": 923900
    },
    {
      "epoch": 35.735004060795916,
      "grad_norm": 11.182308197021484,
      "learning_rate": 2.0220829949336737e-05,
      "loss": 1.8718,
      "step": 924000
    },
    {
      "epoch": 35.73887148547782,
      "grad_norm": 11.371186256408691,
      "learning_rate": 2.021760709543515e-05,
      "loss": 1.8774,
      "step": 924100
    },
    {
      "epoch": 35.74273891015972,
      "grad_norm": 11.555481910705566,
      "learning_rate": 2.0214384241533563e-05,
      "loss": 1.84,
      "step": 924200
    },
    {
      "epoch": 35.74660633484163,
      "grad_norm": 12.66186237335205,
      "learning_rate": 2.0211161387631978e-05,
      "loss": 1.9003,
      "step": 924300
    },
    {
      "epoch": 35.75047375952353,
      "grad_norm": 11.911261558532715,
      "learning_rate": 2.020793853373039e-05,
      "loss": 1.9052,
      "step": 924400
    },
    {
      "epoch": 35.75434118420544,
      "grad_norm": 12.161076545715332,
      "learning_rate": 2.0204715679828804e-05,
      "loss": 1.8261,
      "step": 924500
    },
    {
      "epoch": 35.758208608887344,
      "grad_norm": 10.365572929382324,
      "learning_rate": 2.0201492825927215e-05,
      "loss": 1.8816,
      "step": 924600
    },
    {
      "epoch": 35.762076033569244,
      "grad_norm": 10.764898300170898,
      "learning_rate": 2.019826997202563e-05,
      "loss": 1.834,
      "step": 924700
    },
    {
      "epoch": 35.76594345825115,
      "grad_norm": 12.843708992004395,
      "learning_rate": 2.019504711812404e-05,
      "loss": 1.7878,
      "step": 924800
    },
    {
      "epoch": 35.76981088293306,
      "grad_norm": 10.743758201599121,
      "learning_rate": 2.0191824264222456e-05,
      "loss": 1.7616,
      "step": 924900
    },
    {
      "epoch": 35.77367830761496,
      "grad_norm": 12.623729705810547,
      "learning_rate": 2.0188601410320867e-05,
      "loss": 1.8439,
      "step": 925000
    },
    {
      "epoch": 35.777545732296865,
      "grad_norm": 13.039793014526367,
      "learning_rate": 2.0185378556419282e-05,
      "loss": 1.8422,
      "step": 925100
    },
    {
      "epoch": 35.781413156978765,
      "grad_norm": 9.332144737243652,
      "learning_rate": 2.0182155702517693e-05,
      "loss": 1.8957,
      "step": 925200
    },
    {
      "epoch": 35.78528058166067,
      "grad_norm": 11.756011962890625,
      "learning_rate": 2.0178932848616108e-05,
      "loss": 1.8312,
      "step": 925300
    },
    {
      "epoch": 35.78914800634258,
      "grad_norm": 12.573752403259277,
      "learning_rate": 2.017570999471452e-05,
      "loss": 1.8776,
      "step": 925400
    },
    {
      "epoch": 35.79301543102448,
      "grad_norm": 7.878185749053955,
      "learning_rate": 2.0172487140812934e-05,
      "loss": 1.8796,
      "step": 925500
    },
    {
      "epoch": 35.796882855706386,
      "grad_norm": 11.586759567260742,
      "learning_rate": 2.0169264286911345e-05,
      "loss": 1.8393,
      "step": 925600
    },
    {
      "epoch": 35.80075028038829,
      "grad_norm": 12.270347595214844,
      "learning_rate": 2.016604143300976e-05,
      "loss": 1.8222,
      "step": 925700
    },
    {
      "epoch": 35.80461770507019,
      "grad_norm": 11.598388671875,
      "learning_rate": 2.016281857910817e-05,
      "loss": 1.8903,
      "step": 925800
    },
    {
      "epoch": 35.8084851297521,
      "grad_norm": 10.1084566116333,
      "learning_rate": 2.0159595725206586e-05,
      "loss": 1.7719,
      "step": 925900
    },
    {
      "epoch": 35.812352554434,
      "grad_norm": 11.347857475280762,
      "learning_rate": 2.0156372871304997e-05,
      "loss": 1.8916,
      "step": 926000
    },
    {
      "epoch": 35.81621997911591,
      "grad_norm": 14.911141395568848,
      "learning_rate": 2.0153150017403412e-05,
      "loss": 1.8074,
      "step": 926100
    },
    {
      "epoch": 35.820087403797814,
      "grad_norm": 10.51887321472168,
      "learning_rate": 2.0149927163501823e-05,
      "loss": 1.9314,
      "step": 926200
    },
    {
      "epoch": 35.823954828479714,
      "grad_norm": 11.009356498718262,
      "learning_rate": 2.0146704309600238e-05,
      "loss": 1.7448,
      "step": 926300
    },
    {
      "epoch": 35.82782225316162,
      "grad_norm": 10.073753356933594,
      "learning_rate": 2.0143481455698653e-05,
      "loss": 1.7383,
      "step": 926400
    },
    {
      "epoch": 35.83168967784352,
      "grad_norm": 10.99728775024414,
      "learning_rate": 2.0140258601797064e-05,
      "loss": 1.8433,
      "step": 926500
    },
    {
      "epoch": 35.83555710252543,
      "grad_norm": 12.414641380310059,
      "learning_rate": 2.013703574789548e-05,
      "loss": 1.8782,
      "step": 926600
    },
    {
      "epoch": 35.839424527207335,
      "grad_norm": 14.469911575317383,
      "learning_rate": 2.013381289399389e-05,
      "loss": 1.8709,
      "step": 926700
    },
    {
      "epoch": 35.843291951889235,
      "grad_norm": 10.86159610748291,
      "learning_rate": 2.0130590040092305e-05,
      "loss": 1.7765,
      "step": 926800
    },
    {
      "epoch": 35.84715937657114,
      "grad_norm": 11.337695121765137,
      "learning_rate": 2.0127367186190716e-05,
      "loss": 1.8566,
      "step": 926900
    },
    {
      "epoch": 35.85102680125305,
      "grad_norm": 15.374063491821289,
      "learning_rate": 2.012414433228913e-05,
      "loss": 1.8398,
      "step": 927000
    },
    {
      "epoch": 35.85489422593495,
      "grad_norm": 13.708353996276855,
      "learning_rate": 2.0120921478387542e-05,
      "loss": 1.7536,
      "step": 927100
    },
    {
      "epoch": 35.858761650616856,
      "grad_norm": 9.340502738952637,
      "learning_rate": 2.0117698624485957e-05,
      "loss": 1.845,
      "step": 927200
    },
    {
      "epoch": 35.862629075298756,
      "grad_norm": 11.443768501281738,
      "learning_rate": 2.0114475770584368e-05,
      "loss": 1.8538,
      "step": 927300
    },
    {
      "epoch": 35.86649649998066,
      "grad_norm": 12.519523620605469,
      "learning_rate": 2.0111252916682783e-05,
      "loss": 1.8611,
      "step": 927400
    },
    {
      "epoch": 35.87036392466257,
      "grad_norm": 12.672920227050781,
      "learning_rate": 2.0108030062781194e-05,
      "loss": 1.8547,
      "step": 927500
    },
    {
      "epoch": 35.87423134934447,
      "grad_norm": 11.125753402709961,
      "learning_rate": 2.010480720887961e-05,
      "loss": 1.8075,
      "step": 927600
    },
    {
      "epoch": 35.87809877402638,
      "grad_norm": 11.927342414855957,
      "learning_rate": 2.010158435497802e-05,
      "loss": 1.814,
      "step": 927700
    },
    {
      "epoch": 35.88196619870828,
      "grad_norm": 10.369831085205078,
      "learning_rate": 2.0098361501076435e-05,
      "loss": 1.8293,
      "step": 927800
    },
    {
      "epoch": 35.885833623390184,
      "grad_norm": 9.886731147766113,
      "learning_rate": 2.0095138647174846e-05,
      "loss": 1.8802,
      "step": 927900
    },
    {
      "epoch": 35.88970104807209,
      "grad_norm": 13.341404914855957,
      "learning_rate": 2.009191579327326e-05,
      "loss": 1.8367,
      "step": 928000
    },
    {
      "epoch": 35.89356847275399,
      "grad_norm": 13.196358680725098,
      "learning_rate": 2.0088692939371672e-05,
      "loss": 1.9092,
      "step": 928100
    },
    {
      "epoch": 35.8974358974359,
      "grad_norm": 11.193951606750488,
      "learning_rate": 2.0085470085470087e-05,
      "loss": 1.7855,
      "step": 928200
    },
    {
      "epoch": 35.901303322117805,
      "grad_norm": 11.107669830322266,
      "learning_rate": 2.00822472315685e-05,
      "loss": 1.7578,
      "step": 928300
    },
    {
      "epoch": 35.905170746799705,
      "grad_norm": 15.355325698852539,
      "learning_rate": 2.0079024377666913e-05,
      "loss": 1.8166,
      "step": 928400
    },
    {
      "epoch": 35.90903817148161,
      "grad_norm": 10.23465347290039,
      "learning_rate": 2.0075801523765325e-05,
      "loss": 1.7693,
      "step": 928500
    },
    {
      "epoch": 35.91290559616351,
      "grad_norm": 8.922407150268555,
      "learning_rate": 2.007257866986374e-05,
      "loss": 1.8894,
      "step": 928600
    },
    {
      "epoch": 35.91677302084542,
      "grad_norm": 13.089872360229492,
      "learning_rate": 2.006935581596215e-05,
      "loss": 1.894,
      "step": 928700
    },
    {
      "epoch": 35.920640445527326,
      "grad_norm": 9.241903305053711,
      "learning_rate": 2.0066132962060565e-05,
      "loss": 1.8756,
      "step": 928800
    },
    {
      "epoch": 35.924507870209226,
      "grad_norm": 14.083621978759766,
      "learning_rate": 2.0062910108158977e-05,
      "loss": 1.7765,
      "step": 928900
    },
    {
      "epoch": 35.92837529489113,
      "grad_norm": 10.060258865356445,
      "learning_rate": 2.005968725425739e-05,
      "loss": 1.8055,
      "step": 929000
    },
    {
      "epoch": 35.93224271957304,
      "grad_norm": 16.304948806762695,
      "learning_rate": 2.0056464400355803e-05,
      "loss": 1.9145,
      "step": 929100
    },
    {
      "epoch": 35.93611014425494,
      "grad_norm": 12.01182746887207,
      "learning_rate": 2.0053241546454217e-05,
      "loss": 1.844,
      "step": 929200
    },
    {
      "epoch": 35.93997756893685,
      "grad_norm": 13.689645767211914,
      "learning_rate": 2.005001869255263e-05,
      "loss": 1.8073,
      "step": 929300
    },
    {
      "epoch": 35.94384499361875,
      "grad_norm": 12.820733070373535,
      "learning_rate": 2.0046795838651044e-05,
      "loss": 1.8013,
      "step": 929400
    },
    {
      "epoch": 35.947712418300654,
      "grad_norm": 9.574947357177734,
      "learning_rate": 2.0043572984749455e-05,
      "loss": 1.8716,
      "step": 929500
    },
    {
      "epoch": 35.95157984298256,
      "grad_norm": 14.206594467163086,
      "learning_rate": 2.004035013084787e-05,
      "loss": 1.8336,
      "step": 929600
    },
    {
      "epoch": 35.95544726766446,
      "grad_norm": 13.03244686126709,
      "learning_rate": 2.003712727694628e-05,
      "loss": 1.8009,
      "step": 929700
    },
    {
      "epoch": 35.95931469234637,
      "grad_norm": 12.305021286010742,
      "learning_rate": 2.0033904423044696e-05,
      "loss": 1.8562,
      "step": 929800
    },
    {
      "epoch": 35.96318211702827,
      "grad_norm": 13.081995010375977,
      "learning_rate": 2.003068156914311e-05,
      "loss": 1.8636,
      "step": 929900
    },
    {
      "epoch": 35.967049541710175,
      "grad_norm": 12.719862937927246,
      "learning_rate": 2.002745871524152e-05,
      "loss": 1.8936,
      "step": 930000
    },
    {
      "epoch": 35.97091696639208,
      "grad_norm": 11.921387672424316,
      "learning_rate": 2.0024235861339936e-05,
      "loss": 1.8037,
      "step": 930100
    },
    {
      "epoch": 35.97478439107398,
      "grad_norm": 9.100837707519531,
      "learning_rate": 2.0021013007438348e-05,
      "loss": 1.873,
      "step": 930200
    },
    {
      "epoch": 35.97865181575589,
      "grad_norm": 13.381890296936035,
      "learning_rate": 2.0017790153536762e-05,
      "loss": 1.8276,
      "step": 930300
    },
    {
      "epoch": 35.982519240437796,
      "grad_norm": 15.822274208068848,
      "learning_rate": 2.0014567299635174e-05,
      "loss": 1.813,
      "step": 930400
    },
    {
      "epoch": 35.986386665119696,
      "grad_norm": 13.431097030639648,
      "learning_rate": 2.001134444573359e-05,
      "loss": 1.9712,
      "step": 930500
    },
    {
      "epoch": 35.9902540898016,
      "grad_norm": 9.615747451782227,
      "learning_rate": 2.0008121591832e-05,
      "loss": 1.8905,
      "step": 930600
    },
    {
      "epoch": 35.9941215144835,
      "grad_norm": 14.413962364196777,
      "learning_rate": 2.0004898737930415e-05,
      "loss": 1.7866,
      "step": 930700
    },
    {
      "epoch": 35.99798893916541,
      "grad_norm": 14.081573486328125,
      "learning_rate": 2.0001675884028826e-05,
      "loss": 1.8241,
      "step": 930800
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.7820439338684082,
      "eval_runtime": 2.9975,
      "eval_samples_per_second": 454.049,
      "eval_steps_per_second": 454.049,
      "step": 930852
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.642256736755371,
      "eval_runtime": 55.9553,
      "eval_samples_per_second": 462.101,
      "eval_steps_per_second": 462.101,
      "step": 930852
    },
    {
      "epoch": 36.00185636384732,
      "grad_norm": 12.934301376342773,
      "learning_rate": 1.999845303012724e-05,
      "loss": 1.8149,
      "step": 930900
    },
    {
      "epoch": 36.00572378852922,
      "grad_norm": 12.27440357208252,
      "learning_rate": 1.9995230176225652e-05,
      "loss": 1.8018,
      "step": 931000
    },
    {
      "epoch": 36.009591213211124,
      "grad_norm": 9.288086891174316,
      "learning_rate": 1.9992007322324067e-05,
      "loss": 1.8588,
      "step": 931100
    },
    {
      "epoch": 36.013458637893024,
      "grad_norm": 8.094292640686035,
      "learning_rate": 1.9988784468422478e-05,
      "loss": 1.7565,
      "step": 931200
    },
    {
      "epoch": 36.01732606257493,
      "grad_norm": 9.42314624786377,
      "learning_rate": 1.9985561614520893e-05,
      "loss": 1.8381,
      "step": 931300
    },
    {
      "epoch": 36.02119348725684,
      "grad_norm": 9.198464393615723,
      "learning_rate": 1.9982338760619304e-05,
      "loss": 1.8168,
      "step": 931400
    },
    {
      "epoch": 36.02506091193874,
      "grad_norm": 14.08964729309082,
      "learning_rate": 1.997911590671772e-05,
      "loss": 1.8047,
      "step": 931500
    },
    {
      "epoch": 36.028928336620645,
      "grad_norm": 14.107170104980469,
      "learning_rate": 1.997589305281613e-05,
      "loss": 1.8626,
      "step": 931600
    },
    {
      "epoch": 36.03279576130255,
      "grad_norm": 11.018898010253906,
      "learning_rate": 1.9972670198914545e-05,
      "loss": 1.7914,
      "step": 931700
    },
    {
      "epoch": 36.03666318598445,
      "grad_norm": 10.951723098754883,
      "learning_rate": 1.9969447345012956e-05,
      "loss": 1.7638,
      "step": 931800
    },
    {
      "epoch": 36.04053061066636,
      "grad_norm": 10.799715995788574,
      "learning_rate": 1.996622449111137e-05,
      "loss": 1.8238,
      "step": 931900
    },
    {
      "epoch": 36.04439803534826,
      "grad_norm": 12.919217109680176,
      "learning_rate": 1.9963001637209782e-05,
      "loss": 1.9372,
      "step": 932000
    },
    {
      "epoch": 36.048265460030166,
      "grad_norm": 11.665075302124023,
      "learning_rate": 1.9959778783308197e-05,
      "loss": 1.8642,
      "step": 932100
    },
    {
      "epoch": 36.05213288471207,
      "grad_norm": 11.936009407043457,
      "learning_rate": 1.9956555929406608e-05,
      "loss": 1.7615,
      "step": 932200
    },
    {
      "epoch": 36.05600030939397,
      "grad_norm": 11.489657402038574,
      "learning_rate": 1.9953333075505023e-05,
      "loss": 1.8856,
      "step": 932300
    },
    {
      "epoch": 36.05986773407588,
      "grad_norm": 14.723590850830078,
      "learning_rate": 1.9950110221603434e-05,
      "loss": 1.8402,
      "step": 932400
    },
    {
      "epoch": 36.06373515875778,
      "grad_norm": 13.15542221069336,
      "learning_rate": 1.9946887367701846e-05,
      "loss": 1.7531,
      "step": 932500
    },
    {
      "epoch": 36.06760258343969,
      "grad_norm": 11.574969291687012,
      "learning_rate": 1.994366451380026e-05,
      "loss": 1.8433,
      "step": 932600
    },
    {
      "epoch": 36.071470008121594,
      "grad_norm": 11.433890342712402,
      "learning_rate": 1.994044165989867e-05,
      "loss": 1.9068,
      "step": 932700
    },
    {
      "epoch": 36.075337432803494,
      "grad_norm": 11.379045486450195,
      "learning_rate": 1.9937218805997086e-05,
      "loss": 1.7998,
      "step": 932800
    },
    {
      "epoch": 36.0792048574854,
      "grad_norm": 9.862142562866211,
      "learning_rate": 1.99339959520955e-05,
      "loss": 1.854,
      "step": 932900
    },
    {
      "epoch": 36.08307228216731,
      "grad_norm": 10.414083480834961,
      "learning_rate": 1.9930773098193912e-05,
      "loss": 1.8534,
      "step": 933000
    },
    {
      "epoch": 36.08693970684921,
      "grad_norm": 15.899467468261719,
      "learning_rate": 1.9927550244292327e-05,
      "loss": 1.7383,
      "step": 933100
    },
    {
      "epoch": 36.090807131531115,
      "grad_norm": 12.517502784729004,
      "learning_rate": 1.9924327390390742e-05,
      "loss": 1.8367,
      "step": 933200
    },
    {
      "epoch": 36.094674556213015,
      "grad_norm": 9.638668060302734,
      "learning_rate": 1.9921104536489153e-05,
      "loss": 1.9034,
      "step": 933300
    },
    {
      "epoch": 36.09854198089492,
      "grad_norm": 12.105077743530273,
      "learning_rate": 1.9917881682587568e-05,
      "loss": 1.8071,
      "step": 933400
    },
    {
      "epoch": 36.10240940557683,
      "grad_norm": 9.408824920654297,
      "learning_rate": 1.991465882868598e-05,
      "loss": 1.8074,
      "step": 933500
    },
    {
      "epoch": 36.10627683025873,
      "grad_norm": 9.287191390991211,
      "learning_rate": 1.9911435974784394e-05,
      "loss": 1.7544,
      "step": 933600
    },
    {
      "epoch": 36.110144254940636,
      "grad_norm": 10.661112785339355,
      "learning_rate": 1.9908213120882805e-05,
      "loss": 1.8085,
      "step": 933700
    },
    {
      "epoch": 36.11401167962254,
      "grad_norm": 7.994988441467285,
      "learning_rate": 1.990499026698122e-05,
      "loss": 1.9223,
      "step": 933800
    },
    {
      "epoch": 36.11787910430444,
      "grad_norm": 14.367241859436035,
      "learning_rate": 1.990176741307963e-05,
      "loss": 1.8076,
      "step": 933900
    },
    {
      "epoch": 36.12174652898635,
      "grad_norm": 15.765066146850586,
      "learning_rate": 1.9898544559178046e-05,
      "loss": 1.8685,
      "step": 934000
    },
    {
      "epoch": 36.12561395366825,
      "grad_norm": 10.453872680664062,
      "learning_rate": 1.9895321705276457e-05,
      "loss": 1.8809,
      "step": 934100
    },
    {
      "epoch": 36.12948137835016,
      "grad_norm": 16.516019821166992,
      "learning_rate": 1.9892098851374872e-05,
      "loss": 1.8152,
      "step": 934200
    },
    {
      "epoch": 36.133348803032064,
      "grad_norm": 10.83230972290039,
      "learning_rate": 1.9888875997473283e-05,
      "loss": 1.849,
      "step": 934300
    },
    {
      "epoch": 36.137216227713964,
      "grad_norm": 8.442328453063965,
      "learning_rate": 1.9885653143571698e-05,
      "loss": 1.8162,
      "step": 934400
    },
    {
      "epoch": 36.14108365239587,
      "grad_norm": 12.161645889282227,
      "learning_rate": 1.988243028967011e-05,
      "loss": 1.813,
      "step": 934500
    },
    {
      "epoch": 36.14495107707777,
      "grad_norm": 11.972478866577148,
      "learning_rate": 1.9879207435768524e-05,
      "loss": 1.8554,
      "step": 934600
    },
    {
      "epoch": 36.14881850175968,
      "grad_norm": 13.978119850158691,
      "learning_rate": 1.9875984581866935e-05,
      "loss": 1.8708,
      "step": 934700
    },
    {
      "epoch": 36.152685926441585,
      "grad_norm": 15.437541961669922,
      "learning_rate": 1.987276172796535e-05,
      "loss": 1.7958,
      "step": 934800
    },
    {
      "epoch": 36.156553351123485,
      "grad_norm": 14.32836627960205,
      "learning_rate": 1.986953887406376e-05,
      "loss": 1.7698,
      "step": 934900
    },
    {
      "epoch": 36.16042077580539,
      "grad_norm": 10.962669372558594,
      "learning_rate": 1.9866316020162176e-05,
      "loss": 1.7278,
      "step": 935000
    },
    {
      "epoch": 36.1642882004873,
      "grad_norm": 7.3535590171813965,
      "learning_rate": 1.9863093166260588e-05,
      "loss": 1.8671,
      "step": 935100
    },
    {
      "epoch": 36.1681556251692,
      "grad_norm": 8.371582984924316,
      "learning_rate": 1.9859870312359002e-05,
      "loss": 1.8879,
      "step": 935200
    },
    {
      "epoch": 36.172023049851106,
      "grad_norm": 11.295363426208496,
      "learning_rate": 1.9856647458457414e-05,
      "loss": 1.9238,
      "step": 935300
    },
    {
      "epoch": 36.175890474533006,
      "grad_norm": 12.307753562927246,
      "learning_rate": 1.9853424604555825e-05,
      "loss": 1.8551,
      "step": 935400
    },
    {
      "epoch": 36.17975789921491,
      "grad_norm": 11.918168067932129,
      "learning_rate": 1.985020175065424e-05,
      "loss": 1.9386,
      "step": 935500
    },
    {
      "epoch": 36.18362532389682,
      "grad_norm": 13.183812141418457,
      "learning_rate": 1.984697889675265e-05,
      "loss": 1.8245,
      "step": 935600
    },
    {
      "epoch": 36.18749274857872,
      "grad_norm": 11.979469299316406,
      "learning_rate": 1.9843756042851066e-05,
      "loss": 1.8491,
      "step": 935700
    },
    {
      "epoch": 36.19136017326063,
      "grad_norm": 11.598188400268555,
      "learning_rate": 1.9840533188949477e-05,
      "loss": 1.8696,
      "step": 935800
    },
    {
      "epoch": 36.19522759794253,
      "grad_norm": 13.992341995239258,
      "learning_rate": 1.9837310335047892e-05,
      "loss": 1.8629,
      "step": 935900
    },
    {
      "epoch": 36.199095022624434,
      "grad_norm": 11.35246467590332,
      "learning_rate": 1.9834087481146303e-05,
      "loss": 1.8284,
      "step": 936000
    },
    {
      "epoch": 36.20296244730634,
      "grad_norm": 13.087555885314941,
      "learning_rate": 1.9830864627244718e-05,
      "loss": 1.862,
      "step": 936100
    },
    {
      "epoch": 36.20682987198824,
      "grad_norm": 10.627118110656738,
      "learning_rate": 1.982764177334313e-05,
      "loss": 1.7749,
      "step": 936200
    },
    {
      "epoch": 36.21069729667015,
      "grad_norm": 10.944149017333984,
      "learning_rate": 1.9824418919441544e-05,
      "loss": 1.9012,
      "step": 936300
    },
    {
      "epoch": 36.214564721352055,
      "grad_norm": 11.403600692749023,
      "learning_rate": 1.982119606553996e-05,
      "loss": 1.8513,
      "step": 936400
    },
    {
      "epoch": 36.218432146033955,
      "grad_norm": 13.66299057006836,
      "learning_rate": 1.981797321163837e-05,
      "loss": 1.8202,
      "step": 936500
    },
    {
      "epoch": 36.22229957071586,
      "grad_norm": 9.757275581359863,
      "learning_rate": 1.9814750357736785e-05,
      "loss": 1.8725,
      "step": 936600
    },
    {
      "epoch": 36.22616699539776,
      "grad_norm": 10.816365242004395,
      "learning_rate": 1.98115275038352e-05,
      "loss": 1.8244,
      "step": 936700
    },
    {
      "epoch": 36.23003442007967,
      "grad_norm": 12.71812629699707,
      "learning_rate": 1.980830464993361e-05,
      "loss": 1.8601,
      "step": 936800
    },
    {
      "epoch": 36.233901844761576,
      "grad_norm": 11.86783218383789,
      "learning_rate": 1.9805081796032025e-05,
      "loss": 1.8272,
      "step": 936900
    },
    {
      "epoch": 36.237769269443476,
      "grad_norm": 14.36357307434082,
      "learning_rate": 1.9801858942130437e-05,
      "loss": 1.8143,
      "step": 937000
    },
    {
      "epoch": 36.24163669412538,
      "grad_norm": 12.57618236541748,
      "learning_rate": 1.979863608822885e-05,
      "loss": 1.9135,
      "step": 937100
    },
    {
      "epoch": 36.24550411880729,
      "grad_norm": 16.392436981201172,
      "learning_rate": 1.9795413234327263e-05,
      "loss": 1.7875,
      "step": 937200
    },
    {
      "epoch": 36.24937154348919,
      "grad_norm": 11.592558860778809,
      "learning_rate": 1.9792190380425678e-05,
      "loss": 1.7849,
      "step": 937300
    },
    {
      "epoch": 36.2532389681711,
      "grad_norm": 12.376599311828613,
      "learning_rate": 1.978896752652409e-05,
      "loss": 1.9399,
      "step": 937400
    },
    {
      "epoch": 36.257106392853,
      "grad_norm": 15.35177230834961,
      "learning_rate": 1.9785744672622504e-05,
      "loss": 1.8883,
      "step": 937500
    },
    {
      "epoch": 36.260973817534904,
      "grad_norm": 14.36750316619873,
      "learning_rate": 1.9782521818720915e-05,
      "loss": 1.7716,
      "step": 937600
    },
    {
      "epoch": 36.26484124221681,
      "grad_norm": 9.802587509155273,
      "learning_rate": 1.977929896481933e-05,
      "loss": 1.8352,
      "step": 937700
    },
    {
      "epoch": 36.26870866689871,
      "grad_norm": 9.406814575195312,
      "learning_rate": 1.977607611091774e-05,
      "loss": 1.7661,
      "step": 937800
    },
    {
      "epoch": 36.27257609158062,
      "grad_norm": 12.275676727294922,
      "learning_rate": 1.9772853257016156e-05,
      "loss": 1.8995,
      "step": 937900
    },
    {
      "epoch": 36.27644351626252,
      "grad_norm": 12.273868560791016,
      "learning_rate": 1.9769630403114567e-05,
      "loss": 1.7764,
      "step": 938000
    },
    {
      "epoch": 36.280310940944425,
      "grad_norm": 12.313172340393066,
      "learning_rate": 1.9766407549212982e-05,
      "loss": 1.7676,
      "step": 938100
    },
    {
      "epoch": 36.28417836562633,
      "grad_norm": 11.863296508789062,
      "learning_rate": 1.9763184695311393e-05,
      "loss": 1.8028,
      "step": 938200
    },
    {
      "epoch": 36.28804579030823,
      "grad_norm": 8.605104446411133,
      "learning_rate": 1.9759961841409804e-05,
      "loss": 1.845,
      "step": 938300
    },
    {
      "epoch": 36.29191321499014,
      "grad_norm": 13.74209213256836,
      "learning_rate": 1.975673898750822e-05,
      "loss": 1.7977,
      "step": 938400
    },
    {
      "epoch": 36.295780639672046,
      "grad_norm": 14.749826431274414,
      "learning_rate": 1.975351613360663e-05,
      "loss": 1.8723,
      "step": 938500
    },
    {
      "epoch": 36.299648064353946,
      "grad_norm": 10.209086418151855,
      "learning_rate": 1.9750293279705045e-05,
      "loss": 1.8365,
      "step": 938600
    },
    {
      "epoch": 36.30351548903585,
      "grad_norm": 13.730879783630371,
      "learning_rate": 1.9747070425803456e-05,
      "loss": 1.8699,
      "step": 938700
    },
    {
      "epoch": 36.30738291371775,
      "grad_norm": 12.896918296813965,
      "learning_rate": 1.974384757190187e-05,
      "loss": 1.8077,
      "step": 938800
    },
    {
      "epoch": 36.31125033839966,
      "grad_norm": 12.10097885131836,
      "learning_rate": 1.9740624718000283e-05,
      "loss": 1.8091,
      "step": 938900
    },
    {
      "epoch": 36.31511776308157,
      "grad_norm": 13.205209732055664,
      "learning_rate": 1.9737401864098697e-05,
      "loss": 1.854,
      "step": 939000
    },
    {
      "epoch": 36.31898518776347,
      "grad_norm": 15.792839050292969,
      "learning_rate": 1.973417901019711e-05,
      "loss": 1.8682,
      "step": 939100
    },
    {
      "epoch": 36.322852612445374,
      "grad_norm": 12.545912742614746,
      "learning_rate": 1.9730956156295523e-05,
      "loss": 1.7993,
      "step": 939200
    },
    {
      "epoch": 36.326720037127274,
      "grad_norm": 13.996543884277344,
      "learning_rate": 1.9727733302393935e-05,
      "loss": 1.7744,
      "step": 939300
    },
    {
      "epoch": 36.33058746180918,
      "grad_norm": 11.448470115661621,
      "learning_rate": 1.972451044849235e-05,
      "loss": 1.7888,
      "step": 939400
    },
    {
      "epoch": 36.33445488649109,
      "grad_norm": 14.502728462219238,
      "learning_rate": 1.972128759459076e-05,
      "loss": 1.7857,
      "step": 939500
    },
    {
      "epoch": 36.33832231117299,
      "grad_norm": 8.06830883026123,
      "learning_rate": 1.9718064740689175e-05,
      "loss": 1.7502,
      "step": 939600
    },
    {
      "epoch": 36.342189735854895,
      "grad_norm": 13.137246131896973,
      "learning_rate": 1.9714841886787587e-05,
      "loss": 1.8607,
      "step": 939700
    },
    {
      "epoch": 36.3460571605368,
      "grad_norm": 13.320317268371582,
      "learning_rate": 1.9711619032886e-05,
      "loss": 1.9137,
      "step": 939800
    },
    {
      "epoch": 36.3499245852187,
      "grad_norm": 11.087862968444824,
      "learning_rate": 1.9708396178984416e-05,
      "loss": 1.8391,
      "step": 939900
    },
    {
      "epoch": 36.35379200990061,
      "grad_norm": 11.079214096069336,
      "learning_rate": 1.9705173325082827e-05,
      "loss": 1.8979,
      "step": 940000
    },
    {
      "epoch": 36.35765943458251,
      "grad_norm": 10.9262113571167,
      "learning_rate": 1.9701950471181242e-05,
      "loss": 1.8521,
      "step": 940100
    },
    {
      "epoch": 36.361526859264416,
      "grad_norm": 13.287515640258789,
      "learning_rate": 1.9698727617279657e-05,
      "loss": 1.8182,
      "step": 940200
    },
    {
      "epoch": 36.36539428394632,
      "grad_norm": 11.536657333374023,
      "learning_rate": 1.9695504763378068e-05,
      "loss": 1.7549,
      "step": 940300
    },
    {
      "epoch": 36.36926170862822,
      "grad_norm": 11.98647689819336,
      "learning_rate": 1.9692281909476483e-05,
      "loss": 1.8675,
      "step": 940400
    },
    {
      "epoch": 36.37312913331013,
      "grad_norm": 11.66741943359375,
      "learning_rate": 1.9689059055574894e-05,
      "loss": 1.8478,
      "step": 940500
    },
    {
      "epoch": 36.37699655799203,
      "grad_norm": 9.492931365966797,
      "learning_rate": 1.968583620167331e-05,
      "loss": 1.8348,
      "step": 940600
    },
    {
      "epoch": 36.38086398267394,
      "grad_norm": 9.21662712097168,
      "learning_rate": 1.968261334777172e-05,
      "loss": 1.7904,
      "step": 940700
    },
    {
      "epoch": 36.384731407355844,
      "grad_norm": 11.690954208374023,
      "learning_rate": 1.9679390493870135e-05,
      "loss": 1.787,
      "step": 940800
    },
    {
      "epoch": 36.388598832037744,
      "grad_norm": 15.194820404052734,
      "learning_rate": 1.9676167639968546e-05,
      "loss": 1.8605,
      "step": 940900
    },
    {
      "epoch": 36.39246625671965,
      "grad_norm": 13.25222110748291,
      "learning_rate": 1.967294478606696e-05,
      "loss": 1.912,
      "step": 941000
    },
    {
      "epoch": 36.39633368140156,
      "grad_norm": 12.341573715209961,
      "learning_rate": 1.9669721932165372e-05,
      "loss": 1.9863,
      "step": 941100
    },
    {
      "epoch": 36.40020110608346,
      "grad_norm": 10.199087142944336,
      "learning_rate": 1.9666499078263787e-05,
      "loss": 1.8134,
      "step": 941200
    },
    {
      "epoch": 36.404068530765365,
      "grad_norm": 10.764885902404785,
      "learning_rate": 1.96632762243622e-05,
      "loss": 1.7741,
      "step": 941300
    },
    {
      "epoch": 36.407935955447265,
      "grad_norm": 13.450737953186035,
      "learning_rate": 1.966005337046061e-05,
      "loss": 1.9332,
      "step": 941400
    },
    {
      "epoch": 36.41180338012917,
      "grad_norm": 12.383821487426758,
      "learning_rate": 1.9656830516559025e-05,
      "loss": 1.7872,
      "step": 941500
    },
    {
      "epoch": 36.41567080481108,
      "grad_norm": 9.990421295166016,
      "learning_rate": 1.9653607662657436e-05,
      "loss": 1.8717,
      "step": 941600
    },
    {
      "epoch": 36.41953822949298,
      "grad_norm": 8.932040214538574,
      "learning_rate": 1.965038480875585e-05,
      "loss": 1.881,
      "step": 941700
    },
    {
      "epoch": 36.423405654174886,
      "grad_norm": 13.185258865356445,
      "learning_rate": 1.9647161954854262e-05,
      "loss": 1.897,
      "step": 941800
    },
    {
      "epoch": 36.42727307885679,
      "grad_norm": 12.336623191833496,
      "learning_rate": 1.9643939100952677e-05,
      "loss": 1.8362,
      "step": 941900
    },
    {
      "epoch": 36.43114050353869,
      "grad_norm": 9.91161823272705,
      "learning_rate": 1.9640716247051088e-05,
      "loss": 1.8081,
      "step": 942000
    },
    {
      "epoch": 36.4350079282206,
      "grad_norm": 12.156351089477539,
      "learning_rate": 1.9637493393149503e-05,
      "loss": 1.8682,
      "step": 942100
    },
    {
      "epoch": 36.4388753529025,
      "grad_norm": 11.669626235961914,
      "learning_rate": 1.9634270539247914e-05,
      "loss": 1.8576,
      "step": 942200
    },
    {
      "epoch": 36.44274277758441,
      "grad_norm": 14.979098320007324,
      "learning_rate": 1.963104768534633e-05,
      "loss": 1.9194,
      "step": 942300
    },
    {
      "epoch": 36.446610202266314,
      "grad_norm": 19.4090633392334,
      "learning_rate": 1.962782483144474e-05,
      "loss": 1.7957,
      "step": 942400
    },
    {
      "epoch": 36.450477626948214,
      "grad_norm": 13.783161163330078,
      "learning_rate": 1.9624601977543155e-05,
      "loss": 1.8016,
      "step": 942500
    },
    {
      "epoch": 36.45434505163012,
      "grad_norm": 14.39045238494873,
      "learning_rate": 1.9621379123641566e-05,
      "loss": 1.8297,
      "step": 942600
    },
    {
      "epoch": 36.45821247631202,
      "grad_norm": 9.387797355651855,
      "learning_rate": 1.961815626973998e-05,
      "loss": 1.8265,
      "step": 942700
    },
    {
      "epoch": 36.46207990099393,
      "grad_norm": 14.364039421081543,
      "learning_rate": 1.9614933415838392e-05,
      "loss": 1.873,
      "step": 942800
    },
    {
      "epoch": 36.465947325675835,
      "grad_norm": 14.10669994354248,
      "learning_rate": 1.9611710561936807e-05,
      "loss": 1.9168,
      "step": 942900
    },
    {
      "epoch": 36.469814750357735,
      "grad_norm": 13.996650695800781,
      "learning_rate": 1.9608487708035218e-05,
      "loss": 1.9182,
      "step": 943000
    },
    {
      "epoch": 36.47368217503964,
      "grad_norm": 11.868605613708496,
      "learning_rate": 1.9605264854133633e-05,
      "loss": 1.8279,
      "step": 943100
    },
    {
      "epoch": 36.47754959972155,
      "grad_norm": 8.283062934875488,
      "learning_rate": 1.9602042000232048e-05,
      "loss": 1.7542,
      "step": 943200
    },
    {
      "epoch": 36.48141702440345,
      "grad_norm": 12.167616844177246,
      "learning_rate": 1.959881914633046e-05,
      "loss": 1.79,
      "step": 943300
    },
    {
      "epoch": 36.485284449085356,
      "grad_norm": 15.016585350036621,
      "learning_rate": 1.9595596292428874e-05,
      "loss": 1.8375,
      "step": 943400
    },
    {
      "epoch": 36.489151873767256,
      "grad_norm": 11.792898178100586,
      "learning_rate": 1.9592373438527285e-05,
      "loss": 1.861,
      "step": 943500
    },
    {
      "epoch": 36.49301929844916,
      "grad_norm": 11.804524421691895,
      "learning_rate": 1.95891505846257e-05,
      "loss": 1.8724,
      "step": 943600
    },
    {
      "epoch": 36.49688672313107,
      "grad_norm": 13.342695236206055,
      "learning_rate": 1.9585927730724114e-05,
      "loss": 1.8334,
      "step": 943700
    },
    {
      "epoch": 36.50075414781297,
      "grad_norm": 13.508954048156738,
      "learning_rate": 1.9582704876822526e-05,
      "loss": 1.857,
      "step": 943800
    },
    {
      "epoch": 36.50462157249488,
      "grad_norm": 9.820755004882812,
      "learning_rate": 1.957948202292094e-05,
      "loss": 1.8222,
      "step": 943900
    },
    {
      "epoch": 36.50848899717678,
      "grad_norm": 9.968657493591309,
      "learning_rate": 1.9576259169019352e-05,
      "loss": 1.7839,
      "step": 944000
    },
    {
      "epoch": 36.512356421858684,
      "grad_norm": 12.350741386413574,
      "learning_rate": 1.9573036315117767e-05,
      "loss": 1.7785,
      "step": 944100
    },
    {
      "epoch": 36.51622384654059,
      "grad_norm": 13.004035949707031,
      "learning_rate": 1.9569813461216178e-05,
      "loss": 1.8164,
      "step": 944200
    },
    {
      "epoch": 36.52009127122249,
      "grad_norm": 14.681695938110352,
      "learning_rate": 1.956659060731459e-05,
      "loss": 1.9095,
      "step": 944300
    },
    {
      "epoch": 36.5239586959044,
      "grad_norm": 14.024234771728516,
      "learning_rate": 1.9563367753413004e-05,
      "loss": 1.8761,
      "step": 944400
    },
    {
      "epoch": 36.527826120586305,
      "grad_norm": 13.002335548400879,
      "learning_rate": 1.9560144899511415e-05,
      "loss": 1.8408,
      "step": 944500
    },
    {
      "epoch": 36.531693545268205,
      "grad_norm": 19.894214630126953,
      "learning_rate": 1.955692204560983e-05,
      "loss": 1.8402,
      "step": 944600
    },
    {
      "epoch": 36.53556096995011,
      "grad_norm": 10.837462425231934,
      "learning_rate": 1.955369919170824e-05,
      "loss": 1.8155,
      "step": 944700
    },
    {
      "epoch": 36.53942839463201,
      "grad_norm": 7.199458122253418,
      "learning_rate": 1.9550476337806656e-05,
      "loss": 1.8824,
      "step": 944800
    },
    {
      "epoch": 36.54329581931392,
      "grad_norm": 10.402634620666504,
      "learning_rate": 1.9547253483905067e-05,
      "loss": 1.8743,
      "step": 944900
    },
    {
      "epoch": 36.547163243995826,
      "grad_norm": 11.802590370178223,
      "learning_rate": 1.9544030630003482e-05,
      "loss": 1.817,
      "step": 945000
    },
    {
      "epoch": 36.551030668677726,
      "grad_norm": 11.825636863708496,
      "learning_rate": 1.9540807776101893e-05,
      "loss": 1.8047,
      "step": 945100
    },
    {
      "epoch": 36.55489809335963,
      "grad_norm": 8.661933898925781,
      "learning_rate": 1.9537584922200308e-05,
      "loss": 1.9373,
      "step": 945200
    },
    {
      "epoch": 36.55876551804154,
      "grad_norm": 11.423059463500977,
      "learning_rate": 1.953436206829872e-05,
      "loss": 1.8792,
      "step": 945300
    },
    {
      "epoch": 36.56263294272344,
      "grad_norm": 11.514883041381836,
      "learning_rate": 1.9531139214397134e-05,
      "loss": 1.9171,
      "step": 945400
    },
    {
      "epoch": 36.56650036740535,
      "grad_norm": 13.457260131835938,
      "learning_rate": 1.9527916360495546e-05,
      "loss": 1.8859,
      "step": 945500
    },
    {
      "epoch": 36.57036779208725,
      "grad_norm": 15.259882926940918,
      "learning_rate": 1.952469350659396e-05,
      "loss": 1.7366,
      "step": 945600
    },
    {
      "epoch": 36.574235216769154,
      "grad_norm": 11.95056438446045,
      "learning_rate": 1.952147065269237e-05,
      "loss": 1.7654,
      "step": 945700
    },
    {
      "epoch": 36.57810264145106,
      "grad_norm": 10.81485366821289,
      "learning_rate": 1.9518247798790786e-05,
      "loss": 1.8688,
      "step": 945800
    },
    {
      "epoch": 36.58197006613296,
      "grad_norm": 12.549031257629395,
      "learning_rate": 1.9515024944889198e-05,
      "loss": 1.9091,
      "step": 945900
    },
    {
      "epoch": 36.58583749081487,
      "grad_norm": 14.916420936584473,
      "learning_rate": 1.9511802090987612e-05,
      "loss": 1.9366,
      "step": 946000
    },
    {
      "epoch": 36.58970491549677,
      "grad_norm": 13.543169975280762,
      "learning_rate": 1.9508579237086024e-05,
      "loss": 1.8811,
      "step": 946100
    },
    {
      "epoch": 36.593572340178675,
      "grad_norm": 13.438007354736328,
      "learning_rate": 1.950535638318444e-05,
      "loss": 1.8698,
      "step": 946200
    },
    {
      "epoch": 36.59743976486058,
      "grad_norm": 10.401942253112793,
      "learning_rate": 1.950213352928285e-05,
      "loss": 1.7483,
      "step": 946300
    },
    {
      "epoch": 36.60130718954248,
      "grad_norm": 12.454296112060547,
      "learning_rate": 1.9498910675381264e-05,
      "loss": 1.8673,
      "step": 946400
    },
    {
      "epoch": 36.60517461422439,
      "grad_norm": 11.60388469696045,
      "learning_rate": 1.9495687821479676e-05,
      "loss": 1.8265,
      "step": 946500
    },
    {
      "epoch": 36.609042038906296,
      "grad_norm": 11.632822036743164,
      "learning_rate": 1.949246496757809e-05,
      "loss": 1.8336,
      "step": 946600
    },
    {
      "epoch": 36.612909463588196,
      "grad_norm": 12.392505645751953,
      "learning_rate": 1.9489242113676505e-05,
      "loss": 1.7796,
      "step": 946700
    },
    {
      "epoch": 36.6167768882701,
      "grad_norm": 9.079015731811523,
      "learning_rate": 1.9486019259774917e-05,
      "loss": 1.8627,
      "step": 946800
    },
    {
      "epoch": 36.620644312952,
      "grad_norm": 13.292882919311523,
      "learning_rate": 1.948279640587333e-05,
      "loss": 1.8614,
      "step": 946900
    },
    {
      "epoch": 36.62451173763391,
      "grad_norm": 14.979275703430176,
      "learning_rate": 1.9479573551971746e-05,
      "loss": 1.9143,
      "step": 947000
    },
    {
      "epoch": 36.62837916231582,
      "grad_norm": 12.322090148925781,
      "learning_rate": 1.9476350698070157e-05,
      "loss": 1.8346,
      "step": 947100
    },
    {
      "epoch": 36.63224658699772,
      "grad_norm": 11.92294979095459,
      "learning_rate": 1.947312784416857e-05,
      "loss": 1.8792,
      "step": 947200
    },
    {
      "epoch": 36.636114011679624,
      "grad_norm": 14.186641693115234,
      "learning_rate": 1.9469904990266983e-05,
      "loss": 1.8162,
      "step": 947300
    },
    {
      "epoch": 36.63998143636152,
      "grad_norm": 13.109097480773926,
      "learning_rate": 1.9466682136365395e-05,
      "loss": 1.8129,
      "step": 947400
    },
    {
      "epoch": 36.64384886104343,
      "grad_norm": 14.68916130065918,
      "learning_rate": 1.946345928246381e-05,
      "loss": 1.8329,
      "step": 947500
    },
    {
      "epoch": 36.64771628572534,
      "grad_norm": 17.1212100982666,
      "learning_rate": 1.946023642856222e-05,
      "loss": 1.9218,
      "step": 947600
    },
    {
      "epoch": 36.65158371040724,
      "grad_norm": 15.174397468566895,
      "learning_rate": 1.9457013574660635e-05,
      "loss": 1.7999,
      "step": 947700
    },
    {
      "epoch": 36.655451135089145,
      "grad_norm": 12.262051582336426,
      "learning_rate": 1.9453790720759047e-05,
      "loss": 1.8626,
      "step": 947800
    },
    {
      "epoch": 36.65931855977105,
      "grad_norm": 12.255379676818848,
      "learning_rate": 1.945056786685746e-05,
      "loss": 1.8571,
      "step": 947900
    },
    {
      "epoch": 36.66318598445295,
      "grad_norm": 11.517868041992188,
      "learning_rate": 1.9447345012955873e-05,
      "loss": 1.8415,
      "step": 948000
    },
    {
      "epoch": 36.66705340913486,
      "grad_norm": 11.89608383178711,
      "learning_rate": 1.9444122159054288e-05,
      "loss": 1.9559,
      "step": 948100
    },
    {
      "epoch": 36.67092083381676,
      "grad_norm": 11.045635223388672,
      "learning_rate": 1.94408993051527e-05,
      "loss": 1.8461,
      "step": 948200
    },
    {
      "epoch": 36.674788258498666,
      "grad_norm": 11.656198501586914,
      "learning_rate": 1.9437676451251114e-05,
      "loss": 1.8568,
      "step": 948300
    },
    {
      "epoch": 36.67865568318057,
      "grad_norm": 7.443017959594727,
      "learning_rate": 1.9434453597349525e-05,
      "loss": 1.8396,
      "step": 948400
    },
    {
      "epoch": 36.68252310786247,
      "grad_norm": 12.10356330871582,
      "learning_rate": 1.943123074344794e-05,
      "loss": 1.758,
      "step": 948500
    },
    {
      "epoch": 36.68639053254438,
      "grad_norm": 13.518956184387207,
      "learning_rate": 1.942800788954635e-05,
      "loss": 1.8676,
      "step": 948600
    },
    {
      "epoch": 36.69025795722628,
      "grad_norm": 11.678994178771973,
      "learning_rate": 1.9424785035644766e-05,
      "loss": 1.9328,
      "step": 948700
    },
    {
      "epoch": 36.69412538190819,
      "grad_norm": 10.240635871887207,
      "learning_rate": 1.9421562181743177e-05,
      "loss": 1.7131,
      "step": 948800
    },
    {
      "epoch": 36.69799280659009,
      "grad_norm": 10.550146102905273,
      "learning_rate": 1.9418339327841592e-05,
      "loss": 1.8407,
      "step": 948900
    },
    {
      "epoch": 36.70186023127199,
      "grad_norm": 16.304271697998047,
      "learning_rate": 1.9415116473940003e-05,
      "loss": 1.9216,
      "step": 949000
    },
    {
      "epoch": 36.7057276559539,
      "grad_norm": 11.59071159362793,
      "learning_rate": 1.9411893620038418e-05,
      "loss": 1.8257,
      "step": 949100
    },
    {
      "epoch": 36.70959508063581,
      "grad_norm": 11.413552284240723,
      "learning_rate": 1.940867076613683e-05,
      "loss": 1.8148,
      "step": 949200
    },
    {
      "epoch": 36.71346250531771,
      "grad_norm": 11.735302925109863,
      "learning_rate": 1.9405447912235244e-05,
      "loss": 1.8744,
      "step": 949300
    },
    {
      "epoch": 36.717329929999615,
      "grad_norm": 13.822381019592285,
      "learning_rate": 1.9402225058333655e-05,
      "loss": 1.8707,
      "step": 949400
    },
    {
      "epoch": 36.721197354681514,
      "grad_norm": 14.602858543395996,
      "learning_rate": 1.939900220443207e-05,
      "loss": 1.8944,
      "step": 949500
    },
    {
      "epoch": 36.72506477936342,
      "grad_norm": 12.834860801696777,
      "learning_rate": 1.939577935053048e-05,
      "loss": 1.8307,
      "step": 949600
    },
    {
      "epoch": 36.72893220404533,
      "grad_norm": 11.657325744628906,
      "learning_rate": 1.9392556496628896e-05,
      "loss": 1.8268,
      "step": 949700
    },
    {
      "epoch": 36.73279962872723,
      "grad_norm": 13.042033195495605,
      "learning_rate": 1.9389333642727307e-05,
      "loss": 1.8284,
      "step": 949800
    },
    {
      "epoch": 36.736667053409136,
      "grad_norm": 13.478745460510254,
      "learning_rate": 1.9386110788825722e-05,
      "loss": 1.9072,
      "step": 949900
    },
    {
      "epoch": 36.74053447809104,
      "grad_norm": 13.78290843963623,
      "learning_rate": 1.9382887934924133e-05,
      "loss": 1.8945,
      "step": 950000
    },
    {
      "epoch": 36.74440190277294,
      "grad_norm": 10.882745742797852,
      "learning_rate": 1.9379665081022548e-05,
      "loss": 1.8687,
      "step": 950100
    },
    {
      "epoch": 36.74826932745485,
      "grad_norm": 10.292937278747559,
      "learning_rate": 1.9376442227120963e-05,
      "loss": 1.86,
      "step": 950200
    },
    {
      "epoch": 36.75213675213675,
      "grad_norm": 13.118316650390625,
      "learning_rate": 1.9373219373219374e-05,
      "loss": 1.8776,
      "step": 950300
    },
    {
      "epoch": 36.75600417681866,
      "grad_norm": 11.641204833984375,
      "learning_rate": 1.936999651931779e-05,
      "loss": 1.8286,
      "step": 950400
    },
    {
      "epoch": 36.75987160150056,
      "grad_norm": 9.841135025024414,
      "learning_rate": 1.93667736654162e-05,
      "loss": 1.8058,
      "step": 950500
    },
    {
      "epoch": 36.76373902618246,
      "grad_norm": 13.715824127197266,
      "learning_rate": 1.9363550811514615e-05,
      "loss": 1.8773,
      "step": 950600
    },
    {
      "epoch": 36.76760645086437,
      "grad_norm": 11.867161750793457,
      "learning_rate": 1.9360327957613026e-05,
      "loss": 1.852,
      "step": 950700
    },
    {
      "epoch": 36.77147387554627,
      "grad_norm": 12.178140640258789,
      "learning_rate": 1.935710510371144e-05,
      "loss": 1.8402,
      "step": 950800
    },
    {
      "epoch": 36.77534130022818,
      "grad_norm": 12.105733871459961,
      "learning_rate": 1.9353882249809852e-05,
      "loss": 1.8378,
      "step": 950900
    },
    {
      "epoch": 36.779208724910085,
      "grad_norm": 8.704801559448242,
      "learning_rate": 1.9350659395908267e-05,
      "loss": 1.8435,
      "step": 951000
    },
    {
      "epoch": 36.783076149591984,
      "grad_norm": 11.112600326538086,
      "learning_rate": 1.9347436542006678e-05,
      "loss": 1.8101,
      "step": 951100
    },
    {
      "epoch": 36.78694357427389,
      "grad_norm": 11.153585433959961,
      "learning_rate": 1.9344213688105093e-05,
      "loss": 1.9147,
      "step": 951200
    },
    {
      "epoch": 36.7908109989558,
      "grad_norm": 10.86655330657959,
      "learning_rate": 1.9340990834203504e-05,
      "loss": 1.8314,
      "step": 951300
    },
    {
      "epoch": 36.7946784236377,
      "grad_norm": 10.466719627380371,
      "learning_rate": 1.933776798030192e-05,
      "loss": 1.9043,
      "step": 951400
    },
    {
      "epoch": 36.798545848319606,
      "grad_norm": 17.036035537719727,
      "learning_rate": 1.933454512640033e-05,
      "loss": 1.9099,
      "step": 951500
    },
    {
      "epoch": 36.802413273001505,
      "grad_norm": 14.08984661102295,
      "learning_rate": 1.9331322272498745e-05,
      "loss": 1.8664,
      "step": 951600
    },
    {
      "epoch": 36.80628069768341,
      "grad_norm": 11.69715690612793,
      "learning_rate": 1.9328099418597156e-05,
      "loss": 1.7886,
      "step": 951700
    },
    {
      "epoch": 36.81014812236532,
      "grad_norm": 10.740971565246582,
      "learning_rate": 1.932487656469557e-05,
      "loss": 1.8649,
      "step": 951800
    },
    {
      "epoch": 36.81401554704722,
      "grad_norm": 14.132180213928223,
      "learning_rate": 1.9321653710793982e-05,
      "loss": 1.8655,
      "step": 951900
    },
    {
      "epoch": 36.81788297172913,
      "grad_norm": 12.834970474243164,
      "learning_rate": 1.9318430856892397e-05,
      "loss": 1.8343,
      "step": 952000
    },
    {
      "epoch": 36.821750396411026,
      "grad_norm": 13.366352081298828,
      "learning_rate": 1.931520800299081e-05,
      "loss": 1.9069,
      "step": 952100
    },
    {
      "epoch": 36.82561782109293,
      "grad_norm": 12.190763473510742,
      "learning_rate": 1.9311985149089223e-05,
      "loss": 1.8776,
      "step": 952200
    },
    {
      "epoch": 36.82948524577484,
      "grad_norm": 10.92579174041748,
      "learning_rate": 1.9308762295187635e-05,
      "loss": 1.9509,
      "step": 952300
    },
    {
      "epoch": 36.83335267045674,
      "grad_norm": 14.742353439331055,
      "learning_rate": 1.930553944128605e-05,
      "loss": 1.8787,
      "step": 952400
    },
    {
      "epoch": 36.83722009513865,
      "grad_norm": 13.828835487365723,
      "learning_rate": 1.930231658738446e-05,
      "loss": 1.8833,
      "step": 952500
    },
    {
      "epoch": 36.841087519820555,
      "grad_norm": 11.644476890563965,
      "learning_rate": 1.9299093733482875e-05,
      "loss": 1.8049,
      "step": 952600
    },
    {
      "epoch": 36.844954944502454,
      "grad_norm": 11.376087188720703,
      "learning_rate": 1.9295870879581287e-05,
      "loss": 1.8929,
      "step": 952700
    },
    {
      "epoch": 36.84882236918436,
      "grad_norm": 12.929910659790039,
      "learning_rate": 1.92926480256797e-05,
      "loss": 1.879,
      "step": 952800
    },
    {
      "epoch": 36.85268979386626,
      "grad_norm": 13.20988941192627,
      "learning_rate": 1.9289425171778113e-05,
      "loss": 1.8581,
      "step": 952900
    },
    {
      "epoch": 36.85655721854817,
      "grad_norm": 11.518485069274902,
      "learning_rate": 1.9286202317876527e-05,
      "loss": 1.8626,
      "step": 953000
    },
    {
      "epoch": 36.860424643230076,
      "grad_norm": 12.371818542480469,
      "learning_rate": 1.928297946397494e-05,
      "loss": 1.8906,
      "step": 953100
    },
    {
      "epoch": 36.864292067911975,
      "grad_norm": 10.862170219421387,
      "learning_rate": 1.927975661007335e-05,
      "loss": 1.8154,
      "step": 953200
    },
    {
      "epoch": 36.86815949259388,
      "grad_norm": 7.154199123382568,
      "learning_rate": 1.9276533756171765e-05,
      "loss": 1.8519,
      "step": 953300
    },
    {
      "epoch": 36.87202691727579,
      "grad_norm": 10.390746116638184,
      "learning_rate": 1.927331090227018e-05,
      "loss": 1.9004,
      "step": 953400
    },
    {
      "epoch": 36.87589434195769,
      "grad_norm": 12.576972007751465,
      "learning_rate": 1.927008804836859e-05,
      "loss": 1.8874,
      "step": 953500
    },
    {
      "epoch": 36.8797617666396,
      "grad_norm": 11.900146484375,
      "learning_rate": 1.9266865194467006e-05,
      "loss": 1.8252,
      "step": 953600
    },
    {
      "epoch": 36.883629191321496,
      "grad_norm": 11.694639205932617,
      "learning_rate": 1.926364234056542e-05,
      "loss": 1.8645,
      "step": 953700
    },
    {
      "epoch": 36.8874966160034,
      "grad_norm": 12.111547470092773,
      "learning_rate": 1.926041948666383e-05,
      "loss": 1.7887,
      "step": 953800
    },
    {
      "epoch": 36.89136404068531,
      "grad_norm": 12.40572452545166,
      "learning_rate": 1.9257196632762246e-05,
      "loss": 1.7158,
      "step": 953900
    },
    {
      "epoch": 36.89523146536721,
      "grad_norm": 12.965387344360352,
      "learning_rate": 1.9253973778860658e-05,
      "loss": 1.8065,
      "step": 954000
    },
    {
      "epoch": 36.89909889004912,
      "grad_norm": 14.73798942565918,
      "learning_rate": 1.9250750924959072e-05,
      "loss": 1.9578,
      "step": 954100
    },
    {
      "epoch": 36.90296631473102,
      "grad_norm": 12.294268608093262,
      "learning_rate": 1.9247528071057484e-05,
      "loss": 1.7611,
      "step": 954200
    },
    {
      "epoch": 36.906833739412924,
      "grad_norm": 14.590865135192871,
      "learning_rate": 1.92443052171559e-05,
      "loss": 1.827,
      "step": 954300
    },
    {
      "epoch": 36.91070116409483,
      "grad_norm": 13.620977401733398,
      "learning_rate": 1.924108236325431e-05,
      "loss": 1.8208,
      "step": 954400
    },
    {
      "epoch": 36.91456858877673,
      "grad_norm": 12.284048080444336,
      "learning_rate": 1.9237859509352724e-05,
      "loss": 1.9038,
      "step": 954500
    },
    {
      "epoch": 36.91843601345864,
      "grad_norm": 13.719868659973145,
      "learning_rate": 1.9234636655451136e-05,
      "loss": 1.882,
      "step": 954600
    },
    {
      "epoch": 36.922303438140545,
      "grad_norm": 12.165823936462402,
      "learning_rate": 1.923141380154955e-05,
      "loss": 1.876,
      "step": 954700
    },
    {
      "epoch": 36.926170862822445,
      "grad_norm": 11.875319480895996,
      "learning_rate": 1.9228190947647962e-05,
      "loss": 1.7986,
      "step": 954800
    },
    {
      "epoch": 36.93003828750435,
      "grad_norm": 9.621785163879395,
      "learning_rate": 1.9224968093746377e-05,
      "loss": 1.8854,
      "step": 954900
    },
    {
      "epoch": 36.93390571218625,
      "grad_norm": 12.988661766052246,
      "learning_rate": 1.9221745239844788e-05,
      "loss": 1.8124,
      "step": 955000
    },
    {
      "epoch": 36.93777313686816,
      "grad_norm": 9.498817443847656,
      "learning_rate": 1.9218522385943203e-05,
      "loss": 1.8125,
      "step": 955100
    },
    {
      "epoch": 36.94164056155007,
      "grad_norm": 13.993448257446289,
      "learning_rate": 1.9215299532041614e-05,
      "loss": 1.7876,
      "step": 955200
    },
    {
      "epoch": 36.945507986231966,
      "grad_norm": 12.103787422180176,
      "learning_rate": 1.921207667814003e-05,
      "loss": 1.8036,
      "step": 955300
    },
    {
      "epoch": 36.94937541091387,
      "grad_norm": 9.955381393432617,
      "learning_rate": 1.920885382423844e-05,
      "loss": 1.8172,
      "step": 955400
    },
    {
      "epoch": 36.95324283559577,
      "grad_norm": 13.04792594909668,
      "learning_rate": 1.9205630970336855e-05,
      "loss": 1.8252,
      "step": 955500
    },
    {
      "epoch": 36.95711026027768,
      "grad_norm": 9.891809463500977,
      "learning_rate": 1.9202408116435266e-05,
      "loss": 1.7721,
      "step": 955600
    },
    {
      "epoch": 36.96097768495959,
      "grad_norm": 13.719389915466309,
      "learning_rate": 1.919918526253368e-05,
      "loss": 1.8441,
      "step": 955700
    },
    {
      "epoch": 36.96484510964149,
      "grad_norm": 8.822554588317871,
      "learning_rate": 1.9195962408632092e-05,
      "loss": 1.8233,
      "step": 955800
    },
    {
      "epoch": 36.968712534323394,
      "grad_norm": 11.782358169555664,
      "learning_rate": 1.9192739554730507e-05,
      "loss": 1.7607,
      "step": 955900
    },
    {
      "epoch": 36.9725799590053,
      "grad_norm": 16.399898529052734,
      "learning_rate": 1.9189516700828918e-05,
      "loss": 1.878,
      "step": 956000
    },
    {
      "epoch": 36.9764473836872,
      "grad_norm": 9.236187934875488,
      "learning_rate": 1.918629384692733e-05,
      "loss": 1.8716,
      "step": 956100
    },
    {
      "epoch": 36.98031480836911,
      "grad_norm": 10.050589561462402,
      "learning_rate": 1.9183070993025744e-05,
      "loss": 1.7838,
      "step": 956200
    },
    {
      "epoch": 36.98418223305101,
      "grad_norm": 15.35647964477539,
      "learning_rate": 1.9179848139124156e-05,
      "loss": 1.9033,
      "step": 956300
    },
    {
      "epoch": 36.988049657732915,
      "grad_norm": 9.039947509765625,
      "learning_rate": 1.917662528522257e-05,
      "loss": 1.7915,
      "step": 956400
    },
    {
      "epoch": 36.99191708241482,
      "grad_norm": 13.028670310974121,
      "learning_rate": 1.917340243132098e-05,
      "loss": 1.8762,
      "step": 956500
    },
    {
      "epoch": 36.99578450709672,
      "grad_norm": 11.370247840881348,
      "learning_rate": 1.9170179577419396e-05,
      "loss": 1.8768,
      "step": 956600
    },
    {
      "epoch": 36.99965193177863,
      "grad_norm": 13.154634475708008,
      "learning_rate": 1.916695672351781e-05,
      "loss": 1.9261,
      "step": 956700
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.7777684926986694,
      "eval_runtime": 2.9168,
      "eval_samples_per_second": 466.614,
      "eval_steps_per_second": 466.614,
      "step": 956709
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.6366934776306152,
      "eval_runtime": 56.0196,
      "eval_samples_per_second": 461.571,
      "eval_steps_per_second": 461.571,
      "step": 956709
    },
    {
      "epoch": 37.00351935646054,
      "grad_norm": 11.366312026977539,
      "learning_rate": 1.9163733869616222e-05,
      "loss": 1.8989,
      "step": 956800
    },
    {
      "epoch": 37.007386781142436,
      "grad_norm": 9.445497512817383,
      "learning_rate": 1.9160511015714637e-05,
      "loss": 1.8332,
      "step": 956900
    },
    {
      "epoch": 37.01125420582434,
      "grad_norm": 13.341917991638184,
      "learning_rate": 1.9157288161813052e-05,
      "loss": 1.7472,
      "step": 957000
    },
    {
      "epoch": 37.01512163050624,
      "grad_norm": 6.566661357879639,
      "learning_rate": 1.9154065307911463e-05,
      "loss": 1.6734,
      "step": 957100
    },
    {
      "epoch": 37.01898905518815,
      "grad_norm": 13.282478332519531,
      "learning_rate": 1.9150842454009878e-05,
      "loss": 1.8775,
      "step": 957200
    },
    {
      "epoch": 37.02285647987006,
      "grad_norm": 13.616811752319336,
      "learning_rate": 1.914761960010829e-05,
      "loss": 1.8396,
      "step": 957300
    },
    {
      "epoch": 37.02672390455196,
      "grad_norm": 11.664582252502441,
      "learning_rate": 1.9144396746206704e-05,
      "loss": 1.8489,
      "step": 957400
    },
    {
      "epoch": 37.030591329233864,
      "grad_norm": 11.451482772827148,
      "learning_rate": 1.9141173892305115e-05,
      "loss": 1.7436,
      "step": 957500
    },
    {
      "epoch": 37.034458753915764,
      "grad_norm": 12.4705171585083,
      "learning_rate": 1.913795103840353e-05,
      "loss": 1.8071,
      "step": 957600
    },
    {
      "epoch": 37.03832617859767,
      "grad_norm": 10.957099914550781,
      "learning_rate": 1.913472818450194e-05,
      "loss": 1.7975,
      "step": 957700
    },
    {
      "epoch": 37.04219360327958,
      "grad_norm": 13.144067764282227,
      "learning_rate": 1.9131505330600356e-05,
      "loss": 1.8308,
      "step": 957800
    },
    {
      "epoch": 37.04606102796148,
      "grad_norm": 9.234740257263184,
      "learning_rate": 1.9128282476698767e-05,
      "loss": 1.792,
      "step": 957900
    },
    {
      "epoch": 37.049928452643385,
      "grad_norm": 17.151371002197266,
      "learning_rate": 1.9125059622797182e-05,
      "loss": 1.8266,
      "step": 958000
    },
    {
      "epoch": 37.05379587732529,
      "grad_norm": 8.656495094299316,
      "learning_rate": 1.9121836768895593e-05,
      "loss": 1.8561,
      "step": 958100
    },
    {
      "epoch": 37.05766330200719,
      "grad_norm": 10.693812370300293,
      "learning_rate": 1.9118613914994008e-05,
      "loss": 1.8381,
      "step": 958200
    },
    {
      "epoch": 37.0615307266891,
      "grad_norm": 10.788792610168457,
      "learning_rate": 1.911539106109242e-05,
      "loss": 1.8402,
      "step": 958300
    },
    {
      "epoch": 37.065398151371,
      "grad_norm": 15.974940299987793,
      "learning_rate": 1.9112168207190834e-05,
      "loss": 1.8536,
      "step": 958400
    },
    {
      "epoch": 37.069265576052906,
      "grad_norm": 11.021718978881836,
      "learning_rate": 1.9108945353289245e-05,
      "loss": 1.8945,
      "step": 958500
    },
    {
      "epoch": 37.07313300073481,
      "grad_norm": 11.961649894714355,
      "learning_rate": 1.910572249938766e-05,
      "loss": 1.8429,
      "step": 958600
    },
    {
      "epoch": 37.07700042541671,
      "grad_norm": 10.20162582397461,
      "learning_rate": 1.910249964548607e-05,
      "loss": 1.7921,
      "step": 958700
    },
    {
      "epoch": 37.08086785009862,
      "grad_norm": 14.683181762695312,
      "learning_rate": 1.9099276791584486e-05,
      "loss": 1.8395,
      "step": 958800
    },
    {
      "epoch": 37.08473527478052,
      "grad_norm": 10.765321731567383,
      "learning_rate": 1.9096053937682898e-05,
      "loss": 1.9059,
      "step": 958900
    },
    {
      "epoch": 37.08860269946243,
      "grad_norm": 12.229658126831055,
      "learning_rate": 1.909283108378131e-05,
      "loss": 1.7766,
      "step": 959000
    },
    {
      "epoch": 37.092470124144334,
      "grad_norm": 18.182085037231445,
      "learning_rate": 1.9089608229879724e-05,
      "loss": 1.8633,
      "step": 959100
    },
    {
      "epoch": 37.096337548826234,
      "grad_norm": 10.223082542419434,
      "learning_rate": 1.9086385375978135e-05,
      "loss": 1.8458,
      "step": 959200
    },
    {
      "epoch": 37.10020497350814,
      "grad_norm": 15.629068374633789,
      "learning_rate": 1.908316252207655e-05,
      "loss": 1.8294,
      "step": 959300
    },
    {
      "epoch": 37.10407239819005,
      "grad_norm": 12.44168472290039,
      "learning_rate": 1.907993966817496e-05,
      "loss": 1.7127,
      "step": 959400
    },
    {
      "epoch": 37.10793982287195,
      "grad_norm": 12.59207534790039,
      "learning_rate": 1.9076716814273376e-05,
      "loss": 1.9079,
      "step": 959500
    },
    {
      "epoch": 37.111807247553855,
      "grad_norm": 12.825904846191406,
      "learning_rate": 1.9073493960371787e-05,
      "loss": 1.8758,
      "step": 959600
    },
    {
      "epoch": 37.115674672235755,
      "grad_norm": 11.468189239501953,
      "learning_rate": 1.9070271106470202e-05,
      "loss": 1.7806,
      "step": 959700
    },
    {
      "epoch": 37.11954209691766,
      "grad_norm": 12.751643180847168,
      "learning_rate": 1.9067048252568613e-05,
      "loss": 1.8567,
      "step": 959800
    },
    {
      "epoch": 37.12340952159957,
      "grad_norm": 10.016775131225586,
      "learning_rate": 1.9063825398667028e-05,
      "loss": 1.8431,
      "step": 959900
    },
    {
      "epoch": 37.12727694628147,
      "grad_norm": 10.589424133300781,
      "learning_rate": 1.906060254476544e-05,
      "loss": 1.7686,
      "step": 960000
    },
    {
      "epoch": 37.131144370963376,
      "grad_norm": 11.675588607788086,
      "learning_rate": 1.9057379690863854e-05,
      "loss": 1.8952,
      "step": 960100
    },
    {
      "epoch": 37.135011795645276,
      "grad_norm": 11.465995788574219,
      "learning_rate": 1.905415683696227e-05,
      "loss": 1.8649,
      "step": 960200
    },
    {
      "epoch": 37.13887922032718,
      "grad_norm": 10.699785232543945,
      "learning_rate": 1.905093398306068e-05,
      "loss": 1.7784,
      "step": 960300
    },
    {
      "epoch": 37.14274664500909,
      "grad_norm": 11.148158073425293,
      "learning_rate": 1.9047711129159095e-05,
      "loss": 1.8584,
      "step": 960400
    },
    {
      "epoch": 37.14661406969099,
      "grad_norm": 13.956254005432129,
      "learning_rate": 1.904448827525751e-05,
      "loss": 1.8422,
      "step": 960500
    },
    {
      "epoch": 37.1504814943729,
      "grad_norm": 12.584266662597656,
      "learning_rate": 1.904126542135592e-05,
      "loss": 1.7904,
      "step": 960600
    },
    {
      "epoch": 37.154348919054804,
      "grad_norm": 10.18227481842041,
      "learning_rate": 1.9038042567454335e-05,
      "loss": 1.807,
      "step": 960700
    },
    {
      "epoch": 37.158216343736704,
      "grad_norm": 9.703413963317871,
      "learning_rate": 1.9034819713552747e-05,
      "loss": 1.8529,
      "step": 960800
    },
    {
      "epoch": 37.16208376841861,
      "grad_norm": 9.758440971374512,
      "learning_rate": 1.903159685965116e-05,
      "loss": 1.9033,
      "step": 960900
    },
    {
      "epoch": 37.16595119310051,
      "grad_norm": 13.43726921081543,
      "learning_rate": 1.9028374005749573e-05,
      "loss": 1.866,
      "step": 961000
    },
    {
      "epoch": 37.16981861778242,
      "grad_norm": 13.154596328735352,
      "learning_rate": 1.9025151151847987e-05,
      "loss": 1.8294,
      "step": 961100
    },
    {
      "epoch": 37.173686042464325,
      "grad_norm": 12.493500709533691,
      "learning_rate": 1.90219282979464e-05,
      "loss": 1.807,
      "step": 961200
    },
    {
      "epoch": 37.177553467146225,
      "grad_norm": 12.225783348083496,
      "learning_rate": 1.9018705444044814e-05,
      "loss": 1.7694,
      "step": 961300
    },
    {
      "epoch": 37.18142089182813,
      "grad_norm": 9.761693000793457,
      "learning_rate": 1.9015482590143225e-05,
      "loss": 1.8547,
      "step": 961400
    },
    {
      "epoch": 37.18528831651004,
      "grad_norm": 11.65145206451416,
      "learning_rate": 1.901225973624164e-05,
      "loss": 1.7372,
      "step": 961500
    },
    {
      "epoch": 37.18915574119194,
      "grad_norm": 9.58488941192627,
      "learning_rate": 1.900903688234005e-05,
      "loss": 1.8861,
      "step": 961600
    },
    {
      "epoch": 37.193023165873846,
      "grad_norm": 11.138989448547363,
      "learning_rate": 1.9005814028438466e-05,
      "loss": 1.7789,
      "step": 961700
    },
    {
      "epoch": 37.196890590555746,
      "grad_norm": 16.01008415222168,
      "learning_rate": 1.9002591174536877e-05,
      "loss": 1.7884,
      "step": 961800
    },
    {
      "epoch": 37.20075801523765,
      "grad_norm": 14.127725601196289,
      "learning_rate": 1.8999368320635288e-05,
      "loss": 1.8693,
      "step": 961900
    },
    {
      "epoch": 37.20462543991956,
      "grad_norm": 12.506570816040039,
      "learning_rate": 1.8996145466733703e-05,
      "loss": 1.8782,
      "step": 962000
    },
    {
      "epoch": 37.20849286460146,
      "grad_norm": 13.139447212219238,
      "learning_rate": 1.8992922612832114e-05,
      "loss": 1.8227,
      "step": 962100
    },
    {
      "epoch": 37.21236028928337,
      "grad_norm": 12.096179008483887,
      "learning_rate": 1.898969975893053e-05,
      "loss": 1.9231,
      "step": 962200
    },
    {
      "epoch": 37.21622771396527,
      "grad_norm": 10.233160972595215,
      "learning_rate": 1.898647690502894e-05,
      "loss": 1.923,
      "step": 962300
    },
    {
      "epoch": 37.220095138647174,
      "grad_norm": 14.587332725524902,
      "learning_rate": 1.8983254051127355e-05,
      "loss": 1.7943,
      "step": 962400
    },
    {
      "epoch": 37.22396256332908,
      "grad_norm": 12.754088401794434,
      "learning_rate": 1.8980031197225766e-05,
      "loss": 1.8314,
      "step": 962500
    },
    {
      "epoch": 37.22782998801098,
      "grad_norm": 12.335886001586914,
      "learning_rate": 1.897680834332418e-05,
      "loss": 1.8938,
      "step": 962600
    },
    {
      "epoch": 37.23169741269289,
      "grad_norm": 11.491386413574219,
      "learning_rate": 1.8973585489422592e-05,
      "loss": 1.9334,
      "step": 962700
    },
    {
      "epoch": 37.235564837374795,
      "grad_norm": 11.219379425048828,
      "learning_rate": 1.8970362635521007e-05,
      "loss": 2.0167,
      "step": 962800
    },
    {
      "epoch": 37.239432262056695,
      "grad_norm": 11.86059856414795,
      "learning_rate": 1.896713978161942e-05,
      "loss": 1.9593,
      "step": 962900
    },
    {
      "epoch": 37.2432996867386,
      "grad_norm": 11.674960136413574,
      "learning_rate": 1.8963916927717833e-05,
      "loss": 1.8737,
      "step": 963000
    },
    {
      "epoch": 37.2471671114205,
      "grad_norm": 11.633706092834473,
      "learning_rate": 1.8960694073816245e-05,
      "loss": 1.8752,
      "step": 963100
    },
    {
      "epoch": 37.25103453610241,
      "grad_norm": 11.00472354888916,
      "learning_rate": 1.895747121991466e-05,
      "loss": 1.901,
      "step": 963200
    },
    {
      "epoch": 37.254901960784316,
      "grad_norm": 11.539721488952637,
      "learning_rate": 1.895424836601307e-05,
      "loss": 1.8045,
      "step": 963300
    },
    {
      "epoch": 37.258769385466216,
      "grad_norm": 11.077430725097656,
      "learning_rate": 1.8951025512111485e-05,
      "loss": 1.8628,
      "step": 963400
    },
    {
      "epoch": 37.26263681014812,
      "grad_norm": 12.33417797088623,
      "learning_rate": 1.8947802658209897e-05,
      "loss": 1.8614,
      "step": 963500
    },
    {
      "epoch": 37.26650423483002,
      "grad_norm": 12.191688537597656,
      "learning_rate": 1.894457980430831e-05,
      "loss": 1.82,
      "step": 963600
    },
    {
      "epoch": 37.27037165951193,
      "grad_norm": 14.248431205749512,
      "learning_rate": 1.8941356950406726e-05,
      "loss": 1.8369,
      "step": 963700
    },
    {
      "epoch": 37.27423908419384,
      "grad_norm": 14.292104721069336,
      "learning_rate": 1.8938134096505137e-05,
      "loss": 1.8401,
      "step": 963800
    },
    {
      "epoch": 37.27810650887574,
      "grad_norm": 10.65749454498291,
      "learning_rate": 1.8934911242603552e-05,
      "loss": 1.8698,
      "step": 963900
    },
    {
      "epoch": 37.281973933557644,
      "grad_norm": 11.374847412109375,
      "learning_rate": 1.8931688388701967e-05,
      "loss": 1.8771,
      "step": 964000
    },
    {
      "epoch": 37.28584135823955,
      "grad_norm": 14.349040031433105,
      "learning_rate": 1.8928465534800378e-05,
      "loss": 1.7799,
      "step": 964100
    },
    {
      "epoch": 37.28970878292145,
      "grad_norm": 12.24240779876709,
      "learning_rate": 1.8925242680898793e-05,
      "loss": 1.8976,
      "step": 964200
    },
    {
      "epoch": 37.29357620760336,
      "grad_norm": 8.380826950073242,
      "learning_rate": 1.8922019826997204e-05,
      "loss": 1.8654,
      "step": 964300
    },
    {
      "epoch": 37.29744363228526,
      "grad_norm": 13.84534740447998,
      "learning_rate": 1.891879697309562e-05,
      "loss": 1.8913,
      "step": 964400
    },
    {
      "epoch": 37.301311056967165,
      "grad_norm": 14.83530330657959,
      "learning_rate": 1.891557411919403e-05,
      "loss": 1.7796,
      "step": 964500
    },
    {
      "epoch": 37.30517848164907,
      "grad_norm": 9.277610778808594,
      "learning_rate": 1.8912351265292445e-05,
      "loss": 1.8193,
      "step": 964600
    },
    {
      "epoch": 37.30904590633097,
      "grad_norm": 12.990643501281738,
      "learning_rate": 1.8909128411390856e-05,
      "loss": 1.8175,
      "step": 964700
    },
    {
      "epoch": 37.31291333101288,
      "grad_norm": 12.889605522155762,
      "learning_rate": 1.890590555748927e-05,
      "loss": 1.8301,
      "step": 964800
    },
    {
      "epoch": 37.316780755694786,
      "grad_norm": 11.920598030090332,
      "learning_rate": 1.8902682703587682e-05,
      "loss": 1.7736,
      "step": 964900
    },
    {
      "epoch": 37.320648180376686,
      "grad_norm": 12.472870826721191,
      "learning_rate": 1.8899459849686094e-05,
      "loss": 1.82,
      "step": 965000
    },
    {
      "epoch": 37.32451560505859,
      "grad_norm": 18.723751068115234,
      "learning_rate": 1.889623699578451e-05,
      "loss": 1.8722,
      "step": 965100
    },
    {
      "epoch": 37.32838302974049,
      "grad_norm": 9.848076820373535,
      "learning_rate": 1.889301414188292e-05,
      "loss": 1.7763,
      "step": 965200
    },
    {
      "epoch": 37.3322504544224,
      "grad_norm": 12.825369834899902,
      "learning_rate": 1.8889791287981334e-05,
      "loss": 1.7796,
      "step": 965300
    },
    {
      "epoch": 37.33611787910431,
      "grad_norm": 13.701199531555176,
      "learning_rate": 1.8886568434079746e-05,
      "loss": 1.8036,
      "step": 965400
    },
    {
      "epoch": 37.33998530378621,
      "grad_norm": 11.484368324279785,
      "learning_rate": 1.888334558017816e-05,
      "loss": 1.8916,
      "step": 965500
    },
    {
      "epoch": 37.343852728468114,
      "grad_norm": 11.98915958404541,
      "learning_rate": 1.8880122726276572e-05,
      "loss": 1.9194,
      "step": 965600
    },
    {
      "epoch": 37.347720153150014,
      "grad_norm": 13.139448165893555,
      "learning_rate": 1.8876899872374987e-05,
      "loss": 1.8941,
      "step": 965700
    },
    {
      "epoch": 37.35158757783192,
      "grad_norm": 13.812756538391113,
      "learning_rate": 1.8873677018473398e-05,
      "loss": 1.803,
      "step": 965800
    },
    {
      "epoch": 37.35545500251383,
      "grad_norm": 10.98066520690918,
      "learning_rate": 1.8870454164571813e-05,
      "loss": 1.8886,
      "step": 965900
    },
    {
      "epoch": 37.35932242719573,
      "grad_norm": 15.576288223266602,
      "learning_rate": 1.8867231310670224e-05,
      "loss": 1.8318,
      "step": 966000
    },
    {
      "epoch": 37.363189851877635,
      "grad_norm": 11.513014793395996,
      "learning_rate": 1.886400845676864e-05,
      "loss": 1.818,
      "step": 966100
    },
    {
      "epoch": 37.36705727655954,
      "grad_norm": 10.530494689941406,
      "learning_rate": 1.886078560286705e-05,
      "loss": 1.8114,
      "step": 966200
    },
    {
      "epoch": 37.37092470124144,
      "grad_norm": 13.515810012817383,
      "learning_rate": 1.8857562748965465e-05,
      "loss": 1.8609,
      "step": 966300
    },
    {
      "epoch": 37.37479212592335,
      "grad_norm": 11.905403137207031,
      "learning_rate": 1.8854339895063876e-05,
      "loss": 1.7705,
      "step": 966400
    },
    {
      "epoch": 37.37865955060525,
      "grad_norm": 12.910073280334473,
      "learning_rate": 1.885111704116229e-05,
      "loss": 1.8446,
      "step": 966500
    },
    {
      "epoch": 37.382526975287156,
      "grad_norm": 11.550752639770508,
      "learning_rate": 1.8847894187260702e-05,
      "loss": 1.9596,
      "step": 966600
    },
    {
      "epoch": 37.38639439996906,
      "grad_norm": 12.129904747009277,
      "learning_rate": 1.8844671333359117e-05,
      "loss": 1.8342,
      "step": 966700
    },
    {
      "epoch": 37.39026182465096,
      "grad_norm": 11.074636459350586,
      "learning_rate": 1.8841448479457528e-05,
      "loss": 1.8101,
      "step": 966800
    },
    {
      "epoch": 37.39412924933287,
      "grad_norm": 13.25417423248291,
      "learning_rate": 1.8838225625555943e-05,
      "loss": 1.8488,
      "step": 966900
    },
    {
      "epoch": 37.39799667401477,
      "grad_norm": 12.338467597961426,
      "learning_rate": 1.8835002771654354e-05,
      "loss": 1.8189,
      "step": 967000
    },
    {
      "epoch": 37.40186409869668,
      "grad_norm": 10.001432418823242,
      "learning_rate": 1.883177991775277e-05,
      "loss": 1.8935,
      "step": 967100
    },
    {
      "epoch": 37.405731523378584,
      "grad_norm": 10.60132884979248,
      "learning_rate": 1.8828557063851184e-05,
      "loss": 1.9325,
      "step": 967200
    },
    {
      "epoch": 37.409598948060484,
      "grad_norm": 15.74075698852539,
      "learning_rate": 1.8825334209949595e-05,
      "loss": 1.7871,
      "step": 967300
    },
    {
      "epoch": 37.41346637274239,
      "grad_norm": 16.342994689941406,
      "learning_rate": 1.882211135604801e-05,
      "loss": 1.85,
      "step": 967400
    },
    {
      "epoch": 37.4173337974243,
      "grad_norm": 11.17739200592041,
      "learning_rate": 1.8818888502146424e-05,
      "loss": 1.8537,
      "step": 967500
    },
    {
      "epoch": 37.4212012221062,
      "grad_norm": 9.037153244018555,
      "learning_rate": 1.8815665648244836e-05,
      "loss": 1.8625,
      "step": 967600
    },
    {
      "epoch": 37.425068646788105,
      "grad_norm": 14.807619094848633,
      "learning_rate": 1.881244279434325e-05,
      "loss": 1.8,
      "step": 967700
    },
    {
      "epoch": 37.428936071470005,
      "grad_norm": 14.067524909973145,
      "learning_rate": 1.8809219940441662e-05,
      "loss": 1.7793,
      "step": 967800
    },
    {
      "epoch": 37.43280349615191,
      "grad_norm": 12.503334999084473,
      "learning_rate": 1.8805997086540073e-05,
      "loss": 1.8767,
      "step": 967900
    },
    {
      "epoch": 37.43667092083382,
      "grad_norm": 12.416775703430176,
      "learning_rate": 1.8802774232638488e-05,
      "loss": 1.8224,
      "step": 968000
    },
    {
      "epoch": 37.44053834551572,
      "grad_norm": 9.119211196899414,
      "learning_rate": 1.87995513787369e-05,
      "loss": 1.8505,
      "step": 968100
    },
    {
      "epoch": 37.444405770197626,
      "grad_norm": 10.82116413116455,
      "learning_rate": 1.8796328524835314e-05,
      "loss": 1.8945,
      "step": 968200
    },
    {
      "epoch": 37.448273194879526,
      "grad_norm": 11.841241836547852,
      "learning_rate": 1.8793105670933725e-05,
      "loss": 1.876,
      "step": 968300
    },
    {
      "epoch": 37.45214061956143,
      "grad_norm": 10.967619895935059,
      "learning_rate": 1.878988281703214e-05,
      "loss": 1.8531,
      "step": 968400
    },
    {
      "epoch": 37.45600804424334,
      "grad_norm": 14.183778762817383,
      "learning_rate": 1.878665996313055e-05,
      "loss": 1.8418,
      "step": 968500
    },
    {
      "epoch": 37.45987546892524,
      "grad_norm": 12.874916076660156,
      "learning_rate": 1.8783437109228966e-05,
      "loss": 1.8639,
      "step": 968600
    },
    {
      "epoch": 37.46374289360715,
      "grad_norm": 13.901823997497559,
      "learning_rate": 1.8780214255327377e-05,
      "loss": 1.8011,
      "step": 968700
    },
    {
      "epoch": 37.467610318289054,
      "grad_norm": 12.521110534667969,
      "learning_rate": 1.8776991401425792e-05,
      "loss": 1.8384,
      "step": 968800
    },
    {
      "epoch": 37.471477742970954,
      "grad_norm": 14.348114013671875,
      "learning_rate": 1.8773768547524203e-05,
      "loss": 1.8867,
      "step": 968900
    },
    {
      "epoch": 37.47534516765286,
      "grad_norm": 12.473862648010254,
      "learning_rate": 1.8770545693622618e-05,
      "loss": 1.8676,
      "step": 969000
    },
    {
      "epoch": 37.47921259233476,
      "grad_norm": 10.126903533935547,
      "learning_rate": 1.876732283972103e-05,
      "loss": 1.7869,
      "step": 969100
    },
    {
      "epoch": 37.48308001701667,
      "grad_norm": 14.134714126586914,
      "learning_rate": 1.8764099985819444e-05,
      "loss": 1.8468,
      "step": 969200
    },
    {
      "epoch": 37.486947441698575,
      "grad_norm": 8.914894104003906,
      "learning_rate": 1.8760877131917855e-05,
      "loss": 1.8012,
      "step": 969300
    },
    {
      "epoch": 37.490814866380475,
      "grad_norm": 13.129555702209473,
      "learning_rate": 1.875765427801627e-05,
      "loss": 1.8302,
      "step": 969400
    },
    {
      "epoch": 37.49468229106238,
      "grad_norm": 9.464736938476562,
      "learning_rate": 1.875443142411468e-05,
      "loss": 1.8013,
      "step": 969500
    },
    {
      "epoch": 37.49854971574429,
      "grad_norm": 14.6963529586792,
      "learning_rate": 1.8751208570213096e-05,
      "loss": 1.8479,
      "step": 969600
    },
    {
      "epoch": 37.50241714042619,
      "grad_norm": 10.552152633666992,
      "learning_rate": 1.8747985716311508e-05,
      "loss": 1.8193,
      "step": 969700
    },
    {
      "epoch": 37.506284565108096,
      "grad_norm": 9.478924751281738,
      "learning_rate": 1.8744762862409922e-05,
      "loss": 1.8934,
      "step": 969800
    },
    {
      "epoch": 37.510151989789996,
      "grad_norm": 14.08272647857666,
      "learning_rate": 1.8741540008508334e-05,
      "loss": 1.8467,
      "step": 969900
    },
    {
      "epoch": 37.5140194144719,
      "grad_norm": 11.598472595214844,
      "learning_rate": 1.873831715460675e-05,
      "loss": 1.7599,
      "step": 970000
    },
    {
      "epoch": 37.51788683915381,
      "grad_norm": 15.448284149169922,
      "learning_rate": 1.873509430070516e-05,
      "loss": 1.8723,
      "step": 970100
    },
    {
      "epoch": 37.52175426383571,
      "grad_norm": 14.931746482849121,
      "learning_rate": 1.8731871446803574e-05,
      "loss": 1.764,
      "step": 970200
    },
    {
      "epoch": 37.52562168851762,
      "grad_norm": 12.575000762939453,
      "learning_rate": 1.8728648592901986e-05,
      "loss": 1.9005,
      "step": 970300
    },
    {
      "epoch": 37.52948911319952,
      "grad_norm": 12.816302299499512,
      "learning_rate": 1.87254257390004e-05,
      "loss": 1.8906,
      "step": 970400
    },
    {
      "epoch": 37.533356537881424,
      "grad_norm": 8.707771301269531,
      "learning_rate": 1.8722202885098815e-05,
      "loss": 1.832,
      "step": 970500
    },
    {
      "epoch": 37.53722396256333,
      "grad_norm": 11.068046569824219,
      "learning_rate": 1.8718980031197226e-05,
      "loss": 1.784,
      "step": 970600
    },
    {
      "epoch": 37.54109138724523,
      "grad_norm": 12.22933578491211,
      "learning_rate": 1.871575717729564e-05,
      "loss": 1.732,
      "step": 970700
    },
    {
      "epoch": 37.54495881192714,
      "grad_norm": 12.014720916748047,
      "learning_rate": 1.8712534323394053e-05,
      "loss": 1.8223,
      "step": 970800
    },
    {
      "epoch": 37.548826236609045,
      "grad_norm": 10.066412925720215,
      "learning_rate": 1.8709311469492467e-05,
      "loss": 1.8275,
      "step": 970900
    },
    {
      "epoch": 37.552693661290945,
      "grad_norm": 12.961885452270508,
      "learning_rate": 1.870608861559088e-05,
      "loss": 1.7919,
      "step": 971000
    },
    {
      "epoch": 37.55656108597285,
      "grad_norm": 13.316797256469727,
      "learning_rate": 1.8702865761689293e-05,
      "loss": 1.8531,
      "step": 971100
    },
    {
      "epoch": 37.56042851065475,
      "grad_norm": 11.45352840423584,
      "learning_rate": 1.8699642907787705e-05,
      "loss": 1.8828,
      "step": 971200
    },
    {
      "epoch": 37.56429593533666,
      "grad_norm": 9.450488090515137,
      "learning_rate": 1.869642005388612e-05,
      "loss": 1.7886,
      "step": 971300
    },
    {
      "epoch": 37.568163360018566,
      "grad_norm": 12.370710372924805,
      "learning_rate": 1.869319719998453e-05,
      "loss": 1.8294,
      "step": 971400
    },
    {
      "epoch": 37.572030784700466,
      "grad_norm": 10.421016693115234,
      "learning_rate": 1.8689974346082945e-05,
      "loss": 1.8485,
      "step": 971500
    },
    {
      "epoch": 37.57589820938237,
      "grad_norm": 8.646551132202148,
      "learning_rate": 1.8686751492181357e-05,
      "loss": 1.7815,
      "step": 971600
    },
    {
      "epoch": 37.57976563406427,
      "grad_norm": 13.826181411743164,
      "learning_rate": 1.868352863827977e-05,
      "loss": 1.8955,
      "step": 971700
    },
    {
      "epoch": 37.58363305874618,
      "grad_norm": 13.138869285583496,
      "learning_rate": 1.8680305784378183e-05,
      "loss": 1.8182,
      "step": 971800
    },
    {
      "epoch": 37.58750048342809,
      "grad_norm": 16.78740692138672,
      "learning_rate": 1.8677082930476597e-05,
      "loss": 1.7858,
      "step": 971900
    },
    {
      "epoch": 37.59136790810999,
      "grad_norm": 13.028624534606934,
      "learning_rate": 1.867386007657501e-05,
      "loss": 1.8376,
      "step": 972000
    },
    {
      "epoch": 37.595235332791894,
      "grad_norm": 15.106284141540527,
      "learning_rate": 1.8670637222673424e-05,
      "loss": 1.8258,
      "step": 972100
    },
    {
      "epoch": 37.5991027574738,
      "grad_norm": 10.480817794799805,
      "learning_rate": 1.8667414368771835e-05,
      "loss": 1.8484,
      "step": 972200
    },
    {
      "epoch": 37.6029701821557,
      "grad_norm": 11.717745780944824,
      "learning_rate": 1.866419151487025e-05,
      "loss": 1.8289,
      "step": 972300
    },
    {
      "epoch": 37.60683760683761,
      "grad_norm": 15.84346866607666,
      "learning_rate": 1.866096866096866e-05,
      "loss": 1.8154,
      "step": 972400
    },
    {
      "epoch": 37.61070503151951,
      "grad_norm": 12.97815990447998,
      "learning_rate": 1.8657745807067076e-05,
      "loss": 1.8704,
      "step": 972500
    },
    {
      "epoch": 37.614572456201415,
      "grad_norm": 9.045595169067383,
      "learning_rate": 1.8654522953165487e-05,
      "loss": 1.8537,
      "step": 972600
    },
    {
      "epoch": 37.61843988088332,
      "grad_norm": 13.043523788452148,
      "learning_rate": 1.86513000992639e-05,
      "loss": 1.8178,
      "step": 972700
    },
    {
      "epoch": 37.62230730556522,
      "grad_norm": 15.646150588989258,
      "learning_rate": 1.8648077245362313e-05,
      "loss": 1.8859,
      "step": 972800
    },
    {
      "epoch": 37.62617473024713,
      "grad_norm": 10.36955451965332,
      "learning_rate": 1.8644854391460728e-05,
      "loss": 1.7676,
      "step": 972900
    },
    {
      "epoch": 37.63004215492903,
      "grad_norm": 12.122804641723633,
      "learning_rate": 1.864163153755914e-05,
      "loss": 1.8697,
      "step": 973000
    },
    {
      "epoch": 37.633909579610936,
      "grad_norm": 12.947297096252441,
      "learning_rate": 1.8638408683657554e-05,
      "loss": 1.9261,
      "step": 973100
    },
    {
      "epoch": 37.63777700429284,
      "grad_norm": 10.239397048950195,
      "learning_rate": 1.8635185829755965e-05,
      "loss": 1.8556,
      "step": 973200
    },
    {
      "epoch": 37.64164442897474,
      "grad_norm": 9.152113914489746,
      "learning_rate": 1.863196297585438e-05,
      "loss": 1.9147,
      "step": 973300
    },
    {
      "epoch": 37.64551185365665,
      "grad_norm": 13.452837944030762,
      "learning_rate": 1.862874012195279e-05,
      "loss": 1.8493,
      "step": 973400
    },
    {
      "epoch": 37.64937927833856,
      "grad_norm": 18.732751846313477,
      "learning_rate": 1.8625517268051206e-05,
      "loss": 1.7213,
      "step": 973500
    },
    {
      "epoch": 37.65324670302046,
      "grad_norm": 12.754354476928711,
      "learning_rate": 1.8622294414149617e-05,
      "loss": 1.8117,
      "step": 973600
    },
    {
      "epoch": 37.657114127702364,
      "grad_norm": 9.77700424194336,
      "learning_rate": 1.8619071560248032e-05,
      "loss": 1.8099,
      "step": 973700
    },
    {
      "epoch": 37.660981552384264,
      "grad_norm": 12.735374450683594,
      "learning_rate": 1.8615848706346443e-05,
      "loss": 1.8446,
      "step": 973800
    },
    {
      "epoch": 37.66484897706617,
      "grad_norm": 11.05734634399414,
      "learning_rate": 1.8612625852444858e-05,
      "loss": 1.8733,
      "step": 973900
    },
    {
      "epoch": 37.66871640174808,
      "grad_norm": 13.089143753051758,
      "learning_rate": 1.8609402998543273e-05,
      "loss": 1.9007,
      "step": 974000
    },
    {
      "epoch": 37.67258382642998,
      "grad_norm": 16.481611251831055,
      "learning_rate": 1.8606180144641684e-05,
      "loss": 1.8186,
      "step": 974100
    },
    {
      "epoch": 37.676451251111885,
      "grad_norm": 11.847021102905273,
      "learning_rate": 1.86029572907401e-05,
      "loss": 1.8159,
      "step": 974200
    },
    {
      "epoch": 37.68031867579379,
      "grad_norm": 9.63661003112793,
      "learning_rate": 1.859973443683851e-05,
      "loss": 1.7503,
      "step": 974300
    },
    {
      "epoch": 37.68418610047569,
      "grad_norm": 12.711030960083008,
      "learning_rate": 1.8596511582936925e-05,
      "loss": 1.8609,
      "step": 974400
    },
    {
      "epoch": 37.6880535251576,
      "grad_norm": 12.88126277923584,
      "learning_rate": 1.8593288729035336e-05,
      "loss": 1.795,
      "step": 974500
    },
    {
      "epoch": 37.6919209498395,
      "grad_norm": 12.160001754760742,
      "learning_rate": 1.859006587513375e-05,
      "loss": 1.919,
      "step": 974600
    },
    {
      "epoch": 37.695788374521406,
      "grad_norm": 10.508687973022461,
      "learning_rate": 1.8586843021232162e-05,
      "loss": 1.8184,
      "step": 974700
    },
    {
      "epoch": 37.69965579920331,
      "grad_norm": 13.178373336791992,
      "learning_rate": 1.8583620167330577e-05,
      "loss": 1.8901,
      "step": 974800
    },
    {
      "epoch": 37.70352322388521,
      "grad_norm": 14.78772258758545,
      "learning_rate": 1.8580397313428988e-05,
      "loss": 1.9703,
      "step": 974900
    },
    {
      "epoch": 37.70739064856712,
      "grad_norm": 7.9181365966796875,
      "learning_rate": 1.8577174459527403e-05,
      "loss": 1.9353,
      "step": 975000
    },
    {
      "epoch": 37.71125807324902,
      "grad_norm": 11.56955337524414,
      "learning_rate": 1.8573951605625814e-05,
      "loss": 1.7779,
      "step": 975100
    },
    {
      "epoch": 37.71512549793093,
      "grad_norm": 10.840773582458496,
      "learning_rate": 1.857072875172423e-05,
      "loss": 1.8754,
      "step": 975200
    },
    {
      "epoch": 37.718992922612834,
      "grad_norm": 9.08842658996582,
      "learning_rate": 1.856750589782264e-05,
      "loss": 1.8343,
      "step": 975300
    },
    {
      "epoch": 37.722860347294734,
      "grad_norm": 11.110921859741211,
      "learning_rate": 1.8564283043921055e-05,
      "loss": 1.8463,
      "step": 975400
    },
    {
      "epoch": 37.72672777197664,
      "grad_norm": 11.27157974243164,
      "learning_rate": 1.8561060190019466e-05,
      "loss": 1.7639,
      "step": 975500
    },
    {
      "epoch": 37.73059519665855,
      "grad_norm": 11.346661567687988,
      "learning_rate": 1.855783733611788e-05,
      "loss": 1.8316,
      "step": 975600
    },
    {
      "epoch": 37.73446262134045,
      "grad_norm": 13.119861602783203,
      "learning_rate": 1.8554614482216292e-05,
      "loss": 1.818,
      "step": 975700
    },
    {
      "epoch": 37.738330046022355,
      "grad_norm": 12.529289245605469,
      "learning_rate": 1.8551391628314707e-05,
      "loss": 1.8736,
      "step": 975800
    },
    {
      "epoch": 37.742197470704255,
      "grad_norm": 16.10634422302246,
      "learning_rate": 1.854816877441312e-05,
      "loss": 1.8047,
      "step": 975900
    },
    {
      "epoch": 37.74606489538616,
      "grad_norm": 11.999007225036621,
      "learning_rate": 1.8544945920511533e-05,
      "loss": 1.8082,
      "step": 976000
    },
    {
      "epoch": 37.74993232006807,
      "grad_norm": 10.722148895263672,
      "learning_rate": 1.8541723066609945e-05,
      "loss": 1.7918,
      "step": 976100
    },
    {
      "epoch": 37.75379974474997,
      "grad_norm": 13.9761323928833,
      "learning_rate": 1.853850021270836e-05,
      "loss": 1.8677,
      "step": 976200
    },
    {
      "epoch": 37.757667169431876,
      "grad_norm": 13.705545425415039,
      "learning_rate": 1.853527735880677e-05,
      "loss": 1.7772,
      "step": 976300
    },
    {
      "epoch": 37.76153459411378,
      "grad_norm": 9.44892406463623,
      "learning_rate": 1.8532054504905185e-05,
      "loss": 1.7844,
      "step": 976400
    },
    {
      "epoch": 37.76540201879568,
      "grad_norm": 13.878671646118164,
      "learning_rate": 1.8528831651003597e-05,
      "loss": 1.7838,
      "step": 976500
    },
    {
      "epoch": 37.76926944347759,
      "grad_norm": 10.331205368041992,
      "learning_rate": 1.852560879710201e-05,
      "loss": 1.8409,
      "step": 976600
    },
    {
      "epoch": 37.77313686815949,
      "grad_norm": 12.285037994384766,
      "learning_rate": 1.8522385943200423e-05,
      "loss": 1.81,
      "step": 976700
    },
    {
      "epoch": 37.7770042928414,
      "grad_norm": 8.555381774902344,
      "learning_rate": 1.8519163089298834e-05,
      "loss": 1.7945,
      "step": 976800
    },
    {
      "epoch": 37.780871717523304,
      "grad_norm": 10.354537010192871,
      "learning_rate": 1.851594023539725e-05,
      "loss": 1.8675,
      "step": 976900
    },
    {
      "epoch": 37.784739142205204,
      "grad_norm": 12.104872703552246,
      "learning_rate": 1.851271738149566e-05,
      "loss": 1.7949,
      "step": 977000
    },
    {
      "epoch": 37.78860656688711,
      "grad_norm": 11.946573257446289,
      "learning_rate": 1.8509494527594075e-05,
      "loss": 1.7841,
      "step": 977100
    },
    {
      "epoch": 37.79247399156901,
      "grad_norm": 12.023058891296387,
      "learning_rate": 1.850627167369249e-05,
      "loss": 1.8689,
      "step": 977200
    },
    {
      "epoch": 37.79634141625092,
      "grad_norm": 8.587128639221191,
      "learning_rate": 1.85030488197909e-05,
      "loss": 1.8224,
      "step": 977300
    },
    {
      "epoch": 37.800208840932825,
      "grad_norm": 11.7545166015625,
      "learning_rate": 1.8499825965889316e-05,
      "loss": 1.7892,
      "step": 977400
    },
    {
      "epoch": 37.804076265614725,
      "grad_norm": 11.457568168640137,
      "learning_rate": 1.849660311198773e-05,
      "loss": 1.9184,
      "step": 977500
    },
    {
      "epoch": 37.80794369029663,
      "grad_norm": 10.247817039489746,
      "learning_rate": 1.849338025808614e-05,
      "loss": 1.806,
      "step": 977600
    },
    {
      "epoch": 37.81181111497854,
      "grad_norm": 12.722158432006836,
      "learning_rate": 1.8490157404184556e-05,
      "loss": 1.7334,
      "step": 977700
    },
    {
      "epoch": 37.81567853966044,
      "grad_norm": 10.878745079040527,
      "learning_rate": 1.8486934550282968e-05,
      "loss": 1.8749,
      "step": 977800
    },
    {
      "epoch": 37.819545964342346,
      "grad_norm": 11.073789596557617,
      "learning_rate": 1.8483711696381382e-05,
      "loss": 1.8996,
      "step": 977900
    },
    {
      "epoch": 37.823413389024246,
      "grad_norm": 13.18726634979248,
      "learning_rate": 1.8480488842479794e-05,
      "loss": 1.9094,
      "step": 978000
    },
    {
      "epoch": 37.82728081370615,
      "grad_norm": 12.189414024353027,
      "learning_rate": 1.847726598857821e-05,
      "loss": 1.815,
      "step": 978100
    },
    {
      "epoch": 37.83114823838806,
      "grad_norm": 8.911566734313965,
      "learning_rate": 1.847404313467662e-05,
      "loss": 1.7407,
      "step": 978200
    },
    {
      "epoch": 37.83501566306996,
      "grad_norm": 15.762840270996094,
      "learning_rate": 1.8470820280775034e-05,
      "loss": 1.8298,
      "step": 978300
    },
    {
      "epoch": 37.83888308775187,
      "grad_norm": 13.149832725524902,
      "learning_rate": 1.8467597426873446e-05,
      "loss": 1.8363,
      "step": 978400
    },
    {
      "epoch": 37.84275051243377,
      "grad_norm": 12.099407196044922,
      "learning_rate": 1.846437457297186e-05,
      "loss": 1.8236,
      "step": 978500
    },
    {
      "epoch": 37.846617937115674,
      "grad_norm": 8.563145637512207,
      "learning_rate": 1.8461151719070272e-05,
      "loss": 1.9115,
      "step": 978600
    },
    {
      "epoch": 37.85048536179758,
      "grad_norm": 14.678366661071777,
      "learning_rate": 1.8457928865168687e-05,
      "loss": 1.8698,
      "step": 978700
    },
    {
      "epoch": 37.85435278647948,
      "grad_norm": 10.786577224731445,
      "learning_rate": 1.8454706011267098e-05,
      "loss": 1.8717,
      "step": 978800
    },
    {
      "epoch": 37.85822021116139,
      "grad_norm": 13.89371109008789,
      "learning_rate": 1.8451483157365513e-05,
      "loss": 1.8528,
      "step": 978900
    },
    {
      "epoch": 37.862087635843295,
      "grad_norm": 12.356120109558105,
      "learning_rate": 1.8448260303463924e-05,
      "loss": 1.8225,
      "step": 979000
    },
    {
      "epoch": 37.865955060525195,
      "grad_norm": 12.87557315826416,
      "learning_rate": 1.844503744956234e-05,
      "loss": 1.838,
      "step": 979100
    },
    {
      "epoch": 37.8698224852071,
      "grad_norm": 11.557148933410645,
      "learning_rate": 1.844181459566075e-05,
      "loss": 1.8498,
      "step": 979200
    },
    {
      "epoch": 37.873689909889,
      "grad_norm": 12.595553398132324,
      "learning_rate": 1.8438591741759165e-05,
      "loss": 1.7689,
      "step": 979300
    },
    {
      "epoch": 37.87755733457091,
      "grad_norm": 12.239317893981934,
      "learning_rate": 1.8435368887857576e-05,
      "loss": 1.8361,
      "step": 979400
    },
    {
      "epoch": 37.881424759252816,
      "grad_norm": 11.524474143981934,
      "learning_rate": 1.843214603395599e-05,
      "loss": 1.9257,
      "step": 979500
    },
    {
      "epoch": 37.885292183934716,
      "grad_norm": 10.878316879272461,
      "learning_rate": 1.8428923180054402e-05,
      "loss": 1.9073,
      "step": 979600
    },
    {
      "epoch": 37.88915960861662,
      "grad_norm": 10.41065788269043,
      "learning_rate": 1.8425700326152813e-05,
      "loss": 1.843,
      "step": 979700
    },
    {
      "epoch": 37.89302703329852,
      "grad_norm": 11.951297760009766,
      "learning_rate": 1.8422477472251228e-05,
      "loss": 1.7591,
      "step": 979800
    },
    {
      "epoch": 37.89689445798043,
      "grad_norm": 18.598243713378906,
      "learning_rate": 1.841925461834964e-05,
      "loss": 1.8229,
      "step": 979900
    },
    {
      "epoch": 37.90076188266234,
      "grad_norm": 15.369876861572266,
      "learning_rate": 1.8416031764448054e-05,
      "loss": 1.8713,
      "step": 980000
    },
    {
      "epoch": 37.90462930734424,
      "grad_norm": 10.092751502990723,
      "learning_rate": 1.8412808910546465e-05,
      "loss": 1.8532,
      "step": 980100
    },
    {
      "epoch": 37.908496732026144,
      "grad_norm": 14.263579368591309,
      "learning_rate": 1.840958605664488e-05,
      "loss": 1.8134,
      "step": 980200
    },
    {
      "epoch": 37.91236415670805,
      "grad_norm": 11.603776931762695,
      "learning_rate": 1.840636320274329e-05,
      "loss": 1.8932,
      "step": 980300
    },
    {
      "epoch": 37.91623158138995,
      "grad_norm": 11.549015998840332,
      "learning_rate": 1.8403140348841706e-05,
      "loss": 1.8052,
      "step": 980400
    },
    {
      "epoch": 37.92009900607186,
      "grad_norm": 12.265595436096191,
      "learning_rate": 1.839991749494012e-05,
      "loss": 1.8901,
      "step": 980500
    },
    {
      "epoch": 37.92396643075376,
      "grad_norm": 10.54965591430664,
      "learning_rate": 1.8396694641038532e-05,
      "loss": 1.8635,
      "step": 980600
    },
    {
      "epoch": 37.927833855435665,
      "grad_norm": 8.4391450881958,
      "learning_rate": 1.8393471787136947e-05,
      "loss": 1.7388,
      "step": 980700
    },
    {
      "epoch": 37.93170128011757,
      "grad_norm": 10.857484817504883,
      "learning_rate": 1.839024893323536e-05,
      "loss": 1.8143,
      "step": 980800
    },
    {
      "epoch": 37.93556870479947,
      "grad_norm": 10.01481819152832,
      "learning_rate": 1.8387026079333773e-05,
      "loss": 1.9326,
      "step": 980900
    },
    {
      "epoch": 37.93943612948138,
      "grad_norm": 14.998075485229492,
      "learning_rate": 1.8383803225432188e-05,
      "loss": 1.9082,
      "step": 981000
    },
    {
      "epoch": 37.943303554163286,
      "grad_norm": 14.814322471618652,
      "learning_rate": 1.83805803715306e-05,
      "loss": 1.8567,
      "step": 981100
    },
    {
      "epoch": 37.947170978845186,
      "grad_norm": 9.324992179870605,
      "learning_rate": 1.8377357517629014e-05,
      "loss": 1.8864,
      "step": 981200
    },
    {
      "epoch": 37.95103840352709,
      "grad_norm": 17.61969566345215,
      "learning_rate": 1.8374134663727425e-05,
      "loss": 1.853,
      "step": 981300
    },
    {
      "epoch": 37.95490582820899,
      "grad_norm": 13.18367862701416,
      "learning_rate": 1.837091180982584e-05,
      "loss": 1.8416,
      "step": 981400
    },
    {
      "epoch": 37.9587732528909,
      "grad_norm": 12.521493911743164,
      "learning_rate": 1.836768895592425e-05,
      "loss": 1.7995,
      "step": 981500
    },
    {
      "epoch": 37.96264067757281,
      "grad_norm": 11.054461479187012,
      "learning_rate": 1.8364466102022666e-05,
      "loss": 1.8246,
      "step": 981600
    },
    {
      "epoch": 37.96650810225471,
      "grad_norm": 13.4140043258667,
      "learning_rate": 1.8361243248121077e-05,
      "loss": 1.8712,
      "step": 981700
    },
    {
      "epoch": 37.970375526936614,
      "grad_norm": 13.045574188232422,
      "learning_rate": 1.8358020394219492e-05,
      "loss": 1.8623,
      "step": 981800
    },
    {
      "epoch": 37.974242951618514,
      "grad_norm": 16.524118423461914,
      "learning_rate": 1.8354797540317903e-05,
      "loss": 1.8332,
      "step": 981900
    },
    {
      "epoch": 37.97811037630042,
      "grad_norm": 14.899886131286621,
      "learning_rate": 1.8351574686416318e-05,
      "loss": 1.8383,
      "step": 982000
    },
    {
      "epoch": 37.98197780098233,
      "grad_norm": 10.782386779785156,
      "learning_rate": 1.834835183251473e-05,
      "loss": 1.8205,
      "step": 982100
    },
    {
      "epoch": 37.98584522566423,
      "grad_norm": 13.399641036987305,
      "learning_rate": 1.8345128978613144e-05,
      "loss": 1.9425,
      "step": 982200
    },
    {
      "epoch": 37.989712650346135,
      "grad_norm": 17.220138549804688,
      "learning_rate": 1.8341906124711555e-05,
      "loss": 1.8111,
      "step": 982300
    },
    {
      "epoch": 37.99358007502804,
      "grad_norm": 15.581490516662598,
      "learning_rate": 1.833868327080997e-05,
      "loss": 1.7785,
      "step": 982400
    },
    {
      "epoch": 37.99744749970994,
      "grad_norm": 13.22017765045166,
      "learning_rate": 1.833546041690838e-05,
      "loss": 1.8535,
      "step": 982500
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.777618408203125,
      "eval_runtime": 2.8903,
      "eval_samples_per_second": 470.879,
      "eval_steps_per_second": 470.879,
      "step": 982566
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.6340818405151367,
      "eval_runtime": 55.9713,
      "eval_samples_per_second": 461.969,
      "eval_steps_per_second": 461.969,
      "step": 982566
    },
    {
      "epoch": 38.00131492439185,
      "grad_norm": 11.918512344360352,
      "learning_rate": 1.8332237563006793e-05,
      "loss": 1.7992,
      "step": 982600
    },
    {
      "epoch": 38.00518234907375,
      "grad_norm": 10.82830810546875,
      "learning_rate": 1.8329014709105207e-05,
      "loss": 1.8045,
      "step": 982700
    },
    {
      "epoch": 38.009049773755656,
      "grad_norm": 12.387228012084961,
      "learning_rate": 1.832579185520362e-05,
      "loss": 1.8819,
      "step": 982800
    },
    {
      "epoch": 38.01291719843756,
      "grad_norm": 14.629936218261719,
      "learning_rate": 1.8322569001302034e-05,
      "loss": 1.8422,
      "step": 982900
    },
    {
      "epoch": 38.01678462311946,
      "grad_norm": 11.135140419006348,
      "learning_rate": 1.8319346147400445e-05,
      "loss": 1.7958,
      "step": 983000
    },
    {
      "epoch": 38.02065204780137,
      "grad_norm": 12.267860412597656,
      "learning_rate": 1.831612329349886e-05,
      "loss": 1.7894,
      "step": 983100
    },
    {
      "epoch": 38.02451947248327,
      "grad_norm": 10.728760719299316,
      "learning_rate": 1.831290043959727e-05,
      "loss": 1.7375,
      "step": 983200
    },
    {
      "epoch": 38.02838689716518,
      "grad_norm": 6.325543403625488,
      "learning_rate": 1.8309677585695686e-05,
      "loss": 1.9114,
      "step": 983300
    },
    {
      "epoch": 38.032254321847084,
      "grad_norm": 12.03022289276123,
      "learning_rate": 1.8306454731794097e-05,
      "loss": 1.8479,
      "step": 983400
    },
    {
      "epoch": 38.036121746528984,
      "grad_norm": 13.222336769104004,
      "learning_rate": 1.830323187789251e-05,
      "loss": 1.783,
      "step": 983500
    },
    {
      "epoch": 38.03998917121089,
      "grad_norm": 11.762803077697754,
      "learning_rate": 1.8300009023990923e-05,
      "loss": 1.7484,
      "step": 983600
    },
    {
      "epoch": 38.0438565958928,
      "grad_norm": 7.707987308502197,
      "learning_rate": 1.8296786170089338e-05,
      "loss": 1.9341,
      "step": 983700
    },
    {
      "epoch": 38.0477240205747,
      "grad_norm": 11.228167533874512,
      "learning_rate": 1.829356331618775e-05,
      "loss": 1.8699,
      "step": 983800
    },
    {
      "epoch": 38.051591445256605,
      "grad_norm": 13.14786434173584,
      "learning_rate": 1.8290340462286164e-05,
      "loss": 1.8988,
      "step": 983900
    },
    {
      "epoch": 38.055458869938505,
      "grad_norm": 8.810933113098145,
      "learning_rate": 1.828711760838458e-05,
      "loss": 1.824,
      "step": 984000
    },
    {
      "epoch": 38.05932629462041,
      "grad_norm": 12.702432632446289,
      "learning_rate": 1.828389475448299e-05,
      "loss": 1.7438,
      "step": 984100
    },
    {
      "epoch": 38.06319371930232,
      "grad_norm": 12.150766372680664,
      "learning_rate": 1.8280671900581405e-05,
      "loss": 1.863,
      "step": 984200
    },
    {
      "epoch": 38.06706114398422,
      "grad_norm": 11.74791145324707,
      "learning_rate": 1.827744904667982e-05,
      "loss": 1.9189,
      "step": 984300
    },
    {
      "epoch": 38.070928568666126,
      "grad_norm": 14.86507511138916,
      "learning_rate": 1.827422619277823e-05,
      "loss": 1.8725,
      "step": 984400
    },
    {
      "epoch": 38.074795993348026,
      "grad_norm": 10.459552764892578,
      "learning_rate": 1.8271003338876645e-05,
      "loss": 1.8945,
      "step": 984500
    },
    {
      "epoch": 38.07866341802993,
      "grad_norm": 11.185640335083008,
      "learning_rate": 1.8267780484975057e-05,
      "loss": 1.8245,
      "step": 984600
    },
    {
      "epoch": 38.08253084271184,
      "grad_norm": 13.56381607055664,
      "learning_rate": 1.826455763107347e-05,
      "loss": 1.8509,
      "step": 984700
    },
    {
      "epoch": 38.08639826739374,
      "grad_norm": 11.435564994812012,
      "learning_rate": 1.8261334777171883e-05,
      "loss": 1.9267,
      "step": 984800
    },
    {
      "epoch": 38.09026569207565,
      "grad_norm": 13.323873519897461,
      "learning_rate": 1.8258111923270297e-05,
      "loss": 1.8186,
      "step": 984900
    },
    {
      "epoch": 38.094133116757554,
      "grad_norm": 9.9666748046875,
      "learning_rate": 1.825488906936871e-05,
      "loss": 1.8058,
      "step": 985000
    },
    {
      "epoch": 38.098000541439454,
      "grad_norm": 11.373780250549316,
      "learning_rate": 1.8251666215467123e-05,
      "loss": 1.8337,
      "step": 985100
    },
    {
      "epoch": 38.10186796612136,
      "grad_norm": 11.776322364807129,
      "learning_rate": 1.8248443361565535e-05,
      "loss": 1.8263,
      "step": 985200
    },
    {
      "epoch": 38.10573539080326,
      "grad_norm": 12.324132919311523,
      "learning_rate": 1.824522050766395e-05,
      "loss": 1.8642,
      "step": 985300
    },
    {
      "epoch": 38.10960281548517,
      "grad_norm": 13.029948234558105,
      "learning_rate": 1.824199765376236e-05,
      "loss": 1.8655,
      "step": 985400
    },
    {
      "epoch": 38.113470240167075,
      "grad_norm": 9.895072937011719,
      "learning_rate": 1.8238774799860772e-05,
      "loss": 2.0302,
      "step": 985500
    },
    {
      "epoch": 38.117337664848975,
      "grad_norm": 13.602130889892578,
      "learning_rate": 1.8235551945959187e-05,
      "loss": 1.7584,
      "step": 985600
    },
    {
      "epoch": 38.12120508953088,
      "grad_norm": 11.335993766784668,
      "learning_rate": 1.8232329092057598e-05,
      "loss": 1.9007,
      "step": 985700
    },
    {
      "epoch": 38.12507251421279,
      "grad_norm": 15.619367599487305,
      "learning_rate": 1.8229106238156013e-05,
      "loss": 1.8044,
      "step": 985800
    },
    {
      "epoch": 38.12893993889469,
      "grad_norm": 11.153433799743652,
      "learning_rate": 1.8225883384254424e-05,
      "loss": 1.8289,
      "step": 985900
    },
    {
      "epoch": 38.132807363576596,
      "grad_norm": 8.852694511413574,
      "learning_rate": 1.822266053035284e-05,
      "loss": 1.7465,
      "step": 986000
    },
    {
      "epoch": 38.136674788258496,
      "grad_norm": 13.977987289428711,
      "learning_rate": 1.821943767645125e-05,
      "loss": 1.7496,
      "step": 986100
    },
    {
      "epoch": 38.1405422129404,
      "grad_norm": 11.931696891784668,
      "learning_rate": 1.8216214822549665e-05,
      "loss": 1.7978,
      "step": 986200
    },
    {
      "epoch": 38.14440963762231,
      "grad_norm": 12.26888656616211,
      "learning_rate": 1.8212991968648076e-05,
      "loss": 1.8909,
      "step": 986300
    },
    {
      "epoch": 38.14827706230421,
      "grad_norm": 13.040075302124023,
      "learning_rate": 1.820976911474649e-05,
      "loss": 1.8361,
      "step": 986400
    },
    {
      "epoch": 38.15214448698612,
      "grad_norm": 10.081838607788086,
      "learning_rate": 1.8206546260844902e-05,
      "loss": 1.74,
      "step": 986500
    },
    {
      "epoch": 38.15601191166802,
      "grad_norm": 13.518267631530762,
      "learning_rate": 1.8203323406943317e-05,
      "loss": 1.8239,
      "step": 986600
    },
    {
      "epoch": 38.159879336349924,
      "grad_norm": 13.72094440460205,
      "learning_rate": 1.820010055304173e-05,
      "loss": 1.7675,
      "step": 986700
    },
    {
      "epoch": 38.16374676103183,
      "grad_norm": 9.847208976745605,
      "learning_rate": 1.8196877699140143e-05,
      "loss": 1.8254,
      "step": 986800
    },
    {
      "epoch": 38.16761418571373,
      "grad_norm": 10.261983871459961,
      "learning_rate": 1.8193654845238555e-05,
      "loss": 1.7581,
      "step": 986900
    },
    {
      "epoch": 38.17148161039564,
      "grad_norm": 15.678770065307617,
      "learning_rate": 1.819043199133697e-05,
      "loss": 1.8073,
      "step": 987000
    },
    {
      "epoch": 38.175349035077545,
      "grad_norm": 17.09787368774414,
      "learning_rate": 1.818720913743538e-05,
      "loss": 1.7629,
      "step": 987100
    },
    {
      "epoch": 38.179216459759445,
      "grad_norm": 10.741518020629883,
      "learning_rate": 1.8183986283533795e-05,
      "loss": 1.9615,
      "step": 987200
    },
    {
      "epoch": 38.18308388444135,
      "grad_norm": 10.488456726074219,
      "learning_rate": 1.8180763429632207e-05,
      "loss": 1.8692,
      "step": 987300
    },
    {
      "epoch": 38.18695130912325,
      "grad_norm": 10.657613754272461,
      "learning_rate": 1.817754057573062e-05,
      "loss": 1.8615,
      "step": 987400
    },
    {
      "epoch": 38.19081873380516,
      "grad_norm": 22.891433715820312,
      "learning_rate": 1.8174317721829036e-05,
      "loss": 1.8209,
      "step": 987500
    },
    {
      "epoch": 38.194686158487066,
      "grad_norm": 12.434172630310059,
      "learning_rate": 1.8171094867927447e-05,
      "loss": 1.8045,
      "step": 987600
    },
    {
      "epoch": 38.198553583168966,
      "grad_norm": 11.59752368927002,
      "learning_rate": 1.8167872014025862e-05,
      "loss": 1.8704,
      "step": 987700
    },
    {
      "epoch": 38.20242100785087,
      "grad_norm": 10.771441459655762,
      "learning_rate": 1.8164649160124277e-05,
      "loss": 1.7738,
      "step": 987800
    },
    {
      "epoch": 38.20628843253277,
      "grad_norm": 13.681337356567383,
      "learning_rate": 1.8161426306222688e-05,
      "loss": 1.7169,
      "step": 987900
    },
    {
      "epoch": 38.21015585721468,
      "grad_norm": 11.608736038208008,
      "learning_rate": 1.8158203452321103e-05,
      "loss": 1.8515,
      "step": 988000
    },
    {
      "epoch": 38.21402328189659,
      "grad_norm": 8.500007629394531,
      "learning_rate": 1.8154980598419514e-05,
      "loss": 1.8563,
      "step": 988100
    },
    {
      "epoch": 38.21789070657849,
      "grad_norm": 11.657129287719727,
      "learning_rate": 1.815175774451793e-05,
      "loss": 1.8931,
      "step": 988200
    },
    {
      "epoch": 38.221758131260394,
      "grad_norm": 10.683576583862305,
      "learning_rate": 1.814853489061634e-05,
      "loss": 1.8032,
      "step": 988300
    },
    {
      "epoch": 38.2256255559423,
      "grad_norm": 12.951432228088379,
      "learning_rate": 1.8145312036714755e-05,
      "loss": 1.7668,
      "step": 988400
    },
    {
      "epoch": 38.2294929806242,
      "grad_norm": 11.651991844177246,
      "learning_rate": 1.8142089182813166e-05,
      "loss": 1.8026,
      "step": 988500
    },
    {
      "epoch": 38.23336040530611,
      "grad_norm": 15.03628921508789,
      "learning_rate": 1.8138866328911578e-05,
      "loss": 1.8494,
      "step": 988600
    },
    {
      "epoch": 38.23722782998801,
      "grad_norm": 9.570712089538574,
      "learning_rate": 1.8135643475009992e-05,
      "loss": 1.8501,
      "step": 988700
    },
    {
      "epoch": 38.241095254669915,
      "grad_norm": 13.432527542114258,
      "learning_rate": 1.8132420621108404e-05,
      "loss": 1.9257,
      "step": 988800
    },
    {
      "epoch": 38.24496267935182,
      "grad_norm": 10.676790237426758,
      "learning_rate": 1.812919776720682e-05,
      "loss": 1.8436,
      "step": 988900
    },
    {
      "epoch": 38.24883010403372,
      "grad_norm": 11.925602912902832,
      "learning_rate": 1.812597491330523e-05,
      "loss": 1.8577,
      "step": 989000
    },
    {
      "epoch": 38.25269752871563,
      "grad_norm": 12.137282371520996,
      "learning_rate": 1.8122752059403644e-05,
      "loss": 1.8637,
      "step": 989100
    },
    {
      "epoch": 38.256564953397536,
      "grad_norm": 10.377315521240234,
      "learning_rate": 1.8119529205502056e-05,
      "loss": 1.7381,
      "step": 989200
    },
    {
      "epoch": 38.260432378079436,
      "grad_norm": 9.18067455291748,
      "learning_rate": 1.811630635160047e-05,
      "loss": 1.8353,
      "step": 989300
    },
    {
      "epoch": 38.26429980276134,
      "grad_norm": 12.320267677307129,
      "learning_rate": 1.8113083497698882e-05,
      "loss": 1.8281,
      "step": 989400
    },
    {
      "epoch": 38.26816722744324,
      "grad_norm": 12.77202033996582,
      "learning_rate": 1.8109860643797297e-05,
      "loss": 1.756,
      "step": 989500
    },
    {
      "epoch": 38.27203465212515,
      "grad_norm": 13.979747772216797,
      "learning_rate": 1.8106637789895708e-05,
      "loss": 1.7966,
      "step": 989600
    },
    {
      "epoch": 38.27590207680706,
      "grad_norm": 9.538817405700684,
      "learning_rate": 1.8103414935994123e-05,
      "loss": 1.8661,
      "step": 989700
    },
    {
      "epoch": 38.27976950148896,
      "grad_norm": 11.680054664611816,
      "learning_rate": 1.8100192082092534e-05,
      "loss": 1.8008,
      "step": 989800
    },
    {
      "epoch": 38.283636926170864,
      "grad_norm": 10.654401779174805,
      "learning_rate": 1.809696922819095e-05,
      "loss": 1.8033,
      "step": 989900
    },
    {
      "epoch": 38.287504350852764,
      "grad_norm": 8.328365325927734,
      "learning_rate": 1.809374637428936e-05,
      "loss": 1.8082,
      "step": 990000
    },
    {
      "epoch": 38.29137177553467,
      "grad_norm": 10.396613121032715,
      "learning_rate": 1.8090523520387775e-05,
      "loss": 1.8615,
      "step": 990100
    },
    {
      "epoch": 38.29523920021658,
      "grad_norm": 13.488511085510254,
      "learning_rate": 1.8087300666486186e-05,
      "loss": 1.7907,
      "step": 990200
    },
    {
      "epoch": 38.29910662489848,
      "grad_norm": 14.874492645263672,
      "learning_rate": 1.80840778125846e-05,
      "loss": 1.8283,
      "step": 990300
    },
    {
      "epoch": 38.302974049580385,
      "grad_norm": 9.158138275146484,
      "learning_rate": 1.8080854958683012e-05,
      "loss": 1.8396,
      "step": 990400
    },
    {
      "epoch": 38.30684147426229,
      "grad_norm": 10.942828178405762,
      "learning_rate": 1.8077632104781427e-05,
      "loss": 1.752,
      "step": 990500
    },
    {
      "epoch": 38.31070889894419,
      "grad_norm": 11.017513275146484,
      "learning_rate": 1.8074409250879838e-05,
      "loss": 1.8381,
      "step": 990600
    },
    {
      "epoch": 38.3145763236261,
      "grad_norm": 12.499163627624512,
      "learning_rate": 1.8071186396978253e-05,
      "loss": 1.8622,
      "step": 990700
    },
    {
      "epoch": 38.318443748308,
      "grad_norm": 12.145355224609375,
      "learning_rate": 1.8067963543076664e-05,
      "loss": 1.8626,
      "step": 990800
    },
    {
      "epoch": 38.322311172989906,
      "grad_norm": 10.310503959655762,
      "learning_rate": 1.806474068917508e-05,
      "loss": 1.8141,
      "step": 990900
    },
    {
      "epoch": 38.32617859767181,
      "grad_norm": 12.492083549499512,
      "learning_rate": 1.8061517835273494e-05,
      "loss": 1.8466,
      "step": 991000
    },
    {
      "epoch": 38.33004602235371,
      "grad_norm": 9.087337493896484,
      "learning_rate": 1.8058294981371905e-05,
      "loss": 1.7466,
      "step": 991100
    },
    {
      "epoch": 38.33391344703562,
      "grad_norm": 12.447901725769043,
      "learning_rate": 1.805507212747032e-05,
      "loss": 1.7768,
      "step": 991200
    },
    {
      "epoch": 38.33778087171752,
      "grad_norm": 9.039619445800781,
      "learning_rate": 1.8051849273568734e-05,
      "loss": 1.8115,
      "step": 991300
    },
    {
      "epoch": 38.34164829639943,
      "grad_norm": 10.559207916259766,
      "learning_rate": 1.8048626419667146e-05,
      "loss": 1.8263,
      "step": 991400
    },
    {
      "epoch": 38.345515721081334,
      "grad_norm": 10.841105461120605,
      "learning_rate": 1.8045403565765557e-05,
      "loss": 1.7967,
      "step": 991500
    },
    {
      "epoch": 38.349383145763234,
      "grad_norm": 11.460442543029785,
      "learning_rate": 1.8042180711863972e-05,
      "loss": 1.8282,
      "step": 991600
    },
    {
      "epoch": 38.35325057044514,
      "grad_norm": 10.5922212600708,
      "learning_rate": 1.8038957857962383e-05,
      "loss": 1.9073,
      "step": 991700
    },
    {
      "epoch": 38.35711799512705,
      "grad_norm": 14.143953323364258,
      "learning_rate": 1.8035735004060798e-05,
      "loss": 1.7659,
      "step": 991800
    },
    {
      "epoch": 38.36098541980895,
      "grad_norm": 12.592384338378906,
      "learning_rate": 1.803251215015921e-05,
      "loss": 1.8331,
      "step": 991900
    },
    {
      "epoch": 38.364852844490855,
      "grad_norm": 13.012133598327637,
      "learning_rate": 1.8029289296257624e-05,
      "loss": 1.7725,
      "step": 992000
    },
    {
      "epoch": 38.368720269172755,
      "grad_norm": 12.394936561584473,
      "learning_rate": 1.8026066442356035e-05,
      "loss": 1.8241,
      "step": 992100
    },
    {
      "epoch": 38.37258769385466,
      "grad_norm": 14.064798355102539,
      "learning_rate": 1.802284358845445e-05,
      "loss": 1.828,
      "step": 992200
    },
    {
      "epoch": 38.37645511853657,
      "grad_norm": 16.27650260925293,
      "learning_rate": 1.801962073455286e-05,
      "loss": 1.8462,
      "step": 992300
    },
    {
      "epoch": 38.38032254321847,
      "grad_norm": 10.331196784973145,
      "learning_rate": 1.8016397880651276e-05,
      "loss": 1.8533,
      "step": 992400
    },
    {
      "epoch": 38.384189967900376,
      "grad_norm": 11.585014343261719,
      "learning_rate": 1.8013175026749687e-05,
      "loss": 1.7666,
      "step": 992500
    },
    {
      "epoch": 38.38805739258228,
      "grad_norm": 12.987664222717285,
      "learning_rate": 1.8009952172848102e-05,
      "loss": 1.8031,
      "step": 992600
    },
    {
      "epoch": 38.39192481726418,
      "grad_norm": 12.001511573791504,
      "learning_rate": 1.8006729318946513e-05,
      "loss": 1.7714,
      "step": 992700
    },
    {
      "epoch": 38.39579224194609,
      "grad_norm": 14.977592468261719,
      "learning_rate": 1.8003506465044928e-05,
      "loss": 1.8067,
      "step": 992800
    },
    {
      "epoch": 38.39965966662799,
      "grad_norm": 11.140295028686523,
      "learning_rate": 1.800028361114334e-05,
      "loss": 1.828,
      "step": 992900
    },
    {
      "epoch": 38.4035270913099,
      "grad_norm": 14.722403526306152,
      "learning_rate": 1.7997060757241754e-05,
      "loss": 1.8397,
      "step": 993000
    },
    {
      "epoch": 38.407394515991804,
      "grad_norm": 11.427727699279785,
      "learning_rate": 1.7993837903340165e-05,
      "loss": 1.877,
      "step": 993100
    },
    {
      "epoch": 38.411261940673704,
      "grad_norm": 13.16331672668457,
      "learning_rate": 1.799061504943858e-05,
      "loss": 1.8893,
      "step": 993200
    },
    {
      "epoch": 38.41512936535561,
      "grad_norm": 14.48967170715332,
      "learning_rate": 1.798739219553699e-05,
      "loss": 1.8806,
      "step": 993300
    },
    {
      "epoch": 38.41899679003751,
      "grad_norm": 11.622684478759766,
      "learning_rate": 1.7984169341635406e-05,
      "loss": 1.8001,
      "step": 993400
    },
    {
      "epoch": 38.42286421471942,
      "grad_norm": 10.60755729675293,
      "learning_rate": 1.7980946487733818e-05,
      "loss": 1.8142,
      "step": 993500
    },
    {
      "epoch": 38.426731639401325,
      "grad_norm": 10.837140083312988,
      "learning_rate": 1.7977723633832232e-05,
      "loss": 1.8452,
      "step": 993600
    },
    {
      "epoch": 38.430599064083225,
      "grad_norm": 12.858465194702148,
      "learning_rate": 1.7974500779930644e-05,
      "loss": 1.9121,
      "step": 993700
    },
    {
      "epoch": 38.43446648876513,
      "grad_norm": 13.562334060668945,
      "learning_rate": 1.7971277926029058e-05,
      "loss": 1.8076,
      "step": 993800
    },
    {
      "epoch": 38.43833391344704,
      "grad_norm": 15.09337329864502,
      "learning_rate": 1.796805507212747e-05,
      "loss": 1.9342,
      "step": 993900
    },
    {
      "epoch": 38.44220133812894,
      "grad_norm": 7.482527732849121,
      "learning_rate": 1.7964832218225884e-05,
      "loss": 1.8296,
      "step": 994000
    },
    {
      "epoch": 38.446068762810846,
      "grad_norm": 11.496644973754883,
      "learning_rate": 1.7961609364324296e-05,
      "loss": 1.8339,
      "step": 994100
    },
    {
      "epoch": 38.449936187492746,
      "grad_norm": 10.197239875793457,
      "learning_rate": 1.795838651042271e-05,
      "loss": 1.8521,
      "step": 994200
    },
    {
      "epoch": 38.45380361217465,
      "grad_norm": 15.925068855285645,
      "learning_rate": 1.7955163656521125e-05,
      "loss": 1.865,
      "step": 994300
    },
    {
      "epoch": 38.45767103685656,
      "grad_norm": 12.816983222961426,
      "learning_rate": 1.7951940802619536e-05,
      "loss": 1.8245,
      "step": 994400
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 13.59199333190918,
      "learning_rate": 1.794871794871795e-05,
      "loss": 1.8615,
      "step": 994500
    },
    {
      "epoch": 38.46540588622037,
      "grad_norm": 12.775518417358398,
      "learning_rate": 1.7945495094816362e-05,
      "loss": 1.7397,
      "step": 994600
    },
    {
      "epoch": 38.46927331090227,
      "grad_norm": 15.132328987121582,
      "learning_rate": 1.7942272240914777e-05,
      "loss": 1.7667,
      "step": 994700
    },
    {
      "epoch": 38.473140735584174,
      "grad_norm": 11.794591903686523,
      "learning_rate": 1.793904938701319e-05,
      "loss": 1.8868,
      "step": 994800
    },
    {
      "epoch": 38.47700816026608,
      "grad_norm": 10.695158004760742,
      "learning_rate": 1.7935826533111603e-05,
      "loss": 1.8151,
      "step": 994900
    },
    {
      "epoch": 38.48087558494798,
      "grad_norm": 12.30770492553711,
      "learning_rate": 1.7932603679210015e-05,
      "loss": 1.8597,
      "step": 995000
    },
    {
      "epoch": 38.48474300962989,
      "grad_norm": 13.923602104187012,
      "learning_rate": 1.792938082530843e-05,
      "loss": 1.87,
      "step": 995100
    },
    {
      "epoch": 38.488610434311795,
      "grad_norm": 12.12080192565918,
      "learning_rate": 1.792615797140684e-05,
      "loss": 1.822,
      "step": 995200
    },
    {
      "epoch": 38.492477858993695,
      "grad_norm": 13.579647064208984,
      "learning_rate": 1.7922935117505255e-05,
      "loss": 1.7581,
      "step": 995300
    },
    {
      "epoch": 38.4963452836756,
      "grad_norm": 10.134455680847168,
      "learning_rate": 1.7919712263603667e-05,
      "loss": 1.9588,
      "step": 995400
    },
    {
      "epoch": 38.5002127083575,
      "grad_norm": 12.806841850280762,
      "learning_rate": 1.791648940970208e-05,
      "loss": 1.8019,
      "step": 995500
    },
    {
      "epoch": 38.50408013303941,
      "grad_norm": 10.692444801330566,
      "learning_rate": 1.7913266555800493e-05,
      "loss": 1.9062,
      "step": 995600
    },
    {
      "epoch": 38.507947557721316,
      "grad_norm": 12.227071762084961,
      "learning_rate": 1.7910043701898907e-05,
      "loss": 1.897,
      "step": 995700
    },
    {
      "epoch": 38.511814982403216,
      "grad_norm": 11.6328706741333,
      "learning_rate": 1.790682084799732e-05,
      "loss": 1.7639,
      "step": 995800
    },
    {
      "epoch": 38.51568240708512,
      "grad_norm": 10.51253890991211,
      "learning_rate": 1.7903597994095733e-05,
      "loss": 1.8199,
      "step": 995900
    },
    {
      "epoch": 38.51954983176702,
      "grad_norm": 13.428789138793945,
      "learning_rate": 1.7900375140194145e-05,
      "loss": 1.8351,
      "step": 996000
    },
    {
      "epoch": 38.52341725644893,
      "grad_norm": 12.135723114013672,
      "learning_rate": 1.789715228629256e-05,
      "loss": 1.8484,
      "step": 996100
    },
    {
      "epoch": 38.52728468113084,
      "grad_norm": 12.472784042358398,
      "learning_rate": 1.789392943239097e-05,
      "loss": 1.805,
      "step": 996200
    },
    {
      "epoch": 38.53115210581274,
      "grad_norm": 12.87022590637207,
      "learning_rate": 1.7890706578489386e-05,
      "loss": 1.8596,
      "step": 996300
    },
    {
      "epoch": 38.535019530494644,
      "grad_norm": 11.996217727661133,
      "learning_rate": 1.7887483724587797e-05,
      "loss": 1.8493,
      "step": 996400
    },
    {
      "epoch": 38.53888695517655,
      "grad_norm": 13.039193153381348,
      "learning_rate": 1.788426087068621e-05,
      "loss": 1.8002,
      "step": 996500
    },
    {
      "epoch": 38.54275437985845,
      "grad_norm": 8.872262001037598,
      "learning_rate": 1.7881038016784623e-05,
      "loss": 1.8799,
      "step": 996600
    },
    {
      "epoch": 38.54662180454036,
      "grad_norm": 9.149149894714355,
      "learning_rate": 1.7877815162883038e-05,
      "loss": 1.8443,
      "step": 996700
    },
    {
      "epoch": 38.55048922922226,
      "grad_norm": 15.246581077575684,
      "learning_rate": 1.787459230898145e-05,
      "loss": 1.76,
      "step": 996800
    },
    {
      "epoch": 38.554356653904165,
      "grad_norm": 13.384746551513672,
      "learning_rate": 1.7871369455079864e-05,
      "loss": 1.745,
      "step": 996900
    },
    {
      "epoch": 38.55822407858607,
      "grad_norm": 12.174314498901367,
      "learning_rate": 1.7868146601178275e-05,
      "loss": 1.8476,
      "step": 997000
    },
    {
      "epoch": 38.56209150326797,
      "grad_norm": 14.167012214660645,
      "learning_rate": 1.786492374727669e-05,
      "loss": 1.878,
      "step": 997100
    },
    {
      "epoch": 38.56595892794988,
      "grad_norm": 11.923904418945312,
      "learning_rate": 1.78617008933751e-05,
      "loss": 1.9397,
      "step": 997200
    },
    {
      "epoch": 38.569826352631786,
      "grad_norm": 10.75011157989502,
      "learning_rate": 1.7858478039473512e-05,
      "loss": 1.7996,
      "step": 997300
    },
    {
      "epoch": 38.573693777313686,
      "grad_norm": 11.67078685760498,
      "learning_rate": 1.7855255185571927e-05,
      "loss": 1.6943,
      "step": 997400
    },
    {
      "epoch": 38.57756120199559,
      "grad_norm": 8.160173416137695,
      "learning_rate": 1.7852032331670342e-05,
      "loss": 1.9516,
      "step": 997500
    },
    {
      "epoch": 38.58142862667749,
      "grad_norm": 14.223956108093262,
      "learning_rate": 1.7848809477768753e-05,
      "loss": 1.955,
      "step": 997600
    },
    {
      "epoch": 38.5852960513594,
      "grad_norm": 11.893579483032227,
      "learning_rate": 1.7845586623867168e-05,
      "loss": 1.8314,
      "step": 997700
    },
    {
      "epoch": 38.58916347604131,
      "grad_norm": 11.86495304107666,
      "learning_rate": 1.7842363769965583e-05,
      "loss": 1.7854,
      "step": 997800
    },
    {
      "epoch": 38.59303090072321,
      "grad_norm": 11.186800956726074,
      "learning_rate": 1.7839140916063994e-05,
      "loss": 1.8405,
      "step": 997900
    },
    {
      "epoch": 38.596898325405114,
      "grad_norm": 13.420272827148438,
      "learning_rate": 1.783591806216241e-05,
      "loss": 1.816,
      "step": 998000
    },
    {
      "epoch": 38.600765750087014,
      "grad_norm": 9.846895217895508,
      "learning_rate": 1.783269520826082e-05,
      "loss": 1.8666,
      "step": 998100
    },
    {
      "epoch": 38.60463317476892,
      "grad_norm": 10.953410148620605,
      "learning_rate": 1.7829472354359235e-05,
      "loss": 1.7883,
      "step": 998200
    },
    {
      "epoch": 38.60850059945083,
      "grad_norm": 12.127285957336426,
      "learning_rate": 1.7826249500457646e-05,
      "loss": 1.8509,
      "step": 998300
    },
    {
      "epoch": 38.61236802413273,
      "grad_norm": 15.57280445098877,
      "learning_rate": 1.782302664655606e-05,
      "loss": 1.8447,
      "step": 998400
    },
    {
      "epoch": 38.616235448814635,
      "grad_norm": 10.532320022583008,
      "learning_rate": 1.7819803792654472e-05,
      "loss": 1.8587,
      "step": 998500
    },
    {
      "epoch": 38.62010287349654,
      "grad_norm": 12.063027381896973,
      "learning_rate": 1.7816580938752887e-05,
      "loss": 1.726,
      "step": 998600
    },
    {
      "epoch": 38.62397029817844,
      "grad_norm": 10.534640312194824,
      "learning_rate": 1.7813358084851298e-05,
      "loss": 1.8513,
      "step": 998700
    },
    {
      "epoch": 38.62783772286035,
      "grad_norm": 12.341178894042969,
      "learning_rate": 1.7810135230949713e-05,
      "loss": 1.8025,
      "step": 998800
    },
    {
      "epoch": 38.63170514754225,
      "grad_norm": 13.443105697631836,
      "learning_rate": 1.7806912377048124e-05,
      "loss": 1.8472,
      "step": 998900
    },
    {
      "epoch": 38.635572572224156,
      "grad_norm": 10.97690486907959,
      "learning_rate": 1.780368952314654e-05,
      "loss": 1.8569,
      "step": 999000
    },
    {
      "epoch": 38.63943999690606,
      "grad_norm": 12.013470649719238,
      "learning_rate": 1.780046666924495e-05,
      "loss": 1.8675,
      "step": 999100
    },
    {
      "epoch": 38.64330742158796,
      "grad_norm": 11.145426750183105,
      "learning_rate": 1.7797243815343365e-05,
      "loss": 1.7827,
      "step": 999200
    },
    {
      "epoch": 38.64717484626987,
      "grad_norm": 10.034991264343262,
      "learning_rate": 1.7794020961441776e-05,
      "loss": 1.8431,
      "step": 999300
    },
    {
      "epoch": 38.65104227095177,
      "grad_norm": 16.041015625,
      "learning_rate": 1.779079810754019e-05,
      "loss": 1.69,
      "step": 999400
    },
    {
      "epoch": 38.65490969563368,
      "grad_norm": 10.489078521728516,
      "learning_rate": 1.7787575253638602e-05,
      "loss": 1.8948,
      "step": 999500
    },
    {
      "epoch": 38.658777120315584,
      "grad_norm": 11.793792724609375,
      "learning_rate": 1.7784352399737017e-05,
      "loss": 1.9055,
      "step": 999600
    },
    {
      "epoch": 38.662644544997484,
      "grad_norm": 10.648680686950684,
      "learning_rate": 1.778112954583543e-05,
      "loss": 1.7682,
      "step": 999700
    },
    {
      "epoch": 38.66651196967939,
      "grad_norm": 12.85092830657959,
      "learning_rate": 1.7777906691933843e-05,
      "loss": 1.9415,
      "step": 999800
    },
    {
      "epoch": 38.6703793943613,
      "grad_norm": 11.484101295471191,
      "learning_rate": 1.7774683838032254e-05,
      "loss": 1.7795,
      "step": 999900
    },
    {
      "epoch": 38.6742468190432,
      "grad_norm": 10.40965461730957,
      "learning_rate": 1.777146098413067e-05,
      "loss": 1.8545,
      "step": 1000000
    },
    {
      "epoch": 38.678114243725105,
      "grad_norm": 11.915871620178223,
      "learning_rate": 1.776823813022908e-05,
      "loss": 1.8752,
      "step": 1000100
    },
    {
      "epoch": 38.681981668407005,
      "grad_norm": 13.638286590576172,
      "learning_rate": 1.7765015276327495e-05,
      "loss": 1.8076,
      "step": 1000200
    },
    {
      "epoch": 38.68584909308891,
      "grad_norm": 15.733985900878906,
      "learning_rate": 1.7761792422425907e-05,
      "loss": 1.8799,
      "step": 1000300
    },
    {
      "epoch": 38.68971651777082,
      "grad_norm": 14.154973983764648,
      "learning_rate": 1.7758569568524318e-05,
      "loss": 1.9005,
      "step": 1000400
    },
    {
      "epoch": 38.69358394245272,
      "grad_norm": 12.011672973632812,
      "learning_rate": 1.7755346714622733e-05,
      "loss": 1.85,
      "step": 1000500
    },
    {
      "epoch": 38.697451367134626,
      "grad_norm": 11.636832237243652,
      "learning_rate": 1.7752123860721144e-05,
      "loss": 1.7879,
      "step": 1000600
    },
    {
      "epoch": 38.70131879181653,
      "grad_norm": 11.258759498596191,
      "learning_rate": 1.774890100681956e-05,
      "loss": 1.8025,
      "step": 1000700
    },
    {
      "epoch": 38.70518621649843,
      "grad_norm": 10.353150367736816,
      "learning_rate": 1.774567815291797e-05,
      "loss": 1.8672,
      "step": 1000800
    },
    {
      "epoch": 38.70905364118034,
      "grad_norm": 14.135751724243164,
      "learning_rate": 1.7742455299016385e-05,
      "loss": 1.8224,
      "step": 1000900
    },
    {
      "epoch": 38.71292106586224,
      "grad_norm": 14.69414234161377,
      "learning_rate": 1.77392324451148e-05,
      "loss": 1.9131,
      "step": 1001000
    },
    {
      "epoch": 38.71678849054415,
      "grad_norm": 9.925180435180664,
      "learning_rate": 1.773600959121321e-05,
      "loss": 1.798,
      "step": 1001100
    },
    {
      "epoch": 38.720655915226054,
      "grad_norm": 11.177103042602539,
      "learning_rate": 1.7732786737311625e-05,
      "loss": 1.9264,
      "step": 1001200
    },
    {
      "epoch": 38.724523339907954,
      "grad_norm": 13.050601959228516,
      "learning_rate": 1.772956388341004e-05,
      "loss": 1.8713,
      "step": 1001300
    },
    {
      "epoch": 38.72839076458986,
      "grad_norm": 12.619872093200684,
      "learning_rate": 1.772634102950845e-05,
      "loss": 1.8114,
      "step": 1001400
    },
    {
      "epoch": 38.73225818927176,
      "grad_norm": 13.523698806762695,
      "learning_rate": 1.7723118175606866e-05,
      "loss": 1.8975,
      "step": 1001500
    },
    {
      "epoch": 38.73612561395367,
      "grad_norm": 10.592820167541504,
      "learning_rate": 1.7719895321705278e-05,
      "loss": 1.8356,
      "step": 1001600
    },
    {
      "epoch": 38.739993038635575,
      "grad_norm": 10.695752143859863,
      "learning_rate": 1.7716672467803692e-05,
      "loss": 1.8156,
      "step": 1001700
    },
    {
      "epoch": 38.743860463317475,
      "grad_norm": 10.734474182128906,
      "learning_rate": 1.7713449613902104e-05,
      "loss": 1.8478,
      "step": 1001800
    },
    {
      "epoch": 38.74772788799938,
      "grad_norm": 11.669217109680176,
      "learning_rate": 1.771022676000052e-05,
      "loss": 1.9377,
      "step": 1001900
    },
    {
      "epoch": 38.75159531268129,
      "grad_norm": 15.089104652404785,
      "learning_rate": 1.770700390609893e-05,
      "loss": 1.8268,
      "step": 1002000
    },
    {
      "epoch": 38.75546273736319,
      "grad_norm": 12.101797103881836,
      "learning_rate": 1.7703781052197344e-05,
      "loss": 1.8164,
      "step": 1002100
    },
    {
      "epoch": 38.759330162045096,
      "grad_norm": 13.24386978149414,
      "learning_rate": 1.7700558198295756e-05,
      "loss": 1.8056,
      "step": 1002200
    },
    {
      "epoch": 38.763197586726996,
      "grad_norm": 10.610210418701172,
      "learning_rate": 1.769733534439417e-05,
      "loss": 1.7629,
      "step": 1002300
    },
    {
      "epoch": 38.7670650114089,
      "grad_norm": 11.80793571472168,
      "learning_rate": 1.7694112490492582e-05,
      "loss": 1.8801,
      "step": 1002400
    },
    {
      "epoch": 38.77093243609081,
      "grad_norm": 9.243980407714844,
      "learning_rate": 1.7690889636590996e-05,
      "loss": 1.8073,
      "step": 1002500
    },
    {
      "epoch": 38.77479986077271,
      "grad_norm": 8.826745986938477,
      "learning_rate": 1.7687666782689408e-05,
      "loss": 1.7825,
      "step": 1002600
    },
    {
      "epoch": 38.77866728545462,
      "grad_norm": 10.809618949890137,
      "learning_rate": 1.7684443928787823e-05,
      "loss": 1.7998,
      "step": 1002700
    },
    {
      "epoch": 38.78253471013652,
      "grad_norm": 11.817090034484863,
      "learning_rate": 1.7681221074886234e-05,
      "loss": 1.7434,
      "step": 1002800
    },
    {
      "epoch": 38.786402134818424,
      "grad_norm": 13.378951072692871,
      "learning_rate": 1.767799822098465e-05,
      "loss": 1.8386,
      "step": 1002900
    },
    {
      "epoch": 38.79026955950033,
      "grad_norm": 12.895962715148926,
      "learning_rate": 1.767477536708306e-05,
      "loss": 1.8769,
      "step": 1003000
    },
    {
      "epoch": 38.79413698418223,
      "grad_norm": 14.685771942138672,
      "learning_rate": 1.7671552513181475e-05,
      "loss": 1.8516,
      "step": 1003100
    },
    {
      "epoch": 38.79800440886414,
      "grad_norm": 12.975014686584473,
      "learning_rate": 1.7668329659279886e-05,
      "loss": 1.8257,
      "step": 1003200
    },
    {
      "epoch": 38.801871833546045,
      "grad_norm": 10.207620620727539,
      "learning_rate": 1.7665106805378297e-05,
      "loss": 1.8245,
      "step": 1003300
    },
    {
      "epoch": 38.805739258227945,
      "grad_norm": 14.5530424118042,
      "learning_rate": 1.7661883951476712e-05,
      "loss": 1.8346,
      "step": 1003400
    },
    {
      "epoch": 38.80960668290985,
      "grad_norm": 12.636335372924805,
      "learning_rate": 1.7658661097575123e-05,
      "loss": 1.9622,
      "step": 1003500
    },
    {
      "epoch": 38.81347410759175,
      "grad_norm": 12.683314323425293,
      "learning_rate": 1.7655438243673538e-05,
      "loss": 1.7465,
      "step": 1003600
    },
    {
      "epoch": 38.81734153227366,
      "grad_norm": 9.006608009338379,
      "learning_rate": 1.765221538977195e-05,
      "loss": 1.8918,
      "step": 1003700
    },
    {
      "epoch": 38.821208956955566,
      "grad_norm": 13.209237098693848,
      "learning_rate": 1.7648992535870364e-05,
      "loss": 1.9034,
      "step": 1003800
    },
    {
      "epoch": 38.825076381637466,
      "grad_norm": 13.335982322692871,
      "learning_rate": 1.7645769681968775e-05,
      "loss": 1.8891,
      "step": 1003900
    },
    {
      "epoch": 38.82894380631937,
      "grad_norm": 12.666833877563477,
      "learning_rate": 1.764254682806719e-05,
      "loss": 1.8212,
      "step": 1004000
    },
    {
      "epoch": 38.83281123100127,
      "grad_norm": 11.257208824157715,
      "learning_rate": 1.76393239741656e-05,
      "loss": 1.9427,
      "step": 1004100
    },
    {
      "epoch": 38.83667865568318,
      "grad_norm": 11.569830894470215,
      "learning_rate": 1.7636101120264016e-05,
      "loss": 1.7769,
      "step": 1004200
    },
    {
      "epoch": 38.84054608036509,
      "grad_norm": 10.88038444519043,
      "learning_rate": 1.763287826636243e-05,
      "loss": 1.7492,
      "step": 1004300
    },
    {
      "epoch": 38.84441350504699,
      "grad_norm": 13.045771598815918,
      "learning_rate": 1.7629655412460842e-05,
      "loss": 1.8203,
      "step": 1004400
    },
    {
      "epoch": 38.848280929728894,
      "grad_norm": 12.108832359313965,
      "learning_rate": 1.7626432558559257e-05,
      "loss": 1.895,
      "step": 1004500
    },
    {
      "epoch": 38.8521483544108,
      "grad_norm": 12.731572151184082,
      "learning_rate": 1.7623209704657668e-05,
      "loss": 1.8666,
      "step": 1004600
    },
    {
      "epoch": 38.8560157790927,
      "grad_norm": 8.421242713928223,
      "learning_rate": 1.7619986850756083e-05,
      "loss": 1.8437,
      "step": 1004700
    },
    {
      "epoch": 38.85988320377461,
      "grad_norm": 12.764820098876953,
      "learning_rate": 1.7616763996854498e-05,
      "loss": 1.8058,
      "step": 1004800
    },
    {
      "epoch": 38.86375062845651,
      "grad_norm": 11.37996768951416,
      "learning_rate": 1.761354114295291e-05,
      "loss": 1.8783,
      "step": 1004900
    },
    {
      "epoch": 38.867618053138415,
      "grad_norm": 11.881753921508789,
      "learning_rate": 1.7610318289051324e-05,
      "loss": 1.7844,
      "step": 1005000
    },
    {
      "epoch": 38.87148547782032,
      "grad_norm": 10.446772575378418,
      "learning_rate": 1.7607095435149735e-05,
      "loss": 1.798,
      "step": 1005100
    },
    {
      "epoch": 38.87535290250222,
      "grad_norm": 10.056846618652344,
      "learning_rate": 1.760387258124815e-05,
      "loss": 1.8063,
      "step": 1005200
    },
    {
      "epoch": 38.87922032718413,
      "grad_norm": 7.986625671386719,
      "learning_rate": 1.760064972734656e-05,
      "loss": 1.8197,
      "step": 1005300
    },
    {
      "epoch": 38.883087751866036,
      "grad_norm": 11.258443832397461,
      "learning_rate": 1.7597426873444976e-05,
      "loss": 1.9223,
      "step": 1005400
    },
    {
      "epoch": 38.886955176547936,
      "grad_norm": 11.928056716918945,
      "learning_rate": 1.7594204019543387e-05,
      "loss": 1.8637,
      "step": 1005500
    },
    {
      "epoch": 38.89082260122984,
      "grad_norm": 10.321521759033203,
      "learning_rate": 1.7590981165641802e-05,
      "loss": 1.7647,
      "step": 1005600
    },
    {
      "epoch": 38.89469002591174,
      "grad_norm": 11.059061050415039,
      "learning_rate": 1.7587758311740213e-05,
      "loss": 1.843,
      "step": 1005700
    },
    {
      "epoch": 38.89855745059365,
      "grad_norm": 14.904783248901367,
      "learning_rate": 1.7584535457838628e-05,
      "loss": 1.8999,
      "step": 1005800
    },
    {
      "epoch": 38.90242487527556,
      "grad_norm": 11.127284049987793,
      "learning_rate": 1.758131260393704e-05,
      "loss": 1.8222,
      "step": 1005900
    },
    {
      "epoch": 38.90629229995746,
      "grad_norm": 10.4920072555542,
      "learning_rate": 1.7578089750035454e-05,
      "loss": 1.8894,
      "step": 1006000
    },
    {
      "epoch": 38.910159724639364,
      "grad_norm": 12.784673690795898,
      "learning_rate": 1.7574866896133865e-05,
      "loss": 1.8876,
      "step": 1006100
    },
    {
      "epoch": 38.914027149321264,
      "grad_norm": 17.23621368408203,
      "learning_rate": 1.7571644042232277e-05,
      "loss": 1.9062,
      "step": 1006200
    },
    {
      "epoch": 38.91789457400317,
      "grad_norm": 12.163350105285645,
      "learning_rate": 1.756842118833069e-05,
      "loss": 1.8597,
      "step": 1006300
    },
    {
      "epoch": 38.92176199868508,
      "grad_norm": 11.83475112915039,
      "learning_rate": 1.7565198334429103e-05,
      "loss": 1.8166,
      "step": 1006400
    },
    {
      "epoch": 38.92562942336698,
      "grad_norm": 12.798113822937012,
      "learning_rate": 1.7561975480527517e-05,
      "loss": 1.7735,
      "step": 1006500
    },
    {
      "epoch": 38.929496848048885,
      "grad_norm": 13.673699378967285,
      "learning_rate": 1.755875262662593e-05,
      "loss": 1.8261,
      "step": 1006600
    },
    {
      "epoch": 38.93336427273079,
      "grad_norm": 7.433532238006592,
      "learning_rate": 1.7555529772724343e-05,
      "loss": 1.8607,
      "step": 1006700
    },
    {
      "epoch": 38.93723169741269,
      "grad_norm": 12.504135131835938,
      "learning_rate": 1.7552306918822755e-05,
      "loss": 1.8586,
      "step": 1006800
    },
    {
      "epoch": 38.9410991220946,
      "grad_norm": 12.930782318115234,
      "learning_rate": 1.754908406492117e-05,
      "loss": 1.7557,
      "step": 1006900
    },
    {
      "epoch": 38.9449665467765,
      "grad_norm": 10.967318534851074,
      "learning_rate": 1.754586121101958e-05,
      "loss": 1.8713,
      "step": 1007000
    },
    {
      "epoch": 38.948833971458406,
      "grad_norm": 11.953866958618164,
      "learning_rate": 1.7542638357117996e-05,
      "loss": 1.8337,
      "step": 1007100
    },
    {
      "epoch": 38.95270139614031,
      "grad_norm": 12.96084976196289,
      "learning_rate": 1.7539415503216407e-05,
      "loss": 2.032,
      "step": 1007200
    },
    {
      "epoch": 38.95656882082221,
      "grad_norm": 9.878695487976074,
      "learning_rate": 1.753619264931482e-05,
      "loss": 1.8079,
      "step": 1007300
    },
    {
      "epoch": 38.96043624550412,
      "grad_norm": 14.920331001281738,
      "learning_rate": 1.7532969795413233e-05,
      "loss": 1.8161,
      "step": 1007400
    },
    {
      "epoch": 38.96430367018602,
      "grad_norm": 10.028852462768555,
      "learning_rate": 1.7529746941511648e-05,
      "loss": 1.8373,
      "step": 1007500
    },
    {
      "epoch": 38.96817109486793,
      "grad_norm": 14.79288101196289,
      "learning_rate": 1.752652408761006e-05,
      "loss": 1.8305,
      "step": 1007600
    },
    {
      "epoch": 38.972038519549834,
      "grad_norm": 10.061753273010254,
      "learning_rate": 1.7523301233708474e-05,
      "loss": 1.8352,
      "step": 1007700
    },
    {
      "epoch": 38.975905944231734,
      "grad_norm": 12.1051664352417,
      "learning_rate": 1.752007837980689e-05,
      "loss": 1.7465,
      "step": 1007800
    },
    {
      "epoch": 38.97977336891364,
      "grad_norm": 14.126166343688965,
      "learning_rate": 1.75168555259053e-05,
      "loss": 1.7914,
      "step": 1007900
    },
    {
      "epoch": 38.98364079359555,
      "grad_norm": 10.833666801452637,
      "learning_rate": 1.7513632672003715e-05,
      "loss": 1.8811,
      "step": 1008000
    },
    {
      "epoch": 38.98750821827745,
      "grad_norm": 14.310903549194336,
      "learning_rate": 1.751040981810213e-05,
      "loss": 1.8455,
      "step": 1008100
    },
    {
      "epoch": 38.991375642959355,
      "grad_norm": 10.6926908493042,
      "learning_rate": 1.750718696420054e-05,
      "loss": 1.8651,
      "step": 1008200
    },
    {
      "epoch": 38.995243067641255,
      "grad_norm": 13.217318534851074,
      "learning_rate": 1.7503964110298955e-05,
      "loss": 1.8368,
      "step": 1008300
    },
    {
      "epoch": 38.99911049232316,
      "grad_norm": 12.79220962524414,
      "learning_rate": 1.7500741256397367e-05,
      "loss": 1.7807,
      "step": 1008400
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.770742416381836,
      "eval_runtime": 3.0368,
      "eval_samples_per_second": 448.164,
      "eval_steps_per_second": 448.164,
      "step": 1008423
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.6252293586730957,
      "eval_runtime": 56.2299,
      "eval_samples_per_second": 459.844,
      "eval_steps_per_second": 459.844,
      "step": 1008423
    },
    {
      "epoch": 39.00297791700507,
      "grad_norm": 11.809141159057617,
      "learning_rate": 1.749751840249578e-05,
      "loss": 1.8216,
      "step": 1008500
    },
    {
      "epoch": 39.00684534168697,
      "grad_norm": 11.5353364944458,
      "learning_rate": 1.7494295548594193e-05,
      "loss": 1.8542,
      "step": 1008600
    },
    {
      "epoch": 39.010712766368876,
      "grad_norm": 9.274283409118652,
      "learning_rate": 1.7491072694692607e-05,
      "loss": 1.8484,
      "step": 1008700
    },
    {
      "epoch": 39.01458019105078,
      "grad_norm": 10.221693992614746,
      "learning_rate": 1.748784984079102e-05,
      "loss": 1.7812,
      "step": 1008800
    },
    {
      "epoch": 39.01844761573268,
      "grad_norm": 13.593476295471191,
      "learning_rate": 1.7484626986889433e-05,
      "loss": 1.8028,
      "step": 1008900
    },
    {
      "epoch": 39.02231504041459,
      "grad_norm": 12.560260772705078,
      "learning_rate": 1.7481404132987845e-05,
      "loss": 1.8213,
      "step": 1009000
    },
    {
      "epoch": 39.02618246509649,
      "grad_norm": 11.82193660736084,
      "learning_rate": 1.747818127908626e-05,
      "loss": 1.8118,
      "step": 1009100
    },
    {
      "epoch": 39.0300498897784,
      "grad_norm": 12.55517292022705,
      "learning_rate": 1.747495842518467e-05,
      "loss": 1.8655,
      "step": 1009200
    },
    {
      "epoch": 39.033917314460304,
      "grad_norm": 12.370540618896484,
      "learning_rate": 1.7471735571283082e-05,
      "loss": 1.7393,
      "step": 1009300
    },
    {
      "epoch": 39.037784739142204,
      "grad_norm": 15.13931655883789,
      "learning_rate": 1.7468512717381497e-05,
      "loss": 1.7563,
      "step": 1009400
    },
    {
      "epoch": 39.04165216382411,
      "grad_norm": 10.124972343444824,
      "learning_rate": 1.7465289863479908e-05,
      "loss": 1.879,
      "step": 1009500
    },
    {
      "epoch": 39.04551958850601,
      "grad_norm": 13.881839752197266,
      "learning_rate": 1.7462067009578323e-05,
      "loss": 1.8453,
      "step": 1009600
    },
    {
      "epoch": 39.04938701318792,
      "grad_norm": 9.369781494140625,
      "learning_rate": 1.7458844155676734e-05,
      "loss": 1.8531,
      "step": 1009700
    },
    {
      "epoch": 39.053254437869825,
      "grad_norm": 14.290153503417969,
      "learning_rate": 1.745562130177515e-05,
      "loss": 1.7685,
      "step": 1009800
    },
    {
      "epoch": 39.057121862551725,
      "grad_norm": 14.073616981506348,
      "learning_rate": 1.745239844787356e-05,
      "loss": 1.7727,
      "step": 1009900
    },
    {
      "epoch": 39.06098928723363,
      "grad_norm": 10.029623985290527,
      "learning_rate": 1.7449175593971975e-05,
      "loss": 1.8782,
      "step": 1010000
    },
    {
      "epoch": 39.06485671191554,
      "grad_norm": 10.087718963623047,
      "learning_rate": 1.7445952740070386e-05,
      "loss": 1.7703,
      "step": 1010100
    },
    {
      "epoch": 39.06872413659744,
      "grad_norm": 12.315841674804688,
      "learning_rate": 1.74427298861688e-05,
      "loss": 1.9339,
      "step": 1010200
    },
    {
      "epoch": 39.072591561279346,
      "grad_norm": 12.597589492797852,
      "learning_rate": 1.7439507032267212e-05,
      "loss": 1.8088,
      "step": 1010300
    },
    {
      "epoch": 39.076458985961246,
      "grad_norm": 13.234445571899414,
      "learning_rate": 1.7436284178365627e-05,
      "loss": 1.8452,
      "step": 1010400
    },
    {
      "epoch": 39.08032641064315,
      "grad_norm": 15.128693580627441,
      "learning_rate": 1.743306132446404e-05,
      "loss": 1.7367,
      "step": 1010500
    },
    {
      "epoch": 39.08419383532506,
      "grad_norm": 10.194502830505371,
      "learning_rate": 1.7429838470562453e-05,
      "loss": 1.8428,
      "step": 1010600
    },
    {
      "epoch": 39.08806126000696,
      "grad_norm": 20.789762496948242,
      "learning_rate": 1.7426615616660864e-05,
      "loss": 1.9285,
      "step": 1010700
    },
    {
      "epoch": 39.09192868468887,
      "grad_norm": 11.634105682373047,
      "learning_rate": 1.742339276275928e-05,
      "loss": 1.831,
      "step": 1010800
    },
    {
      "epoch": 39.09579610937077,
      "grad_norm": 14.599827766418457,
      "learning_rate": 1.742016990885769e-05,
      "loss": 1.834,
      "step": 1010900
    },
    {
      "epoch": 39.099663534052674,
      "grad_norm": 8.063465118408203,
      "learning_rate": 1.7416947054956105e-05,
      "loss": 1.8043,
      "step": 1011000
    },
    {
      "epoch": 39.10353095873458,
      "grad_norm": 15.545101165771484,
      "learning_rate": 1.7413724201054517e-05,
      "loss": 1.8838,
      "step": 1011100
    },
    {
      "epoch": 39.10739838341648,
      "grad_norm": 12.455684661865234,
      "learning_rate": 1.741050134715293e-05,
      "loss": 1.8329,
      "step": 1011200
    },
    {
      "epoch": 39.11126580809839,
      "grad_norm": 12.894110679626465,
      "learning_rate": 1.7407278493251346e-05,
      "loss": 1.8648,
      "step": 1011300
    },
    {
      "epoch": 39.115133232780295,
      "grad_norm": 10.25236701965332,
      "learning_rate": 1.7404055639349757e-05,
      "loss": 1.8517,
      "step": 1011400
    },
    {
      "epoch": 39.119000657462195,
      "grad_norm": 11.51125717163086,
      "learning_rate": 1.7400832785448172e-05,
      "loss": 1.8546,
      "step": 1011500
    },
    {
      "epoch": 39.1228680821441,
      "grad_norm": 12.531210899353027,
      "learning_rate": 1.7397609931546587e-05,
      "loss": 1.8207,
      "step": 1011600
    },
    {
      "epoch": 39.126735506826,
      "grad_norm": 12.424697875976562,
      "learning_rate": 1.7394387077644998e-05,
      "loss": 1.8077,
      "step": 1011700
    },
    {
      "epoch": 39.13060293150791,
      "grad_norm": 10.541593551635742,
      "learning_rate": 1.7391164223743413e-05,
      "loss": 1.8902,
      "step": 1011800
    },
    {
      "epoch": 39.134470356189816,
      "grad_norm": 15.017229080200195,
      "learning_rate": 1.7387941369841824e-05,
      "loss": 1.7967,
      "step": 1011900
    },
    {
      "epoch": 39.138337780871716,
      "grad_norm": 11.167807579040527,
      "learning_rate": 1.738471851594024e-05,
      "loss": 1.8626,
      "step": 1012000
    },
    {
      "epoch": 39.14220520555362,
      "grad_norm": 17.232011795043945,
      "learning_rate": 1.738149566203865e-05,
      "loss": 1.7978,
      "step": 1012100
    },
    {
      "epoch": 39.14607263023552,
      "grad_norm": 13.16419792175293,
      "learning_rate": 1.737827280813706e-05,
      "loss": 1.8852,
      "step": 1012200
    },
    {
      "epoch": 39.14994005491743,
      "grad_norm": 10.231958389282227,
      "learning_rate": 1.7375049954235476e-05,
      "loss": 1.7324,
      "step": 1012300
    },
    {
      "epoch": 39.15380747959934,
      "grad_norm": 11.431023597717285,
      "learning_rate": 1.7371827100333888e-05,
      "loss": 1.8205,
      "step": 1012400
    },
    {
      "epoch": 39.15767490428124,
      "grad_norm": 13.305460929870605,
      "learning_rate": 1.7368604246432302e-05,
      "loss": 1.852,
      "step": 1012500
    },
    {
      "epoch": 39.161542328963144,
      "grad_norm": 10.872873306274414,
      "learning_rate": 1.7365381392530714e-05,
      "loss": 1.865,
      "step": 1012600
    },
    {
      "epoch": 39.16540975364505,
      "grad_norm": 12.631688117980957,
      "learning_rate": 1.736215853862913e-05,
      "loss": 1.83,
      "step": 1012700
    },
    {
      "epoch": 39.16927717832695,
      "grad_norm": 10.170755386352539,
      "learning_rate": 1.735893568472754e-05,
      "loss": 1.7749,
      "step": 1012800
    },
    {
      "epoch": 39.17314460300886,
      "grad_norm": 13.935343742370605,
      "learning_rate": 1.7355712830825954e-05,
      "loss": 1.7907,
      "step": 1012900
    },
    {
      "epoch": 39.17701202769076,
      "grad_norm": 9.574417114257812,
      "learning_rate": 1.7352489976924366e-05,
      "loss": 1.7804,
      "step": 1013000
    },
    {
      "epoch": 39.180879452372665,
      "grad_norm": 13.414274215698242,
      "learning_rate": 1.734926712302278e-05,
      "loss": 1.9109,
      "step": 1013100
    },
    {
      "epoch": 39.18474687705457,
      "grad_norm": 10.579070091247559,
      "learning_rate": 1.7346044269121192e-05,
      "loss": 1.9348,
      "step": 1013200
    },
    {
      "epoch": 39.18861430173647,
      "grad_norm": 15.001849174499512,
      "learning_rate": 1.7342821415219606e-05,
      "loss": 1.8868,
      "step": 1013300
    },
    {
      "epoch": 39.19248172641838,
      "grad_norm": 9.599594116210938,
      "learning_rate": 1.7339598561318018e-05,
      "loss": 1.8256,
      "step": 1013400
    },
    {
      "epoch": 39.196349151100286,
      "grad_norm": 9.902169227600098,
      "learning_rate": 1.7336375707416433e-05,
      "loss": 1.9102,
      "step": 1013500
    },
    {
      "epoch": 39.200216575782186,
      "grad_norm": 11.838056564331055,
      "learning_rate": 1.7333152853514844e-05,
      "loss": 1.6884,
      "step": 1013600
    },
    {
      "epoch": 39.20408400046409,
      "grad_norm": 11.083918571472168,
      "learning_rate": 1.732992999961326e-05,
      "loss": 1.7307,
      "step": 1013700
    },
    {
      "epoch": 39.20795142514599,
      "grad_norm": 11.974430084228516,
      "learning_rate": 1.732670714571167e-05,
      "loss": 1.7597,
      "step": 1013800
    },
    {
      "epoch": 39.2118188498279,
      "grad_norm": 12.774730682373047,
      "learning_rate": 1.7323484291810085e-05,
      "loss": 1.7787,
      "step": 1013900
    },
    {
      "epoch": 39.21568627450981,
      "grad_norm": 13.173362731933594,
      "learning_rate": 1.7320261437908496e-05,
      "loss": 1.8674,
      "step": 1014000
    },
    {
      "epoch": 39.21955369919171,
      "grad_norm": 13.059311866760254,
      "learning_rate": 1.731703858400691e-05,
      "loss": 1.8378,
      "step": 1014100
    },
    {
      "epoch": 39.223421123873614,
      "grad_norm": 9.73487663269043,
      "learning_rate": 1.7313815730105322e-05,
      "loss": 1.7707,
      "step": 1014200
    },
    {
      "epoch": 39.227288548555514,
      "grad_norm": 10.411694526672363,
      "learning_rate": 1.7310592876203737e-05,
      "loss": 1.8472,
      "step": 1014300
    },
    {
      "epoch": 39.23115597323742,
      "grad_norm": 13.440173149108887,
      "learning_rate": 1.7307370022302148e-05,
      "loss": 1.807,
      "step": 1014400
    },
    {
      "epoch": 39.23502339791933,
      "grad_norm": 10.126176834106445,
      "learning_rate": 1.7304147168400563e-05,
      "loss": 1.8218,
      "step": 1014500
    },
    {
      "epoch": 39.23889082260123,
      "grad_norm": 11.497838020324707,
      "learning_rate": 1.7300924314498974e-05,
      "loss": 1.8634,
      "step": 1014600
    },
    {
      "epoch": 39.242758247283135,
      "grad_norm": 10.016530990600586,
      "learning_rate": 1.729770146059739e-05,
      "loss": 1.8788,
      "step": 1014700
    },
    {
      "epoch": 39.24662567196504,
      "grad_norm": 13.917816162109375,
      "learning_rate": 1.7294478606695804e-05,
      "loss": 1.8211,
      "step": 1014800
    },
    {
      "epoch": 39.25049309664694,
      "grad_norm": 13.253390312194824,
      "learning_rate": 1.7291255752794215e-05,
      "loss": 1.8494,
      "step": 1014900
    },
    {
      "epoch": 39.25436052132885,
      "grad_norm": 12.169779777526855,
      "learning_rate": 1.728803289889263e-05,
      "loss": 1.8815,
      "step": 1015000
    },
    {
      "epoch": 39.25822794601075,
      "grad_norm": 15.410896301269531,
      "learning_rate": 1.728481004499104e-05,
      "loss": 1.7999,
      "step": 1015100
    },
    {
      "epoch": 39.262095370692656,
      "grad_norm": 13.79997730255127,
      "learning_rate": 1.7281587191089456e-05,
      "loss": 1.7658,
      "step": 1015200
    },
    {
      "epoch": 39.26596279537456,
      "grad_norm": 12.849555015563965,
      "learning_rate": 1.7278364337187867e-05,
      "loss": 1.9107,
      "step": 1015300
    },
    {
      "epoch": 39.26983022005646,
      "grad_norm": 11.168460845947266,
      "learning_rate": 1.7275141483286282e-05,
      "loss": 1.8544,
      "step": 1015400
    },
    {
      "epoch": 39.27369764473837,
      "grad_norm": 8.490448951721191,
      "learning_rate": 1.7271918629384693e-05,
      "loss": 1.7776,
      "step": 1015500
    },
    {
      "epoch": 39.27756506942027,
      "grad_norm": 10.162562370300293,
      "learning_rate": 1.7268695775483108e-05,
      "loss": 1.8025,
      "step": 1015600
    },
    {
      "epoch": 39.28143249410218,
      "grad_norm": 8.239524841308594,
      "learning_rate": 1.726547292158152e-05,
      "loss": 1.8752,
      "step": 1015700
    },
    {
      "epoch": 39.285299918784084,
      "grad_norm": 11.105487823486328,
      "learning_rate": 1.7262250067679934e-05,
      "loss": 1.8099,
      "step": 1015800
    },
    {
      "epoch": 39.289167343465984,
      "grad_norm": 14.602081298828125,
      "learning_rate": 1.7259027213778345e-05,
      "loss": 1.8802,
      "step": 1015900
    },
    {
      "epoch": 39.29303476814789,
      "grad_norm": 14.047086715698242,
      "learning_rate": 1.725580435987676e-05,
      "loss": 1.7953,
      "step": 1016000
    },
    {
      "epoch": 39.2969021928298,
      "grad_norm": 11.686782836914062,
      "learning_rate": 1.725258150597517e-05,
      "loss": 1.7553,
      "step": 1016100
    },
    {
      "epoch": 39.3007696175117,
      "grad_norm": 19.41860580444336,
      "learning_rate": 1.7249358652073586e-05,
      "loss": 1.823,
      "step": 1016200
    },
    {
      "epoch": 39.304637042193605,
      "grad_norm": 13.083282470703125,
      "learning_rate": 1.7246135798171997e-05,
      "loss": 1.8474,
      "step": 1016300
    },
    {
      "epoch": 39.308504466875505,
      "grad_norm": 13.214925765991211,
      "learning_rate": 1.7242912944270412e-05,
      "loss": 1.8012,
      "step": 1016400
    },
    {
      "epoch": 39.31237189155741,
      "grad_norm": 10.93375301361084,
      "learning_rate": 1.7239690090368823e-05,
      "loss": 1.787,
      "step": 1016500
    },
    {
      "epoch": 39.31623931623932,
      "grad_norm": 8.600738525390625,
      "learning_rate": 1.7236467236467238e-05,
      "loss": 1.7592,
      "step": 1016600
    },
    {
      "epoch": 39.32010674092122,
      "grad_norm": 13.873720169067383,
      "learning_rate": 1.723324438256565e-05,
      "loss": 1.7994,
      "step": 1016700
    },
    {
      "epoch": 39.323974165603126,
      "grad_norm": 9.696023941040039,
      "learning_rate": 1.7230021528664064e-05,
      "loss": 1.8827,
      "step": 1016800
    },
    {
      "epoch": 39.32784159028503,
      "grad_norm": 15.290750503540039,
      "learning_rate": 1.7226798674762475e-05,
      "loss": 1.8931,
      "step": 1016900
    },
    {
      "epoch": 39.33170901496693,
      "grad_norm": 14.261080741882324,
      "learning_rate": 1.722357582086089e-05,
      "loss": 1.8364,
      "step": 1017000
    },
    {
      "epoch": 39.33557643964884,
      "grad_norm": 8.50230884552002,
      "learning_rate": 1.72203529669593e-05,
      "loss": 1.8549,
      "step": 1017100
    },
    {
      "epoch": 39.33944386433074,
      "grad_norm": 12.057417869567871,
      "learning_rate": 1.7217130113057716e-05,
      "loss": 1.9254,
      "step": 1017200
    },
    {
      "epoch": 39.34331128901265,
      "grad_norm": 13.840121269226074,
      "learning_rate": 1.7213907259156127e-05,
      "loss": 1.8522,
      "step": 1017300
    },
    {
      "epoch": 39.347178713694554,
      "grad_norm": 12.93167495727539,
      "learning_rate": 1.7210684405254542e-05,
      "loss": 1.8024,
      "step": 1017400
    },
    {
      "epoch": 39.351046138376454,
      "grad_norm": 12.554560661315918,
      "learning_rate": 1.7207461551352954e-05,
      "loss": 1.8464,
      "step": 1017500
    },
    {
      "epoch": 39.35491356305836,
      "grad_norm": 12.627185821533203,
      "learning_rate": 1.7204238697451368e-05,
      "loss": 1.8366,
      "step": 1017600
    },
    {
      "epoch": 39.35878098774026,
      "grad_norm": 11.370872497558594,
      "learning_rate": 1.720101584354978e-05,
      "loss": 1.8993,
      "step": 1017700
    },
    {
      "epoch": 39.36264841242217,
      "grad_norm": 10.611455917358398,
      "learning_rate": 1.7197792989648194e-05,
      "loss": 1.8165,
      "step": 1017800
    },
    {
      "epoch": 39.366515837104075,
      "grad_norm": 10.665543556213379,
      "learning_rate": 1.7194570135746606e-05,
      "loss": 1.7532,
      "step": 1017900
    },
    {
      "epoch": 39.370383261785975,
      "grad_norm": 10.174744606018066,
      "learning_rate": 1.719134728184502e-05,
      "loss": 1.8692,
      "step": 1018000
    },
    {
      "epoch": 39.37425068646788,
      "grad_norm": 14.061270713806152,
      "learning_rate": 1.7188124427943435e-05,
      "loss": 1.8685,
      "step": 1018100
    },
    {
      "epoch": 39.37811811114979,
      "grad_norm": 8.428598403930664,
      "learning_rate": 1.7184901574041846e-05,
      "loss": 1.8252,
      "step": 1018200
    },
    {
      "epoch": 39.38198553583169,
      "grad_norm": 10.989689826965332,
      "learning_rate": 1.718167872014026e-05,
      "loss": 1.7206,
      "step": 1018300
    },
    {
      "epoch": 39.385852960513596,
      "grad_norm": 11.58115291595459,
      "learning_rate": 1.7178455866238672e-05,
      "loss": 1.8341,
      "step": 1018400
    },
    {
      "epoch": 39.389720385195496,
      "grad_norm": 12.715641975402832,
      "learning_rate": 1.7175233012337087e-05,
      "loss": 1.8218,
      "step": 1018500
    },
    {
      "epoch": 39.3935878098774,
      "grad_norm": 11.384931564331055,
      "learning_rate": 1.71720101584355e-05,
      "loss": 1.8539,
      "step": 1018600
    },
    {
      "epoch": 39.39745523455931,
      "grad_norm": 16.49593162536621,
      "learning_rate": 1.7168787304533913e-05,
      "loss": 1.8326,
      "step": 1018700
    },
    {
      "epoch": 39.40132265924121,
      "grad_norm": 13.830982208251953,
      "learning_rate": 1.7165564450632325e-05,
      "loss": 1.8347,
      "step": 1018800
    },
    {
      "epoch": 39.40519008392312,
      "grad_norm": 10.982087135314941,
      "learning_rate": 1.716234159673074e-05,
      "loss": 1.7824,
      "step": 1018900
    },
    {
      "epoch": 39.40905750860502,
      "grad_norm": 11.32026481628418,
      "learning_rate": 1.715911874282915e-05,
      "loss": 1.8375,
      "step": 1019000
    },
    {
      "epoch": 39.412924933286924,
      "grad_norm": 12.688800811767578,
      "learning_rate": 1.7155895888927565e-05,
      "loss": 1.8653,
      "step": 1019100
    },
    {
      "epoch": 39.41679235796883,
      "grad_norm": 11.538596153259277,
      "learning_rate": 1.7152673035025977e-05,
      "loss": 1.7966,
      "step": 1019200
    },
    {
      "epoch": 39.42065978265073,
      "grad_norm": 11.269723892211914,
      "learning_rate": 1.714945018112439e-05,
      "loss": 1.8447,
      "step": 1019300
    },
    {
      "epoch": 39.42452720733264,
      "grad_norm": 11.447402954101562,
      "learning_rate": 1.7146227327222803e-05,
      "loss": 1.8127,
      "step": 1019400
    },
    {
      "epoch": 39.428394632014545,
      "grad_norm": 9.99261474609375,
      "learning_rate": 1.7143004473321217e-05,
      "loss": 1.8814,
      "step": 1019500
    },
    {
      "epoch": 39.432262056696445,
      "grad_norm": 12.120843887329102,
      "learning_rate": 1.713978161941963e-05,
      "loss": 1.8755,
      "step": 1019600
    },
    {
      "epoch": 39.43612948137835,
      "grad_norm": 14.887320518493652,
      "learning_rate": 1.7136558765518043e-05,
      "loss": 1.8242,
      "step": 1019700
    },
    {
      "epoch": 39.43999690606025,
      "grad_norm": 9.585172653198242,
      "learning_rate": 1.7133335911616455e-05,
      "loss": 1.8032,
      "step": 1019800
    },
    {
      "epoch": 39.44386433074216,
      "grad_norm": 10.021198272705078,
      "learning_rate": 1.713011305771487e-05,
      "loss": 1.8997,
      "step": 1019900
    },
    {
      "epoch": 39.447731755424066,
      "grad_norm": 7.965654373168945,
      "learning_rate": 1.712689020381328e-05,
      "loss": 1.8122,
      "step": 1020000
    },
    {
      "epoch": 39.451599180105966,
      "grad_norm": 11.317009925842285,
      "learning_rate": 1.7123667349911696e-05,
      "loss": 1.8298,
      "step": 1020100
    },
    {
      "epoch": 39.45546660478787,
      "grad_norm": 12.22018814086914,
      "learning_rate": 1.7120444496010107e-05,
      "loss": 1.7969,
      "step": 1020200
    },
    {
      "epoch": 39.45933402946977,
      "grad_norm": 7.822749614715576,
      "learning_rate": 1.711722164210852e-05,
      "loss": 1.7924,
      "step": 1020300
    },
    {
      "epoch": 39.46320145415168,
      "grad_norm": 11.0156888961792,
      "learning_rate": 1.7113998788206933e-05,
      "loss": 1.8456,
      "step": 1020400
    },
    {
      "epoch": 39.46706887883359,
      "grad_norm": 13.377754211425781,
      "learning_rate": 1.7110775934305348e-05,
      "loss": 1.7906,
      "step": 1020500
    },
    {
      "epoch": 39.47093630351549,
      "grad_norm": 11.209388732910156,
      "learning_rate": 1.710755308040376e-05,
      "loss": 1.827,
      "step": 1020600
    },
    {
      "epoch": 39.474803728197394,
      "grad_norm": 15.503504753112793,
      "learning_rate": 1.7104330226502174e-05,
      "loss": 1.761,
      "step": 1020700
    },
    {
      "epoch": 39.4786711528793,
      "grad_norm": 10.478784561157227,
      "learning_rate": 1.7101107372600585e-05,
      "loss": 1.919,
      "step": 1020800
    },
    {
      "epoch": 39.4825385775612,
      "grad_norm": 13.722535133361816,
      "learning_rate": 1.7097884518699e-05,
      "loss": 1.7577,
      "step": 1020900
    },
    {
      "epoch": 39.48640600224311,
      "grad_norm": 11.054102897644043,
      "learning_rate": 1.709466166479741e-05,
      "loss": 1.8621,
      "step": 1021000
    },
    {
      "epoch": 39.49027342692501,
      "grad_norm": 9.036375045776367,
      "learning_rate": 1.7091438810895822e-05,
      "loss": 1.8355,
      "step": 1021100
    },
    {
      "epoch": 39.494140851606915,
      "grad_norm": 10.006071090698242,
      "learning_rate": 1.7088215956994237e-05,
      "loss": 1.9142,
      "step": 1021200
    },
    {
      "epoch": 39.49800827628882,
      "grad_norm": 12.085882186889648,
      "learning_rate": 1.7084993103092652e-05,
      "loss": 1.7714,
      "step": 1021300
    },
    {
      "epoch": 39.50187570097072,
      "grad_norm": 12.313993453979492,
      "learning_rate": 1.7081770249191063e-05,
      "loss": 1.8779,
      "step": 1021400
    },
    {
      "epoch": 39.50574312565263,
      "grad_norm": 13.893298149108887,
      "learning_rate": 1.7078547395289478e-05,
      "loss": 1.8192,
      "step": 1021500
    },
    {
      "epoch": 39.509610550334536,
      "grad_norm": 9.940398216247559,
      "learning_rate": 1.7075324541387893e-05,
      "loss": 1.8502,
      "step": 1021600
    },
    {
      "epoch": 39.513477975016436,
      "grad_norm": 14.377357482910156,
      "learning_rate": 1.7072101687486304e-05,
      "loss": 1.805,
      "step": 1021700
    },
    {
      "epoch": 39.51734539969834,
      "grad_norm": 10.679886817932129,
      "learning_rate": 1.706887883358472e-05,
      "loss": 1.8553,
      "step": 1021800
    },
    {
      "epoch": 39.52121282438024,
      "grad_norm": 9.520289421081543,
      "learning_rate": 1.706565597968313e-05,
      "loss": 1.9002,
      "step": 1021900
    },
    {
      "epoch": 39.52508024906215,
      "grad_norm": 12.35215950012207,
      "learning_rate": 1.7062433125781545e-05,
      "loss": 1.8272,
      "step": 1022000
    },
    {
      "epoch": 39.52894767374406,
      "grad_norm": 12.84489631652832,
      "learning_rate": 1.7059210271879956e-05,
      "loss": 1.8611,
      "step": 1022100
    },
    {
      "epoch": 39.53281509842596,
      "grad_norm": 8.456558227539062,
      "learning_rate": 1.705598741797837e-05,
      "loss": 1.8884,
      "step": 1022200
    },
    {
      "epoch": 39.536682523107864,
      "grad_norm": 6.34286642074585,
      "learning_rate": 1.7052764564076782e-05,
      "loss": 1.7297,
      "step": 1022300
    },
    {
      "epoch": 39.54054994778976,
      "grad_norm": 13.358879089355469,
      "learning_rate": 1.7049541710175197e-05,
      "loss": 1.9851,
      "step": 1022400
    },
    {
      "epoch": 39.54441737247167,
      "grad_norm": 10.791488647460938,
      "learning_rate": 1.7046318856273608e-05,
      "loss": 1.8655,
      "step": 1022500
    },
    {
      "epoch": 39.54828479715358,
      "grad_norm": 8.533698081970215,
      "learning_rate": 1.7043096002372023e-05,
      "loss": 1.8259,
      "step": 1022600
    },
    {
      "epoch": 39.55215222183548,
      "grad_norm": 11.180244445800781,
      "learning_rate": 1.7039873148470434e-05,
      "loss": 1.8773,
      "step": 1022700
    },
    {
      "epoch": 39.556019646517385,
      "grad_norm": 11.14700698852539,
      "learning_rate": 1.703665029456885e-05,
      "loss": 1.8138,
      "step": 1022800
    },
    {
      "epoch": 39.55988707119929,
      "grad_norm": 17.39482879638672,
      "learning_rate": 1.703342744066726e-05,
      "loss": 1.8698,
      "step": 1022900
    },
    {
      "epoch": 39.56375449588119,
      "grad_norm": 11.357888221740723,
      "learning_rate": 1.7030204586765675e-05,
      "loss": 1.8413,
      "step": 1023000
    },
    {
      "epoch": 39.5676219205631,
      "grad_norm": 10.237560272216797,
      "learning_rate": 1.7026981732864086e-05,
      "loss": 1.8052,
      "step": 1023100
    },
    {
      "epoch": 39.571489345245,
      "grad_norm": 9.971073150634766,
      "learning_rate": 1.70237588789625e-05,
      "loss": 1.8188,
      "step": 1023200
    },
    {
      "epoch": 39.575356769926906,
      "grad_norm": 10.634799003601074,
      "learning_rate": 1.7020536025060912e-05,
      "loss": 1.7801,
      "step": 1023300
    },
    {
      "epoch": 39.57922419460881,
      "grad_norm": 10.204913139343262,
      "learning_rate": 1.7017313171159327e-05,
      "loss": 1.9041,
      "step": 1023400
    },
    {
      "epoch": 39.58309161929071,
      "grad_norm": 12.274218559265137,
      "learning_rate": 1.701409031725774e-05,
      "loss": 1.8965,
      "step": 1023500
    },
    {
      "epoch": 39.58695904397262,
      "grad_norm": 11.084622383117676,
      "learning_rate": 1.7010867463356153e-05,
      "loss": 1.8425,
      "step": 1023600
    },
    {
      "epoch": 39.59082646865452,
      "grad_norm": 12.911836624145508,
      "learning_rate": 1.7007644609454564e-05,
      "loss": 1.7913,
      "step": 1023700
    },
    {
      "epoch": 39.59469389333643,
      "grad_norm": 10.172377586364746,
      "learning_rate": 1.700442175555298e-05,
      "loss": 1.8732,
      "step": 1023800
    },
    {
      "epoch": 39.598561318018334,
      "grad_norm": 12.31493091583252,
      "learning_rate": 1.700119890165139e-05,
      "loss": 1.7929,
      "step": 1023900
    },
    {
      "epoch": 39.60242874270023,
      "grad_norm": 13.320816993713379,
      "learning_rate": 1.6997976047749802e-05,
      "loss": 1.9022,
      "step": 1024000
    },
    {
      "epoch": 39.60629616738214,
      "grad_norm": 11.345768928527832,
      "learning_rate": 1.6994753193848217e-05,
      "loss": 1.9054,
      "step": 1024100
    },
    {
      "epoch": 39.61016359206405,
      "grad_norm": 10.880754470825195,
      "learning_rate": 1.6991530339946628e-05,
      "loss": 1.7856,
      "step": 1024200
    },
    {
      "epoch": 39.61403101674595,
      "grad_norm": 10.06574535369873,
      "learning_rate": 1.6988307486045043e-05,
      "loss": 1.7607,
      "step": 1024300
    },
    {
      "epoch": 39.617898441427855,
      "grad_norm": 13.379603385925293,
      "learning_rate": 1.6985084632143454e-05,
      "loss": 1.9373,
      "step": 1024400
    },
    {
      "epoch": 39.621765866109754,
      "grad_norm": 9.630716323852539,
      "learning_rate": 1.698186177824187e-05,
      "loss": 1.8276,
      "step": 1024500
    },
    {
      "epoch": 39.62563329079166,
      "grad_norm": 13.782055854797363,
      "learning_rate": 1.697863892434028e-05,
      "loss": 1.8575,
      "step": 1024600
    },
    {
      "epoch": 39.62950071547357,
      "grad_norm": 13.120613098144531,
      "learning_rate": 1.6975416070438695e-05,
      "loss": 1.7914,
      "step": 1024700
    },
    {
      "epoch": 39.63336814015547,
      "grad_norm": 11.766090393066406,
      "learning_rate": 1.697219321653711e-05,
      "loss": 1.8317,
      "step": 1024800
    },
    {
      "epoch": 39.637235564837376,
      "grad_norm": 12.47054672241211,
      "learning_rate": 1.696897036263552e-05,
      "loss": 1.8098,
      "step": 1024900
    },
    {
      "epoch": 39.64110298951928,
      "grad_norm": 13.264480590820312,
      "learning_rate": 1.6965747508733935e-05,
      "loss": 1.8566,
      "step": 1025000
    },
    {
      "epoch": 39.64497041420118,
      "grad_norm": 12.394330978393555,
      "learning_rate": 1.696252465483235e-05,
      "loss": 1.7709,
      "step": 1025100
    },
    {
      "epoch": 39.64883783888309,
      "grad_norm": 11.187010765075684,
      "learning_rate": 1.695930180093076e-05,
      "loss": 1.7826,
      "step": 1025200
    },
    {
      "epoch": 39.65270526356499,
      "grad_norm": 10.767873764038086,
      "learning_rate": 1.6956078947029176e-05,
      "loss": 1.8308,
      "step": 1025300
    },
    {
      "epoch": 39.6565726882469,
      "grad_norm": 10.930740356445312,
      "learning_rate": 1.6952856093127588e-05,
      "loss": 1.8933,
      "step": 1025400
    },
    {
      "epoch": 39.660440112928804,
      "grad_norm": 11.323969841003418,
      "learning_rate": 1.6949633239226002e-05,
      "loss": 1.7814,
      "step": 1025500
    },
    {
      "epoch": 39.6643075376107,
      "grad_norm": 11.570284843444824,
      "learning_rate": 1.6946410385324414e-05,
      "loss": 1.8858,
      "step": 1025600
    },
    {
      "epoch": 39.66817496229261,
      "grad_norm": 13.18642520904541,
      "learning_rate": 1.6943187531422828e-05,
      "loss": 1.8407,
      "step": 1025700
    },
    {
      "epoch": 39.67204238697451,
      "grad_norm": 11.752264976501465,
      "learning_rate": 1.693996467752124e-05,
      "loss": 1.8366,
      "step": 1025800
    },
    {
      "epoch": 39.67590981165642,
      "grad_norm": 12.031697273254395,
      "learning_rate": 1.6936741823619654e-05,
      "loss": 1.8463,
      "step": 1025900
    },
    {
      "epoch": 39.679777236338325,
      "grad_norm": 11.302399635314941,
      "learning_rate": 1.6933518969718066e-05,
      "loss": 1.7483,
      "step": 1026000
    },
    {
      "epoch": 39.683644661020224,
      "grad_norm": 16.2733097076416,
      "learning_rate": 1.693029611581648e-05,
      "loss": 1.7838,
      "step": 1026100
    },
    {
      "epoch": 39.68751208570213,
      "grad_norm": 11.49974250793457,
      "learning_rate": 1.6927073261914892e-05,
      "loss": 1.8098,
      "step": 1026200
    },
    {
      "epoch": 39.69137951038404,
      "grad_norm": 14.215646743774414,
      "learning_rate": 1.6923850408013306e-05,
      "loss": 1.7997,
      "step": 1026300
    },
    {
      "epoch": 39.69524693506594,
      "grad_norm": 10.35976505279541,
      "learning_rate": 1.6920627554111718e-05,
      "loss": 1.8705,
      "step": 1026400
    },
    {
      "epoch": 39.699114359747846,
      "grad_norm": 12.353737831115723,
      "learning_rate": 1.6917404700210132e-05,
      "loss": 1.7898,
      "step": 1026500
    },
    {
      "epoch": 39.702981784429745,
      "grad_norm": 17.072731018066406,
      "learning_rate": 1.6914181846308544e-05,
      "loss": 1.8411,
      "step": 1026600
    },
    {
      "epoch": 39.70684920911165,
      "grad_norm": 14.496068000793457,
      "learning_rate": 1.691095899240696e-05,
      "loss": 1.8262,
      "step": 1026700
    },
    {
      "epoch": 39.71071663379356,
      "grad_norm": 12.78134822845459,
      "learning_rate": 1.690773613850537e-05,
      "loss": 1.7659,
      "step": 1026800
    },
    {
      "epoch": 39.71458405847546,
      "grad_norm": 11.593693733215332,
      "learning_rate": 1.690451328460378e-05,
      "loss": 1.7901,
      "step": 1026900
    },
    {
      "epoch": 39.71845148315737,
      "grad_norm": 15.828434944152832,
      "learning_rate": 1.6901290430702196e-05,
      "loss": 1.8079,
      "step": 1027000
    },
    {
      "epoch": 39.722318907839266,
      "grad_norm": 9.816744804382324,
      "learning_rate": 1.6898067576800607e-05,
      "loss": 1.8156,
      "step": 1027100
    },
    {
      "epoch": 39.72618633252117,
      "grad_norm": 10.0921049118042,
      "learning_rate": 1.6894844722899022e-05,
      "loss": 1.812,
      "step": 1027200
    },
    {
      "epoch": 39.73005375720308,
      "grad_norm": 16.506893157958984,
      "learning_rate": 1.6891621868997433e-05,
      "loss": 1.8247,
      "step": 1027300
    },
    {
      "epoch": 39.73392118188498,
      "grad_norm": 10.744011878967285,
      "learning_rate": 1.6888399015095848e-05,
      "loss": 1.8602,
      "step": 1027400
    },
    {
      "epoch": 39.73778860656689,
      "grad_norm": 10.71094799041748,
      "learning_rate": 1.688517616119426e-05,
      "loss": 1.8087,
      "step": 1027500
    },
    {
      "epoch": 39.741656031248795,
      "grad_norm": 9.68325138092041,
      "learning_rate": 1.6881953307292674e-05,
      "loss": 1.9095,
      "step": 1027600
    },
    {
      "epoch": 39.745523455930694,
      "grad_norm": 13.435850143432617,
      "learning_rate": 1.6878730453391085e-05,
      "loss": 1.8927,
      "step": 1027700
    },
    {
      "epoch": 39.7493908806126,
      "grad_norm": 10.825223922729492,
      "learning_rate": 1.68755075994895e-05,
      "loss": 1.8378,
      "step": 1027800
    },
    {
      "epoch": 39.7532583052945,
      "grad_norm": 14.685586929321289,
      "learning_rate": 1.687228474558791e-05,
      "loss": 1.7353,
      "step": 1027900
    },
    {
      "epoch": 39.75712572997641,
      "grad_norm": 10.642582893371582,
      "learning_rate": 1.6869061891686326e-05,
      "loss": 1.764,
      "step": 1028000
    },
    {
      "epoch": 39.760993154658316,
      "grad_norm": 15.277199745178223,
      "learning_rate": 1.6865839037784737e-05,
      "loss": 1.9382,
      "step": 1028100
    },
    {
      "epoch": 39.764860579340215,
      "grad_norm": 15.41051959991455,
      "learning_rate": 1.6862616183883152e-05,
      "loss": 1.8287,
      "step": 1028200
    },
    {
      "epoch": 39.76872800402212,
      "grad_norm": 11.896385192871094,
      "learning_rate": 1.6859393329981567e-05,
      "loss": 1.7842,
      "step": 1028300
    },
    {
      "epoch": 39.77259542870402,
      "grad_norm": 8.639153480529785,
      "learning_rate": 1.6856170476079978e-05,
      "loss": 1.8404,
      "step": 1028400
    },
    {
      "epoch": 39.77646285338593,
      "grad_norm": 14.039314270019531,
      "learning_rate": 1.6852947622178393e-05,
      "loss": 1.7767,
      "step": 1028500
    },
    {
      "epoch": 39.78033027806784,
      "grad_norm": 11.871273040771484,
      "learning_rate": 1.6849724768276808e-05,
      "loss": 1.834,
      "step": 1028600
    },
    {
      "epoch": 39.784197702749736,
      "grad_norm": 15.337858200073242,
      "learning_rate": 1.684650191437522e-05,
      "loss": 1.906,
      "step": 1028700
    },
    {
      "epoch": 39.78806512743164,
      "grad_norm": 12.633504867553711,
      "learning_rate": 1.6843279060473634e-05,
      "loss": 1.8062,
      "step": 1028800
    },
    {
      "epoch": 39.79193255211355,
      "grad_norm": 12.405243873596191,
      "learning_rate": 1.6840056206572045e-05,
      "loss": 1.8311,
      "step": 1028900
    },
    {
      "epoch": 39.79579997679545,
      "grad_norm": 11.439083099365234,
      "learning_rate": 1.683683335267046e-05,
      "loss": 1.8437,
      "step": 1029000
    },
    {
      "epoch": 39.79966740147736,
      "grad_norm": 13.436480522155762,
      "learning_rate": 1.683361049876887e-05,
      "loss": 1.8501,
      "step": 1029100
    },
    {
      "epoch": 39.80353482615926,
      "grad_norm": 8.860857963562012,
      "learning_rate": 1.6830387644867286e-05,
      "loss": 1.7371,
      "step": 1029200
    },
    {
      "epoch": 39.807402250841164,
      "grad_norm": 13.723930358886719,
      "learning_rate": 1.6827164790965697e-05,
      "loss": 1.7419,
      "step": 1029300
    },
    {
      "epoch": 39.81126967552307,
      "grad_norm": 13.269304275512695,
      "learning_rate": 1.6823941937064112e-05,
      "loss": 1.8311,
      "step": 1029400
    },
    {
      "epoch": 39.81513710020497,
      "grad_norm": 13.465295791625977,
      "learning_rate": 1.6820719083162523e-05,
      "loss": 1.9157,
      "step": 1029500
    },
    {
      "epoch": 39.81900452488688,
      "grad_norm": 10.476393699645996,
      "learning_rate": 1.6817496229260938e-05,
      "loss": 1.8095,
      "step": 1029600
    },
    {
      "epoch": 39.822871949568786,
      "grad_norm": 11.399299621582031,
      "learning_rate": 1.681427337535935e-05,
      "loss": 1.8734,
      "step": 1029700
    },
    {
      "epoch": 39.826739374250685,
      "grad_norm": 18.4102725982666,
      "learning_rate": 1.681105052145776e-05,
      "loss": 1.8656,
      "step": 1029800
    },
    {
      "epoch": 39.83060679893259,
      "grad_norm": 13.491015434265137,
      "learning_rate": 1.6807827667556175e-05,
      "loss": 1.8631,
      "step": 1029900
    },
    {
      "epoch": 39.83447422361449,
      "grad_norm": 15.057938575744629,
      "learning_rate": 1.6804604813654587e-05,
      "loss": 1.8846,
      "step": 1030000
    },
    {
      "epoch": 39.8383416482964,
      "grad_norm": 12.355334281921387,
      "learning_rate": 1.6801381959753e-05,
      "loss": 1.7793,
      "step": 1030100
    },
    {
      "epoch": 39.84220907297831,
      "grad_norm": 12.304981231689453,
      "learning_rate": 1.6798159105851413e-05,
      "loss": 1.8055,
      "step": 1030200
    },
    {
      "epoch": 39.846076497660206,
      "grad_norm": 9.626062393188477,
      "learning_rate": 1.6794936251949827e-05,
      "loss": 1.8505,
      "step": 1030300
    },
    {
      "epoch": 39.84994392234211,
      "grad_norm": 11.60887336730957,
      "learning_rate": 1.679171339804824e-05,
      "loss": 1.7575,
      "step": 1030400
    },
    {
      "epoch": 39.85381134702401,
      "grad_norm": 13.673688888549805,
      "learning_rate": 1.6788490544146653e-05,
      "loss": 1.8187,
      "step": 1030500
    },
    {
      "epoch": 39.85767877170592,
      "grad_norm": 11.957610130310059,
      "learning_rate": 1.6785267690245065e-05,
      "loss": 1.8884,
      "step": 1030600
    },
    {
      "epoch": 39.86154619638783,
      "grad_norm": 12.28844928741455,
      "learning_rate": 1.678204483634348e-05,
      "loss": 1.7146,
      "step": 1030700
    },
    {
      "epoch": 39.86541362106973,
      "grad_norm": 14.802549362182617,
      "learning_rate": 1.677882198244189e-05,
      "loss": 1.8437,
      "step": 1030800
    },
    {
      "epoch": 39.869281045751634,
      "grad_norm": 12.23228931427002,
      "learning_rate": 1.6775599128540306e-05,
      "loss": 1.7482,
      "step": 1030900
    },
    {
      "epoch": 39.87314847043354,
      "grad_norm": 14.458672523498535,
      "learning_rate": 1.6772376274638717e-05,
      "loss": 1.7929,
      "step": 1031000
    },
    {
      "epoch": 39.87701589511544,
      "grad_norm": 10.35306167602539,
      "learning_rate": 1.676915342073713e-05,
      "loss": 1.8366,
      "step": 1031100
    },
    {
      "epoch": 39.88088331979735,
      "grad_norm": 12.385149002075195,
      "learning_rate": 1.6765930566835543e-05,
      "loss": 1.8099,
      "step": 1031200
    },
    {
      "epoch": 39.88475074447925,
      "grad_norm": 15.071681022644043,
      "learning_rate": 1.6762707712933958e-05,
      "loss": 1.8984,
      "step": 1031300
    },
    {
      "epoch": 39.888618169161155,
      "grad_norm": 11.668295860290527,
      "learning_rate": 1.675948485903237e-05,
      "loss": 1.8055,
      "step": 1031400
    },
    {
      "epoch": 39.89248559384306,
      "grad_norm": 9.204900741577148,
      "learning_rate": 1.6756262005130784e-05,
      "loss": 1.781,
      "step": 1031500
    },
    {
      "epoch": 39.89635301852496,
      "grad_norm": 12.826741218566895,
      "learning_rate": 1.67530391512292e-05,
      "loss": 1.7984,
      "step": 1031600
    },
    {
      "epoch": 39.90022044320687,
      "grad_norm": 9.465814590454102,
      "learning_rate": 1.674981629732761e-05,
      "loss": 1.874,
      "step": 1031700
    },
    {
      "epoch": 39.90408786788877,
      "grad_norm": 11.61977767944336,
      "learning_rate": 1.6746593443426024e-05,
      "loss": 1.8588,
      "step": 1031800
    },
    {
      "epoch": 39.907955292570676,
      "grad_norm": 12.782296180725098,
      "learning_rate": 1.674337058952444e-05,
      "loss": 1.7731,
      "step": 1031900
    },
    {
      "epoch": 39.91182271725258,
      "grad_norm": 15.461495399475098,
      "learning_rate": 1.674014773562285e-05,
      "loss": 1.8218,
      "step": 1032000
    },
    {
      "epoch": 39.91569014193448,
      "grad_norm": 11.524733543395996,
      "learning_rate": 1.6736924881721265e-05,
      "loss": 1.8988,
      "step": 1032100
    },
    {
      "epoch": 39.91955756661639,
      "grad_norm": 13.124373435974121,
      "learning_rate": 1.6733702027819677e-05,
      "loss": 1.9055,
      "step": 1032200
    },
    {
      "epoch": 39.9234249912983,
      "grad_norm": 9.197464942932129,
      "learning_rate": 1.673047917391809e-05,
      "loss": 1.8352,
      "step": 1032300
    },
    {
      "epoch": 39.9272924159802,
      "grad_norm": 12.469029426574707,
      "learning_rate": 1.6727256320016503e-05,
      "loss": 1.879,
      "step": 1032400
    },
    {
      "epoch": 39.931159840662104,
      "grad_norm": 10.761941909790039,
      "learning_rate": 1.6724033466114917e-05,
      "loss": 1.8556,
      "step": 1032500
    },
    {
      "epoch": 39.935027265344004,
      "grad_norm": 15.705167770385742,
      "learning_rate": 1.672081061221333e-05,
      "loss": 1.8231,
      "step": 1032600
    },
    {
      "epoch": 39.93889469002591,
      "grad_norm": 12.137442588806152,
      "learning_rate": 1.6717587758311743e-05,
      "loss": 1.8336,
      "step": 1032700
    },
    {
      "epoch": 39.94276211470782,
      "grad_norm": 9.509214401245117,
      "learning_rate": 1.6714364904410155e-05,
      "loss": 1.7908,
      "step": 1032800
    },
    {
      "epoch": 39.94662953938972,
      "grad_norm": 12.620065689086914,
      "learning_rate": 1.6711142050508566e-05,
      "loss": 1.8098,
      "step": 1032900
    },
    {
      "epoch": 39.950496964071625,
      "grad_norm": 10.366312980651855,
      "learning_rate": 1.670791919660698e-05,
      "loss": 1.8304,
      "step": 1033000
    },
    {
      "epoch": 39.95436438875353,
      "grad_norm": 10.99199390411377,
      "learning_rate": 1.6704696342705392e-05,
      "loss": 1.792,
      "step": 1033100
    },
    {
      "epoch": 39.95823181343543,
      "grad_norm": 10.459616661071777,
      "learning_rate": 1.6701473488803807e-05,
      "loss": 1.7979,
      "step": 1033200
    },
    {
      "epoch": 39.96209923811734,
      "grad_norm": 12.694514274597168,
      "learning_rate": 1.6698250634902218e-05,
      "loss": 1.7942,
      "step": 1033300
    },
    {
      "epoch": 39.96596666279924,
      "grad_norm": 11.190570831298828,
      "learning_rate": 1.6695027781000633e-05,
      "loss": 1.8091,
      "step": 1033400
    },
    {
      "epoch": 39.969834087481146,
      "grad_norm": 11.215867042541504,
      "learning_rate": 1.6691804927099044e-05,
      "loss": 1.8659,
      "step": 1033500
    },
    {
      "epoch": 39.97370151216305,
      "grad_norm": 11.935728073120117,
      "learning_rate": 1.668858207319746e-05,
      "loss": 1.833,
      "step": 1033600
    },
    {
      "epoch": 39.97756893684495,
      "grad_norm": 12.924016952514648,
      "learning_rate": 1.668535921929587e-05,
      "loss": 1.886,
      "step": 1033700
    },
    {
      "epoch": 39.98143636152686,
      "grad_norm": 12.959227561950684,
      "learning_rate": 1.6682136365394285e-05,
      "loss": 1.8678,
      "step": 1033800
    },
    {
      "epoch": 39.98530378620876,
      "grad_norm": 12.52094841003418,
      "learning_rate": 1.6678913511492696e-05,
      "loss": 1.8779,
      "step": 1033900
    },
    {
      "epoch": 39.98917121089067,
      "grad_norm": 13.94274616241455,
      "learning_rate": 1.667569065759111e-05,
      "loss": 1.831,
      "step": 1034000
    },
    {
      "epoch": 39.993038635572574,
      "grad_norm": 17.79264259338379,
      "learning_rate": 1.6672467803689522e-05,
      "loss": 1.8877,
      "step": 1034100
    },
    {
      "epoch": 39.996906060254474,
      "grad_norm": 13.039541244506836,
      "learning_rate": 1.6669244949787937e-05,
      "loss": 1.7843,
      "step": 1034200
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.7679872512817383,
      "eval_runtime": 2.9357,
      "eval_samples_per_second": 463.605,
      "eval_steps_per_second": 463.605,
      "step": 1034280
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.6185840368270874,
      "eval_runtime": 56.0178,
      "eval_samples_per_second": 461.585,
      "eval_steps_per_second": 461.585,
      "step": 1034280
    },
    {
      "epoch": 40.00077348493638,
      "grad_norm": 10.580632209777832,
      "learning_rate": 1.666602209588635e-05,
      "loss": 1.8446,
      "step": 1034300
    },
    {
      "epoch": 40.00464090961829,
      "grad_norm": 10.575061798095703,
      "learning_rate": 1.6662799241984763e-05,
      "loss": 1.7564,
      "step": 1034400
    },
    {
      "epoch": 40.00850833430019,
      "grad_norm": 12.81270694732666,
      "learning_rate": 1.6659576388083174e-05,
      "loss": 1.7445,
      "step": 1034500
    },
    {
      "epoch": 40.012375758982095,
      "grad_norm": 10.554150581359863,
      "learning_rate": 1.665635353418159e-05,
      "loss": 1.769,
      "step": 1034600
    },
    {
      "epoch": 40.016243183663995,
      "grad_norm": 14.404373168945312,
      "learning_rate": 1.665313068028e-05,
      "loss": 1.746,
      "step": 1034700
    },
    {
      "epoch": 40.0201106083459,
      "grad_norm": 12.617664337158203,
      "learning_rate": 1.6649907826378415e-05,
      "loss": 1.8857,
      "step": 1034800
    },
    {
      "epoch": 40.02397803302781,
      "grad_norm": 9.917621612548828,
      "learning_rate": 1.6646684972476827e-05,
      "loss": 1.7906,
      "step": 1034900
    },
    {
      "epoch": 40.02784545770971,
      "grad_norm": 9.550080299377441,
      "learning_rate": 1.664346211857524e-05,
      "loss": 1.9012,
      "step": 1035000
    },
    {
      "epoch": 40.031712882391616,
      "grad_norm": 12.630186080932617,
      "learning_rate": 1.6640239264673656e-05,
      "loss": 1.7887,
      "step": 1035100
    },
    {
      "epoch": 40.035580307073516,
      "grad_norm": 12.173754692077637,
      "learning_rate": 1.6637016410772067e-05,
      "loss": 1.8569,
      "step": 1035200
    },
    {
      "epoch": 40.03944773175542,
      "grad_norm": 11.218448638916016,
      "learning_rate": 1.6633793556870482e-05,
      "loss": 1.8276,
      "step": 1035300
    },
    {
      "epoch": 40.04331515643733,
      "grad_norm": 14.106544494628906,
      "learning_rate": 1.6630570702968897e-05,
      "loss": 1.8057,
      "step": 1035400
    },
    {
      "epoch": 40.04718258111923,
      "grad_norm": 14.96371841430664,
      "learning_rate": 1.6627347849067308e-05,
      "loss": 1.7285,
      "step": 1035500
    },
    {
      "epoch": 40.05105000580114,
      "grad_norm": 11.77333927154541,
      "learning_rate": 1.6624124995165723e-05,
      "loss": 1.8339,
      "step": 1035600
    },
    {
      "epoch": 40.054917430483044,
      "grad_norm": 11.621319770812988,
      "learning_rate": 1.6620902141264134e-05,
      "loss": 1.8471,
      "step": 1035700
    },
    {
      "epoch": 40.058784855164944,
      "grad_norm": 13.127599716186523,
      "learning_rate": 1.6617679287362545e-05,
      "loss": 1.8705,
      "step": 1035800
    },
    {
      "epoch": 40.06265227984685,
      "grad_norm": 11.722404479980469,
      "learning_rate": 1.661445643346096e-05,
      "loss": 1.7609,
      "step": 1035900
    },
    {
      "epoch": 40.06651970452875,
      "grad_norm": 12.743645668029785,
      "learning_rate": 1.661123357955937e-05,
      "loss": 1.7275,
      "step": 1036000
    },
    {
      "epoch": 40.07038712921066,
      "grad_norm": 9.352015495300293,
      "learning_rate": 1.6608010725657786e-05,
      "loss": 1.6994,
      "step": 1036100
    },
    {
      "epoch": 40.074254553892565,
      "grad_norm": 12.369473457336426,
      "learning_rate": 1.6604787871756198e-05,
      "loss": 1.8198,
      "step": 1036200
    },
    {
      "epoch": 40.078121978574465,
      "grad_norm": 10.41003704071045,
      "learning_rate": 1.6601565017854612e-05,
      "loss": 1.8121,
      "step": 1036300
    },
    {
      "epoch": 40.08198940325637,
      "grad_norm": 12.663151741027832,
      "learning_rate": 1.6598342163953024e-05,
      "loss": 1.8429,
      "step": 1036400
    },
    {
      "epoch": 40.08585682793828,
      "grad_norm": 13.306510925292969,
      "learning_rate": 1.6595119310051438e-05,
      "loss": 1.8603,
      "step": 1036500
    },
    {
      "epoch": 40.08972425262018,
      "grad_norm": 15.411409378051758,
      "learning_rate": 1.659189645614985e-05,
      "loss": 1.931,
      "step": 1036600
    },
    {
      "epoch": 40.093591677302086,
      "grad_norm": 10.51401138305664,
      "learning_rate": 1.6588673602248264e-05,
      "loss": 1.7589,
      "step": 1036700
    },
    {
      "epoch": 40.097459101983986,
      "grad_norm": 11.766073226928711,
      "learning_rate": 1.6585450748346676e-05,
      "loss": 1.9115,
      "step": 1036800
    },
    {
      "epoch": 40.10132652666589,
      "grad_norm": 13.209800720214844,
      "learning_rate": 1.658222789444509e-05,
      "loss": 1.8424,
      "step": 1036900
    },
    {
      "epoch": 40.1051939513478,
      "grad_norm": 12.269317626953125,
      "learning_rate": 1.6579005040543502e-05,
      "loss": 1.7424,
      "step": 1037000
    },
    {
      "epoch": 40.1090613760297,
      "grad_norm": 11.861496925354004,
      "learning_rate": 1.6575782186641916e-05,
      "loss": 1.78,
      "step": 1037100
    },
    {
      "epoch": 40.11292880071161,
      "grad_norm": 11.352168083190918,
      "learning_rate": 1.6572559332740328e-05,
      "loss": 1.8051,
      "step": 1037200
    },
    {
      "epoch": 40.11679622539351,
      "grad_norm": 14.92005729675293,
      "learning_rate": 1.6569336478838742e-05,
      "loss": 1.7865,
      "step": 1037300
    },
    {
      "epoch": 40.120663650075414,
      "grad_norm": 12.239371299743652,
      "learning_rate": 1.6566113624937154e-05,
      "loss": 1.7615,
      "step": 1037400
    },
    {
      "epoch": 40.12453107475732,
      "grad_norm": 13.108687400817871,
      "learning_rate": 1.656289077103557e-05,
      "loss": 1.8305,
      "step": 1037500
    },
    {
      "epoch": 40.12839849943922,
      "grad_norm": 11.225000381469727,
      "learning_rate": 1.655966791713398e-05,
      "loss": 1.8243,
      "step": 1037600
    },
    {
      "epoch": 40.13226592412113,
      "grad_norm": 10.805730819702148,
      "learning_rate": 1.6556445063232395e-05,
      "loss": 1.8134,
      "step": 1037700
    },
    {
      "epoch": 40.136133348803035,
      "grad_norm": 10.724902153015137,
      "learning_rate": 1.6553222209330806e-05,
      "loss": 1.8446,
      "step": 1037800
    },
    {
      "epoch": 40.140000773484935,
      "grad_norm": 9.33255386352539,
      "learning_rate": 1.654999935542922e-05,
      "loss": 1.7732,
      "step": 1037900
    },
    {
      "epoch": 40.14386819816684,
      "grad_norm": 10.88182544708252,
      "learning_rate": 1.6546776501527632e-05,
      "loss": 1.7573,
      "step": 1038000
    },
    {
      "epoch": 40.14773562284874,
      "grad_norm": 13.81501293182373,
      "learning_rate": 1.6543553647626047e-05,
      "loss": 1.823,
      "step": 1038100
    },
    {
      "epoch": 40.15160304753065,
      "grad_norm": 14.290911674499512,
      "learning_rate": 1.6540330793724458e-05,
      "loss": 1.8514,
      "step": 1038200
    },
    {
      "epoch": 40.155470472212556,
      "grad_norm": 12.61217975616455,
      "learning_rate": 1.6537107939822873e-05,
      "loss": 1.8495,
      "step": 1038300
    },
    {
      "epoch": 40.159337896894456,
      "grad_norm": 13.745396614074707,
      "learning_rate": 1.6533885085921284e-05,
      "loss": 1.768,
      "step": 1038400
    },
    {
      "epoch": 40.16320532157636,
      "grad_norm": 13.3966703414917,
      "learning_rate": 1.65306622320197e-05,
      "loss": 1.8212,
      "step": 1038500
    },
    {
      "epoch": 40.16707274625826,
      "grad_norm": 8.512328147888184,
      "learning_rate": 1.6527439378118114e-05,
      "loss": 1.8118,
      "step": 1038600
    },
    {
      "epoch": 40.17094017094017,
      "grad_norm": 10.506377220153809,
      "learning_rate": 1.6524216524216525e-05,
      "loss": 1.8769,
      "step": 1038700
    },
    {
      "epoch": 40.17480759562208,
      "grad_norm": 12.409228324890137,
      "learning_rate": 1.652099367031494e-05,
      "loss": 1.8241,
      "step": 1038800
    },
    {
      "epoch": 40.17867502030398,
      "grad_norm": 10.160140037536621,
      "learning_rate": 1.651777081641335e-05,
      "loss": 1.7757,
      "step": 1038900
    },
    {
      "epoch": 40.182542444985884,
      "grad_norm": 12.10739517211914,
      "learning_rate": 1.6514547962511766e-05,
      "loss": 1.8502,
      "step": 1039000
    },
    {
      "epoch": 40.18640986966779,
      "grad_norm": 10.448538780212402,
      "learning_rate": 1.6511325108610177e-05,
      "loss": 1.8155,
      "step": 1039100
    },
    {
      "epoch": 40.19027729434969,
      "grad_norm": 11.07563304901123,
      "learning_rate": 1.650810225470859e-05,
      "loss": 1.9203,
      "step": 1039200
    },
    {
      "epoch": 40.1941447190316,
      "grad_norm": 13.524707794189453,
      "learning_rate": 1.6504879400807003e-05,
      "loss": 1.8876,
      "step": 1039300
    },
    {
      "epoch": 40.1980121437135,
      "grad_norm": 7.937406539916992,
      "learning_rate": 1.6501656546905418e-05,
      "loss": 1.779,
      "step": 1039400
    },
    {
      "epoch": 40.201879568395405,
      "grad_norm": 12.150826454162598,
      "learning_rate": 1.649843369300383e-05,
      "loss": 1.8626,
      "step": 1039500
    },
    {
      "epoch": 40.20574699307731,
      "grad_norm": 11.592982292175293,
      "learning_rate": 1.6495210839102244e-05,
      "loss": 1.7843,
      "step": 1039600
    },
    {
      "epoch": 40.20961441775921,
      "grad_norm": 12.803803443908691,
      "learning_rate": 1.6491987985200655e-05,
      "loss": 1.9211,
      "step": 1039700
    },
    {
      "epoch": 40.21348184244112,
      "grad_norm": 10.705830574035645,
      "learning_rate": 1.648876513129907e-05,
      "loss": 1.8514,
      "step": 1039800
    },
    {
      "epoch": 40.21734926712302,
      "grad_norm": 11.702103614807129,
      "learning_rate": 1.648554227739748e-05,
      "loss": 1.8768,
      "step": 1039900
    },
    {
      "epoch": 40.221216691804926,
      "grad_norm": 10.971480369567871,
      "learning_rate": 1.6482319423495896e-05,
      "loss": 1.7458,
      "step": 1040000
    },
    {
      "epoch": 40.22508411648683,
      "grad_norm": 13.271202087402344,
      "learning_rate": 1.6479096569594307e-05,
      "loss": 1.7119,
      "step": 1040100
    },
    {
      "epoch": 40.22895154116873,
      "grad_norm": 11.991729736328125,
      "learning_rate": 1.6475873715692722e-05,
      "loss": 1.7888,
      "step": 1040200
    },
    {
      "epoch": 40.23281896585064,
      "grad_norm": 11.120428085327148,
      "learning_rate": 1.6472650861791133e-05,
      "loss": 1.8207,
      "step": 1040300
    },
    {
      "epoch": 40.23668639053255,
      "grad_norm": 12.881574630737305,
      "learning_rate": 1.6469428007889548e-05,
      "loss": 1.7972,
      "step": 1040400
    },
    {
      "epoch": 40.24055381521445,
      "grad_norm": 11.87896728515625,
      "learning_rate": 1.646620515398796e-05,
      "loss": 1.8527,
      "step": 1040500
    },
    {
      "epoch": 40.244421239896354,
      "grad_norm": 12.43680191040039,
      "learning_rate": 1.6462982300086374e-05,
      "loss": 1.7841,
      "step": 1040600
    },
    {
      "epoch": 40.248288664578254,
      "grad_norm": 14.576983451843262,
      "learning_rate": 1.6459759446184785e-05,
      "loss": 1.837,
      "step": 1040700
    },
    {
      "epoch": 40.25215608926016,
      "grad_norm": 8.588580131530762,
      "learning_rate": 1.64565365922832e-05,
      "loss": 1.7438,
      "step": 1040800
    },
    {
      "epoch": 40.25602351394207,
      "grad_norm": 12.307817459106445,
      "learning_rate": 1.645331373838161e-05,
      "loss": 1.8249,
      "step": 1040900
    },
    {
      "epoch": 40.25989093862397,
      "grad_norm": 10.811661720275879,
      "learning_rate": 1.6450090884480026e-05,
      "loss": 1.8435,
      "step": 1041000
    },
    {
      "epoch": 40.263758363305875,
      "grad_norm": 12.03141975402832,
      "learning_rate": 1.6446868030578437e-05,
      "loss": 1.8392,
      "step": 1041100
    },
    {
      "epoch": 40.26762578798778,
      "grad_norm": 13.424264907836914,
      "learning_rate": 1.6443645176676852e-05,
      "loss": 1.8422,
      "step": 1041200
    },
    {
      "epoch": 40.27149321266968,
      "grad_norm": 10.833881378173828,
      "learning_rate": 1.6440422322775263e-05,
      "loss": 1.776,
      "step": 1041300
    },
    {
      "epoch": 40.27536063735159,
      "grad_norm": 11.900355339050293,
      "learning_rate": 1.6437199468873678e-05,
      "loss": 1.8745,
      "step": 1041400
    },
    {
      "epoch": 40.27922806203349,
      "grad_norm": 8.634639739990234,
      "learning_rate": 1.643397661497209e-05,
      "loss": 1.9093,
      "step": 1041500
    },
    {
      "epoch": 40.283095486715396,
      "grad_norm": 12.183902740478516,
      "learning_rate": 1.6430753761070504e-05,
      "loss": 1.8276,
      "step": 1041600
    },
    {
      "epoch": 40.2869629113973,
      "grad_norm": 13.396785736083984,
      "learning_rate": 1.6427530907168916e-05,
      "loss": 1.8382,
      "step": 1041700
    },
    {
      "epoch": 40.2908303360792,
      "grad_norm": 9.50844669342041,
      "learning_rate": 1.642430805326733e-05,
      "loss": 1.8224,
      "step": 1041800
    },
    {
      "epoch": 40.29469776076111,
      "grad_norm": 11.071647644042969,
      "learning_rate": 1.642108519936574e-05,
      "loss": 1.8336,
      "step": 1041900
    },
    {
      "epoch": 40.29856518544301,
      "grad_norm": 11.187158584594727,
      "learning_rate": 1.6417862345464156e-05,
      "loss": 1.7759,
      "step": 1042000
    },
    {
      "epoch": 40.30243261012492,
      "grad_norm": 11.293919563293457,
      "learning_rate": 1.641463949156257e-05,
      "loss": 1.799,
      "step": 1042100
    },
    {
      "epoch": 40.306300034806824,
      "grad_norm": 12.474681854248047,
      "learning_rate": 1.6411416637660982e-05,
      "loss": 1.8218,
      "step": 1042200
    },
    {
      "epoch": 40.310167459488724,
      "grad_norm": 9.555878639221191,
      "learning_rate": 1.6408193783759397e-05,
      "loss": 1.9014,
      "step": 1042300
    },
    {
      "epoch": 40.31403488417063,
      "grad_norm": 15.845346450805664,
      "learning_rate": 1.640497092985781e-05,
      "loss": 1.8143,
      "step": 1042400
    },
    {
      "epoch": 40.31790230885254,
      "grad_norm": 10.687800407409668,
      "learning_rate": 1.6401748075956223e-05,
      "loss": 1.835,
      "step": 1042500
    },
    {
      "epoch": 40.32176973353444,
      "grad_norm": 11.96747875213623,
      "learning_rate": 1.6398525222054634e-05,
      "loss": 1.934,
      "step": 1042600
    },
    {
      "epoch": 40.325637158216345,
      "grad_norm": 14.559597969055176,
      "learning_rate": 1.639530236815305e-05,
      "loss": 1.7695,
      "step": 1042700
    },
    {
      "epoch": 40.329504582898245,
      "grad_norm": 9.794124603271484,
      "learning_rate": 1.639207951425146e-05,
      "loss": 1.8472,
      "step": 1042800
    },
    {
      "epoch": 40.33337200758015,
      "grad_norm": 9.91842269897461,
      "learning_rate": 1.6388856660349875e-05,
      "loss": 1.8392,
      "step": 1042900
    },
    {
      "epoch": 40.33723943226206,
      "grad_norm": 12.312117576599121,
      "learning_rate": 1.6385633806448287e-05,
      "loss": 1.9318,
      "step": 1043000
    },
    {
      "epoch": 40.34110685694396,
      "grad_norm": 11.455558776855469,
      "learning_rate": 1.63824109525467e-05,
      "loss": 1.8348,
      "step": 1043100
    },
    {
      "epoch": 40.344974281625866,
      "grad_norm": 13.59882926940918,
      "learning_rate": 1.6379188098645113e-05,
      "loss": 1.7709,
      "step": 1043200
    },
    {
      "epoch": 40.348841706307766,
      "grad_norm": 13.883161544799805,
      "learning_rate": 1.6375965244743527e-05,
      "loss": 1.8803,
      "step": 1043300
    },
    {
      "epoch": 40.35270913098967,
      "grad_norm": 13.801661491394043,
      "learning_rate": 1.637274239084194e-05,
      "loss": 1.8024,
      "step": 1043400
    },
    {
      "epoch": 40.35657655567158,
      "grad_norm": 11.224931716918945,
      "learning_rate": 1.6369519536940353e-05,
      "loss": 1.825,
      "step": 1043500
    },
    {
      "epoch": 40.36044398035348,
      "grad_norm": 12.48725700378418,
      "learning_rate": 1.6366296683038765e-05,
      "loss": 1.8267,
      "step": 1043600
    },
    {
      "epoch": 40.36431140503539,
      "grad_norm": 11.846749305725098,
      "learning_rate": 1.636307382913718e-05,
      "loss": 1.7566,
      "step": 1043700
    },
    {
      "epoch": 40.368178829717294,
      "grad_norm": 9.781728744506836,
      "learning_rate": 1.635985097523559e-05,
      "loss": 1.7624,
      "step": 1043800
    },
    {
      "epoch": 40.372046254399194,
      "grad_norm": 10.76024341583252,
      "learning_rate": 1.6356628121334005e-05,
      "loss": 1.7434,
      "step": 1043900
    },
    {
      "epoch": 40.3759136790811,
      "grad_norm": 12.662066459655762,
      "learning_rate": 1.6353405267432417e-05,
      "loss": 1.86,
      "step": 1044000
    },
    {
      "epoch": 40.379781103763,
      "grad_norm": 18.219396591186523,
      "learning_rate": 1.635018241353083e-05,
      "loss": 1.8391,
      "step": 1044100
    },
    {
      "epoch": 40.38364852844491,
      "grad_norm": 12.172771453857422,
      "learning_rate": 1.6346959559629243e-05,
      "loss": 1.801,
      "step": 1044200
    },
    {
      "epoch": 40.387515953126815,
      "grad_norm": 10.620442390441895,
      "learning_rate": 1.6343736705727658e-05,
      "loss": 1.8119,
      "step": 1044300
    },
    {
      "epoch": 40.391383377808715,
      "grad_norm": 9.977139472961426,
      "learning_rate": 1.634051385182607e-05,
      "loss": 1.8332,
      "step": 1044400
    },
    {
      "epoch": 40.39525080249062,
      "grad_norm": 10.245128631591797,
      "learning_rate": 1.6337290997924484e-05,
      "loss": 1.8009,
      "step": 1044500
    },
    {
      "epoch": 40.39911822717253,
      "grad_norm": 10.792509078979492,
      "learning_rate": 1.6334068144022895e-05,
      "loss": 1.7835,
      "step": 1044600
    },
    {
      "epoch": 40.40298565185443,
      "grad_norm": 11.80809211730957,
      "learning_rate": 1.6330845290121306e-05,
      "loss": 1.8865,
      "step": 1044700
    },
    {
      "epoch": 40.406853076536336,
      "grad_norm": 10.989620208740234,
      "learning_rate": 1.632762243621972e-05,
      "loss": 1.817,
      "step": 1044800
    },
    {
      "epoch": 40.410720501218236,
      "grad_norm": 14.235631942749023,
      "learning_rate": 1.6324399582318132e-05,
      "loss": 1.8036,
      "step": 1044900
    },
    {
      "epoch": 40.41458792590014,
      "grad_norm": 12.068504333496094,
      "learning_rate": 1.6321176728416547e-05,
      "loss": 1.7402,
      "step": 1045000
    },
    {
      "epoch": 40.41845535058205,
      "grad_norm": 13.471368789672852,
      "learning_rate": 1.6317953874514962e-05,
      "loss": 1.8099,
      "step": 1045100
    },
    {
      "epoch": 40.42232277526395,
      "grad_norm": 10.967681884765625,
      "learning_rate": 1.6314731020613373e-05,
      "loss": 1.7453,
      "step": 1045200
    },
    {
      "epoch": 40.42619019994586,
      "grad_norm": 10.20590877532959,
      "learning_rate": 1.6311508166711788e-05,
      "loss": 1.765,
      "step": 1045300
    },
    {
      "epoch": 40.43005762462776,
      "grad_norm": 11.149325370788574,
      "learning_rate": 1.6308285312810203e-05,
      "loss": 1.8675,
      "step": 1045400
    },
    {
      "epoch": 40.433925049309664,
      "grad_norm": 14.113194465637207,
      "learning_rate": 1.6305062458908614e-05,
      "loss": 1.794,
      "step": 1045500
    },
    {
      "epoch": 40.43779247399157,
      "grad_norm": 9.951290130615234,
      "learning_rate": 1.630183960500703e-05,
      "loss": 1.7705,
      "step": 1045600
    },
    {
      "epoch": 40.44165989867347,
      "grad_norm": 12.759587287902832,
      "learning_rate": 1.629861675110544e-05,
      "loss": 1.8246,
      "step": 1045700
    },
    {
      "epoch": 40.44552732335538,
      "grad_norm": 10.357707023620605,
      "learning_rate": 1.6295393897203855e-05,
      "loss": 1.8963,
      "step": 1045800
    },
    {
      "epoch": 40.449394748037285,
      "grad_norm": 12.701519966125488,
      "learning_rate": 1.6292171043302266e-05,
      "loss": 1.8621,
      "step": 1045900
    },
    {
      "epoch": 40.453262172719185,
      "grad_norm": 14.778931617736816,
      "learning_rate": 1.628894818940068e-05,
      "loss": 1.8548,
      "step": 1046000
    },
    {
      "epoch": 40.45712959740109,
      "grad_norm": 13.05188274383545,
      "learning_rate": 1.6285725335499092e-05,
      "loss": 1.9352,
      "step": 1046100
    },
    {
      "epoch": 40.46099702208299,
      "grad_norm": 12.629647254943848,
      "learning_rate": 1.6282502481597507e-05,
      "loss": 1.817,
      "step": 1046200
    },
    {
      "epoch": 40.4648644467649,
      "grad_norm": 11.83236312866211,
      "learning_rate": 1.6279279627695918e-05,
      "loss": 1.8192,
      "step": 1046300
    },
    {
      "epoch": 40.468731871446806,
      "grad_norm": 10.161333084106445,
      "learning_rate": 1.6276056773794333e-05,
      "loss": 1.7894,
      "step": 1046400
    },
    {
      "epoch": 40.472599296128706,
      "grad_norm": 8.590179443359375,
      "learning_rate": 1.6272833919892744e-05,
      "loss": 1.8405,
      "step": 1046500
    },
    {
      "epoch": 40.47646672081061,
      "grad_norm": 10.480098724365234,
      "learning_rate": 1.626961106599116e-05,
      "loss": 1.8986,
      "step": 1046600
    },
    {
      "epoch": 40.48033414549251,
      "grad_norm": 9.941265106201172,
      "learning_rate": 1.626638821208957e-05,
      "loss": 1.8556,
      "step": 1046700
    },
    {
      "epoch": 40.48420157017442,
      "grad_norm": 10.315750122070312,
      "learning_rate": 1.6263165358187985e-05,
      "loss": 1.8194,
      "step": 1046800
    },
    {
      "epoch": 40.48806899485633,
      "grad_norm": 12.587077140808105,
      "learning_rate": 1.6259942504286396e-05,
      "loss": 1.8533,
      "step": 1046900
    },
    {
      "epoch": 40.49193641953823,
      "grad_norm": 14.546016693115234,
      "learning_rate": 1.625671965038481e-05,
      "loss": 1.7989,
      "step": 1047000
    },
    {
      "epoch": 40.495803844220134,
      "grad_norm": 10.613992691040039,
      "learning_rate": 1.6253496796483222e-05,
      "loss": 1.8149,
      "step": 1047100
    },
    {
      "epoch": 40.49967126890204,
      "grad_norm": 16.367149353027344,
      "learning_rate": 1.6250273942581637e-05,
      "loss": 1.8182,
      "step": 1047200
    },
    {
      "epoch": 40.50353869358394,
      "grad_norm": 11.053557395935059,
      "learning_rate": 1.624705108868005e-05,
      "loss": 1.8357,
      "step": 1047300
    },
    {
      "epoch": 40.50740611826585,
      "grad_norm": 13.917449951171875,
      "learning_rate": 1.6243828234778463e-05,
      "loss": 1.9907,
      "step": 1047400
    },
    {
      "epoch": 40.51127354294775,
      "grad_norm": 9.703862190246582,
      "learning_rate": 1.6240605380876874e-05,
      "loss": 1.7872,
      "step": 1047500
    },
    {
      "epoch": 40.515140967629655,
      "grad_norm": 11.678740501403809,
      "learning_rate": 1.6237382526975286e-05,
      "loss": 1.8287,
      "step": 1047600
    },
    {
      "epoch": 40.51900839231156,
      "grad_norm": 9.307073593139648,
      "learning_rate": 1.62341596730737e-05,
      "loss": 1.806,
      "step": 1047700
    },
    {
      "epoch": 40.52287581699346,
      "grad_norm": 10.579994201660156,
      "learning_rate": 1.6230936819172112e-05,
      "loss": 1.7453,
      "step": 1047800
    },
    {
      "epoch": 40.52674324167537,
      "grad_norm": 10.36627197265625,
      "learning_rate": 1.6227713965270526e-05,
      "loss": 1.8664,
      "step": 1047900
    },
    {
      "epoch": 40.530610666357276,
      "grad_norm": 11.205914497375488,
      "learning_rate": 1.6224491111368938e-05,
      "loss": 1.8543,
      "step": 1048000
    },
    {
      "epoch": 40.534478091039176,
      "grad_norm": 13.186613082885742,
      "learning_rate": 1.6221268257467353e-05,
      "loss": 1.806,
      "step": 1048100
    },
    {
      "epoch": 40.53834551572108,
      "grad_norm": 10.223140716552734,
      "learning_rate": 1.6218045403565764e-05,
      "loss": 1.8266,
      "step": 1048200
    },
    {
      "epoch": 40.54221294040298,
      "grad_norm": 13.74433708190918,
      "learning_rate": 1.621482254966418e-05,
      "loss": 1.825,
      "step": 1048300
    },
    {
      "epoch": 40.54608036508489,
      "grad_norm": 12.147613525390625,
      "learning_rate": 1.621159969576259e-05,
      "loss": 1.9321,
      "step": 1048400
    },
    {
      "epoch": 40.5499477897668,
      "grad_norm": 11.018519401550293,
      "learning_rate": 1.6208376841861005e-05,
      "loss": 1.7875,
      "step": 1048500
    },
    {
      "epoch": 40.5538152144487,
      "grad_norm": 9.36020278930664,
      "learning_rate": 1.620515398795942e-05,
      "loss": 1.7935,
      "step": 1048600
    },
    {
      "epoch": 40.557682639130604,
      "grad_norm": 10.356136322021484,
      "learning_rate": 1.620193113405783e-05,
      "loss": 1.8143,
      "step": 1048700
    },
    {
      "epoch": 40.561550063812504,
      "grad_norm": 12.059530258178711,
      "learning_rate": 1.6198708280156245e-05,
      "loss": 1.7616,
      "step": 1048800
    },
    {
      "epoch": 40.56541748849441,
      "grad_norm": 11.319900512695312,
      "learning_rate": 1.619548542625466e-05,
      "loss": 1.8737,
      "step": 1048900
    },
    {
      "epoch": 40.56928491317632,
      "grad_norm": 12.93454360961914,
      "learning_rate": 1.619226257235307e-05,
      "loss": 1.8725,
      "step": 1049000
    },
    {
      "epoch": 40.57315233785822,
      "grad_norm": 10.548927307128906,
      "learning_rate": 1.6189039718451486e-05,
      "loss": 1.9593,
      "step": 1049100
    },
    {
      "epoch": 40.577019762540125,
      "grad_norm": 10.159196853637695,
      "learning_rate": 1.6185816864549897e-05,
      "loss": 1.7394,
      "step": 1049200
    },
    {
      "epoch": 40.58088718722203,
      "grad_norm": 10.903478622436523,
      "learning_rate": 1.6182594010648312e-05,
      "loss": 1.8545,
      "step": 1049300
    },
    {
      "epoch": 40.58475461190393,
      "grad_norm": 12.51982593536377,
      "learning_rate": 1.6179371156746724e-05,
      "loss": 1.8375,
      "step": 1049400
    },
    {
      "epoch": 40.58862203658584,
      "grad_norm": 13.14822769165039,
      "learning_rate": 1.6176148302845138e-05,
      "loss": 1.8011,
      "step": 1049500
    },
    {
      "epoch": 40.59248946126774,
      "grad_norm": 11.707712173461914,
      "learning_rate": 1.617292544894355e-05,
      "loss": 1.8787,
      "step": 1049600
    },
    {
      "epoch": 40.596356885949646,
      "grad_norm": 11.85854434967041,
      "learning_rate": 1.6169702595041964e-05,
      "loss": 1.8375,
      "step": 1049700
    },
    {
      "epoch": 40.60022431063155,
      "grad_norm": 13.074976921081543,
      "learning_rate": 1.6166479741140376e-05,
      "loss": 1.9766,
      "step": 1049800
    },
    {
      "epoch": 40.60409173531345,
      "grad_norm": 11.14942741394043,
      "learning_rate": 1.616325688723879e-05,
      "loss": 1.866,
      "step": 1049900
    },
    {
      "epoch": 40.60795915999536,
      "grad_norm": 7.951788425445557,
      "learning_rate": 1.61600340333372e-05,
      "loss": 1.686,
      "step": 1050000
    },
    {
      "epoch": 40.61182658467726,
      "grad_norm": 11.66086196899414,
      "learning_rate": 1.6156811179435616e-05,
      "loss": 1.7873,
      "step": 1050100
    },
    {
      "epoch": 40.61569400935917,
      "grad_norm": 14.276708602905273,
      "learning_rate": 1.6153588325534028e-05,
      "loss": 1.8199,
      "step": 1050200
    },
    {
      "epoch": 40.619561434041074,
      "grad_norm": 9.216050148010254,
      "learning_rate": 1.6150365471632442e-05,
      "loss": 1.8069,
      "step": 1050300
    },
    {
      "epoch": 40.623428858722974,
      "grad_norm": 10.666534423828125,
      "learning_rate": 1.6147142617730854e-05,
      "loss": 1.7762,
      "step": 1050400
    },
    {
      "epoch": 40.62729628340488,
      "grad_norm": 13.886564254760742,
      "learning_rate": 1.6143919763829265e-05,
      "loss": 1.9181,
      "step": 1050500
    },
    {
      "epoch": 40.63116370808679,
      "grad_norm": 11.578081130981445,
      "learning_rate": 1.614069690992768e-05,
      "loss": 1.7929,
      "step": 1050600
    },
    {
      "epoch": 40.63503113276869,
      "grad_norm": 10.663272857666016,
      "learning_rate": 1.613747405602609e-05,
      "loss": 1.9527,
      "step": 1050700
    },
    {
      "epoch": 40.638898557450595,
      "grad_norm": 12.877911567687988,
      "learning_rate": 1.6134251202124506e-05,
      "loss": 1.8072,
      "step": 1050800
    },
    {
      "epoch": 40.642765982132495,
      "grad_norm": 12.433393478393555,
      "learning_rate": 1.6131028348222917e-05,
      "loss": 1.7631,
      "step": 1050900
    },
    {
      "epoch": 40.6466334068144,
      "grad_norm": 14.44050121307373,
      "learning_rate": 1.6127805494321332e-05,
      "loss": 1.8217,
      "step": 1051000
    },
    {
      "epoch": 40.65050083149631,
      "grad_norm": 9.20974063873291,
      "learning_rate": 1.6124582640419743e-05,
      "loss": 1.9373,
      "step": 1051100
    },
    {
      "epoch": 40.65436825617821,
      "grad_norm": 10.002421379089355,
      "learning_rate": 1.6121359786518158e-05,
      "loss": 1.8701,
      "step": 1051200
    },
    {
      "epoch": 40.658235680860116,
      "grad_norm": 13.962255477905273,
      "learning_rate": 1.611813693261657e-05,
      "loss": 1.8077,
      "step": 1051300
    },
    {
      "epoch": 40.662103105542016,
      "grad_norm": 10.896018981933594,
      "learning_rate": 1.6114914078714984e-05,
      "loss": 1.7458,
      "step": 1051400
    },
    {
      "epoch": 40.66597053022392,
      "grad_norm": 10.737141609191895,
      "learning_rate": 1.6111691224813395e-05,
      "loss": 1.7949,
      "step": 1051500
    },
    {
      "epoch": 40.66983795490583,
      "grad_norm": 12.7728271484375,
      "learning_rate": 1.610846837091181e-05,
      "loss": 1.7531,
      "step": 1051600
    },
    {
      "epoch": 40.67370537958773,
      "grad_norm": 11.404884338378906,
      "learning_rate": 1.610524551701022e-05,
      "loss": 1.8072,
      "step": 1051700
    },
    {
      "epoch": 40.67757280426964,
      "grad_norm": 13.329269409179688,
      "learning_rate": 1.6102022663108636e-05,
      "loss": 1.8276,
      "step": 1051800
    },
    {
      "epoch": 40.681440228951544,
      "grad_norm": 8.476624488830566,
      "learning_rate": 1.6098799809207047e-05,
      "loss": 1.7355,
      "step": 1051900
    },
    {
      "epoch": 40.685307653633444,
      "grad_norm": 13.179903984069824,
      "learning_rate": 1.6095576955305462e-05,
      "loss": 1.794,
      "step": 1052000
    },
    {
      "epoch": 40.68917507831535,
      "grad_norm": 11.331416130065918,
      "learning_rate": 1.6092354101403877e-05,
      "loss": 1.8636,
      "step": 1052100
    },
    {
      "epoch": 40.69304250299725,
      "grad_norm": 12.54784870147705,
      "learning_rate": 1.6089131247502288e-05,
      "loss": 1.7188,
      "step": 1052200
    },
    {
      "epoch": 40.69690992767916,
      "grad_norm": 9.194046020507812,
      "learning_rate": 1.6085908393600703e-05,
      "loss": 1.7716,
      "step": 1052300
    },
    {
      "epoch": 40.700777352361065,
      "grad_norm": 12.548100471496582,
      "learning_rate": 1.6082685539699118e-05,
      "loss": 1.8747,
      "step": 1052400
    },
    {
      "epoch": 40.704644777042965,
      "grad_norm": 15.030045509338379,
      "learning_rate": 1.607946268579753e-05,
      "loss": 1.7007,
      "step": 1052500
    },
    {
      "epoch": 40.70851220172487,
      "grad_norm": 10.758234977722168,
      "learning_rate": 1.6076239831895944e-05,
      "loss": 1.8546,
      "step": 1052600
    },
    {
      "epoch": 40.71237962640678,
      "grad_norm": 12.028834342956543,
      "learning_rate": 1.6073016977994355e-05,
      "loss": 1.8666,
      "step": 1052700
    },
    {
      "epoch": 40.71624705108868,
      "grad_norm": 13.902795791625977,
      "learning_rate": 1.606979412409277e-05,
      "loss": 1.8263,
      "step": 1052800
    },
    {
      "epoch": 40.720114475770586,
      "grad_norm": 15.407357215881348,
      "learning_rate": 1.606657127019118e-05,
      "loss": 1.8659,
      "step": 1052900
    },
    {
      "epoch": 40.723981900452486,
      "grad_norm": 16.661165237426758,
      "learning_rate": 1.6063348416289596e-05,
      "loss": 1.8601,
      "step": 1053000
    },
    {
      "epoch": 40.72784932513439,
      "grad_norm": 11.507854461669922,
      "learning_rate": 1.6060125562388007e-05,
      "loss": 1.8514,
      "step": 1053100
    },
    {
      "epoch": 40.7317167498163,
      "grad_norm": 12.664624214172363,
      "learning_rate": 1.6056902708486422e-05,
      "loss": 1.8444,
      "step": 1053200
    },
    {
      "epoch": 40.7355841744982,
      "grad_norm": 9.444635391235352,
      "learning_rate": 1.6053679854584833e-05,
      "loss": 1.7876,
      "step": 1053300
    },
    {
      "epoch": 40.73945159918011,
      "grad_norm": 12.544318199157715,
      "learning_rate": 1.6050457000683244e-05,
      "loss": 1.8337,
      "step": 1053400
    },
    {
      "epoch": 40.74331902386201,
      "grad_norm": 11.900372505187988,
      "learning_rate": 1.604723414678166e-05,
      "loss": 1.7907,
      "step": 1053500
    },
    {
      "epoch": 40.747186448543914,
      "grad_norm": 12.088171005249023,
      "learning_rate": 1.604401129288007e-05,
      "loss": 1.7986,
      "step": 1053600
    },
    {
      "epoch": 40.75105387322582,
      "grad_norm": 10.231595039367676,
      "learning_rate": 1.6040788438978485e-05,
      "loss": 1.7921,
      "step": 1053700
    },
    {
      "epoch": 40.75492129790772,
      "grad_norm": 11.596049308776855,
      "learning_rate": 1.6037565585076897e-05,
      "loss": 1.7954,
      "step": 1053800
    },
    {
      "epoch": 40.75878872258963,
      "grad_norm": 12.375236511230469,
      "learning_rate": 1.603434273117531e-05,
      "loss": 1.7535,
      "step": 1053900
    },
    {
      "epoch": 40.762656147271535,
      "grad_norm": 10.251818656921387,
      "learning_rate": 1.6031119877273723e-05,
      "loss": 1.8098,
      "step": 1054000
    },
    {
      "epoch": 40.766523571953435,
      "grad_norm": 10.72751235961914,
      "learning_rate": 1.6027897023372137e-05,
      "loss": 1.7107,
      "step": 1054100
    },
    {
      "epoch": 40.77039099663534,
      "grad_norm": 10.006172180175781,
      "learning_rate": 1.602467416947055e-05,
      "loss": 1.9049,
      "step": 1054200
    },
    {
      "epoch": 40.77425842131724,
      "grad_norm": 12.294254302978516,
      "learning_rate": 1.6021451315568963e-05,
      "loss": 1.8448,
      "step": 1054300
    },
    {
      "epoch": 40.77812584599915,
      "grad_norm": 13.039726257324219,
      "learning_rate": 1.6018228461667375e-05,
      "loss": 1.8477,
      "step": 1054400
    },
    {
      "epoch": 40.781993270681056,
      "grad_norm": 12.50134563446045,
      "learning_rate": 1.601500560776579e-05,
      "loss": 1.7598,
      "step": 1054500
    },
    {
      "epoch": 40.785860695362956,
      "grad_norm": 15.93818187713623,
      "learning_rate": 1.60117827538642e-05,
      "loss": 1.9737,
      "step": 1054600
    },
    {
      "epoch": 40.78972812004486,
      "grad_norm": 10.81762409210205,
      "learning_rate": 1.6008559899962616e-05,
      "loss": 1.8308,
      "step": 1054700
    },
    {
      "epoch": 40.79359554472676,
      "grad_norm": 13.343825340270996,
      "learning_rate": 1.6005337046061027e-05,
      "loss": 1.879,
      "step": 1054800
    },
    {
      "epoch": 40.79746296940867,
      "grad_norm": 17.107070922851562,
      "learning_rate": 1.600211419215944e-05,
      "loss": 1.7676,
      "step": 1054900
    },
    {
      "epoch": 40.80133039409058,
      "grad_norm": 10.042312622070312,
      "learning_rate": 1.5998891338257853e-05,
      "loss": 1.8052,
      "step": 1055000
    },
    {
      "epoch": 40.80519781877248,
      "grad_norm": 12.853242874145508,
      "learning_rate": 1.5995668484356268e-05,
      "loss": 1.8252,
      "step": 1055100
    },
    {
      "epoch": 40.809065243454384,
      "grad_norm": 9.919318199157715,
      "learning_rate": 1.599244563045468e-05,
      "loss": 1.8433,
      "step": 1055200
    },
    {
      "epoch": 40.81293266813629,
      "grad_norm": 12.91746711730957,
      "learning_rate": 1.5989222776553094e-05,
      "loss": 1.898,
      "step": 1055300
    },
    {
      "epoch": 40.81680009281819,
      "grad_norm": 11.569095611572266,
      "learning_rate": 1.598599992265151e-05,
      "loss": 1.8138,
      "step": 1055400
    },
    {
      "epoch": 40.8206675175001,
      "grad_norm": 10.964475631713867,
      "learning_rate": 1.598277706874992e-05,
      "loss": 1.9093,
      "step": 1055500
    },
    {
      "epoch": 40.824534942182,
      "grad_norm": 13.135878562927246,
      "learning_rate": 1.5979554214848334e-05,
      "loss": 1.8398,
      "step": 1055600
    },
    {
      "epoch": 40.828402366863905,
      "grad_norm": 5.689516067504883,
      "learning_rate": 1.5976331360946746e-05,
      "loss": 1.7926,
      "step": 1055700
    },
    {
      "epoch": 40.83226979154581,
      "grad_norm": 11.66112232208252,
      "learning_rate": 1.597310850704516e-05,
      "loss": 1.8641,
      "step": 1055800
    },
    {
      "epoch": 40.83613721622771,
      "grad_norm": 12.609213829040527,
      "learning_rate": 1.5969885653143575e-05,
      "loss": 1.862,
      "step": 1055900
    },
    {
      "epoch": 40.84000464090962,
      "grad_norm": 13.708179473876953,
      "learning_rate": 1.5966662799241987e-05,
      "loss": 1.8912,
      "step": 1056000
    },
    {
      "epoch": 40.843872065591526,
      "grad_norm": 10.104216575622559,
      "learning_rate": 1.59634399453404e-05,
      "loss": 1.9237,
      "step": 1056100
    },
    {
      "epoch": 40.847739490273426,
      "grad_norm": 13.989823341369629,
      "learning_rate": 1.5960217091438813e-05,
      "loss": 1.7708,
      "step": 1056200
    },
    {
      "epoch": 40.85160691495533,
      "grad_norm": 9.307318687438965,
      "learning_rate": 1.5956994237537227e-05,
      "loss": 1.777,
      "step": 1056300
    },
    {
      "epoch": 40.85547433963723,
      "grad_norm": 10.854045867919922,
      "learning_rate": 1.595377138363564e-05,
      "loss": 1.8619,
      "step": 1056400
    },
    {
      "epoch": 40.85934176431914,
      "grad_norm": 14.49240779876709,
      "learning_rate": 1.595054852973405e-05,
      "loss": 1.8088,
      "step": 1056500
    },
    {
      "epoch": 40.86320918900105,
      "grad_norm": 10.171430587768555,
      "learning_rate": 1.5947325675832465e-05,
      "loss": 1.8028,
      "step": 1056600
    },
    {
      "epoch": 40.86707661368295,
      "grad_norm": 13.721622467041016,
      "learning_rate": 1.5944102821930876e-05,
      "loss": 1.7839,
      "step": 1056700
    },
    {
      "epoch": 40.870944038364854,
      "grad_norm": 11.420633316040039,
      "learning_rate": 1.594087996802929e-05,
      "loss": 1.8829,
      "step": 1056800
    },
    {
      "epoch": 40.874811463046754,
      "grad_norm": 13.237504005432129,
      "learning_rate": 1.5937657114127702e-05,
      "loss": 1.8091,
      "step": 1056900
    },
    {
      "epoch": 40.87867888772866,
      "grad_norm": 10.058517456054688,
      "learning_rate": 1.5934434260226117e-05,
      "loss": 1.7155,
      "step": 1057000
    },
    {
      "epoch": 40.88254631241057,
      "grad_norm": 11.488363265991211,
      "learning_rate": 1.5931211406324528e-05,
      "loss": 1.8587,
      "step": 1057100
    },
    {
      "epoch": 40.88641373709247,
      "grad_norm": 10.722900390625,
      "learning_rate": 1.5927988552422943e-05,
      "loss": 1.8375,
      "step": 1057200
    },
    {
      "epoch": 40.890281161774375,
      "grad_norm": 11.018136978149414,
      "learning_rate": 1.5924765698521354e-05,
      "loss": 1.7809,
      "step": 1057300
    },
    {
      "epoch": 40.89414858645628,
      "grad_norm": 11.526534080505371,
      "learning_rate": 1.592154284461977e-05,
      "loss": 1.8451,
      "step": 1057400
    },
    {
      "epoch": 40.89801601113818,
      "grad_norm": 11.898289680480957,
      "learning_rate": 1.591831999071818e-05,
      "loss": 1.7666,
      "step": 1057500
    },
    {
      "epoch": 40.90188343582009,
      "grad_norm": 9.971217155456543,
      "learning_rate": 1.5915097136816595e-05,
      "loss": 1.8157,
      "step": 1057600
    },
    {
      "epoch": 40.90575086050199,
      "grad_norm": 13.964733123779297,
      "learning_rate": 1.5911874282915006e-05,
      "loss": 1.8973,
      "step": 1057700
    },
    {
      "epoch": 40.909618285183896,
      "grad_norm": 10.80998420715332,
      "learning_rate": 1.590865142901342e-05,
      "loss": 1.8436,
      "step": 1057800
    },
    {
      "epoch": 40.9134857098658,
      "grad_norm": 11.87590503692627,
      "learning_rate": 1.5905428575111832e-05,
      "loss": 1.8691,
      "step": 1057900
    },
    {
      "epoch": 40.9173531345477,
      "grad_norm": 11.217183113098145,
      "learning_rate": 1.5902205721210247e-05,
      "loss": 1.8121,
      "step": 1058000
    },
    {
      "epoch": 40.92122055922961,
      "grad_norm": 13.189045906066895,
      "learning_rate": 1.589898286730866e-05,
      "loss": 1.8086,
      "step": 1058100
    },
    {
      "epoch": 40.92508798391151,
      "grad_norm": 14.501333236694336,
      "learning_rate": 1.5895760013407073e-05,
      "loss": 1.844,
      "step": 1058200
    },
    {
      "epoch": 40.92895540859342,
      "grad_norm": 11.817582130432129,
      "learning_rate": 1.5892537159505484e-05,
      "loss": 1.7699,
      "step": 1058300
    },
    {
      "epoch": 40.932822833275324,
      "grad_norm": 15.320037841796875,
      "learning_rate": 1.58893143056039e-05,
      "loss": 1.9496,
      "step": 1058400
    },
    {
      "epoch": 40.936690257957224,
      "grad_norm": 16.329866409301758,
      "learning_rate": 1.588609145170231e-05,
      "loss": 1.8426,
      "step": 1058500
    },
    {
      "epoch": 40.94055768263913,
      "grad_norm": 12.227469444274902,
      "learning_rate": 1.5882868597800725e-05,
      "loss": 1.9123,
      "step": 1058600
    },
    {
      "epoch": 40.94442510732104,
      "grad_norm": 8.104253768920898,
      "learning_rate": 1.5879645743899136e-05,
      "loss": 1.8176,
      "step": 1058700
    },
    {
      "epoch": 40.94829253200294,
      "grad_norm": 11.451172828674316,
      "learning_rate": 1.587642288999755e-05,
      "loss": 1.9018,
      "step": 1058800
    },
    {
      "epoch": 40.952159956684845,
      "grad_norm": 12.548604965209961,
      "learning_rate": 1.5873200036095966e-05,
      "loss": 1.9403,
      "step": 1058900
    },
    {
      "epoch": 40.956027381366745,
      "grad_norm": 13.03438663482666,
      "learning_rate": 1.5869977182194377e-05,
      "loss": 1.8812,
      "step": 1059000
    },
    {
      "epoch": 40.95989480604865,
      "grad_norm": 11.967523574829102,
      "learning_rate": 1.5866754328292792e-05,
      "loss": 1.9007,
      "step": 1059100
    },
    {
      "epoch": 40.96376223073056,
      "grad_norm": 13.529695510864258,
      "learning_rate": 1.5863531474391207e-05,
      "loss": 1.8264,
      "step": 1059200
    },
    {
      "epoch": 40.96762965541246,
      "grad_norm": 11.140300750732422,
      "learning_rate": 1.5860308620489618e-05,
      "loss": 1.8674,
      "step": 1059300
    },
    {
      "epoch": 40.971497080094366,
      "grad_norm": 12.51414680480957,
      "learning_rate": 1.585708576658803e-05,
      "loss": 1.8539,
      "step": 1059400
    },
    {
      "epoch": 40.975364504776266,
      "grad_norm": 14.325878143310547,
      "learning_rate": 1.5853862912686444e-05,
      "loss": 1.8149,
      "step": 1059500
    },
    {
      "epoch": 40.97923192945817,
      "grad_norm": 11.202033042907715,
      "learning_rate": 1.5850640058784855e-05,
      "loss": 1.8449,
      "step": 1059600
    },
    {
      "epoch": 40.98309935414008,
      "grad_norm": 13.132041931152344,
      "learning_rate": 1.584741720488327e-05,
      "loss": 1.8507,
      "step": 1059700
    },
    {
      "epoch": 40.98696677882198,
      "grad_norm": 10.418109893798828,
      "learning_rate": 1.584419435098168e-05,
      "loss": 1.8311,
      "step": 1059800
    },
    {
      "epoch": 40.99083420350389,
      "grad_norm": 16.461210250854492,
      "learning_rate": 1.5840971497080096e-05,
      "loss": 1.8068,
      "step": 1059900
    },
    {
      "epoch": 40.994701628185794,
      "grad_norm": 13.253776550292969,
      "learning_rate": 1.5837748643178507e-05,
      "loss": 1.8041,
      "step": 1060000
    },
    {
      "epoch": 40.998569052867694,
      "grad_norm": 13.467337608337402,
      "learning_rate": 1.5834525789276922e-05,
      "loss": 1.8346,
      "step": 1060100
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.766112208366394,
      "eval_runtime": 2.9656,
      "eval_samples_per_second": 458.93,
      "eval_steps_per_second": 458.93,
      "step": 1060137
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.6152288913726807,
      "eval_runtime": 56.1737,
      "eval_samples_per_second": 460.305,
      "eval_steps_per_second": 460.305,
      "step": 1060137
    },
    {
      "epoch": 41.0024364775496,
      "grad_norm": 13.020834922790527,
      "learning_rate": 1.5831302935375334e-05,
      "loss": 1.7736,
      "step": 1060200
    },
    {
      "epoch": 41.0063039022315,
      "grad_norm": 14.224687576293945,
      "learning_rate": 1.5828080081473748e-05,
      "loss": 1.7357,
      "step": 1060300
    },
    {
      "epoch": 41.01017132691341,
      "grad_norm": 13.47896957397461,
      "learning_rate": 1.582485722757216e-05,
      "loss": 1.8024,
      "step": 1060400
    },
    {
      "epoch": 41.014038751595315,
      "grad_norm": 12.359649658203125,
      "learning_rate": 1.5821634373670574e-05,
      "loss": 1.8231,
      "step": 1060500
    },
    {
      "epoch": 41.017906176277215,
      "grad_norm": 10.921093940734863,
      "learning_rate": 1.5818411519768986e-05,
      "loss": 1.8668,
      "step": 1060600
    },
    {
      "epoch": 41.02177360095912,
      "grad_norm": 11.813409805297852,
      "learning_rate": 1.58151886658674e-05,
      "loss": 1.8173,
      "step": 1060700
    },
    {
      "epoch": 41.02564102564103,
      "grad_norm": 11.762094497680664,
      "learning_rate": 1.581196581196581e-05,
      "loss": 1.8062,
      "step": 1060800
    },
    {
      "epoch": 41.02950845032293,
      "grad_norm": 15.662858009338379,
      "learning_rate": 1.5808742958064226e-05,
      "loss": 1.8363,
      "step": 1060900
    },
    {
      "epoch": 41.033375875004836,
      "grad_norm": 13.103431701660156,
      "learning_rate": 1.5805520104162638e-05,
      "loss": 1.7493,
      "step": 1061000
    },
    {
      "epoch": 41.037243299686736,
      "grad_norm": 16.025278091430664,
      "learning_rate": 1.5802297250261052e-05,
      "loss": 1.808,
      "step": 1061100
    },
    {
      "epoch": 41.04111072436864,
      "grad_norm": 13.218687057495117,
      "learning_rate": 1.5799074396359464e-05,
      "loss": 1.8646,
      "step": 1061200
    },
    {
      "epoch": 41.04497814905055,
      "grad_norm": 13.608748435974121,
      "learning_rate": 1.579585154245788e-05,
      "loss": 1.763,
      "step": 1061300
    },
    {
      "epoch": 41.04884557373245,
      "grad_norm": 11.048384666442871,
      "learning_rate": 1.579262868855629e-05,
      "loss": 1.8109,
      "step": 1061400
    },
    {
      "epoch": 41.05271299841436,
      "grad_norm": 11.564915657043457,
      "learning_rate": 1.5789405834654705e-05,
      "loss": 1.9234,
      "step": 1061500
    },
    {
      "epoch": 41.05658042309626,
      "grad_norm": 13.924352645874023,
      "learning_rate": 1.5786182980753116e-05,
      "loss": 1.8158,
      "step": 1061600
    },
    {
      "epoch": 41.060447847778164,
      "grad_norm": 13.949599266052246,
      "learning_rate": 1.578296012685153e-05,
      "loss": 1.839,
      "step": 1061700
    },
    {
      "epoch": 41.06431527246007,
      "grad_norm": 13.164078712463379,
      "learning_rate": 1.5779737272949942e-05,
      "loss": 1.8084,
      "step": 1061800
    },
    {
      "epoch": 41.06818269714197,
      "grad_norm": 15.606287002563477,
      "learning_rate": 1.5776514419048357e-05,
      "loss": 1.8292,
      "step": 1061900
    },
    {
      "epoch": 41.07205012182388,
      "grad_norm": 9.948673248291016,
      "learning_rate": 1.5773291565146768e-05,
      "loss": 1.7991,
      "step": 1062000
    },
    {
      "epoch": 41.075917546505785,
      "grad_norm": 13.42543888092041,
      "learning_rate": 1.5770068711245183e-05,
      "loss": 1.7424,
      "step": 1062100
    },
    {
      "epoch": 41.079784971187685,
      "grad_norm": 15.015835762023926,
      "learning_rate": 1.5766845857343594e-05,
      "loss": 1.7667,
      "step": 1062200
    },
    {
      "epoch": 41.08365239586959,
      "grad_norm": 10.713951110839844,
      "learning_rate": 1.576362300344201e-05,
      "loss": 1.7727,
      "step": 1062300
    },
    {
      "epoch": 41.08751982055149,
      "grad_norm": 9.839128494262695,
      "learning_rate": 1.5760400149540423e-05,
      "loss": 1.891,
      "step": 1062400
    },
    {
      "epoch": 41.0913872452334,
      "grad_norm": 7.207095623016357,
      "learning_rate": 1.5757177295638835e-05,
      "loss": 1.8372,
      "step": 1062500
    },
    {
      "epoch": 41.095254669915306,
      "grad_norm": 12.678727149963379,
      "learning_rate": 1.575395444173725e-05,
      "loss": 1.8431,
      "step": 1062600
    },
    {
      "epoch": 41.099122094597206,
      "grad_norm": 10.994860649108887,
      "learning_rate": 1.575073158783566e-05,
      "loss": 1.7834,
      "step": 1062700
    },
    {
      "epoch": 41.10298951927911,
      "grad_norm": 11.76578426361084,
      "learning_rate": 1.5747508733934076e-05,
      "loss": 1.8299,
      "step": 1062800
    },
    {
      "epoch": 41.10685694396101,
      "grad_norm": 15.992066383361816,
      "learning_rate": 1.5744285880032487e-05,
      "loss": 1.8276,
      "step": 1062900
    },
    {
      "epoch": 41.11072436864292,
      "grad_norm": 13.319197654724121,
      "learning_rate": 1.57410630261309e-05,
      "loss": 1.8256,
      "step": 1063000
    },
    {
      "epoch": 41.11459179332483,
      "grad_norm": 11.45606517791748,
      "learning_rate": 1.5737840172229313e-05,
      "loss": 1.7619,
      "step": 1063100
    },
    {
      "epoch": 41.11845921800673,
      "grad_norm": 11.908417701721191,
      "learning_rate": 1.5734617318327728e-05,
      "loss": 1.7054,
      "step": 1063200
    },
    {
      "epoch": 41.122326642688634,
      "grad_norm": 11.830965042114258,
      "learning_rate": 1.573139446442614e-05,
      "loss": 1.7982,
      "step": 1063300
    },
    {
      "epoch": 41.12619406737054,
      "grad_norm": 11.19024658203125,
      "learning_rate": 1.5728171610524554e-05,
      "loss": 1.6865,
      "step": 1063400
    },
    {
      "epoch": 41.13006149205244,
      "grad_norm": 10.8880033493042,
      "learning_rate": 1.5724948756622965e-05,
      "loss": 1.872,
      "step": 1063500
    },
    {
      "epoch": 41.13392891673435,
      "grad_norm": 16.325817108154297,
      "learning_rate": 1.572172590272138e-05,
      "loss": 1.8497,
      "step": 1063600
    },
    {
      "epoch": 41.13779634141625,
      "grad_norm": 10.901354789733887,
      "learning_rate": 1.571850304881979e-05,
      "loss": 1.7776,
      "step": 1063700
    },
    {
      "epoch": 41.141663766098155,
      "grad_norm": 19.868812561035156,
      "learning_rate": 1.5715280194918206e-05,
      "loss": 1.8176,
      "step": 1063800
    },
    {
      "epoch": 41.14553119078006,
      "grad_norm": 14.72286319732666,
      "learning_rate": 1.5712057341016617e-05,
      "loss": 1.7832,
      "step": 1063900
    },
    {
      "epoch": 41.14939861546196,
      "grad_norm": 11.732152938842773,
      "learning_rate": 1.5708834487115032e-05,
      "loss": 1.8897,
      "step": 1064000
    },
    {
      "epoch": 41.15326604014387,
      "grad_norm": 10.679049491882324,
      "learning_rate": 1.5705611633213443e-05,
      "loss": 1.8135,
      "step": 1064100
    },
    {
      "epoch": 41.157133464825776,
      "grad_norm": 10.92361831665039,
      "learning_rate": 1.5702388779311858e-05,
      "loss": 1.7767,
      "step": 1064200
    },
    {
      "epoch": 41.161000889507676,
      "grad_norm": 11.442694664001465,
      "learning_rate": 1.569916592541027e-05,
      "loss": 1.7383,
      "step": 1064300
    },
    {
      "epoch": 41.16486831418958,
      "grad_norm": 15.648261070251465,
      "learning_rate": 1.5695943071508684e-05,
      "loss": 1.8766,
      "step": 1064400
    },
    {
      "epoch": 41.16873573887148,
      "grad_norm": 13.996830940246582,
      "learning_rate": 1.5692720217607095e-05,
      "loss": 1.8011,
      "step": 1064500
    },
    {
      "epoch": 41.17260316355339,
      "grad_norm": 12.01259708404541,
      "learning_rate": 1.568949736370551e-05,
      "loss": 1.8907,
      "step": 1064600
    },
    {
      "epoch": 41.1764705882353,
      "grad_norm": 13.116196632385254,
      "learning_rate": 1.568627450980392e-05,
      "loss": 1.7235,
      "step": 1064700
    },
    {
      "epoch": 41.1803380129172,
      "grad_norm": 19.02816390991211,
      "learning_rate": 1.5683051655902336e-05,
      "loss": 1.8175,
      "step": 1064800
    },
    {
      "epoch": 41.184205437599104,
      "grad_norm": 17.111743927001953,
      "learning_rate": 1.5679828802000747e-05,
      "loss": 1.6935,
      "step": 1064900
    },
    {
      "epoch": 41.188072862281004,
      "grad_norm": 10.94215202331543,
      "learning_rate": 1.5676605948099162e-05,
      "loss": 1.7556,
      "step": 1065000
    },
    {
      "epoch": 41.19194028696291,
      "grad_norm": 12.730020523071289,
      "learning_rate": 1.5673383094197573e-05,
      "loss": 1.8127,
      "step": 1065100
    },
    {
      "epoch": 41.19580771164482,
      "grad_norm": 9.611400604248047,
      "learning_rate": 1.5670160240295985e-05,
      "loss": 1.8574,
      "step": 1065200
    },
    {
      "epoch": 41.19967513632672,
      "grad_norm": 9.949460983276367,
      "learning_rate": 1.56669373863944e-05,
      "loss": 1.8272,
      "step": 1065300
    },
    {
      "epoch": 41.203542561008625,
      "grad_norm": 16.139516830444336,
      "learning_rate": 1.5663714532492814e-05,
      "loss": 1.9125,
      "step": 1065400
    },
    {
      "epoch": 41.20740998569053,
      "grad_norm": 15.227160453796387,
      "learning_rate": 1.5660491678591226e-05,
      "loss": 1.7728,
      "step": 1065500
    },
    {
      "epoch": 41.21127741037243,
      "grad_norm": 12.426431655883789,
      "learning_rate": 1.565726882468964e-05,
      "loss": 1.8555,
      "step": 1065600
    },
    {
      "epoch": 41.21514483505434,
      "grad_norm": 11.14634895324707,
      "learning_rate": 1.565404597078805e-05,
      "loss": 1.8332,
      "step": 1065700
    },
    {
      "epoch": 41.21901225973624,
      "grad_norm": 9.598134994506836,
      "learning_rate": 1.5650823116886466e-05,
      "loss": 1.8602,
      "step": 1065800
    },
    {
      "epoch": 41.222879684418146,
      "grad_norm": 14.640144348144531,
      "learning_rate": 1.564760026298488e-05,
      "loss": 1.784,
      "step": 1065900
    },
    {
      "epoch": 41.22674710910005,
      "grad_norm": 12.309109687805176,
      "learning_rate": 1.5644377409083292e-05,
      "loss": 1.7544,
      "step": 1066000
    },
    {
      "epoch": 41.23061453378195,
      "grad_norm": 18.309736251831055,
      "learning_rate": 1.5641154555181707e-05,
      "loss": 1.7844,
      "step": 1066100
    },
    {
      "epoch": 41.23448195846386,
      "grad_norm": 10.882949829101562,
      "learning_rate": 1.563793170128012e-05,
      "loss": 1.8191,
      "step": 1066200
    },
    {
      "epoch": 41.23834938314576,
      "grad_norm": 12.663568496704102,
      "learning_rate": 1.5634708847378533e-05,
      "loss": 1.8141,
      "step": 1066300
    },
    {
      "epoch": 41.24221680782767,
      "grad_norm": 9.650716781616211,
      "learning_rate": 1.5631485993476944e-05,
      "loss": 1.7716,
      "step": 1066400
    },
    {
      "epoch": 41.246084232509574,
      "grad_norm": 15.401712417602539,
      "learning_rate": 1.562826313957536e-05,
      "loss": 1.8367,
      "step": 1066500
    },
    {
      "epoch": 41.249951657191474,
      "grad_norm": 11.791812896728516,
      "learning_rate": 1.562504028567377e-05,
      "loss": 1.9231,
      "step": 1066600
    },
    {
      "epoch": 41.25381908187338,
      "grad_norm": 10.891534805297852,
      "learning_rate": 1.5621817431772185e-05,
      "loss": 1.9524,
      "step": 1066700
    },
    {
      "epoch": 41.25768650655529,
      "grad_norm": 12.91240406036377,
      "learning_rate": 1.5618594577870597e-05,
      "loss": 1.8402,
      "step": 1066800
    },
    {
      "epoch": 41.26155393123719,
      "grad_norm": 10.189949035644531,
      "learning_rate": 1.561537172396901e-05,
      "loss": 1.7977,
      "step": 1066900
    },
    {
      "epoch": 41.265421355919095,
      "grad_norm": 10.571887016296387,
      "learning_rate": 1.5612148870067423e-05,
      "loss": 1.8225,
      "step": 1067000
    },
    {
      "epoch": 41.269288780600995,
      "grad_norm": 12.328506469726562,
      "learning_rate": 1.5608926016165837e-05,
      "loss": 1.8686,
      "step": 1067100
    },
    {
      "epoch": 41.2731562052829,
      "grad_norm": 11.634525299072266,
      "learning_rate": 1.560570316226425e-05,
      "loss": 1.7928,
      "step": 1067200
    },
    {
      "epoch": 41.27702362996481,
      "grad_norm": 9.86803913116455,
      "learning_rate": 1.5602480308362663e-05,
      "loss": 1.8022,
      "step": 1067300
    },
    {
      "epoch": 41.28089105464671,
      "grad_norm": 15.902853012084961,
      "learning_rate": 1.5599257454461075e-05,
      "loss": 1.8253,
      "step": 1067400
    },
    {
      "epoch": 41.284758479328616,
      "grad_norm": 12.492182731628418,
      "learning_rate": 1.559603460055949e-05,
      "loss": 1.8651,
      "step": 1067500
    },
    {
      "epoch": 41.288625904010516,
      "grad_norm": 11.13186264038086,
      "learning_rate": 1.55928117466579e-05,
      "loss": 1.7767,
      "step": 1067600
    },
    {
      "epoch": 41.29249332869242,
      "grad_norm": 11.4904203414917,
      "learning_rate": 1.5589588892756315e-05,
      "loss": 1.789,
      "step": 1067700
    },
    {
      "epoch": 41.29636075337433,
      "grad_norm": 20.06232261657715,
      "learning_rate": 1.5586366038854727e-05,
      "loss": 1.8463,
      "step": 1067800
    },
    {
      "epoch": 41.30022817805623,
      "grad_norm": 10.760637283325195,
      "learning_rate": 1.558314318495314e-05,
      "loss": 1.7614,
      "step": 1067900
    },
    {
      "epoch": 41.30409560273814,
      "grad_norm": 11.926436424255371,
      "learning_rate": 1.5579920331051553e-05,
      "loss": 1.7689,
      "step": 1068000
    },
    {
      "epoch": 41.307963027420044,
      "grad_norm": 11.003117561340332,
      "learning_rate": 1.5576697477149968e-05,
      "loss": 1.7615,
      "step": 1068100
    },
    {
      "epoch": 41.311830452101944,
      "grad_norm": 12.581543922424316,
      "learning_rate": 1.557347462324838e-05,
      "loss": 1.8459,
      "step": 1068200
    },
    {
      "epoch": 41.31569787678385,
      "grad_norm": 11.611719131469727,
      "learning_rate": 1.557025176934679e-05,
      "loss": 1.8801,
      "step": 1068300
    },
    {
      "epoch": 41.31956530146575,
      "grad_norm": 10.627960205078125,
      "learning_rate": 1.5567028915445205e-05,
      "loss": 1.8433,
      "step": 1068400
    },
    {
      "epoch": 41.32343272614766,
      "grad_norm": 8.885927200317383,
      "learning_rate": 1.5563806061543616e-05,
      "loss": 1.9316,
      "step": 1068500
    },
    {
      "epoch": 41.327300150829565,
      "grad_norm": 12.837470054626465,
      "learning_rate": 1.556058320764203e-05,
      "loss": 1.8671,
      "step": 1068600
    },
    {
      "epoch": 41.331167575511465,
      "grad_norm": 12.1805419921875,
      "learning_rate": 1.5557360353740442e-05,
      "loss": 1.7727,
      "step": 1068700
    },
    {
      "epoch": 41.33503500019337,
      "grad_norm": 14.176141738891602,
      "learning_rate": 1.5554137499838857e-05,
      "loss": 1.8995,
      "step": 1068800
    },
    {
      "epoch": 41.33890242487528,
      "grad_norm": 11.263094902038574,
      "learning_rate": 1.5550914645937272e-05,
      "loss": 1.7534,
      "step": 1068900
    },
    {
      "epoch": 41.34276984955718,
      "grad_norm": 10.798972129821777,
      "learning_rate": 1.5547691792035683e-05,
      "loss": 1.8389,
      "step": 1069000
    },
    {
      "epoch": 41.346637274239086,
      "grad_norm": 13.384317398071289,
      "learning_rate": 1.5544468938134098e-05,
      "loss": 1.7425,
      "step": 1069100
    },
    {
      "epoch": 41.350504698920986,
      "grad_norm": 12.096274375915527,
      "learning_rate": 1.5541246084232513e-05,
      "loss": 1.8435,
      "step": 1069200
    },
    {
      "epoch": 41.35437212360289,
      "grad_norm": 10.824934005737305,
      "learning_rate": 1.5538023230330924e-05,
      "loss": 1.8385,
      "step": 1069300
    },
    {
      "epoch": 41.3582395482848,
      "grad_norm": 16.418493270874023,
      "learning_rate": 1.553480037642934e-05,
      "loss": 1.8009,
      "step": 1069400
    },
    {
      "epoch": 41.3621069729667,
      "grad_norm": 13.711257934570312,
      "learning_rate": 1.553157752252775e-05,
      "loss": 1.7697,
      "step": 1069500
    },
    {
      "epoch": 41.36597439764861,
      "grad_norm": 12.57681941986084,
      "learning_rate": 1.5528354668626165e-05,
      "loss": 1.7363,
      "step": 1069600
    },
    {
      "epoch": 41.36984182233051,
      "grad_norm": 14.48735237121582,
      "learning_rate": 1.5525131814724576e-05,
      "loss": 1.8069,
      "step": 1069700
    },
    {
      "epoch": 41.373709247012414,
      "grad_norm": 11.047940254211426,
      "learning_rate": 1.552190896082299e-05,
      "loss": 1.6912,
      "step": 1069800
    },
    {
      "epoch": 41.37757667169432,
      "grad_norm": 13.346138954162598,
      "learning_rate": 1.5518686106921402e-05,
      "loss": 1.8246,
      "step": 1069900
    },
    {
      "epoch": 41.38144409637622,
      "grad_norm": 16.2004337310791,
      "learning_rate": 1.5515463253019817e-05,
      "loss": 1.8531,
      "step": 1070000
    },
    {
      "epoch": 41.38531152105813,
      "grad_norm": 12.984095573425293,
      "learning_rate": 1.5512240399118228e-05,
      "loss": 1.8244,
      "step": 1070100
    },
    {
      "epoch": 41.389178945740035,
      "grad_norm": 11.352849006652832,
      "learning_rate": 1.5509017545216643e-05,
      "loss": 1.9771,
      "step": 1070200
    },
    {
      "epoch": 41.393046370421935,
      "grad_norm": 10.710272789001465,
      "learning_rate": 1.5505794691315054e-05,
      "loss": 1.8449,
      "step": 1070300
    },
    {
      "epoch": 41.39691379510384,
      "grad_norm": 12.529473304748535,
      "learning_rate": 1.550257183741347e-05,
      "loss": 1.8294,
      "step": 1070400
    },
    {
      "epoch": 41.40078121978574,
      "grad_norm": 14.306260108947754,
      "learning_rate": 1.549934898351188e-05,
      "loss": 1.8255,
      "step": 1070500
    },
    {
      "epoch": 41.40464864446765,
      "grad_norm": 13.938885688781738,
      "learning_rate": 1.5496126129610295e-05,
      "loss": 1.7866,
      "step": 1070600
    },
    {
      "epoch": 41.408516069149556,
      "grad_norm": 10.041085243225098,
      "learning_rate": 1.5492903275708706e-05,
      "loss": 1.7239,
      "step": 1070700
    },
    {
      "epoch": 41.412383493831456,
      "grad_norm": 13.494124412536621,
      "learning_rate": 1.548968042180712e-05,
      "loss": 1.8482,
      "step": 1070800
    },
    {
      "epoch": 41.41625091851336,
      "grad_norm": 12.416031837463379,
      "learning_rate": 1.5486457567905532e-05,
      "loss": 1.764,
      "step": 1070900
    },
    {
      "epoch": 41.42011834319526,
      "grad_norm": 10.556174278259277,
      "learning_rate": 1.5483234714003947e-05,
      "loss": 1.8039,
      "step": 1071000
    },
    {
      "epoch": 41.42398576787717,
      "grad_norm": 10.415818214416504,
      "learning_rate": 1.5480011860102358e-05,
      "loss": 1.7789,
      "step": 1071100
    },
    {
      "epoch": 41.42785319255908,
      "grad_norm": 12.045146942138672,
      "learning_rate": 1.547678900620077e-05,
      "loss": 1.8475,
      "step": 1071200
    },
    {
      "epoch": 41.43172061724098,
      "grad_norm": 9.978825569152832,
      "learning_rate": 1.5473566152299184e-05,
      "loss": 1.8239,
      "step": 1071300
    },
    {
      "epoch": 41.435588041922884,
      "grad_norm": 11.139556884765625,
      "learning_rate": 1.5470343298397596e-05,
      "loss": 1.8201,
      "step": 1071400
    },
    {
      "epoch": 41.43945546660479,
      "grad_norm": 13.239496231079102,
      "learning_rate": 1.546712044449601e-05,
      "loss": 1.8642,
      "step": 1071500
    },
    {
      "epoch": 41.44332289128669,
      "grad_norm": 11.61880111694336,
      "learning_rate": 1.5463897590594422e-05,
      "loss": 1.7907,
      "step": 1071600
    },
    {
      "epoch": 41.4471903159686,
      "grad_norm": 12.319828033447266,
      "learning_rate": 1.5460674736692836e-05,
      "loss": 1.9017,
      "step": 1071700
    },
    {
      "epoch": 41.4510577406505,
      "grad_norm": 12.584468841552734,
      "learning_rate": 1.5457451882791248e-05,
      "loss": 1.7712,
      "step": 1071800
    },
    {
      "epoch": 41.454925165332405,
      "grad_norm": 10.042724609375,
      "learning_rate": 1.5454229028889662e-05,
      "loss": 1.8051,
      "step": 1071900
    },
    {
      "epoch": 41.45879259001431,
      "grad_norm": 12.583189010620117,
      "learning_rate": 1.5451006174988074e-05,
      "loss": 1.794,
      "step": 1072000
    },
    {
      "epoch": 41.46266001469621,
      "grad_norm": 11.922518730163574,
      "learning_rate": 1.544778332108649e-05,
      "loss": 1.8092,
      "step": 1072100
    },
    {
      "epoch": 41.46652743937812,
      "grad_norm": 10.536091804504395,
      "learning_rate": 1.54445604671849e-05,
      "loss": 1.8413,
      "step": 1072200
    },
    {
      "epoch": 41.470394864060026,
      "grad_norm": 12.982427597045898,
      "learning_rate": 1.5441337613283315e-05,
      "loss": 1.8438,
      "step": 1072300
    },
    {
      "epoch": 41.474262288741926,
      "grad_norm": 8.495838165283203,
      "learning_rate": 1.543811475938173e-05,
      "loss": 1.8468,
      "step": 1072400
    },
    {
      "epoch": 41.47812971342383,
      "grad_norm": 16.424358367919922,
      "learning_rate": 1.543489190548014e-05,
      "loss": 1.8527,
      "step": 1072500
    },
    {
      "epoch": 41.48199713810573,
      "grad_norm": 11.263035774230957,
      "learning_rate": 1.5431669051578555e-05,
      "loss": 1.8683,
      "step": 1072600
    },
    {
      "epoch": 41.48586456278764,
      "grad_norm": 14.277050971984863,
      "learning_rate": 1.542844619767697e-05,
      "loss": 1.755,
      "step": 1072700
    },
    {
      "epoch": 41.48973198746955,
      "grad_norm": 13.573628425598145,
      "learning_rate": 1.542522334377538e-05,
      "loss": 1.847,
      "step": 1072800
    },
    {
      "epoch": 41.49359941215145,
      "grad_norm": 14.675254821777344,
      "learning_rate": 1.5422000489873796e-05,
      "loss": 1.827,
      "step": 1072900
    },
    {
      "epoch": 41.497466836833354,
      "grad_norm": 9.066751480102539,
      "learning_rate": 1.5418777635972207e-05,
      "loss": 1.7979,
      "step": 1073000
    },
    {
      "epoch": 41.501334261515254,
      "grad_norm": 16.981807708740234,
      "learning_rate": 1.5415554782070622e-05,
      "loss": 1.8807,
      "step": 1073100
    },
    {
      "epoch": 41.50520168619716,
      "grad_norm": 12.674168586730957,
      "learning_rate": 1.5412331928169033e-05,
      "loss": 1.8147,
      "step": 1073200
    },
    {
      "epoch": 41.50906911087907,
      "grad_norm": 12.836400985717773,
      "learning_rate": 1.5409109074267448e-05,
      "loss": 1.7703,
      "step": 1073300
    },
    {
      "epoch": 41.51293653556097,
      "grad_norm": 9.718473434448242,
      "learning_rate": 1.540588622036586e-05,
      "loss": 1.8204,
      "step": 1073400
    },
    {
      "epoch": 41.516803960242875,
      "grad_norm": 12.689367294311523,
      "learning_rate": 1.5402663366464274e-05,
      "loss": 1.8548,
      "step": 1073500
    },
    {
      "epoch": 41.52067138492478,
      "grad_norm": 13.65923023223877,
      "learning_rate": 1.5399440512562686e-05,
      "loss": 1.8564,
      "step": 1073600
    },
    {
      "epoch": 41.52453880960668,
      "grad_norm": 11.561869621276855,
      "learning_rate": 1.53962176586611e-05,
      "loss": 1.7502,
      "step": 1073700
    },
    {
      "epoch": 41.52840623428859,
      "grad_norm": 14.782339096069336,
      "learning_rate": 1.539299480475951e-05,
      "loss": 1.778,
      "step": 1073800
    },
    {
      "epoch": 41.53227365897049,
      "grad_norm": 12.636300086975098,
      "learning_rate": 1.5389771950857926e-05,
      "loss": 1.7971,
      "step": 1073900
    },
    {
      "epoch": 41.536141083652396,
      "grad_norm": 10.723089218139648,
      "learning_rate": 1.5386549096956338e-05,
      "loss": 1.8078,
      "step": 1074000
    },
    {
      "epoch": 41.5400085083343,
      "grad_norm": 11.231550216674805,
      "learning_rate": 1.538332624305475e-05,
      "loss": 1.8816,
      "step": 1074100
    },
    {
      "epoch": 41.5438759330162,
      "grad_norm": 14.609014511108398,
      "learning_rate": 1.5380103389153164e-05,
      "loss": 1.8693,
      "step": 1074200
    },
    {
      "epoch": 41.54774335769811,
      "grad_norm": 9.16672420501709,
      "learning_rate": 1.5376880535251575e-05,
      "loss": 1.8707,
      "step": 1074300
    },
    {
      "epoch": 41.55161078238001,
      "grad_norm": 11.535630226135254,
      "learning_rate": 1.537365768134999e-05,
      "loss": 1.8491,
      "step": 1074400
    },
    {
      "epoch": 41.55547820706192,
      "grad_norm": 10.060559272766113,
      "learning_rate": 1.53704348274484e-05,
      "loss": 1.87,
      "step": 1074500
    },
    {
      "epoch": 41.559345631743824,
      "grad_norm": 11.794349670410156,
      "learning_rate": 1.5367211973546816e-05,
      "loss": 1.8475,
      "step": 1074600
    },
    {
      "epoch": 41.563213056425724,
      "grad_norm": 12.806330680847168,
      "learning_rate": 1.5363989119645227e-05,
      "loss": 1.8407,
      "step": 1074700
    },
    {
      "epoch": 41.56708048110763,
      "grad_norm": 12.629020690917969,
      "learning_rate": 1.5360766265743642e-05,
      "loss": 1.8816,
      "step": 1074800
    },
    {
      "epoch": 41.57094790578954,
      "grad_norm": 11.015843391418457,
      "learning_rate": 1.5357543411842053e-05,
      "loss": 1.8036,
      "step": 1074900
    },
    {
      "epoch": 41.57481533047144,
      "grad_norm": 10.692578315734863,
      "learning_rate": 1.5354320557940468e-05,
      "loss": 1.9147,
      "step": 1075000
    },
    {
      "epoch": 41.578682755153345,
      "grad_norm": 12.480880737304688,
      "learning_rate": 1.535109770403888e-05,
      "loss": 1.8026,
      "step": 1075100
    },
    {
      "epoch": 41.582550179835245,
      "grad_norm": 10.004399299621582,
      "learning_rate": 1.5347874850137294e-05,
      "loss": 1.8645,
      "step": 1075200
    },
    {
      "epoch": 41.58641760451715,
      "grad_norm": 19.138429641723633,
      "learning_rate": 1.5344651996235705e-05,
      "loss": 1.8141,
      "step": 1075300
    },
    {
      "epoch": 41.59028502919906,
      "grad_norm": 11.872580528259277,
      "learning_rate": 1.534142914233412e-05,
      "loss": 1.6425,
      "step": 1075400
    },
    {
      "epoch": 41.59415245388096,
      "grad_norm": 9.529546737670898,
      "learning_rate": 1.533820628843253e-05,
      "loss": 1.8269,
      "step": 1075500
    },
    {
      "epoch": 41.598019878562866,
      "grad_norm": 11.021432876586914,
      "learning_rate": 1.5334983434530946e-05,
      "loss": 1.8395,
      "step": 1075600
    },
    {
      "epoch": 41.601887303244766,
      "grad_norm": 8.766530990600586,
      "learning_rate": 1.5331760580629357e-05,
      "loss": 1.7544,
      "step": 1075700
    },
    {
      "epoch": 41.60575472792667,
      "grad_norm": 12.750459671020508,
      "learning_rate": 1.5328537726727772e-05,
      "loss": 1.8332,
      "step": 1075800
    },
    {
      "epoch": 41.60962215260858,
      "grad_norm": 9.220100402832031,
      "learning_rate": 1.5325314872826187e-05,
      "loss": 1.8271,
      "step": 1075900
    },
    {
      "epoch": 41.61348957729048,
      "grad_norm": 11.817540168762207,
      "learning_rate": 1.5322092018924598e-05,
      "loss": 1.9275,
      "step": 1076000
    },
    {
      "epoch": 41.61735700197239,
      "grad_norm": 13.851109504699707,
      "learning_rate": 1.5318869165023013e-05,
      "loss": 1.8623,
      "step": 1076100
    },
    {
      "epoch": 41.621224426654294,
      "grad_norm": 16.436376571655273,
      "learning_rate": 1.5315646311121428e-05,
      "loss": 1.8112,
      "step": 1076200
    },
    {
      "epoch": 41.625091851336194,
      "grad_norm": 9.695931434631348,
      "learning_rate": 1.531242345721984e-05,
      "loss": 1.9273,
      "step": 1076300
    },
    {
      "epoch": 41.6289592760181,
      "grad_norm": 13.709551811218262,
      "learning_rate": 1.5309200603318254e-05,
      "loss": 1.8135,
      "step": 1076400
    },
    {
      "epoch": 41.6328267007,
      "grad_norm": 13.40331745147705,
      "learning_rate": 1.5305977749416665e-05,
      "loss": 1.8849,
      "step": 1076500
    },
    {
      "epoch": 41.63669412538191,
      "grad_norm": 12.064035415649414,
      "learning_rate": 1.530275489551508e-05,
      "loss": 1.8737,
      "step": 1076600
    },
    {
      "epoch": 41.640561550063815,
      "grad_norm": 13.514311790466309,
      "learning_rate": 1.529953204161349e-05,
      "loss": 1.7936,
      "step": 1076700
    },
    {
      "epoch": 41.644428974745715,
      "grad_norm": 13.681912422180176,
      "learning_rate": 1.5296309187711906e-05,
      "loss": 1.851,
      "step": 1076800
    },
    {
      "epoch": 41.64829639942762,
      "grad_norm": 14.5714693069458,
      "learning_rate": 1.5293086333810317e-05,
      "loss": 1.8219,
      "step": 1076900
    },
    {
      "epoch": 41.65216382410953,
      "grad_norm": 14.172194480895996,
      "learning_rate": 1.528986347990873e-05,
      "loss": 1.7596,
      "step": 1077000
    },
    {
      "epoch": 41.65603124879143,
      "grad_norm": 11.93692684173584,
      "learning_rate": 1.5286640626007143e-05,
      "loss": 1.8165,
      "step": 1077100
    },
    {
      "epoch": 41.659898673473336,
      "grad_norm": 10.43624496459961,
      "learning_rate": 1.5283417772105554e-05,
      "loss": 1.8459,
      "step": 1077200
    },
    {
      "epoch": 41.663766098155236,
      "grad_norm": 12.84118938446045,
      "learning_rate": 1.528019491820397e-05,
      "loss": 1.8408,
      "step": 1077300
    },
    {
      "epoch": 41.66763352283714,
      "grad_norm": 10.347899436950684,
      "learning_rate": 1.527697206430238e-05,
      "loss": 1.755,
      "step": 1077400
    },
    {
      "epoch": 41.67150094751905,
      "grad_norm": 14.60051155090332,
      "learning_rate": 1.5273749210400795e-05,
      "loss": 1.8678,
      "step": 1077500
    },
    {
      "epoch": 41.67536837220095,
      "grad_norm": 11.759657859802246,
      "learning_rate": 1.5270526356499207e-05,
      "loss": 1.8581,
      "step": 1077600
    },
    {
      "epoch": 41.67923579688286,
      "grad_norm": 11.659221649169922,
      "learning_rate": 1.526730350259762e-05,
      "loss": 1.8642,
      "step": 1077700
    },
    {
      "epoch": 41.68310322156476,
      "grad_norm": 11.506546974182129,
      "learning_rate": 1.5264080648696033e-05,
      "loss": 1.8776,
      "step": 1077800
    },
    {
      "epoch": 41.686970646246664,
      "grad_norm": 12.482040405273438,
      "learning_rate": 1.5260857794794447e-05,
      "loss": 1.8309,
      "step": 1077900
    },
    {
      "epoch": 41.69083807092857,
      "grad_norm": 19.17245101928711,
      "learning_rate": 1.5257634940892859e-05,
      "loss": 1.9215,
      "step": 1078000
    },
    {
      "epoch": 41.69470549561047,
      "grad_norm": 14.714896202087402,
      "learning_rate": 1.5254412086991273e-05,
      "loss": 1.9054,
      "step": 1078100
    },
    {
      "epoch": 41.69857292029238,
      "grad_norm": 11.856481552124023,
      "learning_rate": 1.5251189233089685e-05,
      "loss": 1.7382,
      "step": 1078200
    },
    {
      "epoch": 41.702440344974285,
      "grad_norm": 10.648770332336426,
      "learning_rate": 1.52479663791881e-05,
      "loss": 1.9086,
      "step": 1078300
    },
    {
      "epoch": 41.706307769656185,
      "grad_norm": 11.190417289733887,
      "learning_rate": 1.5244743525286512e-05,
      "loss": 1.814,
      "step": 1078400
    },
    {
      "epoch": 41.71017519433809,
      "grad_norm": 10.957610130310059,
      "learning_rate": 1.5241520671384925e-05,
      "loss": 1.8778,
      "step": 1078500
    },
    {
      "epoch": 41.71404261901999,
      "grad_norm": 11.6072998046875,
      "learning_rate": 1.5238297817483338e-05,
      "loss": 1.7662,
      "step": 1078600
    },
    {
      "epoch": 41.7179100437019,
      "grad_norm": 6.984623908996582,
      "learning_rate": 1.5235074963581753e-05,
      "loss": 1.7738,
      "step": 1078700
    },
    {
      "epoch": 41.721777468383806,
      "grad_norm": 16.240840911865234,
      "learning_rate": 1.5231852109680165e-05,
      "loss": 1.905,
      "step": 1078800
    },
    {
      "epoch": 41.725644893065706,
      "grad_norm": 14.952147483825684,
      "learning_rate": 1.522862925577858e-05,
      "loss": 1.8608,
      "step": 1078900
    },
    {
      "epoch": 41.72951231774761,
      "grad_norm": 14.474125862121582,
      "learning_rate": 1.522540640187699e-05,
      "loss": 1.7385,
      "step": 1079000
    },
    {
      "epoch": 41.73337974242951,
      "grad_norm": 10.570352554321289,
      "learning_rate": 1.5222183547975405e-05,
      "loss": 1.8396,
      "step": 1079100
    },
    {
      "epoch": 41.73724716711142,
      "grad_norm": 15.104310989379883,
      "learning_rate": 1.5218960694073817e-05,
      "loss": 1.8461,
      "step": 1079200
    },
    {
      "epoch": 41.74111459179333,
      "grad_norm": 13.055624008178711,
      "learning_rate": 1.5215737840172231e-05,
      "loss": 1.7435,
      "step": 1079300
    },
    {
      "epoch": 41.74498201647523,
      "grad_norm": 9.829203605651855,
      "learning_rate": 1.5212514986270643e-05,
      "loss": 1.801,
      "step": 1079400
    },
    {
      "epoch": 41.748849441157134,
      "grad_norm": 11.894054412841797,
      "learning_rate": 1.5209292132369057e-05,
      "loss": 1.8011,
      "step": 1079500
    },
    {
      "epoch": 41.75271686583904,
      "grad_norm": 12.453972816467285,
      "learning_rate": 1.5206069278467469e-05,
      "loss": 1.906,
      "step": 1079600
    },
    {
      "epoch": 41.75658429052094,
      "grad_norm": 13.67805290222168,
      "learning_rate": 1.5202846424565883e-05,
      "loss": 1.9197,
      "step": 1079700
    },
    {
      "epoch": 41.76045171520285,
      "grad_norm": 14.455769538879395,
      "learning_rate": 1.5199623570664295e-05,
      "loss": 1.8611,
      "step": 1079800
    },
    {
      "epoch": 41.76431913988475,
      "grad_norm": 10.637299537658691,
      "learning_rate": 1.519640071676271e-05,
      "loss": 1.8508,
      "step": 1079900
    },
    {
      "epoch": 41.768186564566655,
      "grad_norm": 14.853017807006836,
      "learning_rate": 1.519317786286112e-05,
      "loss": 1.8174,
      "step": 1080000
    },
    {
      "epoch": 41.77205398924856,
      "grad_norm": 13.431120872497559,
      "learning_rate": 1.5189955008959534e-05,
      "loss": 1.7914,
      "step": 1080100
    },
    {
      "epoch": 41.77592141393046,
      "grad_norm": 12.424055099487305,
      "learning_rate": 1.5186732155057949e-05,
      "loss": 1.7489,
      "step": 1080200
    },
    {
      "epoch": 41.77978883861237,
      "grad_norm": 13.991324424743652,
      "learning_rate": 1.518350930115636e-05,
      "loss": 1.8133,
      "step": 1080300
    },
    {
      "epoch": 41.783656263294276,
      "grad_norm": 12.502633094787598,
      "learning_rate": 1.5180286447254775e-05,
      "loss": 1.7236,
      "step": 1080400
    },
    {
      "epoch": 41.787523687976176,
      "grad_norm": 9.854964256286621,
      "learning_rate": 1.5177063593353186e-05,
      "loss": 1.8236,
      "step": 1080500
    },
    {
      "epoch": 41.79139111265808,
      "grad_norm": 12.040815353393555,
      "learning_rate": 1.51738407394516e-05,
      "loss": 1.8318,
      "step": 1080600
    },
    {
      "epoch": 41.79525853733998,
      "grad_norm": 15.005431175231934,
      "learning_rate": 1.5170617885550012e-05,
      "loss": 1.8012,
      "step": 1080700
    },
    {
      "epoch": 41.79912596202189,
      "grad_norm": 11.737456321716309,
      "learning_rate": 1.5167395031648427e-05,
      "loss": 1.8944,
      "step": 1080800
    },
    {
      "epoch": 41.8029933867038,
      "grad_norm": 14.357063293457031,
      "learning_rate": 1.5164172177746838e-05,
      "loss": 1.9205,
      "step": 1080900
    },
    {
      "epoch": 41.8068608113857,
      "grad_norm": 10.470799446105957,
      "learning_rate": 1.5160949323845253e-05,
      "loss": 1.8181,
      "step": 1081000
    },
    {
      "epoch": 41.810728236067604,
      "grad_norm": 11.20485782623291,
      "learning_rate": 1.5157726469943664e-05,
      "loss": 1.8227,
      "step": 1081100
    },
    {
      "epoch": 41.814595660749504,
      "grad_norm": 10.24208927154541,
      "learning_rate": 1.5154503616042079e-05,
      "loss": 1.8111,
      "step": 1081200
    },
    {
      "epoch": 41.81846308543141,
      "grad_norm": 12.428756713867188,
      "learning_rate": 1.515128076214049e-05,
      "loss": 1.8362,
      "step": 1081300
    },
    {
      "epoch": 41.82233051011332,
      "grad_norm": 10.16605281829834,
      "learning_rate": 1.5148057908238905e-05,
      "loss": 1.8771,
      "step": 1081400
    },
    {
      "epoch": 41.82619793479522,
      "grad_norm": 12.354650497436523,
      "learning_rate": 1.5144835054337316e-05,
      "loss": 1.8348,
      "step": 1081500
    },
    {
      "epoch": 41.830065359477125,
      "grad_norm": 10.736541748046875,
      "learning_rate": 1.5141612200435731e-05,
      "loss": 1.837,
      "step": 1081600
    },
    {
      "epoch": 41.83393278415903,
      "grad_norm": 13.053011894226074,
      "learning_rate": 1.5138389346534144e-05,
      "loss": 1.8173,
      "step": 1081700
    },
    {
      "epoch": 41.83780020884093,
      "grad_norm": 17.727243423461914,
      "learning_rate": 1.5135166492632557e-05,
      "loss": 1.847,
      "step": 1081800
    },
    {
      "epoch": 41.84166763352284,
      "grad_norm": 14.394280433654785,
      "learning_rate": 1.513194363873097e-05,
      "loss": 1.7019,
      "step": 1081900
    },
    {
      "epoch": 41.84553505820474,
      "grad_norm": 7.460387229919434,
      "learning_rate": 1.5128720784829383e-05,
      "loss": 1.7898,
      "step": 1082000
    },
    {
      "epoch": 41.849402482886646,
      "grad_norm": 11.83956241607666,
      "learning_rate": 1.5125497930927796e-05,
      "loss": 1.8292,
      "step": 1082100
    },
    {
      "epoch": 41.85326990756855,
      "grad_norm": 13.195493698120117,
      "learning_rate": 1.512227507702621e-05,
      "loss": 1.7984,
      "step": 1082200
    },
    {
      "epoch": 41.85713733225045,
      "grad_norm": 11.573444366455078,
      "learning_rate": 1.5119052223124622e-05,
      "loss": 1.8034,
      "step": 1082300
    },
    {
      "epoch": 41.86100475693236,
      "grad_norm": 9.984082221984863,
      "learning_rate": 1.5115829369223037e-05,
      "loss": 1.9055,
      "step": 1082400
    },
    {
      "epoch": 41.86487218161426,
      "grad_norm": 11.24524974822998,
      "learning_rate": 1.5112606515321448e-05,
      "loss": 1.7746,
      "step": 1082500
    },
    {
      "epoch": 41.86873960629617,
      "grad_norm": 15.797831535339355,
      "learning_rate": 1.5109383661419863e-05,
      "loss": 1.8574,
      "step": 1082600
    },
    {
      "epoch": 41.872607030978074,
      "grad_norm": 9.769206047058105,
      "learning_rate": 1.5106160807518274e-05,
      "loss": 1.8518,
      "step": 1082700
    },
    {
      "epoch": 41.876474455659974,
      "grad_norm": 16.32369613647461,
      "learning_rate": 1.5102937953616689e-05,
      "loss": 1.884,
      "step": 1082800
    },
    {
      "epoch": 41.88034188034188,
      "grad_norm": 13.25053882598877,
      "learning_rate": 1.50997150997151e-05,
      "loss": 1.782,
      "step": 1082900
    },
    {
      "epoch": 41.88420930502379,
      "grad_norm": 12.662574768066406,
      "learning_rate": 1.5096492245813512e-05,
      "loss": 1.8779,
      "step": 1083000
    },
    {
      "epoch": 41.88807672970569,
      "grad_norm": 9.777012825012207,
      "learning_rate": 1.5093269391911926e-05,
      "loss": 1.7412,
      "step": 1083100
    },
    {
      "epoch": 41.891944154387595,
      "grad_norm": 7.90964412689209,
      "learning_rate": 1.5090046538010338e-05,
      "loss": 1.7713,
      "step": 1083200
    },
    {
      "epoch": 41.895811579069495,
      "grad_norm": 12.1869535446167,
      "learning_rate": 1.5086823684108752e-05,
      "loss": 1.8602,
      "step": 1083300
    },
    {
      "epoch": 41.8996790037514,
      "grad_norm": 12.0824556350708,
      "learning_rate": 1.5083600830207165e-05,
      "loss": 1.8252,
      "step": 1083400
    },
    {
      "epoch": 41.90354642843331,
      "grad_norm": 13.20512580871582,
      "learning_rate": 1.5080377976305578e-05,
      "loss": 1.7527,
      "step": 1083500
    },
    {
      "epoch": 41.90741385311521,
      "grad_norm": 11.427932739257812,
      "learning_rate": 1.5077155122403991e-05,
      "loss": 1.8029,
      "step": 1083600
    },
    {
      "epoch": 41.911281277797116,
      "grad_norm": 12.584090232849121,
      "learning_rate": 1.5073932268502406e-05,
      "loss": 1.8497,
      "step": 1083700
    },
    {
      "epoch": 41.915148702479016,
      "grad_norm": 12.803404808044434,
      "learning_rate": 1.5070709414600817e-05,
      "loss": 1.7656,
      "step": 1083800
    },
    {
      "epoch": 41.91901612716092,
      "grad_norm": 12.008867263793945,
      "learning_rate": 1.5067486560699232e-05,
      "loss": 1.7578,
      "step": 1083900
    },
    {
      "epoch": 41.92288355184283,
      "grad_norm": 8.15572452545166,
      "learning_rate": 1.5064263706797643e-05,
      "loss": 1.7789,
      "step": 1084000
    },
    {
      "epoch": 41.92675097652473,
      "grad_norm": 12.074053764343262,
      "learning_rate": 1.5061040852896058e-05,
      "loss": 1.8597,
      "step": 1084100
    },
    {
      "epoch": 41.93061840120664,
      "grad_norm": 13.918319702148438,
      "learning_rate": 1.505781799899447e-05,
      "loss": 1.7545,
      "step": 1084200
    },
    {
      "epoch": 41.934485825888544,
      "grad_norm": 11.917823791503906,
      "learning_rate": 1.5054595145092884e-05,
      "loss": 1.8088,
      "step": 1084300
    },
    {
      "epoch": 41.938353250570444,
      "grad_norm": 14.496317863464355,
      "learning_rate": 1.5051372291191296e-05,
      "loss": 1.7654,
      "step": 1084400
    },
    {
      "epoch": 41.94222067525235,
      "grad_norm": 10.579719543457031,
      "learning_rate": 1.504814943728971e-05,
      "loss": 1.824,
      "step": 1084500
    },
    {
      "epoch": 41.94608809993425,
      "grad_norm": 12.567193031311035,
      "learning_rate": 1.5044926583388122e-05,
      "loss": 1.8258,
      "step": 1084600
    },
    {
      "epoch": 41.94995552461616,
      "grad_norm": 10.66243839263916,
      "learning_rate": 1.5041703729486536e-05,
      "loss": 1.8123,
      "step": 1084700
    },
    {
      "epoch": 41.953822949298065,
      "grad_norm": 14.609469413757324,
      "learning_rate": 1.5038480875584948e-05,
      "loss": 1.7746,
      "step": 1084800
    },
    {
      "epoch": 41.957690373979965,
      "grad_norm": 10.27708911895752,
      "learning_rate": 1.5035258021683362e-05,
      "loss": 1.8688,
      "step": 1084900
    },
    {
      "epoch": 41.96155779866187,
      "grad_norm": 12.105451583862305,
      "learning_rate": 1.5032035167781774e-05,
      "loss": 1.8352,
      "step": 1085000
    },
    {
      "epoch": 41.96542522334378,
      "grad_norm": 12.147709846496582,
      "learning_rate": 1.5028812313880188e-05,
      "loss": 1.8313,
      "step": 1085100
    },
    {
      "epoch": 41.96929264802568,
      "grad_norm": 10.519482612609863,
      "learning_rate": 1.5025589459978601e-05,
      "loss": 1.9113,
      "step": 1085200
    },
    {
      "epoch": 41.973160072707586,
      "grad_norm": 10.083658218383789,
      "learning_rate": 1.5022366606077014e-05,
      "loss": 1.9077,
      "step": 1085300
    },
    {
      "epoch": 41.977027497389486,
      "grad_norm": 11.339488983154297,
      "learning_rate": 1.5019143752175428e-05,
      "loss": 1.6985,
      "step": 1085400
    },
    {
      "epoch": 41.98089492207139,
      "grad_norm": 11.604811668395996,
      "learning_rate": 1.5015920898273842e-05,
      "loss": 1.8111,
      "step": 1085500
    },
    {
      "epoch": 41.9847623467533,
      "grad_norm": 15.72024917602539,
      "learning_rate": 1.5012698044372254e-05,
      "loss": 1.8722,
      "step": 1085600
    },
    {
      "epoch": 41.9886297714352,
      "grad_norm": 9.397871971130371,
      "learning_rate": 1.5009475190470668e-05,
      "loss": 1.8169,
      "step": 1085700
    },
    {
      "epoch": 41.99249719611711,
      "grad_norm": 12.93149471282959,
      "learning_rate": 1.500625233656908e-05,
      "loss": 1.878,
      "step": 1085800
    },
    {
      "epoch": 41.99636462079901,
      "grad_norm": 12.927624702453613,
      "learning_rate": 1.5003029482667491e-05,
      "loss": 1.8574,
      "step": 1085900
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.7619301080703735,
      "eval_runtime": 5.9032,
      "eval_samples_per_second": 230.551,
      "eval_steps_per_second": 230.551,
      "step": 1085994
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.611518144607544,
      "eval_runtime": 113.7994,
      "eval_samples_per_second": 227.216,
      "eval_steps_per_second": 227.216,
      "step": 1085994
    },
    {
      "epoch": 42.000232045480914,
      "grad_norm": 9.876535415649414,
      "learning_rate": 1.4999806628765906e-05,
      "loss": 1.7453,
      "step": 1086000
    },
    {
      "epoch": 42.00409947016282,
      "grad_norm": 11.838521003723145,
      "learning_rate": 1.4996583774864317e-05,
      "loss": 1.7769,
      "step": 1086100
    },
    {
      "epoch": 42.00796689484472,
      "grad_norm": 13.282294273376465,
      "learning_rate": 1.4993360920962732e-05,
      "loss": 1.682,
      "step": 1086200
    },
    {
      "epoch": 42.01183431952663,
      "grad_norm": 12.735636711120605,
      "learning_rate": 1.4990138067061143e-05,
      "loss": 1.7943,
      "step": 1086300
    },
    {
      "epoch": 42.015701744208535,
      "grad_norm": 13.102680206298828,
      "learning_rate": 1.4986915213159558e-05,
      "loss": 1.8216,
      "step": 1086400
    },
    {
      "epoch": 42.019569168890435,
      "grad_norm": 11.733197212219238,
      "learning_rate": 1.4983692359257969e-05,
      "loss": 1.8474,
      "step": 1086500
    },
    {
      "epoch": 42.02343659357234,
      "grad_norm": 8.863201141357422,
      "learning_rate": 1.4980469505356384e-05,
      "loss": 1.7678,
      "step": 1086600
    },
    {
      "epoch": 42.02730401825424,
      "grad_norm": 12.506658554077148,
      "learning_rate": 1.4977246651454797e-05,
      "loss": 1.7974,
      "step": 1086700
    },
    {
      "epoch": 42.03117144293615,
      "grad_norm": 12.890287399291992,
      "learning_rate": 1.497402379755321e-05,
      "loss": 1.8252,
      "step": 1086800
    },
    {
      "epoch": 42.035038867618056,
      "grad_norm": 13.752116203308105,
      "learning_rate": 1.4970800943651623e-05,
      "loss": 1.7489,
      "step": 1086900
    },
    {
      "epoch": 42.038906292299956,
      "grad_norm": 14.39891529083252,
      "learning_rate": 1.4967578089750036e-05,
      "loss": 1.7495,
      "step": 1087000
    },
    {
      "epoch": 42.04277371698186,
      "grad_norm": 11.927592277526855,
      "learning_rate": 1.4964355235848449e-05,
      "loss": 1.8023,
      "step": 1087100
    },
    {
      "epoch": 42.04664114166376,
      "grad_norm": 10.574907302856445,
      "learning_rate": 1.4961132381946864e-05,
      "loss": 1.8438,
      "step": 1087200
    },
    {
      "epoch": 42.05050856634567,
      "grad_norm": 14.39466381072998,
      "learning_rate": 1.4957909528045275e-05,
      "loss": 1.8002,
      "step": 1087300
    },
    {
      "epoch": 42.05437599102758,
      "grad_norm": 13.278270721435547,
      "learning_rate": 1.495468667414369e-05,
      "loss": 1.8338,
      "step": 1087400
    },
    {
      "epoch": 42.05824341570948,
      "grad_norm": 11.054939270019531,
      "learning_rate": 1.4951463820242101e-05,
      "loss": 1.7636,
      "step": 1087500
    },
    {
      "epoch": 42.062110840391384,
      "grad_norm": 10.931958198547363,
      "learning_rate": 1.4948240966340516e-05,
      "loss": 1.8431,
      "step": 1087600
    },
    {
      "epoch": 42.06597826507329,
      "grad_norm": 16.48865509033203,
      "learning_rate": 1.4945018112438927e-05,
      "loss": 1.8809,
      "step": 1087700
    },
    {
      "epoch": 42.06984568975519,
      "grad_norm": 10.25178050994873,
      "learning_rate": 1.4941795258537342e-05,
      "loss": 1.775,
      "step": 1087800
    },
    {
      "epoch": 42.0737131144371,
      "grad_norm": 12.798872947692871,
      "learning_rate": 1.4938572404635753e-05,
      "loss": 1.8127,
      "step": 1087900
    },
    {
      "epoch": 42.077580539119,
      "grad_norm": 17.690589904785156,
      "learning_rate": 1.4935349550734168e-05,
      "loss": 1.8404,
      "step": 1088000
    },
    {
      "epoch": 42.081447963800905,
      "grad_norm": 9.98873519897461,
      "learning_rate": 1.493212669683258e-05,
      "loss": 1.8544,
      "step": 1088100
    },
    {
      "epoch": 42.08531538848281,
      "grad_norm": 12.709820747375488,
      "learning_rate": 1.4928903842930994e-05,
      "loss": 1.9055,
      "step": 1088200
    },
    {
      "epoch": 42.08918281316471,
      "grad_norm": 14.48775577545166,
      "learning_rate": 1.4925680989029405e-05,
      "loss": 1.7709,
      "step": 1088300
    },
    {
      "epoch": 42.09305023784662,
      "grad_norm": 12.865303993225098,
      "learning_rate": 1.492245813512782e-05,
      "loss": 1.8283,
      "step": 1088400
    },
    {
      "epoch": 42.096917662528526,
      "grad_norm": 13.189101219177246,
      "learning_rate": 1.4919235281226231e-05,
      "loss": 1.8564,
      "step": 1088500
    },
    {
      "epoch": 42.100785087210426,
      "grad_norm": 15.088991165161133,
      "learning_rate": 1.4916012427324646e-05,
      "loss": 1.7901,
      "step": 1088600
    },
    {
      "epoch": 42.10465251189233,
      "grad_norm": 13.301806449890137,
      "learning_rate": 1.4912789573423059e-05,
      "loss": 1.857,
      "step": 1088700
    },
    {
      "epoch": 42.10851993657423,
      "grad_norm": 10.49465560913086,
      "learning_rate": 1.490956671952147e-05,
      "loss": 1.8409,
      "step": 1088800
    },
    {
      "epoch": 42.11238736125614,
      "grad_norm": 12.292035102844238,
      "learning_rate": 1.4906343865619885e-05,
      "loss": 1.784,
      "step": 1088900
    },
    {
      "epoch": 42.11625478593805,
      "grad_norm": 12.197016716003418,
      "learning_rate": 1.4903121011718296e-05,
      "loss": 1.7901,
      "step": 1089000
    },
    {
      "epoch": 42.12012221061995,
      "grad_norm": 11.18846321105957,
      "learning_rate": 1.4899898157816711e-05,
      "loss": 1.9176,
      "step": 1089100
    },
    {
      "epoch": 42.123989635301854,
      "grad_norm": 14.967306137084961,
      "learning_rate": 1.4896675303915122e-05,
      "loss": 1.8237,
      "step": 1089200
    },
    {
      "epoch": 42.127857059983754,
      "grad_norm": 8.351396560668945,
      "learning_rate": 1.4893452450013537e-05,
      "loss": 1.7838,
      "step": 1089300
    },
    {
      "epoch": 42.13172448466566,
      "grad_norm": 10.74029541015625,
      "learning_rate": 1.4890229596111948e-05,
      "loss": 1.7612,
      "step": 1089400
    },
    {
      "epoch": 42.13559190934757,
      "grad_norm": 12.723692893981934,
      "learning_rate": 1.4887006742210363e-05,
      "loss": 1.8328,
      "step": 1089500
    },
    {
      "epoch": 42.13945933402947,
      "grad_norm": 14.066731452941895,
      "learning_rate": 1.4883783888308775e-05,
      "loss": 1.8175,
      "step": 1089600
    },
    {
      "epoch": 42.143326758711375,
      "grad_norm": 12.002808570861816,
      "learning_rate": 1.488056103440719e-05,
      "loss": 1.7675,
      "step": 1089700
    },
    {
      "epoch": 42.14719418339328,
      "grad_norm": 12.407020568847656,
      "learning_rate": 1.48773381805056e-05,
      "loss": 1.813,
      "step": 1089800
    },
    {
      "epoch": 42.15106160807518,
      "grad_norm": 14.134613037109375,
      "learning_rate": 1.4874115326604015e-05,
      "loss": 1.8169,
      "step": 1089900
    },
    {
      "epoch": 42.15492903275709,
      "grad_norm": 8.601932525634766,
      "learning_rate": 1.4870892472702427e-05,
      "loss": 1.8617,
      "step": 1090000
    },
    {
      "epoch": 42.15879645743899,
      "grad_norm": 13.390348434448242,
      "learning_rate": 1.4867669618800841e-05,
      "loss": 1.8948,
      "step": 1090100
    },
    {
      "epoch": 42.162663882120896,
      "grad_norm": 11.076297760009766,
      "learning_rate": 1.4864446764899254e-05,
      "loss": 1.9271,
      "step": 1090200
    },
    {
      "epoch": 42.1665313068028,
      "grad_norm": 13.425555229187012,
      "learning_rate": 1.4861223910997667e-05,
      "loss": 1.7307,
      "step": 1090300
    },
    {
      "epoch": 42.1703987314847,
      "grad_norm": 10.839312553405762,
      "learning_rate": 1.485800105709608e-05,
      "loss": 1.8269,
      "step": 1090400
    },
    {
      "epoch": 42.17426615616661,
      "grad_norm": 7.551508903503418,
      "learning_rate": 1.4854778203194495e-05,
      "loss": 1.8842,
      "step": 1090500
    },
    {
      "epoch": 42.17813358084851,
      "grad_norm": 14.915639877319336,
      "learning_rate": 1.4851555349292906e-05,
      "loss": 1.8156,
      "step": 1090600
    },
    {
      "epoch": 42.18200100553042,
      "grad_norm": 13.729223251342773,
      "learning_rate": 1.4848332495391321e-05,
      "loss": 1.8969,
      "step": 1090700
    },
    {
      "epoch": 42.185868430212324,
      "grad_norm": 14.00650405883789,
      "learning_rate": 1.4845109641489733e-05,
      "loss": 1.7199,
      "step": 1090800
    },
    {
      "epoch": 42.189735854894224,
      "grad_norm": 10.87214469909668,
      "learning_rate": 1.4841886787588147e-05,
      "loss": 1.7925,
      "step": 1090900
    },
    {
      "epoch": 42.19360327957613,
      "grad_norm": 10.761136054992676,
      "learning_rate": 1.4838663933686559e-05,
      "loss": 1.7213,
      "step": 1091000
    },
    {
      "epoch": 42.19747070425804,
      "grad_norm": 11.61898136138916,
      "learning_rate": 1.4835441079784973e-05,
      "loss": 1.8509,
      "step": 1091100
    },
    {
      "epoch": 42.20133812893994,
      "grad_norm": 14.372535705566406,
      "learning_rate": 1.4832218225883385e-05,
      "loss": 1.9125,
      "step": 1091200
    },
    {
      "epoch": 42.205205553621845,
      "grad_norm": 12.264824867248535,
      "learning_rate": 1.48289953719818e-05,
      "loss": 1.7945,
      "step": 1091300
    },
    {
      "epoch": 42.209072978303745,
      "grad_norm": 6.775186061859131,
      "learning_rate": 1.482577251808021e-05,
      "loss": 1.7848,
      "step": 1091400
    },
    {
      "epoch": 42.21294040298565,
      "grad_norm": 13.62020492553711,
      "learning_rate": 1.4822549664178625e-05,
      "loss": 1.8096,
      "step": 1091500
    },
    {
      "epoch": 42.21680782766756,
      "grad_norm": 11.955942153930664,
      "learning_rate": 1.4819326810277037e-05,
      "loss": 1.8379,
      "step": 1091600
    },
    {
      "epoch": 42.22067525234946,
      "grad_norm": 13.208829879760742,
      "learning_rate": 1.4816103956375451e-05,
      "loss": 1.7675,
      "step": 1091700
    },
    {
      "epoch": 42.224542677031366,
      "grad_norm": 9.120301246643066,
      "learning_rate": 1.4812881102473863e-05,
      "loss": 1.8347,
      "step": 1091800
    },
    {
      "epoch": 42.228410101713266,
      "grad_norm": 10.282397270202637,
      "learning_rate": 1.4809658248572276e-05,
      "loss": 1.9144,
      "step": 1091900
    },
    {
      "epoch": 42.23227752639517,
      "grad_norm": 10.171262741088867,
      "learning_rate": 1.4806435394670689e-05,
      "loss": 1.8943,
      "step": 1092000
    },
    {
      "epoch": 42.23614495107708,
      "grad_norm": 11.021417617797852,
      "learning_rate": 1.4803212540769102e-05,
      "loss": 1.8018,
      "step": 1092100
    },
    {
      "epoch": 42.24001237575898,
      "grad_norm": 15.641252517700195,
      "learning_rate": 1.4799989686867517e-05,
      "loss": 1.7768,
      "step": 1092200
    },
    {
      "epoch": 42.24387980044089,
      "grad_norm": 12.258211135864258,
      "learning_rate": 1.4796766832965928e-05,
      "loss": 1.7184,
      "step": 1092300
    },
    {
      "epoch": 42.247747225122794,
      "grad_norm": 12.424699783325195,
      "learning_rate": 1.4793543979064343e-05,
      "loss": 1.7555,
      "step": 1092400
    },
    {
      "epoch": 42.251614649804694,
      "grad_norm": 12.454568862915039,
      "learning_rate": 1.4790321125162754e-05,
      "loss": 1.8412,
      "step": 1092500
    },
    {
      "epoch": 42.2554820744866,
      "grad_norm": 15.078624725341797,
      "learning_rate": 1.4787098271261169e-05,
      "loss": 1.762,
      "step": 1092600
    },
    {
      "epoch": 42.2593494991685,
      "grad_norm": 15.665332794189453,
      "learning_rate": 1.478387541735958e-05,
      "loss": 1.7318,
      "step": 1092700
    },
    {
      "epoch": 42.26321692385041,
      "grad_norm": 12.086847305297852,
      "learning_rate": 1.4780652563457995e-05,
      "loss": 1.8773,
      "step": 1092800
    },
    {
      "epoch": 42.267084348532315,
      "grad_norm": 11.255830764770508,
      "learning_rate": 1.4777429709556406e-05,
      "loss": 1.8715,
      "step": 1092900
    },
    {
      "epoch": 42.270951773214215,
      "grad_norm": 12.789112091064453,
      "learning_rate": 1.477420685565482e-05,
      "loss": 1.7632,
      "step": 1093000
    },
    {
      "epoch": 42.27481919789612,
      "grad_norm": 12.323610305786133,
      "learning_rate": 1.4770984001753232e-05,
      "loss": 1.822,
      "step": 1093100
    },
    {
      "epoch": 42.27868662257803,
      "grad_norm": 11.878466606140137,
      "learning_rate": 1.4767761147851647e-05,
      "loss": 1.8155,
      "step": 1093200
    },
    {
      "epoch": 42.28255404725993,
      "grad_norm": 12.99360466003418,
      "learning_rate": 1.4764538293950058e-05,
      "loss": 1.7925,
      "step": 1093300
    },
    {
      "epoch": 42.286421471941836,
      "grad_norm": 12.26793098449707,
      "learning_rate": 1.4761315440048473e-05,
      "loss": 1.8687,
      "step": 1093400
    },
    {
      "epoch": 42.290288896623736,
      "grad_norm": 21.32300567626953,
      "learning_rate": 1.4758092586146884e-05,
      "loss": 1.8383,
      "step": 1093500
    },
    {
      "epoch": 42.29415632130564,
      "grad_norm": 13.143667221069336,
      "learning_rate": 1.4754869732245299e-05,
      "loss": 1.8563,
      "step": 1093600
    },
    {
      "epoch": 42.29802374598755,
      "grad_norm": 13.573198318481445,
      "learning_rate": 1.4751646878343712e-05,
      "loss": 1.7925,
      "step": 1093700
    },
    {
      "epoch": 42.30189117066945,
      "grad_norm": 11.830368995666504,
      "learning_rate": 1.4748424024442125e-05,
      "loss": 1.724,
      "step": 1093800
    },
    {
      "epoch": 42.30575859535136,
      "grad_norm": 15.468623161315918,
      "learning_rate": 1.4745201170540538e-05,
      "loss": 1.7305,
      "step": 1093900
    },
    {
      "epoch": 42.30962602003326,
      "grad_norm": 12.982250213623047,
      "learning_rate": 1.4741978316638953e-05,
      "loss": 1.7265,
      "step": 1094000
    },
    {
      "epoch": 42.313493444715164,
      "grad_norm": 11.095124244689941,
      "learning_rate": 1.4738755462737364e-05,
      "loss": 1.8053,
      "step": 1094100
    },
    {
      "epoch": 42.31736086939707,
      "grad_norm": 15.408675193786621,
      "learning_rate": 1.4735532608835779e-05,
      "loss": 1.8048,
      "step": 1094200
    },
    {
      "epoch": 42.32122829407897,
      "grad_norm": 13.480985641479492,
      "learning_rate": 1.473230975493419e-05,
      "loss": 1.6945,
      "step": 1094300
    },
    {
      "epoch": 42.32509571876088,
      "grad_norm": 12.659574508666992,
      "learning_rate": 1.4729086901032605e-05,
      "loss": 1.8212,
      "step": 1094400
    },
    {
      "epoch": 42.328963143442785,
      "grad_norm": 11.254430770874023,
      "learning_rate": 1.4725864047131016e-05,
      "loss": 1.7871,
      "step": 1094500
    },
    {
      "epoch": 42.332830568124685,
      "grad_norm": 13.751493453979492,
      "learning_rate": 1.472264119322943e-05,
      "loss": 1.7597,
      "step": 1094600
    },
    {
      "epoch": 42.33669799280659,
      "grad_norm": 11.981165885925293,
      "learning_rate": 1.4719418339327842e-05,
      "loss": 1.8614,
      "step": 1094700
    },
    {
      "epoch": 42.34056541748849,
      "grad_norm": 11.037240982055664,
      "learning_rate": 1.4716195485426254e-05,
      "loss": 1.7887,
      "step": 1094800
    },
    {
      "epoch": 42.3444328421704,
      "grad_norm": 13.00942611694336,
      "learning_rate": 1.4712972631524668e-05,
      "loss": 1.8605,
      "step": 1094900
    },
    {
      "epoch": 42.348300266852306,
      "grad_norm": 9.671732902526855,
      "learning_rate": 1.470974977762308e-05,
      "loss": 1.849,
      "step": 1095000
    },
    {
      "epoch": 42.352167691534206,
      "grad_norm": 11.109980583190918,
      "learning_rate": 1.4706526923721494e-05,
      "loss": 1.9152,
      "step": 1095100
    },
    {
      "epoch": 42.35603511621611,
      "grad_norm": 9.570927619934082,
      "learning_rate": 1.4703304069819907e-05,
      "loss": 1.7732,
      "step": 1095200
    },
    {
      "epoch": 42.35990254089801,
      "grad_norm": 12.167465209960938,
      "learning_rate": 1.470008121591832e-05,
      "loss": 1.6842,
      "step": 1095300
    },
    {
      "epoch": 42.36376996557992,
      "grad_norm": 13.737197875976562,
      "learning_rate": 1.4696858362016733e-05,
      "loss": 1.7956,
      "step": 1095400
    },
    {
      "epoch": 42.36763739026183,
      "grad_norm": 10.287358283996582,
      "learning_rate": 1.4693635508115148e-05,
      "loss": 1.852,
      "step": 1095500
    },
    {
      "epoch": 42.37150481494373,
      "grad_norm": 13.749885559082031,
      "learning_rate": 1.469041265421356e-05,
      "loss": 1.8097,
      "step": 1095600
    },
    {
      "epoch": 42.375372239625634,
      "grad_norm": 11.574505805969238,
      "learning_rate": 1.4687189800311974e-05,
      "loss": 1.867,
      "step": 1095700
    },
    {
      "epoch": 42.37923966430754,
      "grad_norm": 12.1244535446167,
      "learning_rate": 1.4683966946410385e-05,
      "loss": 1.8049,
      "step": 1095800
    },
    {
      "epoch": 42.38310708898944,
      "grad_norm": 11.718962669372559,
      "learning_rate": 1.46807440925088e-05,
      "loss": 1.8105,
      "step": 1095900
    },
    {
      "epoch": 42.38697451367135,
      "grad_norm": 16.635623931884766,
      "learning_rate": 1.4677521238607211e-05,
      "loss": 1.7764,
      "step": 1096000
    },
    {
      "epoch": 42.39084193835325,
      "grad_norm": 14.329119682312012,
      "learning_rate": 1.4674298384705626e-05,
      "loss": 1.9215,
      "step": 1096100
    },
    {
      "epoch": 42.394709363035155,
      "grad_norm": 13.593501091003418,
      "learning_rate": 1.4671075530804038e-05,
      "loss": 1.7748,
      "step": 1096200
    },
    {
      "epoch": 42.39857678771706,
      "grad_norm": 8.89964485168457,
      "learning_rate": 1.4667852676902452e-05,
      "loss": 1.7027,
      "step": 1096300
    },
    {
      "epoch": 42.40244421239896,
      "grad_norm": 11.782944679260254,
      "learning_rate": 1.4664629823000864e-05,
      "loss": 1.7417,
      "step": 1096400
    },
    {
      "epoch": 42.40631163708087,
      "grad_norm": 13.85453987121582,
      "learning_rate": 1.4661406969099278e-05,
      "loss": 1.7766,
      "step": 1096500
    },
    {
      "epoch": 42.410179061762776,
      "grad_norm": 10.483623504638672,
      "learning_rate": 1.465818411519769e-05,
      "loss": 1.8064,
      "step": 1096600
    },
    {
      "epoch": 42.414046486444676,
      "grad_norm": 14.119451522827148,
      "learning_rate": 1.4654961261296104e-05,
      "loss": 1.8331,
      "step": 1096700
    },
    {
      "epoch": 42.41791391112658,
      "grad_norm": 15.277432441711426,
      "learning_rate": 1.4651738407394516e-05,
      "loss": 1.8342,
      "step": 1096800
    },
    {
      "epoch": 42.42178133580848,
      "grad_norm": 11.051057815551758,
      "learning_rate": 1.464851555349293e-05,
      "loss": 1.8677,
      "step": 1096900
    },
    {
      "epoch": 42.42564876049039,
      "grad_norm": 11.937745094299316,
      "learning_rate": 1.4645292699591342e-05,
      "loss": 1.8176,
      "step": 1097000
    },
    {
      "epoch": 42.4295161851723,
      "grad_norm": 12.780016899108887,
      "learning_rate": 1.4642069845689756e-05,
      "loss": 1.8515,
      "step": 1097100
    },
    {
      "epoch": 42.4333836098542,
      "grad_norm": 14.059711456298828,
      "learning_rate": 1.463884699178817e-05,
      "loss": 1.7867,
      "step": 1097200
    },
    {
      "epoch": 42.437251034536104,
      "grad_norm": 12.860601425170898,
      "learning_rate": 1.4635624137886583e-05,
      "loss": 1.8404,
      "step": 1097300
    },
    {
      "epoch": 42.441118459218,
      "grad_norm": 11.390071868896484,
      "learning_rate": 1.4632401283984996e-05,
      "loss": 1.7651,
      "step": 1097400
    },
    {
      "epoch": 42.44498588389991,
      "grad_norm": 12.918161392211914,
      "learning_rate": 1.462917843008341e-05,
      "loss": 1.8167,
      "step": 1097500
    },
    {
      "epoch": 42.44885330858182,
      "grad_norm": 11.686779022216797,
      "learning_rate": 1.4625955576181822e-05,
      "loss": 1.8635,
      "step": 1097600
    },
    {
      "epoch": 42.45272073326372,
      "grad_norm": 11.377562522888184,
      "learning_rate": 1.4622732722280233e-05,
      "loss": 1.8611,
      "step": 1097700
    },
    {
      "epoch": 42.456588157945625,
      "grad_norm": 10.387422561645508,
      "learning_rate": 1.4619509868378648e-05,
      "loss": 1.7335,
      "step": 1097800
    },
    {
      "epoch": 42.46045558262753,
      "grad_norm": 11.686210632324219,
      "learning_rate": 1.4616287014477059e-05,
      "loss": 1.8291,
      "step": 1097900
    },
    {
      "epoch": 42.46432300730943,
      "grad_norm": 10.80003833770752,
      "learning_rate": 1.4613064160575474e-05,
      "loss": 1.831,
      "step": 1098000
    },
    {
      "epoch": 42.46819043199134,
      "grad_norm": 11.135289192199707,
      "learning_rate": 1.4609841306673885e-05,
      "loss": 1.8337,
      "step": 1098100
    },
    {
      "epoch": 42.47205785667324,
      "grad_norm": 16.08095359802246,
      "learning_rate": 1.46066184527723e-05,
      "loss": 1.8505,
      "step": 1098200
    },
    {
      "epoch": 42.475925281355146,
      "grad_norm": 10.736000061035156,
      "learning_rate": 1.4603395598870711e-05,
      "loss": 1.8351,
      "step": 1098300
    },
    {
      "epoch": 42.47979270603705,
      "grad_norm": 10.45528507232666,
      "learning_rate": 1.4600172744969126e-05,
      "loss": 1.8807,
      "step": 1098400
    },
    {
      "epoch": 42.48366013071895,
      "grad_norm": 11.873228073120117,
      "learning_rate": 1.4596949891067537e-05,
      "loss": 1.7884,
      "step": 1098500
    },
    {
      "epoch": 42.48752755540086,
      "grad_norm": 14.569561004638672,
      "learning_rate": 1.4593727037165952e-05,
      "loss": 1.8032,
      "step": 1098600
    },
    {
      "epoch": 42.49139498008276,
      "grad_norm": 14.052284240722656,
      "learning_rate": 1.4590504183264365e-05,
      "loss": 1.8624,
      "step": 1098700
    },
    {
      "epoch": 42.49526240476467,
      "grad_norm": 12.507033348083496,
      "learning_rate": 1.4587281329362778e-05,
      "loss": 1.8983,
      "step": 1098800
    },
    {
      "epoch": 42.499129829446574,
      "grad_norm": 11.155790328979492,
      "learning_rate": 1.4584058475461191e-05,
      "loss": 1.9167,
      "step": 1098900
    },
    {
      "epoch": 42.50299725412847,
      "grad_norm": 11.336925506591797,
      "learning_rate": 1.4580835621559606e-05,
      "loss": 1.7409,
      "step": 1099000
    },
    {
      "epoch": 42.50686467881038,
      "grad_norm": 11.580404281616211,
      "learning_rate": 1.4577612767658017e-05,
      "loss": 1.783,
      "step": 1099100
    },
    {
      "epoch": 42.51073210349229,
      "grad_norm": 11.326536178588867,
      "learning_rate": 1.4574389913756432e-05,
      "loss": 1.6837,
      "step": 1099200
    },
    {
      "epoch": 42.51459952817419,
      "grad_norm": 16.55535888671875,
      "learning_rate": 1.4571167059854843e-05,
      "loss": 1.8282,
      "step": 1099300
    },
    {
      "epoch": 42.518466952856095,
      "grad_norm": 10.111615180969238,
      "learning_rate": 1.4567944205953258e-05,
      "loss": 1.7806,
      "step": 1099400
    },
    {
      "epoch": 42.522334377537994,
      "grad_norm": 11.924839973449707,
      "learning_rate": 1.4564721352051669e-05,
      "loss": 1.7456,
      "step": 1099500
    },
    {
      "epoch": 42.5262018022199,
      "grad_norm": 13.541165351867676,
      "learning_rate": 1.4561498498150084e-05,
      "loss": 1.8072,
      "step": 1099600
    },
    {
      "epoch": 42.53006922690181,
      "grad_norm": 15.382771492004395,
      "learning_rate": 1.4558275644248495e-05,
      "loss": 1.8539,
      "step": 1099700
    },
    {
      "epoch": 42.53393665158371,
      "grad_norm": 9.8903169631958,
      "learning_rate": 1.455505279034691e-05,
      "loss": 1.8282,
      "step": 1099800
    },
    {
      "epoch": 42.537804076265616,
      "grad_norm": 12.516077995300293,
      "learning_rate": 1.4551829936445321e-05,
      "loss": 1.7939,
      "step": 1099900
    },
    {
      "epoch": 42.541671500947515,
      "grad_norm": 16.738319396972656,
      "learning_rate": 1.4548607082543736e-05,
      "loss": 1.7921,
      "step": 1100000
    },
    {
      "epoch": 42.54553892562942,
      "grad_norm": 11.562106132507324,
      "learning_rate": 1.4545384228642147e-05,
      "loss": 1.8261,
      "step": 1100100
    },
    {
      "epoch": 42.54940635031133,
      "grad_norm": 8.841283798217773,
      "learning_rate": 1.4542161374740562e-05,
      "loss": 1.8029,
      "step": 1100200
    },
    {
      "epoch": 42.55327377499323,
      "grad_norm": 12.378752708435059,
      "learning_rate": 1.4538938520838973e-05,
      "loss": 1.8826,
      "step": 1100300
    },
    {
      "epoch": 42.55714119967514,
      "grad_norm": 8.820769309997559,
      "learning_rate": 1.4535715666937388e-05,
      "loss": 1.8353,
      "step": 1100400
    },
    {
      "epoch": 42.561008624357044,
      "grad_norm": 13.310196876525879,
      "learning_rate": 1.45324928130358e-05,
      "loss": 1.8593,
      "step": 1100500
    },
    {
      "epoch": 42.56487604903894,
      "grad_norm": 13.059688568115234,
      "learning_rate": 1.4529269959134212e-05,
      "loss": 1.8428,
      "step": 1100600
    },
    {
      "epoch": 42.56874347372085,
      "grad_norm": 9.604145050048828,
      "learning_rate": 1.4526047105232627e-05,
      "loss": 1.8014,
      "step": 1100700
    },
    {
      "epoch": 42.57261089840275,
      "grad_norm": 11.744224548339844,
      "learning_rate": 1.4522824251331038e-05,
      "loss": 1.8421,
      "step": 1100800
    },
    {
      "epoch": 42.57647832308466,
      "grad_norm": 14.734837532043457,
      "learning_rate": 1.4519601397429453e-05,
      "loss": 1.7783,
      "step": 1100900
    },
    {
      "epoch": 42.580345747766565,
      "grad_norm": 15.88476848602295,
      "learning_rate": 1.4516378543527864e-05,
      "loss": 1.7451,
      "step": 1101000
    },
    {
      "epoch": 42.584213172448464,
      "grad_norm": 13.068692207336426,
      "learning_rate": 1.4513155689626279e-05,
      "loss": 1.8142,
      "step": 1101100
    },
    {
      "epoch": 42.58808059713037,
      "grad_norm": 9.664289474487305,
      "learning_rate": 1.450993283572469e-05,
      "loss": 1.8198,
      "step": 1101200
    },
    {
      "epoch": 42.59194802181228,
      "grad_norm": 13.803857803344727,
      "learning_rate": 1.4506709981823105e-05,
      "loss": 1.9249,
      "step": 1101300
    },
    {
      "epoch": 42.59581544649418,
      "grad_norm": 10.178028106689453,
      "learning_rate": 1.4503487127921516e-05,
      "loss": 1.8258,
      "step": 1101400
    },
    {
      "epoch": 42.599682871176086,
      "grad_norm": 12.169503211975098,
      "learning_rate": 1.4500264274019931e-05,
      "loss": 1.7362,
      "step": 1101500
    },
    {
      "epoch": 42.603550295857985,
      "grad_norm": 12.698472023010254,
      "learning_rate": 1.4497041420118343e-05,
      "loss": 1.775,
      "step": 1101600
    },
    {
      "epoch": 42.60741772053989,
      "grad_norm": 14.951254844665527,
      "learning_rate": 1.4493818566216757e-05,
      "loss": 1.7676,
      "step": 1101700
    },
    {
      "epoch": 42.6112851452218,
      "grad_norm": 13.174948692321777,
      "learning_rate": 1.4490595712315169e-05,
      "loss": 1.8279,
      "step": 1101800
    },
    {
      "epoch": 42.6151525699037,
      "grad_norm": 14.557906150817871,
      "learning_rate": 1.4487372858413583e-05,
      "loss": 1.9001,
      "step": 1101900
    },
    {
      "epoch": 42.61901999458561,
      "grad_norm": 8.810070037841797,
      "learning_rate": 1.4484150004511995e-05,
      "loss": 1.8094,
      "step": 1102000
    },
    {
      "epoch": 42.62288741926751,
      "grad_norm": 13.269074440002441,
      "learning_rate": 1.448092715061041e-05,
      "loss": 1.7694,
      "step": 1102100
    },
    {
      "epoch": 42.62675484394941,
      "grad_norm": 12.91437816619873,
      "learning_rate": 1.4477704296708822e-05,
      "loss": 1.815,
      "step": 1102200
    },
    {
      "epoch": 42.63062226863132,
      "grad_norm": 15.027311325073242,
      "learning_rate": 1.4474481442807235e-05,
      "loss": 1.7633,
      "step": 1102300
    },
    {
      "epoch": 42.63448969331322,
      "grad_norm": 9.957895278930664,
      "learning_rate": 1.4471258588905648e-05,
      "loss": 1.7798,
      "step": 1102400
    },
    {
      "epoch": 42.63835711799513,
      "grad_norm": 10.888396263122559,
      "learning_rate": 1.4468035735004063e-05,
      "loss": 1.8954,
      "step": 1102500
    },
    {
      "epoch": 42.642224542677035,
      "grad_norm": 10.118491172790527,
      "learning_rate": 1.4464812881102474e-05,
      "loss": 1.8148,
      "step": 1102600
    },
    {
      "epoch": 42.646091967358934,
      "grad_norm": 13.168179512023926,
      "learning_rate": 1.446159002720089e-05,
      "loss": 1.9174,
      "step": 1102700
    },
    {
      "epoch": 42.64995939204084,
      "grad_norm": 14.002961158752441,
      "learning_rate": 1.44583671732993e-05,
      "loss": 1.926,
      "step": 1102800
    },
    {
      "epoch": 42.65382681672274,
      "grad_norm": 12.451231002807617,
      "learning_rate": 1.4455144319397715e-05,
      "loss": 1.7649,
      "step": 1102900
    },
    {
      "epoch": 42.65769424140465,
      "grad_norm": 11.884517669677734,
      "learning_rate": 1.4451921465496127e-05,
      "loss": 1.8226,
      "step": 1103000
    },
    {
      "epoch": 42.661561666086556,
      "grad_norm": 10.307600021362305,
      "learning_rate": 1.4448698611594541e-05,
      "loss": 1.779,
      "step": 1103100
    },
    {
      "epoch": 42.665429090768455,
      "grad_norm": 10.290249824523926,
      "learning_rate": 1.4445475757692953e-05,
      "loss": 1.8148,
      "step": 1103200
    },
    {
      "epoch": 42.66929651545036,
      "grad_norm": 14.589964866638184,
      "learning_rate": 1.4442252903791367e-05,
      "loss": 1.8762,
      "step": 1103300
    },
    {
      "epoch": 42.67316394013226,
      "grad_norm": 11.001327514648438,
      "learning_rate": 1.4439030049889779e-05,
      "loss": 1.8208,
      "step": 1103400
    },
    {
      "epoch": 42.67703136481417,
      "grad_norm": 8.969696044921875,
      "learning_rate": 1.4435807195988193e-05,
      "loss": 1.7487,
      "step": 1103500
    },
    {
      "epoch": 42.68089878949608,
      "grad_norm": 11.878486633300781,
      "learning_rate": 1.4432584342086605e-05,
      "loss": 1.8473,
      "step": 1103600
    },
    {
      "epoch": 42.68476621417798,
      "grad_norm": 15.028334617614746,
      "learning_rate": 1.4429361488185018e-05,
      "loss": 1.847,
      "step": 1103700
    },
    {
      "epoch": 42.68863363885988,
      "grad_norm": 13.706854820251465,
      "learning_rate": 1.442613863428343e-05,
      "loss": 1.884,
      "step": 1103800
    },
    {
      "epoch": 42.69250106354179,
      "grad_norm": 13.490057945251465,
      "learning_rate": 1.4422915780381844e-05,
      "loss": 1.7069,
      "step": 1103900
    },
    {
      "epoch": 42.69636848822369,
      "grad_norm": 13.09909725189209,
      "learning_rate": 1.4419692926480259e-05,
      "loss": 1.8545,
      "step": 1104000
    },
    {
      "epoch": 42.7002359129056,
      "grad_norm": 12.836429595947266,
      "learning_rate": 1.441647007257867e-05,
      "loss": 1.8107,
      "step": 1104100
    },
    {
      "epoch": 42.7041033375875,
      "grad_norm": 6.322776794433594,
      "learning_rate": 1.4413247218677085e-05,
      "loss": 1.7277,
      "step": 1104200
    },
    {
      "epoch": 42.707970762269404,
      "grad_norm": 13.343734741210938,
      "learning_rate": 1.4410024364775496e-05,
      "loss": 1.8521,
      "step": 1104300
    },
    {
      "epoch": 42.71183818695131,
      "grad_norm": 10.356383323669434,
      "learning_rate": 1.440680151087391e-05,
      "loss": 1.8187,
      "step": 1104400
    },
    {
      "epoch": 42.71570561163321,
      "grad_norm": 17.022790908813477,
      "learning_rate": 1.4403578656972322e-05,
      "loss": 1.76,
      "step": 1104500
    },
    {
      "epoch": 42.71957303631512,
      "grad_norm": 9.126359939575195,
      "learning_rate": 1.4400355803070737e-05,
      "loss": 1.7973,
      "step": 1104600
    },
    {
      "epoch": 42.723440460997026,
      "grad_norm": 10.195626258850098,
      "learning_rate": 1.4397132949169148e-05,
      "loss": 1.8937,
      "step": 1104700
    },
    {
      "epoch": 42.727307885678925,
      "grad_norm": 11.008292198181152,
      "learning_rate": 1.4393910095267563e-05,
      "loss": 1.8185,
      "step": 1104800
    },
    {
      "epoch": 42.73117531036083,
      "grad_norm": 14.797199249267578,
      "learning_rate": 1.4390687241365974e-05,
      "loss": 1.7851,
      "step": 1104900
    },
    {
      "epoch": 42.73504273504273,
      "grad_norm": 15.235281944274902,
      "learning_rate": 1.4387464387464389e-05,
      "loss": 1.8467,
      "step": 1105000
    },
    {
      "epoch": 42.73891015972464,
      "grad_norm": 8.581132888793945,
      "learning_rate": 1.43842415335628e-05,
      "loss": 1.7883,
      "step": 1105100
    },
    {
      "epoch": 42.74277758440655,
      "grad_norm": 12.29732894897461,
      "learning_rate": 1.4381018679661215e-05,
      "loss": 1.7594,
      "step": 1105200
    },
    {
      "epoch": 42.746645009088446,
      "grad_norm": 11.816543579101562,
      "learning_rate": 1.4377795825759626e-05,
      "loss": 1.8664,
      "step": 1105300
    },
    {
      "epoch": 42.75051243377035,
      "grad_norm": 13.490522384643555,
      "learning_rate": 1.4374572971858041e-05,
      "loss": 1.7602,
      "step": 1105400
    },
    {
      "epoch": 42.75437985845225,
      "grad_norm": 11.876248359680176,
      "learning_rate": 1.4371350117956452e-05,
      "loss": 1.7799,
      "step": 1105500
    },
    {
      "epoch": 42.75824728313416,
      "grad_norm": 18.424091339111328,
      "learning_rate": 1.4368127264054867e-05,
      "loss": 1.8642,
      "step": 1105600
    },
    {
      "epoch": 42.76211470781607,
      "grad_norm": 8.475940704345703,
      "learning_rate": 1.436490441015328e-05,
      "loss": 1.844,
      "step": 1105700
    },
    {
      "epoch": 42.76598213249797,
      "grad_norm": 10.766410827636719,
      "learning_rate": 1.4361681556251693e-05,
      "loss": 1.8946,
      "step": 1105800
    },
    {
      "epoch": 42.769849557179874,
      "grad_norm": 13.011004447937012,
      "learning_rate": 1.4358458702350106e-05,
      "loss": 1.8583,
      "step": 1105900
    },
    {
      "epoch": 42.77371698186178,
      "grad_norm": 13.866593360900879,
      "learning_rate": 1.435523584844852e-05,
      "loss": 1.8665,
      "step": 1106000
    },
    {
      "epoch": 42.77758440654368,
      "grad_norm": 12.283123016357422,
      "learning_rate": 1.4352012994546932e-05,
      "loss": 1.8022,
      "step": 1106100
    },
    {
      "epoch": 42.78145183122559,
      "grad_norm": 11.495223999023438,
      "learning_rate": 1.4348790140645347e-05,
      "loss": 1.8742,
      "step": 1106200
    },
    {
      "epoch": 42.78531925590749,
      "grad_norm": 11.82102108001709,
      "learning_rate": 1.4345567286743758e-05,
      "loss": 1.8488,
      "step": 1106300
    },
    {
      "epoch": 42.789186680589395,
      "grad_norm": 10.661348342895508,
      "learning_rate": 1.4342344432842173e-05,
      "loss": 1.7816,
      "step": 1106400
    },
    {
      "epoch": 42.7930541052713,
      "grad_norm": 12.512411117553711,
      "learning_rate": 1.4339121578940584e-05,
      "loss": 1.8609,
      "step": 1106500
    },
    {
      "epoch": 42.7969215299532,
      "grad_norm": 12.561369895935059,
      "learning_rate": 1.4335898725038995e-05,
      "loss": 1.7861,
      "step": 1106600
    },
    {
      "epoch": 42.80078895463511,
      "grad_norm": 11.586612701416016,
      "learning_rate": 1.433267587113741e-05,
      "loss": 1.7468,
      "step": 1106700
    },
    {
      "epoch": 42.80465637931701,
      "grad_norm": 10.31700611114502,
      "learning_rate": 1.4329453017235822e-05,
      "loss": 1.8814,
      "step": 1106800
    },
    {
      "epoch": 42.808523803998916,
      "grad_norm": 13.441268920898438,
      "learning_rate": 1.4326230163334236e-05,
      "loss": 1.78,
      "step": 1106900
    },
    {
      "epoch": 42.81239122868082,
      "grad_norm": 12.236713409423828,
      "learning_rate": 1.4323007309432648e-05,
      "loss": 1.8238,
      "step": 1107000
    },
    {
      "epoch": 42.81625865336272,
      "grad_norm": 14.686959266662598,
      "learning_rate": 1.4319784455531062e-05,
      "loss": 1.7802,
      "step": 1107100
    },
    {
      "epoch": 42.82012607804463,
      "grad_norm": 11.900876998901367,
      "learning_rate": 1.4316561601629475e-05,
      "loss": 1.8414,
      "step": 1107200
    },
    {
      "epoch": 42.82399350272654,
      "grad_norm": 10.169290542602539,
      "learning_rate": 1.4313338747727888e-05,
      "loss": 1.8015,
      "step": 1107300
    },
    {
      "epoch": 42.82786092740844,
      "grad_norm": 14.70469856262207,
      "learning_rate": 1.4310115893826301e-05,
      "loss": 1.8331,
      "step": 1107400
    },
    {
      "epoch": 42.831728352090344,
      "grad_norm": 11.542070388793945,
      "learning_rate": 1.4306893039924716e-05,
      "loss": 1.7285,
      "step": 1107500
    },
    {
      "epoch": 42.835595776772244,
      "grad_norm": 9.94259262084961,
      "learning_rate": 1.4303670186023127e-05,
      "loss": 1.7321,
      "step": 1107600
    },
    {
      "epoch": 42.83946320145415,
      "grad_norm": 13.046892166137695,
      "learning_rate": 1.4300447332121542e-05,
      "loss": 1.8195,
      "step": 1107700
    },
    {
      "epoch": 42.84333062613606,
      "grad_norm": 11.819181442260742,
      "learning_rate": 1.4297224478219953e-05,
      "loss": 1.8409,
      "step": 1107800
    },
    {
      "epoch": 42.84719805081796,
      "grad_norm": 9.832356452941895,
      "learning_rate": 1.4294001624318368e-05,
      "loss": 1.8065,
      "step": 1107900
    },
    {
      "epoch": 42.851065475499865,
      "grad_norm": 11.974325180053711,
      "learning_rate": 1.429077877041678e-05,
      "loss": 1.8794,
      "step": 1108000
    },
    {
      "epoch": 42.854932900181765,
      "grad_norm": 12.729061126708984,
      "learning_rate": 1.4287555916515194e-05,
      "loss": 1.8324,
      "step": 1108100
    },
    {
      "epoch": 42.85880032486367,
      "grad_norm": 9.4241304397583,
      "learning_rate": 1.4284333062613606e-05,
      "loss": 1.8334,
      "step": 1108200
    },
    {
      "epoch": 42.86266774954558,
      "grad_norm": 14.7923002243042,
      "learning_rate": 1.428111020871202e-05,
      "loss": 1.8686,
      "step": 1108300
    },
    {
      "epoch": 42.86653517422748,
      "grad_norm": 10.017044067382812,
      "learning_rate": 1.4277887354810432e-05,
      "loss": 1.8612,
      "step": 1108400
    },
    {
      "epoch": 42.870402598909386,
      "grad_norm": 10.383418083190918,
      "learning_rate": 1.4274664500908846e-05,
      "loss": 1.7693,
      "step": 1108500
    },
    {
      "epoch": 42.87427002359129,
      "grad_norm": 12.181157112121582,
      "learning_rate": 1.4271441647007258e-05,
      "loss": 1.8292,
      "step": 1108600
    },
    {
      "epoch": 42.87813744827319,
      "grad_norm": 14.028078079223633,
      "learning_rate": 1.4268218793105672e-05,
      "loss": 1.8781,
      "step": 1108700
    },
    {
      "epoch": 42.8820048729551,
      "grad_norm": 10.346105575561523,
      "learning_rate": 1.4264995939204084e-05,
      "loss": 1.6705,
      "step": 1108800
    },
    {
      "epoch": 42.885872297637,
      "grad_norm": 13.072556495666504,
      "learning_rate": 1.4261773085302498e-05,
      "loss": 1.8177,
      "step": 1108900
    },
    {
      "epoch": 42.88973972231891,
      "grad_norm": 16.211366653442383,
      "learning_rate": 1.4258550231400911e-05,
      "loss": 1.8469,
      "step": 1109000
    },
    {
      "epoch": 42.893607147000814,
      "grad_norm": 11.19530200958252,
      "learning_rate": 1.4255327377499324e-05,
      "loss": 1.7686,
      "step": 1109100
    },
    {
      "epoch": 42.897474571682714,
      "grad_norm": 14.781195640563965,
      "learning_rate": 1.4252104523597737e-05,
      "loss": 1.7974,
      "step": 1109200
    },
    {
      "epoch": 42.90134199636462,
      "grad_norm": 10.57979965209961,
      "learning_rate": 1.424888166969615e-05,
      "loss": 1.8997,
      "step": 1109300
    },
    {
      "epoch": 42.90520942104653,
      "grad_norm": 7.128530979156494,
      "learning_rate": 1.4245658815794564e-05,
      "loss": 1.7855,
      "step": 1109400
    },
    {
      "epoch": 42.90907684572843,
      "grad_norm": 10.878082275390625,
      "learning_rate": 1.4242435961892975e-05,
      "loss": 1.7752,
      "step": 1109500
    },
    {
      "epoch": 42.912944270410335,
      "grad_norm": 5.999863147735596,
      "learning_rate": 1.423921310799139e-05,
      "loss": 1.7714,
      "step": 1109600
    },
    {
      "epoch": 42.916811695092235,
      "grad_norm": 9.5325345993042,
      "learning_rate": 1.4235990254089801e-05,
      "loss": 1.9324,
      "step": 1109700
    },
    {
      "epoch": 42.92067911977414,
      "grad_norm": 12.90634822845459,
      "learning_rate": 1.4232767400188216e-05,
      "loss": 1.7904,
      "step": 1109800
    },
    {
      "epoch": 42.92454654445605,
      "grad_norm": 11.977270126342773,
      "learning_rate": 1.4229544546286627e-05,
      "loss": 1.7827,
      "step": 1109900
    },
    {
      "epoch": 42.92841396913795,
      "grad_norm": 15.304388999938965,
      "learning_rate": 1.4226321692385042e-05,
      "loss": 1.7991,
      "step": 1110000
    },
    {
      "epoch": 42.932281393819856,
      "grad_norm": 14.814576148986816,
      "learning_rate": 1.4223098838483453e-05,
      "loss": 1.884,
      "step": 1110100
    },
    {
      "epoch": 42.936148818501756,
      "grad_norm": 12.647405624389648,
      "learning_rate": 1.4219875984581868e-05,
      "loss": 1.8376,
      "step": 1110200
    },
    {
      "epoch": 42.94001624318366,
      "grad_norm": 10.608858108520508,
      "learning_rate": 1.4216653130680279e-05,
      "loss": 1.8918,
      "step": 1110300
    },
    {
      "epoch": 42.94388366786557,
      "grad_norm": 13.640026092529297,
      "learning_rate": 1.4213430276778694e-05,
      "loss": 1.911,
      "step": 1110400
    },
    {
      "epoch": 42.94775109254747,
      "grad_norm": 16.393701553344727,
      "learning_rate": 1.4210207422877105e-05,
      "loss": 1.8156,
      "step": 1110500
    },
    {
      "epoch": 42.95161851722938,
      "grad_norm": 13.334696769714355,
      "learning_rate": 1.420698456897552e-05,
      "loss": 1.65,
      "step": 1110600
    },
    {
      "epoch": 42.955485941911284,
      "grad_norm": 17.79437255859375,
      "learning_rate": 1.4203761715073933e-05,
      "loss": 1.888,
      "step": 1110700
    },
    {
      "epoch": 42.959353366593184,
      "grad_norm": 14.300396919250488,
      "learning_rate": 1.4200538861172346e-05,
      "loss": 1.8461,
      "step": 1110800
    },
    {
      "epoch": 42.96322079127509,
      "grad_norm": 10.431768417358398,
      "learning_rate": 1.4197316007270759e-05,
      "loss": 1.8207,
      "step": 1110900
    },
    {
      "epoch": 42.96708821595699,
      "grad_norm": 10.928279876708984,
      "learning_rate": 1.4194093153369174e-05,
      "loss": 1.8261,
      "step": 1111000
    },
    {
      "epoch": 42.9709556406389,
      "grad_norm": 13.15881633758545,
      "learning_rate": 1.4190870299467585e-05,
      "loss": 1.8113,
      "step": 1111100
    },
    {
      "epoch": 42.974823065320805,
      "grad_norm": 12.52829360961914,
      "learning_rate": 1.4187647445566e-05,
      "loss": 1.8093,
      "step": 1111200
    },
    {
      "epoch": 42.978690490002705,
      "grad_norm": 12.950757026672363,
      "learning_rate": 1.4184424591664411e-05,
      "loss": 1.8592,
      "step": 1111300
    },
    {
      "epoch": 42.98255791468461,
      "grad_norm": 12.323969841003418,
      "learning_rate": 1.4181201737762826e-05,
      "loss": 1.8308,
      "step": 1111400
    },
    {
      "epoch": 42.98642533936652,
      "grad_norm": 10.805679321289062,
      "learning_rate": 1.4177978883861237e-05,
      "loss": 1.8794,
      "step": 1111500
    },
    {
      "epoch": 42.99029276404842,
      "grad_norm": 11.942136764526367,
      "learning_rate": 1.4174756029959652e-05,
      "loss": 1.9243,
      "step": 1111600
    },
    {
      "epoch": 42.994160188730326,
      "grad_norm": 10.719861030578613,
      "learning_rate": 1.4171533176058063e-05,
      "loss": 1.9088,
      "step": 1111700
    },
    {
      "epoch": 42.998027613412226,
      "grad_norm": 8.893198013305664,
      "learning_rate": 1.4168310322156478e-05,
      "loss": 1.8382,
      "step": 1111800
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.7599762678146362,
      "eval_runtime": 5.7147,
      "eval_samples_per_second": 238.156,
      "eval_steps_per_second": 238.156,
      "step": 1111851
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.6077840328216553,
      "eval_runtime": 108.2869,
      "eval_samples_per_second": 238.782,
      "eval_steps_per_second": 238.782,
      "step": 1111851
    },
    {
      "epoch": 43.00189503809413,
      "grad_norm": 7.075373649597168,
      "learning_rate": 1.4165087468254889e-05,
      "loss": 1.8489,
      "step": 1111900
    },
    {
      "epoch": 43.00576246277604,
      "grad_norm": 11.506378173828125,
      "learning_rate": 1.4161864614353304e-05,
      "loss": 1.7925,
      "step": 1112000
    },
    {
      "epoch": 43.00962988745794,
      "grad_norm": 12.898407936096191,
      "learning_rate": 1.4158641760451715e-05,
      "loss": 1.8925,
      "step": 1112100
    },
    {
      "epoch": 43.01349731213985,
      "grad_norm": 12.695586204528809,
      "learning_rate": 1.415541890655013e-05,
      "loss": 1.7908,
      "step": 1112200
    },
    {
      "epoch": 43.01736473682175,
      "grad_norm": 10.026829719543457,
      "learning_rate": 1.4152196052648541e-05,
      "loss": 1.7724,
      "step": 1112300
    },
    {
      "epoch": 43.021232161503654,
      "grad_norm": 10.021319389343262,
      "learning_rate": 1.4148973198746954e-05,
      "loss": 1.7947,
      "step": 1112400
    },
    {
      "epoch": 43.02509958618556,
      "grad_norm": 12.908329963684082,
      "learning_rate": 1.4145750344845369e-05,
      "loss": 1.8066,
      "step": 1112500
    },
    {
      "epoch": 43.02896701086746,
      "grad_norm": 10.07036018371582,
      "learning_rate": 1.414252749094378e-05,
      "loss": 1.8802,
      "step": 1112600
    },
    {
      "epoch": 43.03283443554937,
      "grad_norm": 9.061077117919922,
      "learning_rate": 1.4139304637042195e-05,
      "loss": 1.864,
      "step": 1112700
    },
    {
      "epoch": 43.036701860231275,
      "grad_norm": 11.828221321105957,
      "learning_rate": 1.4136081783140606e-05,
      "loss": 1.8667,
      "step": 1112800
    },
    {
      "epoch": 43.040569284913175,
      "grad_norm": 9.751283645629883,
      "learning_rate": 1.4132858929239021e-05,
      "loss": 1.7818,
      "step": 1112900
    },
    {
      "epoch": 43.04443670959508,
      "grad_norm": 12.784223556518555,
      "learning_rate": 1.4129636075337432e-05,
      "loss": 1.6445,
      "step": 1113000
    },
    {
      "epoch": 43.04830413427698,
      "grad_norm": 13.250347137451172,
      "learning_rate": 1.4126413221435847e-05,
      "loss": 1.797,
      "step": 1113100
    },
    {
      "epoch": 43.05217155895889,
      "grad_norm": 11.953694343566895,
      "learning_rate": 1.4123190367534258e-05,
      "loss": 1.7641,
      "step": 1113200
    },
    {
      "epoch": 43.056038983640796,
      "grad_norm": 10.882150650024414,
      "learning_rate": 1.4119967513632673e-05,
      "loss": 1.831,
      "step": 1113300
    },
    {
      "epoch": 43.059906408322696,
      "grad_norm": 12.941601753234863,
      "learning_rate": 1.4116744659731084e-05,
      "loss": 1.7281,
      "step": 1113400
    },
    {
      "epoch": 43.0637738330046,
      "grad_norm": 9.466758728027344,
      "learning_rate": 1.41135218058295e-05,
      "loss": 1.8214,
      "step": 1113500
    },
    {
      "epoch": 43.0676412576865,
      "grad_norm": 10.823629379272461,
      "learning_rate": 1.411029895192791e-05,
      "loss": 1.7954,
      "step": 1113600
    },
    {
      "epoch": 43.07150868236841,
      "grad_norm": 12.695438385009766,
      "learning_rate": 1.4107076098026325e-05,
      "loss": 1.8588,
      "step": 1113700
    },
    {
      "epoch": 43.07537610705032,
      "grad_norm": 10.403285026550293,
      "learning_rate": 1.4103853244124737e-05,
      "loss": 1.7824,
      "step": 1113800
    },
    {
      "epoch": 43.07924353173222,
      "grad_norm": 15.516990661621094,
      "learning_rate": 1.4100630390223151e-05,
      "loss": 1.8193,
      "step": 1113900
    },
    {
      "epoch": 43.083110956414124,
      "grad_norm": 12.256072998046875,
      "learning_rate": 1.4097407536321564e-05,
      "loss": 1.8468,
      "step": 1114000
    },
    {
      "epoch": 43.08697838109603,
      "grad_norm": 12.107190132141113,
      "learning_rate": 1.4094184682419977e-05,
      "loss": 1.8784,
      "step": 1114100
    },
    {
      "epoch": 43.09084580577793,
      "grad_norm": 10.918785095214844,
      "learning_rate": 1.409096182851839e-05,
      "loss": 1.8103,
      "step": 1114200
    },
    {
      "epoch": 43.09471323045984,
      "grad_norm": 10.71577262878418,
      "learning_rate": 1.4087738974616803e-05,
      "loss": 1.8185,
      "step": 1114300
    },
    {
      "epoch": 43.09858065514174,
      "grad_norm": 11.567307472229004,
      "learning_rate": 1.4084516120715216e-05,
      "loss": 1.8164,
      "step": 1114400
    },
    {
      "epoch": 43.102448079823645,
      "grad_norm": 11.61417007446289,
      "learning_rate": 1.4081293266813631e-05,
      "loss": 1.8059,
      "step": 1114500
    },
    {
      "epoch": 43.10631550450555,
      "grad_norm": 10.723466873168945,
      "learning_rate": 1.4078070412912042e-05,
      "loss": 1.7391,
      "step": 1114600
    },
    {
      "epoch": 43.11018292918745,
      "grad_norm": 14.65834903717041,
      "learning_rate": 1.4074847559010457e-05,
      "loss": 1.8004,
      "step": 1114700
    },
    {
      "epoch": 43.11405035386936,
      "grad_norm": 11.658341407775879,
      "learning_rate": 1.4071624705108869e-05,
      "loss": 1.773,
      "step": 1114800
    },
    {
      "epoch": 43.11791777855126,
      "grad_norm": 9.526935577392578,
      "learning_rate": 1.4068401851207283e-05,
      "loss": 1.8569,
      "step": 1114900
    },
    {
      "epoch": 43.121785203233166,
      "grad_norm": 10.920900344848633,
      "learning_rate": 1.4065178997305695e-05,
      "loss": 1.7835,
      "step": 1115000
    },
    {
      "epoch": 43.12565262791507,
      "grad_norm": 14.342123031616211,
      "learning_rate": 1.406195614340411e-05,
      "loss": 1.8256,
      "step": 1115100
    },
    {
      "epoch": 43.12952005259697,
      "grad_norm": 13.048489570617676,
      "learning_rate": 1.405873328950252e-05,
      "loss": 1.832,
      "step": 1115200
    },
    {
      "epoch": 43.13338747727888,
      "grad_norm": 12.44372844696045,
      "learning_rate": 1.4055510435600935e-05,
      "loss": 1.8239,
      "step": 1115300
    },
    {
      "epoch": 43.13725490196079,
      "grad_norm": 16.471120834350586,
      "learning_rate": 1.4052287581699347e-05,
      "loss": 1.7395,
      "step": 1115400
    },
    {
      "epoch": 43.14112232664269,
      "grad_norm": 15.403263092041016,
      "learning_rate": 1.4049064727797758e-05,
      "loss": 1.739,
      "step": 1115500
    },
    {
      "epoch": 43.144989751324594,
      "grad_norm": 11.26799201965332,
      "learning_rate": 1.4045841873896173e-05,
      "loss": 1.8694,
      "step": 1115600
    },
    {
      "epoch": 43.148857176006494,
      "grad_norm": 12.641307830810547,
      "learning_rate": 1.4042619019994586e-05,
      "loss": 1.8494,
      "step": 1115700
    },
    {
      "epoch": 43.1527246006884,
      "grad_norm": 9.00247859954834,
      "learning_rate": 1.4039396166092999e-05,
      "loss": 1.7926,
      "step": 1115800
    },
    {
      "epoch": 43.15659202537031,
      "grad_norm": 13.478869438171387,
      "learning_rate": 1.4036173312191412e-05,
      "loss": 1.8001,
      "step": 1115900
    },
    {
      "epoch": 43.16045945005221,
      "grad_norm": 13.722679138183594,
      "learning_rate": 1.4032950458289827e-05,
      "loss": 1.7555,
      "step": 1116000
    },
    {
      "epoch": 43.164326874734115,
      "grad_norm": 15.919754028320312,
      "learning_rate": 1.4029727604388238e-05,
      "loss": 1.7948,
      "step": 1116100
    },
    {
      "epoch": 43.16819429941602,
      "grad_norm": 15.409316062927246,
      "learning_rate": 1.4026504750486653e-05,
      "loss": 1.7561,
      "step": 1116200
    },
    {
      "epoch": 43.17206172409792,
      "grad_norm": 9.545373916625977,
      "learning_rate": 1.4023281896585064e-05,
      "loss": 1.8857,
      "step": 1116300
    },
    {
      "epoch": 43.17592914877983,
      "grad_norm": 15.754733085632324,
      "learning_rate": 1.4020059042683479e-05,
      "loss": 1.8191,
      "step": 1116400
    },
    {
      "epoch": 43.17979657346173,
      "grad_norm": 11.182092666625977,
      "learning_rate": 1.401683618878189e-05,
      "loss": 1.7832,
      "step": 1116500
    },
    {
      "epoch": 43.183663998143636,
      "grad_norm": 12.32314682006836,
      "learning_rate": 1.4013613334880305e-05,
      "loss": 1.7731,
      "step": 1116600
    },
    {
      "epoch": 43.18753142282554,
      "grad_norm": 17.609050750732422,
      "learning_rate": 1.4010390480978716e-05,
      "loss": 1.8932,
      "step": 1116700
    },
    {
      "epoch": 43.19139884750744,
      "grad_norm": 8.786757469177246,
      "learning_rate": 1.400716762707713e-05,
      "loss": 1.8171,
      "step": 1116800
    },
    {
      "epoch": 43.19526627218935,
      "grad_norm": 8.009251594543457,
      "learning_rate": 1.4003944773175542e-05,
      "loss": 1.655,
      "step": 1116900
    },
    {
      "epoch": 43.19913369687125,
      "grad_norm": 12.298062324523926,
      "learning_rate": 1.4000721919273957e-05,
      "loss": 1.8558,
      "step": 1117000
    },
    {
      "epoch": 43.20300112155316,
      "grad_norm": 11.592071533203125,
      "learning_rate": 1.3997499065372368e-05,
      "loss": 1.8523,
      "step": 1117100
    },
    {
      "epoch": 43.206868546235064,
      "grad_norm": 11.869280815124512,
      "learning_rate": 1.3994276211470783e-05,
      "loss": 1.771,
      "step": 1117200
    },
    {
      "epoch": 43.210735970916964,
      "grad_norm": 11.180994987487793,
      "learning_rate": 1.3991053357569194e-05,
      "loss": 1.6595,
      "step": 1117300
    },
    {
      "epoch": 43.21460339559887,
      "grad_norm": 13.233901023864746,
      "learning_rate": 1.3987830503667609e-05,
      "loss": 1.897,
      "step": 1117400
    },
    {
      "epoch": 43.21847082028078,
      "grad_norm": 11.16214370727539,
      "learning_rate": 1.3984607649766022e-05,
      "loss": 1.7876,
      "step": 1117500
    },
    {
      "epoch": 43.22233824496268,
      "grad_norm": 11.3803071975708,
      "learning_rate": 1.3981384795864435e-05,
      "loss": 1.8632,
      "step": 1117600
    },
    {
      "epoch": 43.226205669644585,
      "grad_norm": 11.138493537902832,
      "learning_rate": 1.3978161941962848e-05,
      "loss": 1.8069,
      "step": 1117700
    },
    {
      "epoch": 43.230073094326485,
      "grad_norm": 13.28912353515625,
      "learning_rate": 1.3974939088061263e-05,
      "loss": 1.7289,
      "step": 1117800
    },
    {
      "epoch": 43.23394051900839,
      "grad_norm": 12.074814796447754,
      "learning_rate": 1.3971716234159674e-05,
      "loss": 1.8952,
      "step": 1117900
    },
    {
      "epoch": 43.2378079436903,
      "grad_norm": 15.49655818939209,
      "learning_rate": 1.3968493380258089e-05,
      "loss": 1.8345,
      "step": 1118000
    },
    {
      "epoch": 43.2416753683722,
      "grad_norm": 10.650426864624023,
      "learning_rate": 1.39652705263565e-05,
      "loss": 1.7394,
      "step": 1118100
    },
    {
      "epoch": 43.245542793054106,
      "grad_norm": 14.026823997497559,
      "learning_rate": 1.3962047672454915e-05,
      "loss": 1.7517,
      "step": 1118200
    },
    {
      "epoch": 43.249410217736006,
      "grad_norm": 12.839038848876953,
      "learning_rate": 1.3958824818553326e-05,
      "loss": 1.8477,
      "step": 1118300
    },
    {
      "epoch": 43.25327764241791,
      "grad_norm": 12.969234466552734,
      "learning_rate": 1.3955601964651737e-05,
      "loss": 1.8661,
      "step": 1118400
    },
    {
      "epoch": 43.25714506709982,
      "grad_norm": 12.33816909790039,
      "learning_rate": 1.3952379110750152e-05,
      "loss": 1.7529,
      "step": 1118500
    },
    {
      "epoch": 43.26101249178172,
      "grad_norm": 13.44944953918457,
      "learning_rate": 1.3949156256848563e-05,
      "loss": 1.8748,
      "step": 1118600
    },
    {
      "epoch": 43.26487991646363,
      "grad_norm": 15.77099323272705,
      "learning_rate": 1.3945933402946978e-05,
      "loss": 1.9081,
      "step": 1118700
    },
    {
      "epoch": 43.268747341145534,
      "grad_norm": 13.028182983398438,
      "learning_rate": 1.394271054904539e-05,
      "loss": 1.7932,
      "step": 1118800
    },
    {
      "epoch": 43.272614765827434,
      "grad_norm": 13.851542472839355,
      "learning_rate": 1.3939487695143804e-05,
      "loss": 1.8176,
      "step": 1118900
    },
    {
      "epoch": 43.27648219050934,
      "grad_norm": 10.110063552856445,
      "learning_rate": 1.3936264841242217e-05,
      "loss": 1.8194,
      "step": 1119000
    },
    {
      "epoch": 43.28034961519124,
      "grad_norm": 16.029170989990234,
      "learning_rate": 1.393304198734063e-05,
      "loss": 1.7684,
      "step": 1119100
    },
    {
      "epoch": 43.28421703987315,
      "grad_norm": 15.234488487243652,
      "learning_rate": 1.3929819133439043e-05,
      "loss": 1.7696,
      "step": 1119200
    },
    {
      "epoch": 43.288084464555055,
      "grad_norm": 11.937435150146484,
      "learning_rate": 1.3926596279537456e-05,
      "loss": 1.8295,
      "step": 1119300
    },
    {
      "epoch": 43.291951889236955,
      "grad_norm": 13.49763298034668,
      "learning_rate": 1.392337342563587e-05,
      "loss": 1.7961,
      "step": 1119400
    },
    {
      "epoch": 43.29581931391886,
      "grad_norm": 11.3128080368042,
      "learning_rate": 1.3920150571734284e-05,
      "loss": 1.7774,
      "step": 1119500
    },
    {
      "epoch": 43.29968673860076,
      "grad_norm": 12.06336498260498,
      "learning_rate": 1.3916927717832695e-05,
      "loss": 1.7504,
      "step": 1119600
    },
    {
      "epoch": 43.30355416328267,
      "grad_norm": 12.972269058227539,
      "learning_rate": 1.391370486393111e-05,
      "loss": 1.7896,
      "step": 1119700
    },
    {
      "epoch": 43.307421587964576,
      "grad_norm": 10.495939254760742,
      "learning_rate": 1.3910482010029521e-05,
      "loss": 1.6902,
      "step": 1119800
    },
    {
      "epoch": 43.311289012646476,
      "grad_norm": 15.176526069641113,
      "learning_rate": 1.3907259156127936e-05,
      "loss": 1.8215,
      "step": 1119900
    },
    {
      "epoch": 43.31515643732838,
      "grad_norm": 8.844841003417969,
      "learning_rate": 1.3904036302226347e-05,
      "loss": 1.8242,
      "step": 1120000
    },
    {
      "epoch": 43.31902386201029,
      "grad_norm": 11.000079154968262,
      "learning_rate": 1.3900813448324762e-05,
      "loss": 1.7763,
      "step": 1120100
    },
    {
      "epoch": 43.32289128669219,
      "grad_norm": 10.638278007507324,
      "learning_rate": 1.3897590594423174e-05,
      "loss": 1.8366,
      "step": 1120200
    },
    {
      "epoch": 43.3267587113741,
      "grad_norm": 10.519712448120117,
      "learning_rate": 1.3894367740521588e-05,
      "loss": 1.746,
      "step": 1120300
    },
    {
      "epoch": 43.330626136056,
      "grad_norm": 14.960580825805664,
      "learning_rate": 1.389114488662e-05,
      "loss": 1.6903,
      "step": 1120400
    },
    {
      "epoch": 43.334493560737904,
      "grad_norm": 13.204289436340332,
      "learning_rate": 1.3887922032718414e-05,
      "loss": 1.8473,
      "step": 1120500
    },
    {
      "epoch": 43.33836098541981,
      "grad_norm": 11.879878997802734,
      "learning_rate": 1.3884699178816826e-05,
      "loss": 1.9178,
      "step": 1120600
    },
    {
      "epoch": 43.34222841010171,
      "grad_norm": 10.213361740112305,
      "learning_rate": 1.388147632491524e-05,
      "loss": 1.8351,
      "step": 1120700
    },
    {
      "epoch": 43.34609583478362,
      "grad_norm": 11.600481033325195,
      "learning_rate": 1.3878253471013652e-05,
      "loss": 1.763,
      "step": 1120800
    },
    {
      "epoch": 43.349963259465525,
      "grad_norm": 13.531525611877441,
      "learning_rate": 1.3875030617112066e-05,
      "loss": 1.8484,
      "step": 1120900
    },
    {
      "epoch": 43.353830684147425,
      "grad_norm": 14.271954536437988,
      "learning_rate": 1.387180776321048e-05,
      "loss": 1.7609,
      "step": 1121000
    },
    {
      "epoch": 43.35769810882933,
      "grad_norm": 10.381866455078125,
      "learning_rate": 1.3868584909308892e-05,
      "loss": 1.7457,
      "step": 1121100
    },
    {
      "epoch": 43.36156553351123,
      "grad_norm": 11.188844680786133,
      "learning_rate": 1.3865362055407305e-05,
      "loss": 1.7571,
      "step": 1121200
    },
    {
      "epoch": 43.36543295819314,
      "grad_norm": 9.863987922668457,
      "learning_rate": 1.3862139201505717e-05,
      "loss": 1.8682,
      "step": 1121300
    },
    {
      "epoch": 43.369300382875046,
      "grad_norm": 10.49389934539795,
      "learning_rate": 1.3858916347604132e-05,
      "loss": 1.816,
      "step": 1121400
    },
    {
      "epoch": 43.373167807556946,
      "grad_norm": 11.81836986541748,
      "learning_rate": 1.3855693493702543e-05,
      "loss": 1.7785,
      "step": 1121500
    },
    {
      "epoch": 43.37703523223885,
      "grad_norm": 12.627965927124023,
      "learning_rate": 1.3852470639800958e-05,
      "loss": 1.8292,
      "step": 1121600
    },
    {
      "epoch": 43.38090265692075,
      "grad_norm": 13.034004211425781,
      "learning_rate": 1.3849247785899369e-05,
      "loss": 1.9037,
      "step": 1121700
    },
    {
      "epoch": 43.38477008160266,
      "grad_norm": 9.717061996459961,
      "learning_rate": 1.3846024931997784e-05,
      "loss": 1.7895,
      "step": 1121800
    },
    {
      "epoch": 43.38863750628457,
      "grad_norm": 10.316084861755371,
      "learning_rate": 1.3842802078096195e-05,
      "loss": 1.8278,
      "step": 1121900
    },
    {
      "epoch": 43.39250493096647,
      "grad_norm": 11.344300270080566,
      "learning_rate": 1.383957922419461e-05,
      "loss": 1.8109,
      "step": 1122000
    },
    {
      "epoch": 43.396372355648374,
      "grad_norm": 10.307221412658691,
      "learning_rate": 1.3836356370293021e-05,
      "loss": 1.7712,
      "step": 1122100
    },
    {
      "epoch": 43.40023978033028,
      "grad_norm": 13.469544410705566,
      "learning_rate": 1.3833133516391436e-05,
      "loss": 1.8422,
      "step": 1122200
    },
    {
      "epoch": 43.40410720501218,
      "grad_norm": 10.823249816894531,
      "learning_rate": 1.3829910662489847e-05,
      "loss": 1.7699,
      "step": 1122300
    },
    {
      "epoch": 43.40797462969409,
      "grad_norm": 11.054388046264648,
      "learning_rate": 1.3826687808588262e-05,
      "loss": 1.8449,
      "step": 1122400
    },
    {
      "epoch": 43.41184205437599,
      "grad_norm": 12.694469451904297,
      "learning_rate": 1.3823464954686675e-05,
      "loss": 1.7947,
      "step": 1122500
    },
    {
      "epoch": 43.415709479057895,
      "grad_norm": 12.96069622039795,
      "learning_rate": 1.3820242100785088e-05,
      "loss": 1.8385,
      "step": 1122600
    },
    {
      "epoch": 43.4195769037398,
      "grad_norm": 12.25549030303955,
      "learning_rate": 1.38170192468835e-05,
      "loss": 1.6982,
      "step": 1122700
    },
    {
      "epoch": 43.4234443284217,
      "grad_norm": 12.053180694580078,
      "learning_rate": 1.3813796392981916e-05,
      "loss": 1.7693,
      "step": 1122800
    },
    {
      "epoch": 43.42731175310361,
      "grad_norm": 17.006656646728516,
      "learning_rate": 1.3810573539080327e-05,
      "loss": 1.698,
      "step": 1122900
    },
    {
      "epoch": 43.43117917778551,
      "grad_norm": 12.366310119628906,
      "learning_rate": 1.3807350685178742e-05,
      "loss": 1.7775,
      "step": 1123000
    },
    {
      "epoch": 43.435046602467416,
      "grad_norm": 11.426127433776855,
      "learning_rate": 1.3804127831277153e-05,
      "loss": 1.8092,
      "step": 1123100
    },
    {
      "epoch": 43.43891402714932,
      "grad_norm": 10.186347007751465,
      "learning_rate": 1.3800904977375568e-05,
      "loss": 1.8093,
      "step": 1123200
    },
    {
      "epoch": 43.44278145183122,
      "grad_norm": 11.057038307189941,
      "learning_rate": 1.3797682123473979e-05,
      "loss": 1.9776,
      "step": 1123300
    },
    {
      "epoch": 43.44664887651313,
      "grad_norm": 12.196199417114258,
      "learning_rate": 1.3794459269572394e-05,
      "loss": 1.7976,
      "step": 1123400
    },
    {
      "epoch": 43.45051630119504,
      "grad_norm": 9.915199279785156,
      "learning_rate": 1.3791236415670805e-05,
      "loss": 1.7942,
      "step": 1123500
    },
    {
      "epoch": 43.45438372587694,
      "grad_norm": 9.471077919006348,
      "learning_rate": 1.378801356176922e-05,
      "loss": 1.7625,
      "step": 1123600
    },
    {
      "epoch": 43.458251150558844,
      "grad_norm": 13.668720245361328,
      "learning_rate": 1.3784790707867631e-05,
      "loss": 1.7116,
      "step": 1123700
    },
    {
      "epoch": 43.462118575240744,
      "grad_norm": 12.602560997009277,
      "learning_rate": 1.3781567853966046e-05,
      "loss": 1.7764,
      "step": 1123800
    },
    {
      "epoch": 43.46598599992265,
      "grad_norm": 13.182419776916504,
      "learning_rate": 1.3778345000064457e-05,
      "loss": 1.7962,
      "step": 1123900
    },
    {
      "epoch": 43.46985342460456,
      "grad_norm": 10.746479034423828,
      "learning_rate": 1.3775122146162872e-05,
      "loss": 1.8182,
      "step": 1124000
    },
    {
      "epoch": 43.47372084928646,
      "grad_norm": 12.07011890411377,
      "learning_rate": 1.3771899292261283e-05,
      "loss": 1.8551,
      "step": 1124100
    },
    {
      "epoch": 43.477588273968365,
      "grad_norm": 12.059576988220215,
      "learning_rate": 1.3768676438359696e-05,
      "loss": 1.8204,
      "step": 1124200
    },
    {
      "epoch": 43.48145569865027,
      "grad_norm": 14.746916770935059,
      "learning_rate": 1.376545358445811e-05,
      "loss": 1.8941,
      "step": 1124300
    },
    {
      "epoch": 43.48532312333217,
      "grad_norm": 11.553693771362305,
      "learning_rate": 1.3762230730556522e-05,
      "loss": 1.7912,
      "step": 1124400
    },
    {
      "epoch": 43.48919054801408,
      "grad_norm": 13.38339900970459,
      "learning_rate": 1.3759007876654937e-05,
      "loss": 1.8146,
      "step": 1124500
    },
    {
      "epoch": 43.49305797269598,
      "grad_norm": 6.4524455070495605,
      "learning_rate": 1.3755785022753348e-05,
      "loss": 1.8016,
      "step": 1124600
    },
    {
      "epoch": 43.496925397377886,
      "grad_norm": 12.372723579406738,
      "learning_rate": 1.3752562168851763e-05,
      "loss": 1.895,
      "step": 1124700
    },
    {
      "epoch": 43.50079282205979,
      "grad_norm": 10.511441230773926,
      "learning_rate": 1.3749339314950174e-05,
      "loss": 1.7814,
      "step": 1124800
    },
    {
      "epoch": 43.50466024674169,
      "grad_norm": 14.966160774230957,
      "learning_rate": 1.3746116461048589e-05,
      "loss": 1.7895,
      "step": 1124900
    },
    {
      "epoch": 43.5085276714236,
      "grad_norm": 13.11929988861084,
      "learning_rate": 1.3742893607147e-05,
      "loss": 1.767,
      "step": 1125000
    },
    {
      "epoch": 43.5123950961055,
      "grad_norm": 12.826354026794434,
      "learning_rate": 1.3739670753245415e-05,
      "loss": 1.8574,
      "step": 1125100
    },
    {
      "epoch": 43.51626252078741,
      "grad_norm": 10.344905853271484,
      "learning_rate": 1.3736447899343826e-05,
      "loss": 1.7293,
      "step": 1125200
    },
    {
      "epoch": 43.520129945469314,
      "grad_norm": 12.106698036193848,
      "learning_rate": 1.3733225045442241e-05,
      "loss": 1.7108,
      "step": 1125300
    },
    {
      "epoch": 43.523997370151214,
      "grad_norm": 12.439190864562988,
      "learning_rate": 1.3730002191540653e-05,
      "loss": 1.7671,
      "step": 1125400
    },
    {
      "epoch": 43.52786479483312,
      "grad_norm": 14.00195598602295,
      "learning_rate": 1.3726779337639067e-05,
      "loss": 1.8886,
      "step": 1125500
    },
    {
      "epoch": 43.53173221951503,
      "grad_norm": 10.021403312683105,
      "learning_rate": 1.3723556483737479e-05,
      "loss": 1.8526,
      "step": 1125600
    },
    {
      "epoch": 43.53559964419693,
      "grad_norm": 15.615644454956055,
      "learning_rate": 1.3720333629835893e-05,
      "loss": 1.7741,
      "step": 1125700
    },
    {
      "epoch": 43.539467068878835,
      "grad_norm": 12.598913192749023,
      "learning_rate": 1.3717110775934305e-05,
      "loss": 1.7964,
      "step": 1125800
    },
    {
      "epoch": 43.543334493560735,
      "grad_norm": 12.831335067749023,
      "learning_rate": 1.371388792203272e-05,
      "loss": 1.7405,
      "step": 1125900
    },
    {
      "epoch": 43.54720191824264,
      "grad_norm": 12.380558967590332,
      "learning_rate": 1.3710665068131132e-05,
      "loss": 1.7673,
      "step": 1126000
    },
    {
      "epoch": 43.55106934292455,
      "grad_norm": 13.061365127563477,
      "learning_rate": 1.3707442214229545e-05,
      "loss": 1.744,
      "step": 1126100
    },
    {
      "epoch": 43.55493676760645,
      "grad_norm": 17.583669662475586,
      "learning_rate": 1.3704219360327958e-05,
      "loss": 1.9138,
      "step": 1126200
    },
    {
      "epoch": 43.558804192288356,
      "grad_norm": 12.753588676452637,
      "learning_rate": 1.3700996506426373e-05,
      "loss": 1.8108,
      "step": 1126300
    },
    {
      "epoch": 43.562671616970256,
      "grad_norm": 13.334756851196289,
      "learning_rate": 1.3697773652524784e-05,
      "loss": 1.9383,
      "step": 1126400
    },
    {
      "epoch": 43.56653904165216,
      "grad_norm": 11.762641906738281,
      "learning_rate": 1.36945507986232e-05,
      "loss": 1.8182,
      "step": 1126500
    },
    {
      "epoch": 43.57040646633407,
      "grad_norm": 9.858793258666992,
      "learning_rate": 1.369132794472161e-05,
      "loss": 1.8976,
      "step": 1126600
    },
    {
      "epoch": 43.57427389101597,
      "grad_norm": 12.02623462677002,
      "learning_rate": 1.3688105090820025e-05,
      "loss": 1.8874,
      "step": 1126700
    },
    {
      "epoch": 43.57814131569788,
      "grad_norm": 17.474498748779297,
      "learning_rate": 1.3684882236918437e-05,
      "loss": 1.7897,
      "step": 1126800
    },
    {
      "epoch": 43.582008740379784,
      "grad_norm": 12.61439323425293,
      "learning_rate": 1.3681659383016851e-05,
      "loss": 1.6997,
      "step": 1126900
    },
    {
      "epoch": 43.585876165061684,
      "grad_norm": 11.15992546081543,
      "learning_rate": 1.3678436529115263e-05,
      "loss": 1.9067,
      "step": 1127000
    },
    {
      "epoch": 43.58974358974359,
      "grad_norm": 11.720390319824219,
      "learning_rate": 1.3675213675213677e-05,
      "loss": 1.8226,
      "step": 1127100
    },
    {
      "epoch": 43.59361101442549,
      "grad_norm": 16.0003604888916,
      "learning_rate": 1.3671990821312089e-05,
      "loss": 1.7062,
      "step": 1127200
    },
    {
      "epoch": 43.5974784391074,
      "grad_norm": 13.34002685546875,
      "learning_rate": 1.36687679674105e-05,
      "loss": 1.8115,
      "step": 1127300
    },
    {
      "epoch": 43.601345863789305,
      "grad_norm": 17.593727111816406,
      "learning_rate": 1.3665545113508915e-05,
      "loss": 1.8367,
      "step": 1127400
    },
    {
      "epoch": 43.605213288471205,
      "grad_norm": 11.080339431762695,
      "learning_rate": 1.3662322259607328e-05,
      "loss": 1.8551,
      "step": 1127500
    },
    {
      "epoch": 43.60908071315311,
      "grad_norm": 9.719443321228027,
      "learning_rate": 1.365909940570574e-05,
      "loss": 1.7744,
      "step": 1127600
    },
    {
      "epoch": 43.61294813783502,
      "grad_norm": 11.495601654052734,
      "learning_rate": 1.3655876551804154e-05,
      "loss": 1.8873,
      "step": 1127700
    },
    {
      "epoch": 43.61681556251692,
      "grad_norm": 12.717625617980957,
      "learning_rate": 1.3652653697902568e-05,
      "loss": 1.7029,
      "step": 1127800
    },
    {
      "epoch": 43.620682987198826,
      "grad_norm": 11.551968574523926,
      "learning_rate": 1.364943084400098e-05,
      "loss": 1.7949,
      "step": 1127900
    },
    {
      "epoch": 43.624550411880726,
      "grad_norm": 11.63363265991211,
      "learning_rate": 1.3646207990099395e-05,
      "loss": 1.7752,
      "step": 1128000
    },
    {
      "epoch": 43.62841783656263,
      "grad_norm": 13.49617862701416,
      "learning_rate": 1.3642985136197806e-05,
      "loss": 1.8591,
      "step": 1128100
    },
    {
      "epoch": 43.63228526124454,
      "grad_norm": 20.6550350189209,
      "learning_rate": 1.363976228229622e-05,
      "loss": 1.8755,
      "step": 1128200
    },
    {
      "epoch": 43.63615268592644,
      "grad_norm": 14.088532447814941,
      "learning_rate": 1.3636539428394632e-05,
      "loss": 1.8576,
      "step": 1128300
    },
    {
      "epoch": 43.64002011060835,
      "grad_norm": 13.481139183044434,
      "learning_rate": 1.3633316574493047e-05,
      "loss": 1.8047,
      "step": 1128400
    },
    {
      "epoch": 43.64388753529025,
      "grad_norm": 12.45818042755127,
      "learning_rate": 1.3630093720591458e-05,
      "loss": 1.8933,
      "step": 1128500
    },
    {
      "epoch": 43.647754959972154,
      "grad_norm": 16.674072265625,
      "learning_rate": 1.3626870866689873e-05,
      "loss": 1.8153,
      "step": 1128600
    },
    {
      "epoch": 43.65162238465406,
      "grad_norm": 11.62317943572998,
      "learning_rate": 1.3623648012788284e-05,
      "loss": 1.9172,
      "step": 1128700
    },
    {
      "epoch": 43.65548980933596,
      "grad_norm": 14.151373863220215,
      "learning_rate": 1.3620425158886699e-05,
      "loss": 1.7798,
      "step": 1128800
    },
    {
      "epoch": 43.65935723401787,
      "grad_norm": 13.44051456451416,
      "learning_rate": 1.361720230498511e-05,
      "loss": 1.8166,
      "step": 1128900
    },
    {
      "epoch": 43.663224658699775,
      "grad_norm": 15.117962837219238,
      "learning_rate": 1.3613979451083525e-05,
      "loss": 1.9032,
      "step": 1129000
    },
    {
      "epoch": 43.667092083381675,
      "grad_norm": 12.50255298614502,
      "learning_rate": 1.3610756597181936e-05,
      "loss": 1.9888,
      "step": 1129100
    },
    {
      "epoch": 43.67095950806358,
      "grad_norm": 13.216510772705078,
      "learning_rate": 1.360753374328035e-05,
      "loss": 1.852,
      "step": 1129200
    },
    {
      "epoch": 43.67482693274548,
      "grad_norm": 14.287601470947266,
      "learning_rate": 1.3604310889378762e-05,
      "loss": 1.7822,
      "step": 1129300
    },
    {
      "epoch": 43.67869435742739,
      "grad_norm": 10.677226066589355,
      "learning_rate": 1.3601088035477177e-05,
      "loss": 1.8032,
      "step": 1129400
    },
    {
      "epoch": 43.682561782109296,
      "grad_norm": 13.348345756530762,
      "learning_rate": 1.359786518157559e-05,
      "loss": 1.8207,
      "step": 1129500
    },
    {
      "epoch": 43.686429206791196,
      "grad_norm": 14.648488998413086,
      "learning_rate": 1.3594642327674003e-05,
      "loss": 1.8189,
      "step": 1129600
    },
    {
      "epoch": 43.6902966314731,
      "grad_norm": 12.9744291305542,
      "learning_rate": 1.3591419473772416e-05,
      "loss": 1.8706,
      "step": 1129700
    },
    {
      "epoch": 43.694164056155,
      "grad_norm": 13.588996887207031,
      "learning_rate": 1.358819661987083e-05,
      "loss": 1.8705,
      "step": 1129800
    },
    {
      "epoch": 43.69803148083691,
      "grad_norm": 10.956681251525879,
      "learning_rate": 1.3584973765969242e-05,
      "loss": 1.7461,
      "step": 1129900
    },
    {
      "epoch": 43.70189890551882,
      "grad_norm": 14.706690788269043,
      "learning_rate": 1.3581750912067657e-05,
      "loss": 1.7955,
      "step": 1130000
    },
    {
      "epoch": 43.70576633020072,
      "grad_norm": 12.061564445495605,
      "learning_rate": 1.3578528058166068e-05,
      "loss": 1.8843,
      "step": 1130100
    },
    {
      "epoch": 43.709633754882624,
      "grad_norm": 12.875274658203125,
      "learning_rate": 1.357530520426448e-05,
      "loss": 1.824,
      "step": 1130200
    },
    {
      "epoch": 43.71350117956453,
      "grad_norm": 12.238957405090332,
      "learning_rate": 1.3572082350362894e-05,
      "loss": 1.8604,
      "step": 1130300
    },
    {
      "epoch": 43.71736860424643,
      "grad_norm": 12.831120491027832,
      "learning_rate": 1.3568859496461305e-05,
      "loss": 1.7221,
      "step": 1130400
    },
    {
      "epoch": 43.72123602892834,
      "grad_norm": 12.516423225402832,
      "learning_rate": 1.356563664255972e-05,
      "loss": 1.7841,
      "step": 1130500
    },
    {
      "epoch": 43.72510345361024,
      "grad_norm": 11.202372550964355,
      "learning_rate": 1.3562413788658131e-05,
      "loss": 1.7831,
      "step": 1130600
    },
    {
      "epoch": 43.728970878292145,
      "grad_norm": 15.781180381774902,
      "learning_rate": 1.3559190934756546e-05,
      "loss": 1.8878,
      "step": 1130700
    },
    {
      "epoch": 43.73283830297405,
      "grad_norm": 12.227166175842285,
      "learning_rate": 1.3555968080854958e-05,
      "loss": 1.7765,
      "step": 1130800
    },
    {
      "epoch": 43.73670572765595,
      "grad_norm": 12.46584415435791,
      "learning_rate": 1.3552745226953372e-05,
      "loss": 1.7796,
      "step": 1130900
    },
    {
      "epoch": 43.74057315233786,
      "grad_norm": 13.102126121520996,
      "learning_rate": 1.3549522373051785e-05,
      "loss": 1.7991,
      "step": 1131000
    },
    {
      "epoch": 43.74444057701976,
      "grad_norm": 11.148606300354004,
      "learning_rate": 1.3546299519150198e-05,
      "loss": 1.8593,
      "step": 1131100
    },
    {
      "epoch": 43.748308001701666,
      "grad_norm": 10.306867599487305,
      "learning_rate": 1.3543076665248611e-05,
      "loss": 1.783,
      "step": 1131200
    },
    {
      "epoch": 43.75217542638357,
      "grad_norm": 13.61452579498291,
      "learning_rate": 1.3539853811347026e-05,
      "loss": 1.8293,
      "step": 1131300
    },
    {
      "epoch": 43.75604285106547,
      "grad_norm": 15.338815689086914,
      "learning_rate": 1.3536630957445437e-05,
      "loss": 1.883,
      "step": 1131400
    },
    {
      "epoch": 43.75991027574738,
      "grad_norm": 11.326629638671875,
      "learning_rate": 1.3533408103543852e-05,
      "loss": 1.8425,
      "step": 1131500
    },
    {
      "epoch": 43.76377770042929,
      "grad_norm": 14.92304515838623,
      "learning_rate": 1.3530185249642263e-05,
      "loss": 1.8225,
      "step": 1131600
    },
    {
      "epoch": 43.76764512511119,
      "grad_norm": 12.747300148010254,
      "learning_rate": 1.3526962395740678e-05,
      "loss": 1.7425,
      "step": 1131700
    },
    {
      "epoch": 43.771512549793094,
      "grad_norm": 11.50848388671875,
      "learning_rate": 1.352373954183909e-05,
      "loss": 1.7922,
      "step": 1131800
    },
    {
      "epoch": 43.775379974474994,
      "grad_norm": 14.673802375793457,
      "learning_rate": 1.3520516687937504e-05,
      "loss": 1.714,
      "step": 1131900
    },
    {
      "epoch": 43.7792473991569,
      "grad_norm": 13.801637649536133,
      "learning_rate": 1.3517293834035915e-05,
      "loss": 1.8078,
      "step": 1132000
    },
    {
      "epoch": 43.78311482383881,
      "grad_norm": 13.267940521240234,
      "learning_rate": 1.351407098013433e-05,
      "loss": 1.8531,
      "step": 1132100
    },
    {
      "epoch": 43.78698224852071,
      "grad_norm": 11.999156951904297,
      "learning_rate": 1.3510848126232742e-05,
      "loss": 1.8028,
      "step": 1132200
    },
    {
      "epoch": 43.790849673202615,
      "grad_norm": 12.430557250976562,
      "learning_rate": 1.3507625272331156e-05,
      "loss": 1.8189,
      "step": 1132300
    },
    {
      "epoch": 43.79471709788452,
      "grad_norm": 10.599762916564941,
      "learning_rate": 1.3504402418429568e-05,
      "loss": 1.8114,
      "step": 1132400
    },
    {
      "epoch": 43.79858452256642,
      "grad_norm": 13.27964973449707,
      "learning_rate": 1.3501179564527982e-05,
      "loss": 1.8408,
      "step": 1132500
    },
    {
      "epoch": 43.80245194724833,
      "grad_norm": 11.487122535705566,
      "learning_rate": 1.3497956710626394e-05,
      "loss": 1.8049,
      "step": 1132600
    },
    {
      "epoch": 43.80631937193023,
      "grad_norm": 13.90470027923584,
      "learning_rate": 1.3494733856724808e-05,
      "loss": 1.8265,
      "step": 1132700
    },
    {
      "epoch": 43.810186796612136,
      "grad_norm": 12.73528003692627,
      "learning_rate": 1.3491511002823221e-05,
      "loss": 1.7253,
      "step": 1132800
    },
    {
      "epoch": 43.81405422129404,
      "grad_norm": 8.739503860473633,
      "learning_rate": 1.3488288148921634e-05,
      "loss": 1.8731,
      "step": 1132900
    },
    {
      "epoch": 43.81792164597594,
      "grad_norm": 15.74191951751709,
      "learning_rate": 1.3485065295020047e-05,
      "loss": 1.8312,
      "step": 1133000
    },
    {
      "epoch": 43.82178907065785,
      "grad_norm": 9.309255599975586,
      "learning_rate": 1.3481842441118459e-05,
      "loss": 1.8791,
      "step": 1133100
    },
    {
      "epoch": 43.82565649533975,
      "grad_norm": 12.05907917022705,
      "learning_rate": 1.3478619587216873e-05,
      "loss": 1.819,
      "step": 1133200
    },
    {
      "epoch": 43.82952392002166,
      "grad_norm": 9.89889907836914,
      "learning_rate": 1.3475396733315285e-05,
      "loss": 1.7844,
      "step": 1133300
    },
    {
      "epoch": 43.833391344703564,
      "grad_norm": 17.667158126831055,
      "learning_rate": 1.34721738794137e-05,
      "loss": 1.7186,
      "step": 1133400
    },
    {
      "epoch": 43.837258769385464,
      "grad_norm": 13.738927841186523,
      "learning_rate": 1.3468951025512111e-05,
      "loss": 1.824,
      "step": 1133500
    },
    {
      "epoch": 43.84112619406737,
      "grad_norm": 16.300989151000977,
      "learning_rate": 1.3465728171610526e-05,
      "loss": 1.9072,
      "step": 1133600
    },
    {
      "epoch": 43.84499361874928,
      "grad_norm": 17.389509201049805,
      "learning_rate": 1.3462505317708937e-05,
      "loss": 1.9157,
      "step": 1133700
    },
    {
      "epoch": 43.84886104343118,
      "grad_norm": 10.648322105407715,
      "learning_rate": 1.3459282463807352e-05,
      "loss": 1.7589,
      "step": 1133800
    },
    {
      "epoch": 43.852728468113085,
      "grad_norm": 11.46181869506836,
      "learning_rate": 1.3456059609905763e-05,
      "loss": 1.7606,
      "step": 1133900
    },
    {
      "epoch": 43.856595892794985,
      "grad_norm": 11.838221549987793,
      "learning_rate": 1.3452836756004178e-05,
      "loss": 1.9569,
      "step": 1134000
    },
    {
      "epoch": 43.86046331747689,
      "grad_norm": 13.288880348205566,
      "learning_rate": 1.3449613902102589e-05,
      "loss": 1.697,
      "step": 1134100
    },
    {
      "epoch": 43.8643307421588,
      "grad_norm": 12.684656143188477,
      "learning_rate": 1.3446391048201004e-05,
      "loss": 1.7714,
      "step": 1134200
    },
    {
      "epoch": 43.8681981668407,
      "grad_norm": 12.1841402053833,
      "learning_rate": 1.3443168194299415e-05,
      "loss": 1.9014,
      "step": 1134300
    },
    {
      "epoch": 43.872065591522606,
      "grad_norm": 11.174983978271484,
      "learning_rate": 1.343994534039783e-05,
      "loss": 1.7464,
      "step": 1134400
    },
    {
      "epoch": 43.875933016204506,
      "grad_norm": 10.934700012207031,
      "learning_rate": 1.3436722486496243e-05,
      "loss": 1.7988,
      "step": 1134500
    },
    {
      "epoch": 43.87980044088641,
      "grad_norm": 13.245467185974121,
      "learning_rate": 1.3433499632594656e-05,
      "loss": 1.9703,
      "step": 1134600
    },
    {
      "epoch": 43.88366786556832,
      "grad_norm": 9.678484916687012,
      "learning_rate": 1.3430276778693069e-05,
      "loss": 1.8353,
      "step": 1134700
    },
    {
      "epoch": 43.88753529025022,
      "grad_norm": 12.639673233032227,
      "learning_rate": 1.3427053924791484e-05,
      "loss": 1.8019,
      "step": 1134800
    },
    {
      "epoch": 43.89140271493213,
      "grad_norm": 10.061826705932617,
      "learning_rate": 1.3423831070889895e-05,
      "loss": 1.853,
      "step": 1134900
    },
    {
      "epoch": 43.895270139614034,
      "grad_norm": 14.46274471282959,
      "learning_rate": 1.342060821698831e-05,
      "loss": 1.8746,
      "step": 1135000
    },
    {
      "epoch": 43.899137564295934,
      "grad_norm": 10.0512056350708,
      "learning_rate": 1.3417385363086721e-05,
      "loss": 1.8412,
      "step": 1135100
    },
    {
      "epoch": 43.90300498897784,
      "grad_norm": 11.93875789642334,
      "learning_rate": 1.3414162509185136e-05,
      "loss": 1.7166,
      "step": 1135200
    },
    {
      "epoch": 43.90687241365974,
      "grad_norm": 12.137700080871582,
      "learning_rate": 1.3410939655283547e-05,
      "loss": 1.7095,
      "step": 1135300
    },
    {
      "epoch": 43.91073983834165,
      "grad_norm": 13.578988075256348,
      "learning_rate": 1.3407716801381962e-05,
      "loss": 1.805,
      "step": 1135400
    },
    {
      "epoch": 43.914607263023555,
      "grad_norm": 12.428566932678223,
      "learning_rate": 1.3404493947480373e-05,
      "loss": 1.8789,
      "step": 1135500
    },
    {
      "epoch": 43.918474687705455,
      "grad_norm": 11.938502311706543,
      "learning_rate": 1.3401271093578788e-05,
      "loss": 1.8128,
      "step": 1135600
    },
    {
      "epoch": 43.92234211238736,
      "grad_norm": 11.621882438659668,
      "learning_rate": 1.3398048239677199e-05,
      "loss": 1.9003,
      "step": 1135700
    },
    {
      "epoch": 43.92620953706927,
      "grad_norm": 9.992148399353027,
      "learning_rate": 1.3394825385775614e-05,
      "loss": 1.8188,
      "step": 1135800
    },
    {
      "epoch": 43.93007696175117,
      "grad_norm": 9.132165908813477,
      "learning_rate": 1.3391602531874025e-05,
      "loss": 1.7403,
      "step": 1135900
    },
    {
      "epoch": 43.933944386433076,
      "grad_norm": 10.391731262207031,
      "learning_rate": 1.3388379677972438e-05,
      "loss": 1.8058,
      "step": 1136000
    },
    {
      "epoch": 43.937811811114976,
      "grad_norm": 13.04742431640625,
      "learning_rate": 1.3385156824070851e-05,
      "loss": 1.8447,
      "step": 1136100
    },
    {
      "epoch": 43.94167923579688,
      "grad_norm": 13.633476257324219,
      "learning_rate": 1.3381933970169264e-05,
      "loss": 1.9067,
      "step": 1136200
    },
    {
      "epoch": 43.94554666047879,
      "grad_norm": 13.218133926391602,
      "learning_rate": 1.3378711116267679e-05,
      "loss": 1.8263,
      "step": 1136300
    },
    {
      "epoch": 43.94941408516069,
      "grad_norm": 13.45327377319336,
      "learning_rate": 1.337548826236609e-05,
      "loss": 1.7799,
      "step": 1136400
    },
    {
      "epoch": 43.9532815098426,
      "grad_norm": 10.292563438415527,
      "learning_rate": 1.3372265408464505e-05,
      "loss": 1.8125,
      "step": 1136500
    },
    {
      "epoch": 43.9571489345245,
      "grad_norm": 9.848443031311035,
      "learning_rate": 1.3369042554562916e-05,
      "loss": 1.8592,
      "step": 1136600
    },
    {
      "epoch": 43.961016359206404,
      "grad_norm": 12.880651473999023,
      "learning_rate": 1.3365819700661331e-05,
      "loss": 1.8498,
      "step": 1136700
    },
    {
      "epoch": 43.96488378388831,
      "grad_norm": 14.45721435546875,
      "learning_rate": 1.3362596846759742e-05,
      "loss": 1.8368,
      "step": 1136800
    },
    {
      "epoch": 43.96875120857021,
      "grad_norm": 11.283204078674316,
      "learning_rate": 1.3359373992858157e-05,
      "loss": 1.795,
      "step": 1136900
    },
    {
      "epoch": 43.97261863325212,
      "grad_norm": 13.653305053710938,
      "learning_rate": 1.3356151138956568e-05,
      "loss": 1.9182,
      "step": 1137000
    },
    {
      "epoch": 43.976486057934025,
      "grad_norm": 12.340672492980957,
      "learning_rate": 1.3352928285054983e-05,
      "loss": 1.7823,
      "step": 1137100
    },
    {
      "epoch": 43.980353482615925,
      "grad_norm": 13.985288619995117,
      "learning_rate": 1.3349705431153394e-05,
      "loss": 1.7731,
      "step": 1137200
    },
    {
      "epoch": 43.98422090729783,
      "grad_norm": 14.154640197753906,
      "learning_rate": 1.334648257725181e-05,
      "loss": 1.8787,
      "step": 1137300
    },
    {
      "epoch": 43.98808833197973,
      "grad_norm": 8.306556701660156,
      "learning_rate": 1.334325972335022e-05,
      "loss": 1.8908,
      "step": 1137400
    },
    {
      "epoch": 43.99195575666164,
      "grad_norm": 11.71086311340332,
      "learning_rate": 1.3340036869448635e-05,
      "loss": 1.8845,
      "step": 1137500
    },
    {
      "epoch": 43.995823181343546,
      "grad_norm": 10.01029109954834,
      "learning_rate": 1.3336814015547047e-05,
      "loss": 1.7174,
      "step": 1137600
    },
    {
      "epoch": 43.999690606025446,
      "grad_norm": 9.958494186401367,
      "learning_rate": 1.3333591161645461e-05,
      "loss": 1.7465,
      "step": 1137700
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.7540956735610962,
      "eval_runtime": 5.5791,
      "eval_samples_per_second": 243.944,
      "eval_steps_per_second": 243.944,
      "step": 1137708
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.5999549627304077,
      "eval_runtime": 107.0099,
      "eval_samples_per_second": 241.632,
      "eval_steps_per_second": 241.632,
      "step": 1137708
    },
    {
      "epoch": 44.00355803070735,
      "grad_norm": 14.730388641357422,
      "learning_rate": 1.3330368307743874e-05,
      "loss": 1.8581,
      "step": 1137800
    },
    {
      "epoch": 44.00742545538925,
      "grad_norm": 13.331103324890137,
      "learning_rate": 1.3327145453842287e-05,
      "loss": 1.7819,
      "step": 1137900
    },
    {
      "epoch": 44.01129288007116,
      "grad_norm": 14.439556121826172,
      "learning_rate": 1.33239225999407e-05,
      "loss": 1.7521,
      "step": 1138000
    },
    {
      "epoch": 44.01516030475307,
      "grad_norm": 11.266532897949219,
      "learning_rate": 1.3320699746039113e-05,
      "loss": 1.7836,
      "step": 1138100
    },
    {
      "epoch": 44.01902772943497,
      "grad_norm": 8.351177215576172,
      "learning_rate": 1.3317476892137526e-05,
      "loss": 1.7915,
      "step": 1138200
    },
    {
      "epoch": 44.022895154116874,
      "grad_norm": 11.592260360717773,
      "learning_rate": 1.3314254038235941e-05,
      "loss": 1.8334,
      "step": 1138300
    },
    {
      "epoch": 44.02676257879878,
      "grad_norm": 12.25464916229248,
      "learning_rate": 1.3311031184334352e-05,
      "loss": 1.859,
      "step": 1138400
    },
    {
      "epoch": 44.03063000348068,
      "grad_norm": 11.667597770690918,
      "learning_rate": 1.3307808330432767e-05,
      "loss": 1.7812,
      "step": 1138500
    },
    {
      "epoch": 44.03449742816259,
      "grad_norm": 12.42898941040039,
      "learning_rate": 1.3304585476531178e-05,
      "loss": 1.8624,
      "step": 1138600
    },
    {
      "epoch": 44.03836485284449,
      "grad_norm": 12.656536102294922,
      "learning_rate": 1.3301362622629593e-05,
      "loss": 1.7672,
      "step": 1138700
    },
    {
      "epoch": 44.042232277526395,
      "grad_norm": 14.213074684143066,
      "learning_rate": 1.3298139768728005e-05,
      "loss": 1.9131,
      "step": 1138800
    },
    {
      "epoch": 44.0460997022083,
      "grad_norm": 13.861291885375977,
      "learning_rate": 1.329491691482642e-05,
      "loss": 1.7092,
      "step": 1138900
    },
    {
      "epoch": 44.0499671268902,
      "grad_norm": 11.35152530670166,
      "learning_rate": 1.329169406092483e-05,
      "loss": 1.8274,
      "step": 1139000
    },
    {
      "epoch": 44.05383455157211,
      "grad_norm": 13.772003173828125,
      "learning_rate": 1.3288471207023242e-05,
      "loss": 1.7437,
      "step": 1139100
    },
    {
      "epoch": 44.05770197625401,
      "grad_norm": 19.056428909301758,
      "learning_rate": 1.3285248353121657e-05,
      "loss": 1.8233,
      "step": 1139200
    },
    {
      "epoch": 44.061569400935916,
      "grad_norm": 14.829458236694336,
      "learning_rate": 1.3282025499220068e-05,
      "loss": 1.8092,
      "step": 1139300
    },
    {
      "epoch": 44.06543682561782,
      "grad_norm": 11.334088325500488,
      "learning_rate": 1.3278802645318483e-05,
      "loss": 1.8578,
      "step": 1139400
    },
    {
      "epoch": 44.06930425029972,
      "grad_norm": 10.909255981445312,
      "learning_rate": 1.3275579791416896e-05,
      "loss": 1.8022,
      "step": 1139500
    },
    {
      "epoch": 44.07317167498163,
      "grad_norm": 12.393457412719727,
      "learning_rate": 1.3272356937515309e-05,
      "loss": 1.8428,
      "step": 1139600
    },
    {
      "epoch": 44.07703909966354,
      "grad_norm": 12.621794700622559,
      "learning_rate": 1.3269134083613722e-05,
      "loss": 1.7788,
      "step": 1139700
    },
    {
      "epoch": 44.08090652434544,
      "grad_norm": 10.633456230163574,
      "learning_rate": 1.3265911229712136e-05,
      "loss": 1.7861,
      "step": 1139800
    },
    {
      "epoch": 44.084773949027344,
      "grad_norm": 16.556486129760742,
      "learning_rate": 1.3262688375810548e-05,
      "loss": 1.7362,
      "step": 1139900
    },
    {
      "epoch": 44.088641373709244,
      "grad_norm": 9.890731811523438,
      "learning_rate": 1.3259465521908963e-05,
      "loss": 1.7758,
      "step": 1140000
    },
    {
      "epoch": 44.09250879839115,
      "grad_norm": 14.426141738891602,
      "learning_rate": 1.3256242668007374e-05,
      "loss": 1.8231,
      "step": 1140100
    },
    {
      "epoch": 44.09637622307306,
      "grad_norm": 8.981178283691406,
      "learning_rate": 1.3253019814105789e-05,
      "loss": 1.7292,
      "step": 1140200
    },
    {
      "epoch": 44.10024364775496,
      "grad_norm": 12.407629013061523,
      "learning_rate": 1.32497969602042e-05,
      "loss": 1.7196,
      "step": 1140300
    },
    {
      "epoch": 44.104111072436865,
      "grad_norm": 14.061545372009277,
      "learning_rate": 1.3246574106302615e-05,
      "loss": 1.8927,
      "step": 1140400
    },
    {
      "epoch": 44.10797849711877,
      "grad_norm": 19.7469482421875,
      "learning_rate": 1.3243351252401026e-05,
      "loss": 1.8822,
      "step": 1140500
    },
    {
      "epoch": 44.11184592180067,
      "grad_norm": 11.506484031677246,
      "learning_rate": 1.324012839849944e-05,
      "loss": 1.7441,
      "step": 1140600
    },
    {
      "epoch": 44.11571334648258,
      "grad_norm": 12.713626861572266,
      "learning_rate": 1.3236905544597852e-05,
      "loss": 1.8403,
      "step": 1140700
    },
    {
      "epoch": 44.11958077116448,
      "grad_norm": 11.602354049682617,
      "learning_rate": 1.3233682690696267e-05,
      "loss": 1.825,
      "step": 1140800
    },
    {
      "epoch": 44.123448195846386,
      "grad_norm": 10.806607246398926,
      "learning_rate": 1.3230459836794678e-05,
      "loss": 1.8603,
      "step": 1140900
    },
    {
      "epoch": 44.12731562052829,
      "grad_norm": 14.223336219787598,
      "learning_rate": 1.3227236982893093e-05,
      "loss": 1.8775,
      "step": 1141000
    },
    {
      "epoch": 44.13118304521019,
      "grad_norm": 14.815925598144531,
      "learning_rate": 1.3224014128991504e-05,
      "loss": 1.7873,
      "step": 1141100
    },
    {
      "epoch": 44.1350504698921,
      "grad_norm": 14.517605781555176,
      "learning_rate": 1.3220791275089919e-05,
      "loss": 1.758,
      "step": 1141200
    },
    {
      "epoch": 44.138917894574,
      "grad_norm": 12.673834800720215,
      "learning_rate": 1.3217568421188332e-05,
      "loss": 1.7647,
      "step": 1141300
    },
    {
      "epoch": 44.14278531925591,
      "grad_norm": 12.859829902648926,
      "learning_rate": 1.3214345567286745e-05,
      "loss": 1.7172,
      "step": 1141400
    },
    {
      "epoch": 44.146652743937814,
      "grad_norm": 11.162708282470703,
      "learning_rate": 1.3211122713385158e-05,
      "loss": 1.7597,
      "step": 1141500
    },
    {
      "epoch": 44.150520168619714,
      "grad_norm": 12.142793655395508,
      "learning_rate": 1.3207899859483573e-05,
      "loss": 1.7975,
      "step": 1141600
    },
    {
      "epoch": 44.15438759330162,
      "grad_norm": 15.02380657196045,
      "learning_rate": 1.3204677005581984e-05,
      "loss": 1.8445,
      "step": 1141700
    },
    {
      "epoch": 44.15825501798353,
      "grad_norm": 13.093106269836426,
      "learning_rate": 1.3201454151680399e-05,
      "loss": 1.8099,
      "step": 1141800
    },
    {
      "epoch": 44.16212244266543,
      "grad_norm": 10.330892562866211,
      "learning_rate": 1.319823129777881e-05,
      "loss": 1.8949,
      "step": 1141900
    },
    {
      "epoch": 44.165989867347335,
      "grad_norm": 8.285845756530762,
      "learning_rate": 1.3195008443877221e-05,
      "loss": 1.7772,
      "step": 1142000
    },
    {
      "epoch": 44.169857292029235,
      "grad_norm": 10.361626625061035,
      "learning_rate": 1.3191785589975636e-05,
      "loss": 1.7762,
      "step": 1142100
    },
    {
      "epoch": 44.17372471671114,
      "grad_norm": 15.544920921325684,
      "learning_rate": 1.3188562736074047e-05,
      "loss": 1.8141,
      "step": 1142200
    },
    {
      "epoch": 44.17759214139305,
      "grad_norm": 13.074335098266602,
      "learning_rate": 1.3185339882172462e-05,
      "loss": 1.8264,
      "step": 1142300
    },
    {
      "epoch": 44.18145956607495,
      "grad_norm": 10.053346633911133,
      "learning_rate": 1.3182117028270873e-05,
      "loss": 1.8881,
      "step": 1142400
    },
    {
      "epoch": 44.185326990756856,
      "grad_norm": 12.531171798706055,
      "learning_rate": 1.3178894174369288e-05,
      "loss": 1.8209,
      "step": 1142500
    },
    {
      "epoch": 44.189194415438756,
      "grad_norm": 13.565692901611328,
      "learning_rate": 1.31756713204677e-05,
      "loss": 1.7437,
      "step": 1142600
    },
    {
      "epoch": 44.19306184012066,
      "grad_norm": 13.649733543395996,
      "learning_rate": 1.3172448466566114e-05,
      "loss": 1.8254,
      "step": 1142700
    },
    {
      "epoch": 44.19692926480257,
      "grad_norm": 11.836129188537598,
      "learning_rate": 1.3169225612664527e-05,
      "loss": 1.8492,
      "step": 1142800
    },
    {
      "epoch": 44.20079668948447,
      "grad_norm": 11.34631061553955,
      "learning_rate": 1.316600275876294e-05,
      "loss": 1.8885,
      "step": 1142900
    },
    {
      "epoch": 44.20466411416638,
      "grad_norm": 11.640149116516113,
      "learning_rate": 1.3162779904861353e-05,
      "loss": 1.8141,
      "step": 1143000
    },
    {
      "epoch": 44.208531538848284,
      "grad_norm": 10.238029479980469,
      "learning_rate": 1.3159557050959766e-05,
      "loss": 1.8515,
      "step": 1143100
    },
    {
      "epoch": 44.212398963530184,
      "grad_norm": 8.62939167022705,
      "learning_rate": 1.315633419705818e-05,
      "loss": 1.908,
      "step": 1143200
    },
    {
      "epoch": 44.21626638821209,
      "grad_norm": 11.851811408996582,
      "learning_rate": 1.3153111343156594e-05,
      "loss": 1.792,
      "step": 1143300
    },
    {
      "epoch": 44.22013381289399,
      "grad_norm": 12.082185745239258,
      "learning_rate": 1.3149888489255005e-05,
      "loss": 1.7978,
      "step": 1143400
    },
    {
      "epoch": 44.2240012375759,
      "grad_norm": 8.717464447021484,
      "learning_rate": 1.314666563535342e-05,
      "loss": 1.7674,
      "step": 1143500
    },
    {
      "epoch": 44.227868662257805,
      "grad_norm": 10.156705856323242,
      "learning_rate": 1.3143442781451831e-05,
      "loss": 1.8954,
      "step": 1143600
    },
    {
      "epoch": 44.231736086939705,
      "grad_norm": 13.135652542114258,
      "learning_rate": 1.3140219927550246e-05,
      "loss": 1.839,
      "step": 1143700
    },
    {
      "epoch": 44.23560351162161,
      "grad_norm": 9.6187105178833,
      "learning_rate": 1.3136997073648657e-05,
      "loss": 1.8257,
      "step": 1143800
    },
    {
      "epoch": 44.23947093630352,
      "grad_norm": 12.782822608947754,
      "learning_rate": 1.3133774219747072e-05,
      "loss": 1.8227,
      "step": 1143900
    },
    {
      "epoch": 44.24333836098542,
      "grad_norm": 10.535280227661133,
      "learning_rate": 1.3130551365845483e-05,
      "loss": 1.8445,
      "step": 1144000
    },
    {
      "epoch": 44.247205785667326,
      "grad_norm": 8.718095779418945,
      "learning_rate": 1.3127328511943898e-05,
      "loss": 1.7971,
      "step": 1144100
    },
    {
      "epoch": 44.251073210349226,
      "grad_norm": 14.511087417602539,
      "learning_rate": 1.312410565804231e-05,
      "loss": 1.7425,
      "step": 1144200
    },
    {
      "epoch": 44.25494063503113,
      "grad_norm": 11.406909942626953,
      "learning_rate": 1.3120882804140724e-05,
      "loss": 1.8459,
      "step": 1144300
    },
    {
      "epoch": 44.25880805971304,
      "grad_norm": 16.169157028198242,
      "learning_rate": 1.3117659950239136e-05,
      "loss": 1.8637,
      "step": 1144400
    },
    {
      "epoch": 44.26267548439494,
      "grad_norm": 13.34328556060791,
      "learning_rate": 1.311443709633755e-05,
      "loss": 1.9587,
      "step": 1144500
    },
    {
      "epoch": 44.26654290907685,
      "grad_norm": 10.553967475891113,
      "learning_rate": 1.3111214242435962e-05,
      "loss": 1.7219,
      "step": 1144600
    },
    {
      "epoch": 44.27041033375875,
      "grad_norm": 12.192826271057129,
      "learning_rate": 1.3107991388534376e-05,
      "loss": 1.712,
      "step": 1144700
    },
    {
      "epoch": 44.274277758440654,
      "grad_norm": 11.0236234664917,
      "learning_rate": 1.310476853463279e-05,
      "loss": 1.7689,
      "step": 1144800
    },
    {
      "epoch": 44.27814518312256,
      "grad_norm": 14.798552513122559,
      "learning_rate": 1.31015456807312e-05,
      "loss": 1.7353,
      "step": 1144900
    },
    {
      "epoch": 44.28201260780446,
      "grad_norm": 13.467835426330566,
      "learning_rate": 1.3098322826829615e-05,
      "loss": 1.7753,
      "step": 1145000
    },
    {
      "epoch": 44.28588003248637,
      "grad_norm": 13.872032165527344,
      "learning_rate": 1.3095099972928027e-05,
      "loss": 1.8031,
      "step": 1145100
    },
    {
      "epoch": 44.289747457168275,
      "grad_norm": 9.532723426818848,
      "learning_rate": 1.3091877119026441e-05,
      "loss": 1.7798,
      "step": 1145200
    },
    {
      "epoch": 44.293614881850175,
      "grad_norm": 9.426124572753906,
      "learning_rate": 1.3088654265124853e-05,
      "loss": 1.8822,
      "step": 1145300
    },
    {
      "epoch": 44.29748230653208,
      "grad_norm": 11.823251724243164,
      "learning_rate": 1.3085431411223268e-05,
      "loss": 1.8294,
      "step": 1145400
    },
    {
      "epoch": 44.30134973121398,
      "grad_norm": 11.135435104370117,
      "learning_rate": 1.3082208557321679e-05,
      "loss": 1.8481,
      "step": 1145500
    },
    {
      "epoch": 44.30521715589589,
      "grad_norm": 10.427699089050293,
      "learning_rate": 1.3078985703420094e-05,
      "loss": 1.8029,
      "step": 1145600
    },
    {
      "epoch": 44.309084580577796,
      "grad_norm": 10.137157440185547,
      "learning_rate": 1.3075762849518505e-05,
      "loss": 1.8298,
      "step": 1145700
    },
    {
      "epoch": 44.312952005259696,
      "grad_norm": 11.042074203491211,
      "learning_rate": 1.307253999561692e-05,
      "loss": 1.7604,
      "step": 1145800
    },
    {
      "epoch": 44.3168194299416,
      "grad_norm": 7.69792366027832,
      "learning_rate": 1.3069317141715331e-05,
      "loss": 1.8367,
      "step": 1145900
    },
    {
      "epoch": 44.3206868546235,
      "grad_norm": 14.206338882446289,
      "learning_rate": 1.3066094287813746e-05,
      "loss": 1.7796,
      "step": 1146000
    },
    {
      "epoch": 44.32455427930541,
      "grad_norm": 11.60758113861084,
      "learning_rate": 1.3062871433912157e-05,
      "loss": 1.808,
      "step": 1146100
    },
    {
      "epoch": 44.32842170398732,
      "grad_norm": 10.067376136779785,
      "learning_rate": 1.3059648580010572e-05,
      "loss": 1.8035,
      "step": 1146200
    },
    {
      "epoch": 44.33228912866922,
      "grad_norm": 25.66982078552246,
      "learning_rate": 1.3056425726108985e-05,
      "loss": 1.8085,
      "step": 1146300
    },
    {
      "epoch": 44.336156553351124,
      "grad_norm": 11.36682415008545,
      "learning_rate": 1.3053202872207398e-05,
      "loss": 1.8295,
      "step": 1146400
    },
    {
      "epoch": 44.34002397803303,
      "grad_norm": 12.070624351501465,
      "learning_rate": 1.304998001830581e-05,
      "loss": 1.7347,
      "step": 1146500
    },
    {
      "epoch": 44.34389140271493,
      "grad_norm": 12.091120719909668,
      "learning_rate": 1.3046757164404226e-05,
      "loss": 1.7567,
      "step": 1146600
    },
    {
      "epoch": 44.34775882739684,
      "grad_norm": 13.795162200927734,
      "learning_rate": 1.3043534310502637e-05,
      "loss": 1.8188,
      "step": 1146700
    },
    {
      "epoch": 44.35162625207874,
      "grad_norm": 21.632568359375,
      "learning_rate": 1.3040311456601052e-05,
      "loss": 1.786,
      "step": 1146800
    },
    {
      "epoch": 44.355493676760645,
      "grad_norm": 11.022921562194824,
      "learning_rate": 1.3037088602699463e-05,
      "loss": 1.82,
      "step": 1146900
    },
    {
      "epoch": 44.35936110144255,
      "grad_norm": 13.722427368164062,
      "learning_rate": 1.3033865748797878e-05,
      "loss": 1.881,
      "step": 1147000
    },
    {
      "epoch": 44.36322852612445,
      "grad_norm": 12.272272109985352,
      "learning_rate": 1.3030642894896289e-05,
      "loss": 1.8758,
      "step": 1147100
    },
    {
      "epoch": 44.36709595080636,
      "grad_norm": 12.448856353759766,
      "learning_rate": 1.3027420040994704e-05,
      "loss": 1.8286,
      "step": 1147200
    },
    {
      "epoch": 44.37096337548826,
      "grad_norm": 12.613038063049316,
      "learning_rate": 1.3024197187093115e-05,
      "loss": 1.7686,
      "step": 1147300
    },
    {
      "epoch": 44.374830800170166,
      "grad_norm": 15.832784652709961,
      "learning_rate": 1.302097433319153e-05,
      "loss": 1.767,
      "step": 1147400
    },
    {
      "epoch": 44.37869822485207,
      "grad_norm": 9.965564727783203,
      "learning_rate": 1.3017751479289941e-05,
      "loss": 1.9355,
      "step": 1147500
    },
    {
      "epoch": 44.38256564953397,
      "grad_norm": 15.883248329162598,
      "learning_rate": 1.3014528625388356e-05,
      "loss": 1.823,
      "step": 1147600
    },
    {
      "epoch": 44.38643307421588,
      "grad_norm": 10.484353065490723,
      "learning_rate": 1.3011305771486767e-05,
      "loss": 1.9185,
      "step": 1147700
    },
    {
      "epoch": 44.39030049889779,
      "grad_norm": 9.302370071411133,
      "learning_rate": 1.300808291758518e-05,
      "loss": 1.846,
      "step": 1147800
    },
    {
      "epoch": 44.39416792357969,
      "grad_norm": 14.52369213104248,
      "learning_rate": 1.3004860063683593e-05,
      "loss": 1.7949,
      "step": 1147900
    },
    {
      "epoch": 44.398035348261594,
      "grad_norm": 13.497734069824219,
      "learning_rate": 1.3001637209782006e-05,
      "loss": 1.8407,
      "step": 1148000
    },
    {
      "epoch": 44.401902772943494,
      "grad_norm": 10.55023193359375,
      "learning_rate": 1.299841435588042e-05,
      "loss": 1.8035,
      "step": 1148100
    },
    {
      "epoch": 44.4057701976254,
      "grad_norm": 11.202188491821289,
      "learning_rate": 1.2995191501978832e-05,
      "loss": 1.8365,
      "step": 1148200
    },
    {
      "epoch": 44.40963762230731,
      "grad_norm": 14.30467700958252,
      "learning_rate": 1.2991968648077247e-05,
      "loss": 1.8078,
      "step": 1148300
    },
    {
      "epoch": 44.41350504698921,
      "grad_norm": 14.806668281555176,
      "learning_rate": 1.2988745794175658e-05,
      "loss": 1.8623,
      "step": 1148400
    },
    {
      "epoch": 44.417372471671115,
      "grad_norm": 9.965381622314453,
      "learning_rate": 1.2985522940274073e-05,
      "loss": 1.8028,
      "step": 1148500
    },
    {
      "epoch": 44.42123989635302,
      "grad_norm": 14.361372947692871,
      "learning_rate": 1.2982300086372484e-05,
      "loss": 1.8063,
      "step": 1148600
    },
    {
      "epoch": 44.42510732103492,
      "grad_norm": 16.239852905273438,
      "learning_rate": 1.2979077232470899e-05,
      "loss": 1.8355,
      "step": 1148700
    },
    {
      "epoch": 44.42897474571683,
      "grad_norm": 13.497394561767578,
      "learning_rate": 1.297585437856931e-05,
      "loss": 1.7552,
      "step": 1148800
    },
    {
      "epoch": 44.43284217039873,
      "grad_norm": 13.378230094909668,
      "learning_rate": 1.2972631524667725e-05,
      "loss": 1.7705,
      "step": 1148900
    },
    {
      "epoch": 44.436709595080636,
      "grad_norm": 12.352311134338379,
      "learning_rate": 1.2969408670766136e-05,
      "loss": 1.8304,
      "step": 1149000
    },
    {
      "epoch": 44.44057701976254,
      "grad_norm": 10.020926475524902,
      "learning_rate": 1.2966185816864551e-05,
      "loss": 1.9037,
      "step": 1149100
    },
    {
      "epoch": 44.44444444444444,
      "grad_norm": 12.191729545593262,
      "learning_rate": 1.2962962962962962e-05,
      "loss": 1.7749,
      "step": 1149200
    },
    {
      "epoch": 44.44831186912635,
      "grad_norm": 11.962496757507324,
      "learning_rate": 1.2959740109061377e-05,
      "loss": 1.8092,
      "step": 1149300
    },
    {
      "epoch": 44.45217929380825,
      "grad_norm": 14.628045082092285,
      "learning_rate": 1.2956517255159789e-05,
      "loss": 1.8444,
      "step": 1149400
    },
    {
      "epoch": 44.45604671849016,
      "grad_norm": 14.653120994567871,
      "learning_rate": 1.2953294401258203e-05,
      "loss": 1.7958,
      "step": 1149500
    },
    {
      "epoch": 44.459914143172064,
      "grad_norm": 13.273465156555176,
      "learning_rate": 1.2950071547356615e-05,
      "loss": 1.8534,
      "step": 1149600
    },
    {
      "epoch": 44.463781567853964,
      "grad_norm": 14.920666694641113,
      "learning_rate": 1.294684869345503e-05,
      "loss": 1.8948,
      "step": 1149700
    },
    {
      "epoch": 44.46764899253587,
      "grad_norm": 11.671957015991211,
      "learning_rate": 1.2943625839553442e-05,
      "loss": 1.843,
      "step": 1149800
    },
    {
      "epoch": 44.47151641721778,
      "grad_norm": 13.251388549804688,
      "learning_rate": 1.2940402985651855e-05,
      "loss": 1.7688,
      "step": 1149900
    },
    {
      "epoch": 44.47538384189968,
      "grad_norm": 13.633427619934082,
      "learning_rate": 1.2937180131750268e-05,
      "loss": 1.8976,
      "step": 1150000
    },
    {
      "epoch": 44.479251266581585,
      "grad_norm": 13.07547664642334,
      "learning_rate": 1.2933957277848683e-05,
      "loss": 1.8994,
      "step": 1150100
    },
    {
      "epoch": 44.483118691263485,
      "grad_norm": 17.53129005432129,
      "learning_rate": 1.2930734423947094e-05,
      "loss": 1.8542,
      "step": 1150200
    },
    {
      "epoch": 44.48698611594539,
      "grad_norm": 12.016624450683594,
      "learning_rate": 1.2927511570045509e-05,
      "loss": 1.9207,
      "step": 1150300
    },
    {
      "epoch": 44.4908535406273,
      "grad_norm": 9.901341438293457,
      "learning_rate": 1.292428871614392e-05,
      "loss": 1.8294,
      "step": 1150400
    },
    {
      "epoch": 44.4947209653092,
      "grad_norm": 14.576813697814941,
      "learning_rate": 1.2921065862242335e-05,
      "loss": 1.7511,
      "step": 1150500
    },
    {
      "epoch": 44.498588389991106,
      "grad_norm": 12.759775161743164,
      "learning_rate": 1.2917843008340746e-05,
      "loss": 1.7585,
      "step": 1150600
    },
    {
      "epoch": 44.502455814673006,
      "grad_norm": 16.385204315185547,
      "learning_rate": 1.2914620154439161e-05,
      "loss": 1.8027,
      "step": 1150700
    },
    {
      "epoch": 44.50632323935491,
      "grad_norm": 13.726617813110352,
      "learning_rate": 1.2911397300537573e-05,
      "loss": 1.83,
      "step": 1150800
    },
    {
      "epoch": 44.51019066403682,
      "grad_norm": 10.918299674987793,
      "learning_rate": 1.2908174446635984e-05,
      "loss": 1.8823,
      "step": 1150900
    },
    {
      "epoch": 44.51405808871872,
      "grad_norm": 11.686187744140625,
      "learning_rate": 1.2904951592734399e-05,
      "loss": 1.7252,
      "step": 1151000
    },
    {
      "epoch": 44.51792551340063,
      "grad_norm": 12.43734359741211,
      "learning_rate": 1.290172873883281e-05,
      "loss": 1.859,
      "step": 1151100
    },
    {
      "epoch": 44.521792938082534,
      "grad_norm": 12.342497825622559,
      "learning_rate": 1.2898505884931225e-05,
      "loss": 1.8901,
      "step": 1151200
    },
    {
      "epoch": 44.525660362764434,
      "grad_norm": 12.906230926513672,
      "learning_rate": 1.2895283031029638e-05,
      "loss": 1.731,
      "step": 1151300
    },
    {
      "epoch": 44.52952778744634,
      "grad_norm": 12.347613334655762,
      "learning_rate": 1.289206017712805e-05,
      "loss": 1.7742,
      "step": 1151400
    },
    {
      "epoch": 44.53339521212824,
      "grad_norm": 8.14997673034668,
      "learning_rate": 1.2888837323226464e-05,
      "loss": 1.8056,
      "step": 1151500
    },
    {
      "epoch": 44.53726263681015,
      "grad_norm": 12.589312553405762,
      "learning_rate": 1.2885614469324878e-05,
      "loss": 1.9211,
      "step": 1151600
    },
    {
      "epoch": 44.541130061492055,
      "grad_norm": 10.790816307067871,
      "learning_rate": 1.288239161542329e-05,
      "loss": 1.8364,
      "step": 1151700
    },
    {
      "epoch": 44.544997486173955,
      "grad_norm": 12.380753517150879,
      "learning_rate": 1.2879168761521704e-05,
      "loss": 1.7586,
      "step": 1151800
    },
    {
      "epoch": 44.54886491085586,
      "grad_norm": 10.658442497253418,
      "learning_rate": 1.2875945907620116e-05,
      "loss": 1.8274,
      "step": 1151900
    },
    {
      "epoch": 44.55273233553777,
      "grad_norm": 12.881667137145996,
      "learning_rate": 1.287272305371853e-05,
      "loss": 1.8775,
      "step": 1152000
    },
    {
      "epoch": 44.55659976021967,
      "grad_norm": 11.303502082824707,
      "learning_rate": 1.2869500199816942e-05,
      "loss": 1.6701,
      "step": 1152100
    },
    {
      "epoch": 44.560467184901576,
      "grad_norm": 14.053502082824707,
      "learning_rate": 1.2866277345915357e-05,
      "loss": 1.8391,
      "step": 1152200
    },
    {
      "epoch": 44.564334609583476,
      "grad_norm": 9.788009643554688,
      "learning_rate": 1.2863054492013768e-05,
      "loss": 1.8557,
      "step": 1152300
    },
    {
      "epoch": 44.56820203426538,
      "grad_norm": 12.82675838470459,
      "learning_rate": 1.2859831638112183e-05,
      "loss": 1.7471,
      "step": 1152400
    },
    {
      "epoch": 44.57206945894729,
      "grad_norm": 16.277395248413086,
      "learning_rate": 1.2856608784210594e-05,
      "loss": 1.7961,
      "step": 1152500
    },
    {
      "epoch": 44.57593688362919,
      "grad_norm": 13.422486305236816,
      "learning_rate": 1.2853385930309009e-05,
      "loss": 1.8378,
      "step": 1152600
    },
    {
      "epoch": 44.5798043083111,
      "grad_norm": 13.035477638244629,
      "learning_rate": 1.285016307640742e-05,
      "loss": 1.7697,
      "step": 1152700
    },
    {
      "epoch": 44.583671732993,
      "grad_norm": 10.940732955932617,
      "learning_rate": 1.2846940222505835e-05,
      "loss": 1.8196,
      "step": 1152800
    },
    {
      "epoch": 44.587539157674904,
      "grad_norm": 11.225905418395996,
      "learning_rate": 1.2843717368604246e-05,
      "loss": 1.8334,
      "step": 1152900
    },
    {
      "epoch": 44.59140658235681,
      "grad_norm": 15.1743803024292,
      "learning_rate": 1.284049451470266e-05,
      "loss": 1.7632,
      "step": 1153000
    },
    {
      "epoch": 44.59527400703871,
      "grad_norm": 12.615540504455566,
      "learning_rate": 1.2837271660801072e-05,
      "loss": 1.7528,
      "step": 1153100
    },
    {
      "epoch": 44.59914143172062,
      "grad_norm": 12.405086517333984,
      "learning_rate": 1.2834048806899487e-05,
      "loss": 1.8258,
      "step": 1153200
    },
    {
      "epoch": 44.603008856402525,
      "grad_norm": 15.753216743469238,
      "learning_rate": 1.28308259529979e-05,
      "loss": 1.8125,
      "step": 1153300
    },
    {
      "epoch": 44.606876281084425,
      "grad_norm": 11.854665756225586,
      "learning_rate": 1.2827603099096313e-05,
      "loss": 1.7773,
      "step": 1153400
    },
    {
      "epoch": 44.61074370576633,
      "grad_norm": 11.780255317687988,
      "learning_rate": 1.2824380245194726e-05,
      "loss": 1.8602,
      "step": 1153500
    },
    {
      "epoch": 44.61461113044823,
      "grad_norm": 11.78658390045166,
      "learning_rate": 1.282115739129314e-05,
      "loss": 1.7712,
      "step": 1153600
    },
    {
      "epoch": 44.61847855513014,
      "grad_norm": 12.168071746826172,
      "learning_rate": 1.2817934537391552e-05,
      "loss": 1.8208,
      "step": 1153700
    },
    {
      "epoch": 44.622345979812046,
      "grad_norm": 11.256326675415039,
      "learning_rate": 1.2814711683489963e-05,
      "loss": 1.8873,
      "step": 1153800
    },
    {
      "epoch": 44.626213404493946,
      "grad_norm": 11.463433265686035,
      "learning_rate": 1.2811488829588378e-05,
      "loss": 1.7874,
      "step": 1153900
    },
    {
      "epoch": 44.63008082917585,
      "grad_norm": 11.081948280334473,
      "learning_rate": 1.280826597568679e-05,
      "loss": 1.7533,
      "step": 1154000
    },
    {
      "epoch": 44.63394825385775,
      "grad_norm": 13.456160545349121,
      "learning_rate": 1.2805043121785204e-05,
      "loss": 1.7897,
      "step": 1154100
    },
    {
      "epoch": 44.63781567853966,
      "grad_norm": 11.740056037902832,
      "learning_rate": 1.2801820267883615e-05,
      "loss": 1.8165,
      "step": 1154200
    },
    {
      "epoch": 44.64168310322157,
      "grad_norm": 10.758625984191895,
      "learning_rate": 1.279859741398203e-05,
      "loss": 1.8109,
      "step": 1154300
    },
    {
      "epoch": 44.64555052790347,
      "grad_norm": 12.453824996948242,
      "learning_rate": 1.2795374560080441e-05,
      "loss": 1.7568,
      "step": 1154400
    },
    {
      "epoch": 44.649417952585374,
      "grad_norm": 11.719423294067383,
      "learning_rate": 1.2792151706178856e-05,
      "loss": 1.7962,
      "step": 1154500
    },
    {
      "epoch": 44.65328537726728,
      "grad_norm": 11.738837242126465,
      "learning_rate": 1.2788928852277267e-05,
      "loss": 1.7487,
      "step": 1154600
    },
    {
      "epoch": 44.65715280194918,
      "grad_norm": 11.669196128845215,
      "learning_rate": 1.2785705998375682e-05,
      "loss": 1.8768,
      "step": 1154700
    },
    {
      "epoch": 44.66102022663109,
      "grad_norm": 9.265619277954102,
      "learning_rate": 1.2782483144474095e-05,
      "loss": 1.8416,
      "step": 1154800
    },
    {
      "epoch": 44.66488765131299,
      "grad_norm": 13.62474250793457,
      "learning_rate": 1.2779260290572508e-05,
      "loss": 1.8226,
      "step": 1154900
    },
    {
      "epoch": 44.668755075994895,
      "grad_norm": 9.955902099609375,
      "learning_rate": 1.2776037436670921e-05,
      "loss": 1.8234,
      "step": 1155000
    },
    {
      "epoch": 44.6726225006768,
      "grad_norm": 14.375564575195312,
      "learning_rate": 1.2772814582769336e-05,
      "loss": 1.8238,
      "step": 1155100
    },
    {
      "epoch": 44.6764899253587,
      "grad_norm": 12.27266788482666,
      "learning_rate": 1.2769591728867747e-05,
      "loss": 1.8381,
      "step": 1155200
    },
    {
      "epoch": 44.68035735004061,
      "grad_norm": 12.97705078125,
      "learning_rate": 1.2766368874966162e-05,
      "loss": 1.7028,
      "step": 1155300
    },
    {
      "epoch": 44.68422477472251,
      "grad_norm": 14.954020500183105,
      "learning_rate": 1.2763146021064573e-05,
      "loss": 1.7285,
      "step": 1155400
    },
    {
      "epoch": 44.688092199404416,
      "grad_norm": 10.440631866455078,
      "learning_rate": 1.2759923167162988e-05,
      "loss": 1.7488,
      "step": 1155500
    },
    {
      "epoch": 44.69195962408632,
      "grad_norm": 12.779194831848145,
      "learning_rate": 1.27567003132614e-05,
      "loss": 1.8137,
      "step": 1155600
    },
    {
      "epoch": 44.69582704876822,
      "grad_norm": 12.412201881408691,
      "learning_rate": 1.2753477459359814e-05,
      "loss": 1.8026,
      "step": 1155700
    },
    {
      "epoch": 44.69969447345013,
      "grad_norm": 14.2662353515625,
      "learning_rate": 1.2750254605458225e-05,
      "loss": 1.8328,
      "step": 1155800
    },
    {
      "epoch": 44.70356189813204,
      "grad_norm": 11.491168022155762,
      "learning_rate": 1.274703175155664e-05,
      "loss": 1.7317,
      "step": 1155900
    },
    {
      "epoch": 44.70742932281394,
      "grad_norm": 13.904928207397461,
      "learning_rate": 1.2743808897655051e-05,
      "loss": 1.792,
      "step": 1156000
    },
    {
      "epoch": 44.711296747495844,
      "grad_norm": 12.500418663024902,
      "learning_rate": 1.2740586043753466e-05,
      "loss": 1.8317,
      "step": 1156100
    },
    {
      "epoch": 44.715164172177744,
      "grad_norm": 13.844231605529785,
      "learning_rate": 1.2737363189851878e-05,
      "loss": 1.8749,
      "step": 1156200
    },
    {
      "epoch": 44.71903159685965,
      "grad_norm": 9.82912540435791,
      "learning_rate": 1.2734140335950292e-05,
      "loss": 1.8487,
      "step": 1156300
    },
    {
      "epoch": 44.72289902154156,
      "grad_norm": 11.110761642456055,
      "learning_rate": 1.2730917482048704e-05,
      "loss": 1.7901,
      "step": 1156400
    },
    {
      "epoch": 44.72676644622346,
      "grad_norm": 13.86346435546875,
      "learning_rate": 1.2727694628147118e-05,
      "loss": 1.8552,
      "step": 1156500
    },
    {
      "epoch": 44.730633870905365,
      "grad_norm": 6.238131999969482,
      "learning_rate": 1.2724471774245531e-05,
      "loss": 1.9394,
      "step": 1156600
    },
    {
      "epoch": 44.73450129558727,
      "grad_norm": 15.816222190856934,
      "learning_rate": 1.2721248920343943e-05,
      "loss": 1.8951,
      "step": 1156700
    },
    {
      "epoch": 44.73836872026917,
      "grad_norm": 12.345823287963867,
      "learning_rate": 1.2718026066442357e-05,
      "loss": 1.7346,
      "step": 1156800
    },
    {
      "epoch": 44.74223614495108,
      "grad_norm": 10.86875057220459,
      "learning_rate": 1.2714803212540769e-05,
      "loss": 1.8035,
      "step": 1156900
    },
    {
      "epoch": 44.74610356963298,
      "grad_norm": 13.431715965270996,
      "learning_rate": 1.2711580358639183e-05,
      "loss": 1.7452,
      "step": 1157000
    },
    {
      "epoch": 44.749970994314886,
      "grad_norm": 12.219710350036621,
      "learning_rate": 1.2708357504737595e-05,
      "loss": 1.8793,
      "step": 1157100
    },
    {
      "epoch": 44.75383841899679,
      "grad_norm": 12.737015724182129,
      "learning_rate": 1.270513465083601e-05,
      "loss": 1.8385,
      "step": 1157200
    },
    {
      "epoch": 44.75770584367869,
      "grad_norm": 11.048179626464844,
      "learning_rate": 1.270191179693442e-05,
      "loss": 1.8446,
      "step": 1157300
    },
    {
      "epoch": 44.7615732683606,
      "grad_norm": 11.38039493560791,
      "learning_rate": 1.2698688943032836e-05,
      "loss": 1.798,
      "step": 1157400
    },
    {
      "epoch": 44.7654406930425,
      "grad_norm": 13.532828330993652,
      "learning_rate": 1.2695466089131247e-05,
      "loss": 1.6844,
      "step": 1157500
    },
    {
      "epoch": 44.76930811772441,
      "grad_norm": 14.366806983947754,
      "learning_rate": 1.2692243235229662e-05,
      "loss": 1.7953,
      "step": 1157600
    },
    {
      "epoch": 44.773175542406314,
      "grad_norm": 13.567391395568848,
      "learning_rate": 1.2689020381328073e-05,
      "loss": 1.865,
      "step": 1157700
    },
    {
      "epoch": 44.777042967088214,
      "grad_norm": 13.964458465576172,
      "learning_rate": 1.2685797527426488e-05,
      "loss": 1.8339,
      "step": 1157800
    },
    {
      "epoch": 44.78091039177012,
      "grad_norm": 10.189085960388184,
      "learning_rate": 1.2682574673524899e-05,
      "loss": 1.7419,
      "step": 1157900
    },
    {
      "epoch": 44.78477781645203,
      "grad_norm": 15.519526481628418,
      "learning_rate": 1.2679351819623314e-05,
      "loss": 1.8178,
      "step": 1158000
    },
    {
      "epoch": 44.78864524113393,
      "grad_norm": 10.53610897064209,
      "learning_rate": 1.2676128965721725e-05,
      "loss": 1.7515,
      "step": 1158100
    },
    {
      "epoch": 44.792512665815835,
      "grad_norm": 10.207818984985352,
      "learning_rate": 1.267290611182014e-05,
      "loss": 1.795,
      "step": 1158200
    },
    {
      "epoch": 44.796380090497735,
      "grad_norm": 13.092838287353516,
      "learning_rate": 1.2669683257918553e-05,
      "loss": 1.8731,
      "step": 1158300
    },
    {
      "epoch": 44.80024751517964,
      "grad_norm": 14.56053638458252,
      "learning_rate": 1.2666460404016966e-05,
      "loss": 1.7921,
      "step": 1158400
    },
    {
      "epoch": 44.80411493986155,
      "grad_norm": 14.178683280944824,
      "learning_rate": 1.2663237550115379e-05,
      "loss": 1.8808,
      "step": 1158500
    },
    {
      "epoch": 44.80798236454345,
      "grad_norm": 14.533966064453125,
      "learning_rate": 1.2660014696213794e-05,
      "loss": 1.8345,
      "step": 1158600
    },
    {
      "epoch": 44.811849789225356,
      "grad_norm": 12.03799819946289,
      "learning_rate": 1.2656791842312205e-05,
      "loss": 1.7389,
      "step": 1158700
    },
    {
      "epoch": 44.815717213907256,
      "grad_norm": 11.930305480957031,
      "learning_rate": 1.265356898841062e-05,
      "loss": 1.7702,
      "step": 1158800
    },
    {
      "epoch": 44.81958463858916,
      "grad_norm": 10.498703956604004,
      "learning_rate": 1.2650346134509031e-05,
      "loss": 1.7495,
      "step": 1158900
    },
    {
      "epoch": 44.82345206327107,
      "grad_norm": 11.847375869750977,
      "learning_rate": 1.2647123280607446e-05,
      "loss": 1.8154,
      "step": 1159000
    },
    {
      "epoch": 44.82731948795297,
      "grad_norm": 15.431884765625,
      "learning_rate": 1.2643900426705857e-05,
      "loss": 1.7758,
      "step": 1159100
    },
    {
      "epoch": 44.83118691263488,
      "grad_norm": 14.589555740356445,
      "learning_rate": 1.2640677572804272e-05,
      "loss": 1.7666,
      "step": 1159200
    },
    {
      "epoch": 44.835054337316784,
      "grad_norm": 11.569515228271484,
      "learning_rate": 1.2637454718902683e-05,
      "loss": 1.8082,
      "step": 1159300
    },
    {
      "epoch": 44.838921761998684,
      "grad_norm": 12.060193061828613,
      "learning_rate": 1.2634231865001098e-05,
      "loss": 1.8166,
      "step": 1159400
    },
    {
      "epoch": 44.84278918668059,
      "grad_norm": 11.95671558380127,
      "learning_rate": 1.2631009011099509e-05,
      "loss": 1.7325,
      "step": 1159500
    },
    {
      "epoch": 44.84665661136249,
      "grad_norm": 9.04577922821045,
      "learning_rate": 1.262778615719792e-05,
      "loss": 1.897,
      "step": 1159600
    },
    {
      "epoch": 44.8505240360444,
      "grad_norm": 14.235607147216797,
      "learning_rate": 1.2624563303296335e-05,
      "loss": 1.769,
      "step": 1159700
    },
    {
      "epoch": 44.854391460726305,
      "grad_norm": 15.992341041564941,
      "learning_rate": 1.2621340449394748e-05,
      "loss": 1.7173,
      "step": 1159800
    },
    {
      "epoch": 44.858258885408205,
      "grad_norm": 17.231914520263672,
      "learning_rate": 1.2618117595493161e-05,
      "loss": 1.8264,
      "step": 1159900
    },
    {
      "epoch": 44.86212631009011,
      "grad_norm": 14.788106918334961,
      "learning_rate": 1.2614894741591574e-05,
      "loss": 1.7371,
      "step": 1160000
    },
    {
      "epoch": 44.86599373477202,
      "grad_norm": 11.511438369750977,
      "learning_rate": 1.2611671887689989e-05,
      "loss": 1.69,
      "step": 1160100
    },
    {
      "epoch": 44.86986115945392,
      "grad_norm": 13.939848899841309,
      "learning_rate": 1.26084490337884e-05,
      "loss": 1.7792,
      "step": 1160200
    },
    {
      "epoch": 44.873728584135826,
      "grad_norm": 12.969990730285645,
      "learning_rate": 1.2605226179886815e-05,
      "loss": 1.8167,
      "step": 1160300
    },
    {
      "epoch": 44.877596008817726,
      "grad_norm": 9.88205623626709,
      "learning_rate": 1.2602003325985226e-05,
      "loss": 1.8132,
      "step": 1160400
    },
    {
      "epoch": 44.88146343349963,
      "grad_norm": 12.579004287719727,
      "learning_rate": 1.2598780472083641e-05,
      "loss": 1.8643,
      "step": 1160500
    },
    {
      "epoch": 44.88533085818154,
      "grad_norm": 11.659162521362305,
      "learning_rate": 1.2595557618182052e-05,
      "loss": 1.7615,
      "step": 1160600
    },
    {
      "epoch": 44.88919828286344,
      "grad_norm": 10.113094329833984,
      "learning_rate": 1.2592334764280467e-05,
      "loss": 1.8592,
      "step": 1160700
    },
    {
      "epoch": 44.89306570754535,
      "grad_norm": 11.11108112335205,
      "learning_rate": 1.2589111910378878e-05,
      "loss": 1.7818,
      "step": 1160800
    },
    {
      "epoch": 44.89693313222725,
      "grad_norm": 15.933113098144531,
      "learning_rate": 1.2585889056477293e-05,
      "loss": 1.8666,
      "step": 1160900
    },
    {
      "epoch": 44.900800556909154,
      "grad_norm": 9.916563987731934,
      "learning_rate": 1.2582666202575704e-05,
      "loss": 1.7531,
      "step": 1161000
    },
    {
      "epoch": 44.90466798159106,
      "grad_norm": 8.641597747802734,
      "learning_rate": 1.2579443348674119e-05,
      "loss": 1.8497,
      "step": 1161100
    },
    {
      "epoch": 44.90853540627296,
      "grad_norm": 11.943785667419434,
      "learning_rate": 1.257622049477253e-05,
      "loss": 1.7884,
      "step": 1161200
    },
    {
      "epoch": 44.91240283095487,
      "grad_norm": 11.856456756591797,
      "learning_rate": 1.2572997640870945e-05,
      "loss": 1.767,
      "step": 1161300
    },
    {
      "epoch": 44.916270255636775,
      "grad_norm": 15.336028099060059,
      "learning_rate": 1.2569774786969357e-05,
      "loss": 1.748,
      "step": 1161400
    },
    {
      "epoch": 44.920137680318675,
      "grad_norm": 14.230122566223145,
      "learning_rate": 1.2566551933067771e-05,
      "loss": 1.8099,
      "step": 1161500
    },
    {
      "epoch": 44.92400510500058,
      "grad_norm": 10.446215629577637,
      "learning_rate": 1.2563329079166184e-05,
      "loss": 1.815,
      "step": 1161600
    },
    {
      "epoch": 44.92787252968248,
      "grad_norm": 9.7874174118042,
      "learning_rate": 1.2560106225264597e-05,
      "loss": 1.802,
      "step": 1161700
    },
    {
      "epoch": 44.93173995436439,
      "grad_norm": 10.4324369430542,
      "learning_rate": 1.255688337136301e-05,
      "loss": 1.7549,
      "step": 1161800
    },
    {
      "epoch": 44.935607379046296,
      "grad_norm": 13.637182235717773,
      "learning_rate": 1.2553660517461423e-05,
      "loss": 1.8245,
      "step": 1161900
    },
    {
      "epoch": 44.939474803728196,
      "grad_norm": 9.762223243713379,
      "learning_rate": 1.2550437663559836e-05,
      "loss": 1.7856,
      "step": 1162000
    },
    {
      "epoch": 44.9433422284101,
      "grad_norm": 11.117173194885254,
      "learning_rate": 1.2547214809658251e-05,
      "loss": 1.8509,
      "step": 1162100
    },
    {
      "epoch": 44.947209653092,
      "grad_norm": 12.672889709472656,
      "learning_rate": 1.2543991955756662e-05,
      "loss": 1.8607,
      "step": 1162200
    },
    {
      "epoch": 44.95107707777391,
      "grad_norm": 10.346405982971191,
      "learning_rate": 1.2540769101855077e-05,
      "loss": 1.8199,
      "step": 1162300
    },
    {
      "epoch": 44.95494450245582,
      "grad_norm": 10.859549522399902,
      "learning_rate": 1.2537546247953488e-05,
      "loss": 1.7876,
      "step": 1162400
    },
    {
      "epoch": 44.95881192713772,
      "grad_norm": 10.140155792236328,
      "learning_rate": 1.2534323394051903e-05,
      "loss": 1.6974,
      "step": 1162500
    },
    {
      "epoch": 44.962679351819624,
      "grad_norm": 8.8082857131958,
      "learning_rate": 1.2531100540150314e-05,
      "loss": 1.9038,
      "step": 1162600
    },
    {
      "epoch": 44.96654677650153,
      "grad_norm": 14.225273132324219,
      "learning_rate": 1.2527877686248726e-05,
      "loss": 1.8103,
      "step": 1162700
    },
    {
      "epoch": 44.97041420118343,
      "grad_norm": 14.180691719055176,
      "learning_rate": 1.252465483234714e-05,
      "loss": 1.7723,
      "step": 1162800
    },
    {
      "epoch": 44.97428162586534,
      "grad_norm": 10.14013385772705,
      "learning_rate": 1.2521431978445552e-05,
      "loss": 1.7723,
      "step": 1162900
    },
    {
      "epoch": 44.97814905054724,
      "grad_norm": 14.82715892791748,
      "learning_rate": 1.2518209124543967e-05,
      "loss": 1.7556,
      "step": 1163000
    },
    {
      "epoch": 44.982016475229145,
      "grad_norm": 11.643173217773438,
      "learning_rate": 1.2514986270642378e-05,
      "loss": 1.7664,
      "step": 1163100
    },
    {
      "epoch": 44.98588389991105,
      "grad_norm": 10.775150299072266,
      "learning_rate": 1.2511763416740793e-05,
      "loss": 1.7689,
      "step": 1163200
    },
    {
      "epoch": 44.98975132459295,
      "grad_norm": 12.002750396728516,
      "learning_rate": 1.2508540562839206e-05,
      "loss": 1.8814,
      "step": 1163300
    },
    {
      "epoch": 44.99361874927486,
      "grad_norm": 10.066651344299316,
      "learning_rate": 1.2505317708937619e-05,
      "loss": 1.7521,
      "step": 1163400
    },
    {
      "epoch": 44.99748617395676,
      "grad_norm": 11.140069007873535,
      "learning_rate": 1.2502094855036032e-05,
      "loss": 1.8696,
      "step": 1163500
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.7528222799301147,
      "eval_runtime": 5.7697,
      "eval_samples_per_second": 235.887,
      "eval_steps_per_second": 235.887,
      "step": 1163565
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.5985386371612549,
      "eval_runtime": 108.2801,
      "eval_samples_per_second": 238.797,
      "eval_steps_per_second": 238.797,
      "step": 1163565
    },
    {
      "epoch": 45.001353598638666,
      "grad_norm": 11.512578010559082,
      "learning_rate": 1.2498872001134446e-05,
      "loss": 1.801,
      "step": 1163600
    },
    {
      "epoch": 45.00522102332057,
      "grad_norm": 16.451351165771484,
      "learning_rate": 1.249564914723286e-05,
      "loss": 1.7876,
      "step": 1163700
    },
    {
      "epoch": 45.00908844800247,
      "grad_norm": 14.153104782104492,
      "learning_rate": 1.2492426293331272e-05,
      "loss": 1.8302,
      "step": 1163800
    },
    {
      "epoch": 45.01295587268438,
      "grad_norm": 12.06611442565918,
      "learning_rate": 1.2489203439429686e-05,
      "loss": 1.8263,
      "step": 1163900
    },
    {
      "epoch": 45.01682329736629,
      "grad_norm": 12.284502029418945,
      "learning_rate": 1.2485980585528099e-05,
      "loss": 1.8305,
      "step": 1164000
    },
    {
      "epoch": 45.02069072204819,
      "grad_norm": 13.121476173400879,
      "learning_rate": 1.248275773162651e-05,
      "loss": 1.8216,
      "step": 1164100
    },
    {
      "epoch": 45.024558146730094,
      "grad_norm": 15.103480339050293,
      "learning_rate": 1.2479534877724923e-05,
      "loss": 1.7032,
      "step": 1164200
    },
    {
      "epoch": 45.028425571411994,
      "grad_norm": 10.354377746582031,
      "learning_rate": 1.2476312023823336e-05,
      "loss": 1.8132,
      "step": 1164300
    },
    {
      "epoch": 45.0322929960939,
      "grad_norm": 14.276060104370117,
      "learning_rate": 1.2473089169921749e-05,
      "loss": 1.7518,
      "step": 1164400
    },
    {
      "epoch": 45.03616042077581,
      "grad_norm": 14.262803077697754,
      "learning_rate": 1.2469866316020162e-05,
      "loss": 1.8322,
      "step": 1164500
    },
    {
      "epoch": 45.04002784545771,
      "grad_norm": 12.965560913085938,
      "learning_rate": 1.2466643462118575e-05,
      "loss": 1.8173,
      "step": 1164600
    },
    {
      "epoch": 45.043895270139615,
      "grad_norm": 12.21139144897461,
      "learning_rate": 1.2463420608216988e-05,
      "loss": 1.9242,
      "step": 1164700
    },
    {
      "epoch": 45.04776269482152,
      "grad_norm": 12.905815124511719,
      "learning_rate": 1.2460197754315401e-05,
      "loss": 1.8307,
      "step": 1164800
    },
    {
      "epoch": 45.05163011950342,
      "grad_norm": 10.247648239135742,
      "learning_rate": 1.2456974900413814e-05,
      "loss": 1.7822,
      "step": 1164900
    },
    {
      "epoch": 45.05549754418533,
      "grad_norm": 8.842774391174316,
      "learning_rate": 1.2453752046512227e-05,
      "loss": 1.8147,
      "step": 1165000
    },
    {
      "epoch": 45.05936496886723,
      "grad_norm": 14.826593399047852,
      "learning_rate": 1.2450529192610642e-05,
      "loss": 1.7868,
      "step": 1165100
    },
    {
      "epoch": 45.063232393549136,
      "grad_norm": 15.845829963684082,
      "learning_rate": 1.2447306338709055e-05,
      "loss": 1.7025,
      "step": 1165200
    },
    {
      "epoch": 45.06709981823104,
      "grad_norm": 16.239744186401367,
      "learning_rate": 1.2444083484807468e-05,
      "loss": 1.9317,
      "step": 1165300
    },
    {
      "epoch": 45.07096724291294,
      "grad_norm": 11.559553146362305,
      "learning_rate": 1.2440860630905881e-05,
      "loss": 1.8094,
      "step": 1165400
    },
    {
      "epoch": 45.07483466759485,
      "grad_norm": 10.589367866516113,
      "learning_rate": 1.2437637777004294e-05,
      "loss": 1.7883,
      "step": 1165500
    },
    {
      "epoch": 45.07870209227675,
      "grad_norm": 10.791000366210938,
      "learning_rate": 1.2434414923102707e-05,
      "loss": 1.8137,
      "step": 1165600
    },
    {
      "epoch": 45.08256951695866,
      "grad_norm": 11.231093406677246,
      "learning_rate": 1.243119206920112e-05,
      "loss": 1.931,
      "step": 1165700
    },
    {
      "epoch": 45.086436941640564,
      "grad_norm": 12.021445274353027,
      "learning_rate": 1.2427969215299533e-05,
      "loss": 1.7035,
      "step": 1165800
    },
    {
      "epoch": 45.090304366322464,
      "grad_norm": 12.198939323425293,
      "learning_rate": 1.2424746361397946e-05,
      "loss": 1.7655,
      "step": 1165900
    },
    {
      "epoch": 45.09417179100437,
      "grad_norm": 11.408536911010742,
      "learning_rate": 1.2421523507496359e-05,
      "loss": 1.7676,
      "step": 1166000
    },
    {
      "epoch": 45.09803921568628,
      "grad_norm": 10.917488098144531,
      "learning_rate": 1.2418300653594772e-05,
      "loss": 1.7405,
      "step": 1166100
    },
    {
      "epoch": 45.10190664036818,
      "grad_norm": 11.624137878417969,
      "learning_rate": 1.2415077799693185e-05,
      "loss": 1.711,
      "step": 1166200
    },
    {
      "epoch": 45.105774065050085,
      "grad_norm": 18.48221778869629,
      "learning_rate": 1.2411854945791598e-05,
      "loss": 1.7422,
      "step": 1166300
    },
    {
      "epoch": 45.109641489731985,
      "grad_norm": 11.76495361328125,
      "learning_rate": 1.2408632091890011e-05,
      "loss": 1.756,
      "step": 1166400
    },
    {
      "epoch": 45.11350891441389,
      "grad_norm": 14.93789291381836,
      "learning_rate": 1.2405409237988424e-05,
      "loss": 1.8287,
      "step": 1166500
    },
    {
      "epoch": 45.1173763390958,
      "grad_norm": 10.479676246643066,
      "learning_rate": 1.2402186384086837e-05,
      "loss": 1.8156,
      "step": 1166600
    },
    {
      "epoch": 45.1212437637777,
      "grad_norm": 9.292266845703125,
      "learning_rate": 1.239896353018525e-05,
      "loss": 1.8007,
      "step": 1166700
    },
    {
      "epoch": 45.125111188459606,
      "grad_norm": 13.859357833862305,
      "learning_rate": 1.2395740676283663e-05,
      "loss": 1.7681,
      "step": 1166800
    },
    {
      "epoch": 45.128978613141506,
      "grad_norm": 10.915289878845215,
      "learning_rate": 1.2392517822382076e-05,
      "loss": 1.8064,
      "step": 1166900
    },
    {
      "epoch": 45.13284603782341,
      "grad_norm": 11.801012992858887,
      "learning_rate": 1.238929496848049e-05,
      "loss": 1.766,
      "step": 1167000
    },
    {
      "epoch": 45.13671346250532,
      "grad_norm": 13.835517883300781,
      "learning_rate": 1.2386072114578902e-05,
      "loss": 1.8184,
      "step": 1167100
    },
    {
      "epoch": 45.14058088718722,
      "grad_norm": 15.907832145690918,
      "learning_rate": 1.2382849260677315e-05,
      "loss": 1.7448,
      "step": 1167200
    },
    {
      "epoch": 45.14444831186913,
      "grad_norm": 11.693799018859863,
      "learning_rate": 1.2379626406775728e-05,
      "loss": 1.8992,
      "step": 1167300
    },
    {
      "epoch": 45.148315736551034,
      "grad_norm": 12.717846870422363,
      "learning_rate": 1.2376403552874141e-05,
      "loss": 1.947,
      "step": 1167400
    },
    {
      "epoch": 45.152183161232934,
      "grad_norm": 16.585540771484375,
      "learning_rate": 1.2373180698972554e-05,
      "loss": 1.7603,
      "step": 1167500
    },
    {
      "epoch": 45.15605058591484,
      "grad_norm": 15.155511856079102,
      "learning_rate": 1.2369957845070967e-05,
      "loss": 1.8131,
      "step": 1167600
    },
    {
      "epoch": 45.15991801059674,
      "grad_norm": 10.117051124572754,
      "learning_rate": 1.236673499116938e-05,
      "loss": 1.7772,
      "step": 1167700
    },
    {
      "epoch": 45.16378543527865,
      "grad_norm": 13.546600341796875,
      "learning_rate": 1.2363512137267793e-05,
      "loss": 1.8918,
      "step": 1167800
    },
    {
      "epoch": 45.167652859960555,
      "grad_norm": 13.600247383117676,
      "learning_rate": 1.2360289283366206e-05,
      "loss": 1.884,
      "step": 1167900
    },
    {
      "epoch": 45.171520284642455,
      "grad_norm": 10.915929794311523,
      "learning_rate": 1.235706642946462e-05,
      "loss": 1.7591,
      "step": 1168000
    },
    {
      "epoch": 45.17538770932436,
      "grad_norm": 15.887956619262695,
      "learning_rate": 1.2353843575563033e-05,
      "loss": 1.7664,
      "step": 1168100
    },
    {
      "epoch": 45.17925513400627,
      "grad_norm": 12.378812789916992,
      "learning_rate": 1.2350620721661446e-05,
      "loss": 1.8335,
      "step": 1168200
    },
    {
      "epoch": 45.18312255868817,
      "grad_norm": 13.02529239654541,
      "learning_rate": 1.2347397867759859e-05,
      "loss": 1.7795,
      "step": 1168300
    },
    {
      "epoch": 45.186989983370076,
      "grad_norm": 12.984809875488281,
      "learning_rate": 1.2344175013858272e-05,
      "loss": 1.7904,
      "step": 1168400
    },
    {
      "epoch": 45.190857408051976,
      "grad_norm": 13.321778297424316,
      "learning_rate": 1.2340952159956685e-05,
      "loss": 1.7413,
      "step": 1168500
    },
    {
      "epoch": 45.19472483273388,
      "grad_norm": 10.414555549621582,
      "learning_rate": 1.23377293060551e-05,
      "loss": 1.7897,
      "step": 1168600
    },
    {
      "epoch": 45.19859225741579,
      "grad_norm": 17.016393661499023,
      "learning_rate": 1.2334506452153512e-05,
      "loss": 1.7885,
      "step": 1168700
    },
    {
      "epoch": 45.20245968209769,
      "grad_norm": 15.879826545715332,
      "learning_rate": 1.2331283598251925e-05,
      "loss": 1.7465,
      "step": 1168800
    },
    {
      "epoch": 45.2063271067796,
      "grad_norm": 14.476279258728027,
      "learning_rate": 1.2328060744350338e-05,
      "loss": 1.761,
      "step": 1168900
    },
    {
      "epoch": 45.2101945314615,
      "grad_norm": 14.306983947753906,
      "learning_rate": 1.2324837890448751e-05,
      "loss": 1.7916,
      "step": 1169000
    },
    {
      "epoch": 45.214061956143404,
      "grad_norm": 10.518527030944824,
      "learning_rate": 1.2321615036547164e-05,
      "loss": 1.7514,
      "step": 1169100
    },
    {
      "epoch": 45.21792938082531,
      "grad_norm": 8.501667022705078,
      "learning_rate": 1.2318392182645577e-05,
      "loss": 1.8259,
      "step": 1169200
    },
    {
      "epoch": 45.22179680550721,
      "grad_norm": 13.16208267211914,
      "learning_rate": 1.231516932874399e-05,
      "loss": 1.8153,
      "step": 1169300
    },
    {
      "epoch": 45.22566423018912,
      "grad_norm": 10.113059043884277,
      "learning_rate": 1.2311946474842404e-05,
      "loss": 1.8144,
      "step": 1169400
    },
    {
      "epoch": 45.229531654871025,
      "grad_norm": 12.907184600830078,
      "learning_rate": 1.2308723620940817e-05,
      "loss": 1.8048,
      "step": 1169500
    },
    {
      "epoch": 45.233399079552925,
      "grad_norm": 12.146554946899414,
      "learning_rate": 1.230550076703923e-05,
      "loss": 1.8564,
      "step": 1169600
    },
    {
      "epoch": 45.23726650423483,
      "grad_norm": 12.362378120422363,
      "learning_rate": 1.2302277913137643e-05,
      "loss": 1.7988,
      "step": 1169700
    },
    {
      "epoch": 45.24113392891673,
      "grad_norm": 14.71548080444336,
      "learning_rate": 1.2299055059236056e-05,
      "loss": 1.809,
      "step": 1169800
    },
    {
      "epoch": 45.24500135359864,
      "grad_norm": 10.82127571105957,
      "learning_rate": 1.2295832205334469e-05,
      "loss": 1.838,
      "step": 1169900
    },
    {
      "epoch": 45.248868778280546,
      "grad_norm": 12.720355033874512,
      "learning_rate": 1.229260935143288e-05,
      "loss": 1.8953,
      "step": 1170000
    },
    {
      "epoch": 45.252736202962446,
      "grad_norm": 11.891780853271484,
      "learning_rate": 1.2289386497531295e-05,
      "loss": 1.8492,
      "step": 1170100
    },
    {
      "epoch": 45.25660362764435,
      "grad_norm": 11.019036293029785,
      "learning_rate": 1.2286163643629708e-05,
      "loss": 1.8092,
      "step": 1170200
    },
    {
      "epoch": 45.26047105232625,
      "grad_norm": 10.259760856628418,
      "learning_rate": 1.228294078972812e-05,
      "loss": 1.7509,
      "step": 1170300
    },
    {
      "epoch": 45.26433847700816,
      "grad_norm": 11.024802207946777,
      "learning_rate": 1.2279717935826534e-05,
      "loss": 1.7456,
      "step": 1170400
    },
    {
      "epoch": 45.26820590169007,
      "grad_norm": 12.782988548278809,
      "learning_rate": 1.2276495081924947e-05,
      "loss": 1.8222,
      "step": 1170500
    },
    {
      "epoch": 45.27207332637197,
      "grad_norm": 10.945191383361816,
      "learning_rate": 1.227327222802336e-05,
      "loss": 1.786,
      "step": 1170600
    },
    {
      "epoch": 45.275940751053874,
      "grad_norm": 9.454155921936035,
      "learning_rate": 1.2270049374121773e-05,
      "loss": 1.7545,
      "step": 1170700
    },
    {
      "epoch": 45.27980817573578,
      "grad_norm": 11.30991268157959,
      "learning_rate": 1.2266826520220186e-05,
      "loss": 1.7966,
      "step": 1170800
    },
    {
      "epoch": 45.28367560041768,
      "grad_norm": 13.346787452697754,
      "learning_rate": 1.2263603666318599e-05,
      "loss": 1.6894,
      "step": 1170900
    },
    {
      "epoch": 45.28754302509959,
      "grad_norm": 11.26942253112793,
      "learning_rate": 1.2260380812417012e-05,
      "loss": 1.8088,
      "step": 1171000
    },
    {
      "epoch": 45.29141044978149,
      "grad_norm": 13.563281059265137,
      "learning_rate": 1.2257157958515425e-05,
      "loss": 1.7905,
      "step": 1171100
    },
    {
      "epoch": 45.295277874463395,
      "grad_norm": 10.791133880615234,
      "learning_rate": 1.2253935104613838e-05,
      "loss": 1.731,
      "step": 1171200
    },
    {
      "epoch": 45.2991452991453,
      "grad_norm": 10.78081226348877,
      "learning_rate": 1.2250712250712251e-05,
      "loss": 1.8386,
      "step": 1171300
    },
    {
      "epoch": 45.3030127238272,
      "grad_norm": 13.294440269470215,
      "learning_rate": 1.2247489396810664e-05,
      "loss": 1.8774,
      "step": 1171400
    },
    {
      "epoch": 45.30688014850911,
      "grad_norm": 11.478755950927734,
      "learning_rate": 1.2244266542909077e-05,
      "loss": 1.8105,
      "step": 1171500
    },
    {
      "epoch": 45.31074757319101,
      "grad_norm": 10.809678077697754,
      "learning_rate": 1.224104368900749e-05,
      "loss": 1.7377,
      "step": 1171600
    },
    {
      "epoch": 45.314614997872916,
      "grad_norm": 11.778170585632324,
      "learning_rate": 1.2237820835105903e-05,
      "loss": 1.727,
      "step": 1171700
    },
    {
      "epoch": 45.31848242255482,
      "grad_norm": 13.072986602783203,
      "learning_rate": 1.2234597981204316e-05,
      "loss": 1.7175,
      "step": 1171800
    },
    {
      "epoch": 45.32234984723672,
      "grad_norm": 13.141693115234375,
      "learning_rate": 1.2231375127302729e-05,
      "loss": 1.7867,
      "step": 1171900
    },
    {
      "epoch": 45.32621727191863,
      "grad_norm": 13.028255462646484,
      "learning_rate": 1.2228152273401144e-05,
      "loss": 1.694,
      "step": 1172000
    },
    {
      "epoch": 45.33008469660054,
      "grad_norm": 10.65009593963623,
      "learning_rate": 1.2224929419499557e-05,
      "loss": 1.8287,
      "step": 1172100
    },
    {
      "epoch": 45.33395212128244,
      "grad_norm": 15.113228797912598,
      "learning_rate": 1.222170656559797e-05,
      "loss": 1.776,
      "step": 1172200
    },
    {
      "epoch": 45.337819545964344,
      "grad_norm": 11.151815414428711,
      "learning_rate": 1.2218483711696383e-05,
      "loss": 1.8378,
      "step": 1172300
    },
    {
      "epoch": 45.34168697064624,
      "grad_norm": 10.250957489013672,
      "learning_rate": 1.2215260857794796e-05,
      "loss": 1.8611,
      "step": 1172400
    },
    {
      "epoch": 45.34555439532815,
      "grad_norm": 16.041593551635742,
      "learning_rate": 1.2212038003893209e-05,
      "loss": 1.8888,
      "step": 1172500
    },
    {
      "epoch": 45.34942182001006,
      "grad_norm": 13.525333404541016,
      "learning_rate": 1.2208815149991622e-05,
      "loss": 1.7941,
      "step": 1172600
    },
    {
      "epoch": 45.35328924469196,
      "grad_norm": 11.036243438720703,
      "learning_rate": 1.2205592296090035e-05,
      "loss": 1.7481,
      "step": 1172700
    },
    {
      "epoch": 45.357156669373865,
      "grad_norm": 8.804695129394531,
      "learning_rate": 1.2202369442188448e-05,
      "loss": 1.715,
      "step": 1172800
    },
    {
      "epoch": 45.36102409405577,
      "grad_norm": 13.769502639770508,
      "learning_rate": 1.219914658828686e-05,
      "loss": 1.7692,
      "step": 1172900
    },
    {
      "epoch": 45.36489151873767,
      "grad_norm": 9.242417335510254,
      "learning_rate": 1.2195923734385272e-05,
      "loss": 1.7781,
      "step": 1173000
    },
    {
      "epoch": 45.36875894341958,
      "grad_norm": 11.067975044250488,
      "learning_rate": 1.2192700880483685e-05,
      "loss": 1.8484,
      "step": 1173100
    },
    {
      "epoch": 45.37262636810148,
      "grad_norm": 12.634657859802246,
      "learning_rate": 1.2189478026582098e-05,
      "loss": 1.7816,
      "step": 1173200
    },
    {
      "epoch": 45.376493792783386,
      "grad_norm": 12.108968734741211,
      "learning_rate": 1.2186255172680511e-05,
      "loss": 1.7962,
      "step": 1173300
    },
    {
      "epoch": 45.38036121746529,
      "grad_norm": 13.244087219238281,
      "learning_rate": 1.2183032318778925e-05,
      "loss": 1.7421,
      "step": 1173400
    },
    {
      "epoch": 45.38422864214719,
      "grad_norm": 12.150946617126465,
      "learning_rate": 1.2179809464877338e-05,
      "loss": 1.8143,
      "step": 1173500
    },
    {
      "epoch": 45.3880960668291,
      "grad_norm": 13.756845474243164,
      "learning_rate": 1.2176586610975752e-05,
      "loss": 1.7935,
      "step": 1173600
    },
    {
      "epoch": 45.391963491511,
      "grad_norm": 10.254066467285156,
      "learning_rate": 1.2173363757074165e-05,
      "loss": 1.745,
      "step": 1173700
    },
    {
      "epoch": 45.39583091619291,
      "grad_norm": 12.250986099243164,
      "learning_rate": 1.2170140903172578e-05,
      "loss": 1.8105,
      "step": 1173800
    },
    {
      "epoch": 45.399698340874814,
      "grad_norm": 13.345039367675781,
      "learning_rate": 1.2166918049270991e-05,
      "loss": 1.7822,
      "step": 1173900
    },
    {
      "epoch": 45.40356576555671,
      "grad_norm": 11.981789588928223,
      "learning_rate": 1.2163695195369404e-05,
      "loss": 1.9463,
      "step": 1174000
    },
    {
      "epoch": 45.40743319023862,
      "grad_norm": 11.56447696685791,
      "learning_rate": 1.2160472341467817e-05,
      "loss": 1.7974,
      "step": 1174100
    },
    {
      "epoch": 45.41130061492053,
      "grad_norm": 11.660029411315918,
      "learning_rate": 1.215724948756623e-05,
      "loss": 1.7925,
      "step": 1174200
    },
    {
      "epoch": 45.41516803960243,
      "grad_norm": 9.590229988098145,
      "learning_rate": 1.2154026633664643e-05,
      "loss": 1.8555,
      "step": 1174300
    },
    {
      "epoch": 45.419035464284335,
      "grad_norm": 13.984872817993164,
      "learning_rate": 1.2150803779763056e-05,
      "loss": 1.7407,
      "step": 1174400
    },
    {
      "epoch": 45.422902888966235,
      "grad_norm": 14.060816764831543,
      "learning_rate": 1.214758092586147e-05,
      "loss": 1.8432,
      "step": 1174500
    },
    {
      "epoch": 45.42677031364814,
      "grad_norm": 10.063545227050781,
      "learning_rate": 1.2144358071959882e-05,
      "loss": 1.7773,
      "step": 1174600
    },
    {
      "epoch": 45.43063773833005,
      "grad_norm": 11.36301040649414,
      "learning_rate": 1.2141135218058296e-05,
      "loss": 1.8415,
      "step": 1174700
    },
    {
      "epoch": 45.43450516301195,
      "grad_norm": 12.527009010314941,
      "learning_rate": 1.2137912364156709e-05,
      "loss": 1.7613,
      "step": 1174800
    },
    {
      "epoch": 45.438372587693856,
      "grad_norm": 17.395814895629883,
      "learning_rate": 1.2134689510255122e-05,
      "loss": 1.8824,
      "step": 1174900
    },
    {
      "epoch": 45.442240012375755,
      "grad_norm": 15.364998817443848,
      "learning_rate": 1.2131466656353535e-05,
      "loss": 1.7952,
      "step": 1175000
    },
    {
      "epoch": 45.44610743705766,
      "grad_norm": 12.72713851928711,
      "learning_rate": 1.2128243802451948e-05,
      "loss": 1.7206,
      "step": 1175100
    },
    {
      "epoch": 45.44997486173957,
      "grad_norm": 12.470929145812988,
      "learning_rate": 1.212502094855036e-05,
      "loss": 1.7476,
      "step": 1175200
    },
    {
      "epoch": 45.45384228642147,
      "grad_norm": 8.961264610290527,
      "learning_rate": 1.2121798094648774e-05,
      "loss": 1.8985,
      "step": 1175300
    },
    {
      "epoch": 45.45770971110338,
      "grad_norm": 14.44641399383545,
      "learning_rate": 1.2118575240747187e-05,
      "loss": 1.8105,
      "step": 1175400
    },
    {
      "epoch": 45.461577135785284,
      "grad_norm": 9.411287307739258,
      "learning_rate": 1.2115352386845601e-05,
      "loss": 1.9053,
      "step": 1175500
    },
    {
      "epoch": 45.46544456046718,
      "grad_norm": 13.96505355834961,
      "learning_rate": 1.2112129532944014e-05,
      "loss": 1.8009,
      "step": 1175600
    },
    {
      "epoch": 45.46931198514909,
      "grad_norm": 13.216817855834961,
      "learning_rate": 1.2108906679042427e-05,
      "loss": 1.8751,
      "step": 1175700
    },
    {
      "epoch": 45.47317940983099,
      "grad_norm": 14.633153915405273,
      "learning_rate": 1.210568382514084e-05,
      "loss": 1.8304,
      "step": 1175800
    },
    {
      "epoch": 45.4770468345129,
      "grad_norm": 12.834163665771484,
      "learning_rate": 1.2102460971239252e-05,
      "loss": 1.7926,
      "step": 1175900
    },
    {
      "epoch": 45.480914259194805,
      "grad_norm": 10.76761245727539,
      "learning_rate": 1.2099238117337665e-05,
      "loss": 1.7419,
      "step": 1176000
    },
    {
      "epoch": 45.484781683876705,
      "grad_norm": 14.327888488769531,
      "learning_rate": 1.2096015263436078e-05,
      "loss": 1.77,
      "step": 1176100
    },
    {
      "epoch": 45.48864910855861,
      "grad_norm": 8.777852058410645,
      "learning_rate": 1.2092792409534491e-05,
      "loss": 1.7608,
      "step": 1176200
    },
    {
      "epoch": 45.49251653324052,
      "grad_norm": 13.958442687988281,
      "learning_rate": 1.2089569555632904e-05,
      "loss": 1.8004,
      "step": 1176300
    },
    {
      "epoch": 45.49638395792242,
      "grad_norm": 10.791271209716797,
      "learning_rate": 1.2086346701731317e-05,
      "loss": 1.8126,
      "step": 1176400
    },
    {
      "epoch": 45.500251382604326,
      "grad_norm": 13.057134628295898,
      "learning_rate": 1.208312384782973e-05,
      "loss": 1.7765,
      "step": 1176500
    },
    {
      "epoch": 45.504118807286225,
      "grad_norm": 11.768409729003906,
      "learning_rate": 1.2079900993928143e-05,
      "loss": 1.8172,
      "step": 1176600
    },
    {
      "epoch": 45.50798623196813,
      "grad_norm": 10.482402801513672,
      "learning_rate": 1.2076678140026556e-05,
      "loss": 1.8052,
      "step": 1176700
    },
    {
      "epoch": 45.51185365665004,
      "grad_norm": 13.391552925109863,
      "learning_rate": 1.2073455286124969e-05,
      "loss": 1.838,
      "step": 1176800
    },
    {
      "epoch": 45.51572108133194,
      "grad_norm": 13.046260833740234,
      "learning_rate": 1.2070232432223382e-05,
      "loss": 1.8541,
      "step": 1176900
    },
    {
      "epoch": 45.51958850601385,
      "grad_norm": 9.578776359558105,
      "learning_rate": 1.2067009578321797e-05,
      "loss": 1.9234,
      "step": 1177000
    },
    {
      "epoch": 45.52345593069575,
      "grad_norm": 11.323307037353516,
      "learning_rate": 1.206378672442021e-05,
      "loss": 1.8684,
      "step": 1177100
    },
    {
      "epoch": 45.52732335537765,
      "grad_norm": 12.46259880065918,
      "learning_rate": 1.2060563870518623e-05,
      "loss": 1.8453,
      "step": 1177200
    },
    {
      "epoch": 45.53119078005956,
      "grad_norm": 13.055533409118652,
      "learning_rate": 1.2057341016617036e-05,
      "loss": 1.8281,
      "step": 1177300
    },
    {
      "epoch": 45.53505820474146,
      "grad_norm": 11.578804016113281,
      "learning_rate": 1.2054118162715449e-05,
      "loss": 1.9013,
      "step": 1177400
    },
    {
      "epoch": 45.53892562942337,
      "grad_norm": 11.326616287231445,
      "learning_rate": 1.2050895308813862e-05,
      "loss": 1.804,
      "step": 1177500
    },
    {
      "epoch": 45.542793054105275,
      "grad_norm": 16.03592300415039,
      "learning_rate": 1.2047672454912275e-05,
      "loss": 1.8394,
      "step": 1177600
    },
    {
      "epoch": 45.546660478787174,
      "grad_norm": 14.089679718017578,
      "learning_rate": 1.2044449601010688e-05,
      "loss": 1.8348,
      "step": 1177700
    },
    {
      "epoch": 45.55052790346908,
      "grad_norm": 10.580860137939453,
      "learning_rate": 1.2041226747109101e-05,
      "loss": 1.8812,
      "step": 1177800
    },
    {
      "epoch": 45.55439532815098,
      "grad_norm": 9.09696102142334,
      "learning_rate": 1.2038003893207514e-05,
      "loss": 1.7855,
      "step": 1177900
    },
    {
      "epoch": 45.55826275283289,
      "grad_norm": 10.5556001663208,
      "learning_rate": 1.2034781039305927e-05,
      "loss": 1.9139,
      "step": 1178000
    },
    {
      "epoch": 45.562130177514796,
      "grad_norm": 13.871010780334473,
      "learning_rate": 1.203155818540434e-05,
      "loss": 1.7758,
      "step": 1178100
    },
    {
      "epoch": 45.565997602196695,
      "grad_norm": 11.941157341003418,
      "learning_rate": 1.2028335331502753e-05,
      "loss": 1.7974,
      "step": 1178200
    },
    {
      "epoch": 45.5698650268786,
      "grad_norm": 8.524507522583008,
      "learning_rate": 1.2025112477601166e-05,
      "loss": 1.8548,
      "step": 1178300
    },
    {
      "epoch": 45.5737324515605,
      "grad_norm": 12.404891967773438,
      "learning_rate": 1.2021889623699579e-05,
      "loss": 1.9241,
      "step": 1178400
    },
    {
      "epoch": 45.57759987624241,
      "grad_norm": 12.462092399597168,
      "learning_rate": 1.2018666769797992e-05,
      "loss": 1.7281,
      "step": 1178500
    },
    {
      "epoch": 45.58146730092432,
      "grad_norm": 12.270752906799316,
      "learning_rate": 1.2015443915896405e-05,
      "loss": 1.8119,
      "step": 1178600
    },
    {
      "epoch": 45.58533472560622,
      "grad_norm": 13.352858543395996,
      "learning_rate": 1.2012221061994818e-05,
      "loss": 1.7899,
      "step": 1178700
    },
    {
      "epoch": 45.58920215028812,
      "grad_norm": 10.311039924621582,
      "learning_rate": 1.2008998208093231e-05,
      "loss": 1.716,
      "step": 1178800
    },
    {
      "epoch": 45.59306957497003,
      "grad_norm": 12.573525428771973,
      "learning_rate": 1.2005775354191644e-05,
      "loss": 1.8115,
      "step": 1178900
    },
    {
      "epoch": 45.59693699965193,
      "grad_norm": 12.48106575012207,
      "learning_rate": 1.2002552500290057e-05,
      "loss": 1.7926,
      "step": 1179000
    },
    {
      "epoch": 45.60080442433384,
      "grad_norm": 11.841875076293945,
      "learning_rate": 1.199932964638847e-05,
      "loss": 1.7164,
      "step": 1179100
    },
    {
      "epoch": 45.60467184901574,
      "grad_norm": 16.55703353881836,
      "learning_rate": 1.1996106792486883e-05,
      "loss": 1.8044,
      "step": 1179200
    },
    {
      "epoch": 45.608539273697644,
      "grad_norm": 13.154433250427246,
      "learning_rate": 1.1992883938585296e-05,
      "loss": 1.7873,
      "step": 1179300
    },
    {
      "epoch": 45.61240669837955,
      "grad_norm": 7.75089168548584,
      "learning_rate": 1.198966108468371e-05,
      "loss": 1.8088,
      "step": 1179400
    },
    {
      "epoch": 45.61627412306145,
      "grad_norm": 11.931107521057129,
      "learning_rate": 1.1986438230782122e-05,
      "loss": 1.8608,
      "step": 1179500
    },
    {
      "epoch": 45.62014154774336,
      "grad_norm": 10.231292724609375,
      "learning_rate": 1.1983215376880535e-05,
      "loss": 1.8303,
      "step": 1179600
    },
    {
      "epoch": 45.62400897242526,
      "grad_norm": 10.090887069702148,
      "learning_rate": 1.1979992522978948e-05,
      "loss": 1.7379,
      "step": 1179700
    },
    {
      "epoch": 45.627876397107165,
      "grad_norm": 11.003217697143555,
      "learning_rate": 1.1976769669077361e-05,
      "loss": 1.9019,
      "step": 1179800
    },
    {
      "epoch": 45.63174382178907,
      "grad_norm": 10.405854225158691,
      "learning_rate": 1.1973546815175774e-05,
      "loss": 1.8653,
      "step": 1179900
    },
    {
      "epoch": 45.63561124647097,
      "grad_norm": 9.85632610321045,
      "learning_rate": 1.1970323961274187e-05,
      "loss": 1.7882,
      "step": 1180000
    },
    {
      "epoch": 45.63947867115288,
      "grad_norm": 16.465518951416016,
      "learning_rate": 1.19671011073726e-05,
      "loss": 1.8684,
      "step": 1180100
    },
    {
      "epoch": 45.64334609583479,
      "grad_norm": 17.138731002807617,
      "learning_rate": 1.1963878253471014e-05,
      "loss": 1.849,
      "step": 1180200
    },
    {
      "epoch": 45.64721352051669,
      "grad_norm": 13.800259590148926,
      "learning_rate": 1.1960655399569427e-05,
      "loss": 1.7965,
      "step": 1180300
    },
    {
      "epoch": 45.65108094519859,
      "grad_norm": 11.869176864624023,
      "learning_rate": 1.195743254566784e-05,
      "loss": 1.8074,
      "step": 1180400
    },
    {
      "epoch": 45.65494836988049,
      "grad_norm": 13.018345832824707,
      "learning_rate": 1.1954209691766254e-05,
      "loss": 1.7632,
      "step": 1180500
    },
    {
      "epoch": 45.6588157945624,
      "grad_norm": 11.08272647857666,
      "learning_rate": 1.1950986837864667e-05,
      "loss": 1.866,
      "step": 1180600
    },
    {
      "epoch": 45.66268321924431,
      "grad_norm": 11.989029884338379,
      "learning_rate": 1.194776398396308e-05,
      "loss": 1.8208,
      "step": 1180700
    },
    {
      "epoch": 45.66655064392621,
      "grad_norm": 12.830972671508789,
      "learning_rate": 1.1944541130061493e-05,
      "loss": 1.8386,
      "step": 1180800
    },
    {
      "epoch": 45.670418068608114,
      "grad_norm": 12.384984970092773,
      "learning_rate": 1.1941318276159906e-05,
      "loss": 1.8435,
      "step": 1180900
    },
    {
      "epoch": 45.67428549329002,
      "grad_norm": 13.843822479248047,
      "learning_rate": 1.193809542225832e-05,
      "loss": 1.8564,
      "step": 1181000
    },
    {
      "epoch": 45.67815291797192,
      "grad_norm": 13.50696086883545,
      "learning_rate": 1.1934872568356732e-05,
      "loss": 1.8145,
      "step": 1181100
    },
    {
      "epoch": 45.68202034265383,
      "grad_norm": 12.11265754699707,
      "learning_rate": 1.1931649714455145e-05,
      "loss": 1.7992,
      "step": 1181200
    },
    {
      "epoch": 45.68588776733573,
      "grad_norm": 13.132047653198242,
      "learning_rate": 1.1928426860553559e-05,
      "loss": 1.7665,
      "step": 1181300
    },
    {
      "epoch": 45.689755192017635,
      "grad_norm": 10.810954093933105,
      "learning_rate": 1.1925204006651972e-05,
      "loss": 1.827,
      "step": 1181400
    },
    {
      "epoch": 45.69362261669954,
      "grad_norm": 14.081013679504395,
      "learning_rate": 1.1921981152750385e-05,
      "loss": 1.7867,
      "step": 1181500
    },
    {
      "epoch": 45.69749004138144,
      "grad_norm": 11.155105590820312,
      "learning_rate": 1.1918758298848798e-05,
      "loss": 1.8199,
      "step": 1181600
    },
    {
      "epoch": 45.70135746606335,
      "grad_norm": 12.997591972351074,
      "learning_rate": 1.191553544494721e-05,
      "loss": 1.7558,
      "step": 1181700
    },
    {
      "epoch": 45.70522489074525,
      "grad_norm": 11.98486042022705,
      "learning_rate": 1.1912312591045622e-05,
      "loss": 1.9572,
      "step": 1181800
    },
    {
      "epoch": 45.70909231542716,
      "grad_norm": 14.1822509765625,
      "learning_rate": 1.1909089737144035e-05,
      "loss": 1.7512,
      "step": 1181900
    },
    {
      "epoch": 45.71295974010906,
      "grad_norm": 14.304312705993652,
      "learning_rate": 1.190586688324245e-05,
      "loss": 1.8473,
      "step": 1182000
    },
    {
      "epoch": 45.71682716479096,
      "grad_norm": 12.561308860778809,
      "learning_rate": 1.1902644029340863e-05,
      "loss": 1.8238,
      "step": 1182100
    },
    {
      "epoch": 45.72069458947287,
      "grad_norm": 14.381233215332031,
      "learning_rate": 1.1899421175439276e-05,
      "loss": 1.8395,
      "step": 1182200
    },
    {
      "epoch": 45.72456201415478,
      "grad_norm": 8.197547912597656,
      "learning_rate": 1.1896198321537689e-05,
      "loss": 1.8015,
      "step": 1182300
    },
    {
      "epoch": 45.72842943883668,
      "grad_norm": 13.471898078918457,
      "learning_rate": 1.1892975467636102e-05,
      "loss": 1.8324,
      "step": 1182400
    },
    {
      "epoch": 45.732296863518584,
      "grad_norm": 11.389854431152344,
      "learning_rate": 1.1889752613734515e-05,
      "loss": 1.6683,
      "step": 1182500
    },
    {
      "epoch": 45.736164288200484,
      "grad_norm": 14.982013702392578,
      "learning_rate": 1.1886529759832928e-05,
      "loss": 1.7955,
      "step": 1182600
    },
    {
      "epoch": 45.74003171288239,
      "grad_norm": 14.30305290222168,
      "learning_rate": 1.1883306905931341e-05,
      "loss": 1.7754,
      "step": 1182700
    },
    {
      "epoch": 45.7438991375643,
      "grad_norm": 10.852252960205078,
      "learning_rate": 1.1880084052029754e-05,
      "loss": 1.8409,
      "step": 1182800
    },
    {
      "epoch": 45.7477665622462,
      "grad_norm": 12.032776832580566,
      "learning_rate": 1.1876861198128167e-05,
      "loss": 1.8232,
      "step": 1182900
    },
    {
      "epoch": 45.751633986928105,
      "grad_norm": 17.619815826416016,
      "learning_rate": 1.187363834422658e-05,
      "loss": 1.7459,
      "step": 1183000
    },
    {
      "epoch": 45.75550141161001,
      "grad_norm": 15.99843692779541,
      "learning_rate": 1.1870415490324993e-05,
      "loss": 1.8311,
      "step": 1183100
    },
    {
      "epoch": 45.75936883629191,
      "grad_norm": 14.72492504119873,
      "learning_rate": 1.1867192636423406e-05,
      "loss": 1.7113,
      "step": 1183200
    },
    {
      "epoch": 45.76323626097382,
      "grad_norm": 11.186209678649902,
      "learning_rate": 1.1863969782521819e-05,
      "loss": 1.8545,
      "step": 1183300
    },
    {
      "epoch": 45.76710368565572,
      "grad_norm": 10.40497875213623,
      "learning_rate": 1.1860746928620232e-05,
      "loss": 1.8337,
      "step": 1183400
    },
    {
      "epoch": 45.770971110337626,
      "grad_norm": 14.282463073730469,
      "learning_rate": 1.1857524074718645e-05,
      "loss": 1.7238,
      "step": 1183500
    },
    {
      "epoch": 45.77483853501953,
      "grad_norm": 9.193153381347656,
      "learning_rate": 1.1854301220817058e-05,
      "loss": 1.7444,
      "step": 1183600
    },
    {
      "epoch": 45.77870595970143,
      "grad_norm": 13.995594024658203,
      "learning_rate": 1.1851078366915471e-05,
      "loss": 1.7305,
      "step": 1183700
    },
    {
      "epoch": 45.78257338438334,
      "grad_norm": 9.816269874572754,
      "learning_rate": 1.1847855513013884e-05,
      "loss": 1.7922,
      "step": 1183800
    },
    {
      "epoch": 45.78644080906524,
      "grad_norm": 11.220147132873535,
      "learning_rate": 1.1844632659112299e-05,
      "loss": 1.7561,
      "step": 1183900
    },
    {
      "epoch": 45.79030823374715,
      "grad_norm": 8.87135124206543,
      "learning_rate": 1.1841409805210712e-05,
      "loss": 1.7341,
      "step": 1184000
    },
    {
      "epoch": 45.794175658429054,
      "grad_norm": 16.074817657470703,
      "learning_rate": 1.1838186951309125e-05,
      "loss": 1.853,
      "step": 1184100
    },
    {
      "epoch": 45.798043083110954,
      "grad_norm": 17.827655792236328,
      "learning_rate": 1.1834964097407538e-05,
      "loss": 1.8972,
      "step": 1184200
    },
    {
      "epoch": 45.80191050779286,
      "grad_norm": 14.299778938293457,
      "learning_rate": 1.1831741243505951e-05,
      "loss": 1.8012,
      "step": 1184300
    },
    {
      "epoch": 45.80577793247477,
      "grad_norm": 8.606515884399414,
      "learning_rate": 1.1828518389604364e-05,
      "loss": 1.7439,
      "step": 1184400
    },
    {
      "epoch": 45.80964535715667,
      "grad_norm": 11.187594413757324,
      "learning_rate": 1.1825295535702777e-05,
      "loss": 1.7833,
      "step": 1184500
    },
    {
      "epoch": 45.813512781838575,
      "grad_norm": 11.224830627441406,
      "learning_rate": 1.182207268180119e-05,
      "loss": 1.8359,
      "step": 1184600
    },
    {
      "epoch": 45.817380206520475,
      "grad_norm": 11.301055908203125,
      "learning_rate": 1.1818849827899601e-05,
      "loss": 1.8152,
      "step": 1184700
    },
    {
      "epoch": 45.82124763120238,
      "grad_norm": 13.445005416870117,
      "learning_rate": 1.1815626973998014e-05,
      "loss": 1.8343,
      "step": 1184800
    },
    {
      "epoch": 45.82511505588429,
      "grad_norm": 11.799346923828125,
      "learning_rate": 1.1812404120096427e-05,
      "loss": 1.7914,
      "step": 1184900
    },
    {
      "epoch": 45.82898248056619,
      "grad_norm": 12.248857498168945,
      "learning_rate": 1.180918126619484e-05,
      "loss": 1.7865,
      "step": 1185000
    },
    {
      "epoch": 45.832849905248096,
      "grad_norm": 8.048513412475586,
      "learning_rate": 1.1805958412293253e-05,
      "loss": 1.822,
      "step": 1185100
    },
    {
      "epoch": 45.836717329929996,
      "grad_norm": 13.259934425354004,
      "learning_rate": 1.1802735558391666e-05,
      "loss": 1.7685,
      "step": 1185200
    },
    {
      "epoch": 45.8405847546119,
      "grad_norm": 13.361472129821777,
      "learning_rate": 1.179951270449008e-05,
      "loss": 1.8477,
      "step": 1185300
    },
    {
      "epoch": 45.84445217929381,
      "grad_norm": 13.02918815612793,
      "learning_rate": 1.1796289850588493e-05,
      "loss": 1.8381,
      "step": 1185400
    },
    {
      "epoch": 45.84831960397571,
      "grad_norm": 13.221907615661621,
      "learning_rate": 1.1793066996686907e-05,
      "loss": 1.8144,
      "step": 1185500
    },
    {
      "epoch": 45.85218702865762,
      "grad_norm": 9.444282531738281,
      "learning_rate": 1.178984414278532e-05,
      "loss": 1.7609,
      "step": 1185600
    },
    {
      "epoch": 45.856054453339524,
      "grad_norm": 10.317566871643066,
      "learning_rate": 1.1786621288883733e-05,
      "loss": 1.7522,
      "step": 1185700
    },
    {
      "epoch": 45.859921878021424,
      "grad_norm": 5.834460258483887,
      "learning_rate": 1.1783398434982146e-05,
      "loss": 1.7409,
      "step": 1185800
    },
    {
      "epoch": 45.86378930270333,
      "grad_norm": 14.81279182434082,
      "learning_rate": 1.178017558108056e-05,
      "loss": 1.787,
      "step": 1185900
    },
    {
      "epoch": 45.86765672738523,
      "grad_norm": 11.9974946975708,
      "learning_rate": 1.1776952727178972e-05,
      "loss": 1.7587,
      "step": 1186000
    },
    {
      "epoch": 45.87152415206714,
      "grad_norm": 12.23266887664795,
      "learning_rate": 1.1773729873277385e-05,
      "loss": 1.8278,
      "step": 1186100
    },
    {
      "epoch": 45.875391576749045,
      "grad_norm": 12.383124351501465,
      "learning_rate": 1.1770507019375798e-05,
      "loss": 1.8076,
      "step": 1186200
    },
    {
      "epoch": 45.879259001430945,
      "grad_norm": 16.127824783325195,
      "learning_rate": 1.1767284165474211e-05,
      "loss": 1.8072,
      "step": 1186300
    },
    {
      "epoch": 45.88312642611285,
      "grad_norm": 11.687482833862305,
      "learning_rate": 1.1764061311572624e-05,
      "loss": 1.779,
      "step": 1186400
    },
    {
      "epoch": 45.88699385079475,
      "grad_norm": 10.830548286437988,
      "learning_rate": 1.1760838457671037e-05,
      "loss": 1.76,
      "step": 1186500
    },
    {
      "epoch": 45.89086127547666,
      "grad_norm": 12.475963592529297,
      "learning_rate": 1.175761560376945e-05,
      "loss": 1.817,
      "step": 1186600
    },
    {
      "epoch": 45.894728700158566,
      "grad_norm": 11.410345077514648,
      "learning_rate": 1.1754392749867864e-05,
      "loss": 1.8132,
      "step": 1186700
    },
    {
      "epoch": 45.898596124840466,
      "grad_norm": 9.587450981140137,
      "learning_rate": 1.1751169895966277e-05,
      "loss": 1.7876,
      "step": 1186800
    },
    {
      "epoch": 45.90246354952237,
      "grad_norm": 11.260366439819336,
      "learning_rate": 1.174794704206469e-05,
      "loss": 1.7194,
      "step": 1186900
    },
    {
      "epoch": 45.90633097420428,
      "grad_norm": 16.81977653503418,
      "learning_rate": 1.1744724188163103e-05,
      "loss": 1.8375,
      "step": 1187000
    },
    {
      "epoch": 45.91019839888618,
      "grad_norm": 11.983503341674805,
      "learning_rate": 1.1741501334261516e-05,
      "loss": 1.7626,
      "step": 1187100
    },
    {
      "epoch": 45.91406582356809,
      "grad_norm": 13.335521697998047,
      "learning_rate": 1.1738278480359929e-05,
      "loss": 1.7371,
      "step": 1187200
    },
    {
      "epoch": 45.91793324824999,
      "grad_norm": 13.550187110900879,
      "learning_rate": 1.1735055626458342e-05,
      "loss": 1.7048,
      "step": 1187300
    },
    {
      "epoch": 45.921800672931894,
      "grad_norm": 10.267961502075195,
      "learning_rate": 1.1731832772556756e-05,
      "loss": 1.8053,
      "step": 1187400
    },
    {
      "epoch": 45.9256680976138,
      "grad_norm": 11.286144256591797,
      "learning_rate": 1.172860991865517e-05,
      "loss": 1.8589,
      "step": 1187500
    },
    {
      "epoch": 45.9295355222957,
      "grad_norm": 14.659305572509766,
      "learning_rate": 1.1725387064753582e-05,
      "loss": 1.9181,
      "step": 1187600
    },
    {
      "epoch": 45.93340294697761,
      "grad_norm": 11.921728134155273,
      "learning_rate": 1.1722164210851994e-05,
      "loss": 1.8222,
      "step": 1187700
    },
    {
      "epoch": 45.93727037165951,
      "grad_norm": 12.146431922912598,
      "learning_rate": 1.1718941356950407e-05,
      "loss": 1.8046,
      "step": 1187800
    },
    {
      "epoch": 45.941137796341415,
      "grad_norm": 16.931278228759766,
      "learning_rate": 1.171571850304882e-05,
      "loss": 1.8286,
      "step": 1187900
    },
    {
      "epoch": 45.94500522102332,
      "grad_norm": 14.11376953125,
      "learning_rate": 1.1712495649147233e-05,
      "loss": 1.8059,
      "step": 1188000
    },
    {
      "epoch": 45.94887264570522,
      "grad_norm": 13.992262840270996,
      "learning_rate": 1.1709272795245646e-05,
      "loss": 1.8899,
      "step": 1188100
    },
    {
      "epoch": 45.95274007038713,
      "grad_norm": 13.811498641967773,
      "learning_rate": 1.1706049941344059e-05,
      "loss": 1.8011,
      "step": 1188200
    },
    {
      "epoch": 45.956607495069036,
      "grad_norm": 15.658707618713379,
      "learning_rate": 1.1702827087442472e-05,
      "loss": 1.8254,
      "step": 1188300
    },
    {
      "epoch": 45.960474919750936,
      "grad_norm": 9.527745246887207,
      "learning_rate": 1.1699604233540885e-05,
      "loss": 1.7409,
      "step": 1188400
    },
    {
      "epoch": 45.96434234443284,
      "grad_norm": 11.295455932617188,
      "learning_rate": 1.1696381379639298e-05,
      "loss": 1.8439,
      "step": 1188500
    },
    {
      "epoch": 45.96820976911474,
      "grad_norm": 14.120635032653809,
      "learning_rate": 1.1693158525737711e-05,
      "loss": 1.8851,
      "step": 1188600
    },
    {
      "epoch": 45.97207719379665,
      "grad_norm": 12.181985855102539,
      "learning_rate": 1.1689935671836124e-05,
      "loss": 1.7588,
      "step": 1188700
    },
    {
      "epoch": 45.97594461847856,
      "grad_norm": 14.41409683227539,
      "learning_rate": 1.1686712817934537e-05,
      "loss": 1.8202,
      "step": 1188800
    },
    {
      "epoch": 45.97981204316046,
      "grad_norm": 10.526375770568848,
      "learning_rate": 1.1683489964032952e-05,
      "loss": 1.823,
      "step": 1188900
    },
    {
      "epoch": 45.983679467842364,
      "grad_norm": 8.83421516418457,
      "learning_rate": 1.1680267110131365e-05,
      "loss": 1.7288,
      "step": 1189000
    },
    {
      "epoch": 45.98754689252427,
      "grad_norm": 12.131752014160156,
      "learning_rate": 1.1677044256229778e-05,
      "loss": 1.9166,
      "step": 1189100
    },
    {
      "epoch": 45.99141431720617,
      "grad_norm": 12.857314109802246,
      "learning_rate": 1.167382140232819e-05,
      "loss": 1.8915,
      "step": 1189200
    },
    {
      "epoch": 45.99528174188808,
      "grad_norm": 9.298900604248047,
      "learning_rate": 1.1670598548426604e-05,
      "loss": 1.8162,
      "step": 1189300
    },
    {
      "epoch": 45.99914916656998,
      "grad_norm": 11.117908477783203,
      "learning_rate": 1.1667375694525017e-05,
      "loss": 1.842,
      "step": 1189400
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.7531131505966187,
      "eval_runtime": 5.7249,
      "eval_samples_per_second": 237.733,
      "eval_steps_per_second": 237.733,
      "step": 1189422
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.597029209136963,
      "eval_runtime": 108.5079,
      "eval_samples_per_second": 238.296,
      "eval_steps_per_second": 238.296,
      "step": 1189422
    },
    {
      "epoch": 46.003016591251885,
      "grad_norm": 14.647099494934082,
      "learning_rate": 1.166415284062343e-05,
      "loss": 1.8189,
      "step": 1189500
    },
    {
      "epoch": 46.00688401593379,
      "grad_norm": 11.647407531738281,
      "learning_rate": 1.1660929986721843e-05,
      "loss": 1.8852,
      "step": 1189600
    },
    {
      "epoch": 46.01075144061569,
      "grad_norm": 12.464408874511719,
      "learning_rate": 1.1657707132820256e-05,
      "loss": 1.7127,
      "step": 1189700
    },
    {
      "epoch": 46.0146188652976,
      "grad_norm": 13.240303993225098,
      "learning_rate": 1.1654484278918669e-05,
      "loss": 1.7524,
      "step": 1189800
    },
    {
      "epoch": 46.0184862899795,
      "grad_norm": 11.472945213317871,
      "learning_rate": 1.1651261425017082e-05,
      "loss": 1.7599,
      "step": 1189900
    },
    {
      "epoch": 46.022353714661406,
      "grad_norm": 12.738296508789062,
      "learning_rate": 1.1648038571115495e-05,
      "loss": 1.7557,
      "step": 1190000
    },
    {
      "epoch": 46.02622113934331,
      "grad_norm": 10.84607219696045,
      "learning_rate": 1.1644815717213908e-05,
      "loss": 1.7392,
      "step": 1190100
    },
    {
      "epoch": 46.03008856402521,
      "grad_norm": 11.219005584716797,
      "learning_rate": 1.1641592863312321e-05,
      "loss": 1.7488,
      "step": 1190200
    },
    {
      "epoch": 46.03395598870712,
      "grad_norm": 11.913651466369629,
      "learning_rate": 1.1638370009410734e-05,
      "loss": 1.8296,
      "step": 1190300
    },
    {
      "epoch": 46.03782341338903,
      "grad_norm": 7.692766189575195,
      "learning_rate": 1.1635147155509147e-05,
      "loss": 1.792,
      "step": 1190400
    },
    {
      "epoch": 46.04169083807093,
      "grad_norm": 12.283244132995605,
      "learning_rate": 1.163192430160756e-05,
      "loss": 1.7576,
      "step": 1190500
    },
    {
      "epoch": 46.045558262752834,
      "grad_norm": 13.402113914489746,
      "learning_rate": 1.1628701447705973e-05,
      "loss": 1.8227,
      "step": 1190600
    },
    {
      "epoch": 46.049425687434734,
      "grad_norm": 13.960460662841797,
      "learning_rate": 1.1625478593804386e-05,
      "loss": 1.8467,
      "step": 1190700
    },
    {
      "epoch": 46.05329311211664,
      "grad_norm": 21.20832061767578,
      "learning_rate": 1.16222557399028e-05,
      "loss": 1.8407,
      "step": 1190800
    },
    {
      "epoch": 46.05716053679855,
      "grad_norm": 12.783397674560547,
      "learning_rate": 1.1619032886001212e-05,
      "loss": 1.7273,
      "step": 1190900
    },
    {
      "epoch": 46.06102796148045,
      "grad_norm": 14.247689247131348,
      "learning_rate": 1.1615810032099625e-05,
      "loss": 1.769,
      "step": 1191000
    },
    {
      "epoch": 46.064895386162355,
      "grad_norm": 10.766633987426758,
      "learning_rate": 1.1612587178198038e-05,
      "loss": 1.7681,
      "step": 1191100
    },
    {
      "epoch": 46.068762810844255,
      "grad_norm": 12.282393455505371,
      "learning_rate": 1.1609364324296451e-05,
      "loss": 1.7991,
      "step": 1191200
    },
    {
      "epoch": 46.07263023552616,
      "grad_norm": 14.167686462402344,
      "learning_rate": 1.1606141470394864e-05,
      "loss": 1.7617,
      "step": 1191300
    },
    {
      "epoch": 46.07649766020807,
      "grad_norm": 13.577491760253906,
      "learning_rate": 1.1602918616493277e-05,
      "loss": 1.817,
      "step": 1191400
    },
    {
      "epoch": 46.08036508488997,
      "grad_norm": 12.454537391662598,
      "learning_rate": 1.159969576259169e-05,
      "loss": 1.7414,
      "step": 1191500
    },
    {
      "epoch": 46.084232509571876,
      "grad_norm": 13.206792831420898,
      "learning_rate": 1.1596472908690103e-05,
      "loss": 1.7946,
      "step": 1191600
    },
    {
      "epoch": 46.08809993425378,
      "grad_norm": 13.796143531799316,
      "learning_rate": 1.1593250054788516e-05,
      "loss": 1.8096,
      "step": 1191700
    },
    {
      "epoch": 46.09196735893568,
      "grad_norm": 9.857512474060059,
      "learning_rate": 1.159002720088693e-05,
      "loss": 1.8334,
      "step": 1191800
    },
    {
      "epoch": 46.09583478361759,
      "grad_norm": 7.031872749328613,
      "learning_rate": 1.1586804346985342e-05,
      "loss": 1.7093,
      "step": 1191900
    },
    {
      "epoch": 46.09970220829949,
      "grad_norm": 9.568109512329102,
      "learning_rate": 1.1583581493083756e-05,
      "loss": 1.7709,
      "step": 1192000
    },
    {
      "epoch": 46.1035696329814,
      "grad_norm": 8.586390495300293,
      "learning_rate": 1.1580358639182169e-05,
      "loss": 1.8299,
      "step": 1192100
    },
    {
      "epoch": 46.107437057663304,
      "grad_norm": 9.953200340270996,
      "learning_rate": 1.1577135785280582e-05,
      "loss": 1.7828,
      "step": 1192200
    },
    {
      "epoch": 46.111304482345204,
      "grad_norm": 13.677238464355469,
      "learning_rate": 1.1573912931378995e-05,
      "loss": 1.7988,
      "step": 1192300
    },
    {
      "epoch": 46.11517190702711,
      "grad_norm": 9.590012550354004,
      "learning_rate": 1.157069007747741e-05,
      "loss": 1.8114,
      "step": 1192400
    },
    {
      "epoch": 46.11903933170902,
      "grad_norm": 12.71513557434082,
      "learning_rate": 1.1567467223575822e-05,
      "loss": 1.8802,
      "step": 1192500
    },
    {
      "epoch": 46.12290675639092,
      "grad_norm": 13.562192916870117,
      "learning_rate": 1.1564244369674235e-05,
      "loss": 1.7164,
      "step": 1192600
    },
    {
      "epoch": 46.126774181072825,
      "grad_norm": 12.977815628051758,
      "learning_rate": 1.1561021515772648e-05,
      "loss": 1.6358,
      "step": 1192700
    },
    {
      "epoch": 46.130641605754725,
      "grad_norm": 11.07853889465332,
      "learning_rate": 1.1557798661871061e-05,
      "loss": 1.7558,
      "step": 1192800
    },
    {
      "epoch": 46.13450903043663,
      "grad_norm": 9.267393112182617,
      "learning_rate": 1.1554575807969474e-05,
      "loss": 1.7727,
      "step": 1192900
    },
    {
      "epoch": 46.13837645511854,
      "grad_norm": 10.237443923950195,
      "learning_rate": 1.1551352954067887e-05,
      "loss": 1.7167,
      "step": 1193000
    },
    {
      "epoch": 46.14224387980044,
      "grad_norm": 12.295012474060059,
      "learning_rate": 1.15481301001663e-05,
      "loss": 1.7484,
      "step": 1193100
    },
    {
      "epoch": 46.146111304482346,
      "grad_norm": 9.785616874694824,
      "learning_rate": 1.1544907246264713e-05,
      "loss": 1.7132,
      "step": 1193200
    },
    {
      "epoch": 46.149978729164246,
      "grad_norm": 14.394436836242676,
      "learning_rate": 1.1541684392363127e-05,
      "loss": 1.7276,
      "step": 1193300
    },
    {
      "epoch": 46.15384615384615,
      "grad_norm": 11.921111106872559,
      "learning_rate": 1.153846153846154e-05,
      "loss": 1.8956,
      "step": 1193400
    },
    {
      "epoch": 46.15771357852806,
      "grad_norm": 13.551034927368164,
      "learning_rate": 1.1535238684559953e-05,
      "loss": 1.8341,
      "step": 1193500
    },
    {
      "epoch": 46.16158100320996,
      "grad_norm": 11.034796714782715,
      "learning_rate": 1.1532015830658364e-05,
      "loss": 1.7949,
      "step": 1193600
    },
    {
      "epoch": 46.16544842789187,
      "grad_norm": 12.395495414733887,
      "learning_rate": 1.1528792976756777e-05,
      "loss": 1.7323,
      "step": 1193700
    },
    {
      "epoch": 46.169315852573774,
      "grad_norm": 13.477553367614746,
      "learning_rate": 1.152557012285519e-05,
      "loss": 1.8248,
      "step": 1193800
    },
    {
      "epoch": 46.173183277255674,
      "grad_norm": 10.024017333984375,
      "learning_rate": 1.1522347268953605e-05,
      "loss": 1.8268,
      "step": 1193900
    },
    {
      "epoch": 46.17705070193758,
      "grad_norm": 13.934610366821289,
      "learning_rate": 1.1519124415052018e-05,
      "loss": 1.8112,
      "step": 1194000
    },
    {
      "epoch": 46.18091812661948,
      "grad_norm": 10.31202507019043,
      "learning_rate": 1.151590156115043e-05,
      "loss": 1.8246,
      "step": 1194100
    },
    {
      "epoch": 46.18478555130139,
      "grad_norm": 13.2583589553833,
      "learning_rate": 1.1512678707248844e-05,
      "loss": 1.7935,
      "step": 1194200
    },
    {
      "epoch": 46.188652975983295,
      "grad_norm": 14.033378601074219,
      "learning_rate": 1.1509455853347257e-05,
      "loss": 1.7656,
      "step": 1194300
    },
    {
      "epoch": 46.192520400665195,
      "grad_norm": 17.876239776611328,
      "learning_rate": 1.150623299944567e-05,
      "loss": 1.8224,
      "step": 1194400
    },
    {
      "epoch": 46.1963878253471,
      "grad_norm": 16.027790069580078,
      "learning_rate": 1.1503010145544083e-05,
      "loss": 1.8941,
      "step": 1194500
    },
    {
      "epoch": 46.200255250029,
      "grad_norm": 10.320158004760742,
      "learning_rate": 1.1499787291642496e-05,
      "loss": 1.8239,
      "step": 1194600
    },
    {
      "epoch": 46.20412267471091,
      "grad_norm": 12.214616775512695,
      "learning_rate": 1.1496564437740909e-05,
      "loss": 1.8774,
      "step": 1194700
    },
    {
      "epoch": 46.207990099392816,
      "grad_norm": 13.622818946838379,
      "learning_rate": 1.1493341583839322e-05,
      "loss": 1.8459,
      "step": 1194800
    },
    {
      "epoch": 46.211857524074716,
      "grad_norm": 11.91203498840332,
      "learning_rate": 1.1490118729937735e-05,
      "loss": 1.7947,
      "step": 1194900
    },
    {
      "epoch": 46.21572494875662,
      "grad_norm": 16.369583129882812,
      "learning_rate": 1.1486895876036148e-05,
      "loss": 1.7906,
      "step": 1195000
    },
    {
      "epoch": 46.21959237343853,
      "grad_norm": 16.305891036987305,
      "learning_rate": 1.1483673022134561e-05,
      "loss": 1.8107,
      "step": 1195100
    },
    {
      "epoch": 46.22345979812043,
      "grad_norm": 15.058661460876465,
      "learning_rate": 1.1480450168232974e-05,
      "loss": 1.7942,
      "step": 1195200
    },
    {
      "epoch": 46.22732722280234,
      "grad_norm": 12.081887245178223,
      "learning_rate": 1.1477227314331387e-05,
      "loss": 1.7597,
      "step": 1195300
    },
    {
      "epoch": 46.23119464748424,
      "grad_norm": 15.60076904296875,
      "learning_rate": 1.14740044604298e-05,
      "loss": 1.704,
      "step": 1195400
    },
    {
      "epoch": 46.235062072166144,
      "grad_norm": 9.341292381286621,
      "learning_rate": 1.1470781606528213e-05,
      "loss": 1.8322,
      "step": 1195500
    },
    {
      "epoch": 46.23892949684805,
      "grad_norm": 9.508131980895996,
      "learning_rate": 1.1467558752626626e-05,
      "loss": 1.7852,
      "step": 1195600
    },
    {
      "epoch": 46.24279692152995,
      "grad_norm": 11.073546409606934,
      "learning_rate": 1.1464335898725039e-05,
      "loss": 1.7316,
      "step": 1195700
    },
    {
      "epoch": 46.24666434621186,
      "grad_norm": 10.6058988571167,
      "learning_rate": 1.1461113044823454e-05,
      "loss": 1.7842,
      "step": 1195800
    },
    {
      "epoch": 46.250531770893765,
      "grad_norm": 18.192001342773438,
      "learning_rate": 1.1457890190921867e-05,
      "loss": 1.7451,
      "step": 1195900
    },
    {
      "epoch": 46.254399195575665,
      "grad_norm": 13.357236862182617,
      "learning_rate": 1.145466733702028e-05,
      "loss": 1.8476,
      "step": 1196000
    },
    {
      "epoch": 46.25826662025757,
      "grad_norm": 17.59764289855957,
      "learning_rate": 1.1451444483118693e-05,
      "loss": 1.8233,
      "step": 1196100
    },
    {
      "epoch": 46.26213404493947,
      "grad_norm": 12.393840789794922,
      "learning_rate": 1.1448221629217106e-05,
      "loss": 1.8865,
      "step": 1196200
    },
    {
      "epoch": 46.26600146962138,
      "grad_norm": 11.9967041015625,
      "learning_rate": 1.1444998775315519e-05,
      "loss": 1.734,
      "step": 1196300
    },
    {
      "epoch": 46.269868894303286,
      "grad_norm": 16.049619674682617,
      "learning_rate": 1.1441775921413932e-05,
      "loss": 1.7515,
      "step": 1196400
    },
    {
      "epoch": 46.273736318985186,
      "grad_norm": 16.55838966369629,
      "learning_rate": 1.1438553067512343e-05,
      "loss": 1.7943,
      "step": 1196500
    },
    {
      "epoch": 46.27760374366709,
      "grad_norm": 13.1991605758667,
      "learning_rate": 1.1435330213610756e-05,
      "loss": 1.7815,
      "step": 1196600
    },
    {
      "epoch": 46.28147116834899,
      "grad_norm": 16.274749755859375,
      "learning_rate": 1.143210735970917e-05,
      "loss": 1.9717,
      "step": 1196700
    },
    {
      "epoch": 46.2853385930309,
      "grad_norm": 10.961188316345215,
      "learning_rate": 1.1428884505807582e-05,
      "loss": 1.8549,
      "step": 1196800
    },
    {
      "epoch": 46.28920601771281,
      "grad_norm": 11.560731887817383,
      "learning_rate": 1.1425661651905995e-05,
      "loss": 1.7733,
      "step": 1196900
    },
    {
      "epoch": 46.29307344239471,
      "grad_norm": 12.292311668395996,
      "learning_rate": 1.1422438798004408e-05,
      "loss": 1.8224,
      "step": 1197000
    },
    {
      "epoch": 46.296940867076614,
      "grad_norm": 10.964948654174805,
      "learning_rate": 1.1419215944102821e-05,
      "loss": 1.7814,
      "step": 1197100
    },
    {
      "epoch": 46.30080829175852,
      "grad_norm": 14.585179328918457,
      "learning_rate": 1.1415993090201234e-05,
      "loss": 1.7706,
      "step": 1197200
    },
    {
      "epoch": 46.30467571644042,
      "grad_norm": 12.622718811035156,
      "learning_rate": 1.1412770236299647e-05,
      "loss": 1.8308,
      "step": 1197300
    },
    {
      "epoch": 46.30854314112233,
      "grad_norm": 11.83391284942627,
      "learning_rate": 1.1409547382398062e-05,
      "loss": 1.7942,
      "step": 1197400
    },
    {
      "epoch": 46.31241056580423,
      "grad_norm": 9.003515243530273,
      "learning_rate": 1.1406324528496475e-05,
      "loss": 1.813,
      "step": 1197500
    },
    {
      "epoch": 46.316277990486135,
      "grad_norm": 12.695834159851074,
      "learning_rate": 1.1403101674594888e-05,
      "loss": 1.742,
      "step": 1197600
    },
    {
      "epoch": 46.32014541516804,
      "grad_norm": 11.606392860412598,
      "learning_rate": 1.1399878820693301e-05,
      "loss": 1.7299,
      "step": 1197700
    },
    {
      "epoch": 46.32401283984994,
      "grad_norm": 13.777118682861328,
      "learning_rate": 1.1396655966791714e-05,
      "loss": 1.7284,
      "step": 1197800
    },
    {
      "epoch": 46.32788026453185,
      "grad_norm": 14.92005443572998,
      "learning_rate": 1.1393433112890127e-05,
      "loss": 1.8552,
      "step": 1197900
    },
    {
      "epoch": 46.33174768921375,
      "grad_norm": 8.742460250854492,
      "learning_rate": 1.139021025898854e-05,
      "loss": 1.8171,
      "step": 1198000
    },
    {
      "epoch": 46.335615113895656,
      "grad_norm": 17.726608276367188,
      "learning_rate": 1.1386987405086953e-05,
      "loss": 1.8038,
      "step": 1198100
    },
    {
      "epoch": 46.33948253857756,
      "grad_norm": 8.336968421936035,
      "learning_rate": 1.1383764551185366e-05,
      "loss": 1.7844,
      "step": 1198200
    },
    {
      "epoch": 46.34334996325946,
      "grad_norm": 15.591500282287598,
      "learning_rate": 1.138054169728378e-05,
      "loss": 1.9021,
      "step": 1198300
    },
    {
      "epoch": 46.34721738794137,
      "grad_norm": 10.099931716918945,
      "learning_rate": 1.1377318843382192e-05,
      "loss": 1.7034,
      "step": 1198400
    },
    {
      "epoch": 46.35108481262328,
      "grad_norm": 12.169915199279785,
      "learning_rate": 1.1374095989480605e-05,
      "loss": 1.7433,
      "step": 1198500
    },
    {
      "epoch": 46.35495223730518,
      "grad_norm": 16.830249786376953,
      "learning_rate": 1.1370873135579018e-05,
      "loss": 1.8052,
      "step": 1198600
    },
    {
      "epoch": 46.358819661987084,
      "grad_norm": 12.214319229125977,
      "learning_rate": 1.1367650281677432e-05,
      "loss": 1.7716,
      "step": 1198700
    },
    {
      "epoch": 46.362687086668984,
      "grad_norm": 12.506895065307617,
      "learning_rate": 1.1364427427775845e-05,
      "loss": 1.8692,
      "step": 1198800
    },
    {
      "epoch": 46.36655451135089,
      "grad_norm": 14.267104148864746,
      "learning_rate": 1.1361204573874258e-05,
      "loss": 1.7786,
      "step": 1198900
    },
    {
      "epoch": 46.3704219360328,
      "grad_norm": 14.943246841430664,
      "learning_rate": 1.135798171997267e-05,
      "loss": 1.8983,
      "step": 1199000
    },
    {
      "epoch": 46.3742893607147,
      "grad_norm": 14.080811500549316,
      "learning_rate": 1.1354758866071084e-05,
      "loss": 1.9282,
      "step": 1199100
    },
    {
      "epoch": 46.378156785396605,
      "grad_norm": 12.28055191040039,
      "learning_rate": 1.1351536012169497e-05,
      "loss": 1.8517,
      "step": 1199200
    },
    {
      "epoch": 46.38202421007851,
      "grad_norm": 14.388948440551758,
      "learning_rate": 1.1348313158267911e-05,
      "loss": 1.8773,
      "step": 1199300
    },
    {
      "epoch": 46.38589163476041,
      "grad_norm": 13.036613464355469,
      "learning_rate": 1.1345090304366324e-05,
      "loss": 1.8318,
      "step": 1199400
    },
    {
      "epoch": 46.38975905944232,
      "grad_norm": 10.645312309265137,
      "learning_rate": 1.1341867450464736e-05,
      "loss": 1.7697,
      "step": 1199500
    },
    {
      "epoch": 46.39362648412422,
      "grad_norm": 14.99036693572998,
      "learning_rate": 1.1338644596563149e-05,
      "loss": 1.8445,
      "step": 1199600
    },
    {
      "epoch": 46.397493908806126,
      "grad_norm": 15.393219947814941,
      "learning_rate": 1.1335421742661562e-05,
      "loss": 1.8996,
      "step": 1199700
    },
    {
      "epoch": 46.40136133348803,
      "grad_norm": 11.280582427978516,
      "learning_rate": 1.1332198888759975e-05,
      "loss": 1.8598,
      "step": 1199800
    },
    {
      "epoch": 46.40522875816993,
      "grad_norm": 14.222468376159668,
      "learning_rate": 1.1328976034858388e-05,
      "loss": 1.7258,
      "step": 1199900
    },
    {
      "epoch": 46.40909618285184,
      "grad_norm": 10.490985870361328,
      "learning_rate": 1.13257531809568e-05,
      "loss": 1.7915,
      "step": 1200000
    },
    {
      "epoch": 46.41296360753374,
      "grad_norm": 15.52975082397461,
      "learning_rate": 1.1322530327055214e-05,
      "loss": 1.8801,
      "step": 1200100
    },
    {
      "epoch": 46.41683103221565,
      "grad_norm": 10.583671569824219,
      "learning_rate": 1.1319307473153627e-05,
      "loss": 1.716,
      "step": 1200200
    },
    {
      "epoch": 46.420698456897554,
      "grad_norm": 11.37648868560791,
      "learning_rate": 1.131608461925204e-05,
      "loss": 1.7658,
      "step": 1200300
    },
    {
      "epoch": 46.424565881579454,
      "grad_norm": 12.065889358520508,
      "learning_rate": 1.1312861765350453e-05,
      "loss": 1.8396,
      "step": 1200400
    },
    {
      "epoch": 46.42843330626136,
      "grad_norm": 12.898371696472168,
      "learning_rate": 1.1309638911448866e-05,
      "loss": 1.6546,
      "step": 1200500
    },
    {
      "epoch": 46.43230073094327,
      "grad_norm": 13.281704902648926,
      "learning_rate": 1.1306416057547279e-05,
      "loss": 1.8323,
      "step": 1200600
    },
    {
      "epoch": 46.43616815562517,
      "grad_norm": 13.016863822937012,
      "learning_rate": 1.1303193203645692e-05,
      "loss": 1.7426,
      "step": 1200700
    },
    {
      "epoch": 46.440035580307075,
      "grad_norm": 12.518900871276855,
      "learning_rate": 1.1299970349744107e-05,
      "loss": 1.8068,
      "step": 1200800
    },
    {
      "epoch": 46.443903004988975,
      "grad_norm": 12.184967041015625,
      "learning_rate": 1.129674749584252e-05,
      "loss": 1.7112,
      "step": 1200900
    },
    {
      "epoch": 46.44777042967088,
      "grad_norm": 9.06324577331543,
      "learning_rate": 1.1293524641940933e-05,
      "loss": 1.7772,
      "step": 1201000
    },
    {
      "epoch": 46.45163785435279,
      "grad_norm": 9.885712623596191,
      "learning_rate": 1.1290301788039346e-05,
      "loss": 1.7935,
      "step": 1201100
    },
    {
      "epoch": 46.45550527903469,
      "grad_norm": 12.79936695098877,
      "learning_rate": 1.1287078934137759e-05,
      "loss": 1.825,
      "step": 1201200
    },
    {
      "epoch": 46.459372703716596,
      "grad_norm": 12.63221549987793,
      "learning_rate": 1.1283856080236172e-05,
      "loss": 1.7845,
      "step": 1201300
    },
    {
      "epoch": 46.463240128398496,
      "grad_norm": 16.07640266418457,
      "learning_rate": 1.1280633226334585e-05,
      "loss": 1.8552,
      "step": 1201400
    },
    {
      "epoch": 46.4671075530804,
      "grad_norm": 14.999212265014648,
      "learning_rate": 1.1277410372432998e-05,
      "loss": 1.8991,
      "step": 1201500
    },
    {
      "epoch": 46.47097497776231,
      "grad_norm": 12.50168228149414,
      "learning_rate": 1.1274187518531411e-05,
      "loss": 1.7359,
      "step": 1201600
    },
    {
      "epoch": 46.47484240244421,
      "grad_norm": 12.444765090942383,
      "learning_rate": 1.1270964664629824e-05,
      "loss": 1.7968,
      "step": 1201700
    },
    {
      "epoch": 46.47870982712612,
      "grad_norm": 14.413777351379395,
      "learning_rate": 1.1267741810728237e-05,
      "loss": 1.8395,
      "step": 1201800
    },
    {
      "epoch": 46.482577251808024,
      "grad_norm": 13.216008186340332,
      "learning_rate": 1.126451895682665e-05,
      "loss": 1.8559,
      "step": 1201900
    },
    {
      "epoch": 46.486444676489924,
      "grad_norm": 13.986485481262207,
      "learning_rate": 1.1261296102925063e-05,
      "loss": 1.8176,
      "step": 1202000
    },
    {
      "epoch": 46.49031210117183,
      "grad_norm": 12.173752784729004,
      "learning_rate": 1.1258073249023476e-05,
      "loss": 1.8069,
      "step": 1202100
    },
    {
      "epoch": 46.49417952585373,
      "grad_norm": 12.073624610900879,
      "learning_rate": 1.1254850395121889e-05,
      "loss": 1.838,
      "step": 1202200
    },
    {
      "epoch": 46.49804695053564,
      "grad_norm": 12.4497652053833,
      "learning_rate": 1.1251627541220302e-05,
      "loss": 1.7961,
      "step": 1202300
    },
    {
      "epoch": 46.501914375217545,
      "grad_norm": 10.326822280883789,
      "learning_rate": 1.1248404687318715e-05,
      "loss": 1.7511,
      "step": 1202400
    },
    {
      "epoch": 46.505781799899445,
      "grad_norm": 9.692540168762207,
      "learning_rate": 1.1245181833417128e-05,
      "loss": 1.7089,
      "step": 1202500
    },
    {
      "epoch": 46.50964922458135,
      "grad_norm": 13.8788423538208,
      "learning_rate": 1.1241958979515541e-05,
      "loss": 1.7666,
      "step": 1202600
    },
    {
      "epoch": 46.51351664926325,
      "grad_norm": 10.416730880737305,
      "learning_rate": 1.1238736125613954e-05,
      "loss": 1.8336,
      "step": 1202700
    },
    {
      "epoch": 46.51738407394516,
      "grad_norm": 10.66120433807373,
      "learning_rate": 1.1235513271712367e-05,
      "loss": 1.7945,
      "step": 1202800
    },
    {
      "epoch": 46.521251498627066,
      "grad_norm": 12.393078804016113,
      "learning_rate": 1.123229041781078e-05,
      "loss": 1.9846,
      "step": 1202900
    },
    {
      "epoch": 46.525118923308966,
      "grad_norm": 14.682196617126465,
      "learning_rate": 1.1229067563909193e-05,
      "loss": 1.7743,
      "step": 1203000
    },
    {
      "epoch": 46.52898634799087,
      "grad_norm": 12.740352630615234,
      "learning_rate": 1.1225844710007606e-05,
      "loss": 1.8194,
      "step": 1203100
    },
    {
      "epoch": 46.53285377267278,
      "grad_norm": 13.25133991241455,
      "learning_rate": 1.122262185610602e-05,
      "loss": 1.7776,
      "step": 1203200
    },
    {
      "epoch": 46.53672119735468,
      "grad_norm": 12.11657428741455,
      "learning_rate": 1.1219399002204432e-05,
      "loss": 1.7556,
      "step": 1203300
    },
    {
      "epoch": 46.54058862203659,
      "grad_norm": 15.37423324584961,
      "learning_rate": 1.1216176148302845e-05,
      "loss": 1.6847,
      "step": 1203400
    },
    {
      "epoch": 46.54445604671849,
      "grad_norm": 14.388605117797852,
      "learning_rate": 1.1212953294401258e-05,
      "loss": 1.7913,
      "step": 1203500
    },
    {
      "epoch": 46.548323471400394,
      "grad_norm": 16.17125701904297,
      "learning_rate": 1.1209730440499671e-05,
      "loss": 1.8701,
      "step": 1203600
    },
    {
      "epoch": 46.5521908960823,
      "grad_norm": 12.096688270568848,
      "learning_rate": 1.1206507586598084e-05,
      "loss": 1.7058,
      "step": 1203700
    },
    {
      "epoch": 46.5560583207642,
      "grad_norm": 13.511059761047363,
      "learning_rate": 1.1203284732696497e-05,
      "loss": 1.8386,
      "step": 1203800
    },
    {
      "epoch": 46.55992574544611,
      "grad_norm": 14.943231582641602,
      "learning_rate": 1.120006187879491e-05,
      "loss": 1.8163,
      "step": 1203900
    },
    {
      "epoch": 46.563793170128015,
      "grad_norm": 11.425419807434082,
      "learning_rate": 1.1196839024893324e-05,
      "loss": 1.8471,
      "step": 1204000
    },
    {
      "epoch": 46.567660594809915,
      "grad_norm": 10.339981079101562,
      "learning_rate": 1.1193616170991737e-05,
      "loss": 1.9697,
      "step": 1204100
    },
    {
      "epoch": 46.57152801949182,
      "grad_norm": 12.913106918334961,
      "learning_rate": 1.119039331709015e-05,
      "loss": 1.8823,
      "step": 1204200
    },
    {
      "epoch": 46.57539544417372,
      "grad_norm": 14.413507461547852,
      "learning_rate": 1.1187170463188564e-05,
      "loss": 1.8518,
      "step": 1204300
    },
    {
      "epoch": 46.57926286885563,
      "grad_norm": 13.681572914123535,
      "learning_rate": 1.1183947609286977e-05,
      "loss": 1.7992,
      "step": 1204400
    },
    {
      "epoch": 46.583130293537536,
      "grad_norm": 20.15584945678711,
      "learning_rate": 1.118072475538539e-05,
      "loss": 1.7472,
      "step": 1204500
    },
    {
      "epoch": 46.586997718219436,
      "grad_norm": 13.072490692138672,
      "learning_rate": 1.1177501901483803e-05,
      "loss": 1.7978,
      "step": 1204600
    },
    {
      "epoch": 46.59086514290134,
      "grad_norm": 11.402560234069824,
      "learning_rate": 1.1174279047582216e-05,
      "loss": 1.8522,
      "step": 1204700
    },
    {
      "epoch": 46.59473256758324,
      "grad_norm": 11.484128952026367,
      "learning_rate": 1.117105619368063e-05,
      "loss": 1.6814,
      "step": 1204800
    },
    {
      "epoch": 46.59859999226515,
      "grad_norm": 11.933056831359863,
      "learning_rate": 1.1167833339779042e-05,
      "loss": 1.8038,
      "step": 1204900
    },
    {
      "epoch": 46.60246741694706,
      "grad_norm": 12.776205062866211,
      "learning_rate": 1.1164610485877455e-05,
      "loss": 1.8143,
      "step": 1205000
    },
    {
      "epoch": 46.60633484162896,
      "grad_norm": 12.174651145935059,
      "learning_rate": 1.1161387631975868e-05,
      "loss": 1.7516,
      "step": 1205100
    },
    {
      "epoch": 46.610202266310864,
      "grad_norm": 13.09410572052002,
      "learning_rate": 1.1158164778074281e-05,
      "loss": 1.88,
      "step": 1205200
    },
    {
      "epoch": 46.61406969099277,
      "grad_norm": 12.042696952819824,
      "learning_rate": 1.1154941924172695e-05,
      "loss": 1.7485,
      "step": 1205300
    },
    {
      "epoch": 46.61793711567467,
      "grad_norm": 12.85923957824707,
      "learning_rate": 1.1151719070271106e-05,
      "loss": 1.7522,
      "step": 1205400
    },
    {
      "epoch": 46.62180454035658,
      "grad_norm": 10.95317268371582,
      "learning_rate": 1.1148496216369519e-05,
      "loss": 1.8318,
      "step": 1205500
    },
    {
      "epoch": 46.62567196503848,
      "grad_norm": 19.351591110229492,
      "learning_rate": 1.1145273362467932e-05,
      "loss": 1.783,
      "step": 1205600
    },
    {
      "epoch": 46.629539389720385,
      "grad_norm": 11.524094581604004,
      "learning_rate": 1.1142050508566345e-05,
      "loss": 1.7544,
      "step": 1205700
    },
    {
      "epoch": 46.63340681440229,
      "grad_norm": 11.3961820602417,
      "learning_rate": 1.113882765466476e-05,
      "loss": 1.8509,
      "step": 1205800
    },
    {
      "epoch": 46.63727423908419,
      "grad_norm": 11.153958320617676,
      "learning_rate": 1.1135604800763173e-05,
      "loss": 1.8301,
      "step": 1205900
    },
    {
      "epoch": 46.6411416637661,
      "grad_norm": 12.644018173217773,
      "learning_rate": 1.1132381946861586e-05,
      "loss": 1.822,
      "step": 1206000
    },
    {
      "epoch": 46.645009088448,
      "grad_norm": 15.15611743927002,
      "learning_rate": 1.1129159092959999e-05,
      "loss": 1.8369,
      "step": 1206100
    },
    {
      "epoch": 46.648876513129906,
      "grad_norm": 13.655043601989746,
      "learning_rate": 1.1125936239058412e-05,
      "loss": 1.7072,
      "step": 1206200
    },
    {
      "epoch": 46.65274393781181,
      "grad_norm": 11.957290649414062,
      "learning_rate": 1.1122713385156825e-05,
      "loss": 1.8119,
      "step": 1206300
    },
    {
      "epoch": 46.65661136249371,
      "grad_norm": 12.105867385864258,
      "learning_rate": 1.1119490531255238e-05,
      "loss": 1.7526,
      "step": 1206400
    },
    {
      "epoch": 46.66047878717562,
      "grad_norm": 9.681523323059082,
      "learning_rate": 1.111626767735365e-05,
      "loss": 1.7685,
      "step": 1206500
    },
    {
      "epoch": 46.66434621185753,
      "grad_norm": 14.701299667358398,
      "learning_rate": 1.1113044823452064e-05,
      "loss": 1.785,
      "step": 1206600
    },
    {
      "epoch": 46.66821363653943,
      "grad_norm": 13.367718696594238,
      "learning_rate": 1.1109821969550477e-05,
      "loss": 1.8079,
      "step": 1206700
    },
    {
      "epoch": 46.672081061221334,
      "grad_norm": 11.39799976348877,
      "learning_rate": 1.110659911564889e-05,
      "loss": 1.7789,
      "step": 1206800
    },
    {
      "epoch": 46.675948485903234,
      "grad_norm": 12.486715316772461,
      "learning_rate": 1.1103376261747303e-05,
      "loss": 1.7117,
      "step": 1206900
    },
    {
      "epoch": 46.67981591058514,
      "grad_norm": 12.253512382507324,
      "learning_rate": 1.1100153407845716e-05,
      "loss": 1.8102,
      "step": 1207000
    },
    {
      "epoch": 46.68368333526705,
      "grad_norm": 10.56844425201416,
      "learning_rate": 1.1096930553944129e-05,
      "loss": 1.8925,
      "step": 1207100
    },
    {
      "epoch": 46.68755075994895,
      "grad_norm": 11.780708312988281,
      "learning_rate": 1.1093707700042542e-05,
      "loss": 1.8385,
      "step": 1207200
    },
    {
      "epoch": 46.691418184630855,
      "grad_norm": 13.67961597442627,
      "learning_rate": 1.1090484846140955e-05,
      "loss": 1.7896,
      "step": 1207300
    },
    {
      "epoch": 46.69528560931276,
      "grad_norm": 9.331697463989258,
      "learning_rate": 1.1087261992239368e-05,
      "loss": 1.6945,
      "step": 1207400
    },
    {
      "epoch": 46.69915303399466,
      "grad_norm": 10.471121788024902,
      "learning_rate": 1.1084039138337781e-05,
      "loss": 1.8787,
      "step": 1207500
    },
    {
      "epoch": 46.70302045867657,
      "grad_norm": 14.600534439086914,
      "learning_rate": 1.1080816284436194e-05,
      "loss": 1.8605,
      "step": 1207600
    },
    {
      "epoch": 46.70688788335847,
      "grad_norm": 12.163801193237305,
      "learning_rate": 1.1077593430534609e-05,
      "loss": 1.8475,
      "step": 1207700
    },
    {
      "epoch": 46.710755308040376,
      "grad_norm": 12.926502227783203,
      "learning_rate": 1.1074370576633022e-05,
      "loss": 1.7888,
      "step": 1207800
    },
    {
      "epoch": 46.71462273272228,
      "grad_norm": 14.771820068359375,
      "learning_rate": 1.1071147722731435e-05,
      "loss": 1.8039,
      "step": 1207900
    },
    {
      "epoch": 46.71849015740418,
      "grad_norm": 12.977593421936035,
      "learning_rate": 1.1067924868829848e-05,
      "loss": 1.9126,
      "step": 1208000
    },
    {
      "epoch": 46.72235758208609,
      "grad_norm": 14.899654388427734,
      "learning_rate": 1.1064702014928261e-05,
      "loss": 1.8344,
      "step": 1208100
    },
    {
      "epoch": 46.72622500676799,
      "grad_norm": 13.054237365722656,
      "learning_rate": 1.1061479161026674e-05,
      "loss": 1.8838,
      "step": 1208200
    },
    {
      "epoch": 46.7300924314499,
      "grad_norm": 15.739943504333496,
      "learning_rate": 1.1058256307125085e-05,
      "loss": 1.7217,
      "step": 1208300
    },
    {
      "epoch": 46.733959856131804,
      "grad_norm": 12.86905288696289,
      "learning_rate": 1.1055033453223498e-05,
      "loss": 1.8304,
      "step": 1208400
    },
    {
      "epoch": 46.737827280813704,
      "grad_norm": 12.528478622436523,
      "learning_rate": 1.1051810599321911e-05,
      "loss": 1.7901,
      "step": 1208500
    },
    {
      "epoch": 46.74169470549561,
      "grad_norm": 15.779962539672852,
      "learning_rate": 1.1048587745420324e-05,
      "loss": 1.758,
      "step": 1208600
    },
    {
      "epoch": 46.74556213017752,
      "grad_norm": 10.160670280456543,
      "learning_rate": 1.1045364891518737e-05,
      "loss": 1.7792,
      "step": 1208700
    },
    {
      "epoch": 46.74942955485942,
      "grad_norm": 11.704602241516113,
      "learning_rate": 1.104214203761715e-05,
      "loss": 1.8156,
      "step": 1208800
    },
    {
      "epoch": 46.753296979541325,
      "grad_norm": 12.08890151977539,
      "learning_rate": 1.1038919183715563e-05,
      "loss": 1.814,
      "step": 1208900
    },
    {
      "epoch": 46.757164404223225,
      "grad_norm": 12.581380844116211,
      "learning_rate": 1.1035696329813976e-05,
      "loss": 1.7827,
      "step": 1209000
    },
    {
      "epoch": 46.76103182890513,
      "grad_norm": 10.438305854797363,
      "learning_rate": 1.103247347591239e-05,
      "loss": 1.825,
      "step": 1209100
    },
    {
      "epoch": 46.76489925358704,
      "grad_norm": 13.527480125427246,
      "learning_rate": 1.1029250622010802e-05,
      "loss": 1.8117,
      "step": 1209200
    },
    {
      "epoch": 46.76876667826894,
      "grad_norm": 12.244422912597656,
      "learning_rate": 1.1026027768109217e-05,
      "loss": 1.7741,
      "step": 1209300
    },
    {
      "epoch": 46.772634102950846,
      "grad_norm": 16.242549896240234,
      "learning_rate": 1.102280491420763e-05,
      "loss": 1.7907,
      "step": 1209400
    },
    {
      "epoch": 46.776501527632746,
      "grad_norm": 11.978549003601074,
      "learning_rate": 1.1019582060306043e-05,
      "loss": 1.8629,
      "step": 1209500
    },
    {
      "epoch": 46.78036895231465,
      "grad_norm": 12.16845989227295,
      "learning_rate": 1.1016359206404456e-05,
      "loss": 1.7378,
      "step": 1209600
    },
    {
      "epoch": 46.78423637699656,
      "grad_norm": 13.761813163757324,
      "learning_rate": 1.101313635250287e-05,
      "loss": 1.7523,
      "step": 1209700
    },
    {
      "epoch": 46.78810380167846,
      "grad_norm": 12.58314323425293,
      "learning_rate": 1.1009913498601282e-05,
      "loss": 1.9403,
      "step": 1209800
    },
    {
      "epoch": 46.79197122636037,
      "grad_norm": 13.094239234924316,
      "learning_rate": 1.1006690644699695e-05,
      "loss": 1.8315,
      "step": 1209900
    },
    {
      "epoch": 46.795838651042274,
      "grad_norm": 14.710325241088867,
      "learning_rate": 1.1003467790798108e-05,
      "loss": 1.7673,
      "step": 1210000
    },
    {
      "epoch": 46.799706075724174,
      "grad_norm": 18.896377563476562,
      "learning_rate": 1.1000244936896521e-05,
      "loss": 1.8337,
      "step": 1210100
    },
    {
      "epoch": 46.80357350040608,
      "grad_norm": 13.373433113098145,
      "learning_rate": 1.0997022082994934e-05,
      "loss": 1.9199,
      "step": 1210200
    },
    {
      "epoch": 46.80744092508798,
      "grad_norm": 11.696112632751465,
      "learning_rate": 1.0993799229093347e-05,
      "loss": 1.9634,
      "step": 1210300
    },
    {
      "epoch": 46.81130834976989,
      "grad_norm": 8.010549545288086,
      "learning_rate": 1.099057637519176e-05,
      "loss": 1.759,
      "step": 1210400
    },
    {
      "epoch": 46.815175774451795,
      "grad_norm": 13.35073184967041,
      "learning_rate": 1.0987353521290173e-05,
      "loss": 1.6789,
      "step": 1210500
    },
    {
      "epoch": 46.819043199133695,
      "grad_norm": 13.078696250915527,
      "learning_rate": 1.0984130667388586e-05,
      "loss": 1.7931,
      "step": 1210600
    },
    {
      "epoch": 46.8229106238156,
      "grad_norm": 11.53557300567627,
      "learning_rate": 1.0980907813487e-05,
      "loss": 1.7855,
      "step": 1210700
    },
    {
      "epoch": 46.8267780484975,
      "grad_norm": 14.660612106323242,
      "learning_rate": 1.0977684959585413e-05,
      "loss": 1.8435,
      "step": 1210800
    },
    {
      "epoch": 46.83064547317941,
      "grad_norm": 11.668696403503418,
      "learning_rate": 1.0974462105683826e-05,
      "loss": 1.7842,
      "step": 1210900
    },
    {
      "epoch": 46.834512897861316,
      "grad_norm": 17.35567283630371,
      "learning_rate": 1.0971239251782239e-05,
      "loss": 1.9052,
      "step": 1211000
    },
    {
      "epoch": 46.838380322543216,
      "grad_norm": 13.289592742919922,
      "learning_rate": 1.0968016397880652e-05,
      "loss": 1.7292,
      "step": 1211100
    },
    {
      "epoch": 46.84224774722512,
      "grad_norm": 10.715649604797363,
      "learning_rate": 1.0964793543979066e-05,
      "loss": 1.8395,
      "step": 1211200
    },
    {
      "epoch": 46.84611517190703,
      "grad_norm": 12.244613647460938,
      "learning_rate": 1.0961570690077478e-05,
      "loss": 1.8655,
      "step": 1211300
    },
    {
      "epoch": 46.84998259658893,
      "grad_norm": 12.776138305664062,
      "learning_rate": 1.095834783617589e-05,
      "loss": 1.7869,
      "step": 1211400
    },
    {
      "epoch": 46.85385002127084,
      "grad_norm": 11.545367240905762,
      "learning_rate": 1.0955124982274304e-05,
      "loss": 1.7986,
      "step": 1211500
    },
    {
      "epoch": 46.85771744595274,
      "grad_norm": 15.699975967407227,
      "learning_rate": 1.0951902128372717e-05,
      "loss": 1.7818,
      "step": 1211600
    },
    {
      "epoch": 46.861584870634644,
      "grad_norm": 10.479172706604004,
      "learning_rate": 1.094867927447113e-05,
      "loss": 1.8401,
      "step": 1211700
    },
    {
      "epoch": 46.86545229531655,
      "grad_norm": 11.181467056274414,
      "learning_rate": 1.0945456420569543e-05,
      "loss": 1.7376,
      "step": 1211800
    },
    {
      "epoch": 46.86931971999845,
      "grad_norm": 10.896549224853516,
      "learning_rate": 1.0942233566667956e-05,
      "loss": 1.7745,
      "step": 1211900
    },
    {
      "epoch": 46.87318714468036,
      "grad_norm": 8.511924743652344,
      "learning_rate": 1.0939010712766369e-05,
      "loss": 1.9035,
      "step": 1212000
    },
    {
      "epoch": 46.877054569362265,
      "grad_norm": 11.832864761352539,
      "learning_rate": 1.0935787858864782e-05,
      "loss": 1.7154,
      "step": 1212100
    },
    {
      "epoch": 46.880921994044165,
      "grad_norm": 19.03449821472168,
      "learning_rate": 1.0932565004963195e-05,
      "loss": 1.8196,
      "step": 1212200
    },
    {
      "epoch": 46.88478941872607,
      "grad_norm": 13.799264907836914,
      "learning_rate": 1.0929342151061608e-05,
      "loss": 1.8594,
      "step": 1212300
    },
    {
      "epoch": 46.88865684340797,
      "grad_norm": 17.270572662353516,
      "learning_rate": 1.0926119297160021e-05,
      "loss": 1.8338,
      "step": 1212400
    },
    {
      "epoch": 46.89252426808988,
      "grad_norm": 12.225882530212402,
      "learning_rate": 1.0922896443258434e-05,
      "loss": 1.8079,
      "step": 1212500
    },
    {
      "epoch": 46.896391692771786,
      "grad_norm": 11.648666381835938,
      "learning_rate": 1.0919673589356847e-05,
      "loss": 1.896,
      "step": 1212600
    },
    {
      "epoch": 46.900259117453686,
      "grad_norm": 12.99696159362793,
      "learning_rate": 1.0916450735455262e-05,
      "loss": 1.7991,
      "step": 1212700
    },
    {
      "epoch": 46.90412654213559,
      "grad_norm": 5.091375350952148,
      "learning_rate": 1.0913227881553675e-05,
      "loss": 1.788,
      "step": 1212800
    },
    {
      "epoch": 46.90799396681749,
      "grad_norm": 12.19389533996582,
      "learning_rate": 1.0910005027652088e-05,
      "loss": 1.8027,
      "step": 1212900
    },
    {
      "epoch": 46.9118613914994,
      "grad_norm": 12.066448211669922,
      "learning_rate": 1.09067821737505e-05,
      "loss": 1.7088,
      "step": 1213000
    },
    {
      "epoch": 46.91572881618131,
      "grad_norm": 17.21792984008789,
      "learning_rate": 1.0903559319848914e-05,
      "loss": 1.777,
      "step": 1213100
    },
    {
      "epoch": 46.91959624086321,
      "grad_norm": 12.789241790771484,
      "learning_rate": 1.0900336465947327e-05,
      "loss": 1.7902,
      "step": 1213200
    },
    {
      "epoch": 46.923463665545114,
      "grad_norm": 10.302555084228516,
      "learning_rate": 1.089711361204574e-05,
      "loss": 1.8159,
      "step": 1213300
    },
    {
      "epoch": 46.92733109022702,
      "grad_norm": 10.491264343261719,
      "learning_rate": 1.0893890758144153e-05,
      "loss": 1.8114,
      "step": 1213400
    },
    {
      "epoch": 46.93119851490892,
      "grad_norm": 20.317169189453125,
      "learning_rate": 1.0890667904242566e-05,
      "loss": 1.7859,
      "step": 1213500
    },
    {
      "epoch": 46.93506593959083,
      "grad_norm": 10.521080017089844,
      "learning_rate": 1.0887445050340979e-05,
      "loss": 1.7075,
      "step": 1213600
    },
    {
      "epoch": 46.93893336427273,
      "grad_norm": 10.23525619506836,
      "learning_rate": 1.0884222196439392e-05,
      "loss": 1.7199,
      "step": 1213700
    },
    {
      "epoch": 46.942800788954635,
      "grad_norm": 12.380999565124512,
      "learning_rate": 1.0880999342537805e-05,
      "loss": 1.795,
      "step": 1213800
    },
    {
      "epoch": 46.94666821363654,
      "grad_norm": 14.829731941223145,
      "learning_rate": 1.0877776488636218e-05,
      "loss": 1.7953,
      "step": 1213900
    },
    {
      "epoch": 46.95053563831844,
      "grad_norm": 10.864745140075684,
      "learning_rate": 1.0874553634734631e-05,
      "loss": 1.8084,
      "step": 1214000
    },
    {
      "epoch": 46.95440306300035,
      "grad_norm": 15.43431282043457,
      "learning_rate": 1.0871330780833044e-05,
      "loss": 1.7373,
      "step": 1214100
    },
    {
      "epoch": 46.95827048768225,
      "grad_norm": 12.013750076293945,
      "learning_rate": 1.0868107926931455e-05,
      "loss": 1.7913,
      "step": 1214200
    },
    {
      "epoch": 46.962137912364156,
      "grad_norm": 12.144204139709473,
      "learning_rate": 1.086488507302987e-05,
      "loss": 1.8549,
      "step": 1214300
    },
    {
      "epoch": 46.96600533704606,
      "grad_norm": 9.378738403320312,
      "learning_rate": 1.0861662219128283e-05,
      "loss": 1.846,
      "step": 1214400
    },
    {
      "epoch": 46.96987276172796,
      "grad_norm": 12.776859283447266,
      "learning_rate": 1.0858439365226696e-05,
      "loss": 1.8021,
      "step": 1214500
    },
    {
      "epoch": 46.97374018640987,
      "grad_norm": 10.883282661437988,
      "learning_rate": 1.085521651132511e-05,
      "loss": 1.8773,
      "step": 1214600
    },
    {
      "epoch": 46.97760761109178,
      "grad_norm": 13.081165313720703,
      "learning_rate": 1.0851993657423522e-05,
      "loss": 1.806,
      "step": 1214700
    },
    {
      "epoch": 46.98147503577368,
      "grad_norm": 12.952814102172852,
      "learning_rate": 1.0848770803521935e-05,
      "loss": 1.781,
      "step": 1214800
    },
    {
      "epoch": 46.985342460455584,
      "grad_norm": 11.282469749450684,
      "learning_rate": 1.0845547949620348e-05,
      "loss": 1.8028,
      "step": 1214900
    },
    {
      "epoch": 46.989209885137484,
      "grad_norm": 11.090835571289062,
      "learning_rate": 1.0842325095718761e-05,
      "loss": 1.8787,
      "step": 1215000
    },
    {
      "epoch": 46.99307730981939,
      "grad_norm": 12.774394035339355,
      "learning_rate": 1.0839102241817174e-05,
      "loss": 1.7748,
      "step": 1215100
    },
    {
      "epoch": 46.9969447345013,
      "grad_norm": 6.198421478271484,
      "learning_rate": 1.0835879387915587e-05,
      "loss": 1.8369,
      "step": 1215200
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.7507166862487793,
      "eval_runtime": 5.5769,
      "eval_samples_per_second": 244.042,
      "eval_steps_per_second": 244.042,
      "step": 1215279
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.5940579175949097,
      "eval_runtime": 106.3977,
      "eval_samples_per_second": 243.022,
      "eval_steps_per_second": 243.022,
      "step": 1215279
    },
    {
      "epoch": 47.0008121591832,
      "grad_norm": 12.453390121459961,
      "learning_rate": 1.0832656534014e-05,
      "loss": 1.8382,
      "step": 1215300
    },
    {
      "epoch": 47.004679583865105,
      "grad_norm": 18.952436447143555,
      "learning_rate": 1.0829433680112413e-05,
      "loss": 1.8517,
      "step": 1215400
    },
    {
      "epoch": 47.00854700854701,
      "grad_norm": 11.572827339172363,
      "learning_rate": 1.0826210826210826e-05,
      "loss": 1.6963,
      "step": 1215500
    },
    {
      "epoch": 47.01241443322891,
      "grad_norm": 9.105965614318848,
      "learning_rate": 1.082298797230924e-05,
      "loss": 1.8239,
      "step": 1215600
    },
    {
      "epoch": 47.01628185791082,
      "grad_norm": 11.795010566711426,
      "learning_rate": 1.0819765118407652e-05,
      "loss": 1.8177,
      "step": 1215700
    },
    {
      "epoch": 47.02014928259272,
      "grad_norm": 11.713948249816895,
      "learning_rate": 1.0816542264506065e-05,
      "loss": 1.7776,
      "step": 1215800
    },
    {
      "epoch": 47.024016707274626,
      "grad_norm": 11.40595817565918,
      "learning_rate": 1.0813319410604478e-05,
      "loss": 1.7992,
      "step": 1215900
    },
    {
      "epoch": 47.02788413195653,
      "grad_norm": 8.0950288772583,
      "learning_rate": 1.0810096556702892e-05,
      "loss": 1.8331,
      "step": 1216000
    },
    {
      "epoch": 47.03175155663843,
      "grad_norm": 9.982205390930176,
      "learning_rate": 1.0806873702801305e-05,
      "loss": 1.7559,
      "step": 1216100
    },
    {
      "epoch": 47.03561898132034,
      "grad_norm": 12.319077491760254,
      "learning_rate": 1.080365084889972e-05,
      "loss": 1.8871,
      "step": 1216200
    },
    {
      "epoch": 47.03948640600224,
      "grad_norm": 19.06291389465332,
      "learning_rate": 1.0800427994998132e-05,
      "loss": 1.779,
      "step": 1216300
    },
    {
      "epoch": 47.04335383068415,
      "grad_norm": 10.67088508605957,
      "learning_rate": 1.0797205141096545e-05,
      "loss": 1.7224,
      "step": 1216400
    },
    {
      "epoch": 47.047221255366054,
      "grad_norm": 9.843362808227539,
      "learning_rate": 1.0793982287194958e-05,
      "loss": 1.7686,
      "step": 1216500
    },
    {
      "epoch": 47.051088680047954,
      "grad_norm": 9.37424087524414,
      "learning_rate": 1.0790759433293371e-05,
      "loss": 1.8193,
      "step": 1216600
    },
    {
      "epoch": 47.05495610472986,
      "grad_norm": 12.550713539123535,
      "learning_rate": 1.0787536579391784e-05,
      "loss": 1.8303,
      "step": 1216700
    },
    {
      "epoch": 47.05882352941177,
      "grad_norm": 11.552619934082031,
      "learning_rate": 1.0784313725490197e-05,
      "loss": 1.8586,
      "step": 1216800
    },
    {
      "epoch": 47.06269095409367,
      "grad_norm": 11.021978378295898,
      "learning_rate": 1.078109087158861e-05,
      "loss": 1.8308,
      "step": 1216900
    },
    {
      "epoch": 47.066558378775575,
      "grad_norm": 12.707442283630371,
      "learning_rate": 1.0777868017687023e-05,
      "loss": 1.7405,
      "step": 1217000
    },
    {
      "epoch": 47.070425803457475,
      "grad_norm": 10.59471321105957,
      "learning_rate": 1.0774645163785436e-05,
      "loss": 1.76,
      "step": 1217100
    },
    {
      "epoch": 47.07429322813938,
      "grad_norm": 11.100030899047852,
      "learning_rate": 1.0771422309883848e-05,
      "loss": 1.7887,
      "step": 1217200
    },
    {
      "epoch": 47.07816065282129,
      "grad_norm": 12.824563980102539,
      "learning_rate": 1.076819945598226e-05,
      "loss": 1.8113,
      "step": 1217300
    },
    {
      "epoch": 47.08202807750319,
      "grad_norm": 11.916848182678223,
      "learning_rate": 1.0764976602080674e-05,
      "loss": 1.7508,
      "step": 1217400
    },
    {
      "epoch": 47.085895502185096,
      "grad_norm": 9.093029975891113,
      "learning_rate": 1.0761753748179087e-05,
      "loss": 1.7727,
      "step": 1217500
    },
    {
      "epoch": 47.089762926866996,
      "grad_norm": 14.260687828063965,
      "learning_rate": 1.07585308942775e-05,
      "loss": 1.7943,
      "step": 1217600
    },
    {
      "epoch": 47.0936303515489,
      "grad_norm": 14.801373481750488,
      "learning_rate": 1.0755308040375915e-05,
      "loss": 1.7289,
      "step": 1217700
    },
    {
      "epoch": 47.09749777623081,
      "grad_norm": 11.94197940826416,
      "learning_rate": 1.0752085186474328e-05,
      "loss": 1.8546,
      "step": 1217800
    },
    {
      "epoch": 47.10136520091271,
      "grad_norm": 10.36015510559082,
      "learning_rate": 1.074886233257274e-05,
      "loss": 1.8725,
      "step": 1217900
    },
    {
      "epoch": 47.10523262559462,
      "grad_norm": 10.800701141357422,
      "learning_rate": 1.0745639478671154e-05,
      "loss": 1.8316,
      "step": 1218000
    },
    {
      "epoch": 47.109100050276524,
      "grad_norm": 12.02302074432373,
      "learning_rate": 1.0742416624769567e-05,
      "loss": 1.7386,
      "step": 1218100
    },
    {
      "epoch": 47.112967474958424,
      "grad_norm": 13.819623947143555,
      "learning_rate": 1.073919377086798e-05,
      "loss": 1.8221,
      "step": 1218200
    },
    {
      "epoch": 47.11683489964033,
      "grad_norm": 13.482141494750977,
      "learning_rate": 1.0735970916966393e-05,
      "loss": 1.8066,
      "step": 1218300
    },
    {
      "epoch": 47.12070232432223,
      "grad_norm": 10.125038146972656,
      "learning_rate": 1.0732748063064806e-05,
      "loss": 1.7344,
      "step": 1218400
    },
    {
      "epoch": 47.12456974900414,
      "grad_norm": 12.255688667297363,
      "learning_rate": 1.0729525209163219e-05,
      "loss": 1.7069,
      "step": 1218500
    },
    {
      "epoch": 47.128437173686045,
      "grad_norm": 11.582627296447754,
      "learning_rate": 1.0726302355261632e-05,
      "loss": 1.7626,
      "step": 1218600
    },
    {
      "epoch": 47.132304598367945,
      "grad_norm": 15.139167785644531,
      "learning_rate": 1.0723079501360045e-05,
      "loss": 1.7724,
      "step": 1218700
    },
    {
      "epoch": 47.13617202304985,
      "grad_norm": 16.251846313476562,
      "learning_rate": 1.0719856647458458e-05,
      "loss": 1.8129,
      "step": 1218800
    },
    {
      "epoch": 47.14003944773175,
      "grad_norm": 13.679889678955078,
      "learning_rate": 1.0716633793556871e-05,
      "loss": 1.8153,
      "step": 1218900
    },
    {
      "epoch": 47.14390687241366,
      "grad_norm": 13.47587776184082,
      "learning_rate": 1.0713410939655284e-05,
      "loss": 1.8631,
      "step": 1219000
    },
    {
      "epoch": 47.147774297095566,
      "grad_norm": 10.836112022399902,
      "learning_rate": 1.0710188085753697e-05,
      "loss": 1.7785,
      "step": 1219100
    },
    {
      "epoch": 47.151641721777466,
      "grad_norm": 14.48355770111084,
      "learning_rate": 1.070696523185211e-05,
      "loss": 1.8326,
      "step": 1219200
    },
    {
      "epoch": 47.15550914645937,
      "grad_norm": 13.566383361816406,
      "learning_rate": 1.0703742377950523e-05,
      "loss": 1.7231,
      "step": 1219300
    },
    {
      "epoch": 47.15937657114128,
      "grad_norm": 9.567242622375488,
      "learning_rate": 1.0700519524048936e-05,
      "loss": 1.8494,
      "step": 1219400
    },
    {
      "epoch": 47.16324399582318,
      "grad_norm": 11.950660705566406,
      "learning_rate": 1.0697296670147349e-05,
      "loss": 1.7726,
      "step": 1219500
    },
    {
      "epoch": 47.16711142050509,
      "grad_norm": 15.622834205627441,
      "learning_rate": 1.0694073816245764e-05,
      "loss": 1.8113,
      "step": 1219600
    },
    {
      "epoch": 47.17097884518699,
      "grad_norm": 11.541820526123047,
      "learning_rate": 1.0690850962344177e-05,
      "loss": 1.7786,
      "step": 1219700
    },
    {
      "epoch": 47.174846269868894,
      "grad_norm": 9.064571380615234,
      "learning_rate": 1.068762810844259e-05,
      "loss": 1.8523,
      "step": 1219800
    },
    {
      "epoch": 47.1787136945508,
      "grad_norm": 13.317689895629883,
      "learning_rate": 1.0684405254541003e-05,
      "loss": 1.8005,
      "step": 1219900
    },
    {
      "epoch": 47.1825811192327,
      "grad_norm": 14.49862003326416,
      "learning_rate": 1.0681182400639416e-05,
      "loss": 1.7677,
      "step": 1220000
    },
    {
      "epoch": 47.18644854391461,
      "grad_norm": 16.458398818969727,
      "learning_rate": 1.0677959546737827e-05,
      "loss": 1.761,
      "step": 1220100
    },
    {
      "epoch": 47.190315968596515,
      "grad_norm": 8.864639282226562,
      "learning_rate": 1.067473669283624e-05,
      "loss": 1.7756,
      "step": 1220200
    },
    {
      "epoch": 47.194183393278415,
      "grad_norm": 9.755661964416504,
      "learning_rate": 1.0671513838934653e-05,
      "loss": 1.8584,
      "step": 1220300
    },
    {
      "epoch": 47.19805081796032,
      "grad_norm": 10.900961875915527,
      "learning_rate": 1.0668290985033066e-05,
      "loss": 1.7142,
      "step": 1220400
    },
    {
      "epoch": 47.20191824264222,
      "grad_norm": 15.594858169555664,
      "learning_rate": 1.066506813113148e-05,
      "loss": 1.8361,
      "step": 1220500
    },
    {
      "epoch": 47.20578566732413,
      "grad_norm": 12.9356107711792,
      "learning_rate": 1.0661845277229892e-05,
      "loss": 1.831,
      "step": 1220600
    },
    {
      "epoch": 47.209653092006036,
      "grad_norm": 13.058136940002441,
      "learning_rate": 1.0658622423328305e-05,
      "loss": 1.8342,
      "step": 1220700
    },
    {
      "epoch": 47.213520516687936,
      "grad_norm": 13.444475173950195,
      "learning_rate": 1.0655399569426718e-05,
      "loss": 1.7892,
      "step": 1220800
    },
    {
      "epoch": 47.21738794136984,
      "grad_norm": 13.46061897277832,
      "learning_rate": 1.0652176715525131e-05,
      "loss": 1.736,
      "step": 1220900
    },
    {
      "epoch": 47.22125536605174,
      "grad_norm": 13.06018352508545,
      "learning_rate": 1.0648953861623544e-05,
      "loss": 1.7847,
      "step": 1221000
    },
    {
      "epoch": 47.22512279073365,
      "grad_norm": 13.43540096282959,
      "learning_rate": 1.0645731007721957e-05,
      "loss": 1.7204,
      "step": 1221100
    },
    {
      "epoch": 47.22899021541556,
      "grad_norm": 13.111742973327637,
      "learning_rate": 1.0642508153820372e-05,
      "loss": 1.7519,
      "step": 1221200
    },
    {
      "epoch": 47.23285764009746,
      "grad_norm": 12.657264709472656,
      "learning_rate": 1.0639285299918785e-05,
      "loss": 1.7852,
      "step": 1221300
    },
    {
      "epoch": 47.236725064779364,
      "grad_norm": 10.53004264831543,
      "learning_rate": 1.0636062446017198e-05,
      "loss": 1.8521,
      "step": 1221400
    },
    {
      "epoch": 47.24059248946127,
      "grad_norm": 14.138162612915039,
      "learning_rate": 1.0632839592115611e-05,
      "loss": 1.7775,
      "step": 1221500
    },
    {
      "epoch": 47.24445991414317,
      "grad_norm": 9.54671859741211,
      "learning_rate": 1.0629616738214024e-05,
      "loss": 1.7972,
      "step": 1221600
    },
    {
      "epoch": 47.24832733882508,
      "grad_norm": 9.895380973815918,
      "learning_rate": 1.0626393884312437e-05,
      "loss": 1.7617,
      "step": 1221700
    },
    {
      "epoch": 47.25219476350698,
      "grad_norm": 14.863643646240234,
      "learning_rate": 1.062317103041085e-05,
      "loss": 1.7236,
      "step": 1221800
    },
    {
      "epoch": 47.256062188188885,
      "grad_norm": 11.264704704284668,
      "learning_rate": 1.0619948176509263e-05,
      "loss": 1.834,
      "step": 1221900
    },
    {
      "epoch": 47.25992961287079,
      "grad_norm": 13.842032432556152,
      "learning_rate": 1.0616725322607676e-05,
      "loss": 1.896,
      "step": 1222000
    },
    {
      "epoch": 47.26379703755269,
      "grad_norm": 14.614699363708496,
      "learning_rate": 1.061350246870609e-05,
      "loss": 1.8169,
      "step": 1222100
    },
    {
      "epoch": 47.2676644622346,
      "grad_norm": 12.65086555480957,
      "learning_rate": 1.0610279614804502e-05,
      "loss": 1.7417,
      "step": 1222200
    },
    {
      "epoch": 47.2715318869165,
      "grad_norm": 14.861093521118164,
      "learning_rate": 1.0607056760902915e-05,
      "loss": 1.7755,
      "step": 1222300
    },
    {
      "epoch": 47.275399311598406,
      "grad_norm": 10.787195205688477,
      "learning_rate": 1.0603833907001328e-05,
      "loss": 1.7287,
      "step": 1222400
    },
    {
      "epoch": 47.27926673628031,
      "grad_norm": 11.4895658493042,
      "learning_rate": 1.0600611053099741e-05,
      "loss": 1.8132,
      "step": 1222500
    },
    {
      "epoch": 47.28313416096221,
      "grad_norm": 11.641016006469727,
      "learning_rate": 1.0597388199198154e-05,
      "loss": 1.7912,
      "step": 1222600
    },
    {
      "epoch": 47.28700158564412,
      "grad_norm": 11.979984283447266,
      "learning_rate": 1.0594165345296568e-05,
      "loss": 1.7759,
      "step": 1222700
    },
    {
      "epoch": 47.29086901032603,
      "grad_norm": 11.447826385498047,
      "learning_rate": 1.059094249139498e-05,
      "loss": 1.8982,
      "step": 1222800
    },
    {
      "epoch": 47.29473643500793,
      "grad_norm": 12.808453559875488,
      "learning_rate": 1.0587719637493394e-05,
      "loss": 1.8188,
      "step": 1222900
    },
    {
      "epoch": 47.298603859689834,
      "grad_norm": 11.201959609985352,
      "learning_rate": 1.0584496783591807e-05,
      "loss": 1.7355,
      "step": 1223000
    },
    {
      "epoch": 47.302471284371734,
      "grad_norm": 10.429190635681152,
      "learning_rate": 1.058127392969022e-05,
      "loss": 1.7713,
      "step": 1223100
    },
    {
      "epoch": 47.30633870905364,
      "grad_norm": 13.60303783416748,
      "learning_rate": 1.0578051075788633e-05,
      "loss": 1.8451,
      "step": 1223200
    },
    {
      "epoch": 47.31020613373555,
      "grad_norm": 17.496042251586914,
      "learning_rate": 1.0574828221887046e-05,
      "loss": 1.7806,
      "step": 1223300
    },
    {
      "epoch": 47.31407355841745,
      "grad_norm": 10.84677791595459,
      "learning_rate": 1.0571605367985459e-05,
      "loss": 1.8364,
      "step": 1223400
    },
    {
      "epoch": 47.317940983099355,
      "grad_norm": 14.11636734008789,
      "learning_rate": 1.0568382514083872e-05,
      "loss": 1.7553,
      "step": 1223500
    },
    {
      "epoch": 47.32180840778126,
      "grad_norm": 19.0327205657959,
      "learning_rate": 1.0565159660182285e-05,
      "loss": 1.809,
      "step": 1223600
    },
    {
      "epoch": 47.32567583246316,
      "grad_norm": 10.684535026550293,
      "learning_rate": 1.0561936806280698e-05,
      "loss": 1.8602,
      "step": 1223700
    },
    {
      "epoch": 47.32954325714507,
      "grad_norm": 12.439774513244629,
      "learning_rate": 1.055871395237911e-05,
      "loss": 1.8038,
      "step": 1223800
    },
    {
      "epoch": 47.33341068182697,
      "grad_norm": 14.69869613647461,
      "learning_rate": 1.0555491098477524e-05,
      "loss": 1.8394,
      "step": 1223900
    },
    {
      "epoch": 47.337278106508876,
      "grad_norm": 9.790060043334961,
      "learning_rate": 1.0552268244575937e-05,
      "loss": 1.8497,
      "step": 1224000
    },
    {
      "epoch": 47.34114553119078,
      "grad_norm": 9.996980667114258,
      "learning_rate": 1.054904539067435e-05,
      "loss": 1.7847,
      "step": 1224100
    },
    {
      "epoch": 47.34501295587268,
      "grad_norm": 14.09771728515625,
      "learning_rate": 1.0545822536772763e-05,
      "loss": 1.8211,
      "step": 1224200
    },
    {
      "epoch": 47.34888038055459,
      "grad_norm": 13.557146072387695,
      "learning_rate": 1.0542599682871176e-05,
      "loss": 1.8396,
      "step": 1224300
    },
    {
      "epoch": 47.35274780523649,
      "grad_norm": 16.54916763305664,
      "learning_rate": 1.0539376828969589e-05,
      "loss": 1.7709,
      "step": 1224400
    },
    {
      "epoch": 47.3566152299184,
      "grad_norm": 13.945084571838379,
      "learning_rate": 1.0536153975068002e-05,
      "loss": 1.7593,
      "step": 1224500
    },
    {
      "epoch": 47.360482654600304,
      "grad_norm": 9.255314826965332,
      "learning_rate": 1.0532931121166417e-05,
      "loss": 1.8,
      "step": 1224600
    },
    {
      "epoch": 47.364350079282204,
      "grad_norm": 15.307751655578613,
      "learning_rate": 1.052970826726483e-05,
      "loss": 1.8894,
      "step": 1224700
    },
    {
      "epoch": 47.36821750396411,
      "grad_norm": 17.225603103637695,
      "learning_rate": 1.0526485413363243e-05,
      "loss": 1.7241,
      "step": 1224800
    },
    {
      "epoch": 47.37208492864602,
      "grad_norm": 9.549073219299316,
      "learning_rate": 1.0523262559461656e-05,
      "loss": 1.7131,
      "step": 1224900
    },
    {
      "epoch": 47.37595235332792,
      "grad_norm": 9.859599113464355,
      "learning_rate": 1.0520039705560069e-05,
      "loss": 1.8658,
      "step": 1225000
    },
    {
      "epoch": 47.379819778009825,
      "grad_norm": 11.543508529663086,
      "learning_rate": 1.0516816851658482e-05,
      "loss": 1.8796,
      "step": 1225100
    },
    {
      "epoch": 47.383687202691725,
      "grad_norm": 8.60417366027832,
      "learning_rate": 1.0513593997756895e-05,
      "loss": 1.7697,
      "step": 1225200
    },
    {
      "epoch": 47.38755462737363,
      "grad_norm": 14.0132417678833,
      "learning_rate": 1.0510371143855308e-05,
      "loss": 1.8363,
      "step": 1225300
    },
    {
      "epoch": 47.39142205205554,
      "grad_norm": 12.375069618225098,
      "learning_rate": 1.0507148289953721e-05,
      "loss": 1.8487,
      "step": 1225400
    },
    {
      "epoch": 47.39528947673744,
      "grad_norm": 14.148584365844727,
      "learning_rate": 1.0503925436052134e-05,
      "loss": 1.8236,
      "step": 1225500
    },
    {
      "epoch": 47.399156901419346,
      "grad_norm": 12.666643142700195,
      "learning_rate": 1.0500702582150547e-05,
      "loss": 1.8546,
      "step": 1225600
    },
    {
      "epoch": 47.403024326101246,
      "grad_norm": 11.39398193359375,
      "learning_rate": 1.049747972824896e-05,
      "loss": 1.7872,
      "step": 1225700
    },
    {
      "epoch": 47.40689175078315,
      "grad_norm": 14.473946571350098,
      "learning_rate": 1.0494256874347373e-05,
      "loss": 1.9165,
      "step": 1225800
    },
    {
      "epoch": 47.41075917546506,
      "grad_norm": 9.750676155090332,
      "learning_rate": 1.0491034020445786e-05,
      "loss": 1.7539,
      "step": 1225900
    },
    {
      "epoch": 47.41462660014696,
      "grad_norm": 11.576562881469727,
      "learning_rate": 1.0487811166544197e-05,
      "loss": 1.8413,
      "step": 1226000
    },
    {
      "epoch": 47.41849402482887,
      "grad_norm": 15.94506549835205,
      "learning_rate": 1.048458831264261e-05,
      "loss": 1.8314,
      "step": 1226100
    },
    {
      "epoch": 47.422361449510774,
      "grad_norm": 10.916739463806152,
      "learning_rate": 1.0481365458741025e-05,
      "loss": 1.791,
      "step": 1226200
    },
    {
      "epoch": 47.426228874192674,
      "grad_norm": 13.456401824951172,
      "learning_rate": 1.0478142604839438e-05,
      "loss": 1.8044,
      "step": 1226300
    },
    {
      "epoch": 47.43009629887458,
      "grad_norm": 5.058194160461426,
      "learning_rate": 1.0474919750937851e-05,
      "loss": 1.7675,
      "step": 1226400
    },
    {
      "epoch": 47.43396372355648,
      "grad_norm": 11.175971031188965,
      "learning_rate": 1.0471696897036264e-05,
      "loss": 1.7678,
      "step": 1226500
    },
    {
      "epoch": 47.43783114823839,
      "grad_norm": 11.791515350341797,
      "learning_rate": 1.0468474043134677e-05,
      "loss": 1.8948,
      "step": 1226600
    },
    {
      "epoch": 47.441698572920295,
      "grad_norm": 12.335850715637207,
      "learning_rate": 1.046525118923309e-05,
      "loss": 1.89,
      "step": 1226700
    },
    {
      "epoch": 47.445565997602195,
      "grad_norm": 11.110898971557617,
      "learning_rate": 1.0462028335331503e-05,
      "loss": 1.8197,
      "step": 1226800
    },
    {
      "epoch": 47.4494334222841,
      "grad_norm": 15.294926643371582,
      "learning_rate": 1.0458805481429916e-05,
      "loss": 1.8442,
      "step": 1226900
    },
    {
      "epoch": 47.453300846966,
      "grad_norm": 11.565558433532715,
      "learning_rate": 1.045558262752833e-05,
      "loss": 1.7873,
      "step": 1227000
    },
    {
      "epoch": 47.45716827164791,
      "grad_norm": 13.913330078125,
      "learning_rate": 1.0452359773626742e-05,
      "loss": 1.8257,
      "step": 1227100
    },
    {
      "epoch": 47.461035696329816,
      "grad_norm": 14.459184646606445,
      "learning_rate": 1.0449136919725155e-05,
      "loss": 1.8144,
      "step": 1227200
    },
    {
      "epoch": 47.464903121011716,
      "grad_norm": 9.007927894592285,
      "learning_rate": 1.0445914065823568e-05,
      "loss": 1.7471,
      "step": 1227300
    },
    {
      "epoch": 47.46877054569362,
      "grad_norm": 13.973496437072754,
      "learning_rate": 1.0442691211921981e-05,
      "loss": 1.7328,
      "step": 1227400
    },
    {
      "epoch": 47.47263797037553,
      "grad_norm": 14.138589859008789,
      "learning_rate": 1.0439468358020394e-05,
      "loss": 1.7984,
      "step": 1227500
    },
    {
      "epoch": 47.47650539505743,
      "grad_norm": 14.772491455078125,
      "learning_rate": 1.0436245504118807e-05,
      "loss": 1.7375,
      "step": 1227600
    },
    {
      "epoch": 47.48037281973934,
      "grad_norm": 9.967816352844238,
      "learning_rate": 1.043302265021722e-05,
      "loss": 1.8491,
      "step": 1227700
    },
    {
      "epoch": 47.48424024442124,
      "grad_norm": 13.420494079589844,
      "learning_rate": 1.0429799796315633e-05,
      "loss": 1.7258,
      "step": 1227800
    },
    {
      "epoch": 47.488107669103144,
      "grad_norm": 10.862537384033203,
      "learning_rate": 1.0426576942414046e-05,
      "loss": 1.7794,
      "step": 1227900
    },
    {
      "epoch": 47.49197509378505,
      "grad_norm": 10.432872772216797,
      "learning_rate": 1.042335408851246e-05,
      "loss": 1.874,
      "step": 1228000
    },
    {
      "epoch": 47.49584251846695,
      "grad_norm": 12.631539344787598,
      "learning_rate": 1.0420131234610874e-05,
      "loss": 1.7818,
      "step": 1228100
    },
    {
      "epoch": 47.49970994314886,
      "grad_norm": 12.341389656066895,
      "learning_rate": 1.0416908380709287e-05,
      "loss": 1.7186,
      "step": 1228200
    },
    {
      "epoch": 47.503577367830765,
      "grad_norm": 12.780989646911621,
      "learning_rate": 1.04136855268077e-05,
      "loss": 1.8254,
      "step": 1228300
    },
    {
      "epoch": 47.507444792512665,
      "grad_norm": 10.033854484558105,
      "learning_rate": 1.0410462672906113e-05,
      "loss": 1.7322,
      "step": 1228400
    },
    {
      "epoch": 47.51131221719457,
      "grad_norm": 11.367734909057617,
      "learning_rate": 1.0407239819004526e-05,
      "loss": 1.8458,
      "step": 1228500
    },
    {
      "epoch": 47.51517964187647,
      "grad_norm": 15.450115203857422,
      "learning_rate": 1.040401696510294e-05,
      "loss": 1.8409,
      "step": 1228600
    },
    {
      "epoch": 47.51904706655838,
      "grad_norm": 11.339730262756348,
      "learning_rate": 1.0400794111201352e-05,
      "loss": 1.8043,
      "step": 1228700
    },
    {
      "epoch": 47.522914491240286,
      "grad_norm": 11.938776016235352,
      "learning_rate": 1.0397571257299765e-05,
      "loss": 1.7535,
      "step": 1228800
    },
    {
      "epoch": 47.526781915922186,
      "grad_norm": 12.462688446044922,
      "learning_rate": 1.0394348403398178e-05,
      "loss": 1.8372,
      "step": 1228900
    },
    {
      "epoch": 47.53064934060409,
      "grad_norm": 18.14858627319336,
      "learning_rate": 1.039112554949659e-05,
      "loss": 1.7747,
      "step": 1229000
    },
    {
      "epoch": 47.53451676528599,
      "grad_norm": 16.849197387695312,
      "learning_rate": 1.0387902695595003e-05,
      "loss": 1.7237,
      "step": 1229100
    },
    {
      "epoch": 47.5383841899679,
      "grad_norm": 13.454935073852539,
      "learning_rate": 1.0384679841693416e-05,
      "loss": 1.8303,
      "step": 1229200
    },
    {
      "epoch": 47.54225161464981,
      "grad_norm": 14.387014389038086,
      "learning_rate": 1.0381456987791829e-05,
      "loss": 1.8091,
      "step": 1229300
    },
    {
      "epoch": 47.54611903933171,
      "grad_norm": 12.270694732666016,
      "learning_rate": 1.0378234133890242e-05,
      "loss": 1.7674,
      "step": 1229400
    },
    {
      "epoch": 47.549986464013614,
      "grad_norm": 10.723143577575684,
      "learning_rate": 1.0375011279988655e-05,
      "loss": 1.8094,
      "step": 1229500
    },
    {
      "epoch": 47.55385388869552,
      "grad_norm": 11.840978622436523,
      "learning_rate": 1.037178842608707e-05,
      "loss": 1.7789,
      "step": 1229600
    },
    {
      "epoch": 47.55772131337742,
      "grad_norm": 10.874082565307617,
      "learning_rate": 1.0368565572185483e-05,
      "loss": 1.7697,
      "step": 1229700
    },
    {
      "epoch": 47.56158873805933,
      "grad_norm": 10.502687454223633,
      "learning_rate": 1.0365342718283896e-05,
      "loss": 1.8088,
      "step": 1229800
    },
    {
      "epoch": 47.56545616274123,
      "grad_norm": 13.11668586730957,
      "learning_rate": 1.0362119864382309e-05,
      "loss": 1.674,
      "step": 1229900
    },
    {
      "epoch": 47.569323587423135,
      "grad_norm": 8.839738845825195,
      "learning_rate": 1.0358897010480722e-05,
      "loss": 1.7993,
      "step": 1230000
    },
    {
      "epoch": 47.57319101210504,
      "grad_norm": 14.236452102661133,
      "learning_rate": 1.0355674156579135e-05,
      "loss": 1.8155,
      "step": 1230100
    },
    {
      "epoch": 47.57705843678694,
      "grad_norm": 11.409746170043945,
      "learning_rate": 1.0352451302677548e-05,
      "loss": 1.7415,
      "step": 1230200
    },
    {
      "epoch": 47.58092586146885,
      "grad_norm": 10.353029251098633,
      "learning_rate": 1.034922844877596e-05,
      "loss": 1.7733,
      "step": 1230300
    },
    {
      "epoch": 47.58479328615075,
      "grad_norm": 13.652969360351562,
      "learning_rate": 1.0346005594874374e-05,
      "loss": 1.7107,
      "step": 1230400
    },
    {
      "epoch": 47.588660710832656,
      "grad_norm": 11.48081111907959,
      "learning_rate": 1.0342782740972787e-05,
      "loss": 1.7809,
      "step": 1230500
    },
    {
      "epoch": 47.59252813551456,
      "grad_norm": 12.529935836791992,
      "learning_rate": 1.03395598870712e-05,
      "loss": 1.8,
      "step": 1230600
    },
    {
      "epoch": 47.59639556019646,
      "grad_norm": 16.411388397216797,
      "learning_rate": 1.0336337033169613e-05,
      "loss": 1.7963,
      "step": 1230700
    },
    {
      "epoch": 47.60026298487837,
      "grad_norm": 13.190692901611328,
      "learning_rate": 1.0333114179268026e-05,
      "loss": 1.7871,
      "step": 1230800
    },
    {
      "epoch": 47.60413040956028,
      "grad_norm": 10.203371047973633,
      "learning_rate": 1.0329891325366439e-05,
      "loss": 1.8307,
      "step": 1230900
    },
    {
      "epoch": 47.60799783424218,
      "grad_norm": 10.195281982421875,
      "learning_rate": 1.0326668471464852e-05,
      "loss": 1.8182,
      "step": 1231000
    },
    {
      "epoch": 47.611865258924084,
      "grad_norm": 13.301011085510254,
      "learning_rate": 1.0323445617563265e-05,
      "loss": 1.8921,
      "step": 1231100
    },
    {
      "epoch": 47.615732683605984,
      "grad_norm": 16.071260452270508,
      "learning_rate": 1.0320222763661678e-05,
      "loss": 1.8575,
      "step": 1231200
    },
    {
      "epoch": 47.61960010828789,
      "grad_norm": 12.765645980834961,
      "learning_rate": 1.0316999909760091e-05,
      "loss": 1.7999,
      "step": 1231300
    },
    {
      "epoch": 47.6234675329698,
      "grad_norm": 17.615467071533203,
      "learning_rate": 1.0313777055858504e-05,
      "loss": 1.8368,
      "step": 1231400
    },
    {
      "epoch": 47.6273349576517,
      "grad_norm": 13.703268051147461,
      "learning_rate": 1.0310554201956919e-05,
      "loss": 1.7708,
      "step": 1231500
    },
    {
      "epoch": 47.631202382333605,
      "grad_norm": 12.258861541748047,
      "learning_rate": 1.0307331348055332e-05,
      "loss": 1.8084,
      "step": 1231600
    },
    {
      "epoch": 47.63506980701551,
      "grad_norm": 11.997278213500977,
      "learning_rate": 1.0304108494153745e-05,
      "loss": 1.7591,
      "step": 1231700
    },
    {
      "epoch": 47.63893723169741,
      "grad_norm": 12.722880363464355,
      "learning_rate": 1.0300885640252158e-05,
      "loss": 1.8293,
      "step": 1231800
    },
    {
      "epoch": 47.64280465637932,
      "grad_norm": 12.582324981689453,
      "learning_rate": 1.0297662786350569e-05,
      "loss": 1.8093,
      "step": 1231900
    },
    {
      "epoch": 47.64667208106122,
      "grad_norm": 10.61755084991455,
      "learning_rate": 1.0294439932448982e-05,
      "loss": 1.8643,
      "step": 1232000
    },
    {
      "epoch": 47.650539505743126,
      "grad_norm": 10.781876564025879,
      "learning_rate": 1.0291217078547395e-05,
      "loss": 1.746,
      "step": 1232100
    },
    {
      "epoch": 47.65440693042503,
      "grad_norm": 11.68605899810791,
      "learning_rate": 1.0287994224645808e-05,
      "loss": 1.711,
      "step": 1232200
    },
    {
      "epoch": 47.65827435510693,
      "grad_norm": 14.330204963684082,
      "learning_rate": 1.0284771370744221e-05,
      "loss": 1.8277,
      "step": 1232300
    },
    {
      "epoch": 47.66214177978884,
      "grad_norm": 12.683605194091797,
      "learning_rate": 1.0281548516842634e-05,
      "loss": 1.663,
      "step": 1232400
    },
    {
      "epoch": 47.66600920447074,
      "grad_norm": 14.597057342529297,
      "learning_rate": 1.0278325662941047e-05,
      "loss": 1.8754,
      "step": 1232500
    },
    {
      "epoch": 47.66987662915265,
      "grad_norm": 11.692927360534668,
      "learning_rate": 1.027510280903946e-05,
      "loss": 1.8009,
      "step": 1232600
    },
    {
      "epoch": 47.673744053834554,
      "grad_norm": 12.052042007446289,
      "learning_rate": 1.0271879955137873e-05,
      "loss": 1.9231,
      "step": 1232700
    },
    {
      "epoch": 47.677611478516454,
      "grad_norm": 12.855855941772461,
      "learning_rate": 1.0268657101236286e-05,
      "loss": 1.7435,
      "step": 1232800
    },
    {
      "epoch": 47.68147890319836,
      "grad_norm": 12.028074264526367,
      "learning_rate": 1.02654342473347e-05,
      "loss": 1.7797,
      "step": 1232900
    },
    {
      "epoch": 47.68534632788027,
      "grad_norm": 13.928186416625977,
      "learning_rate": 1.0262211393433112e-05,
      "loss": 1.8661,
      "step": 1233000
    },
    {
      "epoch": 47.68921375256217,
      "grad_norm": 14.200697898864746,
      "learning_rate": 1.0258988539531527e-05,
      "loss": 1.8096,
      "step": 1233100
    },
    {
      "epoch": 47.693081177244075,
      "grad_norm": 11.479504585266113,
      "learning_rate": 1.025576568562994e-05,
      "loss": 1.7909,
      "step": 1233200
    },
    {
      "epoch": 47.696948601925975,
      "grad_norm": 11.818619728088379,
      "learning_rate": 1.0252542831728353e-05,
      "loss": 1.8187,
      "step": 1233300
    },
    {
      "epoch": 47.70081602660788,
      "grad_norm": 10.671957015991211,
      "learning_rate": 1.0249319977826766e-05,
      "loss": 1.8159,
      "step": 1233400
    },
    {
      "epoch": 47.70468345128979,
      "grad_norm": 13.351912498474121,
      "learning_rate": 1.024609712392518e-05,
      "loss": 1.7977,
      "step": 1233500
    },
    {
      "epoch": 47.70855087597169,
      "grad_norm": 14.137975692749023,
      "learning_rate": 1.0242874270023592e-05,
      "loss": 1.7611,
      "step": 1233600
    },
    {
      "epoch": 47.712418300653596,
      "grad_norm": 11.207466125488281,
      "learning_rate": 1.0239651416122005e-05,
      "loss": 1.7664,
      "step": 1233700
    },
    {
      "epoch": 47.716285725335496,
      "grad_norm": 13.40636920928955,
      "learning_rate": 1.0236428562220418e-05,
      "loss": 1.8019,
      "step": 1233800
    },
    {
      "epoch": 47.7201531500174,
      "grad_norm": 16.342151641845703,
      "learning_rate": 1.0233205708318831e-05,
      "loss": 1.8107,
      "step": 1233900
    },
    {
      "epoch": 47.72402057469931,
      "grad_norm": 11.828853607177734,
      "learning_rate": 1.0229982854417244e-05,
      "loss": 1.7399,
      "step": 1234000
    },
    {
      "epoch": 47.72788799938121,
      "grad_norm": 13.560200691223145,
      "learning_rate": 1.0226760000515657e-05,
      "loss": 1.7801,
      "step": 1234100
    },
    {
      "epoch": 47.73175542406312,
      "grad_norm": 14.761399269104004,
      "learning_rate": 1.022353714661407e-05,
      "loss": 1.846,
      "step": 1234200
    },
    {
      "epoch": 47.735622848745024,
      "grad_norm": 13.041677474975586,
      "learning_rate": 1.0220314292712483e-05,
      "loss": 1.7096,
      "step": 1234300
    },
    {
      "epoch": 47.739490273426924,
      "grad_norm": 15.014851570129395,
      "learning_rate": 1.0217091438810896e-05,
      "loss": 1.7939,
      "step": 1234400
    },
    {
      "epoch": 47.74335769810883,
      "grad_norm": 10.652520179748535,
      "learning_rate": 1.021386858490931e-05,
      "loss": 1.8602,
      "step": 1234500
    },
    {
      "epoch": 47.74722512279073,
      "grad_norm": 13.661158561706543,
      "learning_rate": 1.0210645731007722e-05,
      "loss": 1.7502,
      "step": 1234600
    },
    {
      "epoch": 47.75109254747264,
      "grad_norm": 11.34692668914795,
      "learning_rate": 1.0207422877106136e-05,
      "loss": 1.7719,
      "step": 1234700
    },
    {
      "epoch": 47.754959972154545,
      "grad_norm": 10.344103813171387,
      "learning_rate": 1.0204200023204549e-05,
      "loss": 1.9185,
      "step": 1234800
    },
    {
      "epoch": 47.758827396836445,
      "grad_norm": 16.742290496826172,
      "learning_rate": 1.0200977169302962e-05,
      "loss": 1.7145,
      "step": 1234900
    },
    {
      "epoch": 47.76269482151835,
      "grad_norm": 14.145142555236816,
      "learning_rate": 1.0197754315401375e-05,
      "loss": 1.8551,
      "step": 1235000
    },
    {
      "epoch": 47.76656224620025,
      "grad_norm": 13.976202011108398,
      "learning_rate": 1.0194531461499788e-05,
      "loss": 1.8646,
      "step": 1235100
    },
    {
      "epoch": 47.77042967088216,
      "grad_norm": 11.02155876159668,
      "learning_rate": 1.01913086075982e-05,
      "loss": 1.8267,
      "step": 1235200
    },
    {
      "epoch": 47.774297095564066,
      "grad_norm": 10.84408950805664,
      "learning_rate": 1.0188085753696614e-05,
      "loss": 1.7803,
      "step": 1235300
    },
    {
      "epoch": 47.778164520245966,
      "grad_norm": 11.527710914611816,
      "learning_rate": 1.0184862899795027e-05,
      "loss": 1.7263,
      "step": 1235400
    },
    {
      "epoch": 47.78203194492787,
      "grad_norm": 13.862360000610352,
      "learning_rate": 1.018164004589344e-05,
      "loss": 1.8252,
      "step": 1235500
    },
    {
      "epoch": 47.78589936960978,
      "grad_norm": 9.91586685180664,
      "learning_rate": 1.0178417191991853e-05,
      "loss": 1.7772,
      "step": 1235600
    },
    {
      "epoch": 47.78976679429168,
      "grad_norm": 17.774398803710938,
      "learning_rate": 1.0175194338090266e-05,
      "loss": 1.8321,
      "step": 1235700
    },
    {
      "epoch": 47.79363421897359,
      "grad_norm": 14.495020866394043,
      "learning_rate": 1.0171971484188679e-05,
      "loss": 1.752,
      "step": 1235800
    },
    {
      "epoch": 47.79750164365549,
      "grad_norm": 14.343524932861328,
      "learning_rate": 1.0168748630287092e-05,
      "loss": 1.7873,
      "step": 1235900
    },
    {
      "epoch": 47.801369068337394,
      "grad_norm": 10.939279556274414,
      "learning_rate": 1.0165525776385505e-05,
      "loss": 1.723,
      "step": 1236000
    },
    {
      "epoch": 47.8052364930193,
      "grad_norm": 12.112597465515137,
      "learning_rate": 1.0162302922483918e-05,
      "loss": 1.8472,
      "step": 1236100
    },
    {
      "epoch": 47.8091039177012,
      "grad_norm": 13.11456298828125,
      "learning_rate": 1.0159080068582331e-05,
      "loss": 1.9401,
      "step": 1236200
    },
    {
      "epoch": 47.81297134238311,
      "grad_norm": 13.331735610961914,
      "learning_rate": 1.0155857214680744e-05,
      "loss": 1.8315,
      "step": 1236300
    },
    {
      "epoch": 47.816838767065015,
      "grad_norm": 13.542386054992676,
      "learning_rate": 1.0152634360779157e-05,
      "loss": 1.6838,
      "step": 1236400
    },
    {
      "epoch": 47.820706191746915,
      "grad_norm": 11.872279167175293,
      "learning_rate": 1.014941150687757e-05,
      "loss": 1.7915,
      "step": 1236500
    },
    {
      "epoch": 47.82457361642882,
      "grad_norm": 11.414807319641113,
      "learning_rate": 1.0146188652975985e-05,
      "loss": 1.8231,
      "step": 1236600
    },
    {
      "epoch": 47.82844104111072,
      "grad_norm": 11.904074668884277,
      "learning_rate": 1.0142965799074398e-05,
      "loss": 1.7661,
      "step": 1236700
    },
    {
      "epoch": 47.83230846579263,
      "grad_norm": 9.430488586425781,
      "learning_rate": 1.013974294517281e-05,
      "loss": 1.7913,
      "step": 1236800
    },
    {
      "epoch": 47.836175890474536,
      "grad_norm": 12.673162460327148,
      "learning_rate": 1.0136520091271224e-05,
      "loss": 1.7075,
      "step": 1236900
    },
    {
      "epoch": 47.840043315156436,
      "grad_norm": 10.285340309143066,
      "learning_rate": 1.0133297237369637e-05,
      "loss": 1.8238,
      "step": 1237000
    },
    {
      "epoch": 47.84391073983834,
      "grad_norm": 11.971427917480469,
      "learning_rate": 1.013007438346805e-05,
      "loss": 1.828,
      "step": 1237100
    },
    {
      "epoch": 47.84777816452024,
      "grad_norm": 9.809149742126465,
      "learning_rate": 1.0126851529566463e-05,
      "loss": 1.7901,
      "step": 1237200
    },
    {
      "epoch": 47.85164558920215,
      "grad_norm": 8.88683032989502,
      "learning_rate": 1.0123628675664876e-05,
      "loss": 1.7898,
      "step": 1237300
    },
    {
      "epoch": 47.85551301388406,
      "grad_norm": 10.309749603271484,
      "learning_rate": 1.0120405821763289e-05,
      "loss": 1.8378,
      "step": 1237400
    },
    {
      "epoch": 47.85938043856596,
      "grad_norm": 14.078829765319824,
      "learning_rate": 1.0117182967861702e-05,
      "loss": 1.9067,
      "step": 1237500
    },
    {
      "epoch": 47.863247863247864,
      "grad_norm": 11.901542663574219,
      "learning_rate": 1.0113960113960115e-05,
      "loss": 1.878,
      "step": 1237600
    },
    {
      "epoch": 47.86711528792977,
      "grad_norm": 12.697943687438965,
      "learning_rate": 1.0110737260058528e-05,
      "loss": 1.7602,
      "step": 1237700
    },
    {
      "epoch": 47.87098271261167,
      "grad_norm": 13.340970039367676,
      "learning_rate": 1.010751440615694e-05,
      "loss": 1.8455,
      "step": 1237800
    },
    {
      "epoch": 47.87485013729358,
      "grad_norm": 13.134389877319336,
      "learning_rate": 1.0104291552255352e-05,
      "loss": 1.7805,
      "step": 1237900
    },
    {
      "epoch": 47.87871756197548,
      "grad_norm": 9.506179809570312,
      "learning_rate": 1.0101068698353765e-05,
      "loss": 1.8028,
      "step": 1238000
    },
    {
      "epoch": 47.882584986657385,
      "grad_norm": 10.51587200164795,
      "learning_rate": 1.009784584445218e-05,
      "loss": 1.7441,
      "step": 1238100
    },
    {
      "epoch": 47.88645241133929,
      "grad_norm": 13.233543395996094,
      "learning_rate": 1.0094622990550593e-05,
      "loss": 1.8224,
      "step": 1238200
    },
    {
      "epoch": 47.89031983602119,
      "grad_norm": 13.958904266357422,
      "learning_rate": 1.0091400136649006e-05,
      "loss": 1.8302,
      "step": 1238300
    },
    {
      "epoch": 47.8941872607031,
      "grad_norm": 12.332637786865234,
      "learning_rate": 1.0088177282747419e-05,
      "loss": 1.8084,
      "step": 1238400
    },
    {
      "epoch": 47.898054685385,
      "grad_norm": 13.357828140258789,
      "learning_rate": 1.0084954428845832e-05,
      "loss": 1.8347,
      "step": 1238500
    },
    {
      "epoch": 47.901922110066906,
      "grad_norm": 10.07856273651123,
      "learning_rate": 1.0081731574944245e-05,
      "loss": 1.8186,
      "step": 1238600
    },
    {
      "epoch": 47.90578953474881,
      "grad_norm": 14.065592765808105,
      "learning_rate": 1.0078508721042658e-05,
      "loss": 1.8045,
      "step": 1238700
    },
    {
      "epoch": 47.90965695943071,
      "grad_norm": 12.37227725982666,
      "learning_rate": 1.0075285867141071e-05,
      "loss": 1.7951,
      "step": 1238800
    },
    {
      "epoch": 47.91352438411262,
      "grad_norm": 10.261137008666992,
      "learning_rate": 1.0072063013239484e-05,
      "loss": 1.8278,
      "step": 1238900
    },
    {
      "epoch": 47.91739180879453,
      "grad_norm": 14.75922966003418,
      "learning_rate": 1.0068840159337897e-05,
      "loss": 1.7145,
      "step": 1239000
    },
    {
      "epoch": 47.92125923347643,
      "grad_norm": 14.450835227966309,
      "learning_rate": 1.006561730543631e-05,
      "loss": 1.7762,
      "step": 1239100
    },
    {
      "epoch": 47.925126658158334,
      "grad_norm": 13.894057273864746,
      "learning_rate": 1.0062394451534723e-05,
      "loss": 1.8156,
      "step": 1239200
    },
    {
      "epoch": 47.928994082840234,
      "grad_norm": 11.128717422485352,
      "learning_rate": 1.0059171597633136e-05,
      "loss": 1.7793,
      "step": 1239300
    },
    {
      "epoch": 47.93286150752214,
      "grad_norm": 9.376274108886719,
      "learning_rate": 1.005594874373155e-05,
      "loss": 1.8913,
      "step": 1239400
    },
    {
      "epoch": 47.93672893220405,
      "grad_norm": 13.40764331817627,
      "learning_rate": 1.0052725889829962e-05,
      "loss": 1.8051,
      "step": 1239500
    },
    {
      "epoch": 47.94059635688595,
      "grad_norm": 12.253993034362793,
      "learning_rate": 1.0049503035928375e-05,
      "loss": 1.8165,
      "step": 1239600
    },
    {
      "epoch": 47.944463781567855,
      "grad_norm": 12.519451141357422,
      "learning_rate": 1.0046280182026788e-05,
      "loss": 1.7521,
      "step": 1239700
    },
    {
      "epoch": 47.94833120624976,
      "grad_norm": 10.494441032409668,
      "learning_rate": 1.0043057328125201e-05,
      "loss": 1.7789,
      "step": 1239800
    },
    {
      "epoch": 47.95219863093166,
      "grad_norm": 16.455995559692383,
      "learning_rate": 1.0039834474223614e-05,
      "loss": 1.7867,
      "step": 1239900
    },
    {
      "epoch": 47.95606605561357,
      "grad_norm": 16.201908111572266,
      "learning_rate": 1.003661162032203e-05,
      "loss": 1.778,
      "step": 1240000
    },
    {
      "epoch": 47.95993348029547,
      "grad_norm": 13.04900074005127,
      "learning_rate": 1.0033388766420442e-05,
      "loss": 1.8247,
      "step": 1240100
    },
    {
      "epoch": 47.963800904977376,
      "grad_norm": 14.54969596862793,
      "learning_rate": 1.0030165912518855e-05,
      "loss": 1.7803,
      "step": 1240200
    },
    {
      "epoch": 47.96766832965928,
      "grad_norm": 8.992511749267578,
      "learning_rate": 1.0026943058617268e-05,
      "loss": 1.8293,
      "step": 1240300
    },
    {
      "epoch": 47.97153575434118,
      "grad_norm": 15.48856258392334,
      "learning_rate": 1.0023720204715681e-05,
      "loss": 1.8857,
      "step": 1240400
    },
    {
      "epoch": 47.97540317902309,
      "grad_norm": 11.678487777709961,
      "learning_rate": 1.0020497350814094e-05,
      "loss": 1.7367,
      "step": 1240500
    },
    {
      "epoch": 47.97927060370499,
      "grad_norm": 14.3889741897583,
      "learning_rate": 1.0017274496912507e-05,
      "loss": 1.7601,
      "step": 1240600
    },
    {
      "epoch": 47.9831380283869,
      "grad_norm": 13.513032913208008,
      "learning_rate": 1.001405164301092e-05,
      "loss": 1.8588,
      "step": 1240700
    },
    {
      "epoch": 47.987005453068804,
      "grad_norm": 13.148018836975098,
      "learning_rate": 1.0010828789109332e-05,
      "loss": 1.8172,
      "step": 1240800
    },
    {
      "epoch": 47.990872877750704,
      "grad_norm": 13.71627140045166,
      "learning_rate": 1.0007605935207745e-05,
      "loss": 1.6732,
      "step": 1240900
    },
    {
      "epoch": 47.99474030243261,
      "grad_norm": 11.327791213989258,
      "learning_rate": 1.0004383081306158e-05,
      "loss": 1.6592,
      "step": 1241000
    },
    {
      "epoch": 47.99860772711452,
      "grad_norm": 14.349538803100586,
      "learning_rate": 1.000116022740457e-05,
      "loss": 1.7416,
      "step": 1241100
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.756419062614441,
      "eval_runtime": 5.685,
      "eval_samples_per_second": 239.402,
      "eval_steps_per_second": 239.402,
      "step": 1241136
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.5954045057296753,
      "eval_runtime": 106.3264,
      "eval_samples_per_second": 243.185,
      "eval_steps_per_second": 243.185,
      "step": 1241136
    },
    {
      "epoch": 48.00247515179642,
      "grad_norm": 15.36913776397705,
      "learning_rate": 9.997937373502984e-06,
      "loss": 1.6767,
      "step": 1241200
    },
    {
      "epoch": 48.006342576478325,
      "grad_norm": 10.8894681930542,
      "learning_rate": 9.994714519601397e-06,
      "loss": 1.7799,
      "step": 1241300
    },
    {
      "epoch": 48.010210001160225,
      "grad_norm": 13.765934944152832,
      "learning_rate": 9.99149166569981e-06,
      "loss": 1.8363,
      "step": 1241400
    },
    {
      "epoch": 48.01407742584213,
      "grad_norm": 10.69930362701416,
      "learning_rate": 9.988268811798223e-06,
      "loss": 1.731,
      "step": 1241500
    },
    {
      "epoch": 48.01794485052404,
      "grad_norm": 12.996752738952637,
      "learning_rate": 9.985045957896638e-06,
      "loss": 1.7542,
      "step": 1241600
    },
    {
      "epoch": 48.02181227520594,
      "grad_norm": 12.619122505187988,
      "learning_rate": 9.98182310399505e-06,
      "loss": 1.7771,
      "step": 1241700
    },
    {
      "epoch": 48.025679699887846,
      "grad_norm": 14.100020408630371,
      "learning_rate": 9.978600250093464e-06,
      "loss": 1.7735,
      "step": 1241800
    },
    {
      "epoch": 48.029547124569746,
      "grad_norm": 12.147064208984375,
      "learning_rate": 9.975377396191877e-06,
      "loss": 1.8461,
      "step": 1241900
    },
    {
      "epoch": 48.03341454925165,
      "grad_norm": 10.923189163208008,
      "learning_rate": 9.97215454229029e-06,
      "loss": 1.7673,
      "step": 1242000
    },
    {
      "epoch": 48.03728197393356,
      "grad_norm": 11.687841415405273,
      "learning_rate": 9.968931688388703e-06,
      "loss": 1.7513,
      "step": 1242100
    },
    {
      "epoch": 48.04114939861546,
      "grad_norm": 13.91080093383789,
      "learning_rate": 9.965708834487116e-06,
      "loss": 1.6836,
      "step": 1242200
    },
    {
      "epoch": 48.04501682329737,
      "grad_norm": 12.985824584960938,
      "learning_rate": 9.962485980585529e-06,
      "loss": 1.8504,
      "step": 1242300
    },
    {
      "epoch": 48.048884247979274,
      "grad_norm": 11.493772506713867,
      "learning_rate": 9.959263126683942e-06,
      "loss": 1.8437,
      "step": 1242400
    },
    {
      "epoch": 48.052751672661174,
      "grad_norm": 12.356525421142578,
      "learning_rate": 9.956040272782355e-06,
      "loss": 1.7989,
      "step": 1242500
    },
    {
      "epoch": 48.05661909734308,
      "grad_norm": 12.05048656463623,
      "learning_rate": 9.952817418880768e-06,
      "loss": 1.8743,
      "step": 1242600
    },
    {
      "epoch": 48.06048652202498,
      "grad_norm": 10.501168251037598,
      "learning_rate": 9.949594564979181e-06,
      "loss": 1.7936,
      "step": 1242700
    },
    {
      "epoch": 48.06435394670689,
      "grad_norm": 12.426844596862793,
      "learning_rate": 9.946371711077594e-06,
      "loss": 1.836,
      "step": 1242800
    },
    {
      "epoch": 48.068221371388795,
      "grad_norm": 13.027478218078613,
      "learning_rate": 9.943148857176007e-06,
      "loss": 1.79,
      "step": 1242900
    },
    {
      "epoch": 48.072088796070695,
      "grad_norm": 10.58871078491211,
      "learning_rate": 9.93992600327442e-06,
      "loss": 1.7872,
      "step": 1243000
    },
    {
      "epoch": 48.0759562207526,
      "grad_norm": 9.078939437866211,
      "learning_rate": 9.936703149372833e-06,
      "loss": 1.9069,
      "step": 1243100
    },
    {
      "epoch": 48.07982364543451,
      "grad_norm": 16.337474822998047,
      "learning_rate": 9.933480295471246e-06,
      "loss": 1.8651,
      "step": 1243200
    },
    {
      "epoch": 48.08369107011641,
      "grad_norm": 13.846837043762207,
      "learning_rate": 9.930257441569659e-06,
      "loss": 1.8248,
      "step": 1243300
    },
    {
      "epoch": 48.087558494798316,
      "grad_norm": 12.6242036819458,
      "learning_rate": 9.927034587668072e-06,
      "loss": 1.7793,
      "step": 1243400
    },
    {
      "epoch": 48.091425919480216,
      "grad_norm": 12.0697603225708,
      "learning_rate": 9.923811733766487e-06,
      "loss": 1.8339,
      "step": 1243500
    },
    {
      "epoch": 48.09529334416212,
      "grad_norm": 12.010069847106934,
      "learning_rate": 9.9205888798649e-06,
      "loss": 1.7802,
      "step": 1243600
    },
    {
      "epoch": 48.09916076884403,
      "grad_norm": 14.53388786315918,
      "learning_rate": 9.917366025963311e-06,
      "loss": 1.7552,
      "step": 1243700
    },
    {
      "epoch": 48.10302819352593,
      "grad_norm": 16.573759078979492,
      "learning_rate": 9.914143172061724e-06,
      "loss": 1.8485,
      "step": 1243800
    },
    {
      "epoch": 48.10689561820784,
      "grad_norm": 11.63060188293457,
      "learning_rate": 9.910920318160137e-06,
      "loss": 1.8218,
      "step": 1243900
    },
    {
      "epoch": 48.11076304288974,
      "grad_norm": 9.256078720092773,
      "learning_rate": 9.90769746425855e-06,
      "loss": 1.7537,
      "step": 1244000
    },
    {
      "epoch": 48.114630467571644,
      "grad_norm": 11.716150283813477,
      "learning_rate": 9.904474610356963e-06,
      "loss": 1.6734,
      "step": 1244100
    },
    {
      "epoch": 48.11849789225355,
      "grad_norm": 12.704577445983887,
      "learning_rate": 9.901251756455376e-06,
      "loss": 1.7849,
      "step": 1244200
    },
    {
      "epoch": 48.12236531693545,
      "grad_norm": 9.45241928100586,
      "learning_rate": 9.89802890255379e-06,
      "loss": 1.8417,
      "step": 1244300
    },
    {
      "epoch": 48.12623274161736,
      "grad_norm": 12.995237350463867,
      "learning_rate": 9.894806048652202e-06,
      "loss": 1.8338,
      "step": 1244400
    },
    {
      "epoch": 48.130100166299265,
      "grad_norm": 13.264190673828125,
      "learning_rate": 9.891583194750615e-06,
      "loss": 1.7484,
      "step": 1244500
    },
    {
      "epoch": 48.133967590981165,
      "grad_norm": 13.587838172912598,
      "learning_rate": 9.888360340849028e-06,
      "loss": 1.7309,
      "step": 1244600
    },
    {
      "epoch": 48.13783501566307,
      "grad_norm": 12.362879753112793,
      "learning_rate": 9.885137486947441e-06,
      "loss": 1.7455,
      "step": 1244700
    },
    {
      "epoch": 48.14170244034497,
      "grad_norm": 13.089284896850586,
      "learning_rate": 9.881914633045854e-06,
      "loss": 1.7617,
      "step": 1244800
    },
    {
      "epoch": 48.14556986502688,
      "grad_norm": 12.617974281311035,
      "learning_rate": 9.878691779144267e-06,
      "loss": 1.7351,
      "step": 1244900
    },
    {
      "epoch": 48.149437289708786,
      "grad_norm": 12.87181282043457,
      "learning_rate": 9.875468925242682e-06,
      "loss": 1.7397,
      "step": 1245000
    },
    {
      "epoch": 48.153304714390686,
      "grad_norm": 13.356668472290039,
      "learning_rate": 9.872246071341095e-06,
      "loss": 1.8526,
      "step": 1245100
    },
    {
      "epoch": 48.15717213907259,
      "grad_norm": 12.573769569396973,
      "learning_rate": 9.869023217439508e-06,
      "loss": 1.7672,
      "step": 1245200
    },
    {
      "epoch": 48.16103956375449,
      "grad_norm": 11.918251037597656,
      "learning_rate": 9.865800363537921e-06,
      "loss": 1.6706,
      "step": 1245300
    },
    {
      "epoch": 48.1649069884364,
      "grad_norm": 10.115242958068848,
      "learning_rate": 9.862577509636334e-06,
      "loss": 1.7737,
      "step": 1245400
    },
    {
      "epoch": 48.16877441311831,
      "grad_norm": 11.110001564025879,
      "learning_rate": 9.859354655734747e-06,
      "loss": 1.8073,
      "step": 1245500
    },
    {
      "epoch": 48.17264183780021,
      "grad_norm": 14.941848754882812,
      "learning_rate": 9.85613180183316e-06,
      "loss": 1.8077,
      "step": 1245600
    },
    {
      "epoch": 48.176509262482114,
      "grad_norm": 10.52430248260498,
      "learning_rate": 9.852908947931573e-06,
      "loss": 1.7751,
      "step": 1245700
    },
    {
      "epoch": 48.18037668716402,
      "grad_norm": 13.364860534667969,
      "learning_rate": 9.849686094029986e-06,
      "loss": 1.7729,
      "step": 1245800
    },
    {
      "epoch": 48.18424411184592,
      "grad_norm": 18.134845733642578,
      "learning_rate": 9.8464632401284e-06,
      "loss": 1.8537,
      "step": 1245900
    },
    {
      "epoch": 48.18811153652783,
      "grad_norm": 12.108147621154785,
      "learning_rate": 9.843240386226812e-06,
      "loss": 1.7432,
      "step": 1246000
    },
    {
      "epoch": 48.19197896120973,
      "grad_norm": 11.36971664428711,
      "learning_rate": 9.840017532325225e-06,
      "loss": 1.8286,
      "step": 1246100
    },
    {
      "epoch": 48.195846385891635,
      "grad_norm": 9.810415267944336,
      "learning_rate": 9.836794678423638e-06,
      "loss": 1.8787,
      "step": 1246200
    },
    {
      "epoch": 48.19971381057354,
      "grad_norm": 9.885248184204102,
      "learning_rate": 9.833571824522051e-06,
      "loss": 1.8197,
      "step": 1246300
    },
    {
      "epoch": 48.20358123525544,
      "grad_norm": 15.627680778503418,
      "learning_rate": 9.830348970620464e-06,
      "loss": 1.7376,
      "step": 1246400
    },
    {
      "epoch": 48.20744865993735,
      "grad_norm": 12.263349533081055,
      "learning_rate": 9.827126116718877e-06,
      "loss": 1.7577,
      "step": 1246500
    },
    {
      "epoch": 48.21131608461925,
      "grad_norm": 12.85498332977295,
      "learning_rate": 9.82390326281729e-06,
      "loss": 1.7387,
      "step": 1246600
    },
    {
      "epoch": 48.215183509301156,
      "grad_norm": 11.570140838623047,
      "learning_rate": 9.820680408915704e-06,
      "loss": 1.8378,
      "step": 1246700
    },
    {
      "epoch": 48.21905093398306,
      "grad_norm": 9.13331413269043,
      "learning_rate": 9.817457555014117e-06,
      "loss": 1.7713,
      "step": 1246800
    },
    {
      "epoch": 48.22291835866496,
      "grad_norm": 11.340975761413574,
      "learning_rate": 9.81423470111253e-06,
      "loss": 1.8017,
      "step": 1246900
    },
    {
      "epoch": 48.22678578334687,
      "grad_norm": 12.31676959991455,
      "learning_rate": 9.811011847210943e-06,
      "loss": 1.8314,
      "step": 1247000
    },
    {
      "epoch": 48.23065320802878,
      "grad_norm": 10.431633949279785,
      "learning_rate": 9.807788993309356e-06,
      "loss": 1.7809,
      "step": 1247100
    },
    {
      "epoch": 48.23452063271068,
      "grad_norm": 10.337166786193848,
      "learning_rate": 9.804566139407769e-06,
      "loss": 1.7978,
      "step": 1247200
    },
    {
      "epoch": 48.238388057392584,
      "grad_norm": 10.918100357055664,
      "learning_rate": 9.801343285506182e-06,
      "loss": 1.7735,
      "step": 1247300
    },
    {
      "epoch": 48.24225548207448,
      "grad_norm": 12.03033447265625,
      "learning_rate": 9.798120431604595e-06,
      "loss": 1.8478,
      "step": 1247400
    },
    {
      "epoch": 48.24612290675639,
      "grad_norm": 14.546446800231934,
      "learning_rate": 9.794897577703008e-06,
      "loss": 1.8261,
      "step": 1247500
    },
    {
      "epoch": 48.2499903314383,
      "grad_norm": 14.963661193847656,
      "learning_rate": 9.79167472380142e-06,
      "loss": 1.7818,
      "step": 1247600
    },
    {
      "epoch": 48.2538577561202,
      "grad_norm": 11.262755393981934,
      "learning_rate": 9.788451869899834e-06,
      "loss": 1.6827,
      "step": 1247700
    },
    {
      "epoch": 48.257725180802105,
      "grad_norm": 14.051828384399414,
      "learning_rate": 9.785229015998247e-06,
      "loss": 1.7622,
      "step": 1247800
    },
    {
      "epoch": 48.26159260548401,
      "grad_norm": 14.32011890411377,
      "learning_rate": 9.78200616209666e-06,
      "loss": 1.8169,
      "step": 1247900
    },
    {
      "epoch": 48.26546003016591,
      "grad_norm": 8.190839767456055,
      "learning_rate": 9.778783308195073e-06,
      "loss": 1.7701,
      "step": 1248000
    },
    {
      "epoch": 48.26932745484782,
      "grad_norm": 10.765195846557617,
      "learning_rate": 9.775560454293486e-06,
      "loss": 1.835,
      "step": 1248100
    },
    {
      "epoch": 48.27319487952972,
      "grad_norm": 12.984278678894043,
      "learning_rate": 9.772337600391899e-06,
      "loss": 1.8101,
      "step": 1248200
    },
    {
      "epoch": 48.277062304211626,
      "grad_norm": 10.571786880493164,
      "learning_rate": 9.769114746490312e-06,
      "loss": 1.7809,
      "step": 1248300
    },
    {
      "epoch": 48.28092972889353,
      "grad_norm": 12.331184387207031,
      "learning_rate": 9.765891892588725e-06,
      "loss": 1.7825,
      "step": 1248400
    },
    {
      "epoch": 48.28479715357543,
      "grad_norm": 13.541804313659668,
      "learning_rate": 9.76266903868714e-06,
      "loss": 1.7679,
      "step": 1248500
    },
    {
      "epoch": 48.28866457825734,
      "grad_norm": 11.5520658493042,
      "learning_rate": 9.759446184785553e-06,
      "loss": 1.7621,
      "step": 1248600
    },
    {
      "epoch": 48.29253200293924,
      "grad_norm": 16.403791427612305,
      "learning_rate": 9.756223330883966e-06,
      "loss": 1.7894,
      "step": 1248700
    },
    {
      "epoch": 48.29639942762115,
      "grad_norm": 12.347661972045898,
      "learning_rate": 9.753000476982379e-06,
      "loss": 1.6843,
      "step": 1248800
    },
    {
      "epoch": 48.300266852303054,
      "grad_norm": 13.345102310180664,
      "learning_rate": 9.749777623080792e-06,
      "loss": 1.7263,
      "step": 1248900
    },
    {
      "epoch": 48.30413427698495,
      "grad_norm": 14.007187843322754,
      "learning_rate": 9.746554769179205e-06,
      "loss": 1.8021,
      "step": 1249000
    },
    {
      "epoch": 48.30800170166686,
      "grad_norm": 14.221830368041992,
      "learning_rate": 9.743331915277618e-06,
      "loss": 1.7783,
      "step": 1249100
    },
    {
      "epoch": 48.31186912634877,
      "grad_norm": 14.371856689453125,
      "learning_rate": 9.74010906137603e-06,
      "loss": 1.7378,
      "step": 1249200
    },
    {
      "epoch": 48.31573655103067,
      "grad_norm": 12.996421813964844,
      "learning_rate": 9.736886207474444e-06,
      "loss": 1.7117,
      "step": 1249300
    },
    {
      "epoch": 48.319603975712575,
      "grad_norm": 9.911142349243164,
      "learning_rate": 9.733663353572857e-06,
      "loss": 1.8209,
      "step": 1249400
    },
    {
      "epoch": 48.323471400394475,
      "grad_norm": 11.235551834106445,
      "learning_rate": 9.73044049967127e-06,
      "loss": 1.8705,
      "step": 1249500
    },
    {
      "epoch": 48.32733882507638,
      "grad_norm": 11.77073860168457,
      "learning_rate": 9.727217645769681e-06,
      "loss": 1.831,
      "step": 1249600
    },
    {
      "epoch": 48.33120624975829,
      "grad_norm": 11.381749153137207,
      "learning_rate": 9.723994791868094e-06,
      "loss": 1.8119,
      "step": 1249700
    },
    {
      "epoch": 48.33507367444019,
      "grad_norm": 12.40872573852539,
      "learning_rate": 9.720771937966507e-06,
      "loss": 1.8634,
      "step": 1249800
    },
    {
      "epoch": 48.338941099122096,
      "grad_norm": 13.761407852172852,
      "learning_rate": 9.71754908406492e-06,
      "loss": 1.8132,
      "step": 1249900
    },
    {
      "epoch": 48.342808523803996,
      "grad_norm": 12.298489570617676,
      "learning_rate": 9.714326230163335e-06,
      "loss": 1.7573,
      "step": 1250000
    },
    {
      "epoch": 48.3466759484859,
      "grad_norm": 12.710230827331543,
      "learning_rate": 9.711103376261748e-06,
      "loss": 1.723,
      "step": 1250100
    },
    {
      "epoch": 48.35054337316781,
      "grad_norm": 14.977729797363281,
      "learning_rate": 9.707880522360161e-06,
      "loss": 1.7923,
      "step": 1250200
    },
    {
      "epoch": 48.35441079784971,
      "grad_norm": 18.766464233398438,
      "learning_rate": 9.704657668458574e-06,
      "loss": 1.8579,
      "step": 1250300
    },
    {
      "epoch": 48.35827822253162,
      "grad_norm": 13.817497253417969,
      "learning_rate": 9.701434814556987e-06,
      "loss": 1.8652,
      "step": 1250400
    },
    {
      "epoch": 48.362145647213524,
      "grad_norm": 12.889942169189453,
      "learning_rate": 9.6982119606554e-06,
      "loss": 1.8726,
      "step": 1250500
    },
    {
      "epoch": 48.36601307189542,
      "grad_norm": 10.723278999328613,
      "learning_rate": 9.694989106753813e-06,
      "loss": 1.794,
      "step": 1250600
    },
    {
      "epoch": 48.36988049657733,
      "grad_norm": 11.74270248413086,
      "learning_rate": 9.691766252852226e-06,
      "loss": 1.6967,
      "step": 1250700
    },
    {
      "epoch": 48.37374792125923,
      "grad_norm": 16.597253799438477,
      "learning_rate": 9.68854339895064e-06,
      "loss": 1.764,
      "step": 1250800
    },
    {
      "epoch": 48.37761534594114,
      "grad_norm": 12.934920310974121,
      "learning_rate": 9.685320545049052e-06,
      "loss": 1.7266,
      "step": 1250900
    },
    {
      "epoch": 48.381482770623045,
      "grad_norm": 11.695073127746582,
      "learning_rate": 9.682097691147465e-06,
      "loss": 1.7342,
      "step": 1251000
    },
    {
      "epoch": 48.385350195304945,
      "grad_norm": 13.222785949707031,
      "learning_rate": 9.678874837245878e-06,
      "loss": 1.8604,
      "step": 1251100
    },
    {
      "epoch": 48.38921761998685,
      "grad_norm": 10.244056701660156,
      "learning_rate": 9.675651983344291e-06,
      "loss": 1.8019,
      "step": 1251200
    },
    {
      "epoch": 48.39308504466876,
      "grad_norm": 15.269286155700684,
      "learning_rate": 9.672429129442704e-06,
      "loss": 1.7306,
      "step": 1251300
    },
    {
      "epoch": 48.39695246935066,
      "grad_norm": 6.386001110076904,
      "learning_rate": 9.669206275541117e-06,
      "loss": 1.7737,
      "step": 1251400
    },
    {
      "epoch": 48.400819894032566,
      "grad_norm": 12.55341625213623,
      "learning_rate": 9.66598342163953e-06,
      "loss": 1.8571,
      "step": 1251500
    },
    {
      "epoch": 48.404687318714466,
      "grad_norm": 8.946572303771973,
      "learning_rate": 9.662760567737943e-06,
      "loss": 1.8616,
      "step": 1251600
    },
    {
      "epoch": 48.40855474339637,
      "grad_norm": 11.121018409729004,
      "learning_rate": 9.659537713836356e-06,
      "loss": 1.8459,
      "step": 1251700
    },
    {
      "epoch": 48.41242216807828,
      "grad_norm": 10.135197639465332,
      "learning_rate": 9.65631485993477e-06,
      "loss": 1.8719,
      "step": 1251800
    },
    {
      "epoch": 48.41628959276018,
      "grad_norm": 11.103951454162598,
      "learning_rate": 9.653092006033184e-06,
      "loss": 1.8348,
      "step": 1251900
    },
    {
      "epoch": 48.42015701744209,
      "grad_norm": 10.851508140563965,
      "learning_rate": 9.649869152131597e-06,
      "loss": 1.7959,
      "step": 1252000
    },
    {
      "epoch": 48.42402444212399,
      "grad_norm": 12.205345153808594,
      "learning_rate": 9.64664629823001e-06,
      "loss": 1.8554,
      "step": 1252100
    },
    {
      "epoch": 48.42789186680589,
      "grad_norm": 11.129765510559082,
      "learning_rate": 9.643423444328423e-06,
      "loss": 1.7116,
      "step": 1252200
    },
    {
      "epoch": 48.4317592914878,
      "grad_norm": 12.24166488647461,
      "learning_rate": 9.640200590426836e-06,
      "loss": 1.7924,
      "step": 1252300
    },
    {
      "epoch": 48.4356267161697,
      "grad_norm": 10.74964714050293,
      "learning_rate": 9.63697773652525e-06,
      "loss": 1.8519,
      "step": 1252400
    },
    {
      "epoch": 48.43949414085161,
      "grad_norm": 11.975701332092285,
      "learning_rate": 9.633754882623662e-06,
      "loss": 1.7845,
      "step": 1252500
    },
    {
      "epoch": 48.443361565533515,
      "grad_norm": 15.127360343933105,
      "learning_rate": 9.630532028722074e-06,
      "loss": 1.8444,
      "step": 1252600
    },
    {
      "epoch": 48.447228990215415,
      "grad_norm": 12.344005584716797,
      "learning_rate": 9.627309174820487e-06,
      "loss": 1.7993,
      "step": 1252700
    },
    {
      "epoch": 48.45109641489732,
      "grad_norm": 15.088061332702637,
      "learning_rate": 9.6240863209189e-06,
      "loss": 1.8105,
      "step": 1252800
    },
    {
      "epoch": 48.45496383957922,
      "grad_norm": 11.662945747375488,
      "learning_rate": 9.620863467017313e-06,
      "loss": 1.8296,
      "step": 1252900
    },
    {
      "epoch": 48.45883126426113,
      "grad_norm": 13.022051811218262,
      "learning_rate": 9.617640613115726e-06,
      "loss": 1.7978,
      "step": 1253000
    },
    {
      "epoch": 48.462698688943036,
      "grad_norm": 15.554424285888672,
      "learning_rate": 9.614417759214139e-06,
      "loss": 1.7377,
      "step": 1253100
    },
    {
      "epoch": 48.466566113624935,
      "grad_norm": 12.288143157958984,
      "learning_rate": 9.611194905312552e-06,
      "loss": 1.898,
      "step": 1253200
    },
    {
      "epoch": 48.47043353830684,
      "grad_norm": 11.891777992248535,
      "learning_rate": 9.607972051410965e-06,
      "loss": 1.7748,
      "step": 1253300
    },
    {
      "epoch": 48.47430096298874,
      "grad_norm": 16.82126235961914,
      "learning_rate": 9.604749197509378e-06,
      "loss": 1.8576,
      "step": 1253400
    },
    {
      "epoch": 48.47816838767065,
      "grad_norm": 13.50461196899414,
      "learning_rate": 9.601526343607793e-06,
      "loss": 1.7725,
      "step": 1253500
    },
    {
      "epoch": 48.48203581235256,
      "grad_norm": 11.134553909301758,
      "learning_rate": 9.598303489706206e-06,
      "loss": 1.8157,
      "step": 1253600
    },
    {
      "epoch": 48.48590323703446,
      "grad_norm": 17.807668685913086,
      "learning_rate": 9.595080635804619e-06,
      "loss": 1.7629,
      "step": 1253700
    },
    {
      "epoch": 48.48977066171636,
      "grad_norm": 10.733931541442871,
      "learning_rate": 9.591857781903032e-06,
      "loss": 1.7723,
      "step": 1253800
    },
    {
      "epoch": 48.49363808639827,
      "grad_norm": 10.51719856262207,
      "learning_rate": 9.588634928001445e-06,
      "loss": 1.7842,
      "step": 1253900
    },
    {
      "epoch": 48.49750551108017,
      "grad_norm": 12.54228687286377,
      "learning_rate": 9.585412074099858e-06,
      "loss": 1.842,
      "step": 1254000
    },
    {
      "epoch": 48.50137293576208,
      "grad_norm": 13.315382957458496,
      "learning_rate": 9.58218922019827e-06,
      "loss": 1.8186,
      "step": 1254100
    },
    {
      "epoch": 48.50524036044398,
      "grad_norm": 10.45506477355957,
      "learning_rate": 9.578966366296684e-06,
      "loss": 1.7992,
      "step": 1254200
    },
    {
      "epoch": 48.509107785125885,
      "grad_norm": 15.336915016174316,
      "learning_rate": 9.575743512395097e-06,
      "loss": 1.8218,
      "step": 1254300
    },
    {
      "epoch": 48.51297520980779,
      "grad_norm": 11.810124397277832,
      "learning_rate": 9.57252065849351e-06,
      "loss": 1.7016,
      "step": 1254400
    },
    {
      "epoch": 48.51684263448969,
      "grad_norm": 9.88845443725586,
      "learning_rate": 9.569297804591923e-06,
      "loss": 1.7465,
      "step": 1254500
    },
    {
      "epoch": 48.5207100591716,
      "grad_norm": 15.779160499572754,
      "learning_rate": 9.566074950690336e-06,
      "loss": 1.8178,
      "step": 1254600
    },
    {
      "epoch": 48.524577483853506,
      "grad_norm": 12.795300483703613,
      "learning_rate": 9.562852096788749e-06,
      "loss": 1.9041,
      "step": 1254700
    },
    {
      "epoch": 48.528444908535405,
      "grad_norm": 10.723450660705566,
      "learning_rate": 9.559629242887162e-06,
      "loss": 1.7198,
      "step": 1254800
    },
    {
      "epoch": 48.53231233321731,
      "grad_norm": 17.909120559692383,
      "learning_rate": 9.556406388985575e-06,
      "loss": 1.8067,
      "step": 1254900
    },
    {
      "epoch": 48.53617975789921,
      "grad_norm": 12.445725440979004,
      "learning_rate": 9.553183535083988e-06,
      "loss": 1.8025,
      "step": 1255000
    },
    {
      "epoch": 48.54004718258112,
      "grad_norm": 11.239069938659668,
      "learning_rate": 9.549960681182401e-06,
      "loss": 1.8211,
      "step": 1255100
    },
    {
      "epoch": 48.54391460726303,
      "grad_norm": 12.428885459899902,
      "learning_rate": 9.546737827280814e-06,
      "loss": 1.8293,
      "step": 1255200
    },
    {
      "epoch": 48.54778203194493,
      "grad_norm": 12.579819679260254,
      "learning_rate": 9.543514973379227e-06,
      "loss": 1.7815,
      "step": 1255300
    },
    {
      "epoch": 48.55164945662683,
      "grad_norm": 11.234579086303711,
      "learning_rate": 9.540292119477642e-06,
      "loss": 1.7158,
      "step": 1255400
    },
    {
      "epoch": 48.55551688130873,
      "grad_norm": 13.033897399902344,
      "learning_rate": 9.537069265576053e-06,
      "loss": 1.7289,
      "step": 1255500
    },
    {
      "epoch": 48.55938430599064,
      "grad_norm": 12.773981094360352,
      "learning_rate": 9.533846411674466e-06,
      "loss": 1.87,
      "step": 1255600
    },
    {
      "epoch": 48.56325173067255,
      "grad_norm": 9.817455291748047,
      "learning_rate": 9.530623557772879e-06,
      "loss": 1.8312,
      "step": 1255700
    },
    {
      "epoch": 48.56711915535445,
      "grad_norm": 11.80781364440918,
      "learning_rate": 9.527400703871292e-06,
      "loss": 1.8392,
      "step": 1255800
    },
    {
      "epoch": 48.570986580036354,
      "grad_norm": 14.284442901611328,
      "learning_rate": 9.524177849969705e-06,
      "loss": 1.8817,
      "step": 1255900
    },
    {
      "epoch": 48.57485400471826,
      "grad_norm": 17.15354347229004,
      "learning_rate": 9.520954996068118e-06,
      "loss": 1.7362,
      "step": 1256000
    },
    {
      "epoch": 48.57872142940016,
      "grad_norm": 12.332473754882812,
      "learning_rate": 9.517732142166531e-06,
      "loss": 1.716,
      "step": 1256100
    },
    {
      "epoch": 48.58258885408207,
      "grad_norm": 13.53428840637207,
      "learning_rate": 9.514509288264944e-06,
      "loss": 1.7859,
      "step": 1256200
    },
    {
      "epoch": 48.58645627876397,
      "grad_norm": 12.416582107543945,
      "learning_rate": 9.511286434363357e-06,
      "loss": 1.7859,
      "step": 1256300
    },
    {
      "epoch": 48.590323703445875,
      "grad_norm": 13.191967964172363,
      "learning_rate": 9.50806358046177e-06,
      "loss": 1.7818,
      "step": 1256400
    },
    {
      "epoch": 48.59419112812778,
      "grad_norm": 13.010343551635742,
      "learning_rate": 9.504840726560183e-06,
      "loss": 1.7586,
      "step": 1256500
    },
    {
      "epoch": 48.59805855280968,
      "grad_norm": 16.728351593017578,
      "learning_rate": 9.501617872658596e-06,
      "loss": 1.7746,
      "step": 1256600
    },
    {
      "epoch": 48.60192597749159,
      "grad_norm": 5.914063930511475,
      "learning_rate": 9.49839501875701e-06,
      "loss": 1.7275,
      "step": 1256700
    },
    {
      "epoch": 48.60579340217349,
      "grad_norm": 12.141140937805176,
      "learning_rate": 9.495172164855422e-06,
      "loss": 1.8198,
      "step": 1256800
    },
    {
      "epoch": 48.6096608268554,
      "grad_norm": 13.481497764587402,
      "learning_rate": 9.491949310953837e-06,
      "loss": 1.8485,
      "step": 1256900
    },
    {
      "epoch": 48.6135282515373,
      "grad_norm": 12.155675888061523,
      "learning_rate": 9.48872645705225e-06,
      "loss": 1.8582,
      "step": 1257000
    },
    {
      "epoch": 48.6173956762192,
      "grad_norm": 10.90493106842041,
      "learning_rate": 9.485503603150663e-06,
      "loss": 1.8275,
      "step": 1257100
    },
    {
      "epoch": 48.62126310090111,
      "grad_norm": 9.853653907775879,
      "learning_rate": 9.482280749249076e-06,
      "loss": 1.8038,
      "step": 1257200
    },
    {
      "epoch": 48.62513052558302,
      "grad_norm": 11.166149139404297,
      "learning_rate": 9.47905789534749e-06,
      "loss": 1.7286,
      "step": 1257300
    },
    {
      "epoch": 48.62899795026492,
      "grad_norm": 8.451216697692871,
      "learning_rate": 9.475835041445902e-06,
      "loss": 1.8275,
      "step": 1257400
    },
    {
      "epoch": 48.632865374946824,
      "grad_norm": 15.994080543518066,
      "learning_rate": 9.472612187544315e-06,
      "loss": 1.8677,
      "step": 1257500
    },
    {
      "epoch": 48.636732799628724,
      "grad_norm": 12.585865020751953,
      "learning_rate": 9.469389333642728e-06,
      "loss": 1.7806,
      "step": 1257600
    },
    {
      "epoch": 48.64060022431063,
      "grad_norm": 11.644543647766113,
      "learning_rate": 9.466166479741141e-06,
      "loss": 1.8694,
      "step": 1257700
    },
    {
      "epoch": 48.64446764899254,
      "grad_norm": 10.85020923614502,
      "learning_rate": 9.462943625839554e-06,
      "loss": 1.7945,
      "step": 1257800
    },
    {
      "epoch": 48.64833507367444,
      "grad_norm": 14.162028312683105,
      "learning_rate": 9.459720771937967e-06,
      "loss": 1.8145,
      "step": 1257900
    },
    {
      "epoch": 48.652202498356345,
      "grad_norm": 15.666421890258789,
      "learning_rate": 9.45649791803638e-06,
      "loss": 1.7842,
      "step": 1258000
    },
    {
      "epoch": 48.656069923038245,
      "grad_norm": 14.085077285766602,
      "learning_rate": 9.453275064134793e-06,
      "loss": 1.7365,
      "step": 1258100
    },
    {
      "epoch": 48.65993734772015,
      "grad_norm": 13.984841346740723,
      "learning_rate": 9.450052210233206e-06,
      "loss": 1.7818,
      "step": 1258200
    },
    {
      "epoch": 48.66380477240206,
      "grad_norm": 11.83707332611084,
      "learning_rate": 9.44682935633162e-06,
      "loss": 1.7232,
      "step": 1258300
    },
    {
      "epoch": 48.66767219708396,
      "grad_norm": 9.79916763305664,
      "learning_rate": 9.443606502430032e-06,
      "loss": 1.818,
      "step": 1258400
    },
    {
      "epoch": 48.67153962176587,
      "grad_norm": 14.030691146850586,
      "learning_rate": 9.440383648528445e-06,
      "loss": 1.7964,
      "step": 1258500
    },
    {
      "epoch": 48.67540704644777,
      "grad_norm": 10.466789245605469,
      "learning_rate": 9.437160794626859e-06,
      "loss": 1.7993,
      "step": 1258600
    },
    {
      "epoch": 48.67927447112967,
      "grad_norm": 12.55413818359375,
      "learning_rate": 9.433937940725272e-06,
      "loss": 1.7698,
      "step": 1258700
    },
    {
      "epoch": 48.68314189581158,
      "grad_norm": 16.303213119506836,
      "learning_rate": 9.430715086823685e-06,
      "loss": 1.8015,
      "step": 1258800
    },
    {
      "epoch": 48.68700932049348,
      "grad_norm": 15.101065635681152,
      "learning_rate": 9.427492232922098e-06,
      "loss": 1.7346,
      "step": 1258900
    },
    {
      "epoch": 48.69087674517539,
      "grad_norm": 13.748661041259766,
      "learning_rate": 9.42426937902051e-06,
      "loss": 1.8475,
      "step": 1259000
    },
    {
      "epoch": 48.694744169857294,
      "grad_norm": 14.635642051696777,
      "learning_rate": 9.421046525118924e-06,
      "loss": 1.7575,
      "step": 1259100
    },
    {
      "epoch": 48.698611594539194,
      "grad_norm": 13.226995468139648,
      "learning_rate": 9.417823671217337e-06,
      "loss": 1.8438,
      "step": 1259200
    },
    {
      "epoch": 48.7024790192211,
      "grad_norm": 11.588971138000488,
      "learning_rate": 9.41460081731575e-06,
      "loss": 1.7978,
      "step": 1259300
    },
    {
      "epoch": 48.706346443903,
      "grad_norm": 13.809685707092285,
      "learning_rate": 9.411377963414163e-06,
      "loss": 1.7958,
      "step": 1259400
    },
    {
      "epoch": 48.71021386858491,
      "grad_norm": 12.118885040283203,
      "learning_rate": 9.408155109512576e-06,
      "loss": 1.8737,
      "step": 1259500
    },
    {
      "epoch": 48.714081293266815,
      "grad_norm": 12.088787078857422,
      "learning_rate": 9.404932255610989e-06,
      "loss": 1.8158,
      "step": 1259600
    },
    {
      "epoch": 48.717948717948715,
      "grad_norm": 8.144501686096191,
      "learning_rate": 9.401709401709402e-06,
      "loss": 1.7575,
      "step": 1259700
    },
    {
      "epoch": 48.72181614263062,
      "grad_norm": 11.030330657958984,
      "learning_rate": 9.398486547807815e-06,
      "loss": 1.8553,
      "step": 1259800
    },
    {
      "epoch": 48.72568356731253,
      "grad_norm": 10.886218070983887,
      "learning_rate": 9.395263693906228e-06,
      "loss": 1.7464,
      "step": 1259900
    },
    {
      "epoch": 48.72955099199443,
      "grad_norm": 11.193821907043457,
      "learning_rate": 9.39204084000464e-06,
      "loss": 1.8009,
      "step": 1260000
    },
    {
      "epoch": 48.73341841667634,
      "grad_norm": 11.003436088562012,
      "learning_rate": 9.388817986103054e-06,
      "loss": 1.8753,
      "step": 1260100
    },
    {
      "epoch": 48.737285841358236,
      "grad_norm": 12.215185165405273,
      "learning_rate": 9.385595132201467e-06,
      "loss": 1.8384,
      "step": 1260200
    },
    {
      "epoch": 48.74115326604014,
      "grad_norm": 12.423064231872559,
      "learning_rate": 9.38237227829988e-06,
      "loss": 1.7822,
      "step": 1260300
    },
    {
      "epoch": 48.74502069072205,
      "grad_norm": 10.370319366455078,
      "learning_rate": 9.379149424398295e-06,
      "loss": 1.7627,
      "step": 1260400
    },
    {
      "epoch": 48.74888811540395,
      "grad_norm": 13.649808883666992,
      "learning_rate": 9.375926570496708e-06,
      "loss": 1.8545,
      "step": 1260500
    },
    {
      "epoch": 48.75275554008586,
      "grad_norm": 10.266011238098145,
      "learning_rate": 9.37270371659512e-06,
      "loss": 1.7677,
      "step": 1260600
    },
    {
      "epoch": 48.756622964767764,
      "grad_norm": 12.80918025970459,
      "learning_rate": 9.369480862693534e-06,
      "loss": 1.809,
      "step": 1260700
    },
    {
      "epoch": 48.760490389449664,
      "grad_norm": 11.196433067321777,
      "learning_rate": 9.366258008791947e-06,
      "loss": 1.8364,
      "step": 1260800
    },
    {
      "epoch": 48.76435781413157,
      "grad_norm": 12.41794490814209,
      "learning_rate": 9.36303515489036e-06,
      "loss": 1.808,
      "step": 1260900
    },
    {
      "epoch": 48.76822523881347,
      "grad_norm": 11.046122550964355,
      "learning_rate": 9.359812300988773e-06,
      "loss": 1.9047,
      "step": 1261000
    },
    {
      "epoch": 48.77209266349538,
      "grad_norm": 14.11878776550293,
      "learning_rate": 9.356589447087186e-06,
      "loss": 1.7525,
      "step": 1261100
    },
    {
      "epoch": 48.775960088177285,
      "grad_norm": 10.806647300720215,
      "learning_rate": 9.353366593185599e-06,
      "loss": 1.7984,
      "step": 1261200
    },
    {
      "epoch": 48.779827512859185,
      "grad_norm": 11.692163467407227,
      "learning_rate": 9.350143739284012e-06,
      "loss": 1.8049,
      "step": 1261300
    },
    {
      "epoch": 48.78369493754109,
      "grad_norm": 11.903013229370117,
      "learning_rate": 9.346920885382423e-06,
      "loss": 1.8208,
      "step": 1261400
    },
    {
      "epoch": 48.78756236222299,
      "grad_norm": 12.408493041992188,
      "learning_rate": 9.343698031480836e-06,
      "loss": 1.8061,
      "step": 1261500
    },
    {
      "epoch": 48.7914297869049,
      "grad_norm": 15.098718643188477,
      "learning_rate": 9.34047517757925e-06,
      "loss": 1.7755,
      "step": 1261600
    },
    {
      "epoch": 48.795297211586806,
      "grad_norm": 15.542882919311523,
      "learning_rate": 9.337252323677662e-06,
      "loss": 1.7593,
      "step": 1261700
    },
    {
      "epoch": 48.799164636268706,
      "grad_norm": 10.31885051727295,
      "learning_rate": 9.334029469776075e-06,
      "loss": 1.7683,
      "step": 1261800
    },
    {
      "epoch": 48.80303206095061,
      "grad_norm": 8.849478721618652,
      "learning_rate": 9.33080661587449e-06,
      "loss": 1.8341,
      "step": 1261900
    },
    {
      "epoch": 48.80689948563252,
      "grad_norm": 12.10302734375,
      "learning_rate": 9.327583761972903e-06,
      "loss": 1.7731,
      "step": 1262000
    },
    {
      "epoch": 48.81076691031442,
      "grad_norm": 11.323251724243164,
      "learning_rate": 9.324360908071316e-06,
      "loss": 1.8383,
      "step": 1262100
    },
    {
      "epoch": 48.81463433499633,
      "grad_norm": 12.901690483093262,
      "learning_rate": 9.321138054169729e-06,
      "loss": 1.8068,
      "step": 1262200
    },
    {
      "epoch": 48.81850175967823,
      "grad_norm": 12.744467735290527,
      "learning_rate": 9.317915200268142e-06,
      "loss": 1.8216,
      "step": 1262300
    },
    {
      "epoch": 48.822369184360134,
      "grad_norm": 9.869111061096191,
      "learning_rate": 9.314692346366555e-06,
      "loss": 1.7563,
      "step": 1262400
    },
    {
      "epoch": 48.82623660904204,
      "grad_norm": 12.324040412902832,
      "learning_rate": 9.311469492464968e-06,
      "loss": 1.8351,
      "step": 1262500
    },
    {
      "epoch": 48.83010403372394,
      "grad_norm": 13.594040870666504,
      "learning_rate": 9.308246638563381e-06,
      "loss": 1.7418,
      "step": 1262600
    },
    {
      "epoch": 48.83397145840585,
      "grad_norm": 12.261148452758789,
      "learning_rate": 9.305023784661794e-06,
      "loss": 1.7173,
      "step": 1262700
    },
    {
      "epoch": 48.837838883087755,
      "grad_norm": 10.859325408935547,
      "learning_rate": 9.301800930760207e-06,
      "loss": 1.7922,
      "step": 1262800
    },
    {
      "epoch": 48.841706307769655,
      "grad_norm": 12.897174835205078,
      "learning_rate": 9.29857807685862e-06,
      "loss": 1.8215,
      "step": 1262900
    },
    {
      "epoch": 48.84557373245156,
      "grad_norm": 10.723856925964355,
      "learning_rate": 9.295355222957033e-06,
      "loss": 1.7577,
      "step": 1263000
    },
    {
      "epoch": 48.84944115713346,
      "grad_norm": 15.657732009887695,
      "learning_rate": 9.292132369055446e-06,
      "loss": 1.8499,
      "step": 1263100
    },
    {
      "epoch": 48.85330858181537,
      "grad_norm": 13.564940452575684,
      "learning_rate": 9.28890951515386e-06,
      "loss": 1.8167,
      "step": 1263200
    },
    {
      "epoch": 48.857176006497276,
      "grad_norm": 12.148677825927734,
      "learning_rate": 9.285686661252272e-06,
      "loss": 1.8189,
      "step": 1263300
    },
    {
      "epoch": 48.861043431179176,
      "grad_norm": 8.109562873840332,
      "learning_rate": 9.282463807350685e-06,
      "loss": 1.7657,
      "step": 1263400
    },
    {
      "epoch": 48.86491085586108,
      "grad_norm": 14.014760971069336,
      "learning_rate": 9.279240953449098e-06,
      "loss": 1.7044,
      "step": 1263500
    },
    {
      "epoch": 48.86877828054298,
      "grad_norm": 15.246164321899414,
      "learning_rate": 9.276018099547511e-06,
      "loss": 1.7741,
      "step": 1263600
    },
    {
      "epoch": 48.87264570522489,
      "grad_norm": 16.956867218017578,
      "learning_rate": 9.272795245645924e-06,
      "loss": 1.8179,
      "step": 1263700
    },
    {
      "epoch": 48.8765131299068,
      "grad_norm": 12.806225776672363,
      "learning_rate": 9.26957239174434e-06,
      "loss": 1.8048,
      "step": 1263800
    },
    {
      "epoch": 48.8803805545887,
      "grad_norm": 14.992595672607422,
      "learning_rate": 9.266349537842752e-06,
      "loss": 1.8282,
      "step": 1263900
    },
    {
      "epoch": 48.884247979270604,
      "grad_norm": 11.72189712524414,
      "learning_rate": 9.263126683941165e-06,
      "loss": 1.7258,
      "step": 1264000
    },
    {
      "epoch": 48.88811540395251,
      "grad_norm": 12.404593467712402,
      "learning_rate": 9.259903830039578e-06,
      "loss": 1.8002,
      "step": 1264100
    },
    {
      "epoch": 48.89198282863441,
      "grad_norm": 12.538497924804688,
      "learning_rate": 9.256680976137991e-06,
      "loss": 1.7631,
      "step": 1264200
    },
    {
      "epoch": 48.89585025331632,
      "grad_norm": 13.301910400390625,
      "learning_rate": 9.253458122236404e-06,
      "loss": 1.776,
      "step": 1264300
    },
    {
      "epoch": 48.89971767799822,
      "grad_norm": 12.299545288085938,
      "learning_rate": 9.250235268334816e-06,
      "loss": 1.8591,
      "step": 1264400
    },
    {
      "epoch": 48.903585102680125,
      "grad_norm": 10.597942352294922,
      "learning_rate": 9.247012414433229e-06,
      "loss": 1.85,
      "step": 1264500
    },
    {
      "epoch": 48.90745252736203,
      "grad_norm": 11.869373321533203,
      "learning_rate": 9.243789560531642e-06,
      "loss": 1.741,
      "step": 1264600
    },
    {
      "epoch": 48.91131995204393,
      "grad_norm": 9.773500442504883,
      "learning_rate": 9.240566706630055e-06,
      "loss": 1.7526,
      "step": 1264700
    },
    {
      "epoch": 48.91518737672584,
      "grad_norm": 12.436349868774414,
      "learning_rate": 9.237343852728468e-06,
      "loss": 1.7825,
      "step": 1264800
    },
    {
      "epoch": 48.91905480140774,
      "grad_norm": 9.33526611328125,
      "learning_rate": 9.23412099882688e-06,
      "loss": 1.7594,
      "step": 1264900
    },
    {
      "epoch": 48.922922226089646,
      "grad_norm": 11.279791831970215,
      "learning_rate": 9.230898144925294e-06,
      "loss": 1.7831,
      "step": 1265000
    },
    {
      "epoch": 48.92678965077155,
      "grad_norm": 10.161925315856934,
      "learning_rate": 9.227675291023707e-06,
      "loss": 1.8371,
      "step": 1265100
    },
    {
      "epoch": 48.93065707545345,
      "grad_norm": 12.53414535522461,
      "learning_rate": 9.22445243712212e-06,
      "loss": 1.8096,
      "step": 1265200
    },
    {
      "epoch": 48.93452450013536,
      "grad_norm": 15.24203872680664,
      "learning_rate": 9.221229583220533e-06,
      "loss": 1.7503,
      "step": 1265300
    },
    {
      "epoch": 48.93839192481727,
      "grad_norm": 13.194872856140137,
      "learning_rate": 9.218006729318948e-06,
      "loss": 1.7936,
      "step": 1265400
    },
    {
      "epoch": 48.94225934949917,
      "grad_norm": 12.046545028686523,
      "learning_rate": 9.21478387541736e-06,
      "loss": 1.7607,
      "step": 1265500
    },
    {
      "epoch": 48.946126774181074,
      "grad_norm": 13.677177429199219,
      "learning_rate": 9.211561021515774e-06,
      "loss": 1.8157,
      "step": 1265600
    },
    {
      "epoch": 48.949994198862974,
      "grad_norm": 11.969780921936035,
      "learning_rate": 9.208338167614187e-06,
      "loss": 1.7815,
      "step": 1265700
    },
    {
      "epoch": 48.95386162354488,
      "grad_norm": 13.40931510925293,
      "learning_rate": 9.2051153137126e-06,
      "loss": 1.8251,
      "step": 1265800
    },
    {
      "epoch": 48.95772904822679,
      "grad_norm": 11.294947624206543,
      "learning_rate": 9.201892459811013e-06,
      "loss": 1.8875,
      "step": 1265900
    },
    {
      "epoch": 48.96159647290869,
      "grad_norm": 12.755818367004395,
      "learning_rate": 9.198669605909426e-06,
      "loss": 1.7317,
      "step": 1266000
    },
    {
      "epoch": 48.965463897590595,
      "grad_norm": 13.490102767944336,
      "learning_rate": 9.195446752007839e-06,
      "loss": 1.7407,
      "step": 1266100
    },
    {
      "epoch": 48.969331322272495,
      "grad_norm": 10.426298141479492,
      "learning_rate": 9.192223898106252e-06,
      "loss": 1.7913,
      "step": 1266200
    },
    {
      "epoch": 48.9731987469544,
      "grad_norm": 17.34431266784668,
      "learning_rate": 9.189001044204665e-06,
      "loss": 1.7889,
      "step": 1266300
    },
    {
      "epoch": 48.97706617163631,
      "grad_norm": 15.319208145141602,
      "learning_rate": 9.185778190303078e-06,
      "loss": 1.7387,
      "step": 1266400
    },
    {
      "epoch": 48.98093359631821,
      "grad_norm": 13.4784517288208,
      "learning_rate": 9.18255533640149e-06,
      "loss": 1.7955,
      "step": 1266500
    },
    {
      "epoch": 48.984801021000116,
      "grad_norm": 13.327141761779785,
      "learning_rate": 9.179332482499904e-06,
      "loss": 1.684,
      "step": 1266600
    },
    {
      "epoch": 48.98866844568202,
      "grad_norm": 9.90857219696045,
      "learning_rate": 9.176109628598317e-06,
      "loss": 1.7455,
      "step": 1266700
    },
    {
      "epoch": 48.99253587036392,
      "grad_norm": 11.0889310836792,
      "learning_rate": 9.17288677469673e-06,
      "loss": 1.731,
      "step": 1266800
    },
    {
      "epoch": 48.99640329504583,
      "grad_norm": 11.081747055053711,
      "learning_rate": 9.169663920795143e-06,
      "loss": 1.7325,
      "step": 1266900
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.74907648563385,
      "eval_runtime": 5.7792,
      "eval_samples_per_second": 235.501,
      "eval_steps_per_second": 235.501,
      "step": 1266993
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.5856335163116455,
      "eval_runtime": 107.7521,
      "eval_samples_per_second": 239.968,
      "eval_steps_per_second": 239.968,
      "step": 1266993
    },
    {
      "epoch": 49.00027071972773,
      "grad_norm": 10.61839485168457,
      "learning_rate": 9.166441066893556e-06,
      "loss": 1.7603,
      "step": 1267000
    },
    {
      "epoch": 49.00413814440964,
      "grad_norm": 14.243158340454102,
      "learning_rate": 9.163218212991969e-06,
      "loss": 1.7762,
      "step": 1267100
    },
    {
      "epoch": 49.008005569091544,
      "grad_norm": 12.086952209472656,
      "learning_rate": 9.159995359090382e-06,
      "loss": 1.744,
      "step": 1267200
    },
    {
      "epoch": 49.011872993773444,
      "grad_norm": 10.113546371459961,
      "learning_rate": 9.156772505188795e-06,
      "loss": 1.7632,
      "step": 1267300
    },
    {
      "epoch": 49.01574041845535,
      "grad_norm": 11.945908546447754,
      "learning_rate": 9.153549651287208e-06,
      "loss": 1.8356,
      "step": 1267400
    },
    {
      "epoch": 49.01960784313726,
      "grad_norm": 13.398177146911621,
      "learning_rate": 9.150326797385621e-06,
      "loss": 1.8592,
      "step": 1267500
    },
    {
      "epoch": 49.02347526781916,
      "grad_norm": 11.503351211547852,
      "learning_rate": 9.147103943484034e-06,
      "loss": 1.9009,
      "step": 1267600
    },
    {
      "epoch": 49.027342692501065,
      "grad_norm": 9.336782455444336,
      "learning_rate": 9.143881089582447e-06,
      "loss": 1.7091,
      "step": 1267700
    },
    {
      "epoch": 49.031210117182965,
      "grad_norm": 11.373565673828125,
      "learning_rate": 9.14065823568086e-06,
      "loss": 1.8682,
      "step": 1267800
    },
    {
      "epoch": 49.03507754186487,
      "grad_norm": 11.746813774108887,
      "learning_rate": 9.137435381779273e-06,
      "loss": 1.73,
      "step": 1267900
    },
    {
      "epoch": 49.03894496654678,
      "grad_norm": 12.250782012939453,
      "learning_rate": 9.134212527877686e-06,
      "loss": 1.7656,
      "step": 1268000
    },
    {
      "epoch": 49.04281239122868,
      "grad_norm": 11.212966918945312,
      "learning_rate": 9.1309896739761e-06,
      "loss": 1.7891,
      "step": 1268100
    },
    {
      "epoch": 49.046679815910586,
      "grad_norm": 11.479921340942383,
      "learning_rate": 9.127766820074512e-06,
      "loss": 1.7751,
      "step": 1268200
    },
    {
      "epoch": 49.050547240592486,
      "grad_norm": 11.510605812072754,
      "learning_rate": 9.124543966172925e-06,
      "loss": 1.7939,
      "step": 1268300
    },
    {
      "epoch": 49.05441466527439,
      "grad_norm": 17.724828720092773,
      "learning_rate": 9.121321112271338e-06,
      "loss": 1.8005,
      "step": 1268400
    },
    {
      "epoch": 49.0582820899563,
      "grad_norm": 13.869353294372559,
      "learning_rate": 9.118098258369751e-06,
      "loss": 1.7547,
      "step": 1268500
    },
    {
      "epoch": 49.0621495146382,
      "grad_norm": 11.402555465698242,
      "learning_rate": 9.114875404468164e-06,
      "loss": 1.7968,
      "step": 1268600
    },
    {
      "epoch": 49.06601693932011,
      "grad_norm": 13.373229026794434,
      "learning_rate": 9.111652550566577e-06,
      "loss": 1.7656,
      "step": 1268700
    },
    {
      "epoch": 49.069884364002014,
      "grad_norm": 15.588737487792969,
      "learning_rate": 9.108429696664992e-06,
      "loss": 1.7693,
      "step": 1268800
    },
    {
      "epoch": 49.073751788683914,
      "grad_norm": 13.020359992980957,
      "learning_rate": 9.105206842763405e-06,
      "loss": 1.808,
      "step": 1268900
    },
    {
      "epoch": 49.07761921336582,
      "grad_norm": 12.246054649353027,
      "learning_rate": 9.101983988861818e-06,
      "loss": 1.7736,
      "step": 1269000
    },
    {
      "epoch": 49.08148663804772,
      "grad_norm": 13.821617126464844,
      "learning_rate": 9.098761134960231e-06,
      "loss": 1.7398,
      "step": 1269100
    },
    {
      "epoch": 49.08535406272963,
      "grad_norm": 12.35950756072998,
      "learning_rate": 9.095538281058644e-06,
      "loss": 1.7203,
      "step": 1269200
    },
    {
      "epoch": 49.089221487411535,
      "grad_norm": 8.349506378173828,
      "learning_rate": 9.092315427157057e-06,
      "loss": 1.7925,
      "step": 1269300
    },
    {
      "epoch": 49.093088912093435,
      "grad_norm": 12.20197868347168,
      "learning_rate": 9.08909257325547e-06,
      "loss": 1.9301,
      "step": 1269400
    },
    {
      "epoch": 49.09695633677534,
      "grad_norm": 11.112786293029785,
      "learning_rate": 9.085869719353883e-06,
      "loss": 1.8095,
      "step": 1269500
    },
    {
      "epoch": 49.10082376145724,
      "grad_norm": 14.659180641174316,
      "learning_rate": 9.082646865452296e-06,
      "loss": 1.7723,
      "step": 1269600
    },
    {
      "epoch": 49.10469118613915,
      "grad_norm": 12.245041847229004,
      "learning_rate": 9.07942401155071e-06,
      "loss": 1.7836,
      "step": 1269700
    },
    {
      "epoch": 49.108558610821056,
      "grad_norm": 11.922006607055664,
      "learning_rate": 9.076201157649122e-06,
      "loss": 1.8381,
      "step": 1269800
    },
    {
      "epoch": 49.112426035502956,
      "grad_norm": 15.683819770812988,
      "learning_rate": 9.072978303747535e-06,
      "loss": 1.8195,
      "step": 1269900
    },
    {
      "epoch": 49.11629346018486,
      "grad_norm": 13.555000305175781,
      "learning_rate": 9.069755449845948e-06,
      "loss": 1.8308,
      "step": 1270000
    },
    {
      "epoch": 49.12016088486677,
      "grad_norm": 11.08434772491455,
      "learning_rate": 9.066532595944361e-06,
      "loss": 1.7743,
      "step": 1270100
    },
    {
      "epoch": 49.12402830954867,
      "grad_norm": 12.650100708007812,
      "learning_rate": 9.063309742042774e-06,
      "loss": 1.9106,
      "step": 1270200
    },
    {
      "epoch": 49.12789573423058,
      "grad_norm": 15.147936820983887,
      "learning_rate": 9.060086888141186e-06,
      "loss": 1.7491,
      "step": 1270300
    },
    {
      "epoch": 49.13176315891248,
      "grad_norm": 13.229253768920898,
      "learning_rate": 9.0568640342396e-06,
      "loss": 1.7568,
      "step": 1270400
    },
    {
      "epoch": 49.135630583594384,
      "grad_norm": 12.675329208374023,
      "learning_rate": 9.053641180338013e-06,
      "loss": 1.8008,
      "step": 1270500
    },
    {
      "epoch": 49.13949800827629,
      "grad_norm": 14.94873332977295,
      "learning_rate": 9.050418326436427e-06,
      "loss": 1.7315,
      "step": 1270600
    },
    {
      "epoch": 49.14336543295819,
      "grad_norm": 12.333527565002441,
      "learning_rate": 9.04719547253484e-06,
      "loss": 1.755,
      "step": 1270700
    },
    {
      "epoch": 49.1472328576401,
      "grad_norm": 12.753053665161133,
      "learning_rate": 9.043972618633253e-06,
      "loss": 1.7699,
      "step": 1270800
    },
    {
      "epoch": 49.151100282322005,
      "grad_norm": 8.558076858520508,
      "learning_rate": 9.040749764731666e-06,
      "loss": 1.7413,
      "step": 1270900
    },
    {
      "epoch": 49.154967707003905,
      "grad_norm": 11.027868270874023,
      "learning_rate": 9.037526910830079e-06,
      "loss": 1.7025,
      "step": 1271000
    },
    {
      "epoch": 49.15883513168581,
      "grad_norm": 14.591168403625488,
      "learning_rate": 9.034304056928492e-06,
      "loss": 1.7726,
      "step": 1271100
    },
    {
      "epoch": 49.16270255636771,
      "grad_norm": 15.364679336547852,
      "learning_rate": 9.031081203026905e-06,
      "loss": 1.8005,
      "step": 1271200
    },
    {
      "epoch": 49.16656998104962,
      "grad_norm": 12.653042793273926,
      "learning_rate": 9.027858349125318e-06,
      "loss": 1.7312,
      "step": 1271300
    },
    {
      "epoch": 49.170437405731526,
      "grad_norm": 10.393662452697754,
      "learning_rate": 9.02463549522373e-06,
      "loss": 1.6815,
      "step": 1271400
    },
    {
      "epoch": 49.174304830413426,
      "grad_norm": 14.32800006866455,
      "learning_rate": 9.021412641322144e-06,
      "loss": 1.7713,
      "step": 1271500
    },
    {
      "epoch": 49.17817225509533,
      "grad_norm": 12.705597877502441,
      "learning_rate": 9.018189787420557e-06,
      "loss": 1.8276,
      "step": 1271600
    },
    {
      "epoch": 49.18203967977723,
      "grad_norm": 11.557347297668457,
      "learning_rate": 9.01496693351897e-06,
      "loss": 1.799,
      "step": 1271700
    },
    {
      "epoch": 49.18590710445914,
      "grad_norm": 12.178197860717773,
      "learning_rate": 9.011744079617383e-06,
      "loss": 1.8236,
      "step": 1271800
    },
    {
      "epoch": 49.18977452914105,
      "grad_norm": 11.54123592376709,
      "learning_rate": 9.008521225715796e-06,
      "loss": 1.8303,
      "step": 1271900
    },
    {
      "epoch": 49.19364195382295,
      "grad_norm": 14.164528846740723,
      "learning_rate": 9.005298371814209e-06,
      "loss": 1.8283,
      "step": 1272000
    },
    {
      "epoch": 49.197509378504854,
      "grad_norm": 11.13534927368164,
      "learning_rate": 9.002075517912622e-06,
      "loss": 1.6103,
      "step": 1272100
    },
    {
      "epoch": 49.20137680318676,
      "grad_norm": 13.033933639526367,
      "learning_rate": 8.998852664011035e-06,
      "loss": 1.8138,
      "step": 1272200
    },
    {
      "epoch": 49.20524422786866,
      "grad_norm": 11.743041038513184,
      "learning_rate": 8.99562981010945e-06,
      "loss": 1.8226,
      "step": 1272300
    },
    {
      "epoch": 49.20911165255057,
      "grad_norm": 11.898767471313477,
      "learning_rate": 8.992406956207863e-06,
      "loss": 1.802,
      "step": 1272400
    },
    {
      "epoch": 49.21297907723247,
      "grad_norm": 14.882100105285645,
      "learning_rate": 8.989184102306276e-06,
      "loss": 1.7888,
      "step": 1272500
    },
    {
      "epoch": 49.216846501914375,
      "grad_norm": 13.202627182006836,
      "learning_rate": 8.985961248404689e-06,
      "loss": 1.7641,
      "step": 1272600
    },
    {
      "epoch": 49.22071392659628,
      "grad_norm": 15.107728004455566,
      "learning_rate": 8.982738394503102e-06,
      "loss": 1.841,
      "step": 1272700
    },
    {
      "epoch": 49.22458135127818,
      "grad_norm": 12.598857879638672,
      "learning_rate": 8.979515540601515e-06,
      "loss": 1.8082,
      "step": 1272800
    },
    {
      "epoch": 49.22844877596009,
      "grad_norm": 13.588435173034668,
      "learning_rate": 8.976292686699928e-06,
      "loss": 1.8041,
      "step": 1272900
    },
    {
      "epoch": 49.23231620064199,
      "grad_norm": 13.371429443359375,
      "learning_rate": 8.97306983279834e-06,
      "loss": 1.8232,
      "step": 1273000
    },
    {
      "epoch": 49.236183625323896,
      "grad_norm": 14.699631690979004,
      "learning_rate": 8.969846978896754e-06,
      "loss": 1.7659,
      "step": 1273100
    },
    {
      "epoch": 49.2400510500058,
      "grad_norm": 14.708395957946777,
      "learning_rate": 8.966624124995165e-06,
      "loss": 1.8067,
      "step": 1273200
    },
    {
      "epoch": 49.2439184746877,
      "grad_norm": 12.05660629272461,
      "learning_rate": 8.963401271093578e-06,
      "loss": 1.7964,
      "step": 1273300
    },
    {
      "epoch": 49.24778589936961,
      "grad_norm": 13.102690696716309,
      "learning_rate": 8.960178417191991e-06,
      "loss": 1.8724,
      "step": 1273400
    },
    {
      "epoch": 49.25165332405152,
      "grad_norm": 13.147786140441895,
      "learning_rate": 8.956955563290404e-06,
      "loss": 1.8098,
      "step": 1273500
    },
    {
      "epoch": 49.25552074873342,
      "grad_norm": 12.50979995727539,
      "learning_rate": 8.953732709388817e-06,
      "loss": 1.7518,
      "step": 1273600
    },
    {
      "epoch": 49.259388173415324,
      "grad_norm": 12.999302864074707,
      "learning_rate": 8.95050985548723e-06,
      "loss": 1.7665,
      "step": 1273700
    },
    {
      "epoch": 49.263255598097224,
      "grad_norm": 15.921201705932617,
      "learning_rate": 8.947287001585645e-06,
      "loss": 1.8061,
      "step": 1273800
    },
    {
      "epoch": 49.26712302277913,
      "grad_norm": 13.51498031616211,
      "learning_rate": 8.944064147684058e-06,
      "loss": 1.8117,
      "step": 1273900
    },
    {
      "epoch": 49.27099044746104,
      "grad_norm": 12.904766082763672,
      "learning_rate": 8.940841293782471e-06,
      "loss": 1.8858,
      "step": 1274000
    },
    {
      "epoch": 49.27485787214294,
      "grad_norm": 19.04970359802246,
      "learning_rate": 8.937618439880884e-06,
      "loss": 1.811,
      "step": 1274100
    },
    {
      "epoch": 49.278725296824845,
      "grad_norm": 10.100903511047363,
      "learning_rate": 8.934395585979297e-06,
      "loss": 1.7883,
      "step": 1274200
    },
    {
      "epoch": 49.282592721506745,
      "grad_norm": 15.40826416015625,
      "learning_rate": 8.93117273207771e-06,
      "loss": 1.7943,
      "step": 1274300
    },
    {
      "epoch": 49.28646014618865,
      "grad_norm": 14.263293266296387,
      "learning_rate": 8.927949878176123e-06,
      "loss": 1.7486,
      "step": 1274400
    },
    {
      "epoch": 49.29032757087056,
      "grad_norm": 13.158555030822754,
      "learning_rate": 8.924727024274536e-06,
      "loss": 1.7355,
      "step": 1274500
    },
    {
      "epoch": 49.29419499555246,
      "grad_norm": 10.97370433807373,
      "learning_rate": 8.92150417037295e-06,
      "loss": 1.8424,
      "step": 1274600
    },
    {
      "epoch": 49.298062420234366,
      "grad_norm": 16.35688591003418,
      "learning_rate": 8.918281316471362e-06,
      "loss": 1.7486,
      "step": 1274700
    },
    {
      "epoch": 49.30192984491627,
      "grad_norm": 16.21467399597168,
      "learning_rate": 8.915058462569775e-06,
      "loss": 1.7334,
      "step": 1274800
    },
    {
      "epoch": 49.30579726959817,
      "grad_norm": 14.862163543701172,
      "learning_rate": 8.911835608668188e-06,
      "loss": 1.8348,
      "step": 1274900
    },
    {
      "epoch": 49.30966469428008,
      "grad_norm": 10.859728813171387,
      "learning_rate": 8.908612754766601e-06,
      "loss": 1.84,
      "step": 1275000
    },
    {
      "epoch": 49.31353211896198,
      "grad_norm": 10.348251342773438,
      "learning_rate": 8.905389900865014e-06,
      "loss": 1.8227,
      "step": 1275100
    },
    {
      "epoch": 49.31739954364389,
      "grad_norm": 12.522857666015625,
      "learning_rate": 8.902167046963427e-06,
      "loss": 1.7974,
      "step": 1275200
    },
    {
      "epoch": 49.321266968325794,
      "grad_norm": 14.558372497558594,
      "learning_rate": 8.89894419306184e-06,
      "loss": 1.7749,
      "step": 1275300
    },
    {
      "epoch": 49.325134393007694,
      "grad_norm": 12.287707328796387,
      "learning_rate": 8.895721339160253e-06,
      "loss": 1.7445,
      "step": 1275400
    },
    {
      "epoch": 49.3290018176896,
      "grad_norm": 12.074039459228516,
      "learning_rate": 8.892498485258666e-06,
      "loss": 1.8609,
      "step": 1275500
    },
    {
      "epoch": 49.33286924237151,
      "grad_norm": 12.948027610778809,
      "learning_rate": 8.88927563135708e-06,
      "loss": 1.7728,
      "step": 1275600
    },
    {
      "epoch": 49.33673666705341,
      "grad_norm": 13.49215316772461,
      "learning_rate": 8.886052777455494e-06,
      "loss": 1.6995,
      "step": 1275700
    },
    {
      "epoch": 49.340604091735315,
      "grad_norm": 12.827869415283203,
      "learning_rate": 8.882829923553907e-06,
      "loss": 1.6609,
      "step": 1275800
    },
    {
      "epoch": 49.344471516417215,
      "grad_norm": 12.27718734741211,
      "learning_rate": 8.87960706965232e-06,
      "loss": 1.8625,
      "step": 1275900
    },
    {
      "epoch": 49.34833894109912,
      "grad_norm": 13.747389793395996,
      "learning_rate": 8.876384215750733e-06,
      "loss": 1.8329,
      "step": 1276000
    },
    {
      "epoch": 49.35220636578103,
      "grad_norm": 12.408172607421875,
      "learning_rate": 8.873161361849146e-06,
      "loss": 1.8465,
      "step": 1276100
    },
    {
      "epoch": 49.35607379046293,
      "grad_norm": 11.824559211730957,
      "learning_rate": 8.869938507947558e-06,
      "loss": 1.7388,
      "step": 1276200
    },
    {
      "epoch": 49.359941215144836,
      "grad_norm": 14.410658836364746,
      "learning_rate": 8.86671565404597e-06,
      "loss": 1.7595,
      "step": 1276300
    },
    {
      "epoch": 49.363808639826736,
      "grad_norm": 8.840216636657715,
      "learning_rate": 8.863492800144384e-06,
      "loss": 1.7913,
      "step": 1276400
    },
    {
      "epoch": 49.36767606450864,
      "grad_norm": 12.472698211669922,
      "learning_rate": 8.860269946242797e-06,
      "loss": 1.8345,
      "step": 1276500
    },
    {
      "epoch": 49.37154348919055,
      "grad_norm": 12.21757698059082,
      "learning_rate": 8.85704709234121e-06,
      "loss": 1.853,
      "step": 1276600
    },
    {
      "epoch": 49.37541091387245,
      "grad_norm": 12.482891082763672,
      "learning_rate": 8.853824238439623e-06,
      "loss": 1.7619,
      "step": 1276700
    },
    {
      "epoch": 49.37927833855436,
      "grad_norm": 14.415786743164062,
      "learning_rate": 8.850601384538036e-06,
      "loss": 1.8236,
      "step": 1276800
    },
    {
      "epoch": 49.383145763236264,
      "grad_norm": 14.480803489685059,
      "learning_rate": 8.847378530636449e-06,
      "loss": 1.775,
      "step": 1276900
    },
    {
      "epoch": 49.387013187918164,
      "grad_norm": 10.010235786437988,
      "learning_rate": 8.844155676734862e-06,
      "loss": 1.7591,
      "step": 1277000
    },
    {
      "epoch": 49.39088061260007,
      "grad_norm": 13.55211067199707,
      "learning_rate": 8.840932822833275e-06,
      "loss": 1.8999,
      "step": 1277100
    },
    {
      "epoch": 49.39474803728197,
      "grad_norm": 12.53186321258545,
      "learning_rate": 8.837709968931688e-06,
      "loss": 1.7845,
      "step": 1277200
    },
    {
      "epoch": 49.39861546196388,
      "grad_norm": 14.355819702148438,
      "learning_rate": 8.834487115030103e-06,
      "loss": 1.7822,
      "step": 1277300
    },
    {
      "epoch": 49.402482886645785,
      "grad_norm": 10.16037654876709,
      "learning_rate": 8.831264261128516e-06,
      "loss": 1.7317,
      "step": 1277400
    },
    {
      "epoch": 49.406350311327685,
      "grad_norm": 10.738738059997559,
      "learning_rate": 8.828041407226929e-06,
      "loss": 1.7777,
      "step": 1277500
    },
    {
      "epoch": 49.41021773600959,
      "grad_norm": 12.806685447692871,
      "learning_rate": 8.824818553325342e-06,
      "loss": 1.8012,
      "step": 1277600
    },
    {
      "epoch": 49.41408516069149,
      "grad_norm": 12.621943473815918,
      "learning_rate": 8.821595699423755e-06,
      "loss": 1.711,
      "step": 1277700
    },
    {
      "epoch": 49.4179525853734,
      "grad_norm": 11.573688507080078,
      "learning_rate": 8.818372845522168e-06,
      "loss": 1.6547,
      "step": 1277800
    },
    {
      "epoch": 49.421820010055306,
      "grad_norm": 10.346747398376465,
      "learning_rate": 8.81514999162058e-06,
      "loss": 1.8176,
      "step": 1277900
    },
    {
      "epoch": 49.425687434737206,
      "grad_norm": 13.091109275817871,
      "learning_rate": 8.811927137718994e-06,
      "loss": 1.7368,
      "step": 1278000
    },
    {
      "epoch": 49.42955485941911,
      "grad_norm": 10.677337646484375,
      "learning_rate": 8.808704283817407e-06,
      "loss": 1.7582,
      "step": 1278100
    },
    {
      "epoch": 49.43342228410102,
      "grad_norm": 7.681869029998779,
      "learning_rate": 8.80548142991582e-06,
      "loss": 1.8058,
      "step": 1278200
    },
    {
      "epoch": 49.43728970878292,
      "grad_norm": 12.536186218261719,
      "learning_rate": 8.802258576014233e-06,
      "loss": 1.7221,
      "step": 1278300
    },
    {
      "epoch": 49.44115713346483,
      "grad_norm": 10.209492683410645,
      "learning_rate": 8.799035722112646e-06,
      "loss": 1.7092,
      "step": 1278400
    },
    {
      "epoch": 49.44502455814673,
      "grad_norm": 11.991061210632324,
      "learning_rate": 8.795812868211059e-06,
      "loss": 1.7494,
      "step": 1278500
    },
    {
      "epoch": 49.448891982828634,
      "grad_norm": 14.299271583557129,
      "learning_rate": 8.792590014309472e-06,
      "loss": 1.8355,
      "step": 1278600
    },
    {
      "epoch": 49.45275940751054,
      "grad_norm": 13.988977432250977,
      "learning_rate": 8.789367160407885e-06,
      "loss": 1.8433,
      "step": 1278700
    },
    {
      "epoch": 49.45662683219244,
      "grad_norm": 14.73564338684082,
      "learning_rate": 8.786144306506298e-06,
      "loss": 1.8131,
      "step": 1278800
    },
    {
      "epoch": 49.46049425687435,
      "grad_norm": 9.645553588867188,
      "learning_rate": 8.782921452604711e-06,
      "loss": 1.7435,
      "step": 1278900
    },
    {
      "epoch": 49.464361681556255,
      "grad_norm": 9.434868812561035,
      "learning_rate": 8.779698598703124e-06,
      "loss": 1.8616,
      "step": 1279000
    },
    {
      "epoch": 49.468229106238155,
      "grad_norm": 13.423179626464844,
      "learning_rate": 8.776475744801537e-06,
      "loss": 1.8288,
      "step": 1279100
    },
    {
      "epoch": 49.47209653092006,
      "grad_norm": 14.293750762939453,
      "learning_rate": 8.77325289089995e-06,
      "loss": 1.6911,
      "step": 1279200
    },
    {
      "epoch": 49.47596395560196,
      "grad_norm": 11.330755233764648,
      "learning_rate": 8.770030036998363e-06,
      "loss": 1.7395,
      "step": 1279300
    },
    {
      "epoch": 49.47983138028387,
      "grad_norm": 13.908475875854492,
      "learning_rate": 8.766807183096776e-06,
      "loss": 1.8582,
      "step": 1279400
    },
    {
      "epoch": 49.483698804965776,
      "grad_norm": 10.273073196411133,
      "learning_rate": 8.763584329195189e-06,
      "loss": 1.8194,
      "step": 1279500
    },
    {
      "epoch": 49.487566229647676,
      "grad_norm": 11.76276683807373,
      "learning_rate": 8.760361475293602e-06,
      "loss": 1.8491,
      "step": 1279600
    },
    {
      "epoch": 49.49143365432958,
      "grad_norm": 12.798534393310547,
      "learning_rate": 8.757138621392015e-06,
      "loss": 1.83,
      "step": 1279700
    },
    {
      "epoch": 49.49530107901148,
      "grad_norm": 10.928646087646484,
      "learning_rate": 8.753915767490428e-06,
      "loss": 1.7978,
      "step": 1279800
    },
    {
      "epoch": 49.49916850369339,
      "grad_norm": 9.422891616821289,
      "learning_rate": 8.750692913588841e-06,
      "loss": 1.7963,
      "step": 1279900
    },
    {
      "epoch": 49.5030359283753,
      "grad_norm": 8.53649616241455,
      "learning_rate": 8.747470059687254e-06,
      "loss": 1.7734,
      "step": 1280000
    },
    {
      "epoch": 49.5069033530572,
      "grad_norm": 14.838547706604004,
      "learning_rate": 8.744247205785667e-06,
      "loss": 1.7572,
      "step": 1280100
    },
    {
      "epoch": 49.510770777739104,
      "grad_norm": 11.858528137207031,
      "learning_rate": 8.74102435188408e-06,
      "loss": 1.801,
      "step": 1280200
    },
    {
      "epoch": 49.51463820242101,
      "grad_norm": 10.931289672851562,
      "learning_rate": 8.737801497982493e-06,
      "loss": 1.8282,
      "step": 1280300
    },
    {
      "epoch": 49.51850562710291,
      "grad_norm": 10.379051208496094,
      "learning_rate": 8.734578644080906e-06,
      "loss": 1.773,
      "step": 1280400
    },
    {
      "epoch": 49.52237305178482,
      "grad_norm": 8.841608047485352,
      "learning_rate": 8.73135579017932e-06,
      "loss": 1.7855,
      "step": 1280500
    },
    {
      "epoch": 49.52624047646672,
      "grad_norm": 13.326818466186523,
      "learning_rate": 8.728132936277732e-06,
      "loss": 1.8327,
      "step": 1280600
    },
    {
      "epoch": 49.530107901148625,
      "grad_norm": 12.679143905639648,
      "learning_rate": 8.724910082376147e-06,
      "loss": 1.7276,
      "step": 1280700
    },
    {
      "epoch": 49.53397532583053,
      "grad_norm": 9.566214561462402,
      "learning_rate": 8.72168722847456e-06,
      "loss": 1.7928,
      "step": 1280800
    },
    {
      "epoch": 49.53784275051243,
      "grad_norm": 11.737614631652832,
      "learning_rate": 8.718464374572973e-06,
      "loss": 1.7669,
      "step": 1280900
    },
    {
      "epoch": 49.54171017519434,
      "grad_norm": 11.60303020477295,
      "learning_rate": 8.715241520671386e-06,
      "loss": 1.7657,
      "step": 1281000
    },
    {
      "epoch": 49.54557759987624,
      "grad_norm": 11.290372848510742,
      "learning_rate": 8.712018666769799e-06,
      "loss": 1.7558,
      "step": 1281100
    },
    {
      "epoch": 49.549445024558146,
      "grad_norm": 11.224745750427246,
      "learning_rate": 8.708795812868212e-06,
      "loss": 1.7286,
      "step": 1281200
    },
    {
      "epoch": 49.55331244924005,
      "grad_norm": 10.006084442138672,
      "learning_rate": 8.705572958966625e-06,
      "loss": 1.762,
      "step": 1281300
    },
    {
      "epoch": 49.55717987392195,
      "grad_norm": 12.60882568359375,
      "learning_rate": 8.702350105065038e-06,
      "loss": 1.7548,
      "step": 1281400
    },
    {
      "epoch": 49.56104729860386,
      "grad_norm": 13.26306438446045,
      "learning_rate": 8.699127251163451e-06,
      "loss": 1.8513,
      "step": 1281500
    },
    {
      "epoch": 49.56491472328577,
      "grad_norm": 12.046727180480957,
      "learning_rate": 8.695904397261864e-06,
      "loss": 1.7953,
      "step": 1281600
    },
    {
      "epoch": 49.56878214796767,
      "grad_norm": 10.225119590759277,
      "learning_rate": 8.692681543360277e-06,
      "loss": 1.8208,
      "step": 1281700
    },
    {
      "epoch": 49.572649572649574,
      "grad_norm": 10.154830932617188,
      "learning_rate": 8.68945868945869e-06,
      "loss": 1.7555,
      "step": 1281800
    },
    {
      "epoch": 49.576516997331474,
      "grad_norm": 11.136472702026367,
      "learning_rate": 8.686235835557103e-06,
      "loss": 1.9583,
      "step": 1281900
    },
    {
      "epoch": 49.58038442201338,
      "grad_norm": 11.922133445739746,
      "learning_rate": 8.683012981655516e-06,
      "loss": 1.7988,
      "step": 1282000
    },
    {
      "epoch": 49.58425184669529,
      "grad_norm": 13.768692016601562,
      "learning_rate": 8.679790127753928e-06,
      "loss": 1.7177,
      "step": 1282100
    },
    {
      "epoch": 49.58811927137719,
      "grad_norm": 11.520644187927246,
      "learning_rate": 8.67656727385234e-06,
      "loss": 1.8059,
      "step": 1282200
    },
    {
      "epoch": 49.591986696059095,
      "grad_norm": 10.146086692810059,
      "learning_rate": 8.673344419950755e-06,
      "loss": 1.7492,
      "step": 1282300
    },
    {
      "epoch": 49.595854120740995,
      "grad_norm": 9.700767517089844,
      "learning_rate": 8.670121566049168e-06,
      "loss": 1.7677,
      "step": 1282400
    },
    {
      "epoch": 49.5997215454229,
      "grad_norm": 12.546645164489746,
      "learning_rate": 8.666898712147581e-06,
      "loss": 1.833,
      "step": 1282500
    },
    {
      "epoch": 49.60358897010481,
      "grad_norm": 7.773950576782227,
      "learning_rate": 8.663675858245995e-06,
      "loss": 1.7898,
      "step": 1282600
    },
    {
      "epoch": 49.60745639478671,
      "grad_norm": 14.740257263183594,
      "learning_rate": 8.660453004344408e-06,
      "loss": 1.7194,
      "step": 1282700
    },
    {
      "epoch": 49.611323819468616,
      "grad_norm": 14.142844200134277,
      "learning_rate": 8.65723015044282e-06,
      "loss": 1.7362,
      "step": 1282800
    },
    {
      "epoch": 49.61519124415052,
      "grad_norm": 12.131237030029297,
      "learning_rate": 8.654007296541234e-06,
      "loss": 1.8945,
      "step": 1282900
    },
    {
      "epoch": 49.61905866883242,
      "grad_norm": 13.529861450195312,
      "learning_rate": 8.650784442639647e-06,
      "loss": 1.8796,
      "step": 1283000
    },
    {
      "epoch": 49.62292609351433,
      "grad_norm": 12.091815948486328,
      "learning_rate": 8.64756158873806e-06,
      "loss": 1.7916,
      "step": 1283100
    },
    {
      "epoch": 49.62679351819623,
      "grad_norm": 10.962944984436035,
      "learning_rate": 8.644338734836473e-06,
      "loss": 1.7645,
      "step": 1283200
    },
    {
      "epoch": 49.63066094287814,
      "grad_norm": 12.78221607208252,
      "learning_rate": 8.641115880934886e-06,
      "loss": 1.7207,
      "step": 1283300
    },
    {
      "epoch": 49.634528367560044,
      "grad_norm": 12.0001802444458,
      "learning_rate": 8.637893027033299e-06,
      "loss": 1.7848,
      "step": 1283400
    },
    {
      "epoch": 49.638395792241944,
      "grad_norm": 10.661733627319336,
      "learning_rate": 8.634670173131712e-06,
      "loss": 1.8041,
      "step": 1283500
    },
    {
      "epoch": 49.64226321692385,
      "grad_norm": 12.364304542541504,
      "learning_rate": 8.631447319230125e-06,
      "loss": 1.8584,
      "step": 1283600
    },
    {
      "epoch": 49.64613064160576,
      "grad_norm": 13.555927276611328,
      "learning_rate": 8.628224465328538e-06,
      "loss": 1.7424,
      "step": 1283700
    },
    {
      "epoch": 49.64999806628766,
      "grad_norm": 13.3892240524292,
      "learning_rate": 8.62500161142695e-06,
      "loss": 1.7892,
      "step": 1283800
    },
    {
      "epoch": 49.653865490969565,
      "grad_norm": 10.286876678466797,
      "learning_rate": 8.621778757525364e-06,
      "loss": 1.8649,
      "step": 1283900
    },
    {
      "epoch": 49.657732915651465,
      "grad_norm": 15.424966812133789,
      "learning_rate": 8.618555903623777e-06,
      "loss": 1.8556,
      "step": 1284000
    },
    {
      "epoch": 49.66160034033337,
      "grad_norm": 8.954610824584961,
      "learning_rate": 8.61533304972219e-06,
      "loss": 1.7912,
      "step": 1284100
    },
    {
      "epoch": 49.66546776501528,
      "grad_norm": 15.510574340820312,
      "learning_rate": 8.612110195820605e-06,
      "loss": 1.7887,
      "step": 1284200
    },
    {
      "epoch": 49.66933518969718,
      "grad_norm": 10.912696838378906,
      "learning_rate": 8.608887341919018e-06,
      "loss": 1.7888,
      "step": 1284300
    },
    {
      "epoch": 49.673202614379086,
      "grad_norm": 9.281779289245605,
      "learning_rate": 8.60566448801743e-06,
      "loss": 1.7926,
      "step": 1284400
    },
    {
      "epoch": 49.677070039060986,
      "grad_norm": 12.950828552246094,
      "learning_rate": 8.602441634115844e-06,
      "loss": 1.7811,
      "step": 1284500
    },
    {
      "epoch": 49.68093746374289,
      "grad_norm": 13.582372665405273,
      "learning_rate": 8.599218780214257e-06,
      "loss": 1.8166,
      "step": 1284600
    },
    {
      "epoch": 49.6848048884248,
      "grad_norm": 14.895984649658203,
      "learning_rate": 8.59599592631267e-06,
      "loss": 1.7705,
      "step": 1284700
    },
    {
      "epoch": 49.6886723131067,
      "grad_norm": 14.108123779296875,
      "learning_rate": 8.592773072411083e-06,
      "loss": 1.7389,
      "step": 1284800
    },
    {
      "epoch": 49.69253973778861,
      "grad_norm": 12.768099784851074,
      "learning_rate": 8.589550218509496e-06,
      "loss": 1.815,
      "step": 1284900
    },
    {
      "epoch": 49.696407162470514,
      "grad_norm": 12.263334274291992,
      "learning_rate": 8.586327364607907e-06,
      "loss": 1.9112,
      "step": 1285000
    },
    {
      "epoch": 49.700274587152414,
      "grad_norm": 13.578876495361328,
      "learning_rate": 8.58310451070632e-06,
      "loss": 1.8237,
      "step": 1285100
    },
    {
      "epoch": 49.70414201183432,
      "grad_norm": 12.901660919189453,
      "learning_rate": 8.579881656804733e-06,
      "loss": 1.7814,
      "step": 1285200
    },
    {
      "epoch": 49.70800943651622,
      "grad_norm": 13.361230850219727,
      "learning_rate": 8.576658802903146e-06,
      "loss": 1.8342,
      "step": 1285300
    },
    {
      "epoch": 49.71187686119813,
      "grad_norm": 12.698952674865723,
      "learning_rate": 8.57343594900156e-06,
      "loss": 1.8815,
      "step": 1285400
    },
    {
      "epoch": 49.715744285880035,
      "grad_norm": 13.23593521118164,
      "learning_rate": 8.570213095099972e-06,
      "loss": 1.8384,
      "step": 1285500
    },
    {
      "epoch": 49.719611710561935,
      "grad_norm": 10.577959060668945,
      "learning_rate": 8.566990241198385e-06,
      "loss": 1.773,
      "step": 1285600
    },
    {
      "epoch": 49.72347913524384,
      "grad_norm": 10.358240127563477,
      "learning_rate": 8.5637673872968e-06,
      "loss": 1.768,
      "step": 1285700
    },
    {
      "epoch": 49.72734655992574,
      "grad_norm": 9.18645191192627,
      "learning_rate": 8.560544533395213e-06,
      "loss": 1.7306,
      "step": 1285800
    },
    {
      "epoch": 49.73121398460765,
      "grad_norm": 13.525320053100586,
      "learning_rate": 8.557321679493626e-06,
      "loss": 1.7994,
      "step": 1285900
    },
    {
      "epoch": 49.735081409289556,
      "grad_norm": 10.385345458984375,
      "learning_rate": 8.554098825592039e-06,
      "loss": 1.8455,
      "step": 1286000
    },
    {
      "epoch": 49.738948833971456,
      "grad_norm": 11.967076301574707,
      "learning_rate": 8.550875971690452e-06,
      "loss": 1.7967,
      "step": 1286100
    },
    {
      "epoch": 49.74281625865336,
      "grad_norm": 15.787457466125488,
      "learning_rate": 8.547653117788865e-06,
      "loss": 1.8522,
      "step": 1286200
    },
    {
      "epoch": 49.74668368333527,
      "grad_norm": 11.376320838928223,
      "learning_rate": 8.544430263887278e-06,
      "loss": 1.8972,
      "step": 1286300
    },
    {
      "epoch": 49.75055110801717,
      "grad_norm": 12.166178703308105,
      "learning_rate": 8.541207409985691e-06,
      "loss": 1.8566,
      "step": 1286400
    },
    {
      "epoch": 49.75441853269908,
      "grad_norm": 13.495043754577637,
      "learning_rate": 8.537984556084104e-06,
      "loss": 1.7993,
      "step": 1286500
    },
    {
      "epoch": 49.75828595738098,
      "grad_norm": 12.535651206970215,
      "learning_rate": 8.534761702182517e-06,
      "loss": 1.8518,
      "step": 1286600
    },
    {
      "epoch": 49.762153382062884,
      "grad_norm": 13.591691017150879,
      "learning_rate": 8.53153884828093e-06,
      "loss": 1.7469,
      "step": 1286700
    },
    {
      "epoch": 49.76602080674479,
      "grad_norm": 16.460718154907227,
      "learning_rate": 8.528315994379343e-06,
      "loss": 1.7135,
      "step": 1286800
    },
    {
      "epoch": 49.76988823142669,
      "grad_norm": 13.041762351989746,
      "learning_rate": 8.525093140477756e-06,
      "loss": 1.8599,
      "step": 1286900
    },
    {
      "epoch": 49.7737556561086,
      "grad_norm": 13.059500694274902,
      "learning_rate": 8.52187028657617e-06,
      "loss": 1.7673,
      "step": 1287000
    },
    {
      "epoch": 49.777623080790505,
      "grad_norm": 10.227667808532715,
      "learning_rate": 8.518647432674582e-06,
      "loss": 1.7991,
      "step": 1287100
    },
    {
      "epoch": 49.781490505472405,
      "grad_norm": 16.70524787902832,
      "learning_rate": 8.515424578772995e-06,
      "loss": 1.743,
      "step": 1287200
    },
    {
      "epoch": 49.78535793015431,
      "grad_norm": 13.16814136505127,
      "learning_rate": 8.512201724871408e-06,
      "loss": 1.7635,
      "step": 1287300
    },
    {
      "epoch": 49.78922535483621,
      "grad_norm": 10.595602035522461,
      "learning_rate": 8.508978870969821e-06,
      "loss": 1.8074,
      "step": 1287400
    },
    {
      "epoch": 49.79309277951812,
      "grad_norm": 13.447660446166992,
      "learning_rate": 8.505756017068234e-06,
      "loss": 1.8087,
      "step": 1287500
    },
    {
      "epoch": 49.796960204200026,
      "grad_norm": 12.629005432128906,
      "learning_rate": 8.502533163166649e-06,
      "loss": 1.6876,
      "step": 1287600
    },
    {
      "epoch": 49.800827628881926,
      "grad_norm": 12.738394737243652,
      "learning_rate": 8.499310309265062e-06,
      "loss": 1.8656,
      "step": 1287700
    },
    {
      "epoch": 49.80469505356383,
      "grad_norm": 14.487298965454102,
      "learning_rate": 8.496087455363475e-06,
      "loss": 1.7483,
      "step": 1287800
    },
    {
      "epoch": 49.80856247824573,
      "grad_norm": 14.614264488220215,
      "learning_rate": 8.492864601461888e-06,
      "loss": 1.8681,
      "step": 1287900
    },
    {
      "epoch": 49.81242990292764,
      "grad_norm": 12.40341854095459,
      "learning_rate": 8.4896417475603e-06,
      "loss": 1.7464,
      "step": 1288000
    },
    {
      "epoch": 49.81629732760955,
      "grad_norm": 14.405606269836426,
      "learning_rate": 8.486418893658713e-06,
      "loss": 1.746,
      "step": 1288100
    },
    {
      "epoch": 49.82016475229145,
      "grad_norm": 10.78933334350586,
      "learning_rate": 8.483196039757126e-06,
      "loss": 1.8221,
      "step": 1288200
    },
    {
      "epoch": 49.824032176973354,
      "grad_norm": 15.554616928100586,
      "learning_rate": 8.479973185855539e-06,
      "loss": 1.8893,
      "step": 1288300
    },
    {
      "epoch": 49.82789960165526,
      "grad_norm": 9.715903282165527,
      "learning_rate": 8.476750331953952e-06,
      "loss": 1.7222,
      "step": 1288400
    },
    {
      "epoch": 49.83176702633716,
      "grad_norm": 10.23103141784668,
      "learning_rate": 8.473527478052365e-06,
      "loss": 1.7475,
      "step": 1288500
    },
    {
      "epoch": 49.83563445101907,
      "grad_norm": 8.896587371826172,
      "learning_rate": 8.470304624150778e-06,
      "loss": 1.7794,
      "step": 1288600
    },
    {
      "epoch": 49.83950187570097,
      "grad_norm": 12.910001754760742,
      "learning_rate": 8.46708177024919e-06,
      "loss": 1.8107,
      "step": 1288700
    },
    {
      "epoch": 49.843369300382875,
      "grad_norm": 11.709638595581055,
      "learning_rate": 8.463858916347604e-06,
      "loss": 1.7519,
      "step": 1288800
    },
    {
      "epoch": 49.84723672506478,
      "grad_norm": 12.693419456481934,
      "learning_rate": 8.460636062446017e-06,
      "loss": 1.8219,
      "step": 1288900
    },
    {
      "epoch": 49.85110414974668,
      "grad_norm": 13.559419631958008,
      "learning_rate": 8.45741320854443e-06,
      "loss": 1.7933,
      "step": 1289000
    },
    {
      "epoch": 49.85497157442859,
      "grad_norm": 12.860745429992676,
      "learning_rate": 8.454190354642843e-06,
      "loss": 1.7933,
      "step": 1289100
    },
    {
      "epoch": 49.85883899911049,
      "grad_norm": 20.135818481445312,
      "learning_rate": 8.450967500741257e-06,
      "loss": 1.7906,
      "step": 1289200
    },
    {
      "epoch": 49.862706423792396,
      "grad_norm": 10.012063026428223,
      "learning_rate": 8.44774464683967e-06,
      "loss": 1.761,
      "step": 1289300
    },
    {
      "epoch": 49.8665738484743,
      "grad_norm": 16.603435516357422,
      "learning_rate": 8.444521792938084e-06,
      "loss": 1.7402,
      "step": 1289400
    },
    {
      "epoch": 49.8704412731562,
      "grad_norm": 15.469866752624512,
      "learning_rate": 8.441298939036497e-06,
      "loss": 1.7308,
      "step": 1289500
    },
    {
      "epoch": 49.87430869783811,
      "grad_norm": 16.804113388061523,
      "learning_rate": 8.43807608513491e-06,
      "loss": 1.8167,
      "step": 1289600
    },
    {
      "epoch": 49.87817612252002,
      "grad_norm": 12.302531242370605,
      "learning_rate": 8.434853231233323e-06,
      "loss": 1.8844,
      "step": 1289700
    },
    {
      "epoch": 49.88204354720192,
      "grad_norm": 13.345871925354004,
      "learning_rate": 8.431630377331736e-06,
      "loss": 1.7264,
      "step": 1289800
    },
    {
      "epoch": 49.885910971883824,
      "grad_norm": 10.316473007202148,
      "learning_rate": 8.428407523430149e-06,
      "loss": 1.7722,
      "step": 1289900
    },
    {
      "epoch": 49.889778396565724,
      "grad_norm": 10.681754112243652,
      "learning_rate": 8.425184669528562e-06,
      "loss": 1.8889,
      "step": 1290000
    },
    {
      "epoch": 49.89364582124763,
      "grad_norm": 11.784391403198242,
      "learning_rate": 8.421961815626975e-06,
      "loss": 1.7454,
      "step": 1290100
    },
    {
      "epoch": 49.89751324592954,
      "grad_norm": 12.875846862792969,
      "learning_rate": 8.418738961725388e-06,
      "loss": 1.7969,
      "step": 1290200
    },
    {
      "epoch": 49.90138067061144,
      "grad_norm": 6.390954494476318,
      "learning_rate": 8.4155161078238e-06,
      "loss": 1.7066,
      "step": 1290300
    },
    {
      "epoch": 49.905248095293345,
      "grad_norm": 11.443594932556152,
      "learning_rate": 8.412293253922214e-06,
      "loss": 1.7176,
      "step": 1290400
    },
    {
      "epoch": 49.909115519975245,
      "grad_norm": 9.567654609680176,
      "learning_rate": 8.409070400020627e-06,
      "loss": 1.7605,
      "step": 1290500
    },
    {
      "epoch": 49.91298294465715,
      "grad_norm": 13.347982406616211,
      "learning_rate": 8.40584754611904e-06,
      "loss": 1.8577,
      "step": 1290600
    },
    {
      "epoch": 49.91685036933906,
      "grad_norm": 12.061030387878418,
      "learning_rate": 8.402624692217453e-06,
      "loss": 1.8497,
      "step": 1290700
    },
    {
      "epoch": 49.92071779402096,
      "grad_norm": 11.9843168258667,
      "learning_rate": 8.399401838315866e-06,
      "loss": 1.7357,
      "step": 1290800
    },
    {
      "epoch": 49.924585218702866,
      "grad_norm": 11.60423755645752,
      "learning_rate": 8.396178984414279e-06,
      "loss": 1.8394,
      "step": 1290900
    },
    {
      "epoch": 49.92845264338477,
      "grad_norm": 11.736169815063477,
      "learning_rate": 8.392956130512692e-06,
      "loss": 1.8853,
      "step": 1291000
    },
    {
      "epoch": 49.93232006806667,
      "grad_norm": 10.580578804016113,
      "learning_rate": 8.389733276611105e-06,
      "loss": 1.7944,
      "step": 1291100
    },
    {
      "epoch": 49.93618749274858,
      "grad_norm": 11.712753295898438,
      "learning_rate": 8.386510422709518e-06,
      "loss": 1.8738,
      "step": 1291200
    },
    {
      "epoch": 49.94005491743048,
      "grad_norm": 14.389713287353516,
      "learning_rate": 8.383287568807931e-06,
      "loss": 1.7988,
      "step": 1291300
    },
    {
      "epoch": 49.94392234211239,
      "grad_norm": 14.708620071411133,
      "learning_rate": 8.380064714906344e-06,
      "loss": 1.8266,
      "step": 1291400
    },
    {
      "epoch": 49.947789766794294,
      "grad_norm": 12.076498031616211,
      "learning_rate": 8.376841861004757e-06,
      "loss": 1.7783,
      "step": 1291500
    },
    {
      "epoch": 49.951657191476194,
      "grad_norm": 8.065112113952637,
      "learning_rate": 8.37361900710317e-06,
      "loss": 1.8197,
      "step": 1291600
    },
    {
      "epoch": 49.9555246161581,
      "grad_norm": 11.486394882202148,
      "learning_rate": 8.370396153201583e-06,
      "loss": 1.9087,
      "step": 1291700
    },
    {
      "epoch": 49.95939204084001,
      "grad_norm": 10.771191596984863,
      "learning_rate": 8.367173299299996e-06,
      "loss": 1.7683,
      "step": 1291800
    },
    {
      "epoch": 49.96325946552191,
      "grad_norm": 13.486590385437012,
      "learning_rate": 8.36395044539841e-06,
      "loss": 1.812,
      "step": 1291900
    },
    {
      "epoch": 49.967126890203815,
      "grad_norm": 8.991011619567871,
      "learning_rate": 8.360727591496822e-06,
      "loss": 1.7717,
      "step": 1292000
    },
    {
      "epoch": 49.970994314885715,
      "grad_norm": 12.310742378234863,
      "learning_rate": 8.357504737595235e-06,
      "loss": 1.8022,
      "step": 1292100
    },
    {
      "epoch": 49.97486173956762,
      "grad_norm": 14.153706550598145,
      "learning_rate": 8.354281883693648e-06,
      "loss": 1.794,
      "step": 1292200
    },
    {
      "epoch": 49.97872916424953,
      "grad_norm": 16.1389102935791,
      "learning_rate": 8.351059029792061e-06,
      "loss": 1.7444,
      "step": 1292300
    },
    {
      "epoch": 49.98259658893143,
      "grad_norm": 10.435050964355469,
      "learning_rate": 8.347836175890474e-06,
      "loss": 1.7546,
      "step": 1292400
    },
    {
      "epoch": 49.986464013613336,
      "grad_norm": 11.320116996765137,
      "learning_rate": 8.344613321988887e-06,
      "loss": 1.8081,
      "step": 1292500
    },
    {
      "epoch": 49.990331438295236,
      "grad_norm": 11.151890754699707,
      "learning_rate": 8.341390468087302e-06,
      "loss": 1.8012,
      "step": 1292600
    },
    {
      "epoch": 49.99419886297714,
      "grad_norm": 13.584832191467285,
      "learning_rate": 8.338167614185715e-06,
      "loss": 1.731,
      "step": 1292700
    },
    {
      "epoch": 49.99806628765905,
      "grad_norm": 14.55721378326416,
      "learning_rate": 8.334944760284128e-06,
      "loss": 1.7519,
      "step": 1292800
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.7475475072860718,
      "eval_runtime": 5.6411,
      "eval_samples_per_second": 241.265,
      "eval_steps_per_second": 241.265,
      "step": 1292850
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.5837727785110474,
      "eval_runtime": 108.0503,
      "eval_samples_per_second": 239.305,
      "eval_steps_per_second": 239.305,
      "step": 1292850
    },
    {
      "epoch": 50.00193371234095,
      "grad_norm": 10.451065063476562,
      "learning_rate": 8.331721906382541e-06,
      "loss": 1.7355,
      "step": 1292900
    },
    {
      "epoch": 50.00580113702286,
      "grad_norm": 10.551837921142578,
      "learning_rate": 8.328499052480954e-06,
      "loss": 1.9028,
      "step": 1293000
    },
    {
      "epoch": 50.009668561704764,
      "grad_norm": 13.89844036102295,
      "learning_rate": 8.325276198579367e-06,
      "loss": 1.8513,
      "step": 1293100
    },
    {
      "epoch": 50.013535986386664,
      "grad_norm": 10.457571029663086,
      "learning_rate": 8.32205334467778e-06,
      "loss": 1.7681,
      "step": 1293200
    },
    {
      "epoch": 50.01740341106857,
      "grad_norm": 11.00709342956543,
      "learning_rate": 8.318830490776193e-06,
      "loss": 1.7737,
      "step": 1293300
    },
    {
      "epoch": 50.02127083575047,
      "grad_norm": 13.021347999572754,
      "learning_rate": 8.315607636874606e-06,
      "loss": 1.7329,
      "step": 1293400
    },
    {
      "epoch": 50.02513826043238,
      "grad_norm": 8.701802253723145,
      "learning_rate": 8.31238478297302e-06,
      "loss": 1.8491,
      "step": 1293500
    },
    {
      "epoch": 50.029005685114285,
      "grad_norm": 13.296614646911621,
      "learning_rate": 8.309161929071432e-06,
      "loss": 1.7891,
      "step": 1293600
    },
    {
      "epoch": 50.032873109796185,
      "grad_norm": 15.428035736083984,
      "learning_rate": 8.305939075169845e-06,
      "loss": 1.8133,
      "step": 1293700
    },
    {
      "epoch": 50.03674053447809,
      "grad_norm": 9.121746063232422,
      "learning_rate": 8.302716221268258e-06,
      "loss": 1.7952,
      "step": 1293800
    },
    {
      "epoch": 50.04060795915999,
      "grad_norm": 13.477851867675781,
      "learning_rate": 8.29949336736667e-06,
      "loss": 1.7664,
      "step": 1293900
    },
    {
      "epoch": 50.0444753838419,
      "grad_norm": 13.87256145477295,
      "learning_rate": 8.296270513465083e-06,
      "loss": 1.7885,
      "step": 1294000
    },
    {
      "epoch": 50.048342808523806,
      "grad_norm": 12.382068634033203,
      "learning_rate": 8.293047659563496e-06,
      "loss": 1.8968,
      "step": 1294100
    },
    {
      "epoch": 50.052210233205706,
      "grad_norm": 9.990071296691895,
      "learning_rate": 8.28982480566191e-06,
      "loss": 1.7266,
      "step": 1294200
    },
    {
      "epoch": 50.05607765788761,
      "grad_norm": 12.813508987426758,
      "learning_rate": 8.286601951760323e-06,
      "loss": 1.7351,
      "step": 1294300
    },
    {
      "epoch": 50.05994508256952,
      "grad_norm": 13.557053565979004,
      "learning_rate": 8.283379097858736e-06,
      "loss": 1.7722,
      "step": 1294400
    },
    {
      "epoch": 50.06381250725142,
      "grad_norm": 11.974370956420898,
      "learning_rate": 8.28015624395715e-06,
      "loss": 1.7913,
      "step": 1294500
    },
    {
      "epoch": 50.06767993193333,
      "grad_norm": 13.242315292358398,
      "learning_rate": 8.276933390055563e-06,
      "loss": 1.861,
      "step": 1294600
    },
    {
      "epoch": 50.07154735661523,
      "grad_norm": 11.974233627319336,
      "learning_rate": 8.273710536153976e-06,
      "loss": 1.7965,
      "step": 1294700
    },
    {
      "epoch": 50.075414781297134,
      "grad_norm": 13.362410545349121,
      "learning_rate": 8.270487682252389e-06,
      "loss": 1.7287,
      "step": 1294800
    },
    {
      "epoch": 50.07928220597904,
      "grad_norm": 11.280326843261719,
      "learning_rate": 8.267264828350802e-06,
      "loss": 1.8591,
      "step": 1294900
    },
    {
      "epoch": 50.08314963066094,
      "grad_norm": 12.282825469970703,
      "learning_rate": 8.264041974449215e-06,
      "loss": 1.7677,
      "step": 1295000
    },
    {
      "epoch": 50.08701705534285,
      "grad_norm": 13.806289672851562,
      "learning_rate": 8.260819120547628e-06,
      "loss": 1.7634,
      "step": 1295100
    },
    {
      "epoch": 50.090884480024755,
      "grad_norm": 14.629940032958984,
      "learning_rate": 8.25759626664604e-06,
      "loss": 1.6644,
      "step": 1295200
    },
    {
      "epoch": 50.094751904706655,
      "grad_norm": 11.736238479614258,
      "learning_rate": 8.254373412744454e-06,
      "loss": 1.7946,
      "step": 1295300
    },
    {
      "epoch": 50.09861932938856,
      "grad_norm": 10.922201156616211,
      "learning_rate": 8.251150558842867e-06,
      "loss": 1.7934,
      "step": 1295400
    },
    {
      "epoch": 50.10248675407046,
      "grad_norm": 11.91590690612793,
      "learning_rate": 8.24792770494128e-06,
      "loss": 1.8148,
      "step": 1295500
    },
    {
      "epoch": 50.10635417875237,
      "grad_norm": 12.059256553649902,
      "learning_rate": 8.244704851039693e-06,
      "loss": 1.7372,
      "step": 1295600
    },
    {
      "epoch": 50.110221603434276,
      "grad_norm": 12.080805778503418,
      "learning_rate": 8.241481997138106e-06,
      "loss": 1.7674,
      "step": 1295700
    },
    {
      "epoch": 50.114089028116176,
      "grad_norm": 13.965932846069336,
      "learning_rate": 8.238259143236519e-06,
      "loss": 1.8514,
      "step": 1295800
    },
    {
      "epoch": 50.11795645279808,
      "grad_norm": 4.388583183288574,
      "learning_rate": 8.235036289334932e-06,
      "loss": 1.769,
      "step": 1295900
    },
    {
      "epoch": 50.12182387747998,
      "grad_norm": 10.32792854309082,
      "learning_rate": 8.231813435433345e-06,
      "loss": 1.8373,
      "step": 1296000
    },
    {
      "epoch": 50.12569130216189,
      "grad_norm": 11.582305908203125,
      "learning_rate": 8.22859058153176e-06,
      "loss": 1.7715,
      "step": 1296100
    },
    {
      "epoch": 50.1295587268438,
      "grad_norm": 11.195649147033691,
      "learning_rate": 8.225367727630173e-06,
      "loss": 1.7685,
      "step": 1296200
    },
    {
      "epoch": 50.1334261515257,
      "grad_norm": 13.163822174072266,
      "learning_rate": 8.222144873728586e-06,
      "loss": 1.7326,
      "step": 1296300
    },
    {
      "epoch": 50.137293576207604,
      "grad_norm": 10.705710411071777,
      "learning_rate": 8.218922019826999e-06,
      "loss": 1.7281,
      "step": 1296400
    },
    {
      "epoch": 50.14116100088951,
      "grad_norm": 15.650324821472168,
      "learning_rate": 8.215699165925412e-06,
      "loss": 1.776,
      "step": 1296500
    },
    {
      "epoch": 50.14502842557141,
      "grad_norm": 9.302960395812988,
      "learning_rate": 8.212476312023825e-06,
      "loss": 1.7873,
      "step": 1296600
    },
    {
      "epoch": 50.14889585025332,
      "grad_norm": 10.316143989562988,
      "learning_rate": 8.209253458122238e-06,
      "loss": 1.8773,
      "step": 1296700
    },
    {
      "epoch": 50.15276327493522,
      "grad_norm": 12.887072563171387,
      "learning_rate": 8.206030604220649e-06,
      "loss": 1.8242,
      "step": 1296800
    },
    {
      "epoch": 50.156630699617125,
      "grad_norm": 14.068628311157227,
      "learning_rate": 8.202807750319062e-06,
      "loss": 1.8182,
      "step": 1296900
    },
    {
      "epoch": 50.16049812429903,
      "grad_norm": 11.817030906677246,
      "learning_rate": 8.199584896417475e-06,
      "loss": 1.774,
      "step": 1297000
    },
    {
      "epoch": 50.16436554898093,
      "grad_norm": 11.889317512512207,
      "learning_rate": 8.196362042515888e-06,
      "loss": 1.8009,
      "step": 1297100
    },
    {
      "epoch": 50.16823297366284,
      "grad_norm": 16.100345611572266,
      "learning_rate": 8.193139188614301e-06,
      "loss": 1.7646,
      "step": 1297200
    },
    {
      "epoch": 50.17210039834474,
      "grad_norm": 12.819605827331543,
      "learning_rate": 8.189916334712714e-06,
      "loss": 1.7926,
      "step": 1297300
    },
    {
      "epoch": 50.175967823026646,
      "grad_norm": 13.258247375488281,
      "learning_rate": 8.186693480811127e-06,
      "loss": 1.6789,
      "step": 1297400
    },
    {
      "epoch": 50.17983524770855,
      "grad_norm": 11.684425354003906,
      "learning_rate": 8.18347062690954e-06,
      "loss": 1.7937,
      "step": 1297500
    },
    {
      "epoch": 50.18370267239045,
      "grad_norm": 12.638559341430664,
      "learning_rate": 8.180247773007955e-06,
      "loss": 1.7593,
      "step": 1297600
    },
    {
      "epoch": 50.18757009707236,
      "grad_norm": 13.666364669799805,
      "learning_rate": 8.177024919106368e-06,
      "loss": 1.7978,
      "step": 1297700
    },
    {
      "epoch": 50.19143752175427,
      "grad_norm": 9.848690032958984,
      "learning_rate": 8.173802065204781e-06,
      "loss": 1.8135,
      "step": 1297800
    },
    {
      "epoch": 50.19530494643617,
      "grad_norm": 13.296958923339844,
      "learning_rate": 8.170579211303194e-06,
      "loss": 1.7824,
      "step": 1297900
    },
    {
      "epoch": 50.199172371118074,
      "grad_norm": 13.435501098632812,
      "learning_rate": 8.167356357401607e-06,
      "loss": 1.7961,
      "step": 1298000
    },
    {
      "epoch": 50.203039795799974,
      "grad_norm": 10.57929515838623,
      "learning_rate": 8.16413350350002e-06,
      "loss": 1.854,
      "step": 1298100
    },
    {
      "epoch": 50.20690722048188,
      "grad_norm": 10.777778625488281,
      "learning_rate": 8.160910649598433e-06,
      "loss": 1.8351,
      "step": 1298200
    },
    {
      "epoch": 50.21077464516379,
      "grad_norm": 13.79174518585205,
      "learning_rate": 8.157687795696846e-06,
      "loss": 1.758,
      "step": 1298300
    },
    {
      "epoch": 50.21464206984569,
      "grad_norm": 10.341767311096191,
      "learning_rate": 8.154464941795259e-06,
      "loss": 1.7873,
      "step": 1298400
    },
    {
      "epoch": 50.218509494527595,
      "grad_norm": 12.297232627868652,
      "learning_rate": 8.151242087893672e-06,
      "loss": 1.7668,
      "step": 1298500
    },
    {
      "epoch": 50.222376919209495,
      "grad_norm": 14.344964981079102,
      "learning_rate": 8.148019233992085e-06,
      "loss": 1.7402,
      "step": 1298600
    },
    {
      "epoch": 50.2262443438914,
      "grad_norm": 18.098514556884766,
      "learning_rate": 8.144796380090498e-06,
      "loss": 1.7482,
      "step": 1298700
    },
    {
      "epoch": 50.23011176857331,
      "grad_norm": 10.821061134338379,
      "learning_rate": 8.141573526188911e-06,
      "loss": 1.8165,
      "step": 1298800
    },
    {
      "epoch": 50.23397919325521,
      "grad_norm": 13.906665802001953,
      "learning_rate": 8.138350672287324e-06,
      "loss": 1.871,
      "step": 1298900
    },
    {
      "epoch": 50.237846617937116,
      "grad_norm": 11.972471237182617,
      "learning_rate": 8.135127818385737e-06,
      "loss": 1.7727,
      "step": 1299000
    },
    {
      "epoch": 50.24171404261902,
      "grad_norm": 13.684951782226562,
      "learning_rate": 8.13190496448415e-06,
      "loss": 1.8006,
      "step": 1299100
    },
    {
      "epoch": 50.24558146730092,
      "grad_norm": 10.749690055847168,
      "learning_rate": 8.128682110582563e-06,
      "loss": 1.735,
      "step": 1299200
    },
    {
      "epoch": 50.24944889198283,
      "grad_norm": 15.621554374694824,
      "learning_rate": 8.125459256680976e-06,
      "loss": 1.8534,
      "step": 1299300
    },
    {
      "epoch": 50.25331631666473,
      "grad_norm": 12.793590545654297,
      "learning_rate": 8.12223640277939e-06,
      "loss": 1.7748,
      "step": 1299400
    },
    {
      "epoch": 50.25718374134664,
      "grad_norm": 12.018452644348145,
      "learning_rate": 8.119013548877804e-06,
      "loss": 1.7465,
      "step": 1299500
    },
    {
      "epoch": 50.261051166028544,
      "grad_norm": 13.778803825378418,
      "learning_rate": 8.115790694976217e-06,
      "loss": 1.7794,
      "step": 1299600
    },
    {
      "epoch": 50.264918590710444,
      "grad_norm": 11.947193145751953,
      "learning_rate": 8.11256784107463e-06,
      "loss": 1.8332,
      "step": 1299700
    },
    {
      "epoch": 50.26878601539235,
      "grad_norm": 12.51236629486084,
      "learning_rate": 8.109344987173041e-06,
      "loss": 1.6878,
      "step": 1299800
    },
    {
      "epoch": 50.27265344007426,
      "grad_norm": 12.940629959106445,
      "learning_rate": 8.106122133271454e-06,
      "loss": 1.8484,
      "step": 1299900
    },
    {
      "epoch": 50.27652086475616,
      "grad_norm": 10.04556655883789,
      "learning_rate": 8.102899279369868e-06,
      "loss": 1.8141,
      "step": 1300000
    },
    {
      "epoch": 50.280388289438065,
      "grad_norm": 13.414278984069824,
      "learning_rate": 8.09967642546828e-06,
      "loss": 1.6878,
      "step": 1300100
    },
    {
      "epoch": 50.284255714119965,
      "grad_norm": 11.32381534576416,
      "learning_rate": 8.096453571566694e-06,
      "loss": 1.7628,
      "step": 1300200
    },
    {
      "epoch": 50.28812313880187,
      "grad_norm": 13.922839164733887,
      "learning_rate": 8.093230717665107e-06,
      "loss": 1.785,
      "step": 1300300
    },
    {
      "epoch": 50.29199056348378,
      "grad_norm": 15.647114753723145,
      "learning_rate": 8.09000786376352e-06,
      "loss": 1.7878,
      "step": 1300400
    },
    {
      "epoch": 50.29585798816568,
      "grad_norm": 14.88891315460205,
      "learning_rate": 8.086785009861933e-06,
      "loss": 1.743,
      "step": 1300500
    },
    {
      "epoch": 50.299725412847586,
      "grad_norm": 13.011018753051758,
      "learning_rate": 8.083562155960346e-06,
      "loss": 1.8108,
      "step": 1300600
    },
    {
      "epoch": 50.303592837529486,
      "grad_norm": 13.149953842163086,
      "learning_rate": 8.080339302058759e-06,
      "loss": 1.814,
      "step": 1300700
    },
    {
      "epoch": 50.30746026221139,
      "grad_norm": 16.1571102142334,
      "learning_rate": 8.077116448157172e-06,
      "loss": 1.7595,
      "step": 1300800
    },
    {
      "epoch": 50.3113276868933,
      "grad_norm": 15.091872215270996,
      "learning_rate": 8.073893594255585e-06,
      "loss": 1.7669,
      "step": 1300900
    },
    {
      "epoch": 50.3151951115752,
      "grad_norm": 11.439638137817383,
      "learning_rate": 8.070670740353998e-06,
      "loss": 1.8299,
      "step": 1301000
    },
    {
      "epoch": 50.31906253625711,
      "grad_norm": 11.508183479309082,
      "learning_rate": 8.067447886452412e-06,
      "loss": 1.7325,
      "step": 1301100
    },
    {
      "epoch": 50.322929960939014,
      "grad_norm": 11.607779502868652,
      "learning_rate": 8.064225032550825e-06,
      "loss": 1.8216,
      "step": 1301200
    },
    {
      "epoch": 50.326797385620914,
      "grad_norm": 12.977339744567871,
      "learning_rate": 8.061002178649239e-06,
      "loss": 1.7244,
      "step": 1301300
    },
    {
      "epoch": 50.33066481030282,
      "grad_norm": 10.764236450195312,
      "learning_rate": 8.057779324747652e-06,
      "loss": 1.75,
      "step": 1301400
    },
    {
      "epoch": 50.33453223498472,
      "grad_norm": 10.292213439941406,
      "learning_rate": 8.054556470846065e-06,
      "loss": 1.8202,
      "step": 1301500
    },
    {
      "epoch": 50.33839965966663,
      "grad_norm": 14.15239429473877,
      "learning_rate": 8.051333616944478e-06,
      "loss": 1.8171,
      "step": 1301600
    },
    {
      "epoch": 50.342267084348535,
      "grad_norm": 16.61275291442871,
      "learning_rate": 8.04811076304289e-06,
      "loss": 1.8616,
      "step": 1301700
    },
    {
      "epoch": 50.346134509030435,
      "grad_norm": 8.862264633178711,
      "learning_rate": 8.044887909141304e-06,
      "loss": 1.8523,
      "step": 1301800
    },
    {
      "epoch": 50.35000193371234,
      "grad_norm": 11.451436042785645,
      "learning_rate": 8.041665055239717e-06,
      "loss": 1.714,
      "step": 1301900
    },
    {
      "epoch": 50.35386935839424,
      "grad_norm": 12.20549201965332,
      "learning_rate": 8.03844220133813e-06,
      "loss": 1.7202,
      "step": 1302000
    },
    {
      "epoch": 50.35773678307615,
      "grad_norm": 13.82379150390625,
      "learning_rate": 8.035219347436543e-06,
      "loss": 1.7598,
      "step": 1302100
    },
    {
      "epoch": 50.361604207758056,
      "grad_norm": 11.729463577270508,
      "learning_rate": 8.031996493534956e-06,
      "loss": 1.8264,
      "step": 1302200
    },
    {
      "epoch": 50.365471632439956,
      "grad_norm": 12.301169395446777,
      "learning_rate": 8.028773639633369e-06,
      "loss": 1.8102,
      "step": 1302300
    },
    {
      "epoch": 50.36933905712186,
      "grad_norm": 11.256171226501465,
      "learning_rate": 8.025550785731782e-06,
      "loss": 1.7473,
      "step": 1302400
    },
    {
      "epoch": 50.37320648180377,
      "grad_norm": 14.718952178955078,
      "learning_rate": 8.022327931830195e-06,
      "loss": 1.8104,
      "step": 1302500
    },
    {
      "epoch": 50.37707390648567,
      "grad_norm": 13.424220085144043,
      "learning_rate": 8.019105077928608e-06,
      "loss": 1.8727,
      "step": 1302600
    },
    {
      "epoch": 50.38094133116758,
      "grad_norm": 13.392986297607422,
      "learning_rate": 8.015882224027021e-06,
      "loss": 1.876,
      "step": 1302700
    },
    {
      "epoch": 50.38480875584948,
      "grad_norm": 12.49439811706543,
      "learning_rate": 8.012659370125434e-06,
      "loss": 1.7693,
      "step": 1302800
    },
    {
      "epoch": 50.388676180531384,
      "grad_norm": 13.457303047180176,
      "learning_rate": 8.009436516223847e-06,
      "loss": 1.8596,
      "step": 1302900
    },
    {
      "epoch": 50.39254360521329,
      "grad_norm": 14.686450004577637,
      "learning_rate": 8.00621366232226e-06,
      "loss": 1.8355,
      "step": 1303000
    },
    {
      "epoch": 50.39641102989519,
      "grad_norm": 11.856130599975586,
      "learning_rate": 8.002990808420673e-06,
      "loss": 1.8505,
      "step": 1303100
    },
    {
      "epoch": 50.4002784545771,
      "grad_norm": 13.254118919372559,
      "learning_rate": 7.999767954519086e-06,
      "loss": 1.7451,
      "step": 1303200
    },
    {
      "epoch": 50.404145879259005,
      "grad_norm": 9.65589714050293,
      "learning_rate": 7.996545100617499e-06,
      "loss": 1.8102,
      "step": 1303300
    },
    {
      "epoch": 50.408013303940905,
      "grad_norm": 11.468361854553223,
      "learning_rate": 7.993322246715912e-06,
      "loss": 1.8888,
      "step": 1303400
    },
    {
      "epoch": 50.41188072862281,
      "grad_norm": 11.76318359375,
      "learning_rate": 7.990099392814325e-06,
      "loss": 1.7523,
      "step": 1303500
    },
    {
      "epoch": 50.41574815330471,
      "grad_norm": 11.595354080200195,
      "learning_rate": 7.986876538912738e-06,
      "loss": 1.7313,
      "step": 1303600
    },
    {
      "epoch": 50.41961557798662,
      "grad_norm": 12.526001930236816,
      "learning_rate": 7.983653685011151e-06,
      "loss": 1.8157,
      "step": 1303700
    },
    {
      "epoch": 50.423483002668526,
      "grad_norm": 10.428679466247559,
      "learning_rate": 7.980430831109564e-06,
      "loss": 1.7849,
      "step": 1303800
    },
    {
      "epoch": 50.427350427350426,
      "grad_norm": 14.528125762939453,
      "learning_rate": 7.977207977207977e-06,
      "loss": 1.7396,
      "step": 1303900
    },
    {
      "epoch": 50.43121785203233,
      "grad_norm": 14.265050888061523,
      "learning_rate": 7.97398512330639e-06,
      "loss": 1.873,
      "step": 1304000
    },
    {
      "epoch": 50.43508527671423,
      "grad_norm": 13.637796401977539,
      "learning_rate": 7.970762269404803e-06,
      "loss": 1.73,
      "step": 1304100
    },
    {
      "epoch": 50.43895270139614,
      "grad_norm": 14.498505592346191,
      "learning_rate": 7.967539415503216e-06,
      "loss": 1.7584,
      "step": 1304200
    },
    {
      "epoch": 50.44282012607805,
      "grad_norm": 12.920825958251953,
      "learning_rate": 7.96431656160163e-06,
      "loss": 1.8487,
      "step": 1304300
    },
    {
      "epoch": 50.44668755075995,
      "grad_norm": 9.637287139892578,
      "learning_rate": 7.961093707700042e-06,
      "loss": 1.7572,
      "step": 1304400
    },
    {
      "epoch": 50.450554975441854,
      "grad_norm": 12.911958694458008,
      "learning_rate": 7.957870853798457e-06,
      "loss": 1.7316,
      "step": 1304500
    },
    {
      "epoch": 50.45442240012376,
      "grad_norm": 10.612021446228027,
      "learning_rate": 7.95464799989687e-06,
      "loss": 1.8448,
      "step": 1304600
    },
    {
      "epoch": 50.45828982480566,
      "grad_norm": 10.797438621520996,
      "learning_rate": 7.951425145995283e-06,
      "loss": 1.7797,
      "step": 1304700
    },
    {
      "epoch": 50.46215724948757,
      "grad_norm": 12.290984153747559,
      "learning_rate": 7.948202292093696e-06,
      "loss": 1.7602,
      "step": 1304800
    },
    {
      "epoch": 50.46602467416947,
      "grad_norm": 11.633898735046387,
      "learning_rate": 7.944979438192109e-06,
      "loss": 1.7122,
      "step": 1304900
    },
    {
      "epoch": 50.469892098851375,
      "grad_norm": 12.966029167175293,
      "learning_rate": 7.941756584290522e-06,
      "loss": 1.8063,
      "step": 1305000
    },
    {
      "epoch": 50.47375952353328,
      "grad_norm": 12.37587833404541,
      "learning_rate": 7.938533730388935e-06,
      "loss": 1.8143,
      "step": 1305100
    },
    {
      "epoch": 50.47762694821518,
      "grad_norm": 13.434020042419434,
      "learning_rate": 7.935310876487348e-06,
      "loss": 1.742,
      "step": 1305200
    },
    {
      "epoch": 50.48149437289709,
      "grad_norm": 14.29024887084961,
      "learning_rate": 7.932088022585761e-06,
      "loss": 1.6551,
      "step": 1305300
    },
    {
      "epoch": 50.48536179757899,
      "grad_norm": 12.593907356262207,
      "learning_rate": 7.928865168684174e-06,
      "loss": 1.7237,
      "step": 1305400
    },
    {
      "epoch": 50.489229222260896,
      "grad_norm": 9.652886390686035,
      "learning_rate": 7.925642314782587e-06,
      "loss": 1.9076,
      "step": 1305500
    },
    {
      "epoch": 50.4930966469428,
      "grad_norm": 14.858553886413574,
      "learning_rate": 7.922419460881e-06,
      "loss": 1.7657,
      "step": 1305600
    },
    {
      "epoch": 50.4969640716247,
      "grad_norm": 13.160749435424805,
      "learning_rate": 7.919196606979412e-06,
      "loss": 1.6845,
      "step": 1305700
    },
    {
      "epoch": 50.50083149630661,
      "grad_norm": 14.552410125732422,
      "learning_rate": 7.915973753077825e-06,
      "loss": 1.7533,
      "step": 1305800
    },
    {
      "epoch": 50.50469892098852,
      "grad_norm": 12.614514350891113,
      "learning_rate": 7.912750899176238e-06,
      "loss": 1.8228,
      "step": 1305900
    },
    {
      "epoch": 50.50856634567042,
      "grad_norm": 9.55537223815918,
      "learning_rate": 7.90952804527465e-06,
      "loss": 1.7297,
      "step": 1306000
    },
    {
      "epoch": 50.512433770352324,
      "grad_norm": 12.965682029724121,
      "learning_rate": 7.906305191373065e-06,
      "loss": 1.7691,
      "step": 1306100
    },
    {
      "epoch": 50.516301195034224,
      "grad_norm": 9.648333549499512,
      "learning_rate": 7.903082337471478e-06,
      "loss": 1.8332,
      "step": 1306200
    },
    {
      "epoch": 50.52016861971613,
      "grad_norm": 12.673284530639648,
      "learning_rate": 7.899859483569891e-06,
      "loss": 1.709,
      "step": 1306300
    },
    {
      "epoch": 50.52403604439804,
      "grad_norm": 11.117945671081543,
      "learning_rate": 7.896636629668304e-06,
      "loss": 1.831,
      "step": 1306400
    },
    {
      "epoch": 50.52790346907994,
      "grad_norm": 10.94844913482666,
      "learning_rate": 7.893413775766717e-06,
      "loss": 1.7817,
      "step": 1306500
    },
    {
      "epoch": 50.531770893761845,
      "grad_norm": 16.89995765686035,
      "learning_rate": 7.89019092186513e-06,
      "loss": 1.907,
      "step": 1306600
    },
    {
      "epoch": 50.535638318443745,
      "grad_norm": 14.637706756591797,
      "learning_rate": 7.886968067963544e-06,
      "loss": 1.6899,
      "step": 1306700
    },
    {
      "epoch": 50.53950574312565,
      "grad_norm": 11.872143745422363,
      "learning_rate": 7.883745214061957e-06,
      "loss": 1.8055,
      "step": 1306800
    },
    {
      "epoch": 50.54337316780756,
      "grad_norm": 11.66571044921875,
      "learning_rate": 7.88052236016037e-06,
      "loss": 1.825,
      "step": 1306900
    },
    {
      "epoch": 50.54724059248946,
      "grad_norm": 10.409847259521484,
      "learning_rate": 7.877299506258783e-06,
      "loss": 1.6919,
      "step": 1307000
    },
    {
      "epoch": 50.551108017171366,
      "grad_norm": 9.40732192993164,
      "learning_rate": 7.874076652357196e-06,
      "loss": 1.8864,
      "step": 1307100
    },
    {
      "epoch": 50.55497544185327,
      "grad_norm": 13.223366737365723,
      "learning_rate": 7.870853798455609e-06,
      "loss": 1.8137,
      "step": 1307200
    },
    {
      "epoch": 50.55884286653517,
      "grad_norm": 10.778901100158691,
      "learning_rate": 7.867630944554022e-06,
      "loss": 1.8076,
      "step": 1307300
    },
    {
      "epoch": 50.56271029121708,
      "grad_norm": 11.16025447845459,
      "learning_rate": 7.864408090652435e-06,
      "loss": 1.7705,
      "step": 1307400
    },
    {
      "epoch": 50.56657771589898,
      "grad_norm": 10.69504451751709,
      "learning_rate": 7.861185236750848e-06,
      "loss": 1.7899,
      "step": 1307500
    },
    {
      "epoch": 50.57044514058089,
      "grad_norm": 12.695727348327637,
      "learning_rate": 7.85796238284926e-06,
      "loss": 1.7312,
      "step": 1307600
    },
    {
      "epoch": 50.574312565262794,
      "grad_norm": 11.26628303527832,
      "learning_rate": 7.854739528947674e-06,
      "loss": 1.7715,
      "step": 1307700
    },
    {
      "epoch": 50.578179989944694,
      "grad_norm": 14.770193099975586,
      "learning_rate": 7.851516675046087e-06,
      "loss": 1.7062,
      "step": 1307800
    },
    {
      "epoch": 50.5820474146266,
      "grad_norm": 13.159523963928223,
      "learning_rate": 7.8482938211445e-06,
      "loss": 1.7894,
      "step": 1307900
    },
    {
      "epoch": 50.58591483930851,
      "grad_norm": 12.986846923828125,
      "learning_rate": 7.845070967242915e-06,
      "loss": 1.7263,
      "step": 1308000
    },
    {
      "epoch": 50.58978226399041,
      "grad_norm": 11.346319198608398,
      "learning_rate": 7.841848113341328e-06,
      "loss": 1.7819,
      "step": 1308100
    },
    {
      "epoch": 50.593649688672315,
      "grad_norm": 12.721923828125,
      "learning_rate": 7.83862525943974e-06,
      "loss": 1.7518,
      "step": 1308200
    },
    {
      "epoch": 50.597517113354215,
      "grad_norm": 13.172101974487305,
      "learning_rate": 7.835402405538154e-06,
      "loss": 1.845,
      "step": 1308300
    },
    {
      "epoch": 50.60138453803612,
      "grad_norm": 13.028412818908691,
      "learning_rate": 7.832179551636567e-06,
      "loss": 1.787,
      "step": 1308400
    },
    {
      "epoch": 50.60525196271803,
      "grad_norm": 14.194352149963379,
      "learning_rate": 7.82895669773498e-06,
      "loss": 1.829,
      "step": 1308500
    },
    {
      "epoch": 50.60911938739993,
      "grad_norm": 16.842422485351562,
      "learning_rate": 7.825733843833391e-06,
      "loss": 1.7244,
      "step": 1308600
    },
    {
      "epoch": 50.612986812081836,
      "grad_norm": 13.713587760925293,
      "learning_rate": 7.822510989931804e-06,
      "loss": 1.8544,
      "step": 1308700
    },
    {
      "epoch": 50.616854236763736,
      "grad_norm": 13.12980842590332,
      "learning_rate": 7.819288136030217e-06,
      "loss": 1.8152,
      "step": 1308800
    },
    {
      "epoch": 50.62072166144564,
      "grad_norm": 11.972801208496094,
      "learning_rate": 7.81606528212863e-06,
      "loss": 1.7305,
      "step": 1308900
    },
    {
      "epoch": 50.62458908612755,
      "grad_norm": 12.759227752685547,
      "learning_rate": 7.812842428227043e-06,
      "loss": 1.8687,
      "step": 1309000
    },
    {
      "epoch": 50.62845651080945,
      "grad_norm": 9.435431480407715,
      "learning_rate": 7.809619574325456e-06,
      "loss": 1.7473,
      "step": 1309100
    },
    {
      "epoch": 50.63232393549136,
      "grad_norm": 8.770959854125977,
      "learning_rate": 7.806396720423869e-06,
      "loss": 1.7686,
      "step": 1309200
    },
    {
      "epoch": 50.636191360173264,
      "grad_norm": 12.346986770629883,
      "learning_rate": 7.803173866522282e-06,
      "loss": 1.77,
      "step": 1309300
    },
    {
      "epoch": 50.640058784855164,
      "grad_norm": 10.602770805358887,
      "learning_rate": 7.799951012620695e-06,
      "loss": 1.6897,
      "step": 1309400
    },
    {
      "epoch": 50.64392620953707,
      "grad_norm": 11.407159805297852,
      "learning_rate": 7.796728158719108e-06,
      "loss": 1.802,
      "step": 1309500
    },
    {
      "epoch": 50.64779363421897,
      "grad_norm": 9.926549911499023,
      "learning_rate": 7.793505304817523e-06,
      "loss": 1.7813,
      "step": 1309600
    },
    {
      "epoch": 50.65166105890088,
      "grad_norm": 10.344249725341797,
      "learning_rate": 7.790282450915936e-06,
      "loss": 1.7637,
      "step": 1309700
    },
    {
      "epoch": 50.655528483582785,
      "grad_norm": 10.082846641540527,
      "learning_rate": 7.787059597014349e-06,
      "loss": 1.7847,
      "step": 1309800
    },
    {
      "epoch": 50.659395908264685,
      "grad_norm": 18.533424377441406,
      "learning_rate": 7.783836743112762e-06,
      "loss": 1.7637,
      "step": 1309900
    },
    {
      "epoch": 50.66326333294659,
      "grad_norm": 12.965408325195312,
      "learning_rate": 7.780613889211175e-06,
      "loss": 1.8827,
      "step": 1310000
    },
    {
      "epoch": 50.66713075762849,
      "grad_norm": 13.575953483581543,
      "learning_rate": 7.777391035309588e-06,
      "loss": 1.7565,
      "step": 1310100
    },
    {
      "epoch": 50.6709981823104,
      "grad_norm": 16.1921329498291,
      "learning_rate": 7.774168181408001e-06,
      "loss": 1.7269,
      "step": 1310200
    },
    {
      "epoch": 50.674865606992306,
      "grad_norm": 13.239912986755371,
      "learning_rate": 7.770945327506414e-06,
      "loss": 1.8872,
      "step": 1310300
    },
    {
      "epoch": 50.678733031674206,
      "grad_norm": 12.404513359069824,
      "learning_rate": 7.767722473604827e-06,
      "loss": 1.7683,
      "step": 1310400
    },
    {
      "epoch": 50.68260045635611,
      "grad_norm": 13.616360664367676,
      "learning_rate": 7.76449961970324e-06,
      "loss": 1.8376,
      "step": 1310500
    },
    {
      "epoch": 50.68646788103802,
      "grad_norm": 12.891548156738281,
      "learning_rate": 7.761276765801653e-06,
      "loss": 1.8109,
      "step": 1310600
    },
    {
      "epoch": 50.69033530571992,
      "grad_norm": 13.35929012298584,
      "learning_rate": 7.758053911900066e-06,
      "loss": 1.7629,
      "step": 1310700
    },
    {
      "epoch": 50.69420273040183,
      "grad_norm": 13.4788818359375,
      "learning_rate": 7.75483105799848e-06,
      "loss": 1.7192,
      "step": 1310800
    },
    {
      "epoch": 50.69807015508373,
      "grad_norm": 11.604601860046387,
      "learning_rate": 7.751608204096892e-06,
      "loss": 1.7629,
      "step": 1310900
    },
    {
      "epoch": 50.701937579765634,
      "grad_norm": 12.123239517211914,
      "learning_rate": 7.748385350195305e-06,
      "loss": 1.7004,
      "step": 1311000
    },
    {
      "epoch": 50.70580500444754,
      "grad_norm": 12.449827194213867,
      "learning_rate": 7.745162496293718e-06,
      "loss": 1.848,
      "step": 1311100
    },
    {
      "epoch": 50.70967242912944,
      "grad_norm": 10.249073028564453,
      "learning_rate": 7.741939642392131e-06,
      "loss": 1.8093,
      "step": 1311200
    },
    {
      "epoch": 50.71353985381135,
      "grad_norm": 10.56356430053711,
      "learning_rate": 7.738716788490544e-06,
      "loss": 1.7268,
      "step": 1311300
    },
    {
      "epoch": 50.717407278493255,
      "grad_norm": 12.78065299987793,
      "learning_rate": 7.735493934588957e-06,
      "loss": 1.8982,
      "step": 1311400
    },
    {
      "epoch": 50.721274703175155,
      "grad_norm": 14.170205116271973,
      "learning_rate": 7.732271080687372e-06,
      "loss": 1.8001,
      "step": 1311500
    },
    {
      "epoch": 50.72514212785706,
      "grad_norm": 14.661818504333496,
      "learning_rate": 7.729048226785783e-06,
      "loss": 1.8258,
      "step": 1311600
    },
    {
      "epoch": 50.72900955253896,
      "grad_norm": 11.440730094909668,
      "learning_rate": 7.725825372884196e-06,
      "loss": 1.811,
      "step": 1311700
    },
    {
      "epoch": 50.73287697722087,
      "grad_norm": 14.312697410583496,
      "learning_rate": 7.72260251898261e-06,
      "loss": 1.8202,
      "step": 1311800
    },
    {
      "epoch": 50.736744401902776,
      "grad_norm": 15.804067611694336,
      "learning_rate": 7.719379665081022e-06,
      "loss": 1.8536,
      "step": 1311900
    },
    {
      "epoch": 50.740611826584676,
      "grad_norm": 13.871192932128906,
      "learning_rate": 7.716156811179436e-06,
      "loss": 1.7424,
      "step": 1312000
    },
    {
      "epoch": 50.74447925126658,
      "grad_norm": 14.068504333496094,
      "learning_rate": 7.712933957277849e-06,
      "loss": 1.6634,
      "step": 1312100
    },
    {
      "epoch": 50.74834667594848,
      "grad_norm": 13.006749153137207,
      "learning_rate": 7.709711103376262e-06,
      "loss": 1.7716,
      "step": 1312200
    },
    {
      "epoch": 50.75221410063039,
      "grad_norm": 14.107817649841309,
      "learning_rate": 7.706488249474675e-06,
      "loss": 1.8431,
      "step": 1312300
    },
    {
      "epoch": 50.7560815253123,
      "grad_norm": 11.092397689819336,
      "learning_rate": 7.703265395573088e-06,
      "loss": 1.6837,
      "step": 1312400
    },
    {
      "epoch": 50.7599489499942,
      "grad_norm": 14.907660484313965,
      "learning_rate": 7.7000425416715e-06,
      "loss": 1.7891,
      "step": 1312500
    },
    {
      "epoch": 50.763816374676104,
      "grad_norm": 15.55827522277832,
      "learning_rate": 7.696819687769914e-06,
      "loss": 1.8925,
      "step": 1312600
    },
    {
      "epoch": 50.76768379935801,
      "grad_norm": 12.599322319030762,
      "learning_rate": 7.693596833868327e-06,
      "loss": 1.7809,
      "step": 1312700
    },
    {
      "epoch": 50.77155122403991,
      "grad_norm": 9.624405860900879,
      "learning_rate": 7.69037397996674e-06,
      "loss": 1.8299,
      "step": 1312800
    },
    {
      "epoch": 50.77541864872182,
      "grad_norm": 14.522461891174316,
      "learning_rate": 7.687151126065153e-06,
      "loss": 1.8359,
      "step": 1312900
    },
    {
      "epoch": 50.77928607340372,
      "grad_norm": 11.260435104370117,
      "learning_rate": 7.683928272163567e-06,
      "loss": 1.7982,
      "step": 1313000
    },
    {
      "epoch": 50.783153498085625,
      "grad_norm": 15.525253295898438,
      "learning_rate": 7.68070541826198e-06,
      "loss": 1.8505,
      "step": 1313100
    },
    {
      "epoch": 50.78702092276753,
      "grad_norm": 12.491043090820312,
      "learning_rate": 7.677482564360394e-06,
      "loss": 1.6963,
      "step": 1313200
    },
    {
      "epoch": 50.79088834744943,
      "grad_norm": 13.131094932556152,
      "learning_rate": 7.674259710458807e-06,
      "loss": 1.791,
      "step": 1313300
    },
    {
      "epoch": 50.79475577213134,
      "grad_norm": 12.57121467590332,
      "learning_rate": 7.67103685655722e-06,
      "loss": 1.7356,
      "step": 1313400
    },
    {
      "epoch": 50.79862319681324,
      "grad_norm": 12.908576011657715,
      "learning_rate": 7.667814002655633e-06,
      "loss": 1.8502,
      "step": 1313500
    },
    {
      "epoch": 50.802490621495146,
      "grad_norm": 15.283100128173828,
      "learning_rate": 7.664591148754046e-06,
      "loss": 1.8502,
      "step": 1313600
    },
    {
      "epoch": 50.80635804617705,
      "grad_norm": 12.800110816955566,
      "learning_rate": 7.661368294852459e-06,
      "loss": 1.7157,
      "step": 1313700
    },
    {
      "epoch": 50.81022547085895,
      "grad_norm": 13.372552871704102,
      "learning_rate": 7.658145440950872e-06,
      "loss": 1.7482,
      "step": 1313800
    },
    {
      "epoch": 50.81409289554086,
      "grad_norm": 7.2703328132629395,
      "learning_rate": 7.654922587049285e-06,
      "loss": 1.7673,
      "step": 1313900
    },
    {
      "epoch": 50.81796032022277,
      "grad_norm": 11.829389572143555,
      "learning_rate": 7.651699733147698e-06,
      "loss": 1.8148,
      "step": 1314000
    },
    {
      "epoch": 50.82182774490467,
      "grad_norm": 12.423585891723633,
      "learning_rate": 7.64847687924611e-06,
      "loss": 1.7398,
      "step": 1314100
    },
    {
      "epoch": 50.825695169586574,
      "grad_norm": 13.812222480773926,
      "learning_rate": 7.645254025344524e-06,
      "loss": 1.7439,
      "step": 1314200
    },
    {
      "epoch": 50.829562594268474,
      "grad_norm": 16.82908821105957,
      "learning_rate": 7.642031171442937e-06,
      "loss": 1.7814,
      "step": 1314300
    },
    {
      "epoch": 50.83343001895038,
      "grad_norm": 12.906695365905762,
      "learning_rate": 7.63880831754135e-06,
      "loss": 1.9126,
      "step": 1314400
    },
    {
      "epoch": 50.83729744363229,
      "grad_norm": 13.782695770263672,
      "learning_rate": 7.635585463639761e-06,
      "loss": 1.8501,
      "step": 1314500
    },
    {
      "epoch": 50.84116486831419,
      "grad_norm": 11.329712867736816,
      "learning_rate": 7.632362609738176e-06,
      "loss": 1.8623,
      "step": 1314600
    },
    {
      "epoch": 50.845032292996095,
      "grad_norm": 12.828685760498047,
      "learning_rate": 7.629139755836588e-06,
      "loss": 1.8389,
      "step": 1314700
    },
    {
      "epoch": 50.848899717677995,
      "grad_norm": 13.798408508300781,
      "learning_rate": 7.625916901935001e-06,
      "loss": 1.8131,
      "step": 1314800
    },
    {
      "epoch": 50.8527671423599,
      "grad_norm": 11.817604064941406,
      "learning_rate": 7.622694048033414e-06,
      "loss": 1.8624,
      "step": 1314900
    },
    {
      "epoch": 50.85663456704181,
      "grad_norm": 14.291102409362793,
      "learning_rate": 7.619471194131828e-06,
      "loss": 1.7686,
      "step": 1315000
    },
    {
      "epoch": 50.86050199172371,
      "grad_norm": 16.453266143798828,
      "learning_rate": 7.616248340230241e-06,
      "loss": 1.876,
      "step": 1315100
    },
    {
      "epoch": 50.864369416405616,
      "grad_norm": 11.836993217468262,
      "learning_rate": 7.613025486328654e-06,
      "loss": 1.6716,
      "step": 1315200
    },
    {
      "epoch": 50.86823684108752,
      "grad_norm": 9.90371036529541,
      "learning_rate": 7.609802632427067e-06,
      "loss": 1.7437,
      "step": 1315300
    },
    {
      "epoch": 50.87210426576942,
      "grad_norm": 10.038484573364258,
      "learning_rate": 7.60657977852548e-06,
      "loss": 1.7942,
      "step": 1315400
    },
    {
      "epoch": 50.87597169045133,
      "grad_norm": 15.542171478271484,
      "learning_rate": 7.603356924623893e-06,
      "loss": 1.7803,
      "step": 1315500
    },
    {
      "epoch": 50.87983911513323,
      "grad_norm": 10.975425720214844,
      "learning_rate": 7.600134070722306e-06,
      "loss": 1.8153,
      "step": 1315600
    },
    {
      "epoch": 50.88370653981514,
      "grad_norm": 9.552448272705078,
      "learning_rate": 7.596911216820719e-06,
      "loss": 1.8,
      "step": 1315700
    },
    {
      "epoch": 50.887573964497044,
      "grad_norm": 14.515192985534668,
      "learning_rate": 7.593688362919132e-06,
      "loss": 1.8501,
      "step": 1315800
    },
    {
      "epoch": 50.891441389178944,
      "grad_norm": 11.571407318115234,
      "learning_rate": 7.590465509017545e-06,
      "loss": 1.8819,
      "step": 1315900
    },
    {
      "epoch": 50.89530881386085,
      "grad_norm": 11.256805419921875,
      "learning_rate": 7.587242655115959e-06,
      "loss": 1.8476,
      "step": 1316000
    },
    {
      "epoch": 50.89917623854276,
      "grad_norm": 12.749815940856934,
      "learning_rate": 7.584019801214372e-06,
      "loss": 1.7696,
      "step": 1316100
    },
    {
      "epoch": 50.90304366322466,
      "grad_norm": 17.889629364013672,
      "learning_rate": 7.580796947312785e-06,
      "loss": 1.9227,
      "step": 1316200
    },
    {
      "epoch": 50.906911087906565,
      "grad_norm": 13.182607650756836,
      "learning_rate": 7.577574093411198e-06,
      "loss": 1.7119,
      "step": 1316300
    },
    {
      "epoch": 50.910778512588465,
      "grad_norm": 10.221057891845703,
      "learning_rate": 7.574351239509611e-06,
      "loss": 1.7436,
      "step": 1316400
    },
    {
      "epoch": 50.91464593727037,
      "grad_norm": 11.65024471282959,
      "learning_rate": 7.571128385608024e-06,
      "loss": 1.7409,
      "step": 1316500
    },
    {
      "epoch": 50.91851336195228,
      "grad_norm": 11.19387149810791,
      "learning_rate": 7.567905531706437e-06,
      "loss": 1.716,
      "step": 1316600
    },
    {
      "epoch": 50.92238078663418,
      "grad_norm": 10.738039016723633,
      "learning_rate": 7.56468267780485e-06,
      "loss": 1.6906,
      "step": 1316700
    },
    {
      "epoch": 50.926248211316086,
      "grad_norm": 14.345758438110352,
      "learning_rate": 7.561459823903263e-06,
      "loss": 1.7715,
      "step": 1316800
    },
    {
      "epoch": 50.930115635997986,
      "grad_norm": 15.06311321258545,
      "learning_rate": 7.558236970001677e-06,
      "loss": 1.9871,
      "step": 1316900
    },
    {
      "epoch": 50.93398306067989,
      "grad_norm": 11.190240859985352,
      "learning_rate": 7.55501411610009e-06,
      "loss": 1.747,
      "step": 1317000
    },
    {
      "epoch": 50.9378504853618,
      "grad_norm": 9.301787376403809,
      "learning_rate": 7.551791262198503e-06,
      "loss": 1.7892,
      "step": 1317100
    },
    {
      "epoch": 50.9417179100437,
      "grad_norm": 8.378875732421875,
      "learning_rate": 7.548568408296916e-06,
      "loss": 1.7845,
      "step": 1317200
    },
    {
      "epoch": 50.94558533472561,
      "grad_norm": 10.821822166442871,
      "learning_rate": 7.545345554395329e-06,
      "loss": 1.815,
      "step": 1317300
    },
    {
      "epoch": 50.949452759407514,
      "grad_norm": 12.090598106384277,
      "learning_rate": 7.542122700493742e-06,
      "loss": 1.7729,
      "step": 1317400
    },
    {
      "epoch": 50.953320184089414,
      "grad_norm": 9.902141571044922,
      "learning_rate": 7.538899846592154e-06,
      "loss": 1.7912,
      "step": 1317500
    },
    {
      "epoch": 50.95718760877132,
      "grad_norm": 8.5516939163208,
      "learning_rate": 7.535676992690567e-06,
      "loss": 1.7322,
      "step": 1317600
    },
    {
      "epoch": 50.96105503345322,
      "grad_norm": 14.248517036437988,
      "learning_rate": 7.5324541387889804e-06,
      "loss": 1.785,
      "step": 1317700
    },
    {
      "epoch": 50.96492245813513,
      "grad_norm": 10.13579273223877,
      "learning_rate": 7.5292312848873935e-06,
      "loss": 1.7589,
      "step": 1317800
    },
    {
      "epoch": 50.968789882817035,
      "grad_norm": 11.963579177856445,
      "learning_rate": 7.5260084309858065e-06,
      "loss": 1.7535,
      "step": 1317900
    },
    {
      "epoch": 50.972657307498935,
      "grad_norm": 12.343151092529297,
      "learning_rate": 7.5227855770842195e-06,
      "loss": 1.8064,
      "step": 1318000
    },
    {
      "epoch": 50.97652473218084,
      "grad_norm": 11.452629089355469,
      "learning_rate": 7.5195627231826325e-06,
      "loss": 1.8369,
      "step": 1318100
    },
    {
      "epoch": 50.98039215686274,
      "grad_norm": 13.179471969604492,
      "learning_rate": 7.5163398692810456e-06,
      "loss": 1.7605,
      "step": 1318200
    },
    {
      "epoch": 50.98425958154465,
      "grad_norm": 13.866500854492188,
      "learning_rate": 7.513117015379459e-06,
      "loss": 1.6953,
      "step": 1318300
    },
    {
      "epoch": 50.988127006226556,
      "grad_norm": 12.812623977661133,
      "learning_rate": 7.509894161477872e-06,
      "loss": 1.7516,
      "step": 1318400
    },
    {
      "epoch": 50.991994430908456,
      "grad_norm": 12.57941722869873,
      "learning_rate": 7.5066713075762855e-06,
      "loss": 1.7777,
      "step": 1318500
    },
    {
      "epoch": 50.99586185559036,
      "grad_norm": 13.78803539276123,
      "learning_rate": 7.5034484536746985e-06,
      "loss": 1.7469,
      "step": 1318600
    },
    {
      "epoch": 50.99972928027227,
      "grad_norm": 11.818962097167969,
      "learning_rate": 7.5002255997731115e-06,
      "loss": 1.8528,
      "step": 1318700
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.743560552597046,
      "eval_runtime": 5.6018,
      "eval_samples_per_second": 242.959,
      "eval_steps_per_second": 242.959,
      "step": 1318707
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.579681158065796,
      "eval_runtime": 105.7036,
      "eval_samples_per_second": 244.618,
      "eval_steps_per_second": 244.618,
      "step": 1318707
    },
    {
      "epoch": 51.00359670495417,
      "grad_norm": 11.020126342773438,
      "learning_rate": 7.4970027458715246e-06,
      "loss": 1.81,
      "step": 1318800
    },
    {
      "epoch": 51.00746412963608,
      "grad_norm": 12.33597183227539,
      "learning_rate": 7.493779891969938e-06,
      "loss": 1.8737,
      "step": 1318900
    },
    {
      "epoch": 51.01133155431798,
      "grad_norm": 12.386637687683105,
      "learning_rate": 7.490557038068351e-06,
      "loss": 1.8073,
      "step": 1319000
    },
    {
      "epoch": 51.015198978999884,
      "grad_norm": 12.07044506072998,
      "learning_rate": 7.487334184166764e-06,
      "loss": 1.7797,
      "step": 1319100
    },
    {
      "epoch": 51.01906640368179,
      "grad_norm": 13.39449405670166,
      "learning_rate": 7.484111330265177e-06,
      "loss": 1.7403,
      "step": 1319200
    },
    {
      "epoch": 51.02293382836369,
      "grad_norm": 8.29450511932373,
      "learning_rate": 7.48088847636359e-06,
      "loss": 1.8201,
      "step": 1319300
    },
    {
      "epoch": 51.0268012530456,
      "grad_norm": 13.051488876342773,
      "learning_rate": 7.4776656224620035e-06,
      "loss": 1.7802,
      "step": 1319400
    },
    {
      "epoch": 51.030668677727505,
      "grad_norm": 9.776156425476074,
      "learning_rate": 7.4744427685604166e-06,
      "loss": 1.8271,
      "step": 1319500
    },
    {
      "epoch": 51.034536102409405,
      "grad_norm": 14.408411026000977,
      "learning_rate": 7.47121991465883e-06,
      "loss": 1.7436,
      "step": 1319600
    },
    {
      "epoch": 51.03840352709131,
      "grad_norm": 12.417652130126953,
      "learning_rate": 7.467997060757243e-06,
      "loss": 1.6667,
      "step": 1319700
    },
    {
      "epoch": 51.04227095177321,
      "grad_norm": 13.721698760986328,
      "learning_rate": 7.464774206855656e-06,
      "loss": 1.8294,
      "step": 1319800
    },
    {
      "epoch": 51.04613837645512,
      "grad_norm": 14.261977195739746,
      "learning_rate": 7.461551352954069e-06,
      "loss": 1.7484,
      "step": 1319900
    },
    {
      "epoch": 51.050005801137026,
      "grad_norm": 13.970808982849121,
      "learning_rate": 7.458328499052482e-06,
      "loss": 1.7332,
      "step": 1320000
    },
    {
      "epoch": 51.053873225818926,
      "grad_norm": 11.078500747680664,
      "learning_rate": 7.455105645150895e-06,
      "loss": 1.7769,
      "step": 1320100
    },
    {
      "epoch": 51.05774065050083,
      "grad_norm": 12.316206932067871,
      "learning_rate": 7.451882791249308e-06,
      "loss": 1.8272,
      "step": 1320200
    },
    {
      "epoch": 51.06160807518273,
      "grad_norm": 12.277420043945312,
      "learning_rate": 7.448659937347721e-06,
      "loss": 1.7783,
      "step": 1320300
    },
    {
      "epoch": 51.06547549986464,
      "grad_norm": 13.292325019836426,
      "learning_rate": 7.445437083446133e-06,
      "loss": 1.7741,
      "step": 1320400
    },
    {
      "epoch": 51.06934292454655,
      "grad_norm": 13.840049743652344,
      "learning_rate": 7.442214229544546e-06,
      "loss": 1.8916,
      "step": 1320500
    },
    {
      "epoch": 51.07321034922845,
      "grad_norm": 10.879250526428223,
      "learning_rate": 7.438991375642959e-06,
      "loss": 1.8022,
      "step": 1320600
    },
    {
      "epoch": 51.077077773910354,
      "grad_norm": 11.173666000366211,
      "learning_rate": 7.435768521741372e-06,
      "loss": 1.8166,
      "step": 1320700
    },
    {
      "epoch": 51.08094519859226,
      "grad_norm": 16.557384490966797,
      "learning_rate": 7.432545667839785e-06,
      "loss": 1.7559,
      "step": 1320800
    },
    {
      "epoch": 51.08481262327416,
      "grad_norm": 17.633556365966797,
      "learning_rate": 7.429322813938198e-06,
      "loss": 1.8405,
      "step": 1320900
    },
    {
      "epoch": 51.08868004795607,
      "grad_norm": 12.840372085571289,
      "learning_rate": 7.426099960036612e-06,
      "loss": 1.757,
      "step": 1321000
    },
    {
      "epoch": 51.09254747263797,
      "grad_norm": 11.920703887939453,
      "learning_rate": 7.422877106135025e-06,
      "loss": 1.8145,
      "step": 1321100
    },
    {
      "epoch": 51.096414897319875,
      "grad_norm": 18.55093002319336,
      "learning_rate": 7.419654252233438e-06,
      "loss": 1.8925,
      "step": 1321200
    },
    {
      "epoch": 51.10028232200178,
      "grad_norm": 14.709824562072754,
      "learning_rate": 7.416431398331851e-06,
      "loss": 1.7544,
      "step": 1321300
    },
    {
      "epoch": 51.10414974668368,
      "grad_norm": 13.938619613647461,
      "learning_rate": 7.413208544430264e-06,
      "loss": 1.7372,
      "step": 1321400
    },
    {
      "epoch": 51.10801717136559,
      "grad_norm": 12.93474006652832,
      "learning_rate": 7.409985690528677e-06,
      "loss": 1.7869,
      "step": 1321500
    },
    {
      "epoch": 51.11188459604749,
      "grad_norm": 11.358549118041992,
      "learning_rate": 7.40676283662709e-06,
      "loss": 1.8143,
      "step": 1321600
    },
    {
      "epoch": 51.115752020729396,
      "grad_norm": 11.19510555267334,
      "learning_rate": 7.403539982725503e-06,
      "loss": 1.6915,
      "step": 1321700
    },
    {
      "epoch": 51.1196194454113,
      "grad_norm": 12.112144470214844,
      "learning_rate": 7.400317128823916e-06,
      "loss": 1.7413,
      "step": 1321800
    },
    {
      "epoch": 51.1234868700932,
      "grad_norm": 12.770098686218262,
      "learning_rate": 7.39709427492233e-06,
      "loss": 1.8469,
      "step": 1321900
    },
    {
      "epoch": 51.12735429477511,
      "grad_norm": 10.912917137145996,
      "learning_rate": 7.393871421020743e-06,
      "loss": 1.6865,
      "step": 1322000
    },
    {
      "epoch": 51.13122171945702,
      "grad_norm": 10.632346153259277,
      "learning_rate": 7.390648567119156e-06,
      "loss": 1.7616,
      "step": 1322100
    },
    {
      "epoch": 51.13508914413892,
      "grad_norm": 13.246895790100098,
      "learning_rate": 7.387425713217569e-06,
      "loss": 1.6652,
      "step": 1322200
    },
    {
      "epoch": 51.138956568820824,
      "grad_norm": 9.828216552734375,
      "learning_rate": 7.384202859315982e-06,
      "loss": 1.7647,
      "step": 1322300
    },
    {
      "epoch": 51.142823993502724,
      "grad_norm": 12.549335479736328,
      "learning_rate": 7.380980005414395e-06,
      "loss": 1.8316,
      "step": 1322400
    },
    {
      "epoch": 51.14669141818463,
      "grad_norm": 13.41452407836914,
      "learning_rate": 7.377757151512808e-06,
      "loss": 1.7395,
      "step": 1322500
    },
    {
      "epoch": 51.15055884286654,
      "grad_norm": 12.523601531982422,
      "learning_rate": 7.374534297611221e-06,
      "loss": 1.8585,
      "step": 1322600
    },
    {
      "epoch": 51.15442626754844,
      "grad_norm": 12.171276092529297,
      "learning_rate": 7.371311443709634e-06,
      "loss": 1.763,
      "step": 1322700
    },
    {
      "epoch": 51.158293692230345,
      "grad_norm": 12.299363136291504,
      "learning_rate": 7.368088589808047e-06,
      "loss": 1.8626,
      "step": 1322800
    },
    {
      "epoch": 51.16216111691225,
      "grad_norm": 12.525171279907227,
      "learning_rate": 7.364865735906461e-06,
      "loss": 1.7835,
      "step": 1322900
    },
    {
      "epoch": 51.16602854159415,
      "grad_norm": 13.009363174438477,
      "learning_rate": 7.361642882004874e-06,
      "loss": 1.8421,
      "step": 1323000
    },
    {
      "epoch": 51.16989596627606,
      "grad_norm": 12.36536979675293,
      "learning_rate": 7.358420028103287e-06,
      "loss": 1.7513,
      "step": 1323100
    },
    {
      "epoch": 51.17376339095796,
      "grad_norm": 13.931550025939941,
      "learning_rate": 7.3551971742017e-06,
      "loss": 1.7979,
      "step": 1323200
    },
    {
      "epoch": 51.177630815639866,
      "grad_norm": 13.84547233581543,
      "learning_rate": 7.351974320300113e-06,
      "loss": 1.7999,
      "step": 1323300
    },
    {
      "epoch": 51.18149824032177,
      "grad_norm": 14.259684562683105,
      "learning_rate": 7.3487514663985245e-06,
      "loss": 1.7911,
      "step": 1323400
    },
    {
      "epoch": 51.18536566500367,
      "grad_norm": 13.117436408996582,
      "learning_rate": 7.345528612496938e-06,
      "loss": 1.7908,
      "step": 1323500
    },
    {
      "epoch": 51.18923308968558,
      "grad_norm": 14.078619003295898,
      "learning_rate": 7.342305758595351e-06,
      "loss": 1.83,
      "step": 1323600
    },
    {
      "epoch": 51.19310051436748,
      "grad_norm": 13.173727989196777,
      "learning_rate": 7.3390829046937644e-06,
      "loss": 1.7758,
      "step": 1323700
    },
    {
      "epoch": 51.19696793904939,
      "grad_norm": 10.34575080871582,
      "learning_rate": 7.3358600507921775e-06,
      "loss": 1.7984,
      "step": 1323800
    },
    {
      "epoch": 51.200835363731294,
      "grad_norm": 11.458639144897461,
      "learning_rate": 7.3326371968905905e-06,
      "loss": 1.7801,
      "step": 1323900
    },
    {
      "epoch": 51.204702788413194,
      "grad_norm": 10.783935546875,
      "learning_rate": 7.3294143429890035e-06,
      "loss": 1.7317,
      "step": 1324000
    },
    {
      "epoch": 51.2085702130951,
      "grad_norm": 12.115225791931152,
      "learning_rate": 7.3261914890874165e-06,
      "loss": 1.7705,
      "step": 1324100
    },
    {
      "epoch": 51.21243763777701,
      "grad_norm": 11.944159507751465,
      "learning_rate": 7.3229686351858296e-06,
      "loss": 1.7292,
      "step": 1324200
    },
    {
      "epoch": 51.21630506245891,
      "grad_norm": 10.34577465057373,
      "learning_rate": 7.319745781284243e-06,
      "loss": 1.704,
      "step": 1324300
    },
    {
      "epoch": 51.220172487140815,
      "grad_norm": 11.142038345336914,
      "learning_rate": 7.3165229273826565e-06,
      "loss": 1.6823,
      "step": 1324400
    },
    {
      "epoch": 51.224039911822715,
      "grad_norm": 12.175076484680176,
      "learning_rate": 7.3133000734810695e-06,
      "loss": 1.6984,
      "step": 1324500
    },
    {
      "epoch": 51.22790733650462,
      "grad_norm": 13.652547836303711,
      "learning_rate": 7.3100772195794825e-06,
      "loss": 1.8951,
      "step": 1324600
    },
    {
      "epoch": 51.23177476118653,
      "grad_norm": 14.625244140625,
      "learning_rate": 7.3068543656778955e-06,
      "loss": 1.84,
      "step": 1324700
    },
    {
      "epoch": 51.23564218586843,
      "grad_norm": 20.12468147277832,
      "learning_rate": 7.3036315117763086e-06,
      "loss": 1.7343,
      "step": 1324800
    },
    {
      "epoch": 51.239509610550336,
      "grad_norm": 15.509246826171875,
      "learning_rate": 7.300408657874722e-06,
      "loss": 1.8395,
      "step": 1324900
    },
    {
      "epoch": 51.243377035232236,
      "grad_norm": 13.031246185302734,
      "learning_rate": 7.297185803973135e-06,
      "loss": 1.7913,
      "step": 1325000
    },
    {
      "epoch": 51.24724445991414,
      "grad_norm": 13.64494800567627,
      "learning_rate": 7.293962950071548e-06,
      "loss": 1.7576,
      "step": 1325100
    },
    {
      "epoch": 51.25111188459605,
      "grad_norm": 17.14057731628418,
      "learning_rate": 7.290740096169961e-06,
      "loss": 1.7217,
      "step": 1325200
    },
    {
      "epoch": 51.25497930927795,
      "grad_norm": 14.170709609985352,
      "learning_rate": 7.287517242268374e-06,
      "loss": 1.754,
      "step": 1325300
    },
    {
      "epoch": 51.25884673395986,
      "grad_norm": 12.1655912399292,
      "learning_rate": 7.2842943883667876e-06,
      "loss": 1.7451,
      "step": 1325400
    },
    {
      "epoch": 51.262714158641764,
      "grad_norm": 13.388680458068848,
      "learning_rate": 7.2810715344652006e-06,
      "loss": 1.8066,
      "step": 1325500
    },
    {
      "epoch": 51.26658158332366,
      "grad_norm": 15.119681358337402,
      "learning_rate": 7.277848680563614e-06,
      "loss": 1.8152,
      "step": 1325600
    },
    {
      "epoch": 51.27044900800557,
      "grad_norm": 10.593883514404297,
      "learning_rate": 7.274625826662027e-06,
      "loss": 1.7996,
      "step": 1325700
    },
    {
      "epoch": 51.27431643268747,
      "grad_norm": 15.338018417358398,
      "learning_rate": 7.27140297276044e-06,
      "loss": 1.7537,
      "step": 1325800
    },
    {
      "epoch": 51.27818385736938,
      "grad_norm": 16.39945411682129,
      "learning_rate": 7.268180118858853e-06,
      "loss": 1.751,
      "step": 1325900
    },
    {
      "epoch": 51.282051282051285,
      "grad_norm": 8.537689208984375,
      "learning_rate": 7.264957264957266e-06,
      "loss": 1.8193,
      "step": 1326000
    },
    {
      "epoch": 51.285918706733185,
      "grad_norm": 11.960558891296387,
      "learning_rate": 7.261734411055679e-06,
      "loss": 1.8367,
      "step": 1326100
    },
    {
      "epoch": 51.28978613141509,
      "grad_norm": 14.185598373413086,
      "learning_rate": 7.258511557154092e-06,
      "loss": 1.8019,
      "step": 1326200
    },
    {
      "epoch": 51.29365355609699,
      "grad_norm": 10.492679595947266,
      "learning_rate": 7.255288703252504e-06,
      "loss": 1.781,
      "step": 1326300
    },
    {
      "epoch": 51.2975209807789,
      "grad_norm": 10.150924682617188,
      "learning_rate": 7.252065849350917e-06,
      "loss": 1.7115,
      "step": 1326400
    },
    {
      "epoch": 51.301388405460806,
      "grad_norm": 11.258317947387695,
      "learning_rate": 7.24884299544933e-06,
      "loss": 1.7388,
      "step": 1326500
    },
    {
      "epoch": 51.305255830142706,
      "grad_norm": 10.589447975158691,
      "learning_rate": 7.245620141547743e-06,
      "loss": 1.8235,
      "step": 1326600
    },
    {
      "epoch": 51.30912325482461,
      "grad_norm": 9.919646263122559,
      "learning_rate": 7.242397287646156e-06,
      "loss": 1.8261,
      "step": 1326700
    },
    {
      "epoch": 51.31299067950652,
      "grad_norm": 9.218889236450195,
      "learning_rate": 7.239174433744569e-06,
      "loss": 1.8373,
      "step": 1326800
    },
    {
      "epoch": 51.31685810418842,
      "grad_norm": 14.214601516723633,
      "learning_rate": 7.235951579842983e-06,
      "loss": 1.8358,
      "step": 1326900
    },
    {
      "epoch": 51.32072552887033,
      "grad_norm": 10.7144136428833,
      "learning_rate": 7.232728725941396e-06,
      "loss": 1.758,
      "step": 1327000
    },
    {
      "epoch": 51.32459295355223,
      "grad_norm": 12.865998268127441,
      "learning_rate": 7.229505872039809e-06,
      "loss": 1.7396,
      "step": 1327100
    },
    {
      "epoch": 51.32846037823413,
      "grad_norm": 16.43863296508789,
      "learning_rate": 7.226283018138222e-06,
      "loss": 1.7747,
      "step": 1327200
    },
    {
      "epoch": 51.33232780291604,
      "grad_norm": 9.964876174926758,
      "learning_rate": 7.223060164236635e-06,
      "loss": 1.7721,
      "step": 1327300
    },
    {
      "epoch": 51.33619522759794,
      "grad_norm": 8.773018836975098,
      "learning_rate": 7.219837310335048e-06,
      "loss": 1.672,
      "step": 1327400
    },
    {
      "epoch": 51.34006265227985,
      "grad_norm": 13.628466606140137,
      "learning_rate": 7.216614456433461e-06,
      "loss": 1.8118,
      "step": 1327500
    },
    {
      "epoch": 51.343930076961755,
      "grad_norm": 9.221379280090332,
      "learning_rate": 7.213391602531874e-06,
      "loss": 1.8065,
      "step": 1327600
    },
    {
      "epoch": 51.347797501643655,
      "grad_norm": 12.673816680908203,
      "learning_rate": 7.210168748630287e-06,
      "loss": 1.805,
      "step": 1327700
    },
    {
      "epoch": 51.35166492632556,
      "grad_norm": 13.151765823364258,
      "learning_rate": 7.2069458947287e-06,
      "loss": 1.7462,
      "step": 1327800
    },
    {
      "epoch": 51.35553235100746,
      "grad_norm": 11.748427391052246,
      "learning_rate": 7.203723040827114e-06,
      "loss": 1.7677,
      "step": 1327900
    },
    {
      "epoch": 51.35939977568937,
      "grad_norm": 13.457062721252441,
      "learning_rate": 7.200500186925527e-06,
      "loss": 1.8516,
      "step": 1328000
    },
    {
      "epoch": 51.363267200371276,
      "grad_norm": 13.284174919128418,
      "learning_rate": 7.19727733302394e-06,
      "loss": 1.764,
      "step": 1328100
    },
    {
      "epoch": 51.367134625053176,
      "grad_norm": 12.992310523986816,
      "learning_rate": 7.194054479122353e-06,
      "loss": 1.7895,
      "step": 1328200
    },
    {
      "epoch": 51.37100204973508,
      "grad_norm": 11.557904243469238,
      "learning_rate": 7.190831625220766e-06,
      "loss": 1.7771,
      "step": 1328300
    },
    {
      "epoch": 51.37486947441698,
      "grad_norm": 10.868022918701172,
      "learning_rate": 7.187608771319179e-06,
      "loss": 1.8175,
      "step": 1328400
    },
    {
      "epoch": 51.37873689909889,
      "grad_norm": 8.47095012664795,
      "learning_rate": 7.184385917417592e-06,
      "loss": 1.7844,
      "step": 1328500
    },
    {
      "epoch": 51.3826043237808,
      "grad_norm": 10.994974136352539,
      "learning_rate": 7.181163063516005e-06,
      "loss": 1.7524,
      "step": 1328600
    },
    {
      "epoch": 51.3864717484627,
      "grad_norm": 10.60422420501709,
      "learning_rate": 7.177940209614418e-06,
      "loss": 1.7826,
      "step": 1328700
    },
    {
      "epoch": 51.3903391731446,
      "grad_norm": 11.892187118530273,
      "learning_rate": 7.174717355712832e-06,
      "loss": 1.7185,
      "step": 1328800
    },
    {
      "epoch": 51.39420659782651,
      "grad_norm": 12.788070678710938,
      "learning_rate": 7.171494501811245e-06,
      "loss": 1.8029,
      "step": 1328900
    },
    {
      "epoch": 51.39807402250841,
      "grad_norm": 11.23654842376709,
      "learning_rate": 7.168271647909658e-06,
      "loss": 1.7248,
      "step": 1329000
    },
    {
      "epoch": 51.40194144719032,
      "grad_norm": 14.963526725769043,
      "learning_rate": 7.165048794008071e-06,
      "loss": 1.92,
      "step": 1329100
    },
    {
      "epoch": 51.40580887187222,
      "grad_norm": 14.6593017578125,
      "learning_rate": 7.161825940106484e-06,
      "loss": 1.8139,
      "step": 1329200
    },
    {
      "epoch": 51.409676296554125,
      "grad_norm": 13.306676864624023,
      "learning_rate": 7.1586030862048955e-06,
      "loss": 1.7787,
      "step": 1329300
    },
    {
      "epoch": 51.41354372123603,
      "grad_norm": 15.81606674194336,
      "learning_rate": 7.155380232303309e-06,
      "loss": 1.7762,
      "step": 1329400
    },
    {
      "epoch": 51.41741114591793,
      "grad_norm": 10.970779418945312,
      "learning_rate": 7.152157378401722e-06,
      "loss": 1.8244,
      "step": 1329500
    },
    {
      "epoch": 51.42127857059984,
      "grad_norm": 11.990473747253418,
      "learning_rate": 7.148934524500135e-06,
      "loss": 1.7025,
      "step": 1329600
    },
    {
      "epoch": 51.42514599528174,
      "grad_norm": 12.729669570922852,
      "learning_rate": 7.1457116705985484e-06,
      "loss": 1.8307,
      "step": 1329700
    },
    {
      "epoch": 51.429013419963646,
      "grad_norm": 13.113510131835938,
      "learning_rate": 7.1424888166969615e-06,
      "loss": 1.8093,
      "step": 1329800
    },
    {
      "epoch": 51.43288084464555,
      "grad_norm": 7.876097679138184,
      "learning_rate": 7.1392659627953745e-06,
      "loss": 1.8373,
      "step": 1329900
    },
    {
      "epoch": 51.43674826932745,
      "grad_norm": 12.245917320251465,
      "learning_rate": 7.1360431088937875e-06,
      "loss": 1.6768,
      "step": 1330000
    },
    {
      "epoch": 51.44061569400936,
      "grad_norm": 12.876152992248535,
      "learning_rate": 7.1328202549922005e-06,
      "loss": 1.8036,
      "step": 1330100
    },
    {
      "epoch": 51.44448311869127,
      "grad_norm": 13.134440422058105,
      "learning_rate": 7.1295974010906136e-06,
      "loss": 1.8114,
      "step": 1330200
    },
    {
      "epoch": 51.44835054337317,
      "grad_norm": 13.576958656311035,
      "learning_rate": 7.126374547189027e-06,
      "loss": 1.7646,
      "step": 1330300
    },
    {
      "epoch": 51.45221796805507,
      "grad_norm": 13.783659934997559,
      "learning_rate": 7.1231516932874405e-06,
      "loss": 1.7185,
      "step": 1330400
    },
    {
      "epoch": 51.45608539273697,
      "grad_norm": 11.763224601745605,
      "learning_rate": 7.1199288393858535e-06,
      "loss": 1.8023,
      "step": 1330500
    },
    {
      "epoch": 51.45995281741888,
      "grad_norm": 20.179906845092773,
      "learning_rate": 7.1167059854842665e-06,
      "loss": 1.8086,
      "step": 1330600
    },
    {
      "epoch": 51.46382024210079,
      "grad_norm": 16.280166625976562,
      "learning_rate": 7.1134831315826795e-06,
      "loss": 1.7501,
      "step": 1330700
    },
    {
      "epoch": 51.46768766678269,
      "grad_norm": 15.310214042663574,
      "learning_rate": 7.1102602776810926e-06,
      "loss": 1.8307,
      "step": 1330800
    },
    {
      "epoch": 51.471555091464595,
      "grad_norm": 8.883111000061035,
      "learning_rate": 7.107037423779506e-06,
      "loss": 1.6803,
      "step": 1330900
    },
    {
      "epoch": 51.4754225161465,
      "grad_norm": 8.119510650634766,
      "learning_rate": 7.103814569877919e-06,
      "loss": 1.7431,
      "step": 1331000
    },
    {
      "epoch": 51.4792899408284,
      "grad_norm": 10.99188232421875,
      "learning_rate": 7.100591715976332e-06,
      "loss": 1.7671,
      "step": 1331100
    },
    {
      "epoch": 51.48315736551031,
      "grad_norm": 11.143167495727539,
      "learning_rate": 7.097368862074745e-06,
      "loss": 1.7816,
      "step": 1331200
    },
    {
      "epoch": 51.48702479019221,
      "grad_norm": 11.01107406616211,
      "learning_rate": 7.0941460081731585e-06,
      "loss": 1.7104,
      "step": 1331300
    },
    {
      "epoch": 51.490892214874115,
      "grad_norm": 11.926837921142578,
      "learning_rate": 7.0909231542715716e-06,
      "loss": 1.8102,
      "step": 1331400
    },
    {
      "epoch": 51.49475963955602,
      "grad_norm": 10.732847213745117,
      "learning_rate": 7.0877003003699846e-06,
      "loss": 1.8249,
      "step": 1331500
    },
    {
      "epoch": 51.49862706423792,
      "grad_norm": 14.568906784057617,
      "learning_rate": 7.084477446468398e-06,
      "loss": 1.769,
      "step": 1331600
    },
    {
      "epoch": 51.50249448891983,
      "grad_norm": 13.143807411193848,
      "learning_rate": 7.081254592566811e-06,
      "loss": 1.7825,
      "step": 1331700
    },
    {
      "epoch": 51.50636191360173,
      "grad_norm": 10.462380409240723,
      "learning_rate": 7.078031738665224e-06,
      "loss": 1.8141,
      "step": 1331800
    },
    {
      "epoch": 51.51022933828364,
      "grad_norm": 12.119305610656738,
      "learning_rate": 7.074808884763637e-06,
      "loss": 1.7367,
      "step": 1331900
    },
    {
      "epoch": 51.51409676296554,
      "grad_norm": 13.33907699584961,
      "learning_rate": 7.07158603086205e-06,
      "loss": 1.7326,
      "step": 1332000
    },
    {
      "epoch": 51.51796418764744,
      "grad_norm": 8.98993968963623,
      "learning_rate": 7.068363176960463e-06,
      "loss": 1.7718,
      "step": 1332100
    },
    {
      "epoch": 51.52183161232935,
      "grad_norm": 15.670402526855469,
      "learning_rate": 7.065140323058875e-06,
      "loss": 1.8355,
      "step": 1332200
    },
    {
      "epoch": 51.52569903701126,
      "grad_norm": 12.536529541015625,
      "learning_rate": 7.061917469157288e-06,
      "loss": 1.803,
      "step": 1332300
    },
    {
      "epoch": 51.52956646169316,
      "grad_norm": 13.503169059753418,
      "learning_rate": 7.058694615255701e-06,
      "loss": 1.725,
      "step": 1332400
    },
    {
      "epoch": 51.533433886375065,
      "grad_norm": 13.012994766235352,
      "learning_rate": 7.055471761354114e-06,
      "loss": 1.8264,
      "step": 1332500
    },
    {
      "epoch": 51.537301311056964,
      "grad_norm": 17.922115325927734,
      "learning_rate": 7.052248907452527e-06,
      "loss": 1.7499,
      "step": 1332600
    },
    {
      "epoch": 51.54116873573887,
      "grad_norm": 14.557778358459473,
      "learning_rate": 7.04902605355094e-06,
      "loss": 1.7815,
      "step": 1332700
    },
    {
      "epoch": 51.54503616042078,
      "grad_norm": 15.134666442871094,
      "learning_rate": 7.045803199649353e-06,
      "loss": 1.7084,
      "step": 1332800
    },
    {
      "epoch": 51.54890358510268,
      "grad_norm": 15.549198150634766,
      "learning_rate": 7.042580345747767e-06,
      "loss": 1.713,
      "step": 1332900
    },
    {
      "epoch": 51.552771009784585,
      "grad_norm": 10.840845108032227,
      "learning_rate": 7.03935749184618e-06,
      "loss": 1.7446,
      "step": 1333000
    },
    {
      "epoch": 51.556638434466485,
      "grad_norm": 13.452115058898926,
      "learning_rate": 7.036134637944593e-06,
      "loss": 1.8465,
      "step": 1333100
    },
    {
      "epoch": 51.56050585914839,
      "grad_norm": 12.128873825073242,
      "learning_rate": 7.032911784043006e-06,
      "loss": 1.7933,
      "step": 1333200
    },
    {
      "epoch": 51.5643732838303,
      "grad_norm": 12.782730102539062,
      "learning_rate": 7.029688930141419e-06,
      "loss": 1.772,
      "step": 1333300
    },
    {
      "epoch": 51.5682407085122,
      "grad_norm": 13.049915313720703,
      "learning_rate": 7.026466076239832e-06,
      "loss": 1.8046,
      "step": 1333400
    },
    {
      "epoch": 51.57210813319411,
      "grad_norm": 9.106488227844238,
      "learning_rate": 7.023243222338245e-06,
      "loss": 1.8157,
      "step": 1333500
    },
    {
      "epoch": 51.57597555787601,
      "grad_norm": 16.95892906188965,
      "learning_rate": 7.020020368436658e-06,
      "loss": 1.8039,
      "step": 1333600
    },
    {
      "epoch": 51.57984298255791,
      "grad_norm": 12.260705947875977,
      "learning_rate": 7.016797514535071e-06,
      "loss": 1.806,
      "step": 1333700
    },
    {
      "epoch": 51.58371040723982,
      "grad_norm": 12.180439949035645,
      "learning_rate": 7.013574660633485e-06,
      "loss": 1.7261,
      "step": 1333800
    },
    {
      "epoch": 51.58757783192172,
      "grad_norm": 16.487773895263672,
      "learning_rate": 7.010351806731898e-06,
      "loss": 1.7513,
      "step": 1333900
    },
    {
      "epoch": 51.59144525660363,
      "grad_norm": 12.775782585144043,
      "learning_rate": 7.007128952830311e-06,
      "loss": 1.7658,
      "step": 1334000
    },
    {
      "epoch": 51.595312681285534,
      "grad_norm": 17.85951042175293,
      "learning_rate": 7.003906098928724e-06,
      "loss": 1.8189,
      "step": 1334100
    },
    {
      "epoch": 51.599180105967434,
      "grad_norm": 12.495078086853027,
      "learning_rate": 7.000683245027137e-06,
      "loss": 1.7794,
      "step": 1334200
    },
    {
      "epoch": 51.60304753064934,
      "grad_norm": 12.328926086425781,
      "learning_rate": 6.99746039112555e-06,
      "loss": 1.7921,
      "step": 1334300
    },
    {
      "epoch": 51.60691495533125,
      "grad_norm": 12.505142211914062,
      "learning_rate": 6.994237537223963e-06,
      "loss": 1.7851,
      "step": 1334400
    },
    {
      "epoch": 51.61078238001315,
      "grad_norm": 12.131108283996582,
      "learning_rate": 6.991014683322376e-06,
      "loss": 1.8435,
      "step": 1334500
    },
    {
      "epoch": 51.614649804695055,
      "grad_norm": 9.333847045898438,
      "learning_rate": 6.987791829420789e-06,
      "loss": 1.7307,
      "step": 1334600
    },
    {
      "epoch": 51.618517229376955,
      "grad_norm": 14.66129207611084,
      "learning_rate": 6.984568975519202e-06,
      "loss": 1.779,
      "step": 1334700
    },
    {
      "epoch": 51.62238465405886,
      "grad_norm": 16.146053314208984,
      "learning_rate": 6.981346121617616e-06,
      "loss": 1.7835,
      "step": 1334800
    },
    {
      "epoch": 51.62625207874077,
      "grad_norm": 14.249693870544434,
      "learning_rate": 6.978123267716029e-06,
      "loss": 1.7628,
      "step": 1334900
    },
    {
      "epoch": 51.63011950342267,
      "grad_norm": 14.597246170043945,
      "learning_rate": 6.974900413814442e-06,
      "loss": 1.761,
      "step": 1335000
    },
    {
      "epoch": 51.63398692810458,
      "grad_norm": 13.549468994140625,
      "learning_rate": 6.971677559912855e-06,
      "loss": 1.8267,
      "step": 1335100
    },
    {
      "epoch": 51.637854352786476,
      "grad_norm": 7.256871700286865,
      "learning_rate": 6.9684547060112665e-06,
      "loss": 1.8388,
      "step": 1335200
    },
    {
      "epoch": 51.64172177746838,
      "grad_norm": 11.446998596191406,
      "learning_rate": 6.9652318521096795e-06,
      "loss": 1.7959,
      "step": 1335300
    },
    {
      "epoch": 51.64558920215029,
      "grad_norm": 14.821459770202637,
      "learning_rate": 6.962008998208093e-06,
      "loss": 1.8236,
      "step": 1335400
    },
    {
      "epoch": 51.64945662683219,
      "grad_norm": 12.153233528137207,
      "learning_rate": 6.958786144306506e-06,
      "loss": 1.8001,
      "step": 1335500
    },
    {
      "epoch": 51.6533240515141,
      "grad_norm": 11.650853157043457,
      "learning_rate": 6.955563290404919e-06,
      "loss": 1.779,
      "step": 1335600
    },
    {
      "epoch": 51.657191476196004,
      "grad_norm": 12.326849937438965,
      "learning_rate": 6.9523404365033324e-06,
      "loss": 1.8587,
      "step": 1335700
    },
    {
      "epoch": 51.661058900877904,
      "grad_norm": 14.419425010681152,
      "learning_rate": 6.9491175826017455e-06,
      "loss": 1.7459,
      "step": 1335800
    },
    {
      "epoch": 51.66492632555981,
      "grad_norm": 12.915239334106445,
      "learning_rate": 6.9458947287001585e-06,
      "loss": 1.7726,
      "step": 1335900
    },
    {
      "epoch": 51.66879375024171,
      "grad_norm": 12.834400177001953,
      "learning_rate": 6.9426718747985715e-06,
      "loss": 1.7409,
      "step": 1336000
    },
    {
      "epoch": 51.67266117492362,
      "grad_norm": 14.62205696105957,
      "learning_rate": 6.9394490208969845e-06,
      "loss": 1.8046,
      "step": 1336100
    },
    {
      "epoch": 51.676528599605525,
      "grad_norm": 12.954919815063477,
      "learning_rate": 6.9362261669953976e-06,
      "loss": 1.7977,
      "step": 1336200
    },
    {
      "epoch": 51.680396024287425,
      "grad_norm": 12.417217254638672,
      "learning_rate": 6.9330033130938114e-06,
      "loss": 1.7526,
      "step": 1336300
    },
    {
      "epoch": 51.68426344896933,
      "grad_norm": 12.48228645324707,
      "learning_rate": 6.9297804591922245e-06,
      "loss": 1.8078,
      "step": 1336400
    },
    {
      "epoch": 51.68813087365123,
      "grad_norm": 11.64274787902832,
      "learning_rate": 6.9265576052906375e-06,
      "loss": 1.7897,
      "step": 1336500
    },
    {
      "epoch": 51.69199829833314,
      "grad_norm": 13.02096939086914,
      "learning_rate": 6.9233347513890505e-06,
      "loss": 1.7634,
      "step": 1336600
    },
    {
      "epoch": 51.69586572301505,
      "grad_norm": 11.715137481689453,
      "learning_rate": 6.9201118974874635e-06,
      "loss": 1.8219,
      "step": 1336700
    },
    {
      "epoch": 51.699733147696946,
      "grad_norm": 11.14370346069336,
      "learning_rate": 6.9168890435858766e-06,
      "loss": 1.7663,
      "step": 1336800
    },
    {
      "epoch": 51.70360057237885,
      "grad_norm": 9.25361156463623,
      "learning_rate": 6.91366618968429e-06,
      "loss": 1.7965,
      "step": 1336900
    },
    {
      "epoch": 51.70746799706076,
      "grad_norm": 10.976664543151855,
      "learning_rate": 6.910443335782703e-06,
      "loss": 1.8009,
      "step": 1337000
    },
    {
      "epoch": 51.71133542174266,
      "grad_norm": 11.575203895568848,
      "learning_rate": 6.907220481881116e-06,
      "loss": 1.7174,
      "step": 1337100
    },
    {
      "epoch": 51.71520284642457,
      "grad_norm": 11.802205085754395,
      "learning_rate": 6.903997627979529e-06,
      "loss": 1.7741,
      "step": 1337200
    },
    {
      "epoch": 51.71907027110647,
      "grad_norm": 14.42923641204834,
      "learning_rate": 6.9007747740779425e-06,
      "loss": 1.7192,
      "step": 1337300
    },
    {
      "epoch": 51.722937695788374,
      "grad_norm": 12.821736335754395,
      "learning_rate": 6.8975519201763556e-06,
      "loss": 1.8537,
      "step": 1337400
    },
    {
      "epoch": 51.72680512047028,
      "grad_norm": 17.313264846801758,
      "learning_rate": 6.8943290662747686e-06,
      "loss": 1.7236,
      "step": 1337500
    },
    {
      "epoch": 51.73067254515218,
      "grad_norm": 11.728497505187988,
      "learning_rate": 6.891106212373182e-06,
      "loss": 1.7829,
      "step": 1337600
    },
    {
      "epoch": 51.73453996983409,
      "grad_norm": 12.780245780944824,
      "learning_rate": 6.887883358471595e-06,
      "loss": 1.7866,
      "step": 1337700
    },
    {
      "epoch": 51.73840739451599,
      "grad_norm": 11.519803047180176,
      "learning_rate": 6.884660504570008e-06,
      "loss": 1.7356,
      "step": 1337800
    },
    {
      "epoch": 51.742274819197895,
      "grad_norm": 11.075790405273438,
      "learning_rate": 6.881437650668421e-06,
      "loss": 1.7845,
      "step": 1337900
    },
    {
      "epoch": 51.7461422438798,
      "grad_norm": 5.137162685394287,
      "learning_rate": 6.878214796766834e-06,
      "loss": 1.6986,
      "step": 1338000
    },
    {
      "epoch": 51.7500096685617,
      "grad_norm": 13.096726417541504,
      "learning_rate": 6.874991942865246e-06,
      "loss": 1.7328,
      "step": 1338100
    },
    {
      "epoch": 51.75387709324361,
      "grad_norm": 13.152765274047852,
      "learning_rate": 6.871769088963659e-06,
      "loss": 1.7859,
      "step": 1338200
    },
    {
      "epoch": 51.75774451792552,
      "grad_norm": 10.533207893371582,
      "learning_rate": 6.868546235062072e-06,
      "loss": 1.7128,
      "step": 1338300
    },
    {
      "epoch": 51.761611942607416,
      "grad_norm": 11.816722869873047,
      "learning_rate": 6.865323381160485e-06,
      "loss": 1.8206,
      "step": 1338400
    },
    {
      "epoch": 51.76547936728932,
      "grad_norm": 12.467381477355957,
      "learning_rate": 6.862100527258898e-06,
      "loss": 1.7931,
      "step": 1338500
    },
    {
      "epoch": 51.76934679197122,
      "grad_norm": 12.027822494506836,
      "learning_rate": 6.858877673357311e-06,
      "loss": 1.7104,
      "step": 1338600
    },
    {
      "epoch": 51.77321421665313,
      "grad_norm": 12.350261688232422,
      "learning_rate": 6.855654819455724e-06,
      "loss": 1.8348,
      "step": 1338700
    },
    {
      "epoch": 51.77708164133504,
      "grad_norm": 13.065791130065918,
      "learning_rate": 6.852431965554137e-06,
      "loss": 1.7816,
      "step": 1338800
    },
    {
      "epoch": 51.78094906601694,
      "grad_norm": 11.35709285736084,
      "learning_rate": 6.849209111652551e-06,
      "loss": 1.7937,
      "step": 1338900
    },
    {
      "epoch": 51.784816490698844,
      "grad_norm": 16.912084579467773,
      "learning_rate": 6.845986257750964e-06,
      "loss": 1.8585,
      "step": 1339000
    },
    {
      "epoch": 51.78868391538075,
      "grad_norm": 8.522788047790527,
      "learning_rate": 6.842763403849377e-06,
      "loss": 1.6698,
      "step": 1339100
    },
    {
      "epoch": 51.79255134006265,
      "grad_norm": 16.76004409790039,
      "learning_rate": 6.83954054994779e-06,
      "loss": 1.7652,
      "step": 1339200
    },
    {
      "epoch": 51.79641876474456,
      "grad_norm": 12.571039199829102,
      "learning_rate": 6.836317696046203e-06,
      "loss": 1.7822,
      "step": 1339300
    },
    {
      "epoch": 51.80028618942646,
      "grad_norm": 11.764710426330566,
      "learning_rate": 6.833094842144616e-06,
      "loss": 1.7762,
      "step": 1339400
    },
    {
      "epoch": 51.804153614108365,
      "grad_norm": 12.42782211303711,
      "learning_rate": 6.829871988243029e-06,
      "loss": 1.8015,
      "step": 1339500
    },
    {
      "epoch": 51.80802103879027,
      "grad_norm": 14.123108863830566,
      "learning_rate": 6.826649134341442e-06,
      "loss": 1.7643,
      "step": 1339600
    },
    {
      "epoch": 51.81188846347217,
      "grad_norm": 13.729874610900879,
      "learning_rate": 6.823426280439855e-06,
      "loss": 1.7572,
      "step": 1339700
    },
    {
      "epoch": 51.81575588815408,
      "grad_norm": 12.560038566589355,
      "learning_rate": 6.820203426538269e-06,
      "loss": 1.7426,
      "step": 1339800
    },
    {
      "epoch": 51.81962331283598,
      "grad_norm": 14.148181915283203,
      "learning_rate": 6.816980572636682e-06,
      "loss": 1.8483,
      "step": 1339900
    },
    {
      "epoch": 51.823490737517886,
      "grad_norm": 10.69796371459961,
      "learning_rate": 6.813757718735095e-06,
      "loss": 1.7522,
      "step": 1340000
    },
    {
      "epoch": 51.82735816219979,
      "grad_norm": 12.059617042541504,
      "learning_rate": 6.810534864833508e-06,
      "loss": 1.7845,
      "step": 1340100
    },
    {
      "epoch": 51.83122558688169,
      "grad_norm": 16.609661102294922,
      "learning_rate": 6.807312010931921e-06,
      "loss": 1.7458,
      "step": 1340200
    },
    {
      "epoch": 51.8350930115636,
      "grad_norm": 13.816597938537598,
      "learning_rate": 6.804089157030334e-06,
      "loss": 1.794,
      "step": 1340300
    },
    {
      "epoch": 51.83896043624551,
      "grad_norm": 12.624776840209961,
      "learning_rate": 6.800866303128747e-06,
      "loss": 1.8182,
      "step": 1340400
    },
    {
      "epoch": 51.84282786092741,
      "grad_norm": 12.580967903137207,
      "learning_rate": 6.79764344922716e-06,
      "loss": 1.8294,
      "step": 1340500
    },
    {
      "epoch": 51.846695285609314,
      "grad_norm": 10.38331127166748,
      "learning_rate": 6.794420595325573e-06,
      "loss": 1.827,
      "step": 1340600
    },
    {
      "epoch": 51.850562710291214,
      "grad_norm": 11.858854293823242,
      "learning_rate": 6.791197741423987e-06,
      "loss": 1.7281,
      "step": 1340700
    },
    {
      "epoch": 51.85443013497312,
      "grad_norm": 12.963747024536133,
      "learning_rate": 6.7879748875224e-06,
      "loss": 1.8838,
      "step": 1340800
    },
    {
      "epoch": 51.85829755965503,
      "grad_norm": 11.176844596862793,
      "learning_rate": 6.784752033620813e-06,
      "loss": 1.8864,
      "step": 1340900
    },
    {
      "epoch": 51.86216498433693,
      "grad_norm": 12.907992362976074,
      "learning_rate": 6.781529179719226e-06,
      "loss": 1.8481,
      "step": 1341000
    },
    {
      "epoch": 51.866032409018835,
      "grad_norm": 13.504883766174316,
      "learning_rate": 6.7783063258176375e-06,
      "loss": 1.7231,
      "step": 1341100
    },
    {
      "epoch": 51.869899833700735,
      "grad_norm": 11.210237503051758,
      "learning_rate": 6.7750834719160505e-06,
      "loss": 1.8086,
      "step": 1341200
    },
    {
      "epoch": 51.87376725838264,
      "grad_norm": 11.673274040222168,
      "learning_rate": 6.7718606180144635e-06,
      "loss": 1.7387,
      "step": 1341300
    },
    {
      "epoch": 51.87763468306455,
      "grad_norm": 12.315673828125,
      "learning_rate": 6.768637764112877e-06,
      "loss": 1.7784,
      "step": 1341400
    },
    {
      "epoch": 51.88150210774645,
      "grad_norm": 11.927974700927734,
      "learning_rate": 6.76541491021129e-06,
      "loss": 1.7777,
      "step": 1341500
    },
    {
      "epoch": 51.885369532428356,
      "grad_norm": 15.330769538879395,
      "learning_rate": 6.762192056309703e-06,
      "loss": 1.7646,
      "step": 1341600
    },
    {
      "epoch": 51.88923695711026,
      "grad_norm": 10.376373291015625,
      "learning_rate": 6.7589692024081164e-06,
      "loss": 1.8693,
      "step": 1341700
    },
    {
      "epoch": 51.89310438179216,
      "grad_norm": 10.613842010498047,
      "learning_rate": 6.7557463485065295e-06,
      "loss": 1.8237,
      "step": 1341800
    },
    {
      "epoch": 51.89697180647407,
      "grad_norm": 13.538751602172852,
      "learning_rate": 6.7525234946049425e-06,
      "loss": 1.861,
      "step": 1341900
    },
    {
      "epoch": 51.90083923115597,
      "grad_norm": 12.40259075164795,
      "learning_rate": 6.7493006407033555e-06,
      "loss": 1.8042,
      "step": 1342000
    },
    {
      "epoch": 51.90470665583788,
      "grad_norm": 18.741914749145508,
      "learning_rate": 6.7460777868017685e-06,
      "loss": 1.8208,
      "step": 1342100
    },
    {
      "epoch": 51.908574080519784,
      "grad_norm": 13.644217491149902,
      "learning_rate": 6.7428549329001816e-06,
      "loss": 1.7406,
      "step": 1342200
    },
    {
      "epoch": 51.912441505201684,
      "grad_norm": 11.35527229309082,
      "learning_rate": 6.7396320789985954e-06,
      "loss": 1.8281,
      "step": 1342300
    },
    {
      "epoch": 51.91630892988359,
      "grad_norm": 14.699803352355957,
      "learning_rate": 6.7364092250970085e-06,
      "loss": 1.7964,
      "step": 1342400
    },
    {
      "epoch": 51.9201763545655,
      "grad_norm": 12.883013725280762,
      "learning_rate": 6.7331863711954215e-06,
      "loss": 1.8466,
      "step": 1342500
    },
    {
      "epoch": 51.9240437792474,
      "grad_norm": 18.71197509765625,
      "learning_rate": 6.7299635172938345e-06,
      "loss": 1.7588,
      "step": 1342600
    },
    {
      "epoch": 51.927911203929305,
      "grad_norm": 12.322288513183594,
      "learning_rate": 6.7267406633922475e-06,
      "loss": 1.6827,
      "step": 1342700
    },
    {
      "epoch": 51.931778628611205,
      "grad_norm": 12.886544227600098,
      "learning_rate": 6.7235178094906606e-06,
      "loss": 1.7538,
      "step": 1342800
    },
    {
      "epoch": 51.93564605329311,
      "grad_norm": 14.853845596313477,
      "learning_rate": 6.720294955589074e-06,
      "loss": 1.8733,
      "step": 1342900
    },
    {
      "epoch": 51.93951347797502,
      "grad_norm": 13.56688404083252,
      "learning_rate": 6.717072101687487e-06,
      "loss": 1.7795,
      "step": 1343000
    },
    {
      "epoch": 51.94338090265692,
      "grad_norm": 11.78614330291748,
      "learning_rate": 6.7138492477859e-06,
      "loss": 1.6933,
      "step": 1343100
    },
    {
      "epoch": 51.947248327338826,
      "grad_norm": 12.702621459960938,
      "learning_rate": 6.710626393884313e-06,
      "loss": 1.668,
      "step": 1343200
    },
    {
      "epoch": 51.951115752020726,
      "grad_norm": 10.217835426330566,
      "learning_rate": 6.7074035399827265e-06,
      "loss": 1.8337,
      "step": 1343300
    },
    {
      "epoch": 51.95498317670263,
      "grad_norm": 12.532183647155762,
      "learning_rate": 6.7041806860811396e-06,
      "loss": 1.6891,
      "step": 1343400
    },
    {
      "epoch": 51.95885060138454,
      "grad_norm": 11.99245834350586,
      "learning_rate": 6.700957832179553e-06,
      "loss": 1.8113,
      "step": 1343500
    },
    {
      "epoch": 51.96271802606644,
      "grad_norm": 12.0961275100708,
      "learning_rate": 6.697734978277966e-06,
      "loss": 1.7543,
      "step": 1343600
    },
    {
      "epoch": 51.96658545074835,
      "grad_norm": 9.218660354614258,
      "learning_rate": 6.694512124376379e-06,
      "loss": 1.8812,
      "step": 1343700
    },
    {
      "epoch": 51.970452875430254,
      "grad_norm": 11.958976745605469,
      "learning_rate": 6.691289270474792e-06,
      "loss": 1.9114,
      "step": 1343800
    },
    {
      "epoch": 51.974320300112154,
      "grad_norm": 12.61545181274414,
      "learning_rate": 6.688066416573205e-06,
      "loss": 1.8251,
      "step": 1343900
    },
    {
      "epoch": 51.97818772479406,
      "grad_norm": 10.576305389404297,
      "learning_rate": 6.684843562671617e-06,
      "loss": 1.7704,
      "step": 1344000
    },
    {
      "epoch": 51.98205514947596,
      "grad_norm": 13.57042121887207,
      "learning_rate": 6.68162070877003e-06,
      "loss": 1.7437,
      "step": 1344100
    },
    {
      "epoch": 51.98592257415787,
      "grad_norm": 12.822480201721191,
      "learning_rate": 6.678397854868443e-06,
      "loss": 1.8972,
      "step": 1344200
    },
    {
      "epoch": 51.989789998839775,
      "grad_norm": 11.82695484161377,
      "learning_rate": 6.675175000966856e-06,
      "loss": 1.8456,
      "step": 1344300
    },
    {
      "epoch": 51.993657423521675,
      "grad_norm": 12.095558166503906,
      "learning_rate": 6.671952147065269e-06,
      "loss": 1.9076,
      "step": 1344400
    },
    {
      "epoch": 51.99752484820358,
      "grad_norm": 12.90982437133789,
      "learning_rate": 6.668729293163682e-06,
      "loss": 1.713,
      "step": 1344500
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.7460126876831055,
      "eval_runtime": 2.9526,
      "eval_samples_per_second": 460.944,
      "eval_steps_per_second": 460.944,
      "step": 1344564
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.5793498754501343,
      "eval_runtime": 56.3877,
      "eval_samples_per_second": 458.557,
      "eval_steps_per_second": 458.557,
      "step": 1344564
    },
    {
      "epoch": 52.00139227288548,
      "grad_norm": 13.497379302978516,
      "learning_rate": 6.665506439262095e-06,
      "loss": 1.85,
      "step": 1344600
    },
    {
      "epoch": 52.00525969756739,
      "grad_norm": 13.655765533447266,
      "learning_rate": 6.662283585360508e-06,
      "loss": 1.7301,
      "step": 1344700
    },
    {
      "epoch": 52.009127122249296,
      "grad_norm": 11.880901336669922,
      "learning_rate": 6.659060731458922e-06,
      "loss": 1.868,
      "step": 1344800
    },
    {
      "epoch": 52.012994546931196,
      "grad_norm": 13.773818969726562,
      "learning_rate": 6.655837877557335e-06,
      "loss": 1.806,
      "step": 1344900
    },
    {
      "epoch": 52.0168619716131,
      "grad_norm": 12.392658233642578,
      "learning_rate": 6.652615023655748e-06,
      "loss": 1.7899,
      "step": 1345000
    },
    {
      "epoch": 52.02072939629501,
      "grad_norm": 11.054262161254883,
      "learning_rate": 6.649392169754161e-06,
      "loss": 1.7617,
      "step": 1345100
    },
    {
      "epoch": 52.02459682097691,
      "grad_norm": 15.585359573364258,
      "learning_rate": 6.646169315852574e-06,
      "loss": 1.6688,
      "step": 1345200
    },
    {
      "epoch": 52.02846424565882,
      "grad_norm": 14.515094757080078,
      "learning_rate": 6.642946461950987e-06,
      "loss": 1.6521,
      "step": 1345300
    },
    {
      "epoch": 52.03233167034072,
      "grad_norm": 14.061918258666992,
      "learning_rate": 6.6397236080494e-06,
      "loss": 1.748,
      "step": 1345400
    },
    {
      "epoch": 52.036199095022624,
      "grad_norm": 15.10035514831543,
      "learning_rate": 6.636500754147813e-06,
      "loss": 1.7679,
      "step": 1345500
    },
    {
      "epoch": 52.04006651970453,
      "grad_norm": 10.688619613647461,
      "learning_rate": 6.633277900246226e-06,
      "loss": 1.8081,
      "step": 1345600
    },
    {
      "epoch": 52.04393394438643,
      "grad_norm": 11.583009719848633,
      "learning_rate": 6.630055046344639e-06,
      "loss": 1.7683,
      "step": 1345700
    },
    {
      "epoch": 52.04780136906834,
      "grad_norm": 10.960916519165039,
      "learning_rate": 6.626832192443053e-06,
      "loss": 1.7895,
      "step": 1345800
    },
    {
      "epoch": 52.05166879375024,
      "grad_norm": 12.055015563964844,
      "learning_rate": 6.623609338541466e-06,
      "loss": 1.8056,
      "step": 1345900
    },
    {
      "epoch": 52.055536218432145,
      "grad_norm": 10.210773468017578,
      "learning_rate": 6.620386484639879e-06,
      "loss": 1.8237,
      "step": 1346000
    },
    {
      "epoch": 52.05940364311405,
      "grad_norm": 16.102298736572266,
      "learning_rate": 6.617163630738292e-06,
      "loss": 1.7503,
      "step": 1346100
    },
    {
      "epoch": 52.06327106779595,
      "grad_norm": 13.001286506652832,
      "learning_rate": 6.613940776836705e-06,
      "loss": 1.735,
      "step": 1346200
    },
    {
      "epoch": 52.06713849247786,
      "grad_norm": 11.315032958984375,
      "learning_rate": 6.610717922935118e-06,
      "loss": 1.6913,
      "step": 1346300
    },
    {
      "epoch": 52.071005917159766,
      "grad_norm": 14.369368553161621,
      "learning_rate": 6.607495069033531e-06,
      "loss": 1.7582,
      "step": 1346400
    },
    {
      "epoch": 52.074873341841666,
      "grad_norm": 10.442972183227539,
      "learning_rate": 6.604272215131944e-06,
      "loss": 1.7053,
      "step": 1346500
    },
    {
      "epoch": 52.07874076652357,
      "grad_norm": 12.828675270080566,
      "learning_rate": 6.601049361230357e-06,
      "loss": 1.8168,
      "step": 1346600
    },
    {
      "epoch": 52.08260819120547,
      "grad_norm": 11.233417510986328,
      "learning_rate": 6.597826507328771e-06,
      "loss": 1.7771,
      "step": 1346700
    },
    {
      "epoch": 52.08647561588738,
      "grad_norm": 9.413848876953125,
      "learning_rate": 6.594603653427184e-06,
      "loss": 1.691,
      "step": 1346800
    },
    {
      "epoch": 52.09034304056929,
      "grad_norm": 14.199069023132324,
      "learning_rate": 6.591380799525597e-06,
      "loss": 1.7014,
      "step": 1346900
    },
    {
      "epoch": 52.09421046525119,
      "grad_norm": 14.024291038513184,
      "learning_rate": 6.5881579456240084e-06,
      "loss": 1.7227,
      "step": 1347000
    },
    {
      "epoch": 52.098077889933094,
      "grad_norm": 13.499053001403809,
      "learning_rate": 6.5849350917224215e-06,
      "loss": 1.836,
      "step": 1347100
    },
    {
      "epoch": 52.101945314615,
      "grad_norm": 13.449258804321289,
      "learning_rate": 6.5817122378208345e-06,
      "loss": 1.745,
      "step": 1347200
    },
    {
      "epoch": 52.1058127392969,
      "grad_norm": 11.789474487304688,
      "learning_rate": 6.578489383919248e-06,
      "loss": 1.8276,
      "step": 1347300
    },
    {
      "epoch": 52.10968016397881,
      "grad_norm": 10.40480899810791,
      "learning_rate": 6.575266530017661e-06,
      "loss": 1.8362,
      "step": 1347400
    },
    {
      "epoch": 52.11354758866071,
      "grad_norm": 12.065388679504395,
      "learning_rate": 6.572043676116074e-06,
      "loss": 1.7082,
      "step": 1347500
    },
    {
      "epoch": 52.117415013342615,
      "grad_norm": 10.76099681854248,
      "learning_rate": 6.568820822214487e-06,
      "loss": 1.8216,
      "step": 1347600
    },
    {
      "epoch": 52.12128243802452,
      "grad_norm": 14.122944831848145,
      "learning_rate": 6.5655979683129004e-06,
      "loss": 1.8594,
      "step": 1347700
    },
    {
      "epoch": 52.12514986270642,
      "grad_norm": 12.757719993591309,
      "learning_rate": 6.5623751144113135e-06,
      "loss": 1.8205,
      "step": 1347800
    },
    {
      "epoch": 52.12901728738833,
      "grad_norm": 12.999637603759766,
      "learning_rate": 6.5591522605097265e-06,
      "loss": 1.7017,
      "step": 1347900
    },
    {
      "epoch": 52.13288471207023,
      "grad_norm": 14.212145805358887,
      "learning_rate": 6.5559294066081395e-06,
      "loss": 1.7443,
      "step": 1348000
    },
    {
      "epoch": 52.136752136752136,
      "grad_norm": 12.936295509338379,
      "learning_rate": 6.5527065527065525e-06,
      "loss": 1.7465,
      "step": 1348100
    },
    {
      "epoch": 52.14061956143404,
      "grad_norm": 14.540821075439453,
      "learning_rate": 6.5494836988049656e-06,
      "loss": 1.7871,
      "step": 1348200
    },
    {
      "epoch": 52.14448698611594,
      "grad_norm": 12.518815040588379,
      "learning_rate": 6.5462608449033794e-06,
      "loss": 1.7492,
      "step": 1348300
    },
    {
      "epoch": 52.14835441079785,
      "grad_norm": 14.6680269241333,
      "learning_rate": 6.5430379910017925e-06,
      "loss": 1.7503,
      "step": 1348400
    },
    {
      "epoch": 52.15222183547976,
      "grad_norm": 18.01211929321289,
      "learning_rate": 6.5398151371002055e-06,
      "loss": 1.7654,
      "step": 1348500
    },
    {
      "epoch": 52.15608926016166,
      "grad_norm": 12.89289665222168,
      "learning_rate": 6.5365922831986185e-06,
      "loss": 1.827,
      "step": 1348600
    },
    {
      "epoch": 52.159956684843564,
      "grad_norm": 16.15725326538086,
      "learning_rate": 6.5333694292970315e-06,
      "loss": 1.7717,
      "step": 1348700
    },
    {
      "epoch": 52.163824109525464,
      "grad_norm": 12.259937286376953,
      "learning_rate": 6.5301465753954446e-06,
      "loss": 1.7449,
      "step": 1348800
    },
    {
      "epoch": 52.16769153420737,
      "grad_norm": 17.301029205322266,
      "learning_rate": 6.526923721493858e-06,
      "loss": 1.6886,
      "step": 1348900
    },
    {
      "epoch": 52.17155895888928,
      "grad_norm": 11.910528182983398,
      "learning_rate": 6.523700867592271e-06,
      "loss": 1.8366,
      "step": 1349000
    },
    {
      "epoch": 52.17542638357118,
      "grad_norm": 12.902390480041504,
      "learning_rate": 6.520478013690684e-06,
      "loss": 1.7435,
      "step": 1349100
    },
    {
      "epoch": 52.179293808253085,
      "grad_norm": 10.777676582336426,
      "learning_rate": 6.5172551597890975e-06,
      "loss": 1.8299,
      "step": 1349200
    },
    {
      "epoch": 52.183161232934985,
      "grad_norm": 14.621803283691406,
      "learning_rate": 6.5140323058875105e-06,
      "loss": 1.6988,
      "step": 1349300
    },
    {
      "epoch": 52.18702865761689,
      "grad_norm": 14.191479682922363,
      "learning_rate": 6.5108094519859236e-06,
      "loss": 1.751,
      "step": 1349400
    },
    {
      "epoch": 52.1908960822988,
      "grad_norm": 12.401208877563477,
      "learning_rate": 6.507586598084337e-06,
      "loss": 1.7511,
      "step": 1349500
    },
    {
      "epoch": 52.1947635069807,
      "grad_norm": 8.020912170410156,
      "learning_rate": 6.50436374418275e-06,
      "loss": 1.7541,
      "step": 1349600
    },
    {
      "epoch": 52.198630931662606,
      "grad_norm": 10.871148109436035,
      "learning_rate": 6.501140890281163e-06,
      "loss": 1.7396,
      "step": 1349700
    },
    {
      "epoch": 52.20249835634451,
      "grad_norm": 18.3437557220459,
      "learning_rate": 6.497918036379576e-06,
      "loss": 1.8723,
      "step": 1349800
    },
    {
      "epoch": 52.20636578102641,
      "grad_norm": 13.320384979248047,
      "learning_rate": 6.494695182477988e-06,
      "loss": 1.7904,
      "step": 1349900
    },
    {
      "epoch": 52.21023320570832,
      "grad_norm": 11.192466735839844,
      "learning_rate": 6.491472328576401e-06,
      "loss": 1.7836,
      "step": 1350000
    },
    {
      "epoch": 52.21410063039022,
      "grad_norm": 9.919416427612305,
      "learning_rate": 6.488249474674814e-06,
      "loss": 1.8321,
      "step": 1350100
    },
    {
      "epoch": 52.21796805507213,
      "grad_norm": 12.743962287902832,
      "learning_rate": 6.485026620773227e-06,
      "loss": 1.8051,
      "step": 1350200
    },
    {
      "epoch": 52.221835479754034,
      "grad_norm": 14.839282035827637,
      "learning_rate": 6.48180376687164e-06,
      "loss": 1.8219,
      "step": 1350300
    },
    {
      "epoch": 52.225702904435934,
      "grad_norm": 12.523293495178223,
      "learning_rate": 6.478580912970053e-06,
      "loss": 1.9072,
      "step": 1350400
    },
    {
      "epoch": 52.22957032911784,
      "grad_norm": 13.24953842163086,
      "learning_rate": 6.475358059068466e-06,
      "loss": 1.7862,
      "step": 1350500
    },
    {
      "epoch": 52.23343775379975,
      "grad_norm": 13.118029594421387,
      "learning_rate": 6.472135205166879e-06,
      "loss": 1.8177,
      "step": 1350600
    },
    {
      "epoch": 52.23730517848165,
      "grad_norm": 12.698773384094238,
      "learning_rate": 6.468912351265292e-06,
      "loss": 1.7344,
      "step": 1350700
    },
    {
      "epoch": 52.241172603163555,
      "grad_norm": 15.342243194580078,
      "learning_rate": 6.465689497363706e-06,
      "loss": 1.8911,
      "step": 1350800
    },
    {
      "epoch": 52.245040027845455,
      "grad_norm": 13.81091022491455,
      "learning_rate": 6.462466643462119e-06,
      "loss": 1.6912,
      "step": 1350900
    },
    {
      "epoch": 52.24890745252736,
      "grad_norm": 15.760305404663086,
      "learning_rate": 6.459243789560532e-06,
      "loss": 1.7856,
      "step": 1351000
    },
    {
      "epoch": 52.25277487720927,
      "grad_norm": 11.859147071838379,
      "learning_rate": 6.456020935658945e-06,
      "loss": 1.7932,
      "step": 1351100
    },
    {
      "epoch": 52.25664230189117,
      "grad_norm": 12.347807884216309,
      "learning_rate": 6.452798081757358e-06,
      "loss": 1.9151,
      "step": 1351200
    },
    {
      "epoch": 52.260509726573076,
      "grad_norm": 13.775405883789062,
      "learning_rate": 6.449575227855771e-06,
      "loss": 1.7862,
      "step": 1351300
    },
    {
      "epoch": 52.264377151254976,
      "grad_norm": 11.684319496154785,
      "learning_rate": 6.446352373954184e-06,
      "loss": 1.8029,
      "step": 1351400
    },
    {
      "epoch": 52.26824457593688,
      "grad_norm": 11.62013053894043,
      "learning_rate": 6.443129520052597e-06,
      "loss": 1.772,
      "step": 1351500
    },
    {
      "epoch": 52.27211200061879,
      "grad_norm": 15.539677619934082,
      "learning_rate": 6.43990666615101e-06,
      "loss": 1.7984,
      "step": 1351600
    },
    {
      "epoch": 52.27597942530069,
      "grad_norm": 14.425337791442871,
      "learning_rate": 6.436683812249424e-06,
      "loss": 1.8895,
      "step": 1351700
    },
    {
      "epoch": 52.2798468499826,
      "grad_norm": 14.474567413330078,
      "learning_rate": 6.433460958347837e-06,
      "loss": 1.6424,
      "step": 1351800
    },
    {
      "epoch": 52.283714274664504,
      "grad_norm": 11.77186107635498,
      "learning_rate": 6.43023810444625e-06,
      "loss": 1.6829,
      "step": 1351900
    },
    {
      "epoch": 52.287581699346404,
      "grad_norm": 16.449234008789062,
      "learning_rate": 6.427015250544663e-06,
      "loss": 1.8314,
      "step": 1352000
    },
    {
      "epoch": 52.29144912402831,
      "grad_norm": 11.6497220993042,
      "learning_rate": 6.423792396643076e-06,
      "loss": 1.864,
      "step": 1352100
    },
    {
      "epoch": 52.29531654871021,
      "grad_norm": 12.808797836303711,
      "learning_rate": 6.420569542741489e-06,
      "loss": 1.7723,
      "step": 1352200
    },
    {
      "epoch": 52.29918397339212,
      "grad_norm": 14.356733322143555,
      "learning_rate": 6.417346688839902e-06,
      "loss": 1.7907,
      "step": 1352300
    },
    {
      "epoch": 52.303051398074025,
      "grad_norm": 10.385659217834473,
      "learning_rate": 6.414123834938315e-06,
      "loss": 1.7913,
      "step": 1352400
    },
    {
      "epoch": 52.306918822755925,
      "grad_norm": 13.007779121398926,
      "learning_rate": 6.410900981036728e-06,
      "loss": 1.7859,
      "step": 1352500
    },
    {
      "epoch": 52.31078624743783,
      "grad_norm": 11.681733131408691,
      "learning_rate": 6.407678127135141e-06,
      "loss": 1.7927,
      "step": 1352600
    },
    {
      "epoch": 52.31465367211973,
      "grad_norm": 16.607975006103516,
      "learning_rate": 6.404455273233555e-06,
      "loss": 1.7836,
      "step": 1352700
    },
    {
      "epoch": 52.31852109680164,
      "grad_norm": 13.552557945251465,
      "learning_rate": 6.401232419331968e-06,
      "loss": 1.7659,
      "step": 1352800
    },
    {
      "epoch": 52.322388521483546,
      "grad_norm": 16.473310470581055,
      "learning_rate": 6.398009565430379e-06,
      "loss": 1.7346,
      "step": 1352900
    },
    {
      "epoch": 52.326255946165446,
      "grad_norm": 13.395041465759277,
      "learning_rate": 6.3947867115287924e-06,
      "loss": 1.8143,
      "step": 1353000
    },
    {
      "epoch": 52.33012337084735,
      "grad_norm": 16.90395164489746,
      "learning_rate": 6.3915638576272055e-06,
      "loss": 1.8819,
      "step": 1353100
    },
    {
      "epoch": 52.33399079552926,
      "grad_norm": 11.8416748046875,
      "learning_rate": 6.3883410037256185e-06,
      "loss": 1.7564,
      "step": 1353200
    },
    {
      "epoch": 52.33785822021116,
      "grad_norm": 10.42113208770752,
      "learning_rate": 6.385118149824032e-06,
      "loss": 1.8205,
      "step": 1353300
    },
    {
      "epoch": 52.34172564489307,
      "grad_norm": 13.05384635925293,
      "learning_rate": 6.381895295922445e-06,
      "loss": 1.7695,
      "step": 1353400
    },
    {
      "epoch": 52.34559306957497,
      "grad_norm": 11.092180252075195,
      "learning_rate": 6.378672442020858e-06,
      "loss": 1.7357,
      "step": 1353500
    },
    {
      "epoch": 52.349460494256874,
      "grad_norm": 10.717799186706543,
      "learning_rate": 6.375449588119271e-06,
      "loss": 1.7935,
      "step": 1353600
    },
    {
      "epoch": 52.35332791893878,
      "grad_norm": 13.627386093139648,
      "learning_rate": 6.3722267342176844e-06,
      "loss": 1.7217,
      "step": 1353700
    },
    {
      "epoch": 52.35719534362068,
      "grad_norm": 11.183629035949707,
      "learning_rate": 6.3690038803160975e-06,
      "loss": 1.7335,
      "step": 1353800
    },
    {
      "epoch": 52.36106276830259,
      "grad_norm": 13.571381568908691,
      "learning_rate": 6.3657810264145105e-06,
      "loss": 1.8682,
      "step": 1353900
    },
    {
      "epoch": 52.36493019298449,
      "grad_norm": 9.915244102478027,
      "learning_rate": 6.3625581725129235e-06,
      "loss": 1.7115,
      "step": 1354000
    },
    {
      "epoch": 52.368797617666395,
      "grad_norm": 14.7998685836792,
      "learning_rate": 6.3593353186113365e-06,
      "loss": 1.7613,
      "step": 1354100
    },
    {
      "epoch": 52.3726650423483,
      "grad_norm": 12.488842010498047,
      "learning_rate": 6.35611246470975e-06,
      "loss": 1.8026,
      "step": 1354200
    },
    {
      "epoch": 52.3765324670302,
      "grad_norm": 10.173887252807617,
      "learning_rate": 6.3528896108081634e-06,
      "loss": 1.8082,
      "step": 1354300
    },
    {
      "epoch": 52.38039989171211,
      "grad_norm": 9.647645950317383,
      "learning_rate": 6.3496667569065765e-06,
      "loss": 1.7323,
      "step": 1354400
    },
    {
      "epoch": 52.384267316394016,
      "grad_norm": 11.337818145751953,
      "learning_rate": 6.3464439030049895e-06,
      "loss": 1.8416,
      "step": 1354500
    },
    {
      "epoch": 52.388134741075916,
      "grad_norm": 8.785778045654297,
      "learning_rate": 6.3432210491034025e-06,
      "loss": 1.8445,
      "step": 1354600
    },
    {
      "epoch": 52.39200216575782,
      "grad_norm": 16.028167724609375,
      "learning_rate": 6.3399981952018155e-06,
      "loss": 1.8539,
      "step": 1354700
    },
    {
      "epoch": 52.39586959043972,
      "grad_norm": 11.668354034423828,
      "learning_rate": 6.3367753413002286e-06,
      "loss": 1.7492,
      "step": 1354800
    },
    {
      "epoch": 52.39973701512163,
      "grad_norm": 9.459388732910156,
      "learning_rate": 6.333552487398642e-06,
      "loss": 1.8194,
      "step": 1354900
    },
    {
      "epoch": 52.40360443980354,
      "grad_norm": 10.732531547546387,
      "learning_rate": 6.330329633497055e-06,
      "loss": 1.825,
      "step": 1355000
    },
    {
      "epoch": 52.40747186448544,
      "grad_norm": 14.546915054321289,
      "learning_rate": 6.327106779595468e-06,
      "loss": 1.7701,
      "step": 1355100
    },
    {
      "epoch": 52.411339289167344,
      "grad_norm": 8.900605201721191,
      "learning_rate": 6.3238839256938815e-06,
      "loss": 1.6924,
      "step": 1355200
    },
    {
      "epoch": 52.41520671384925,
      "grad_norm": 10.073859214782715,
      "learning_rate": 6.3206610717922945e-06,
      "loss": 1.731,
      "step": 1355300
    },
    {
      "epoch": 52.41907413853115,
      "grad_norm": 8.25190544128418,
      "learning_rate": 6.3174382178907076e-06,
      "loss": 1.6813,
      "step": 1355400
    },
    {
      "epoch": 52.42294156321306,
      "grad_norm": 11.642768859863281,
      "learning_rate": 6.314215363989121e-06,
      "loss": 1.6769,
      "step": 1355500
    },
    {
      "epoch": 52.42680898789496,
      "grad_norm": 12.948070526123047,
      "learning_rate": 6.310992510087534e-06,
      "loss": 1.897,
      "step": 1355600
    },
    {
      "epoch": 52.430676412576865,
      "grad_norm": 15.913232803344727,
      "learning_rate": 6.307769656185947e-06,
      "loss": 1.788,
      "step": 1355700
    },
    {
      "epoch": 52.43454383725877,
      "grad_norm": 8.566198348999023,
      "learning_rate": 6.304546802284359e-06,
      "loss": 1.8315,
      "step": 1355800
    },
    {
      "epoch": 52.43841126194067,
      "grad_norm": 12.012797355651855,
      "learning_rate": 6.301323948382772e-06,
      "loss": 1.6751,
      "step": 1355900
    },
    {
      "epoch": 52.44227868662258,
      "grad_norm": 17.94825553894043,
      "learning_rate": 6.298101094481185e-06,
      "loss": 1.7872,
      "step": 1356000
    },
    {
      "epoch": 52.44614611130448,
      "grad_norm": 14.665690422058105,
      "learning_rate": 6.294878240579598e-06,
      "loss": 1.7701,
      "step": 1356100
    },
    {
      "epoch": 52.450013535986386,
      "grad_norm": 16.20377540588379,
      "learning_rate": 6.291655386678011e-06,
      "loss": 1.7459,
      "step": 1356200
    },
    {
      "epoch": 52.45388096066829,
      "grad_norm": 13.095706939697266,
      "learning_rate": 6.288432532776424e-06,
      "loss": 1.8135,
      "step": 1356300
    },
    {
      "epoch": 52.45774838535019,
      "grad_norm": 10.26738166809082,
      "learning_rate": 6.285209678874837e-06,
      "loss": 1.7949,
      "step": 1356400
    },
    {
      "epoch": 52.4616158100321,
      "grad_norm": 13.771980285644531,
      "learning_rate": 6.28198682497325e-06,
      "loss": 1.8052,
      "step": 1356500
    },
    {
      "epoch": 52.46548323471401,
      "grad_norm": 12.004549980163574,
      "learning_rate": 6.278763971071663e-06,
      "loss": 1.7793,
      "step": 1356600
    },
    {
      "epoch": 52.46935065939591,
      "grad_norm": 17.316957473754883,
      "learning_rate": 6.275541117170077e-06,
      "loss": 1.7596,
      "step": 1356700
    },
    {
      "epoch": 52.473218084077814,
      "grad_norm": 9.833739280700684,
      "learning_rate": 6.27231826326849e-06,
      "loss": 1.7936,
      "step": 1356800
    },
    {
      "epoch": 52.477085508759714,
      "grad_norm": 8.512129783630371,
      "learning_rate": 6.269095409366903e-06,
      "loss": 1.7182,
      "step": 1356900
    },
    {
      "epoch": 52.48095293344162,
      "grad_norm": 12.292533874511719,
      "learning_rate": 6.265872555465316e-06,
      "loss": 1.719,
      "step": 1357000
    },
    {
      "epoch": 52.48482035812353,
      "grad_norm": 14.671775817871094,
      "learning_rate": 6.262649701563729e-06,
      "loss": 1.8557,
      "step": 1357100
    },
    {
      "epoch": 52.48868778280543,
      "grad_norm": 13.497537612915039,
      "learning_rate": 6.259426847662142e-06,
      "loss": 1.6821,
      "step": 1357200
    },
    {
      "epoch": 52.492555207487335,
      "grad_norm": 11.109496116638184,
      "learning_rate": 6.256203993760555e-06,
      "loss": 1.7308,
      "step": 1357300
    },
    {
      "epoch": 52.496422632169235,
      "grad_norm": 12.68572998046875,
      "learning_rate": 6.252981139858968e-06,
      "loss": 1.7845,
      "step": 1357400
    },
    {
      "epoch": 52.50029005685114,
      "grad_norm": 14.06751823425293,
      "learning_rate": 6.249758285957381e-06,
      "loss": 1.7148,
      "step": 1357500
    },
    {
      "epoch": 52.50415748153305,
      "grad_norm": 9.97606086730957,
      "learning_rate": 6.246535432055794e-06,
      "loss": 1.726,
      "step": 1357600
    },
    {
      "epoch": 52.50802490621495,
      "grad_norm": 10.925985336303711,
      "learning_rate": 6.243312578154208e-06,
      "loss": 1.7414,
      "step": 1357700
    },
    {
      "epoch": 52.511892330896856,
      "grad_norm": 13.362513542175293,
      "learning_rate": 6.240089724252621e-06,
      "loss": 1.8241,
      "step": 1357800
    },
    {
      "epoch": 52.51575975557876,
      "grad_norm": 15.624323844909668,
      "learning_rate": 6.236866870351034e-06,
      "loss": 1.7615,
      "step": 1357900
    },
    {
      "epoch": 52.51962718026066,
      "grad_norm": 11.790699005126953,
      "learning_rate": 6.233644016449447e-06,
      "loss": 1.7903,
      "step": 1358000
    },
    {
      "epoch": 52.52349460494257,
      "grad_norm": 12.617831230163574,
      "learning_rate": 6.230421162547859e-06,
      "loss": 1.8009,
      "step": 1358100
    },
    {
      "epoch": 52.52736202962447,
      "grad_norm": 13.787805557250977,
      "learning_rate": 6.227198308646272e-06,
      "loss": 1.7874,
      "step": 1358200
    },
    {
      "epoch": 52.53122945430638,
      "grad_norm": 15.024744033813477,
      "learning_rate": 6.223975454744685e-06,
      "loss": 1.7627,
      "step": 1358300
    },
    {
      "epoch": 52.535096878988284,
      "grad_norm": 13.982034683227539,
      "learning_rate": 6.220752600843099e-06,
      "loss": 1.7769,
      "step": 1358400
    },
    {
      "epoch": 52.538964303670184,
      "grad_norm": 12.997583389282227,
      "learning_rate": 6.217529746941512e-06,
      "loss": 1.786,
      "step": 1358500
    },
    {
      "epoch": 52.54283172835209,
      "grad_norm": 10.891592979431152,
      "learning_rate": 6.214306893039925e-06,
      "loss": 1.7534,
      "step": 1358600
    },
    {
      "epoch": 52.546699153034,
      "grad_norm": 13.59167194366455,
      "learning_rate": 6.211084039138338e-06,
      "loss": 1.7872,
      "step": 1358700
    },
    {
      "epoch": 52.5505665777159,
      "grad_norm": 12.696428298950195,
      "learning_rate": 6.207861185236751e-06,
      "loss": 1.8446,
      "step": 1358800
    },
    {
      "epoch": 52.554434002397805,
      "grad_norm": 9.086653709411621,
      "learning_rate": 6.204638331335164e-06,
      "loss": 1.8153,
      "step": 1358900
    },
    {
      "epoch": 52.558301427079705,
      "grad_norm": 11.29761791229248,
      "learning_rate": 6.201415477433577e-06,
      "loss": 1.7573,
      "step": 1359000
    },
    {
      "epoch": 52.56216885176161,
      "grad_norm": 9.413361549377441,
      "learning_rate": 6.19819262353199e-06,
      "loss": 1.8791,
      "step": 1359100
    },
    {
      "epoch": 52.56603627644352,
      "grad_norm": 13.619237899780273,
      "learning_rate": 6.194969769630403e-06,
      "loss": 1.8186,
      "step": 1359200
    },
    {
      "epoch": 52.56990370112542,
      "grad_norm": 13.910810470581055,
      "learning_rate": 6.191746915728816e-06,
      "loss": 1.7432,
      "step": 1359300
    },
    {
      "epoch": 52.573771125807326,
      "grad_norm": 11.805612564086914,
      "learning_rate": 6.18852406182723e-06,
      "loss": 1.7578,
      "step": 1359400
    },
    {
      "epoch": 52.577638550489226,
      "grad_norm": 17.058292388916016,
      "learning_rate": 6.185301207925642e-06,
      "loss": 1.7463,
      "step": 1359500
    },
    {
      "epoch": 52.58150597517113,
      "grad_norm": 13.79780387878418,
      "learning_rate": 6.182078354024055e-06,
      "loss": 1.8369,
      "step": 1359600
    },
    {
      "epoch": 52.58537339985304,
      "grad_norm": 11.719902038574219,
      "learning_rate": 6.1788555001224684e-06,
      "loss": 1.8113,
      "step": 1359700
    },
    {
      "epoch": 52.58924082453494,
      "grad_norm": 13.078693389892578,
      "learning_rate": 6.1756326462208815e-06,
      "loss": 1.8174,
      "step": 1359800
    },
    {
      "epoch": 52.59310824921685,
      "grad_norm": 18.70123863220215,
      "learning_rate": 6.1724097923192945e-06,
      "loss": 1.7496,
      "step": 1359900
    },
    {
      "epoch": 52.596975673898754,
      "grad_norm": 16.108064651489258,
      "learning_rate": 6.1691869384177075e-06,
      "loss": 1.7977,
      "step": 1360000
    },
    {
      "epoch": 52.600843098580654,
      "grad_norm": 14.635993003845215,
      "learning_rate": 6.1659640845161205e-06,
      "loss": 1.6714,
      "step": 1360100
    },
    {
      "epoch": 52.60471052326256,
      "grad_norm": 13.772101402282715,
      "learning_rate": 6.162741230614534e-06,
      "loss": 1.7929,
      "step": 1360200
    },
    {
      "epoch": 52.60857794794446,
      "grad_norm": 14.35097599029541,
      "learning_rate": 6.1595183767129474e-06,
      "loss": 1.827,
      "step": 1360300
    },
    {
      "epoch": 52.61244537262637,
      "grad_norm": 16.72554588317871,
      "learning_rate": 6.1562955228113605e-06,
      "loss": 1.7823,
      "step": 1360400
    },
    {
      "epoch": 52.616312797308275,
      "grad_norm": 14.183987617492676,
      "learning_rate": 6.1530726689097735e-06,
      "loss": 1.7878,
      "step": 1360500
    },
    {
      "epoch": 52.620180221990175,
      "grad_norm": 14.004271507263184,
      "learning_rate": 6.1498498150081865e-06,
      "loss": 1.7922,
      "step": 1360600
    },
    {
      "epoch": 52.62404764667208,
      "grad_norm": 5.296911239624023,
      "learning_rate": 6.1466269611065995e-06,
      "loss": 1.7819,
      "step": 1360700
    },
    {
      "epoch": 52.62791507135398,
      "grad_norm": 14.869808197021484,
      "learning_rate": 6.1434041072050126e-06,
      "loss": 1.7996,
      "step": 1360800
    },
    {
      "epoch": 52.63178249603589,
      "grad_norm": 14.24898624420166,
      "learning_rate": 6.140181253303426e-06,
      "loss": 1.7941,
      "step": 1360900
    },
    {
      "epoch": 52.635649920717796,
      "grad_norm": 15.013620376586914,
      "learning_rate": 6.136958399401839e-06,
      "loss": 1.8981,
      "step": 1361000
    },
    {
      "epoch": 52.639517345399696,
      "grad_norm": 13.224892616271973,
      "learning_rate": 6.133735545500252e-06,
      "loss": 1.7801,
      "step": 1361100
    },
    {
      "epoch": 52.6433847700816,
      "grad_norm": 17.694904327392578,
      "learning_rate": 6.130512691598665e-06,
      "loss": 1.8294,
      "step": 1361200
    },
    {
      "epoch": 52.64725219476351,
      "grad_norm": 6.409363746643066,
      "learning_rate": 6.127289837697078e-06,
      "loss": 1.7532,
      "step": 1361300
    },
    {
      "epoch": 52.65111961944541,
      "grad_norm": 13.81238842010498,
      "learning_rate": 6.124066983795491e-06,
      "loss": 1.7387,
      "step": 1361400
    },
    {
      "epoch": 52.65498704412732,
      "grad_norm": 10.896940231323242,
      "learning_rate": 6.120844129893904e-06,
      "loss": 1.7385,
      "step": 1361500
    },
    {
      "epoch": 52.65885446880922,
      "grad_norm": 10.385175704956055,
      "learning_rate": 6.117621275992317e-06,
      "loss": 1.775,
      "step": 1361600
    },
    {
      "epoch": 52.662721893491124,
      "grad_norm": 8.642939567565918,
      "learning_rate": 6.11439842209073e-06,
      "loss": 1.827,
      "step": 1361700
    },
    {
      "epoch": 52.66658931817303,
      "grad_norm": 15.910775184631348,
      "learning_rate": 6.111175568189143e-06,
      "loss": 1.7896,
      "step": 1361800
    },
    {
      "epoch": 52.67045674285493,
      "grad_norm": 7.9810004234313965,
      "learning_rate": 6.107952714287557e-06,
      "loss": 1.845,
      "step": 1361900
    },
    {
      "epoch": 52.67432416753684,
      "grad_norm": 13.98698616027832,
      "learning_rate": 6.10472986038597e-06,
      "loss": 1.7453,
      "step": 1362000
    },
    {
      "epoch": 52.67819159221874,
      "grad_norm": 10.383405685424805,
      "learning_rate": 6.101507006484383e-06,
      "loss": 1.6948,
      "step": 1362100
    },
    {
      "epoch": 52.682059016900645,
      "grad_norm": 13.963597297668457,
      "learning_rate": 6.098284152582796e-06,
      "loss": 1.7934,
      "step": 1362200
    },
    {
      "epoch": 52.68592644158255,
      "grad_norm": 13.269115447998047,
      "learning_rate": 6.095061298681209e-06,
      "loss": 1.8099,
      "step": 1362300
    },
    {
      "epoch": 52.68979386626445,
      "grad_norm": 10.403059959411621,
      "learning_rate": 6.091838444779622e-06,
      "loss": 1.668,
      "step": 1362400
    },
    {
      "epoch": 52.69366129094636,
      "grad_norm": 20.744770050048828,
      "learning_rate": 6.088615590878034e-06,
      "loss": 1.7313,
      "step": 1362500
    },
    {
      "epoch": 52.697528715628266,
      "grad_norm": 13.360007286071777,
      "learning_rate": 6.085392736976447e-06,
      "loss": 1.7276,
      "step": 1362600
    },
    {
      "epoch": 52.701396140310166,
      "grad_norm": 12.926949501037598,
      "learning_rate": 6.082169883074861e-06,
      "loss": 1.8461,
      "step": 1362700
    },
    {
      "epoch": 52.70526356499207,
      "grad_norm": 10.498856544494629,
      "learning_rate": 6.078947029173274e-06,
      "loss": 1.755,
      "step": 1362800
    },
    {
      "epoch": 52.70913098967397,
      "grad_norm": 12.468795776367188,
      "learning_rate": 6.075724175271687e-06,
      "loss": 1.7562,
      "step": 1362900
    },
    {
      "epoch": 52.71299841435588,
      "grad_norm": 14.610560417175293,
      "learning_rate": 6.0725013213701e-06,
      "loss": 1.8257,
      "step": 1363000
    },
    {
      "epoch": 52.71686583903779,
      "grad_norm": 14.855973243713379,
      "learning_rate": 6.069278467468513e-06,
      "loss": 1.7426,
      "step": 1363100
    },
    {
      "epoch": 52.72073326371969,
      "grad_norm": 12.0498685836792,
      "learning_rate": 6.066055613566926e-06,
      "loss": 1.8219,
      "step": 1363200
    },
    {
      "epoch": 52.724600688401594,
      "grad_norm": 13.762462615966797,
      "learning_rate": 6.062832759665339e-06,
      "loss": 1.7999,
      "step": 1363300
    },
    {
      "epoch": 52.7284681130835,
      "grad_norm": 7.848213195800781,
      "learning_rate": 6.059609905763752e-06,
      "loss": 1.8016,
      "step": 1363400
    },
    {
      "epoch": 52.7323355377654,
      "grad_norm": 10.897624969482422,
      "learning_rate": 6.056387051862165e-06,
      "loss": 1.7675,
      "step": 1363500
    },
    {
      "epoch": 52.73620296244731,
      "grad_norm": 11.127165794372559,
      "learning_rate": 6.053164197960579e-06,
      "loss": 1.8441,
      "step": 1363600
    },
    {
      "epoch": 52.74007038712921,
      "grad_norm": 14.239921569824219,
      "learning_rate": 6.049941344058992e-06,
      "loss": 1.9044,
      "step": 1363700
    },
    {
      "epoch": 52.743937811811115,
      "grad_norm": 10.970876693725586,
      "learning_rate": 6.046718490157405e-06,
      "loss": 1.8048,
      "step": 1363800
    },
    {
      "epoch": 52.74780523649302,
      "grad_norm": 11.224961280822754,
      "learning_rate": 6.043495636255818e-06,
      "loss": 1.8835,
      "step": 1363900
    },
    {
      "epoch": 52.75167266117492,
      "grad_norm": 15.090815544128418,
      "learning_rate": 6.04027278235423e-06,
      "loss": 1.7728,
      "step": 1364000
    },
    {
      "epoch": 52.75554008585683,
      "grad_norm": 11.585232734680176,
      "learning_rate": 6.037049928452643e-06,
      "loss": 1.799,
      "step": 1364100
    },
    {
      "epoch": 52.75940751053873,
      "grad_norm": 17.839962005615234,
      "learning_rate": 6.033827074551056e-06,
      "loss": 1.844,
      "step": 1364200
    },
    {
      "epoch": 52.763274935220636,
      "grad_norm": 15.11452579498291,
      "learning_rate": 6.030604220649469e-06,
      "loss": 1.743,
      "step": 1364300
    },
    {
      "epoch": 52.76714235990254,
      "grad_norm": 15.56943416595459,
      "learning_rate": 6.027381366747883e-06,
      "loss": 1.7582,
      "step": 1364400
    },
    {
      "epoch": 52.77100978458444,
      "grad_norm": 13.81905460357666,
      "learning_rate": 6.024158512846296e-06,
      "loss": 1.8012,
      "step": 1364500
    },
    {
      "epoch": 52.77487720926635,
      "grad_norm": 10.398711204528809,
      "learning_rate": 6.020935658944709e-06,
      "loss": 1.7471,
      "step": 1364600
    },
    {
      "epoch": 52.77874463394826,
      "grad_norm": 15.393138885498047,
      "learning_rate": 6.017712805043122e-06,
      "loss": 1.8273,
      "step": 1364700
    },
    {
      "epoch": 52.78261205863016,
      "grad_norm": 11.855989456176758,
      "learning_rate": 6.014489951141535e-06,
      "loss": 1.964,
      "step": 1364800
    },
    {
      "epoch": 52.786479483312064,
      "grad_norm": 7.123167514801025,
      "learning_rate": 6.011267097239948e-06,
      "loss": 1.7709,
      "step": 1364900
    },
    {
      "epoch": 52.790346907993964,
      "grad_norm": 14.810076713562012,
      "learning_rate": 6.008044243338361e-06,
      "loss": 1.7952,
      "step": 1365000
    },
    {
      "epoch": 52.79421433267587,
      "grad_norm": 11.626103401184082,
      "learning_rate": 6.004821389436774e-06,
      "loss": 1.7151,
      "step": 1365100
    },
    {
      "epoch": 52.79808175735778,
      "grad_norm": 12.05723762512207,
      "learning_rate": 6.001598535535187e-06,
      "loss": 1.7828,
      "step": 1365200
    },
    {
      "epoch": 52.80194918203968,
      "grad_norm": 12.411028861999512,
      "learning_rate": 5.998375681633601e-06,
      "loss": 1.8026,
      "step": 1365300
    },
    {
      "epoch": 52.805816606721585,
      "grad_norm": 12.016878128051758,
      "learning_rate": 5.995152827732013e-06,
      "loss": 1.7839,
      "step": 1365400
    },
    {
      "epoch": 52.809684031403485,
      "grad_norm": 14.838138580322266,
      "learning_rate": 5.991929973830426e-06,
      "loss": 1.7897,
      "step": 1365500
    },
    {
      "epoch": 52.81355145608539,
      "grad_norm": 14.272785186767578,
      "learning_rate": 5.988707119928839e-06,
      "loss": 1.7676,
      "step": 1365600
    },
    {
      "epoch": 52.8174188807673,
      "grad_norm": 9.393463134765625,
      "learning_rate": 5.9854842660272524e-06,
      "loss": 1.7845,
      "step": 1365700
    },
    {
      "epoch": 52.8212863054492,
      "grad_norm": 11.806960105895996,
      "learning_rate": 5.9822614121256655e-06,
      "loss": 1.7257,
      "step": 1365800
    },
    {
      "epoch": 52.825153730131106,
      "grad_norm": 12.354528427124023,
      "learning_rate": 5.9790385582240785e-06,
      "loss": 1.8055,
      "step": 1365900
    },
    {
      "epoch": 52.82902115481301,
      "grad_norm": 8.90674877166748,
      "learning_rate": 5.9758157043224915e-06,
      "loss": 1.7146,
      "step": 1366000
    },
    {
      "epoch": 52.83288857949491,
      "grad_norm": 10.355270385742188,
      "learning_rate": 5.972592850420905e-06,
      "loss": 1.8673,
      "step": 1366100
    },
    {
      "epoch": 52.83675600417682,
      "grad_norm": 12.554036140441895,
      "learning_rate": 5.969369996519318e-06,
      "loss": 1.7995,
      "step": 1366200
    },
    {
      "epoch": 52.84062342885872,
      "grad_norm": 12.194001197814941,
      "learning_rate": 5.9661471426177314e-06,
      "loss": 1.7714,
      "step": 1366300
    },
    {
      "epoch": 52.84449085354063,
      "grad_norm": 13.602657318115234,
      "learning_rate": 5.9629242887161445e-06,
      "loss": 1.8419,
      "step": 1366400
    },
    {
      "epoch": 52.848358278222534,
      "grad_norm": 11.881492614746094,
      "learning_rate": 5.9597014348145575e-06,
      "loss": 1.8094,
      "step": 1366500
    },
    {
      "epoch": 52.852225702904434,
      "grad_norm": 11.128870964050293,
      "learning_rate": 5.9564785809129705e-06,
      "loss": 1.6555,
      "step": 1366600
    },
    {
      "epoch": 52.85609312758634,
      "grad_norm": 11.473699569702148,
      "learning_rate": 5.9532557270113835e-06,
      "loss": 1.7723,
      "step": 1366700
    },
    {
      "epoch": 52.85996055226825,
      "grad_norm": 15.909645080566406,
      "learning_rate": 5.9500328731097966e-06,
      "loss": 1.7728,
      "step": 1366800
    },
    {
      "epoch": 52.86382797695015,
      "grad_norm": 10.0689115524292,
      "learning_rate": 5.94681001920821e-06,
      "loss": 1.7403,
      "step": 1366900
    },
    {
      "epoch": 52.867695401632055,
      "grad_norm": 13.505132675170898,
      "learning_rate": 5.943587165306623e-06,
      "loss": 1.855,
      "step": 1367000
    },
    {
      "epoch": 52.871562826313955,
      "grad_norm": 10.882367134094238,
      "learning_rate": 5.940364311405036e-06,
      "loss": 1.7782,
      "step": 1367100
    },
    {
      "epoch": 52.87543025099586,
      "grad_norm": 7.509197235107422,
      "learning_rate": 5.937141457503449e-06,
      "loss": 1.723,
      "step": 1367200
    },
    {
      "epoch": 52.87929767567777,
      "grad_norm": 13.639817237854004,
      "learning_rate": 5.933918603601862e-06,
      "loss": 1.7395,
      "step": 1367300
    },
    {
      "epoch": 52.88316510035967,
      "grad_norm": 10.363306999206543,
      "learning_rate": 5.930695749700275e-06,
      "loss": 1.7298,
      "step": 1367400
    },
    {
      "epoch": 52.887032525041576,
      "grad_norm": 7.320667266845703,
      "learning_rate": 5.927472895798688e-06,
      "loss": 1.7374,
      "step": 1367500
    },
    {
      "epoch": 52.890899949723476,
      "grad_norm": 14.167085647583008,
      "learning_rate": 5.924250041897101e-06,
      "loss": 1.8737,
      "step": 1367600
    },
    {
      "epoch": 52.89476737440538,
      "grad_norm": 9.149898529052734,
      "learning_rate": 5.921027187995514e-06,
      "loss": 1.7655,
      "step": 1367700
    },
    {
      "epoch": 52.89863479908729,
      "grad_norm": 12.141458511352539,
      "learning_rate": 5.917804334093928e-06,
      "loss": 1.7101,
      "step": 1367800
    },
    {
      "epoch": 52.90250222376919,
      "grad_norm": 14.316097259521484,
      "learning_rate": 5.914581480192341e-06,
      "loss": 1.6964,
      "step": 1367900
    },
    {
      "epoch": 52.9063696484511,
      "grad_norm": 13.048399925231934,
      "learning_rate": 5.911358626290754e-06,
      "loss": 1.7161,
      "step": 1368000
    },
    {
      "epoch": 52.910237073133004,
      "grad_norm": 9.36352252960205,
      "learning_rate": 5.908135772389167e-06,
      "loss": 1.8928,
      "step": 1368100
    },
    {
      "epoch": 52.914104497814904,
      "grad_norm": 13.955855369567871,
      "learning_rate": 5.90491291848758e-06,
      "loss": 1.8336,
      "step": 1368200
    },
    {
      "epoch": 52.91797192249681,
      "grad_norm": 12.718894958496094,
      "learning_rate": 5.901690064585993e-06,
      "loss": 1.8245,
      "step": 1368300
    },
    {
      "epoch": 52.92183934717871,
      "grad_norm": 15.134723663330078,
      "learning_rate": 5.898467210684405e-06,
      "loss": 1.7101,
      "step": 1368400
    },
    {
      "epoch": 52.92570677186062,
      "grad_norm": 11.424483299255371,
      "learning_rate": 5.895244356782818e-06,
      "loss": 1.7322,
      "step": 1368500
    },
    {
      "epoch": 52.929574196542525,
      "grad_norm": 12.930330276489258,
      "learning_rate": 5.892021502881232e-06,
      "loss": 1.7442,
      "step": 1368600
    },
    {
      "epoch": 52.933441621224425,
      "grad_norm": 14.217490196228027,
      "learning_rate": 5.888798648979645e-06,
      "loss": 1.8476,
      "step": 1368700
    },
    {
      "epoch": 52.93730904590633,
      "grad_norm": 15.471035957336426,
      "learning_rate": 5.885575795078058e-06,
      "loss": 1.7808,
      "step": 1368800
    },
    {
      "epoch": 52.94117647058823,
      "grad_norm": 15.584932327270508,
      "learning_rate": 5.882352941176471e-06,
      "loss": 1.7907,
      "step": 1368900
    },
    {
      "epoch": 52.94504389527014,
      "grad_norm": 15.483201026916504,
      "learning_rate": 5.879130087274884e-06,
      "loss": 1.8718,
      "step": 1369000
    },
    {
      "epoch": 52.948911319952046,
      "grad_norm": 15.34827709197998,
      "learning_rate": 5.875907233373297e-06,
      "loss": 1.7862,
      "step": 1369100
    },
    {
      "epoch": 52.952778744633946,
      "grad_norm": 11.332911491394043,
      "learning_rate": 5.87268437947171e-06,
      "loss": 1.8883,
      "step": 1369200
    },
    {
      "epoch": 52.95664616931585,
      "grad_norm": 13.061864852905273,
      "learning_rate": 5.869461525570123e-06,
      "loss": 1.7476,
      "step": 1369300
    },
    {
      "epoch": 52.96051359399776,
      "grad_norm": 16.510217666625977,
      "learning_rate": 5.866238671668536e-06,
      "loss": 1.7189,
      "step": 1369400
    },
    {
      "epoch": 52.96438101867966,
      "grad_norm": 13.009599685668945,
      "learning_rate": 5.863015817766949e-06,
      "loss": 1.8079,
      "step": 1369500
    },
    {
      "epoch": 52.96824844336157,
      "grad_norm": 13.825490951538086,
      "learning_rate": 5.859792963865363e-06,
      "loss": 1.7467,
      "step": 1369600
    },
    {
      "epoch": 52.97211586804347,
      "grad_norm": 10.939718246459961,
      "learning_rate": 5.856570109963776e-06,
      "loss": 1.8187,
      "step": 1369700
    },
    {
      "epoch": 52.975983292725374,
      "grad_norm": 15.4530611038208,
      "learning_rate": 5.853347256062189e-06,
      "loss": 1.785,
      "step": 1369800
    },
    {
      "epoch": 52.97985071740728,
      "grad_norm": 12.006610870361328,
      "learning_rate": 5.850124402160601e-06,
      "loss": 1.8301,
      "step": 1369900
    },
    {
      "epoch": 52.98371814208918,
      "grad_norm": 10.218057632446289,
      "learning_rate": 5.846901548259014e-06,
      "loss": 1.6822,
      "step": 1370000
    },
    {
      "epoch": 52.98758556677109,
      "grad_norm": 10.367280006408691,
      "learning_rate": 5.843678694357427e-06,
      "loss": 1.7951,
      "step": 1370100
    },
    {
      "epoch": 52.99145299145299,
      "grad_norm": 12.548564910888672,
      "learning_rate": 5.84045584045584e-06,
      "loss": 1.8016,
      "step": 1370200
    },
    {
      "epoch": 52.995320416134895,
      "grad_norm": 14.753755569458008,
      "learning_rate": 5.837232986554254e-06,
      "loss": 1.7343,
      "step": 1370300
    },
    {
      "epoch": 52.9991878408168,
      "grad_norm": 12.106993675231934,
      "learning_rate": 5.834010132652667e-06,
      "loss": 1.8338,
      "step": 1370400
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.7430208921432495,
      "eval_runtime": 2.9471,
      "eval_samples_per_second": 461.804,
      "eval_steps_per_second": 461.804,
      "step": 1370421
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.5758963823318481,
      "eval_runtime": 55.5165,
      "eval_samples_per_second": 465.753,
      "eval_steps_per_second": 465.753,
      "step": 1370421
    },
    {
      "epoch": 53.0030552654987,
      "grad_norm": 14.872373580932617,
      "learning_rate": 5.83078727875108e-06,
      "loss": 1.7555,
      "step": 1370500
    },
    {
      "epoch": 53.00692269018061,
      "grad_norm": 16.971851348876953,
      "learning_rate": 5.827564424849493e-06,
      "loss": 1.8289,
      "step": 1370600
    },
    {
      "epoch": 53.010790114862516,
      "grad_norm": 11.793865203857422,
      "learning_rate": 5.824341570947906e-06,
      "loss": 1.7247,
      "step": 1370700
    },
    {
      "epoch": 53.014657539544416,
      "grad_norm": 14.376970291137695,
      "learning_rate": 5.821118717046319e-06,
      "loss": 1.7479,
      "step": 1370800
    },
    {
      "epoch": 53.01852496422632,
      "grad_norm": 10.888395309448242,
      "learning_rate": 5.817895863144732e-06,
      "loss": 1.7806,
      "step": 1370900
    },
    {
      "epoch": 53.02239238890822,
      "grad_norm": 11.034262657165527,
      "learning_rate": 5.814673009243145e-06,
      "loss": 1.79,
      "step": 1371000
    },
    {
      "epoch": 53.02625981359013,
      "grad_norm": 10.617633819580078,
      "learning_rate": 5.811450155341558e-06,
      "loss": 1.8015,
      "step": 1371100
    },
    {
      "epoch": 53.03012723827204,
      "grad_norm": 11.979532241821289,
      "learning_rate": 5.808227301439971e-06,
      "loss": 1.6885,
      "step": 1371200
    },
    {
      "epoch": 53.03399466295394,
      "grad_norm": 11.665726661682129,
      "learning_rate": 5.805004447538384e-06,
      "loss": 1.7047,
      "step": 1371300
    },
    {
      "epoch": 53.037862087635844,
      "grad_norm": 12.411118507385254,
      "learning_rate": 5.801781593636797e-06,
      "loss": 1.7534,
      "step": 1371400
    },
    {
      "epoch": 53.04172951231775,
      "grad_norm": 15.34061336517334,
      "learning_rate": 5.79855873973521e-06,
      "loss": 1.6883,
      "step": 1371500
    },
    {
      "epoch": 53.04559693699965,
      "grad_norm": 13.72797966003418,
      "learning_rate": 5.7953358858336234e-06,
      "loss": 1.7361,
      "step": 1371600
    },
    {
      "epoch": 53.04946436168156,
      "grad_norm": 15.02480697631836,
      "learning_rate": 5.7921130319320364e-06,
      "loss": 1.8106,
      "step": 1371700
    },
    {
      "epoch": 53.05333178636346,
      "grad_norm": 12.232417106628418,
      "learning_rate": 5.7888901780304495e-06,
      "loss": 1.7951,
      "step": 1371800
    },
    {
      "epoch": 53.057199211045365,
      "grad_norm": 12.944182395935059,
      "learning_rate": 5.7856673241288625e-06,
      "loss": 1.7653,
      "step": 1371900
    },
    {
      "epoch": 53.06106663572727,
      "grad_norm": 14.85424518585205,
      "learning_rate": 5.7824444702272755e-06,
      "loss": 1.8117,
      "step": 1372000
    },
    {
      "epoch": 53.06493406040917,
      "grad_norm": 12.485365867614746,
      "learning_rate": 5.779221616325689e-06,
      "loss": 1.8326,
      "step": 1372100
    },
    {
      "epoch": 53.06880148509108,
      "grad_norm": 11.624049186706543,
      "learning_rate": 5.775998762424102e-06,
      "loss": 1.8532,
      "step": 1372200
    },
    {
      "epoch": 53.07266890977298,
      "grad_norm": 13.392565727233887,
      "learning_rate": 5.7727759085225154e-06,
      "loss": 1.7512,
      "step": 1372300
    },
    {
      "epoch": 53.076536334454886,
      "grad_norm": 12.264569282531738,
      "learning_rate": 5.7695530546209285e-06,
      "loss": 1.762,
      "step": 1372400
    },
    {
      "epoch": 53.08040375913679,
      "grad_norm": 11.957137107849121,
      "learning_rate": 5.7663302007193415e-06,
      "loss": 1.7978,
      "step": 1372500
    },
    {
      "epoch": 53.08427118381869,
      "grad_norm": 15.000391006469727,
      "learning_rate": 5.7631073468177545e-06,
      "loss": 1.7736,
      "step": 1372600
    },
    {
      "epoch": 53.0881386085006,
      "grad_norm": 11.771445274353027,
      "learning_rate": 5.7598844929161675e-06,
      "loss": 1.6865,
      "step": 1372700
    },
    {
      "epoch": 53.09200603318251,
      "grad_norm": 11.828980445861816,
      "learning_rate": 5.75666163901458e-06,
      "loss": 1.7294,
      "step": 1372800
    },
    {
      "epoch": 53.09587345786441,
      "grad_norm": 11.552364349365234,
      "learning_rate": 5.753438785112994e-06,
      "loss": 1.7485,
      "step": 1372900
    },
    {
      "epoch": 53.099740882546314,
      "grad_norm": 10.224783897399902,
      "learning_rate": 5.750215931211407e-06,
      "loss": 1.7683,
      "step": 1373000
    },
    {
      "epoch": 53.103608307228214,
      "grad_norm": 10.635415077209473,
      "learning_rate": 5.74699307730982e-06,
      "loss": 1.8974,
      "step": 1373100
    },
    {
      "epoch": 53.10747573191012,
      "grad_norm": 13.524325370788574,
      "learning_rate": 5.743770223408233e-06,
      "loss": 1.7341,
      "step": 1373200
    },
    {
      "epoch": 53.11134315659203,
      "grad_norm": 9.240017890930176,
      "learning_rate": 5.740547369506646e-06,
      "loss": 1.8134,
      "step": 1373300
    },
    {
      "epoch": 53.11521058127393,
      "grad_norm": 12.566965103149414,
      "learning_rate": 5.737324515605059e-06,
      "loss": 1.7572,
      "step": 1373400
    },
    {
      "epoch": 53.119078005955835,
      "grad_norm": 13.839995384216309,
      "learning_rate": 5.734101661703472e-06,
      "loss": 1.7506,
      "step": 1373500
    },
    {
      "epoch": 53.122945430637735,
      "grad_norm": 14.044116973876953,
      "learning_rate": 5.730878807801885e-06,
      "loss": 1.7781,
      "step": 1373600
    },
    {
      "epoch": 53.12681285531964,
      "grad_norm": 12.956594467163086,
      "learning_rate": 5.727655953900298e-06,
      "loss": 1.839,
      "step": 1373700
    },
    {
      "epoch": 53.13068028000155,
      "grad_norm": 13.921062469482422,
      "learning_rate": 5.724433099998712e-06,
      "loss": 1.78,
      "step": 1373800
    },
    {
      "epoch": 53.13454770468345,
      "grad_norm": 15.10568904876709,
      "learning_rate": 5.721210246097125e-06,
      "loss": 1.7726,
      "step": 1373900
    },
    {
      "epoch": 53.138415129365356,
      "grad_norm": 13.493303298950195,
      "learning_rate": 5.717987392195538e-06,
      "loss": 1.7286,
      "step": 1374000
    },
    {
      "epoch": 53.14228255404726,
      "grad_norm": 13.723411560058594,
      "learning_rate": 5.714764538293951e-06,
      "loss": 1.7845,
      "step": 1374100
    },
    {
      "epoch": 53.14614997872916,
      "grad_norm": 12.50904369354248,
      "learning_rate": 5.711541684392364e-06,
      "loss": 1.7546,
      "step": 1374200
    },
    {
      "epoch": 53.15001740341107,
      "grad_norm": 14.328486442565918,
      "learning_rate": 5.708318830490776e-06,
      "loss": 1.6536,
      "step": 1374300
    },
    {
      "epoch": 53.15388482809297,
      "grad_norm": 11.533971786499023,
      "learning_rate": 5.705095976589189e-06,
      "loss": 1.6767,
      "step": 1374400
    },
    {
      "epoch": 53.15775225277488,
      "grad_norm": 13.891839027404785,
      "learning_rate": 5.701873122687602e-06,
      "loss": 1.7872,
      "step": 1374500
    },
    {
      "epoch": 53.161619677456784,
      "grad_norm": 11.309951782226562,
      "learning_rate": 5.698650268786016e-06,
      "loss": 1.8624,
      "step": 1374600
    },
    {
      "epoch": 53.165487102138684,
      "grad_norm": 12.926284790039062,
      "learning_rate": 5.695427414884429e-06,
      "loss": 1.8066,
      "step": 1374700
    },
    {
      "epoch": 53.16935452682059,
      "grad_norm": 12.668951034545898,
      "learning_rate": 5.692204560982842e-06,
      "loss": 1.7699,
      "step": 1374800
    },
    {
      "epoch": 53.1732219515025,
      "grad_norm": 13.427700996398926,
      "learning_rate": 5.688981707081255e-06,
      "loss": 1.7332,
      "step": 1374900
    },
    {
      "epoch": 53.1770893761844,
      "grad_norm": 14.228631973266602,
      "learning_rate": 5.685758853179668e-06,
      "loss": 1.7543,
      "step": 1375000
    },
    {
      "epoch": 53.180956800866305,
      "grad_norm": 10.736059188842773,
      "learning_rate": 5.682535999278081e-06,
      "loss": 1.9411,
      "step": 1375100
    },
    {
      "epoch": 53.184824225548205,
      "grad_norm": 8.767619132995605,
      "learning_rate": 5.679313145376494e-06,
      "loss": 1.8082,
      "step": 1375200
    },
    {
      "epoch": 53.18869165023011,
      "grad_norm": 10.688056945800781,
      "learning_rate": 5.676090291474907e-06,
      "loss": 1.8302,
      "step": 1375300
    },
    {
      "epoch": 53.19255907491202,
      "grad_norm": 17.933307647705078,
      "learning_rate": 5.67286743757332e-06,
      "loss": 1.7507,
      "step": 1375400
    },
    {
      "epoch": 53.19642649959392,
      "grad_norm": 11.01301097869873,
      "learning_rate": 5.669644583671734e-06,
      "loss": 1.754,
      "step": 1375500
    },
    {
      "epoch": 53.200293924275826,
      "grad_norm": 13.466756820678711,
      "learning_rate": 5.666421729770147e-06,
      "loss": 1.8019,
      "step": 1375600
    },
    {
      "epoch": 53.204161348957726,
      "grad_norm": 11.541922569274902,
      "learning_rate": 5.66319887586856e-06,
      "loss": 1.8419,
      "step": 1375700
    },
    {
      "epoch": 53.20802877363963,
      "grad_norm": 14.117349624633789,
      "learning_rate": 5.659976021966972e-06,
      "loss": 1.7919,
      "step": 1375800
    },
    {
      "epoch": 53.21189619832154,
      "grad_norm": 11.123035430908203,
      "learning_rate": 5.656753168065385e-06,
      "loss": 1.8161,
      "step": 1375900
    },
    {
      "epoch": 53.21576362300344,
      "grad_norm": 14.228397369384766,
      "learning_rate": 5.653530314163798e-06,
      "loss": 1.8393,
      "step": 1376000
    },
    {
      "epoch": 53.21963104768535,
      "grad_norm": 11.726147651672363,
      "learning_rate": 5.650307460262211e-06,
      "loss": 1.8125,
      "step": 1376100
    },
    {
      "epoch": 53.223498472367254,
      "grad_norm": 9.312736511230469,
      "learning_rate": 5.647084606360624e-06,
      "loss": 1.6794,
      "step": 1376200
    },
    {
      "epoch": 53.227365897049154,
      "grad_norm": 14.118470191955566,
      "learning_rate": 5.643861752459038e-06,
      "loss": 1.7906,
      "step": 1376300
    },
    {
      "epoch": 53.23123332173106,
      "grad_norm": 12.626975059509277,
      "learning_rate": 5.640638898557451e-06,
      "loss": 1.7623,
      "step": 1376400
    },
    {
      "epoch": 53.23510074641296,
      "grad_norm": 12.901853561401367,
      "learning_rate": 5.637416044655864e-06,
      "loss": 1.8184,
      "step": 1376500
    },
    {
      "epoch": 53.23896817109487,
      "grad_norm": 9.983146667480469,
      "learning_rate": 5.634193190754277e-06,
      "loss": 1.8036,
      "step": 1376600
    },
    {
      "epoch": 53.242835595776775,
      "grad_norm": 11.943833351135254,
      "learning_rate": 5.63097033685269e-06,
      "loss": 1.7717,
      "step": 1376700
    },
    {
      "epoch": 53.246703020458675,
      "grad_norm": 13.599894523620605,
      "learning_rate": 5.627747482951103e-06,
      "loss": 1.818,
      "step": 1376800
    },
    {
      "epoch": 53.25057044514058,
      "grad_norm": 15.526100158691406,
      "learning_rate": 5.624524629049516e-06,
      "loss": 1.7822,
      "step": 1376900
    },
    {
      "epoch": 53.25443786982248,
      "grad_norm": 12.095989227294922,
      "learning_rate": 5.621301775147929e-06,
      "loss": 1.7566,
      "step": 1377000
    },
    {
      "epoch": 53.25830529450439,
      "grad_norm": 12.450226783752441,
      "learning_rate": 5.618078921246342e-06,
      "loss": 1.7864,
      "step": 1377100
    },
    {
      "epoch": 53.262172719186296,
      "grad_norm": 13.7244234085083,
      "learning_rate": 5.614856067344755e-06,
      "loss": 1.8633,
      "step": 1377200
    },
    {
      "epoch": 53.266040143868196,
      "grad_norm": 13.562544822692871,
      "learning_rate": 5.611633213443168e-06,
      "loss": 1.831,
      "step": 1377300
    },
    {
      "epoch": 53.2699075685501,
      "grad_norm": 14.716106414794922,
      "learning_rate": 5.608410359541581e-06,
      "loss": 1.799,
      "step": 1377400
    },
    {
      "epoch": 53.27377499323201,
      "grad_norm": 18.005151748657227,
      "learning_rate": 5.605187505639994e-06,
      "loss": 1.7283,
      "step": 1377500
    },
    {
      "epoch": 53.27764241791391,
      "grad_norm": 10.64150333404541,
      "learning_rate": 5.6019646517384074e-06,
      "loss": 1.7654,
      "step": 1377600
    },
    {
      "epoch": 53.28150984259582,
      "grad_norm": 10.744006156921387,
      "learning_rate": 5.5987417978368204e-06,
      "loss": 1.8087,
      "step": 1377700
    },
    {
      "epoch": 53.28537726727772,
      "grad_norm": 11.440135955810547,
      "learning_rate": 5.5955189439352335e-06,
      "loss": 1.7889,
      "step": 1377800
    },
    {
      "epoch": 53.289244691959624,
      "grad_norm": 10.904915809631348,
      "learning_rate": 5.5922960900336465e-06,
      "loss": 1.7414,
      "step": 1377900
    },
    {
      "epoch": 53.29311211664153,
      "grad_norm": 11.250386238098145,
      "learning_rate": 5.58907323613206e-06,
      "loss": 1.7528,
      "step": 1378000
    },
    {
      "epoch": 53.29697954132343,
      "grad_norm": 14.719061851501465,
      "learning_rate": 5.585850382230473e-06,
      "loss": 1.7307,
      "step": 1378100
    },
    {
      "epoch": 53.30084696600534,
      "grad_norm": 13.928863525390625,
      "learning_rate": 5.582627528328886e-06,
      "loss": 1.8579,
      "step": 1378200
    },
    {
      "epoch": 53.30471439068724,
      "grad_norm": 13.31757640838623,
      "learning_rate": 5.5794046744272994e-06,
      "loss": 1.7603,
      "step": 1378300
    },
    {
      "epoch": 53.308581815369145,
      "grad_norm": 12.909613609313965,
      "learning_rate": 5.5761818205257125e-06,
      "loss": 1.7272,
      "step": 1378400
    },
    {
      "epoch": 53.31244924005105,
      "grad_norm": 15.742720603942871,
      "learning_rate": 5.5729589666241255e-06,
      "loss": 1.7021,
      "step": 1378500
    },
    {
      "epoch": 53.31631666473295,
      "grad_norm": 12.223912239074707,
      "learning_rate": 5.5697361127225385e-06,
      "loss": 1.7646,
      "step": 1378600
    },
    {
      "epoch": 53.32018408941486,
      "grad_norm": 13.477624893188477,
      "learning_rate": 5.566513258820951e-06,
      "loss": 1.816,
      "step": 1378700
    },
    {
      "epoch": 53.324051514096766,
      "grad_norm": 10.749434471130371,
      "learning_rate": 5.5632904049193646e-06,
      "loss": 1.7923,
      "step": 1378800
    },
    {
      "epoch": 53.327918938778666,
      "grad_norm": 17.746232986450195,
      "learning_rate": 5.560067551017778e-06,
      "loss": 1.8341,
      "step": 1378900
    },
    {
      "epoch": 53.33178636346057,
      "grad_norm": 10.595495223999023,
      "learning_rate": 5.556844697116191e-06,
      "loss": 1.8477,
      "step": 1379000
    },
    {
      "epoch": 53.33565378814247,
      "grad_norm": 14.349605560302734,
      "learning_rate": 5.553621843214604e-06,
      "loss": 1.8166,
      "step": 1379100
    },
    {
      "epoch": 53.33952121282438,
      "grad_norm": 14.597807884216309,
      "learning_rate": 5.550398989313017e-06,
      "loss": 1.7344,
      "step": 1379200
    },
    {
      "epoch": 53.34338863750629,
      "grad_norm": 16.51661491394043,
      "learning_rate": 5.54717613541143e-06,
      "loss": 1.8895,
      "step": 1379300
    },
    {
      "epoch": 53.34725606218819,
      "grad_norm": 11.123619079589844,
      "learning_rate": 5.543953281509843e-06,
      "loss": 1.6644,
      "step": 1379400
    },
    {
      "epoch": 53.351123486870094,
      "grad_norm": 10.753063201904297,
      "learning_rate": 5.540730427608256e-06,
      "loss": 1.6996,
      "step": 1379500
    },
    {
      "epoch": 53.354990911552,
      "grad_norm": 12.523412704467773,
      "learning_rate": 5.537507573706669e-06,
      "loss": 1.7771,
      "step": 1379600
    },
    {
      "epoch": 53.3588583362339,
      "grad_norm": 12.20298957824707,
      "learning_rate": 5.534284719805082e-06,
      "loss": 1.7623,
      "step": 1379700
    },
    {
      "epoch": 53.36272576091581,
      "grad_norm": 14.904690742492676,
      "learning_rate": 5.531061865903496e-06,
      "loss": 1.8595,
      "step": 1379800
    },
    {
      "epoch": 53.36659318559771,
      "grad_norm": 12.302092552185059,
      "learning_rate": 5.527839012001909e-06,
      "loss": 1.7263,
      "step": 1379900
    },
    {
      "epoch": 53.370460610279615,
      "grad_norm": 13.35181999206543,
      "learning_rate": 5.524616158100322e-06,
      "loss": 1.673,
      "step": 1380000
    },
    {
      "epoch": 53.37432803496152,
      "grad_norm": 11.613572120666504,
      "learning_rate": 5.521393304198735e-06,
      "loss": 1.7522,
      "step": 1380100
    },
    {
      "epoch": 53.37819545964342,
      "grad_norm": 13.199522972106934,
      "learning_rate": 5.518170450297147e-06,
      "loss": 1.6865,
      "step": 1380200
    },
    {
      "epoch": 53.38206288432533,
      "grad_norm": 9.90345573425293,
      "learning_rate": 5.51494759639556e-06,
      "loss": 1.7513,
      "step": 1380300
    },
    {
      "epoch": 53.38593030900723,
      "grad_norm": 11.006580352783203,
      "learning_rate": 5.511724742493973e-06,
      "loss": 1.7966,
      "step": 1380400
    },
    {
      "epoch": 53.389797733689136,
      "grad_norm": 13.999760627746582,
      "learning_rate": 5.508501888592387e-06,
      "loss": 1.7602,
      "step": 1380500
    },
    {
      "epoch": 53.39366515837104,
      "grad_norm": 13.74432373046875,
      "learning_rate": 5.5052790346908e-06,
      "loss": 1.8435,
      "step": 1380600
    },
    {
      "epoch": 53.39753258305294,
      "grad_norm": 14.412699699401855,
      "learning_rate": 5.502056180789213e-06,
      "loss": 1.6548,
      "step": 1380700
    },
    {
      "epoch": 53.40140000773485,
      "grad_norm": 13.413766860961914,
      "learning_rate": 5.498833326887626e-06,
      "loss": 1.7801,
      "step": 1380800
    },
    {
      "epoch": 53.40526743241676,
      "grad_norm": 12.652422904968262,
      "learning_rate": 5.495610472986039e-06,
      "loss": 1.7493,
      "step": 1380900
    },
    {
      "epoch": 53.40913485709866,
      "grad_norm": 14.515701293945312,
      "learning_rate": 5.492387619084452e-06,
      "loss": 1.7194,
      "step": 1381000
    },
    {
      "epoch": 53.413002281780564,
      "grad_norm": 12.95488452911377,
      "learning_rate": 5.489164765182865e-06,
      "loss": 1.6843,
      "step": 1381100
    },
    {
      "epoch": 53.416869706462464,
      "grad_norm": 11.49808406829834,
      "learning_rate": 5.485941911281278e-06,
      "loss": 1.7184,
      "step": 1381200
    },
    {
      "epoch": 53.42073713114437,
      "grad_norm": 12.878007888793945,
      "learning_rate": 5.482719057379691e-06,
      "loss": 1.7772,
      "step": 1381300
    },
    {
      "epoch": 53.42460455582628,
      "grad_norm": 13.160933494567871,
      "learning_rate": 5.479496203478104e-06,
      "loss": 1.7448,
      "step": 1381400
    },
    {
      "epoch": 53.42847198050818,
      "grad_norm": 13.084168434143066,
      "learning_rate": 5.476273349576518e-06,
      "loss": 1.8017,
      "step": 1381500
    },
    {
      "epoch": 53.432339405190085,
      "grad_norm": 12.294204711914062,
      "learning_rate": 5.473050495674931e-06,
      "loss": 1.7334,
      "step": 1381600
    },
    {
      "epoch": 53.436206829871985,
      "grad_norm": 13.76145076751709,
      "learning_rate": 5.469827641773343e-06,
      "loss": 1.7842,
      "step": 1381700
    },
    {
      "epoch": 53.44007425455389,
      "grad_norm": 11.040074348449707,
      "learning_rate": 5.466604787871756e-06,
      "loss": 1.7931,
      "step": 1381800
    },
    {
      "epoch": 53.4439416792358,
      "grad_norm": 14.610970497131348,
      "learning_rate": 5.463381933970169e-06,
      "loss": 1.7242,
      "step": 1381900
    },
    {
      "epoch": 53.4478091039177,
      "grad_norm": 10.776288986206055,
      "learning_rate": 5.460159080068582e-06,
      "loss": 1.7613,
      "step": 1382000
    },
    {
      "epoch": 53.451676528599606,
      "grad_norm": 10.358327865600586,
      "learning_rate": 5.456936226166995e-06,
      "loss": 1.7507,
      "step": 1382100
    },
    {
      "epoch": 53.45554395328151,
      "grad_norm": 11.45457935333252,
      "learning_rate": 5.453713372265408e-06,
      "loss": 1.7566,
      "step": 1382200
    },
    {
      "epoch": 53.45941137796341,
      "grad_norm": 12.39759349822998,
      "learning_rate": 5.450490518363822e-06,
      "loss": 1.6733,
      "step": 1382300
    },
    {
      "epoch": 53.46327880264532,
      "grad_norm": 15.644131660461426,
      "learning_rate": 5.447267664462235e-06,
      "loss": 1.7011,
      "step": 1382400
    },
    {
      "epoch": 53.46714622732722,
      "grad_norm": 14.88464069366455,
      "learning_rate": 5.444044810560648e-06,
      "loss": 1.7482,
      "step": 1382500
    },
    {
      "epoch": 53.47101365200913,
      "grad_norm": 8.740859031677246,
      "learning_rate": 5.440821956659061e-06,
      "loss": 1.8105,
      "step": 1382600
    },
    {
      "epoch": 53.474881076691034,
      "grad_norm": 9.963958740234375,
      "learning_rate": 5.437599102757474e-06,
      "loss": 1.8172,
      "step": 1382700
    },
    {
      "epoch": 53.478748501372934,
      "grad_norm": 15.77823257446289,
      "learning_rate": 5.434376248855887e-06,
      "loss": 1.8057,
      "step": 1382800
    },
    {
      "epoch": 53.48261592605484,
      "grad_norm": 13.033807754516602,
      "learning_rate": 5.4311533949543e-06,
      "loss": 1.8576,
      "step": 1382900
    },
    {
      "epoch": 53.48648335073675,
      "grad_norm": 17.166610717773438,
      "learning_rate": 5.427930541052713e-06,
      "loss": 1.7768,
      "step": 1383000
    },
    {
      "epoch": 53.49035077541865,
      "grad_norm": 14.933775901794434,
      "learning_rate": 5.424707687151126e-06,
      "loss": 1.8974,
      "step": 1383100
    },
    {
      "epoch": 53.494218200100555,
      "grad_norm": 13.57859992980957,
      "learning_rate": 5.421484833249539e-06,
      "loss": 1.8216,
      "step": 1383200
    },
    {
      "epoch": 53.498085624782455,
      "grad_norm": 16.294281005859375,
      "learning_rate": 5.418261979347952e-06,
      "loss": 1.7959,
      "step": 1383300
    },
    {
      "epoch": 53.50195304946436,
      "grad_norm": 11.024187088012695,
      "learning_rate": 5.415039125446365e-06,
      "loss": 1.74,
      "step": 1383400
    },
    {
      "epoch": 53.50582047414627,
      "grad_norm": 12.912765502929688,
      "learning_rate": 5.411816271544778e-06,
      "loss": 1.7013,
      "step": 1383500
    },
    {
      "epoch": 53.50968789882817,
      "grad_norm": 13.116271018981934,
      "learning_rate": 5.4085934176431914e-06,
      "loss": 1.7312,
      "step": 1383600
    },
    {
      "epoch": 53.513555323510076,
      "grad_norm": 14.927091598510742,
      "learning_rate": 5.4053705637416044e-06,
      "loss": 1.7463,
      "step": 1383700
    },
    {
      "epoch": 53.517422748191976,
      "grad_norm": 14.439471244812012,
      "learning_rate": 5.4021477098400175e-06,
      "loss": 1.8706,
      "step": 1383800
    },
    {
      "epoch": 53.52129017287388,
      "grad_norm": 12.055727005004883,
      "learning_rate": 5.3989248559384305e-06,
      "loss": 1.7894,
      "step": 1383900
    },
    {
      "epoch": 53.52515759755579,
      "grad_norm": 8.546707153320312,
      "learning_rate": 5.395702002036844e-06,
      "loss": 1.7404,
      "step": 1384000
    },
    {
      "epoch": 53.52902502223769,
      "grad_norm": 10.42375373840332,
      "learning_rate": 5.392479148135257e-06,
      "loss": 1.7449,
      "step": 1384100
    },
    {
      "epoch": 53.5328924469196,
      "grad_norm": 12.921326637268066,
      "learning_rate": 5.38925629423367e-06,
      "loss": 1.7226,
      "step": 1384200
    },
    {
      "epoch": 53.536759871601504,
      "grad_norm": 13.76291275024414,
      "learning_rate": 5.3860334403320834e-06,
      "loss": 1.8241,
      "step": 1384300
    },
    {
      "epoch": 53.540627296283404,
      "grad_norm": 12.49833869934082,
      "learning_rate": 5.3828105864304965e-06,
      "loss": 1.7898,
      "step": 1384400
    },
    {
      "epoch": 53.54449472096531,
      "grad_norm": 14.036787033081055,
      "learning_rate": 5.3795877325289095e-06,
      "loss": 1.7717,
      "step": 1384500
    },
    {
      "epoch": 53.54836214564721,
      "grad_norm": 11.189729690551758,
      "learning_rate": 5.376364878627322e-06,
      "loss": 1.755,
      "step": 1384600
    },
    {
      "epoch": 53.55222957032912,
      "grad_norm": 13.227344512939453,
      "learning_rate": 5.373142024725735e-06,
      "loss": 1.7687,
      "step": 1384700
    },
    {
      "epoch": 53.556096995011025,
      "grad_norm": 8.683168411254883,
      "learning_rate": 5.3699191708241486e-06,
      "loss": 1.7559,
      "step": 1384800
    },
    {
      "epoch": 53.559964419692925,
      "grad_norm": 14.373645782470703,
      "learning_rate": 5.366696316922562e-06,
      "loss": 1.6621,
      "step": 1384900
    },
    {
      "epoch": 53.56383184437483,
      "grad_norm": 15.588664054870605,
      "learning_rate": 5.363473463020975e-06,
      "loss": 1.7933,
      "step": 1385000
    },
    {
      "epoch": 53.56769926905673,
      "grad_norm": 8.223774909973145,
      "learning_rate": 5.360250609119388e-06,
      "loss": 1.7477,
      "step": 1385100
    },
    {
      "epoch": 53.57156669373864,
      "grad_norm": 12.817269325256348,
      "learning_rate": 5.357027755217801e-06,
      "loss": 1.731,
      "step": 1385200
    },
    {
      "epoch": 53.575434118420546,
      "grad_norm": 12.363988876342773,
      "learning_rate": 5.353804901316214e-06,
      "loss": 1.7626,
      "step": 1385300
    },
    {
      "epoch": 53.579301543102446,
      "grad_norm": 12.767192840576172,
      "learning_rate": 5.350582047414627e-06,
      "loss": 1.7993,
      "step": 1385400
    },
    {
      "epoch": 53.58316896778435,
      "grad_norm": 13.999899864196777,
      "learning_rate": 5.34735919351304e-06,
      "loss": 1.8113,
      "step": 1385500
    },
    {
      "epoch": 53.58703639246626,
      "grad_norm": 12.793708801269531,
      "learning_rate": 5.344136339611453e-06,
      "loss": 1.7434,
      "step": 1385600
    },
    {
      "epoch": 53.59090381714816,
      "grad_norm": 11.005901336669922,
      "learning_rate": 5.340913485709867e-06,
      "loss": 1.8576,
      "step": 1385700
    },
    {
      "epoch": 53.59477124183007,
      "grad_norm": 12.078263282775879,
      "learning_rate": 5.33769063180828e-06,
      "loss": 1.7785,
      "step": 1385800
    },
    {
      "epoch": 53.59863866651197,
      "grad_norm": 11.515448570251465,
      "learning_rate": 5.334467777906693e-06,
      "loss": 1.7514,
      "step": 1385900
    },
    {
      "epoch": 53.602506091193874,
      "grad_norm": 13.956117630004883,
      "learning_rate": 5.331244924005106e-06,
      "loss": 1.7866,
      "step": 1386000
    },
    {
      "epoch": 53.60637351587578,
      "grad_norm": 8.182929992675781,
      "learning_rate": 5.328022070103518e-06,
      "loss": 1.8175,
      "step": 1386100
    },
    {
      "epoch": 53.61024094055768,
      "grad_norm": 15.0106840133667,
      "learning_rate": 5.324799216201931e-06,
      "loss": 1.7463,
      "step": 1386200
    },
    {
      "epoch": 53.61410836523959,
      "grad_norm": 10.431697845458984,
      "learning_rate": 5.321576362300344e-06,
      "loss": 1.7375,
      "step": 1386300
    },
    {
      "epoch": 53.61797578992149,
      "grad_norm": 14.269560813903809,
      "learning_rate": 5.318353508398757e-06,
      "loss": 1.7787,
      "step": 1386400
    },
    {
      "epoch": 53.621843214603395,
      "grad_norm": 11.77189826965332,
      "learning_rate": 5.315130654497171e-06,
      "loss": 1.8501,
      "step": 1386500
    },
    {
      "epoch": 53.6257106392853,
      "grad_norm": 11.582133293151855,
      "learning_rate": 5.311907800595584e-06,
      "loss": 1.811,
      "step": 1386600
    },
    {
      "epoch": 53.6295780639672,
      "grad_norm": 11.674318313598633,
      "learning_rate": 5.308684946693997e-06,
      "loss": 1.7624,
      "step": 1386700
    },
    {
      "epoch": 53.63344548864911,
      "grad_norm": 13.820837020874023,
      "learning_rate": 5.30546209279241e-06,
      "loss": 1.7242,
      "step": 1386800
    },
    {
      "epoch": 53.637312913331016,
      "grad_norm": 12.751520156860352,
      "learning_rate": 5.302239238890823e-06,
      "loss": 1.7612,
      "step": 1386900
    },
    {
      "epoch": 53.641180338012916,
      "grad_norm": 12.764716148376465,
      "learning_rate": 5.299016384989236e-06,
      "loss": 1.7116,
      "step": 1387000
    },
    {
      "epoch": 53.64504776269482,
      "grad_norm": 12.71077823638916,
      "learning_rate": 5.295793531087649e-06,
      "loss": 1.8668,
      "step": 1387100
    },
    {
      "epoch": 53.64891518737672,
      "grad_norm": 16.325393676757812,
      "learning_rate": 5.292570677186062e-06,
      "loss": 1.735,
      "step": 1387200
    },
    {
      "epoch": 53.65278261205863,
      "grad_norm": 12.509087562561035,
      "learning_rate": 5.289347823284475e-06,
      "loss": 1.7765,
      "step": 1387300
    },
    {
      "epoch": 53.65665003674054,
      "grad_norm": 12.002099990844727,
      "learning_rate": 5.286124969382889e-06,
      "loss": 1.7375,
      "step": 1387400
    },
    {
      "epoch": 53.66051746142244,
      "grad_norm": 9.983813285827637,
      "learning_rate": 5.282902115481302e-06,
      "loss": 1.7435,
      "step": 1387500
    },
    {
      "epoch": 53.664384886104344,
      "grad_norm": 11.556755065917969,
      "learning_rate": 5.279679261579714e-06,
      "loss": 1.8573,
      "step": 1387600
    },
    {
      "epoch": 53.66825231078625,
      "grad_norm": 14.947184562683105,
      "learning_rate": 5.276456407678127e-06,
      "loss": 1.7661,
      "step": 1387700
    },
    {
      "epoch": 53.67211973546815,
      "grad_norm": 13.440293312072754,
      "learning_rate": 5.27323355377654e-06,
      "loss": 1.6906,
      "step": 1387800
    },
    {
      "epoch": 53.67598716015006,
      "grad_norm": 10.689215660095215,
      "learning_rate": 5.270010699874953e-06,
      "loss": 1.7434,
      "step": 1387900
    },
    {
      "epoch": 53.67985458483196,
      "grad_norm": 11.907904624938965,
      "learning_rate": 5.266787845973366e-06,
      "loss": 1.7976,
      "step": 1388000
    },
    {
      "epoch": 53.683722009513865,
      "grad_norm": 13.078659057617188,
      "learning_rate": 5.263564992071779e-06,
      "loss": 1.7735,
      "step": 1388100
    },
    {
      "epoch": 53.68758943419577,
      "grad_norm": 11.667756080627441,
      "learning_rate": 5.260342138170193e-06,
      "loss": 1.7623,
      "step": 1388200
    },
    {
      "epoch": 53.69145685887767,
      "grad_norm": 10.085372924804688,
      "learning_rate": 5.257119284268606e-06,
      "loss": 1.6138,
      "step": 1388300
    },
    {
      "epoch": 53.69532428355958,
      "grad_norm": 11.953585624694824,
      "learning_rate": 5.253896430367019e-06,
      "loss": 1.8155,
      "step": 1388400
    },
    {
      "epoch": 53.69919170824148,
      "grad_norm": 10.52559757232666,
      "learning_rate": 5.250673576465432e-06,
      "loss": 1.7687,
      "step": 1388500
    },
    {
      "epoch": 53.703059132923386,
      "grad_norm": 9.88458251953125,
      "learning_rate": 5.247450722563845e-06,
      "loss": 1.8326,
      "step": 1388600
    },
    {
      "epoch": 53.70692655760529,
      "grad_norm": 14.703685760498047,
      "learning_rate": 5.244227868662258e-06,
      "loss": 1.7207,
      "step": 1388700
    },
    {
      "epoch": 53.71079398228719,
      "grad_norm": 11.305644989013672,
      "learning_rate": 5.241005014760671e-06,
      "loss": 1.7815,
      "step": 1388800
    },
    {
      "epoch": 53.7146614069691,
      "grad_norm": 9.456916809082031,
      "learning_rate": 5.237782160859084e-06,
      "loss": 1.784,
      "step": 1388900
    },
    {
      "epoch": 53.71852883165101,
      "grad_norm": 11.878779411315918,
      "learning_rate": 5.234559306957497e-06,
      "loss": 1.7712,
      "step": 1389000
    },
    {
      "epoch": 53.72239625633291,
      "grad_norm": 14.812959671020508,
      "learning_rate": 5.23133645305591e-06,
      "loss": 1.7468,
      "step": 1389100
    },
    {
      "epoch": 53.726263681014814,
      "grad_norm": 13.652042388916016,
      "learning_rate": 5.228113599154323e-06,
      "loss": 1.772,
      "step": 1389200
    },
    {
      "epoch": 53.730131105696714,
      "grad_norm": 10.858698844909668,
      "learning_rate": 5.224890745252736e-06,
      "loss": 1.8258,
      "step": 1389300
    },
    {
      "epoch": 53.73399853037862,
      "grad_norm": 12.819238662719727,
      "learning_rate": 5.221667891351149e-06,
      "loss": 1.7704,
      "step": 1389400
    },
    {
      "epoch": 53.73786595506053,
      "grad_norm": 15.304031372070312,
      "learning_rate": 5.218445037449562e-06,
      "loss": 1.8182,
      "step": 1389500
    },
    {
      "epoch": 53.74173337974243,
      "grad_norm": 12.02910327911377,
      "learning_rate": 5.2152221835479754e-06,
      "loss": 1.7649,
      "step": 1389600
    },
    {
      "epoch": 53.745600804424335,
      "grad_norm": 12.191741943359375,
      "learning_rate": 5.2119993296463885e-06,
      "loss": 1.8271,
      "step": 1389700
    },
    {
      "epoch": 53.749468229106235,
      "grad_norm": 22.9830322265625,
      "learning_rate": 5.2087764757448015e-06,
      "loss": 1.8194,
      "step": 1389800
    },
    {
      "epoch": 53.75333565378814,
      "grad_norm": 11.371248245239258,
      "learning_rate": 5.205553621843215e-06,
      "loss": 1.8212,
      "step": 1389900
    },
    {
      "epoch": 53.75720307847005,
      "grad_norm": 12.431831359863281,
      "learning_rate": 5.202330767941628e-06,
      "loss": 1.6689,
      "step": 1390000
    },
    {
      "epoch": 53.76107050315195,
      "grad_norm": 12.654935836791992,
      "learning_rate": 5.199107914040041e-06,
      "loss": 1.7704,
      "step": 1390100
    },
    {
      "epoch": 53.764937927833856,
      "grad_norm": 11.919611930847168,
      "learning_rate": 5.195885060138454e-06,
      "loss": 1.8265,
      "step": 1390200
    },
    {
      "epoch": 53.76880535251576,
      "grad_norm": 10.15847396850586,
      "learning_rate": 5.1926622062368674e-06,
      "loss": 1.8937,
      "step": 1390300
    },
    {
      "epoch": 53.77267277719766,
      "grad_norm": 12.894497871398926,
      "learning_rate": 5.1894393523352805e-06,
      "loss": 1.7554,
      "step": 1390400
    },
    {
      "epoch": 53.77654020187957,
      "grad_norm": 11.786497116088867,
      "learning_rate": 5.186216498433693e-06,
      "loss": 1.7463,
      "step": 1390500
    },
    {
      "epoch": 53.78040762656147,
      "grad_norm": 13.53509521484375,
      "learning_rate": 5.182993644532106e-06,
      "loss": 1.7557,
      "step": 1390600
    },
    {
      "epoch": 53.78427505124338,
      "grad_norm": 15.823712348937988,
      "learning_rate": 5.1797707906305195e-06,
      "loss": 1.7812,
      "step": 1390700
    },
    {
      "epoch": 53.788142475925284,
      "grad_norm": 11.595926284790039,
      "learning_rate": 5.1765479367289326e-06,
      "loss": 1.7483,
      "step": 1390800
    },
    {
      "epoch": 53.792009900607184,
      "grad_norm": 12.194256782531738,
      "learning_rate": 5.173325082827346e-06,
      "loss": 1.8022,
      "step": 1390900
    },
    {
      "epoch": 53.79587732528909,
      "grad_norm": 13.459880828857422,
      "learning_rate": 5.170102228925759e-06,
      "loss": 1.8236,
      "step": 1391000
    },
    {
      "epoch": 53.799744749971,
      "grad_norm": 12.147579193115234,
      "learning_rate": 5.166879375024172e-06,
      "loss": 1.834,
      "step": 1391100
    },
    {
      "epoch": 53.8036121746529,
      "grad_norm": 15.515027046203613,
      "learning_rate": 5.163656521122585e-06,
      "loss": 1.7825,
      "step": 1391200
    },
    {
      "epoch": 53.807479599334805,
      "grad_norm": 5.409813404083252,
      "learning_rate": 5.160433667220998e-06,
      "loss": 1.7538,
      "step": 1391300
    },
    {
      "epoch": 53.811347024016705,
      "grad_norm": 18.17226791381836,
      "learning_rate": 5.157210813319411e-06,
      "loss": 1.838,
      "step": 1391400
    },
    {
      "epoch": 53.81521444869861,
      "grad_norm": 9.264613151550293,
      "learning_rate": 5.153987959417824e-06,
      "loss": 1.8353,
      "step": 1391500
    },
    {
      "epoch": 53.81908187338052,
      "grad_norm": 13.647815704345703,
      "learning_rate": 5.150765105516237e-06,
      "loss": 1.7626,
      "step": 1391600
    },
    {
      "epoch": 53.82294929806242,
      "grad_norm": 10.6306734085083,
      "learning_rate": 5.147542251614651e-06,
      "loss": 1.8632,
      "step": 1391700
    },
    {
      "epoch": 53.826816722744326,
      "grad_norm": 13.89098834991455,
      "learning_rate": 5.144319397713064e-06,
      "loss": 1.7877,
      "step": 1391800
    },
    {
      "epoch": 53.830684147426226,
      "grad_norm": 22.866710662841797,
      "learning_rate": 5.141096543811477e-06,
      "loss": 1.7712,
      "step": 1391900
    },
    {
      "epoch": 53.83455157210813,
      "grad_norm": 15.227262496948242,
      "learning_rate": 5.137873689909889e-06,
      "loss": 1.8143,
      "step": 1392000
    },
    {
      "epoch": 53.83841899679004,
      "grad_norm": 13.529773712158203,
      "learning_rate": 5.134650836008302e-06,
      "loss": 1.7611,
      "step": 1392100
    },
    {
      "epoch": 53.84228642147194,
      "grad_norm": 14.17934513092041,
      "learning_rate": 5.131427982106715e-06,
      "loss": 1.7069,
      "step": 1392200
    },
    {
      "epoch": 53.84615384615385,
      "grad_norm": 11.685077667236328,
      "learning_rate": 5.128205128205128e-06,
      "loss": 1.7904,
      "step": 1392300
    },
    {
      "epoch": 53.850021270835754,
      "grad_norm": 11.702717781066895,
      "learning_rate": 5.124982274303542e-06,
      "loss": 1.7813,
      "step": 1392400
    },
    {
      "epoch": 53.853888695517654,
      "grad_norm": 12.544495582580566,
      "learning_rate": 5.121759420401955e-06,
      "loss": 1.8889,
      "step": 1392500
    },
    {
      "epoch": 53.85775612019956,
      "grad_norm": 12.008960723876953,
      "learning_rate": 5.118536566500368e-06,
      "loss": 1.737,
      "step": 1392600
    },
    {
      "epoch": 53.86162354488146,
      "grad_norm": 11.473722457885742,
      "learning_rate": 5.115313712598781e-06,
      "loss": 1.8289,
      "step": 1392700
    },
    {
      "epoch": 53.86549096956337,
      "grad_norm": 14.479385375976562,
      "learning_rate": 5.112090858697194e-06,
      "loss": 1.8323,
      "step": 1392800
    },
    {
      "epoch": 53.869358394245275,
      "grad_norm": 14.324498176574707,
      "learning_rate": 5.108868004795607e-06,
      "loss": 1.8466,
      "step": 1392900
    },
    {
      "epoch": 53.873225818927175,
      "grad_norm": 12.802544593811035,
      "learning_rate": 5.10564515089402e-06,
      "loss": 1.9051,
      "step": 1393000
    },
    {
      "epoch": 53.87709324360908,
      "grad_norm": 12.378962516784668,
      "learning_rate": 5.102422296992433e-06,
      "loss": 1.7484,
      "step": 1393100
    },
    {
      "epoch": 53.88096066829098,
      "grad_norm": 13.449917793273926,
      "learning_rate": 5.099199443090846e-06,
      "loss": 1.7564,
      "step": 1393200
    },
    {
      "epoch": 53.88482809297289,
      "grad_norm": 12.746162414550781,
      "learning_rate": 5.095976589189259e-06,
      "loss": 1.7718,
      "step": 1393300
    },
    {
      "epoch": 53.888695517654796,
      "grad_norm": 13.253104209899902,
      "learning_rate": 5.092753735287673e-06,
      "loss": 1.8492,
      "step": 1393400
    },
    {
      "epoch": 53.892562942336696,
      "grad_norm": 11.929300308227539,
      "learning_rate": 5.089530881386085e-06,
      "loss": 1.7007,
      "step": 1393500
    },
    {
      "epoch": 53.8964303670186,
      "grad_norm": 16.381793975830078,
      "learning_rate": 5.086308027484498e-06,
      "loss": 1.8593,
      "step": 1393600
    },
    {
      "epoch": 53.90029779170051,
      "grad_norm": 11.998846054077148,
      "learning_rate": 5.083085173582911e-06,
      "loss": 1.7598,
      "step": 1393700
    },
    {
      "epoch": 53.90416521638241,
      "grad_norm": 12.22896671295166,
      "learning_rate": 5.079862319681324e-06,
      "loss": 1.8143,
      "step": 1393800
    },
    {
      "epoch": 53.90803264106432,
      "grad_norm": 12.604562759399414,
      "learning_rate": 5.076639465779737e-06,
      "loss": 1.873,
      "step": 1393900
    },
    {
      "epoch": 53.91190006574622,
      "grad_norm": 12.136763572692871,
      "learning_rate": 5.07341661187815e-06,
      "loss": 1.7605,
      "step": 1394000
    },
    {
      "epoch": 53.915767490428124,
      "grad_norm": 11.416266441345215,
      "learning_rate": 5.070193757976563e-06,
      "loss": 1.7705,
      "step": 1394100
    },
    {
      "epoch": 53.91963491511003,
      "grad_norm": 12.226882934570312,
      "learning_rate": 5.066970904074977e-06,
      "loss": 1.7559,
      "step": 1394200
    },
    {
      "epoch": 53.92350233979193,
      "grad_norm": 10.83812141418457,
      "learning_rate": 5.06374805017339e-06,
      "loss": 1.8149,
      "step": 1394300
    },
    {
      "epoch": 53.92736976447384,
      "grad_norm": 14.993151664733887,
      "learning_rate": 5.060525196271803e-06,
      "loss": 1.7491,
      "step": 1394400
    },
    {
      "epoch": 53.93123718915574,
      "grad_norm": 12.076138496398926,
      "learning_rate": 5.057302342370216e-06,
      "loss": 1.7876,
      "step": 1394500
    },
    {
      "epoch": 53.935104613837645,
      "grad_norm": 10.72149658203125,
      "learning_rate": 5.054079488468629e-06,
      "loss": 1.7493,
      "step": 1394600
    },
    {
      "epoch": 53.93897203851955,
      "grad_norm": 14.320982933044434,
      "learning_rate": 5.050856634567042e-06,
      "loss": 1.7557,
      "step": 1394700
    },
    {
      "epoch": 53.94283946320145,
      "grad_norm": 13.344457626342773,
      "learning_rate": 5.047633780665455e-06,
      "loss": 1.7227,
      "step": 1394800
    },
    {
      "epoch": 53.94670688788336,
      "grad_norm": 8.94479751586914,
      "learning_rate": 5.044410926763868e-06,
      "loss": 1.7768,
      "step": 1394900
    },
    {
      "epoch": 53.950574312565266,
      "grad_norm": 11.87492561340332,
      "learning_rate": 5.041188072862281e-06,
      "loss": 1.8234,
      "step": 1395000
    },
    {
      "epoch": 53.954441737247166,
      "grad_norm": 12.213295936584473,
      "learning_rate": 5.037965218960694e-06,
      "loss": 1.8069,
      "step": 1395100
    },
    {
      "epoch": 53.95830916192907,
      "grad_norm": 13.315389633178711,
      "learning_rate": 5.034742365059107e-06,
      "loss": 1.7312,
      "step": 1395200
    },
    {
      "epoch": 53.96217658661097,
      "grad_norm": 14.3806734085083,
      "learning_rate": 5.03151951115752e-06,
      "loss": 1.8072,
      "step": 1395300
    },
    {
      "epoch": 53.96604401129288,
      "grad_norm": 10.720195770263672,
      "learning_rate": 5.028296657255933e-06,
      "loss": 1.9351,
      "step": 1395400
    },
    {
      "epoch": 53.96991143597479,
      "grad_norm": 10.32690143585205,
      "learning_rate": 5.025073803354346e-06,
      "loss": 1.7622,
      "step": 1395500
    },
    {
      "epoch": 53.97377886065669,
      "grad_norm": 17.21539306640625,
      "learning_rate": 5.0218509494527594e-06,
      "loss": 1.772,
      "step": 1395600
    },
    {
      "epoch": 53.977646285338594,
      "grad_norm": 15.984797477722168,
      "learning_rate": 5.0186280955511725e-06,
      "loss": 1.8707,
      "step": 1395700
    },
    {
      "epoch": 53.9815137100205,
      "grad_norm": 14.622504234313965,
      "learning_rate": 5.0154052416495855e-06,
      "loss": 1.8529,
      "step": 1395800
    },
    {
      "epoch": 53.9853811347024,
      "grad_norm": 14.699166297912598,
      "learning_rate": 5.012182387747999e-06,
      "loss": 1.7692,
      "step": 1395900
    },
    {
      "epoch": 53.98924855938431,
      "grad_norm": 12.464966773986816,
      "learning_rate": 5.008959533846412e-06,
      "loss": 1.8262,
      "step": 1396000
    },
    {
      "epoch": 53.99311598406621,
      "grad_norm": 12.006653785705566,
      "learning_rate": 5.005736679944825e-06,
      "loss": 1.7424,
      "step": 1396100
    },
    {
      "epoch": 53.996983408748115,
      "grad_norm": 12.085314750671387,
      "learning_rate": 5.002513826043238e-06,
      "loss": 1.8742,
      "step": 1396200
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.7400022745132446,
      "eval_runtime": 3.0309,
      "eval_samples_per_second": 449.048,
      "eval_steps_per_second": 449.048,
      "step": 1396278
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.572365403175354,
      "eval_runtime": 55.7548,
      "eval_samples_per_second": 463.763,
      "eval_steps_per_second": 463.763,
      "step": 1396278
    },
    {
      "epoch": 54.00085083343002,
      "grad_norm": 14.687826156616211,
      "learning_rate": 4.9992909721416514e-06,
      "loss": 1.7987,
      "step": 1396300
    },
    {
      "epoch": 54.00471825811192,
      "grad_norm": 12.492151260375977,
      "learning_rate": 4.996068118240064e-06,
      "loss": 1.7813,
      "step": 1396400
    },
    {
      "epoch": 54.00858568279383,
      "grad_norm": 14.095602989196777,
      "learning_rate": 4.992845264338477e-06,
      "loss": 1.6973,
      "step": 1396500
    },
    {
      "epoch": 54.01245310747573,
      "grad_norm": 12.557737350463867,
      "learning_rate": 4.98962241043689e-06,
      "loss": 1.7934,
      "step": 1396600
    },
    {
      "epoch": 54.016320532157636,
      "grad_norm": 13.289387702941895,
      "learning_rate": 4.9863995565353035e-06,
      "loss": 1.7786,
      "step": 1396700
    },
    {
      "epoch": 54.02018795683954,
      "grad_norm": 11.954812049865723,
      "learning_rate": 4.9831767026337166e-06,
      "loss": 1.784,
      "step": 1396800
    },
    {
      "epoch": 54.02405538152144,
      "grad_norm": 11.649675369262695,
      "learning_rate": 4.97995384873213e-06,
      "loss": 1.7507,
      "step": 1396900
    },
    {
      "epoch": 54.02792280620335,
      "grad_norm": 10.901422500610352,
      "learning_rate": 4.976730994830543e-06,
      "loss": 1.7076,
      "step": 1397000
    },
    {
      "epoch": 54.03179023088526,
      "grad_norm": 13.323736190795898,
      "learning_rate": 4.973508140928956e-06,
      "loss": 1.7575,
      "step": 1397100
    },
    {
      "epoch": 54.03565765556716,
      "grad_norm": 11.573156356811523,
      "learning_rate": 4.970285287027369e-06,
      "loss": 1.7053,
      "step": 1397200
    },
    {
      "epoch": 54.039525080249064,
      "grad_norm": 14.16793155670166,
      "learning_rate": 4.967062433125782e-06,
      "loss": 1.7975,
      "step": 1397300
    },
    {
      "epoch": 54.043392504930964,
      "grad_norm": 9.728160858154297,
      "learning_rate": 4.963839579224195e-06,
      "loss": 1.7078,
      "step": 1397400
    },
    {
      "epoch": 54.04725992961287,
      "grad_norm": 12.291023254394531,
      "learning_rate": 4.960616725322608e-06,
      "loss": 1.7799,
      "step": 1397500
    },
    {
      "epoch": 54.05112735429478,
      "grad_norm": 8.916433334350586,
      "learning_rate": 4.957393871421022e-06,
      "loss": 1.6746,
      "step": 1397600
    },
    {
      "epoch": 54.05499477897668,
      "grad_norm": 11.536360740661621,
      "learning_rate": 4.954171017519435e-06,
      "loss": 1.7058,
      "step": 1397700
    },
    {
      "epoch": 54.058862203658585,
      "grad_norm": 12.58944320678711,
      "learning_rate": 4.950948163617848e-06,
      "loss": 1.7065,
      "step": 1397800
    },
    {
      "epoch": 54.062729628340485,
      "grad_norm": 14.518584251403809,
      "learning_rate": 4.94772530971626e-06,
      "loss": 1.7473,
      "step": 1397900
    },
    {
      "epoch": 54.06659705302239,
      "grad_norm": 13.208794593811035,
      "learning_rate": 4.944502455814673e-06,
      "loss": 1.753,
      "step": 1398000
    },
    {
      "epoch": 54.0704644777043,
      "grad_norm": 12.829976081848145,
      "learning_rate": 4.941279601913086e-06,
      "loss": 1.7276,
      "step": 1398100
    },
    {
      "epoch": 54.0743319023862,
      "grad_norm": 13.860119819641113,
      "learning_rate": 4.938056748011499e-06,
      "loss": 1.7948,
      "step": 1398200
    },
    {
      "epoch": 54.078199327068106,
      "grad_norm": 11.568777084350586,
      "learning_rate": 4.934833894109912e-06,
      "loss": 1.8121,
      "step": 1398300
    },
    {
      "epoch": 54.08206675175001,
      "grad_norm": 12.2826566696167,
      "learning_rate": 4.931611040208326e-06,
      "loss": 1.7636,
      "step": 1398400
    },
    {
      "epoch": 54.08593417643191,
      "grad_norm": 15.714091300964355,
      "learning_rate": 4.928388186306739e-06,
      "loss": 1.7753,
      "step": 1398500
    },
    {
      "epoch": 54.08980160111382,
      "grad_norm": 14.18451976776123,
      "learning_rate": 4.925165332405152e-06,
      "loss": 1.7506,
      "step": 1398600
    },
    {
      "epoch": 54.09366902579572,
      "grad_norm": 11.42210578918457,
      "learning_rate": 4.921942478503565e-06,
      "loss": 1.8301,
      "step": 1398700
    },
    {
      "epoch": 54.09753645047763,
      "grad_norm": 12.069396018981934,
      "learning_rate": 4.918719624601978e-06,
      "loss": 1.8347,
      "step": 1398800
    },
    {
      "epoch": 54.101403875159534,
      "grad_norm": 12.320998191833496,
      "learning_rate": 4.915496770700391e-06,
      "loss": 1.7603,
      "step": 1398900
    },
    {
      "epoch": 54.105271299841434,
      "grad_norm": 12.014999389648438,
      "learning_rate": 4.912273916798804e-06,
      "loss": 1.8067,
      "step": 1399000
    },
    {
      "epoch": 54.10913872452334,
      "grad_norm": 13.644165992736816,
      "learning_rate": 4.909051062897217e-06,
      "loss": 1.7955,
      "step": 1399100
    },
    {
      "epoch": 54.11300614920525,
      "grad_norm": 11.12357234954834,
      "learning_rate": 4.90582820899563e-06,
      "loss": 1.7667,
      "step": 1399200
    },
    {
      "epoch": 54.11687357388715,
      "grad_norm": 11.066818237304688,
      "learning_rate": 4.902605355094044e-06,
      "loss": 1.7356,
      "step": 1399300
    },
    {
      "epoch": 54.120740998569055,
      "grad_norm": 10.83698558807373,
      "learning_rate": 4.899382501192456e-06,
      "loss": 1.7602,
      "step": 1399400
    },
    {
      "epoch": 54.124608423250955,
      "grad_norm": 12.384697914123535,
      "learning_rate": 4.896159647290869e-06,
      "loss": 1.7542,
      "step": 1399500
    },
    {
      "epoch": 54.12847584793286,
      "grad_norm": 11.519801139831543,
      "learning_rate": 4.892936793389282e-06,
      "loss": 1.8149,
      "step": 1399600
    },
    {
      "epoch": 54.13234327261477,
      "grad_norm": 11.709689140319824,
      "learning_rate": 4.889713939487695e-06,
      "loss": 1.7939,
      "step": 1399700
    },
    {
      "epoch": 54.13621069729667,
      "grad_norm": 14.087331771850586,
      "learning_rate": 4.886491085586108e-06,
      "loss": 1.7488,
      "step": 1399800
    },
    {
      "epoch": 54.140078121978576,
      "grad_norm": 12.984382629394531,
      "learning_rate": 4.883268231684521e-06,
      "loss": 1.7823,
      "step": 1399900
    },
    {
      "epoch": 54.143945546660476,
      "grad_norm": 12.560574531555176,
      "learning_rate": 4.880045377782934e-06,
      "loss": 1.7727,
      "step": 1400000
    },
    {
      "epoch": 54.14781297134238,
      "grad_norm": 13.263071060180664,
      "learning_rate": 4.876822523881348e-06,
      "loss": 1.735,
      "step": 1400100
    },
    {
      "epoch": 54.15168039602429,
      "grad_norm": 11.195500373840332,
      "learning_rate": 4.873599669979761e-06,
      "loss": 1.7816,
      "step": 1400200
    },
    {
      "epoch": 54.15554782070619,
      "grad_norm": 10.471330642700195,
      "learning_rate": 4.870376816078174e-06,
      "loss": 1.7295,
      "step": 1400300
    },
    {
      "epoch": 54.1594152453881,
      "grad_norm": 12.45411205291748,
      "learning_rate": 4.867153962176587e-06,
      "loss": 1.8279,
      "step": 1400400
    },
    {
      "epoch": 54.163282670070004,
      "grad_norm": 12.587931632995605,
      "learning_rate": 4.863931108275e-06,
      "loss": 1.772,
      "step": 1400500
    },
    {
      "epoch": 54.167150094751904,
      "grad_norm": 11.015471458435059,
      "learning_rate": 4.860708254373413e-06,
      "loss": 1.788,
      "step": 1400600
    },
    {
      "epoch": 54.17101751943381,
      "grad_norm": 10.639337539672852,
      "learning_rate": 4.857485400471826e-06,
      "loss": 1.7742,
      "step": 1400700
    },
    {
      "epoch": 54.17488494411571,
      "grad_norm": 12.589932441711426,
      "learning_rate": 4.854262546570238e-06,
      "loss": 1.8483,
      "step": 1400800
    },
    {
      "epoch": 54.17875236879762,
      "grad_norm": 12.173295021057129,
      "learning_rate": 4.851039692668652e-06,
      "loss": 1.7039,
      "step": 1400900
    },
    {
      "epoch": 54.182619793479525,
      "grad_norm": 12.812471389770508,
      "learning_rate": 4.847816838767065e-06,
      "loss": 1.8522,
      "step": 1401000
    },
    {
      "epoch": 54.186487218161425,
      "grad_norm": 15.149059295654297,
      "learning_rate": 4.844593984865478e-06,
      "loss": 1.7418,
      "step": 1401100
    },
    {
      "epoch": 54.19035464284333,
      "grad_norm": 14.189346313476562,
      "learning_rate": 4.841371130963891e-06,
      "loss": 1.77,
      "step": 1401200
    },
    {
      "epoch": 54.19422206752523,
      "grad_norm": 10.116914749145508,
      "learning_rate": 4.838148277062304e-06,
      "loss": 1.6508,
      "step": 1401300
    },
    {
      "epoch": 54.19808949220714,
      "grad_norm": 14.978431701660156,
      "learning_rate": 4.834925423160717e-06,
      "loss": 1.7531,
      "step": 1401400
    },
    {
      "epoch": 54.201956916889046,
      "grad_norm": 12.858732223510742,
      "learning_rate": 4.83170256925913e-06,
      "loss": 1.7279,
      "step": 1401500
    },
    {
      "epoch": 54.205824341570946,
      "grad_norm": 12.485365867614746,
      "learning_rate": 4.8284797153575434e-06,
      "loss": 1.705,
      "step": 1401600
    },
    {
      "epoch": 54.20969176625285,
      "grad_norm": 10.987809181213379,
      "learning_rate": 4.8252568614559565e-06,
      "loss": 1.7076,
      "step": 1401700
    },
    {
      "epoch": 54.21355919093476,
      "grad_norm": 13.885563850402832,
      "learning_rate": 4.82203400755437e-06,
      "loss": 1.7997,
      "step": 1401800
    },
    {
      "epoch": 54.21742661561666,
      "grad_norm": 12.56674575805664,
      "learning_rate": 4.818811153652783e-06,
      "loss": 1.7031,
      "step": 1401900
    },
    {
      "epoch": 54.22129404029857,
      "grad_norm": 13.348310470581055,
      "learning_rate": 4.815588299751196e-06,
      "loss": 1.8092,
      "step": 1402000
    },
    {
      "epoch": 54.22516146498047,
      "grad_norm": 10.893266677856445,
      "learning_rate": 4.812365445849609e-06,
      "loss": 1.832,
      "step": 1402100
    },
    {
      "epoch": 54.229028889662374,
      "grad_norm": 15.611478805541992,
      "learning_rate": 4.809142591948022e-06,
      "loss": 1.8062,
      "step": 1402200
    },
    {
      "epoch": 54.23289631434428,
      "grad_norm": 12.557282447814941,
      "learning_rate": 4.805919738046435e-06,
      "loss": 1.8033,
      "step": 1402300
    },
    {
      "epoch": 54.23676373902618,
      "grad_norm": 16.427539825439453,
      "learning_rate": 4.802696884144848e-06,
      "loss": 1.793,
      "step": 1402400
    },
    {
      "epoch": 54.24063116370809,
      "grad_norm": 10.576778411865234,
      "learning_rate": 4.799474030243261e-06,
      "loss": 1.7016,
      "step": 1402500
    },
    {
      "epoch": 54.244498588389995,
      "grad_norm": 11.970834732055664,
      "learning_rate": 4.7962511763416745e-06,
      "loss": 1.773,
      "step": 1402600
    },
    {
      "epoch": 54.248366013071895,
      "grad_norm": 10.761582374572754,
      "learning_rate": 4.7930283224400875e-06,
      "loss": 1.7457,
      "step": 1402700
    },
    {
      "epoch": 54.2522334377538,
      "grad_norm": 11.584354400634766,
      "learning_rate": 4.7898054685385006e-06,
      "loss": 1.7591,
      "step": 1402800
    },
    {
      "epoch": 54.2561008624357,
      "grad_norm": 11.431401252746582,
      "learning_rate": 4.786582614636914e-06,
      "loss": 1.7852,
      "step": 1402900
    },
    {
      "epoch": 54.25996828711761,
      "grad_norm": 12.520207405090332,
      "learning_rate": 4.783359760735327e-06,
      "loss": 1.7865,
      "step": 1403000
    },
    {
      "epoch": 54.263835711799516,
      "grad_norm": 10.855875015258789,
      "learning_rate": 4.78013690683374e-06,
      "loss": 1.7172,
      "step": 1403100
    },
    {
      "epoch": 54.267703136481416,
      "grad_norm": 10.909013748168945,
      "learning_rate": 4.776914052932153e-06,
      "loss": 1.7647,
      "step": 1403200
    },
    {
      "epoch": 54.27157056116332,
      "grad_norm": 20.826414108276367,
      "learning_rate": 4.773691199030566e-06,
      "loss": 1.7878,
      "step": 1403300
    },
    {
      "epoch": 54.27543798584522,
      "grad_norm": 12.757857322692871,
      "learning_rate": 4.770468345128979e-06,
      "loss": 1.733,
      "step": 1403400
    },
    {
      "epoch": 54.27930541052713,
      "grad_norm": 14.450268745422363,
      "learning_rate": 4.767245491227392e-06,
      "loss": 1.7917,
      "step": 1403500
    },
    {
      "epoch": 54.28317283520904,
      "grad_norm": 10.5755615234375,
      "learning_rate": 4.764022637325806e-06,
      "loss": 1.8425,
      "step": 1403600
    },
    {
      "epoch": 54.28704025989094,
      "grad_norm": 10.274560928344727,
      "learning_rate": 4.760799783424219e-06,
      "loss": 1.6878,
      "step": 1403700
    },
    {
      "epoch": 54.29090768457284,
      "grad_norm": 12.01110553741455,
      "learning_rate": 4.757576929522631e-06,
      "loss": 1.8959,
      "step": 1403800
    },
    {
      "epoch": 54.29477510925475,
      "grad_norm": 16.15678596496582,
      "learning_rate": 4.754354075621044e-06,
      "loss": 1.8062,
      "step": 1403900
    },
    {
      "epoch": 54.29864253393665,
      "grad_norm": 11.760627746582031,
      "learning_rate": 4.751131221719457e-06,
      "loss": 1.6824,
      "step": 1404000
    },
    {
      "epoch": 54.30250995861856,
      "grad_norm": 13.410673141479492,
      "learning_rate": 4.74790836781787e-06,
      "loss": 1.7359,
      "step": 1404100
    },
    {
      "epoch": 54.30637738330046,
      "grad_norm": 10.274511337280273,
      "learning_rate": 4.744685513916283e-06,
      "loss": 1.9279,
      "step": 1404200
    },
    {
      "epoch": 54.310244807982365,
      "grad_norm": 10.637713432312012,
      "learning_rate": 4.741462660014697e-06,
      "loss": 1.803,
      "step": 1404300
    },
    {
      "epoch": 54.31411223266427,
      "grad_norm": 10.686493873596191,
      "learning_rate": 4.73823980611311e-06,
      "loss": 1.7837,
      "step": 1404400
    },
    {
      "epoch": 54.31797965734617,
      "grad_norm": 11.253678321838379,
      "learning_rate": 4.735016952211523e-06,
      "loss": 1.6875,
      "step": 1404500
    },
    {
      "epoch": 54.32184708202808,
      "grad_norm": 10.380483627319336,
      "learning_rate": 4.731794098309936e-06,
      "loss": 1.8636,
      "step": 1404600
    },
    {
      "epoch": 54.32571450670998,
      "grad_norm": 11.162810325622559,
      "learning_rate": 4.728571244408349e-06,
      "loss": 1.744,
      "step": 1404700
    },
    {
      "epoch": 54.329581931391886,
      "grad_norm": 12.331608772277832,
      "learning_rate": 4.725348390506762e-06,
      "loss": 1.7428,
      "step": 1404800
    },
    {
      "epoch": 54.33344935607379,
      "grad_norm": 12.784845352172852,
      "learning_rate": 4.722125536605175e-06,
      "loss": 1.6929,
      "step": 1404900
    },
    {
      "epoch": 54.33731678075569,
      "grad_norm": 15.482276916503906,
      "learning_rate": 4.718902682703588e-06,
      "loss": 1.824,
      "step": 1405000
    },
    {
      "epoch": 54.3411842054376,
      "grad_norm": 13.472640037536621,
      "learning_rate": 4.715679828802001e-06,
      "loss": 1.8078,
      "step": 1405100
    },
    {
      "epoch": 54.34505163011951,
      "grad_norm": 10.954291343688965,
      "learning_rate": 4.712456974900414e-06,
      "loss": 1.8432,
      "step": 1405200
    },
    {
      "epoch": 54.34891905480141,
      "grad_norm": 11.59127140045166,
      "learning_rate": 4.709234120998827e-06,
      "loss": 1.8166,
      "step": 1405300
    },
    {
      "epoch": 54.35278647948331,
      "grad_norm": 14.600817680358887,
      "learning_rate": 4.70601126709724e-06,
      "loss": 1.8,
      "step": 1405400
    },
    {
      "epoch": 54.35665390416521,
      "grad_norm": 12.798626899719238,
      "learning_rate": 4.702788413195653e-06,
      "loss": 1.7788,
      "step": 1405500
    },
    {
      "epoch": 54.36052132884712,
      "grad_norm": 10.604743003845215,
      "learning_rate": 4.699565559294066e-06,
      "loss": 1.8323,
      "step": 1405600
    },
    {
      "epoch": 54.36438875352903,
      "grad_norm": 11.13684368133545,
      "learning_rate": 4.696342705392479e-06,
      "loss": 1.7712,
      "step": 1405700
    },
    {
      "epoch": 54.36825617821093,
      "grad_norm": 11.518321990966797,
      "learning_rate": 4.693119851490892e-06,
      "loss": 1.7452,
      "step": 1405800
    },
    {
      "epoch": 54.372123602892835,
      "grad_norm": 13.914968490600586,
      "learning_rate": 4.689896997589305e-06,
      "loss": 1.7035,
      "step": 1405900
    },
    {
      "epoch": 54.375991027574734,
      "grad_norm": 10.162981986999512,
      "learning_rate": 4.686674143687718e-06,
      "loss": 1.7203,
      "step": 1406000
    },
    {
      "epoch": 54.37985845225664,
      "grad_norm": 13.4208402633667,
      "learning_rate": 4.683451289786132e-06,
      "loss": 1.7572,
      "step": 1406100
    },
    {
      "epoch": 54.38372587693855,
      "grad_norm": 13.48395824432373,
      "learning_rate": 4.680228435884545e-06,
      "loss": 1.809,
      "step": 1406200
    },
    {
      "epoch": 54.38759330162045,
      "grad_norm": 12.409425735473633,
      "learning_rate": 4.677005581982958e-06,
      "loss": 1.7749,
      "step": 1406300
    },
    {
      "epoch": 54.391460726302356,
      "grad_norm": 16.076923370361328,
      "learning_rate": 4.673782728081371e-06,
      "loss": 1.6876,
      "step": 1406400
    },
    {
      "epoch": 54.39532815098426,
      "grad_norm": 11.337202072143555,
      "learning_rate": 4.670559874179784e-06,
      "loss": 1.659,
      "step": 1406500
    },
    {
      "epoch": 54.39919557566616,
      "grad_norm": 13.202977180480957,
      "learning_rate": 4.667337020278197e-06,
      "loss": 1.8045,
      "step": 1406600
    },
    {
      "epoch": 54.40306300034807,
      "grad_norm": 13.095671653747559,
      "learning_rate": 4.664114166376609e-06,
      "loss": 1.6286,
      "step": 1406700
    },
    {
      "epoch": 54.40693042502997,
      "grad_norm": 15.318269729614258,
      "learning_rate": 4.660891312475022e-06,
      "loss": 1.7959,
      "step": 1406800
    },
    {
      "epoch": 54.41079784971188,
      "grad_norm": 9.374391555786133,
      "learning_rate": 4.657668458573436e-06,
      "loss": 1.7571,
      "step": 1406900
    },
    {
      "epoch": 54.41466527439378,
      "grad_norm": 9.502456665039062,
      "learning_rate": 4.654445604671849e-06,
      "loss": 1.7057,
      "step": 1407000
    },
    {
      "epoch": 54.41853269907568,
      "grad_norm": 12.791582107543945,
      "learning_rate": 4.651222750770262e-06,
      "loss": 1.7793,
      "step": 1407100
    },
    {
      "epoch": 54.42240012375759,
      "grad_norm": 11.478567123413086,
      "learning_rate": 4.647999896868675e-06,
      "loss": 1.7616,
      "step": 1407200
    },
    {
      "epoch": 54.4262675484395,
      "grad_norm": 12.648459434509277,
      "learning_rate": 4.644777042967088e-06,
      "loss": 1.8592,
      "step": 1407300
    },
    {
      "epoch": 54.4301349731214,
      "grad_norm": 11.778596878051758,
      "learning_rate": 4.641554189065501e-06,
      "loss": 1.8504,
      "step": 1407400
    },
    {
      "epoch": 54.434002397803305,
      "grad_norm": 13.926158905029297,
      "learning_rate": 4.638331335163914e-06,
      "loss": 1.7829,
      "step": 1407500
    },
    {
      "epoch": 54.437869822485204,
      "grad_norm": 13.876916885375977,
      "learning_rate": 4.6351084812623274e-06,
      "loss": 1.704,
      "step": 1407600
    },
    {
      "epoch": 54.44173724716711,
      "grad_norm": 17.425935745239258,
      "learning_rate": 4.6318856273607405e-06,
      "loss": 1.8578,
      "step": 1407700
    },
    {
      "epoch": 54.44560467184902,
      "grad_norm": 12.325448036193848,
      "learning_rate": 4.628662773459154e-06,
      "loss": 1.833,
      "step": 1407800
    },
    {
      "epoch": 54.44947209653092,
      "grad_norm": 12.164250373840332,
      "learning_rate": 4.625439919557567e-06,
      "loss": 1.7958,
      "step": 1407900
    },
    {
      "epoch": 54.453339521212826,
      "grad_norm": 11.689064979553223,
      "learning_rate": 4.62221706565598e-06,
      "loss": 1.7134,
      "step": 1408000
    },
    {
      "epoch": 54.457206945894725,
      "grad_norm": 13.218673706054688,
      "learning_rate": 4.618994211754393e-06,
      "loss": 1.8595,
      "step": 1408100
    },
    {
      "epoch": 54.46107437057663,
      "grad_norm": 11.485237121582031,
      "learning_rate": 4.6157713578528056e-06,
      "loss": 1.8306,
      "step": 1408200
    },
    {
      "epoch": 54.46494179525854,
      "grad_norm": 15.862617492675781,
      "learning_rate": 4.612548503951219e-06,
      "loss": 1.7915,
      "step": 1408300
    },
    {
      "epoch": 54.46880921994044,
      "grad_norm": 10.764699935913086,
      "learning_rate": 4.609325650049632e-06,
      "loss": 1.8286,
      "step": 1408400
    },
    {
      "epoch": 54.47267664462235,
      "grad_norm": 10.656135559082031,
      "learning_rate": 4.606102796148045e-06,
      "loss": 1.8226,
      "step": 1408500
    },
    {
      "epoch": 54.47654406930425,
      "grad_norm": 12.44759750366211,
      "learning_rate": 4.6028799422464585e-06,
      "loss": 1.6805,
      "step": 1408600
    },
    {
      "epoch": 54.48041149398615,
      "grad_norm": 12.586280822753906,
      "learning_rate": 4.5996570883448715e-06,
      "loss": 1.8126,
      "step": 1408700
    },
    {
      "epoch": 54.48427891866806,
      "grad_norm": 17.24155044555664,
      "learning_rate": 4.5964342344432846e-06,
      "loss": 1.8504,
      "step": 1408800
    },
    {
      "epoch": 54.48814634334996,
      "grad_norm": 14.567468643188477,
      "learning_rate": 4.593211380541698e-06,
      "loss": 1.8791,
      "step": 1408900
    },
    {
      "epoch": 54.49201376803187,
      "grad_norm": 14.005151748657227,
      "learning_rate": 4.589988526640111e-06,
      "loss": 1.7836,
      "step": 1409000
    },
    {
      "epoch": 54.495881192713775,
      "grad_norm": 13.445463180541992,
      "learning_rate": 4.586765672738524e-06,
      "loss": 1.7734,
      "step": 1409100
    },
    {
      "epoch": 54.499748617395674,
      "grad_norm": 15.809306144714355,
      "learning_rate": 4.583542818836937e-06,
      "loss": 1.7557,
      "step": 1409200
    },
    {
      "epoch": 54.50361604207758,
      "grad_norm": 14.298934936523438,
      "learning_rate": 4.58031996493535e-06,
      "loss": 1.9162,
      "step": 1409300
    },
    {
      "epoch": 54.50748346675948,
      "grad_norm": 15.765960693359375,
      "learning_rate": 4.577097111033763e-06,
      "loss": 1.8132,
      "step": 1409400
    },
    {
      "epoch": 54.51135089144139,
      "grad_norm": 16.08576011657715,
      "learning_rate": 4.573874257132177e-06,
      "loss": 1.7689,
      "step": 1409500
    },
    {
      "epoch": 54.515218316123295,
      "grad_norm": 15.86562442779541,
      "learning_rate": 4.57065140323059e-06,
      "loss": 1.7623,
      "step": 1409600
    },
    {
      "epoch": 54.519085740805195,
      "grad_norm": 15.927123069763184,
      "learning_rate": 4.567428549329002e-06,
      "loss": 1.7296,
      "step": 1409700
    },
    {
      "epoch": 54.5229531654871,
      "grad_norm": 13.608444213867188,
      "learning_rate": 4.564205695427415e-06,
      "loss": 1.7643,
      "step": 1409800
    },
    {
      "epoch": 54.52682059016901,
      "grad_norm": 9.460407257080078,
      "learning_rate": 4.560982841525828e-06,
      "loss": 1.8083,
      "step": 1409900
    },
    {
      "epoch": 54.53068801485091,
      "grad_norm": 12.980570793151855,
      "learning_rate": 4.557759987624241e-06,
      "loss": 1.736,
      "step": 1410000
    },
    {
      "epoch": 54.53455543953282,
      "grad_norm": 10.95794677734375,
      "learning_rate": 4.554537133722654e-06,
      "loss": 1.7741,
      "step": 1410100
    },
    {
      "epoch": 54.538422864214716,
      "grad_norm": 12.95382022857666,
      "learning_rate": 4.551314279821067e-06,
      "loss": 1.7962,
      "step": 1410200
    },
    {
      "epoch": 54.54229028889662,
      "grad_norm": 11.327653884887695,
      "learning_rate": 4.548091425919481e-06,
      "loss": 1.8734,
      "step": 1410300
    },
    {
      "epoch": 54.54615771357853,
      "grad_norm": 14.35597038269043,
      "learning_rate": 4.544868572017894e-06,
      "loss": 1.6655,
      "step": 1410400
    },
    {
      "epoch": 54.55002513826043,
      "grad_norm": 11.650635719299316,
      "learning_rate": 4.541645718116307e-06,
      "loss": 1.7412,
      "step": 1410500
    },
    {
      "epoch": 54.55389256294234,
      "grad_norm": 12.946575164794922,
      "learning_rate": 4.53842286421472e-06,
      "loss": 1.7693,
      "step": 1410600
    },
    {
      "epoch": 54.557759987624245,
      "grad_norm": 12.36782455444336,
      "learning_rate": 4.535200010313133e-06,
      "loss": 1.7982,
      "step": 1410700
    },
    {
      "epoch": 54.561627412306144,
      "grad_norm": 13.07823371887207,
      "learning_rate": 4.531977156411546e-06,
      "loss": 1.6776,
      "step": 1410800
    },
    {
      "epoch": 54.56549483698805,
      "grad_norm": 10.642216682434082,
      "learning_rate": 4.528754302509959e-06,
      "loss": 1.6742,
      "step": 1410900
    },
    {
      "epoch": 54.56936226166995,
      "grad_norm": 11.033548355102539,
      "learning_rate": 4.525531448608372e-06,
      "loss": 1.7478,
      "step": 1411000
    },
    {
      "epoch": 54.57322968635186,
      "grad_norm": 12.191193580627441,
      "learning_rate": 4.522308594706785e-06,
      "loss": 1.8138,
      "step": 1411100
    },
    {
      "epoch": 54.577097111033765,
      "grad_norm": 9.809986114501953,
      "learning_rate": 4.519085740805198e-06,
      "loss": 1.809,
      "step": 1411200
    },
    {
      "epoch": 54.580964535715665,
      "grad_norm": 14.086215019226074,
      "learning_rate": 4.515862886903611e-06,
      "loss": 1.791,
      "step": 1411300
    },
    {
      "epoch": 54.58483196039757,
      "grad_norm": 11.978190422058105,
      "learning_rate": 4.512640033002024e-06,
      "loss": 1.8597,
      "step": 1411400
    },
    {
      "epoch": 54.58869938507947,
      "grad_norm": 11.984655380249023,
      "learning_rate": 4.509417179100437e-06,
      "loss": 1.7423,
      "step": 1411500
    },
    {
      "epoch": 54.59256680976138,
      "grad_norm": 15.091719627380371,
      "learning_rate": 4.50619432519885e-06,
      "loss": 1.69,
      "step": 1411600
    },
    {
      "epoch": 54.59643423444329,
      "grad_norm": 12.958247184753418,
      "learning_rate": 4.502971471297263e-06,
      "loss": 1.8568,
      "step": 1411700
    },
    {
      "epoch": 54.600301659125186,
      "grad_norm": 14.939659118652344,
      "learning_rate": 4.499748617395676e-06,
      "loss": 1.8407,
      "step": 1411800
    },
    {
      "epoch": 54.60416908380709,
      "grad_norm": 11.503868103027344,
      "learning_rate": 4.496525763494089e-06,
      "loss": 1.7804,
      "step": 1411900
    },
    {
      "epoch": 54.608036508489,
      "grad_norm": 9.975332260131836,
      "learning_rate": 4.493302909592503e-06,
      "loss": 1.7124,
      "step": 1412000
    },
    {
      "epoch": 54.6119039331709,
      "grad_norm": 10.985557556152344,
      "learning_rate": 4.490080055690916e-06,
      "loss": 1.8087,
      "step": 1412100
    },
    {
      "epoch": 54.61577135785281,
      "grad_norm": 11.797245979309082,
      "learning_rate": 4.486857201789329e-06,
      "loss": 1.7969,
      "step": 1412200
    },
    {
      "epoch": 54.61963878253471,
      "grad_norm": 12.05762767791748,
      "learning_rate": 4.483634347887742e-06,
      "loss": 1.8588,
      "step": 1412300
    },
    {
      "epoch": 54.623506207216614,
      "grad_norm": 12.726496696472168,
      "learning_rate": 4.480411493986155e-06,
      "loss": 1.9084,
      "step": 1412400
    },
    {
      "epoch": 54.62737363189852,
      "grad_norm": 8.156194686889648,
      "learning_rate": 4.477188640084568e-06,
      "loss": 1.7216,
      "step": 1412500
    },
    {
      "epoch": 54.63124105658042,
      "grad_norm": 13.530304908752441,
      "learning_rate": 4.47396578618298e-06,
      "loss": 1.8311,
      "step": 1412600
    },
    {
      "epoch": 54.63510848126233,
      "grad_norm": 11.842042922973633,
      "learning_rate": 4.470742932281393e-06,
      "loss": 1.8006,
      "step": 1412700
    },
    {
      "epoch": 54.63897590594423,
      "grad_norm": 12.100564956665039,
      "learning_rate": 4.467520078379807e-06,
      "loss": 1.7152,
      "step": 1412800
    },
    {
      "epoch": 54.642843330626135,
      "grad_norm": 12.054874420166016,
      "learning_rate": 4.46429722447822e-06,
      "loss": 1.7408,
      "step": 1412900
    },
    {
      "epoch": 54.64671075530804,
      "grad_norm": 14.13139820098877,
      "learning_rate": 4.461074370576633e-06,
      "loss": 1.7518,
      "step": 1413000
    },
    {
      "epoch": 54.65057817998994,
      "grad_norm": 14.36378002166748,
      "learning_rate": 4.457851516675046e-06,
      "loss": 1.8515,
      "step": 1413100
    },
    {
      "epoch": 54.65444560467185,
      "grad_norm": 13.22379207611084,
      "learning_rate": 4.454628662773459e-06,
      "loss": 1.7356,
      "step": 1413200
    },
    {
      "epoch": 54.65831302935376,
      "grad_norm": 11.785231590270996,
      "learning_rate": 4.451405808871872e-06,
      "loss": 1.8446,
      "step": 1413300
    },
    {
      "epoch": 54.662180454035656,
      "grad_norm": 14.356005668640137,
      "learning_rate": 4.448182954970285e-06,
      "loss": 1.8002,
      "step": 1413400
    },
    {
      "epoch": 54.66604787871756,
      "grad_norm": 16.354856491088867,
      "learning_rate": 4.444960101068698e-06,
      "loss": 1.7251,
      "step": 1413500
    },
    {
      "epoch": 54.66991530339946,
      "grad_norm": 11.985503196716309,
      "learning_rate": 4.4417372471671114e-06,
      "loss": 1.7152,
      "step": 1413600
    },
    {
      "epoch": 54.67378272808137,
      "grad_norm": 12.842556953430176,
      "learning_rate": 4.4385143932655245e-06,
      "loss": 1.7564,
      "step": 1413700
    },
    {
      "epoch": 54.67765015276328,
      "grad_norm": 12.403594970703125,
      "learning_rate": 4.435291539363938e-06,
      "loss": 1.8267,
      "step": 1413800
    },
    {
      "epoch": 54.68151757744518,
      "grad_norm": 14.922075271606445,
      "learning_rate": 4.432068685462351e-06,
      "loss": 1.7098,
      "step": 1413900
    },
    {
      "epoch": 54.685385002127084,
      "grad_norm": 19.250804901123047,
      "learning_rate": 4.428845831560764e-06,
      "loss": 1.9473,
      "step": 1414000
    },
    {
      "epoch": 54.68925242680899,
      "grad_norm": 8.174882888793945,
      "learning_rate": 4.4256229776591766e-06,
      "loss": 1.7923,
      "step": 1414100
    },
    {
      "epoch": 54.69311985149089,
      "grad_norm": 11.725515365600586,
      "learning_rate": 4.4224001237575896e-06,
      "loss": 1.865,
      "step": 1414200
    },
    {
      "epoch": 54.6969872761728,
      "grad_norm": 11.125381469726562,
      "learning_rate": 4.419177269856003e-06,
      "loss": 1.7452,
      "step": 1414300
    },
    {
      "epoch": 54.7008547008547,
      "grad_norm": 13.226287841796875,
      "learning_rate": 4.415954415954416e-06,
      "loss": 1.7579,
      "step": 1414400
    },
    {
      "epoch": 54.704722125536605,
      "grad_norm": 11.60008716583252,
      "learning_rate": 4.4127315620528295e-06,
      "loss": 1.8372,
      "step": 1414500
    },
    {
      "epoch": 54.70858955021851,
      "grad_norm": 14.95693302154541,
      "learning_rate": 4.4095087081512425e-06,
      "loss": 1.7364,
      "step": 1414600
    },
    {
      "epoch": 54.71245697490041,
      "grad_norm": 13.726776123046875,
      "learning_rate": 4.4062858542496555e-06,
      "loss": 1.802,
      "step": 1414700
    },
    {
      "epoch": 54.71632439958232,
      "grad_norm": 12.590795516967773,
      "learning_rate": 4.4030630003480686e-06,
      "loss": 1.8011,
      "step": 1414800
    },
    {
      "epoch": 54.72019182426422,
      "grad_norm": 15.451034545898438,
      "learning_rate": 4.399840146446482e-06,
      "loss": 1.8036,
      "step": 1414900
    },
    {
      "epoch": 54.724059248946126,
      "grad_norm": 15.414961814880371,
      "learning_rate": 4.396617292544895e-06,
      "loss": 1.7185,
      "step": 1415000
    },
    {
      "epoch": 54.72792667362803,
      "grad_norm": 15.149752616882324,
      "learning_rate": 4.393394438643308e-06,
      "loss": 1.7571,
      "step": 1415100
    },
    {
      "epoch": 54.73179409830993,
      "grad_norm": 13.398747444152832,
      "learning_rate": 4.390171584741721e-06,
      "loss": 1.8464,
      "step": 1415200
    },
    {
      "epoch": 54.73566152299184,
      "grad_norm": 10.14754867553711,
      "learning_rate": 4.386948730840134e-06,
      "loss": 1.8063,
      "step": 1415300
    },
    {
      "epoch": 54.73952894767375,
      "grad_norm": 9.263981819152832,
      "learning_rate": 4.383725876938547e-06,
      "loss": 1.7624,
      "step": 1415400
    },
    {
      "epoch": 54.74339637235565,
      "grad_norm": 12.717326164245605,
      "learning_rate": 4.380503023036961e-06,
      "loss": 1.6825,
      "step": 1415500
    },
    {
      "epoch": 54.747263797037554,
      "grad_norm": 11.950921058654785,
      "learning_rate": 4.377280169135373e-06,
      "loss": 1.8182,
      "step": 1415600
    },
    {
      "epoch": 54.751131221719454,
      "grad_norm": 12.750771522521973,
      "learning_rate": 4.374057315233786e-06,
      "loss": 1.8287,
      "step": 1415700
    },
    {
      "epoch": 54.75499864640136,
      "grad_norm": 13.783185005187988,
      "learning_rate": 4.370834461332199e-06,
      "loss": 1.755,
      "step": 1415800
    },
    {
      "epoch": 54.75886607108327,
      "grad_norm": 13.95386028289795,
      "learning_rate": 4.367611607430612e-06,
      "loss": 1.7604,
      "step": 1415900
    },
    {
      "epoch": 54.76273349576517,
      "grad_norm": 15.151637077331543,
      "learning_rate": 4.364388753529025e-06,
      "loss": 1.7271,
      "step": 1416000
    },
    {
      "epoch": 54.766600920447075,
      "grad_norm": 12.542795181274414,
      "learning_rate": 4.361165899627438e-06,
      "loss": 1.8175,
      "step": 1416100
    },
    {
      "epoch": 54.770468345128975,
      "grad_norm": 13.528152465820312,
      "learning_rate": 4.357943045725851e-06,
      "loss": 1.7516,
      "step": 1416200
    },
    {
      "epoch": 54.77433576981088,
      "grad_norm": 13.730961799621582,
      "learning_rate": 4.354720191824265e-06,
      "loss": 1.8398,
      "step": 1416300
    },
    {
      "epoch": 54.77820319449279,
      "grad_norm": 11.280029296875,
      "learning_rate": 4.351497337922678e-06,
      "loss": 1.7761,
      "step": 1416400
    },
    {
      "epoch": 54.78207061917469,
      "grad_norm": 15.837459564208984,
      "learning_rate": 4.348274484021091e-06,
      "loss": 1.7283,
      "step": 1416500
    },
    {
      "epoch": 54.785938043856596,
      "grad_norm": 13.507426261901855,
      "learning_rate": 4.345051630119504e-06,
      "loss": 1.7967,
      "step": 1416600
    },
    {
      "epoch": 54.7898054685385,
      "grad_norm": 11.258049011230469,
      "learning_rate": 4.341828776217917e-06,
      "loss": 1.7695,
      "step": 1416700
    },
    {
      "epoch": 54.7936728932204,
      "grad_norm": 12.06662654876709,
      "learning_rate": 4.33860592231633e-06,
      "loss": 1.6933,
      "step": 1416800
    },
    {
      "epoch": 54.79754031790231,
      "grad_norm": 8.121549606323242,
      "learning_rate": 4.335383068414743e-06,
      "loss": 1.7227,
      "step": 1416900
    },
    {
      "epoch": 54.80140774258421,
      "grad_norm": 9.887807846069336,
      "learning_rate": 4.332160214513156e-06,
      "loss": 1.8514,
      "step": 1417000
    },
    {
      "epoch": 54.80527516726612,
      "grad_norm": 13.301876068115234,
      "learning_rate": 4.328937360611569e-06,
      "loss": 1.8137,
      "step": 1417100
    },
    {
      "epoch": 54.809142591948024,
      "grad_norm": 14.436902046203613,
      "learning_rate": 4.325714506709982e-06,
      "loss": 1.8042,
      "step": 1417200
    },
    {
      "epoch": 54.813010016629924,
      "grad_norm": 11.489089012145996,
      "learning_rate": 4.322491652808395e-06,
      "loss": 1.7878,
      "step": 1417300
    },
    {
      "epoch": 54.81687744131183,
      "grad_norm": 16.127687454223633,
      "learning_rate": 4.319268798906808e-06,
      "loss": 1.8249,
      "step": 1417400
    },
    {
      "epoch": 54.82074486599373,
      "grad_norm": 14.822139739990234,
      "learning_rate": 4.316045945005221e-06,
      "loss": 1.7727,
      "step": 1417500
    },
    {
      "epoch": 54.82461229067564,
      "grad_norm": 13.568656921386719,
      "learning_rate": 4.312823091103634e-06,
      "loss": 1.8302,
      "step": 1417600
    },
    {
      "epoch": 54.828479715357545,
      "grad_norm": 10.879884719848633,
      "learning_rate": 4.309600237202047e-06,
      "loss": 1.7017,
      "step": 1417700
    },
    {
      "epoch": 54.832347140039445,
      "grad_norm": 14.64413070678711,
      "learning_rate": 4.30637738330046e-06,
      "loss": 1.8315,
      "step": 1417800
    },
    {
      "epoch": 54.83621456472135,
      "grad_norm": 14.368454933166504,
      "learning_rate": 4.303154529398873e-06,
      "loss": 1.7315,
      "step": 1417900
    },
    {
      "epoch": 54.84008198940326,
      "grad_norm": 11.897266387939453,
      "learning_rate": 4.299931675497287e-06,
      "loss": 1.7472,
      "step": 1418000
    },
    {
      "epoch": 54.84394941408516,
      "grad_norm": 15.203856468200684,
      "learning_rate": 4.2967088215957e-06,
      "loss": 1.8556,
      "step": 1418100
    },
    {
      "epoch": 54.847816838767066,
      "grad_norm": 12.080085754394531,
      "learning_rate": 4.293485967694113e-06,
      "loss": 1.8138,
      "step": 1418200
    },
    {
      "epoch": 54.851684263448966,
      "grad_norm": 16.019359588623047,
      "learning_rate": 4.290263113792526e-06,
      "loss": 1.7387,
      "step": 1418300
    },
    {
      "epoch": 54.85555168813087,
      "grad_norm": 12.784144401550293,
      "learning_rate": 4.287040259890939e-06,
      "loss": 1.7378,
      "step": 1418400
    },
    {
      "epoch": 54.85941911281278,
      "grad_norm": 10.352079391479492,
      "learning_rate": 4.283817405989351e-06,
      "loss": 1.6422,
      "step": 1418500
    },
    {
      "epoch": 54.86328653749468,
      "grad_norm": 10.743005752563477,
      "learning_rate": 4.280594552087764e-06,
      "loss": 1.6779,
      "step": 1418600
    },
    {
      "epoch": 54.86715396217659,
      "grad_norm": 13.230985641479492,
      "learning_rate": 4.277371698186177e-06,
      "loss": 1.7448,
      "step": 1418700
    },
    {
      "epoch": 54.871021386858494,
      "grad_norm": 15.26689624786377,
      "learning_rate": 4.274148844284591e-06,
      "loss": 1.8545,
      "step": 1418800
    },
    {
      "epoch": 54.874888811540394,
      "grad_norm": 14.598072052001953,
      "learning_rate": 4.270925990383004e-06,
      "loss": 1.8442,
      "step": 1418900
    },
    {
      "epoch": 54.8787562362223,
      "grad_norm": 14.529189109802246,
      "learning_rate": 4.267703136481417e-06,
      "loss": 1.7536,
      "step": 1419000
    },
    {
      "epoch": 54.8826236609042,
      "grad_norm": 11.386598587036133,
      "learning_rate": 4.26448028257983e-06,
      "loss": 1.7651,
      "step": 1419100
    },
    {
      "epoch": 54.88649108558611,
      "grad_norm": 15.082233428955078,
      "learning_rate": 4.261257428678243e-06,
      "loss": 1.6781,
      "step": 1419200
    },
    {
      "epoch": 54.890358510268015,
      "grad_norm": 15.247974395751953,
      "learning_rate": 4.258034574776656e-06,
      "loss": 1.7674,
      "step": 1419300
    },
    {
      "epoch": 54.894225934949915,
      "grad_norm": 11.60420036315918,
      "learning_rate": 4.254811720875069e-06,
      "loss": 1.8153,
      "step": 1419400
    },
    {
      "epoch": 54.89809335963182,
      "grad_norm": 13.939146041870117,
      "learning_rate": 4.251588866973482e-06,
      "loss": 1.7788,
      "step": 1419500
    },
    {
      "epoch": 54.90196078431372,
      "grad_norm": 14.983181953430176,
      "learning_rate": 4.2483660130718954e-06,
      "loss": 1.7858,
      "step": 1419600
    },
    {
      "epoch": 54.90582820899563,
      "grad_norm": 10.344452857971191,
      "learning_rate": 4.245143159170309e-06,
      "loss": 1.7547,
      "step": 1419700
    },
    {
      "epoch": 54.909695633677536,
      "grad_norm": 10.86508846282959,
      "learning_rate": 4.241920305268722e-06,
      "loss": 1.7838,
      "step": 1419800
    },
    {
      "epoch": 54.913563058359436,
      "grad_norm": 12.076394081115723,
      "learning_rate": 4.238697451367135e-06,
      "loss": 1.7196,
      "step": 1419900
    },
    {
      "epoch": 54.91743048304134,
      "grad_norm": 14.42575454711914,
      "learning_rate": 4.2354745974655475e-06,
      "loss": 1.7778,
      "step": 1420000
    },
    {
      "epoch": 54.92129790772325,
      "grad_norm": 11.237584114074707,
      "learning_rate": 4.2322517435639606e-06,
      "loss": 1.8524,
      "step": 1420100
    },
    {
      "epoch": 54.92516533240515,
      "grad_norm": 8.720843315124512,
      "learning_rate": 4.2290288896623736e-06,
      "loss": 1.8232,
      "step": 1420200
    },
    {
      "epoch": 54.92903275708706,
      "grad_norm": 13.033068656921387,
      "learning_rate": 4.225806035760787e-06,
      "loss": 1.8486,
      "step": 1420300
    },
    {
      "epoch": 54.93290018176896,
      "grad_norm": 14.77588939666748,
      "learning_rate": 4.2225831818592e-06,
      "loss": 1.8446,
      "step": 1420400
    },
    {
      "epoch": 54.936767606450864,
      "grad_norm": 15.954723358154297,
      "learning_rate": 4.2193603279576135e-06,
      "loss": 1.7608,
      "step": 1420500
    },
    {
      "epoch": 54.94063503113277,
      "grad_norm": 14.397113800048828,
      "learning_rate": 4.2161374740560265e-06,
      "loss": 1.7844,
      "step": 1420600
    },
    {
      "epoch": 54.94450245581467,
      "grad_norm": 13.011245727539062,
      "learning_rate": 4.2129146201544395e-06,
      "loss": 1.7667,
      "step": 1420700
    },
    {
      "epoch": 54.94836988049658,
      "grad_norm": 14.52824878692627,
      "learning_rate": 4.2096917662528526e-06,
      "loss": 1.8341,
      "step": 1420800
    },
    {
      "epoch": 54.95223730517848,
      "grad_norm": 12.181467056274414,
      "learning_rate": 4.206468912351266e-06,
      "loss": 1.9,
      "step": 1420900
    },
    {
      "epoch": 54.956104729860385,
      "grad_norm": 12.473506927490234,
      "learning_rate": 4.203246058449679e-06,
      "loss": 1.821,
      "step": 1421000
    },
    {
      "epoch": 54.95997215454229,
      "grad_norm": 13.639416694641113,
      "learning_rate": 4.200023204548092e-06,
      "loss": 1.8162,
      "step": 1421100
    },
    {
      "epoch": 54.96383957922419,
      "grad_norm": 11.261338233947754,
      "learning_rate": 4.196800350646505e-06,
      "loss": 1.7578,
      "step": 1421200
    },
    {
      "epoch": 54.9677070039061,
      "grad_norm": 13.74452018737793,
      "learning_rate": 4.193577496744918e-06,
      "loss": 1.829,
      "step": 1421300
    },
    {
      "epoch": 54.971574428588006,
      "grad_norm": 17.207735061645508,
      "learning_rate": 4.1903546428433316e-06,
      "loss": 1.7723,
      "step": 1421400
    },
    {
      "epoch": 54.975441853269906,
      "grad_norm": 13.174012184143066,
      "learning_rate": 4.187131788941744e-06,
      "loss": 1.7777,
      "step": 1421500
    },
    {
      "epoch": 54.97930927795181,
      "grad_norm": 14.458282470703125,
      "learning_rate": 4.183908935040157e-06,
      "loss": 1.7659,
      "step": 1421600
    },
    {
      "epoch": 54.98317670263371,
      "grad_norm": 13.270001411437988,
      "learning_rate": 4.18068608113857e-06,
      "loss": 1.7913,
      "step": 1421700
    },
    {
      "epoch": 54.98704412731562,
      "grad_norm": 11.955275535583496,
      "learning_rate": 4.177463227236983e-06,
      "loss": 1.762,
      "step": 1421800
    },
    {
      "epoch": 54.99091155199753,
      "grad_norm": 10.212636947631836,
      "learning_rate": 4.174240373335396e-06,
      "loss": 1.7931,
      "step": 1421900
    },
    {
      "epoch": 54.99477897667943,
      "grad_norm": 11.139805793762207,
      "learning_rate": 4.171017519433809e-06,
      "loss": 1.7226,
      "step": 1422000
    },
    {
      "epoch": 54.998646401361334,
      "grad_norm": 9.864340782165527,
      "learning_rate": 4.167794665532222e-06,
      "loss": 1.7236,
      "step": 1422100
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.739187479019165,
      "eval_runtime": 3.0956,
      "eval_samples_per_second": 439.651,
      "eval_steps_per_second": 439.651,
      "step": 1422135
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.5707447528839111,
      "eval_runtime": 55.496,
      "eval_samples_per_second": 465.926,
      "eval_steps_per_second": 465.926,
      "step": 1422135
    },
    {
      "epoch": 55.00251382604324,
      "grad_norm": 13.858477592468262,
      "learning_rate": 4.164571811630636e-06,
      "loss": 1.7842,
      "step": 1422200
    },
    {
      "epoch": 55.00638125072514,
      "grad_norm": 11.1722412109375,
      "learning_rate": 4.161348957729049e-06,
      "loss": 1.6841,
      "step": 1422300
    },
    {
      "epoch": 55.01024867540705,
      "grad_norm": 10.682098388671875,
      "learning_rate": 4.158126103827462e-06,
      "loss": 1.9194,
      "step": 1422400
    },
    {
      "epoch": 55.01411610008895,
      "grad_norm": 14.384720802307129,
      "learning_rate": 4.154903249925875e-06,
      "loss": 1.7732,
      "step": 1422500
    },
    {
      "epoch": 55.017983524770855,
      "grad_norm": 12.418960571289062,
      "learning_rate": 4.151680396024288e-06,
      "loss": 1.6974,
      "step": 1422600
    },
    {
      "epoch": 55.02185094945276,
      "grad_norm": 12.692893981933594,
      "learning_rate": 4.148457542122701e-06,
      "loss": 1.7719,
      "step": 1422700
    },
    {
      "epoch": 55.02571837413466,
      "grad_norm": 13.339338302612305,
      "learning_rate": 4.145234688221114e-06,
      "loss": 1.6861,
      "step": 1422800
    },
    {
      "epoch": 55.02958579881657,
      "grad_norm": 12.559502601623535,
      "learning_rate": 4.142011834319527e-06,
      "loss": 1.7704,
      "step": 1422900
    },
    {
      "epoch": 55.03345322349847,
      "grad_norm": 14.99484920501709,
      "learning_rate": 4.13878898041794e-06,
      "loss": 1.7712,
      "step": 1423000
    },
    {
      "epoch": 55.037320648180376,
      "grad_norm": 12.23832893371582,
      "learning_rate": 4.135566126516353e-06,
      "loss": 1.755,
      "step": 1423100
    },
    {
      "epoch": 55.04118807286228,
      "grad_norm": 15.20236873626709,
      "learning_rate": 4.132343272614766e-06,
      "loss": 1.7375,
      "step": 1423200
    },
    {
      "epoch": 55.04505549754418,
      "grad_norm": 13.092268943786621,
      "learning_rate": 4.129120418713179e-06,
      "loss": 1.7225,
      "step": 1423300
    },
    {
      "epoch": 55.04892292222609,
      "grad_norm": 11.152942657470703,
      "learning_rate": 4.125897564811592e-06,
      "loss": 1.7757,
      "step": 1423400
    },
    {
      "epoch": 55.052790346908,
      "grad_norm": 12.235550880432129,
      "learning_rate": 4.122674710910005e-06,
      "loss": 1.7976,
      "step": 1423500
    },
    {
      "epoch": 55.0566577715899,
      "grad_norm": 12.293359756469727,
      "learning_rate": 4.119451857008418e-06,
      "loss": 1.815,
      "step": 1423600
    },
    {
      "epoch": 55.060525196271804,
      "grad_norm": 14.34181022644043,
      "learning_rate": 4.116229003106831e-06,
      "loss": 1.7078,
      "step": 1423700
    },
    {
      "epoch": 55.064392620953704,
      "grad_norm": 12.847647666931152,
      "learning_rate": 4.113006149205244e-06,
      "loss": 1.7405,
      "step": 1423800
    },
    {
      "epoch": 55.06826004563561,
      "grad_norm": 11.06412410736084,
      "learning_rate": 4.109783295303658e-06,
      "loss": 1.6966,
      "step": 1423900
    },
    {
      "epoch": 55.07212747031752,
      "grad_norm": 13.009048461914062,
      "learning_rate": 4.106560441402071e-06,
      "loss": 1.8234,
      "step": 1424000
    },
    {
      "epoch": 55.07599489499942,
      "grad_norm": 14.812175750732422,
      "learning_rate": 4.103337587500484e-06,
      "loss": 1.8107,
      "step": 1424100
    },
    {
      "epoch": 55.079862319681325,
      "grad_norm": 14.33890438079834,
      "learning_rate": 4.100114733598897e-06,
      "loss": 1.7343,
      "step": 1424200
    },
    {
      "epoch": 55.083729744363225,
      "grad_norm": 17.182392120361328,
      "learning_rate": 4.09689187969731e-06,
      "loss": 1.8114,
      "step": 1424300
    },
    {
      "epoch": 55.08759716904513,
      "grad_norm": 9.499516487121582,
      "learning_rate": 4.093669025795722e-06,
      "loss": 1.7211,
      "step": 1424400
    },
    {
      "epoch": 55.09146459372704,
      "grad_norm": 10.805214881896973,
      "learning_rate": 4.090446171894135e-06,
      "loss": 1.7524,
      "step": 1424500
    },
    {
      "epoch": 55.09533201840894,
      "grad_norm": 12.459772109985352,
      "learning_rate": 4.087223317992548e-06,
      "loss": 1.7496,
      "step": 1424600
    },
    {
      "epoch": 55.099199443090846,
      "grad_norm": 11.202189445495605,
      "learning_rate": 4.084000464090962e-06,
      "loss": 1.6881,
      "step": 1424700
    },
    {
      "epoch": 55.10306686777275,
      "grad_norm": 9.620516777038574,
      "learning_rate": 4.080777610189375e-06,
      "loss": 1.6918,
      "step": 1424800
    },
    {
      "epoch": 55.10693429245465,
      "grad_norm": 7.995291233062744,
      "learning_rate": 4.077554756287788e-06,
      "loss": 1.9439,
      "step": 1424900
    },
    {
      "epoch": 55.11080171713656,
      "grad_norm": 12.39417839050293,
      "learning_rate": 4.074331902386201e-06,
      "loss": 1.7728,
      "step": 1425000
    },
    {
      "epoch": 55.11466914181846,
      "grad_norm": 14.525860786437988,
      "learning_rate": 4.071109048484614e-06,
      "loss": 1.7498,
      "step": 1425100
    },
    {
      "epoch": 55.11853656650037,
      "grad_norm": 14.847433090209961,
      "learning_rate": 4.067886194583027e-06,
      "loss": 1.7664,
      "step": 1425200
    },
    {
      "epoch": 55.122403991182274,
      "grad_norm": 14.286408424377441,
      "learning_rate": 4.06466334068144e-06,
      "loss": 1.8046,
      "step": 1425300
    },
    {
      "epoch": 55.126271415864174,
      "grad_norm": 15.272554397583008,
      "learning_rate": 4.061440486779853e-06,
      "loss": 1.736,
      "step": 1425400
    },
    {
      "epoch": 55.13013884054608,
      "grad_norm": 15.41723918914795,
      "learning_rate": 4.058217632878266e-06,
      "loss": 1.8796,
      "step": 1425500
    },
    {
      "epoch": 55.13400626522798,
      "grad_norm": 13.879326820373535,
      "learning_rate": 4.0549947789766794e-06,
      "loss": 1.7542,
      "step": 1425600
    },
    {
      "epoch": 55.13787368990989,
      "grad_norm": 13.075044631958008,
      "learning_rate": 4.051771925075093e-06,
      "loss": 1.8591,
      "step": 1425700
    },
    {
      "epoch": 55.141741114591795,
      "grad_norm": 10.521455764770508,
      "learning_rate": 4.048549071173506e-06,
      "loss": 1.7129,
      "step": 1425800
    },
    {
      "epoch": 55.145608539273695,
      "grad_norm": 12.550639152526855,
      "learning_rate": 4.0453262172719185e-06,
      "loss": 1.6986,
      "step": 1425900
    },
    {
      "epoch": 55.1494759639556,
      "grad_norm": 13.556646347045898,
      "learning_rate": 4.0421033633703315e-06,
      "loss": 1.7316,
      "step": 1426000
    },
    {
      "epoch": 55.15334338863751,
      "grad_norm": 8.33425235748291,
      "learning_rate": 4.0388805094687446e-06,
      "loss": 1.6432,
      "step": 1426100
    },
    {
      "epoch": 55.15721081331941,
      "grad_norm": 15.034587860107422,
      "learning_rate": 4.0356576555671576e-06,
      "loss": 1.7808,
      "step": 1426200
    },
    {
      "epoch": 55.161078238001316,
      "grad_norm": 14.080573081970215,
      "learning_rate": 4.032434801665571e-06,
      "loss": 1.8256,
      "step": 1426300
    },
    {
      "epoch": 55.164945662683216,
      "grad_norm": 14.523470878601074,
      "learning_rate": 4.0292119477639845e-06,
      "loss": 1.8331,
      "step": 1426400
    },
    {
      "epoch": 55.16881308736512,
      "grad_norm": 12.448246955871582,
      "learning_rate": 4.0259890938623975e-06,
      "loss": 1.8098,
      "step": 1426500
    },
    {
      "epoch": 55.17268051204703,
      "grad_norm": 11.381952285766602,
      "learning_rate": 4.0227662399608105e-06,
      "loss": 1.7877,
      "step": 1426600
    },
    {
      "epoch": 55.17654793672893,
      "grad_norm": 22.961997985839844,
      "learning_rate": 4.0195433860592235e-06,
      "loss": 1.7579,
      "step": 1426700
    },
    {
      "epoch": 55.18041536141084,
      "grad_norm": 15.405261993408203,
      "learning_rate": 4.0163205321576366e-06,
      "loss": 1.8363,
      "step": 1426800
    },
    {
      "epoch": 55.184282786092744,
      "grad_norm": 13.200830459594727,
      "learning_rate": 4.01309767825605e-06,
      "loss": 1.8503,
      "step": 1426900
    },
    {
      "epoch": 55.188150210774644,
      "grad_norm": 14.042726516723633,
      "learning_rate": 4.009874824354463e-06,
      "loss": 1.7315,
      "step": 1427000
    },
    {
      "epoch": 55.19201763545655,
      "grad_norm": 16.863971710205078,
      "learning_rate": 4.006651970452876e-06,
      "loss": 1.6986,
      "step": 1427100
    },
    {
      "epoch": 55.19588506013845,
      "grad_norm": 18.587133407592773,
      "learning_rate": 4.003429116551289e-06,
      "loss": 1.7081,
      "step": 1427200
    },
    {
      "epoch": 55.19975248482036,
      "grad_norm": 11.545083999633789,
      "learning_rate": 4.000206262649702e-06,
      "loss": 1.8097,
      "step": 1427300
    },
    {
      "epoch": 55.203619909502265,
      "grad_norm": 14.25879192352295,
      "learning_rate": 3.996983408748115e-06,
      "loss": 1.7501,
      "step": 1427400
    },
    {
      "epoch": 55.207487334184165,
      "grad_norm": 13.182817459106445,
      "learning_rate": 3.993760554846528e-06,
      "loss": 1.788,
      "step": 1427500
    },
    {
      "epoch": 55.21135475886607,
      "grad_norm": 13.909143447875977,
      "learning_rate": 3.990537700944941e-06,
      "loss": 1.7626,
      "step": 1427600
    },
    {
      "epoch": 55.21522218354797,
      "grad_norm": 15.269001007080078,
      "learning_rate": 3.987314847043354e-06,
      "loss": 1.8155,
      "step": 1427700
    },
    {
      "epoch": 55.21908960822988,
      "grad_norm": 13.919281959533691,
      "learning_rate": 3.984091993141767e-06,
      "loss": 1.7605,
      "step": 1427800
    },
    {
      "epoch": 55.222957032911786,
      "grad_norm": 10.370887756347656,
      "learning_rate": 3.98086913924018e-06,
      "loss": 1.8155,
      "step": 1427900
    },
    {
      "epoch": 55.226824457593686,
      "grad_norm": 11.92661190032959,
      "learning_rate": 3.977646285338593e-06,
      "loss": 1.7276,
      "step": 1428000
    },
    {
      "epoch": 55.23069188227559,
      "grad_norm": 13.56597900390625,
      "learning_rate": 3.974423431437006e-06,
      "loss": 1.8013,
      "step": 1428100
    },
    {
      "epoch": 55.2345593069575,
      "grad_norm": 13.08871078491211,
      "learning_rate": 3.97120057753542e-06,
      "loss": 1.7301,
      "step": 1428200
    },
    {
      "epoch": 55.2384267316394,
      "grad_norm": 8.569748878479004,
      "learning_rate": 3.967977723633833e-06,
      "loss": 1.738,
      "step": 1428300
    },
    {
      "epoch": 55.24229415632131,
      "grad_norm": 8.131597518920898,
      "learning_rate": 3.964754869732246e-06,
      "loss": 1.7821,
      "step": 1428400
    },
    {
      "epoch": 55.24616158100321,
      "grad_norm": 13.355365753173828,
      "learning_rate": 3.961532015830659e-06,
      "loss": 1.747,
      "step": 1428500
    },
    {
      "epoch": 55.250029005685114,
      "grad_norm": 13.637120246887207,
      "learning_rate": 3.958309161929072e-06,
      "loss": 1.8037,
      "step": 1428600
    },
    {
      "epoch": 55.25389643036702,
      "grad_norm": 13.396644592285156,
      "learning_rate": 3.955086308027485e-06,
      "loss": 1.7808,
      "step": 1428700
    },
    {
      "epoch": 55.25776385504892,
      "grad_norm": 13.089454650878906,
      "learning_rate": 3.951863454125898e-06,
      "loss": 1.7097,
      "step": 1428800
    },
    {
      "epoch": 55.26163127973083,
      "grad_norm": 8.666756629943848,
      "learning_rate": 3.948640600224311e-06,
      "loss": 1.7891,
      "step": 1428900
    },
    {
      "epoch": 55.26549870441273,
      "grad_norm": 14.739692687988281,
      "learning_rate": 3.945417746322724e-06,
      "loss": 1.8188,
      "step": 1429000
    },
    {
      "epoch": 55.269366129094635,
      "grad_norm": 10.74720573425293,
      "learning_rate": 3.942194892421137e-06,
      "loss": 1.7586,
      "step": 1429100
    },
    {
      "epoch": 55.27323355377654,
      "grad_norm": 12.411530494689941,
      "learning_rate": 3.93897203851955e-06,
      "loss": 1.7693,
      "step": 1429200
    },
    {
      "epoch": 55.27710097845844,
      "grad_norm": 14.25224781036377,
      "learning_rate": 3.935749184617963e-06,
      "loss": 1.78,
      "step": 1429300
    },
    {
      "epoch": 55.28096840314035,
      "grad_norm": 13.890857696533203,
      "learning_rate": 3.932526330716376e-06,
      "loss": 1.7444,
      "step": 1429400
    },
    {
      "epoch": 55.284835827822256,
      "grad_norm": 12.348762512207031,
      "learning_rate": 3.929303476814789e-06,
      "loss": 1.7228,
      "step": 1429500
    },
    {
      "epoch": 55.288703252504156,
      "grad_norm": 13.448269844055176,
      "learning_rate": 3.926080622913202e-06,
      "loss": 1.7188,
      "step": 1429600
    },
    {
      "epoch": 55.29257067718606,
      "grad_norm": 9.3946533203125,
      "learning_rate": 3.922857769011615e-06,
      "loss": 1.7329,
      "step": 1429700
    },
    {
      "epoch": 55.29643810186796,
      "grad_norm": 12.233698844909668,
      "learning_rate": 3.919634915110028e-06,
      "loss": 1.8366,
      "step": 1429800
    },
    {
      "epoch": 55.30030552654987,
      "grad_norm": 14.653141021728516,
      "learning_rate": 3.916412061208442e-06,
      "loss": 1.7145,
      "step": 1429900
    },
    {
      "epoch": 55.30417295123178,
      "grad_norm": 15.713617324829102,
      "learning_rate": 3.913189207306855e-06,
      "loss": 1.7897,
      "step": 1430000
    },
    {
      "epoch": 55.30804037591368,
      "grad_norm": 12.099745750427246,
      "learning_rate": 3.909966353405268e-06,
      "loss": 1.7707,
      "step": 1430100
    },
    {
      "epoch": 55.311907800595584,
      "grad_norm": 11.846991539001465,
      "learning_rate": 3.906743499503681e-06,
      "loss": 1.7975,
      "step": 1430200
    },
    {
      "epoch": 55.31577522527749,
      "grad_norm": 15.528754234313965,
      "learning_rate": 3.903520645602093e-06,
      "loss": 1.7564,
      "step": 1430300
    },
    {
      "epoch": 55.31964264995939,
      "grad_norm": 16.036211013793945,
      "learning_rate": 3.900297791700506e-06,
      "loss": 1.7778,
      "step": 1430400
    },
    {
      "epoch": 55.3235100746413,
      "grad_norm": 15.891718864440918,
      "learning_rate": 3.897074937798919e-06,
      "loss": 1.7568,
      "step": 1430500
    },
    {
      "epoch": 55.3273774993232,
      "grad_norm": 17.390918731689453,
      "learning_rate": 3.893852083897332e-06,
      "loss": 1.8049,
      "step": 1430600
    },
    {
      "epoch": 55.331244924005105,
      "grad_norm": 9.955157279968262,
      "learning_rate": 3.890629229995746e-06,
      "loss": 1.7607,
      "step": 1430700
    },
    {
      "epoch": 55.33511234868701,
      "grad_norm": 14.38245677947998,
      "learning_rate": 3.887406376094159e-06,
      "loss": 1.7779,
      "step": 1430800
    },
    {
      "epoch": 55.33897977336891,
      "grad_norm": 13.304428100585938,
      "learning_rate": 3.884183522192572e-06,
      "loss": 1.756,
      "step": 1430900
    },
    {
      "epoch": 55.34284719805082,
      "grad_norm": 13.65335464477539,
      "learning_rate": 3.880960668290985e-06,
      "loss": 1.834,
      "step": 1431000
    },
    {
      "epoch": 55.34671462273272,
      "grad_norm": 12.428483963012695,
      "learning_rate": 3.877737814389398e-06,
      "loss": 1.7841,
      "step": 1431100
    },
    {
      "epoch": 55.350582047414626,
      "grad_norm": 12.250187873840332,
      "learning_rate": 3.874514960487811e-06,
      "loss": 1.7293,
      "step": 1431200
    },
    {
      "epoch": 55.35444947209653,
      "grad_norm": 12.370405197143555,
      "learning_rate": 3.871292106586224e-06,
      "loss": 1.7906,
      "step": 1431300
    },
    {
      "epoch": 55.35831689677843,
      "grad_norm": 15.348516464233398,
      "learning_rate": 3.868069252684637e-06,
      "loss": 1.7226,
      "step": 1431400
    },
    {
      "epoch": 55.36218432146034,
      "grad_norm": 10.304415702819824,
      "learning_rate": 3.86484639878305e-06,
      "loss": 1.8533,
      "step": 1431500
    },
    {
      "epoch": 55.36605174614225,
      "grad_norm": 13.444938659667969,
      "learning_rate": 3.861623544881464e-06,
      "loss": 1.8154,
      "step": 1431600
    },
    {
      "epoch": 55.36991917082415,
      "grad_norm": 7.778769493103027,
      "learning_rate": 3.858400690979877e-06,
      "loss": 1.7094,
      "step": 1431700
    },
    {
      "epoch": 55.373786595506054,
      "grad_norm": 12.694169044494629,
      "learning_rate": 3.8551778370782895e-06,
      "loss": 1.7823,
      "step": 1431800
    },
    {
      "epoch": 55.377654020187954,
      "grad_norm": 8.837174415588379,
      "learning_rate": 3.8519549831767025e-06,
      "loss": 1.7002,
      "step": 1431900
    },
    {
      "epoch": 55.38152144486986,
      "grad_norm": 15.468545913696289,
      "learning_rate": 3.8487321292751155e-06,
      "loss": 1.7753,
      "step": 1432000
    },
    {
      "epoch": 55.38538886955177,
      "grad_norm": 15.388697624206543,
      "learning_rate": 3.8455092753735286e-06,
      "loss": 1.8264,
      "step": 1432100
    },
    {
      "epoch": 55.38925629423367,
      "grad_norm": 16.27302360534668,
      "learning_rate": 3.8422864214719416e-06,
      "loss": 1.644,
      "step": 1432200
    },
    {
      "epoch": 55.393123718915575,
      "grad_norm": 10.524087905883789,
      "learning_rate": 3.839063567570355e-06,
      "loss": 1.7406,
      "step": 1432300
    },
    {
      "epoch": 55.396991143597475,
      "grad_norm": 15.614351272583008,
      "learning_rate": 3.8358407136687685e-06,
      "loss": 1.8029,
      "step": 1432400
    },
    {
      "epoch": 55.40085856827938,
      "grad_norm": 10.473650932312012,
      "learning_rate": 3.8326178597671815e-06,
      "loss": 1.7037,
      "step": 1432500
    },
    {
      "epoch": 55.40472599296129,
      "grad_norm": 11.886516571044922,
      "learning_rate": 3.8293950058655945e-06,
      "loss": 1.7884,
      "step": 1432600
    },
    {
      "epoch": 55.40859341764319,
      "grad_norm": 11.255361557006836,
      "learning_rate": 3.8261721519640075e-06,
      "loss": 1.6792,
      "step": 1432700
    },
    {
      "epoch": 55.412460842325096,
      "grad_norm": 15.556416511535645,
      "learning_rate": 3.8229492980624206e-06,
      "loss": 1.7788,
      "step": 1432800
    },
    {
      "epoch": 55.416328267007,
      "grad_norm": 15.95251178741455,
      "learning_rate": 3.819726444160834e-06,
      "loss": 1.746,
      "step": 1432900
    },
    {
      "epoch": 55.4201956916889,
      "grad_norm": 11.159788131713867,
      "learning_rate": 3.816503590259247e-06,
      "loss": 1.7615,
      "step": 1433000
    },
    {
      "epoch": 55.42406311637081,
      "grad_norm": 11.936357498168945,
      "learning_rate": 3.81328073635766e-06,
      "loss": 1.8207,
      "step": 1433100
    },
    {
      "epoch": 55.42793054105271,
      "grad_norm": 12.712186813354492,
      "learning_rate": 3.810057882456073e-06,
      "loss": 1.8122,
      "step": 1433200
    },
    {
      "epoch": 55.43179796573462,
      "grad_norm": 10.177481651306152,
      "learning_rate": 3.8068350285544853e-06,
      "loss": 1.7131,
      "step": 1433300
    },
    {
      "epoch": 55.435665390416524,
      "grad_norm": 13.945651054382324,
      "learning_rate": 3.8036121746528987e-06,
      "loss": 1.8664,
      "step": 1433400
    },
    {
      "epoch": 55.439532815098424,
      "grad_norm": 12.616232872009277,
      "learning_rate": 3.8003893207513117e-06,
      "loss": 1.6974,
      "step": 1433500
    },
    {
      "epoch": 55.44340023978033,
      "grad_norm": 12.035358428955078,
      "learning_rate": 3.7971664668497248e-06,
      "loss": 1.7313,
      "step": 1433600
    },
    {
      "epoch": 55.44726766446223,
      "grad_norm": 12.140084266662598,
      "learning_rate": 3.793943612948138e-06,
      "loss": 1.8665,
      "step": 1433700
    },
    {
      "epoch": 55.45113508914414,
      "grad_norm": 11.694608688354492,
      "learning_rate": 3.790720759046551e-06,
      "loss": 1.7705,
      "step": 1433800
    },
    {
      "epoch": 55.455002513826045,
      "grad_norm": 14.087180137634277,
      "learning_rate": 3.7874979051449643e-06,
      "loss": 1.8473,
      "step": 1433900
    },
    {
      "epoch": 55.458869938507945,
      "grad_norm": 10.368671417236328,
      "learning_rate": 3.7842750512433773e-06,
      "loss": 1.6818,
      "step": 1434000
    },
    {
      "epoch": 55.46273736318985,
      "grad_norm": 9.97425651550293,
      "learning_rate": 3.7810521973417903e-06,
      "loss": 1.7243,
      "step": 1434100
    },
    {
      "epoch": 55.46660478787176,
      "grad_norm": 15.506011962890625,
      "learning_rate": 3.7778293434402033e-06,
      "loss": 1.766,
      "step": 1434200
    },
    {
      "epoch": 55.47047221255366,
      "grad_norm": 13.040457725524902,
      "learning_rate": 3.7746064895386164e-06,
      "loss": 1.7776,
      "step": 1434300
    },
    {
      "epoch": 55.474339637235566,
      "grad_norm": 13.88199520111084,
      "learning_rate": 3.77138363563703e-06,
      "loss": 1.7545,
      "step": 1434400
    },
    {
      "epoch": 55.478207061917466,
      "grad_norm": 13.932343482971191,
      "learning_rate": 3.768160781735443e-06,
      "loss": 1.7701,
      "step": 1434500
    },
    {
      "epoch": 55.48207448659937,
      "grad_norm": 13.061891555786133,
      "learning_rate": 3.764937927833856e-06,
      "loss": 1.7374,
      "step": 1434600
    },
    {
      "epoch": 55.48594191128128,
      "grad_norm": 13.338644981384277,
      "learning_rate": 3.761715073932269e-06,
      "loss": 1.8328,
      "step": 1434700
    },
    {
      "epoch": 55.48980933596318,
      "grad_norm": 10.859399795532227,
      "learning_rate": 3.7584922200306815e-06,
      "loss": 1.7924,
      "step": 1434800
    },
    {
      "epoch": 55.49367676064509,
      "grad_norm": 17.510040283203125,
      "learning_rate": 3.7552693661290945e-06,
      "loss": 1.8257,
      "step": 1434900
    },
    {
      "epoch": 55.497544185326994,
      "grad_norm": 12.597350120544434,
      "learning_rate": 3.7520465122275075e-06,
      "loss": 1.7751,
      "step": 1435000
    },
    {
      "epoch": 55.501411610008894,
      "grad_norm": 12.914424896240234,
      "learning_rate": 3.748823658325921e-06,
      "loss": 1.7746,
      "step": 1435100
    },
    {
      "epoch": 55.5052790346908,
      "grad_norm": 12.474306106567383,
      "learning_rate": 3.745600804424334e-06,
      "loss": 1.7783,
      "step": 1435200
    },
    {
      "epoch": 55.5091464593727,
      "grad_norm": 9.710155487060547,
      "learning_rate": 3.742377950522747e-06,
      "loss": 1.7841,
      "step": 1435300
    },
    {
      "epoch": 55.51301388405461,
      "grad_norm": 14.401505470275879,
      "learning_rate": 3.73915509662116e-06,
      "loss": 1.7279,
      "step": 1435400
    },
    {
      "epoch": 55.516881308736515,
      "grad_norm": 11.530213356018066,
      "learning_rate": 3.735932242719573e-06,
      "loss": 1.698,
      "step": 1435500
    },
    {
      "epoch": 55.520748733418415,
      "grad_norm": 11.980051040649414,
      "learning_rate": 3.7327093888179865e-06,
      "loss": 1.7962,
      "step": 1435600
    },
    {
      "epoch": 55.52461615810032,
      "grad_norm": 11.45677661895752,
      "learning_rate": 3.7294865349163995e-06,
      "loss": 1.7921,
      "step": 1435700
    },
    {
      "epoch": 55.52848358278222,
      "grad_norm": 10.273183822631836,
      "learning_rate": 3.7262636810148126e-06,
      "loss": 1.7387,
      "step": 1435800
    },
    {
      "epoch": 55.53235100746413,
      "grad_norm": 9.27953052520752,
      "learning_rate": 3.7230408271132256e-06,
      "loss": 1.8488,
      "step": 1435900
    },
    {
      "epoch": 55.536218432146036,
      "grad_norm": 11.7230863571167,
      "learning_rate": 3.7198179732116386e-06,
      "loss": 1.8031,
      "step": 1436000
    },
    {
      "epoch": 55.540085856827936,
      "grad_norm": 12.14897346496582,
      "learning_rate": 3.716595119310052e-06,
      "loss": 1.8392,
      "step": 1436100
    },
    {
      "epoch": 55.54395328150984,
      "grad_norm": 10.823624610900879,
      "learning_rate": 3.7133722654084642e-06,
      "loss": 1.7348,
      "step": 1436200
    },
    {
      "epoch": 55.54782070619175,
      "grad_norm": 14.966034889221191,
      "learning_rate": 3.7101494115068773e-06,
      "loss": 1.7726,
      "step": 1436300
    },
    {
      "epoch": 55.55168813087365,
      "grad_norm": 13.906092643737793,
      "learning_rate": 3.7069265576052907e-06,
      "loss": 1.6909,
      "step": 1436400
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 21.419902801513672,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 1.7472,
      "step": 1436500
    },
    {
      "epoch": 55.55942298023746,
      "grad_norm": 12.120776176452637,
      "learning_rate": 3.7004808498021168e-06,
      "loss": 1.7178,
      "step": 1436600
    },
    {
      "epoch": 55.563290404919364,
      "grad_norm": 11.481287956237793,
      "learning_rate": 3.69725799590053e-06,
      "loss": 1.7445,
      "step": 1436700
    },
    {
      "epoch": 55.56715782960127,
      "grad_norm": 12.493183135986328,
      "learning_rate": 3.694035141998943e-06,
      "loss": 1.7382,
      "step": 1436800
    },
    {
      "epoch": 55.57102525428317,
      "grad_norm": 12.07925033569336,
      "learning_rate": 3.6908122880973563e-06,
      "loss": 1.7833,
      "step": 1436900
    },
    {
      "epoch": 55.57489267896508,
      "grad_norm": 13.925887107849121,
      "learning_rate": 3.6875894341957693e-06,
      "loss": 1.8049,
      "step": 1437000
    },
    {
      "epoch": 55.57876010364698,
      "grad_norm": 11.880738258361816,
      "learning_rate": 3.6843665802941823e-06,
      "loss": 1.7746,
      "step": 1437100
    },
    {
      "epoch": 55.582627528328885,
      "grad_norm": 11.708468437194824,
      "learning_rate": 3.6811437263925953e-06,
      "loss": 1.8197,
      "step": 1437200
    },
    {
      "epoch": 55.58649495301079,
      "grad_norm": 16.63413429260254,
      "learning_rate": 3.6779208724910088e-06,
      "loss": 1.8592,
      "step": 1437300
    },
    {
      "epoch": 55.59036237769269,
      "grad_norm": 12.907706260681152,
      "learning_rate": 3.674698018589422e-06,
      "loss": 1.797,
      "step": 1437400
    },
    {
      "epoch": 55.5942298023746,
      "grad_norm": 13.077912330627441,
      "learning_rate": 3.671475164687835e-06,
      "loss": 1.8035,
      "step": 1437500
    },
    {
      "epoch": 55.598097227056506,
      "grad_norm": 11.261713027954102,
      "learning_rate": 3.668252310786248e-06,
      "loss": 1.8341,
      "step": 1437600
    },
    {
      "epoch": 55.601964651738406,
      "grad_norm": 14.670068740844727,
      "learning_rate": 3.6650294568846605e-06,
      "loss": 1.798,
      "step": 1437700
    },
    {
      "epoch": 55.60583207642031,
      "grad_norm": 8.480542182922363,
      "learning_rate": 3.6618066029830735e-06,
      "loss": 1.7541,
      "step": 1437800
    },
    {
      "epoch": 55.60969950110221,
      "grad_norm": 11.723161697387695,
      "learning_rate": 3.6585837490814865e-06,
      "loss": 1.7903,
      "step": 1437900
    },
    {
      "epoch": 55.61356692578412,
      "grad_norm": 14.328404426574707,
      "learning_rate": 3.6553608951798995e-06,
      "loss": 1.7964,
      "step": 1438000
    },
    {
      "epoch": 55.61743435046603,
      "grad_norm": 13.265887260437012,
      "learning_rate": 3.652138041278313e-06,
      "loss": 1.788,
      "step": 1438100
    },
    {
      "epoch": 55.62130177514793,
      "grad_norm": 10.808385848999023,
      "learning_rate": 3.648915187376726e-06,
      "loss": 1.7902,
      "step": 1438200
    },
    {
      "epoch": 55.625169199829834,
      "grad_norm": 11.8800048828125,
      "learning_rate": 3.645692333475139e-06,
      "loss": 1.6794,
      "step": 1438300
    },
    {
      "epoch": 55.62903662451174,
      "grad_norm": 13.983895301818848,
      "learning_rate": 3.642469479573552e-06,
      "loss": 1.6866,
      "step": 1438400
    },
    {
      "epoch": 55.63290404919364,
      "grad_norm": 11.793452262878418,
      "learning_rate": 3.639246625671965e-06,
      "loss": 1.7426,
      "step": 1438500
    },
    {
      "epoch": 55.63677147387555,
      "grad_norm": 12.626496315002441,
      "learning_rate": 3.6360237717703785e-06,
      "loss": 1.8472,
      "step": 1438600
    },
    {
      "epoch": 55.64063889855745,
      "grad_norm": 10.736200332641602,
      "learning_rate": 3.6328009178687915e-06,
      "loss": 1.7884,
      "step": 1438700
    },
    {
      "epoch": 55.644506323239355,
      "grad_norm": 14.04466724395752,
      "learning_rate": 3.6295780639672046e-06,
      "loss": 1.776,
      "step": 1438800
    },
    {
      "epoch": 55.64837374792126,
      "grad_norm": 13.18085765838623,
      "learning_rate": 3.6263552100656176e-06,
      "loss": 1.8278,
      "step": 1438900
    },
    {
      "epoch": 55.65224117260316,
      "grad_norm": 11.889820098876953,
      "learning_rate": 3.6231323561640306e-06,
      "loss": 1.7347,
      "step": 1439000
    },
    {
      "epoch": 55.65610859728507,
      "grad_norm": 14.086357116699219,
      "learning_rate": 3.619909502262444e-06,
      "loss": 1.7722,
      "step": 1439100
    },
    {
      "epoch": 55.65997602196697,
      "grad_norm": 10.311267852783203,
      "learning_rate": 3.6166866483608562e-06,
      "loss": 1.7745,
      "step": 1439200
    },
    {
      "epoch": 55.663843446648876,
      "grad_norm": 9.513577461242676,
      "learning_rate": 3.6134637944592693e-06,
      "loss": 1.794,
      "step": 1439300
    },
    {
      "epoch": 55.66771087133078,
      "grad_norm": 14.673066139221191,
      "learning_rate": 3.6102409405576827e-06,
      "loss": 1.7912,
      "step": 1439400
    },
    {
      "epoch": 55.67157829601268,
      "grad_norm": 15.411826133728027,
      "learning_rate": 3.6070180866560957e-06,
      "loss": 1.8376,
      "step": 1439500
    },
    {
      "epoch": 55.67544572069459,
      "grad_norm": 17.481016159057617,
      "learning_rate": 3.6037952327545088e-06,
      "loss": 1.8111,
      "step": 1439600
    },
    {
      "epoch": 55.6793131453765,
      "grad_norm": 9.484367370605469,
      "learning_rate": 3.600572378852922e-06,
      "loss": 1.7811,
      "step": 1439700
    },
    {
      "epoch": 55.6831805700584,
      "grad_norm": 14.1756591796875,
      "learning_rate": 3.5973495249513352e-06,
      "loss": 1.855,
      "step": 1439800
    },
    {
      "epoch": 55.687047994740304,
      "grad_norm": 12.952539443969727,
      "learning_rate": 3.5941266710497483e-06,
      "loss": 1.7955,
      "step": 1439900
    },
    {
      "epoch": 55.690915419422204,
      "grad_norm": 10.2013578414917,
      "learning_rate": 3.5909038171481613e-06,
      "loss": 1.7666,
      "step": 1440000
    },
    {
      "epoch": 55.69478284410411,
      "grad_norm": 15.01032543182373,
      "learning_rate": 3.5876809632465743e-06,
      "loss": 1.7697,
      "step": 1440100
    },
    {
      "epoch": 55.69865026878602,
      "grad_norm": 10.612338066101074,
      "learning_rate": 3.5844581093449873e-06,
      "loss": 1.8098,
      "step": 1440200
    },
    {
      "epoch": 55.70251769346792,
      "grad_norm": 16.665119171142578,
      "learning_rate": 3.5812352554434008e-06,
      "loss": 1.7606,
      "step": 1440300
    },
    {
      "epoch": 55.706385118149825,
      "grad_norm": 12.519776344299316,
      "learning_rate": 3.578012401541814e-06,
      "loss": 1.7608,
      "step": 1440400
    },
    {
      "epoch": 55.710252542831725,
      "grad_norm": 14.95773696899414,
      "learning_rate": 3.574789547640227e-06,
      "loss": 1.8032,
      "step": 1440500
    },
    {
      "epoch": 55.71411996751363,
      "grad_norm": 17.947132110595703,
      "learning_rate": 3.57156669373864e-06,
      "loss": 1.8087,
      "step": 1440600
    },
    {
      "epoch": 55.71798739219554,
      "grad_norm": 8.117777824401855,
      "learning_rate": 3.5683438398370525e-06,
      "loss": 1.7236,
      "step": 1440700
    },
    {
      "epoch": 55.72185481687744,
      "grad_norm": 13.217527389526367,
      "learning_rate": 3.5651209859354655e-06,
      "loss": 1.6696,
      "step": 1440800
    },
    {
      "epoch": 55.725722241559346,
      "grad_norm": 19.703813552856445,
      "learning_rate": 3.5618981320338785e-06,
      "loss": 1.7242,
      "step": 1440900
    },
    {
      "epoch": 55.72958966624125,
      "grad_norm": 13.053079605102539,
      "learning_rate": 3.5586752781322915e-06,
      "loss": 1.7435,
      "step": 1441000
    },
    {
      "epoch": 55.73345709092315,
      "grad_norm": 12.169941902160645,
      "learning_rate": 3.555452424230705e-06,
      "loss": 1.8209,
      "step": 1441100
    },
    {
      "epoch": 55.73732451560506,
      "grad_norm": 9.299030303955078,
      "learning_rate": 3.552229570329118e-06,
      "loss": 1.83,
      "step": 1441200
    },
    {
      "epoch": 55.74119194028696,
      "grad_norm": 14.457196235656738,
      "learning_rate": 3.549006716427531e-06,
      "loss": 1.7423,
      "step": 1441300
    },
    {
      "epoch": 55.74505936496887,
      "grad_norm": 15.640071868896484,
      "learning_rate": 3.545783862525944e-06,
      "loss": 1.8455,
      "step": 1441400
    },
    {
      "epoch": 55.748926789650774,
      "grad_norm": 12.090203285217285,
      "learning_rate": 3.542561008624357e-06,
      "loss": 1.8545,
      "step": 1441500
    },
    {
      "epoch": 55.752794214332674,
      "grad_norm": 13.372944831848145,
      "learning_rate": 3.5393381547227705e-06,
      "loss": 1.7735,
      "step": 1441600
    },
    {
      "epoch": 55.75666163901458,
      "grad_norm": 11.73509693145752,
      "learning_rate": 3.5361153008211835e-06,
      "loss": 1.7713,
      "step": 1441700
    },
    {
      "epoch": 55.76052906369648,
      "grad_norm": 13.941983222961426,
      "learning_rate": 3.5328924469195966e-06,
      "loss": 1.8046,
      "step": 1441800
    },
    {
      "epoch": 55.76439648837839,
      "grad_norm": 11.679388046264648,
      "learning_rate": 3.5296695930180096e-06,
      "loss": 1.7818,
      "step": 1441900
    },
    {
      "epoch": 55.768263913060295,
      "grad_norm": 16.662221908569336,
      "learning_rate": 3.526446739116423e-06,
      "loss": 1.7298,
      "step": 1442000
    },
    {
      "epoch": 55.772131337742195,
      "grad_norm": 11.467536926269531,
      "learning_rate": 3.523223885214836e-06,
      "loss": 1.75,
      "step": 1442100
    },
    {
      "epoch": 55.7759987624241,
      "grad_norm": 14.112223625183105,
      "learning_rate": 3.5200010313132482e-06,
      "loss": 1.8257,
      "step": 1442200
    },
    {
      "epoch": 55.77986618710601,
      "grad_norm": 11.334181785583496,
      "learning_rate": 3.5167781774116617e-06,
      "loss": 1.8038,
      "step": 1442300
    },
    {
      "epoch": 55.78373361178791,
      "grad_norm": 15.451346397399902,
      "learning_rate": 3.5135553235100747e-06,
      "loss": 1.7757,
      "step": 1442400
    },
    {
      "epoch": 55.787601036469816,
      "grad_norm": 7.660789966583252,
      "learning_rate": 3.5103324696084877e-06,
      "loss": 1.7946,
      "step": 1442500
    },
    {
      "epoch": 55.791468461151716,
      "grad_norm": 4.869770526885986,
      "learning_rate": 3.5071096157069008e-06,
      "loss": 1.7495,
      "step": 1442600
    },
    {
      "epoch": 55.79533588583362,
      "grad_norm": 11.469380378723145,
      "learning_rate": 3.503886761805314e-06,
      "loss": 1.7948,
      "step": 1442700
    },
    {
      "epoch": 55.79920331051553,
      "grad_norm": 15.790718078613281,
      "learning_rate": 3.5006639079037272e-06,
      "loss": 1.7932,
      "step": 1442800
    },
    {
      "epoch": 55.80307073519743,
      "grad_norm": 12.329174995422363,
      "learning_rate": 3.4974410540021403e-06,
      "loss": 1.774,
      "step": 1442900
    },
    {
      "epoch": 55.80693815987934,
      "grad_norm": 12.834222793579102,
      "learning_rate": 3.4942182001005533e-06,
      "loss": 1.8462,
      "step": 1443000
    },
    {
      "epoch": 55.810805584561244,
      "grad_norm": 12.950956344604492,
      "learning_rate": 3.4909953461989663e-06,
      "loss": 1.7884,
      "step": 1443100
    },
    {
      "epoch": 55.814673009243144,
      "grad_norm": 15.535226821899414,
      "learning_rate": 3.4877724922973793e-06,
      "loss": 1.7491,
      "step": 1443200
    },
    {
      "epoch": 55.81854043392505,
      "grad_norm": 12.909540176391602,
      "learning_rate": 3.4845496383957928e-06,
      "loss": 1.8071,
      "step": 1443300
    },
    {
      "epoch": 55.82240785860695,
      "grad_norm": 15.081825256347656,
      "learning_rate": 3.481326784494206e-06,
      "loss": 1.7943,
      "step": 1443400
    },
    {
      "epoch": 55.82627528328886,
      "grad_norm": 16.897125244140625,
      "learning_rate": 3.478103930592619e-06,
      "loss": 1.6987,
      "step": 1443500
    },
    {
      "epoch": 55.830142707970765,
      "grad_norm": 11.720624923706055,
      "learning_rate": 3.4748810766910314e-06,
      "loss": 1.776,
      "step": 1443600
    },
    {
      "epoch": 55.834010132652665,
      "grad_norm": 11.193190574645996,
      "learning_rate": 3.4716582227894445e-06,
      "loss": 1.7589,
      "step": 1443700
    },
    {
      "epoch": 55.83787755733457,
      "grad_norm": 17.719383239746094,
      "learning_rate": 3.4684353688878575e-06,
      "loss": 1.7656,
      "step": 1443800
    },
    {
      "epoch": 55.84174498201647,
      "grad_norm": 11.22249984741211,
      "learning_rate": 3.4652125149862705e-06,
      "loss": 1.7077,
      "step": 1443900
    },
    {
      "epoch": 55.84561240669838,
      "grad_norm": 9.605920791625977,
      "learning_rate": 3.4619896610846835e-06,
      "loss": 1.7678,
      "step": 1444000
    },
    {
      "epoch": 55.849479831380286,
      "grad_norm": 13.75673770904541,
      "learning_rate": 3.458766807183097e-06,
      "loss": 1.7889,
      "step": 1444100
    },
    {
      "epoch": 55.853347256062186,
      "grad_norm": 15.255149841308594,
      "learning_rate": 3.45554395328151e-06,
      "loss": 1.7163,
      "step": 1444200
    },
    {
      "epoch": 55.85721468074409,
      "grad_norm": 13.855538368225098,
      "learning_rate": 3.452321099379923e-06,
      "loss": 1.6871,
      "step": 1444300
    },
    {
      "epoch": 55.861082105426,
      "grad_norm": 10.801677703857422,
      "learning_rate": 3.449098245478336e-06,
      "loss": 1.7397,
      "step": 1444400
    },
    {
      "epoch": 55.8649495301079,
      "grad_norm": 13.900923728942871,
      "learning_rate": 3.4458753915767495e-06,
      "loss": 1.7681,
      "step": 1444500
    },
    {
      "epoch": 55.86881695478981,
      "grad_norm": 12.171274185180664,
      "learning_rate": 3.4426525376751625e-06,
      "loss": 1.8425,
      "step": 1444600
    },
    {
      "epoch": 55.87268437947171,
      "grad_norm": 12.593215942382812,
      "learning_rate": 3.4394296837735755e-06,
      "loss": 1.8427,
      "step": 1444700
    },
    {
      "epoch": 55.876551804153614,
      "grad_norm": 16.344358444213867,
      "learning_rate": 3.4362068298719886e-06,
      "loss": 1.8128,
      "step": 1444800
    },
    {
      "epoch": 55.88041922883552,
      "grad_norm": 8.323297500610352,
      "learning_rate": 3.4329839759704016e-06,
      "loss": 1.786,
      "step": 1444900
    },
    {
      "epoch": 55.88428665351742,
      "grad_norm": 13.821998596191406,
      "learning_rate": 3.429761122068815e-06,
      "loss": 1.7535,
      "step": 1445000
    },
    {
      "epoch": 55.88815407819933,
      "grad_norm": 13.069091796875,
      "learning_rate": 3.4265382681672272e-06,
      "loss": 1.7547,
      "step": 1445100
    },
    {
      "epoch": 55.89202150288123,
      "grad_norm": 10.258646011352539,
      "learning_rate": 3.4233154142656402e-06,
      "loss": 1.7391,
      "step": 1445200
    },
    {
      "epoch": 55.895888927563135,
      "grad_norm": 12.557190895080566,
      "learning_rate": 3.4200925603640537e-06,
      "loss": 1.7561,
      "step": 1445300
    },
    {
      "epoch": 55.89975635224504,
      "grad_norm": 10.971494674682617,
      "learning_rate": 3.4168697064624667e-06,
      "loss": 1.8074,
      "step": 1445400
    },
    {
      "epoch": 55.90362377692694,
      "grad_norm": 15.361519813537598,
      "learning_rate": 3.4136468525608797e-06,
      "loss": 1.8138,
      "step": 1445500
    },
    {
      "epoch": 55.90749120160885,
      "grad_norm": 15.62801456451416,
      "learning_rate": 3.4104239986592928e-06,
      "loss": 1.6937,
      "step": 1445600
    },
    {
      "epoch": 55.911358626290756,
      "grad_norm": 16.50604248046875,
      "learning_rate": 3.407201144757706e-06,
      "loss": 1.9126,
      "step": 1445700
    },
    {
      "epoch": 55.915226050972656,
      "grad_norm": 13.564403533935547,
      "learning_rate": 3.4039782908561192e-06,
      "loss": 1.8744,
      "step": 1445800
    },
    {
      "epoch": 55.91909347565456,
      "grad_norm": 14.046234130859375,
      "learning_rate": 3.4007554369545323e-06,
      "loss": 1.7445,
      "step": 1445900
    },
    {
      "epoch": 55.92296090033646,
      "grad_norm": 12.745392799377441,
      "learning_rate": 3.3975325830529453e-06,
      "loss": 1.8162,
      "step": 1446000
    },
    {
      "epoch": 55.92682832501837,
      "grad_norm": 10.585077285766602,
      "learning_rate": 3.3943097291513583e-06,
      "loss": 1.7308,
      "step": 1446100
    },
    {
      "epoch": 55.93069574970028,
      "grad_norm": 12.271397590637207,
      "learning_rate": 3.3910868752497713e-06,
      "loss": 1.8379,
      "step": 1446200
    },
    {
      "epoch": 55.93456317438218,
      "grad_norm": 10.854259490966797,
      "learning_rate": 3.3878640213481848e-06,
      "loss": 1.7427,
      "step": 1446300
    },
    {
      "epoch": 55.938430599064084,
      "grad_norm": 11.75214672088623,
      "learning_rate": 3.384641167446598e-06,
      "loss": 1.7488,
      "step": 1446400
    },
    {
      "epoch": 55.94229802374599,
      "grad_norm": 13.125886917114258,
      "learning_rate": 3.381418313545011e-06,
      "loss": 1.7401,
      "step": 1446500
    },
    {
      "epoch": 55.94616544842789,
      "grad_norm": 9.04941463470459,
      "learning_rate": 3.3781954596434234e-06,
      "loss": 1.8577,
      "step": 1446600
    },
    {
      "epoch": 55.9500328731098,
      "grad_norm": 20.028003692626953,
      "learning_rate": 3.3749726057418365e-06,
      "loss": 1.7437,
      "step": 1446700
    },
    {
      "epoch": 55.9539002977917,
      "grad_norm": 15.674384117126465,
      "learning_rate": 3.3717497518402495e-06,
      "loss": 1.769,
      "step": 1446800
    },
    {
      "epoch": 55.957767722473605,
      "grad_norm": 11.175395965576172,
      "learning_rate": 3.3685268979386625e-06,
      "loss": 1.8161,
      "step": 1446900
    },
    {
      "epoch": 55.96163514715551,
      "grad_norm": 15.382155418395996,
      "learning_rate": 3.365304044037076e-06,
      "loss": 1.8269,
      "step": 1447000
    },
    {
      "epoch": 55.96550257183741,
      "grad_norm": 14.709144592285156,
      "learning_rate": 3.362081190135489e-06,
      "loss": 1.7278,
      "step": 1447100
    },
    {
      "epoch": 55.96936999651932,
      "grad_norm": 11.454240798950195,
      "learning_rate": 3.358858336233902e-06,
      "loss": 1.7297,
      "step": 1447200
    },
    {
      "epoch": 55.97323742120122,
      "grad_norm": 12.805521965026855,
      "learning_rate": 3.355635482332315e-06,
      "loss": 1.8617,
      "step": 1447300
    },
    {
      "epoch": 55.977104845883126,
      "grad_norm": 16.053617477416992,
      "learning_rate": 3.352412628430728e-06,
      "loss": 1.8218,
      "step": 1447400
    },
    {
      "epoch": 55.98097227056503,
      "grad_norm": 14.095389366149902,
      "learning_rate": 3.3491897745291415e-06,
      "loss": 1.7393,
      "step": 1447500
    },
    {
      "epoch": 55.98483969524693,
      "grad_norm": 11.355683326721191,
      "learning_rate": 3.3459669206275545e-06,
      "loss": 1.7406,
      "step": 1447600
    },
    {
      "epoch": 55.98870711992884,
      "grad_norm": 11.37632942199707,
      "learning_rate": 3.3427440667259675e-06,
      "loss": 1.7654,
      "step": 1447700
    },
    {
      "epoch": 55.99257454461075,
      "grad_norm": 11.716778755187988,
      "learning_rate": 3.3395212128243806e-06,
      "loss": 1.7692,
      "step": 1447800
    },
    {
      "epoch": 55.99644196929265,
      "grad_norm": 19.853178024291992,
      "learning_rate": 3.3362983589227936e-06,
      "loss": 1.7904,
      "step": 1447900
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.7374686002731323,
      "eval_runtime": 2.9342,
      "eval_samples_per_second": 463.842,
      "eval_steps_per_second": 463.842,
      "step": 1447992
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.5683468580245972,
      "eval_runtime": 55.6827,
      "eval_samples_per_second": 464.364,
      "eval_steps_per_second": 464.364,
      "step": 1447992
    },
    {
      "epoch": 56.000309393974554,
      "grad_norm": 14.983447074890137,
      "learning_rate": 3.333075505021207e-06,
      "loss": 1.7452,
      "step": 1448000
    },
    {
      "epoch": 56.004176818656454,
      "grad_norm": 15.333403587341309,
      "learning_rate": 3.3298526511196192e-06,
      "loss": 1.782,
      "step": 1448100
    },
    {
      "epoch": 56.00804424333836,
      "grad_norm": 13.323445320129395,
      "learning_rate": 3.3266297972180322e-06,
      "loss": 1.8093,
      "step": 1448200
    },
    {
      "epoch": 56.01191166802027,
      "grad_norm": 16.657793045043945,
      "learning_rate": 3.3234069433164457e-06,
      "loss": 1.7918,
      "step": 1448300
    },
    {
      "epoch": 56.01577909270217,
      "grad_norm": 13.583468437194824,
      "learning_rate": 3.3201840894148587e-06,
      "loss": 1.7561,
      "step": 1448400
    },
    {
      "epoch": 56.019646517384075,
      "grad_norm": 14.850330352783203,
      "learning_rate": 3.3169612355132717e-06,
      "loss": 1.7268,
      "step": 1448500
    },
    {
      "epoch": 56.023513942065975,
      "grad_norm": 15.317880630493164,
      "learning_rate": 3.3137383816116848e-06,
      "loss": 1.8753,
      "step": 1448600
    },
    {
      "epoch": 56.02738136674788,
      "grad_norm": 8.388189315795898,
      "learning_rate": 3.310515527710098e-06,
      "loss": 1.7729,
      "step": 1448700
    },
    {
      "epoch": 56.03124879142979,
      "grad_norm": 9.712285041809082,
      "learning_rate": 3.3072926738085112e-06,
      "loss": 1.7595,
      "step": 1448800
    },
    {
      "epoch": 56.03511621611169,
      "grad_norm": 11.476035118103027,
      "learning_rate": 3.3040698199069243e-06,
      "loss": 1.7996,
      "step": 1448900
    },
    {
      "epoch": 56.038983640793596,
      "grad_norm": 9.388331413269043,
      "learning_rate": 3.3008469660053373e-06,
      "loss": 1.7351,
      "step": 1449000
    },
    {
      "epoch": 56.0428510654755,
      "grad_norm": 12.494331359863281,
      "learning_rate": 3.2976241121037503e-06,
      "loss": 1.7423,
      "step": 1449100
    },
    {
      "epoch": 56.0467184901574,
      "grad_norm": 12.223418235778809,
      "learning_rate": 3.2944012582021638e-06,
      "loss": 1.7745,
      "step": 1449200
    },
    {
      "epoch": 56.05058591483931,
      "grad_norm": 19.52383041381836,
      "learning_rate": 3.2911784043005768e-06,
      "loss": 1.7637,
      "step": 1449300
    },
    {
      "epoch": 56.05445333952121,
      "grad_norm": 14.944953918457031,
      "learning_rate": 3.28795555039899e-06,
      "loss": 1.8196,
      "step": 1449400
    },
    {
      "epoch": 56.05832076420312,
      "grad_norm": 11.663936614990234,
      "learning_rate": 3.2847326964974024e-06,
      "loss": 1.7845,
      "step": 1449500
    },
    {
      "epoch": 56.062188188885024,
      "grad_norm": 10.597307205200195,
      "learning_rate": 3.2815098425958154e-06,
      "loss": 1.728,
      "step": 1449600
    },
    {
      "epoch": 56.066055613566924,
      "grad_norm": 12.60291576385498,
      "learning_rate": 3.2782869886942285e-06,
      "loss": 1.8027,
      "step": 1449700
    },
    {
      "epoch": 56.06992303824883,
      "grad_norm": 12.005861282348633,
      "learning_rate": 3.2750641347926415e-06,
      "loss": 1.7395,
      "step": 1449800
    },
    {
      "epoch": 56.07379046293073,
      "grad_norm": 13.112421035766602,
      "learning_rate": 3.2718412808910545e-06,
      "loss": 1.7297,
      "step": 1449900
    },
    {
      "epoch": 56.07765788761264,
      "grad_norm": 13.535584449768066,
      "learning_rate": 3.268618426989468e-06,
      "loss": 1.8091,
      "step": 1450000
    },
    {
      "epoch": 56.081525312294545,
      "grad_norm": 14.097517013549805,
      "learning_rate": 3.265395573087881e-06,
      "loss": 1.7592,
      "step": 1450100
    },
    {
      "epoch": 56.085392736976445,
      "grad_norm": 11.19222354888916,
      "learning_rate": 3.262172719186294e-06,
      "loss": 1.6701,
      "step": 1450200
    },
    {
      "epoch": 56.08926016165835,
      "grad_norm": 11.549311637878418,
      "learning_rate": 3.258949865284707e-06,
      "loss": 1.8343,
      "step": 1450300
    },
    {
      "epoch": 56.09312758634026,
      "grad_norm": 11.821171760559082,
      "learning_rate": 3.25572701138312e-06,
      "loss": 1.767,
      "step": 1450400
    },
    {
      "epoch": 56.09699501102216,
      "grad_norm": 12.69827651977539,
      "learning_rate": 3.2525041574815335e-06,
      "loss": 1.7717,
      "step": 1450500
    },
    {
      "epoch": 56.100862435704066,
      "grad_norm": 15.292957305908203,
      "learning_rate": 3.2492813035799465e-06,
      "loss": 1.7718,
      "step": 1450600
    },
    {
      "epoch": 56.104729860385966,
      "grad_norm": 11.368904113769531,
      "learning_rate": 3.2460584496783595e-06,
      "loss": 1.7695,
      "step": 1450700
    },
    {
      "epoch": 56.10859728506787,
      "grad_norm": 11.927072525024414,
      "learning_rate": 3.2428355957767726e-06,
      "loss": 1.7466,
      "step": 1450800
    },
    {
      "epoch": 56.11246470974978,
      "grad_norm": 13.939311027526855,
      "learning_rate": 3.2396127418751856e-06,
      "loss": 1.871,
      "step": 1450900
    },
    {
      "epoch": 56.11633213443168,
      "grad_norm": 9.941268920898438,
      "learning_rate": 3.236389887973598e-06,
      "loss": 1.7799,
      "step": 1451000
    },
    {
      "epoch": 56.12019955911359,
      "grad_norm": 15.19677448272705,
      "learning_rate": 3.2331670340720112e-06,
      "loss": 1.7143,
      "step": 1451100
    },
    {
      "epoch": 56.124066983795494,
      "grad_norm": 13.141562461853027,
      "learning_rate": 3.2299441801704242e-06,
      "loss": 1.7975,
      "step": 1451200
    },
    {
      "epoch": 56.127934408477394,
      "grad_norm": 12.872695922851562,
      "learning_rate": 3.2267213262688377e-06,
      "loss": 1.8343,
      "step": 1451300
    },
    {
      "epoch": 56.1318018331593,
      "grad_norm": 10.969761848449707,
      "learning_rate": 3.2234984723672507e-06,
      "loss": 1.7781,
      "step": 1451400
    },
    {
      "epoch": 56.1356692578412,
      "grad_norm": 12.833674430847168,
      "learning_rate": 3.2202756184656637e-06,
      "loss": 1.8811,
      "step": 1451500
    },
    {
      "epoch": 56.13953668252311,
      "grad_norm": 11.70218276977539,
      "learning_rate": 3.2170527645640768e-06,
      "loss": 1.8314,
      "step": 1451600
    },
    {
      "epoch": 56.143404107205015,
      "grad_norm": 12.674410820007324,
      "learning_rate": 3.2138299106624902e-06,
      "loss": 1.6871,
      "step": 1451700
    },
    {
      "epoch": 56.147271531886915,
      "grad_norm": 19.224763870239258,
      "learning_rate": 3.2106070567609032e-06,
      "loss": 1.799,
      "step": 1451800
    },
    {
      "epoch": 56.15113895656882,
      "grad_norm": 11.359637260437012,
      "learning_rate": 3.2073842028593163e-06,
      "loss": 1.7732,
      "step": 1451900
    },
    {
      "epoch": 56.15500638125072,
      "grad_norm": 14.562713623046875,
      "learning_rate": 3.2041613489577293e-06,
      "loss": 1.8203,
      "step": 1452000
    },
    {
      "epoch": 56.15887380593263,
      "grad_norm": 9.99842357635498,
      "learning_rate": 3.2009384950561423e-06,
      "loss": 1.8732,
      "step": 1452100
    },
    {
      "epoch": 56.162741230614536,
      "grad_norm": 12.34725570678711,
      "learning_rate": 3.1977156411545558e-06,
      "loss": 1.7256,
      "step": 1452200
    },
    {
      "epoch": 56.166608655296436,
      "grad_norm": 11.448809623718262,
      "learning_rate": 3.1944927872529688e-06,
      "loss": 1.8096,
      "step": 1452300
    },
    {
      "epoch": 56.17047607997834,
      "grad_norm": 13.202455520629883,
      "learning_rate": 3.191269933351382e-06,
      "loss": 1.7933,
      "step": 1452400
    },
    {
      "epoch": 56.17434350466025,
      "grad_norm": 12.772102355957031,
      "learning_rate": 3.1880470794497944e-06,
      "loss": 1.7511,
      "step": 1452500
    },
    {
      "epoch": 56.17821092934215,
      "grad_norm": 13.838716506958008,
      "learning_rate": 3.1848242255482074e-06,
      "loss": 1.7822,
      "step": 1452600
    },
    {
      "epoch": 56.18207835402406,
      "grad_norm": 11.899964332580566,
      "learning_rate": 3.1816013716466205e-06,
      "loss": 1.7676,
      "step": 1452700
    },
    {
      "epoch": 56.18594577870596,
      "grad_norm": 11.224214553833008,
      "learning_rate": 3.1783785177450335e-06,
      "loss": 1.7385,
      "step": 1452800
    },
    {
      "epoch": 56.189813203387864,
      "grad_norm": 25.81112289428711,
      "learning_rate": 3.1751556638434465e-06,
      "loss": 1.7794,
      "step": 1452900
    },
    {
      "epoch": 56.19368062806977,
      "grad_norm": 10.421441078186035,
      "learning_rate": 3.17193280994186e-06,
      "loss": 1.722,
      "step": 1453000
    },
    {
      "epoch": 56.19754805275167,
      "grad_norm": 12.140312194824219,
      "learning_rate": 3.168709956040273e-06,
      "loss": 1.7732,
      "step": 1453100
    },
    {
      "epoch": 56.20141547743358,
      "grad_norm": 13.227154731750488,
      "learning_rate": 3.165487102138686e-06,
      "loss": 1.809,
      "step": 1453200
    },
    {
      "epoch": 56.20528290211548,
      "grad_norm": 12.91528034210205,
      "learning_rate": 3.162264248237099e-06,
      "loss": 1.7483,
      "step": 1453300
    },
    {
      "epoch": 56.209150326797385,
      "grad_norm": 13.283743858337402,
      "learning_rate": 3.159041394335512e-06,
      "loss": 1.7591,
      "step": 1453400
    },
    {
      "epoch": 56.21301775147929,
      "grad_norm": 11.932361602783203,
      "learning_rate": 3.1558185404339255e-06,
      "loss": 1.6728,
      "step": 1453500
    },
    {
      "epoch": 56.21688517616119,
      "grad_norm": 15.21570110321045,
      "learning_rate": 3.1525956865323385e-06,
      "loss": 1.7067,
      "step": 1453600
    },
    {
      "epoch": 56.2207526008431,
      "grad_norm": 11.651151657104492,
      "learning_rate": 3.1493728326307515e-06,
      "loss": 1.7732,
      "step": 1453700
    },
    {
      "epoch": 56.224620025525006,
      "grad_norm": 14.0601224899292,
      "learning_rate": 3.1461499787291646e-06,
      "loss": 1.6958,
      "step": 1453800
    },
    {
      "epoch": 56.228487450206906,
      "grad_norm": 12.179133415222168,
      "learning_rate": 3.142927124827578e-06,
      "loss": 1.8285,
      "step": 1453900
    },
    {
      "epoch": 56.23235487488881,
      "grad_norm": 15.90652847290039,
      "learning_rate": 3.13970427092599e-06,
      "loss": 1.728,
      "step": 1454000
    },
    {
      "epoch": 56.23622229957071,
      "grad_norm": 9.087187767028809,
      "learning_rate": 3.1364814170244032e-06,
      "loss": 1.7185,
      "step": 1454100
    },
    {
      "epoch": 56.24008972425262,
      "grad_norm": 12.461906433105469,
      "learning_rate": 3.1332585631228167e-06,
      "loss": 1.7507,
      "step": 1454200
    },
    {
      "epoch": 56.24395714893453,
      "grad_norm": 12.434619903564453,
      "learning_rate": 3.1300357092212297e-06,
      "loss": 1.7339,
      "step": 1454300
    },
    {
      "epoch": 56.24782457361643,
      "grad_norm": 11.050148010253906,
      "learning_rate": 3.1268128553196427e-06,
      "loss": 1.8149,
      "step": 1454400
    },
    {
      "epoch": 56.251691998298334,
      "grad_norm": 10.74599552154541,
      "learning_rate": 3.1235900014180557e-06,
      "loss": 1.7796,
      "step": 1454500
    },
    {
      "epoch": 56.25555942298024,
      "grad_norm": 12.68542766571045,
      "learning_rate": 3.1203671475164688e-06,
      "loss": 1.7118,
      "step": 1454600
    },
    {
      "epoch": 56.25942684766214,
      "grad_norm": 12.764776229858398,
      "learning_rate": 3.1171442936148822e-06,
      "loss": 1.7838,
      "step": 1454700
    },
    {
      "epoch": 56.26329427234405,
      "grad_norm": 10.48485279083252,
      "learning_rate": 3.1139214397132952e-06,
      "loss": 1.7393,
      "step": 1454800
    },
    {
      "epoch": 56.26716169702595,
      "grad_norm": 16.18124008178711,
      "learning_rate": 3.1106985858117083e-06,
      "loss": 1.7447,
      "step": 1454900
    },
    {
      "epoch": 56.271029121707855,
      "grad_norm": 9.15892505645752,
      "learning_rate": 3.1074757319101213e-06,
      "loss": 1.8401,
      "step": 1455000
    },
    {
      "epoch": 56.27489654638976,
      "grad_norm": 22.84074592590332,
      "learning_rate": 3.1042528780085343e-06,
      "loss": 1.8831,
      "step": 1455100
    },
    {
      "epoch": 56.27876397107166,
      "grad_norm": 14.759571075439453,
      "learning_rate": 3.1010300241069473e-06,
      "loss": 1.8749,
      "step": 1455200
    },
    {
      "epoch": 56.28263139575357,
      "grad_norm": 11.678582191467285,
      "learning_rate": 3.0978071702053604e-06,
      "loss": 1.8366,
      "step": 1455300
    },
    {
      "epoch": 56.28649882043547,
      "grad_norm": 10.746256828308105,
      "learning_rate": 3.0945843163037734e-06,
      "loss": 1.7912,
      "step": 1455400
    },
    {
      "epoch": 56.290366245117376,
      "grad_norm": 11.841988563537598,
      "learning_rate": 3.0913614624021864e-06,
      "loss": 1.8454,
      "step": 1455500
    },
    {
      "epoch": 56.29423366979928,
      "grad_norm": 16.884939193725586,
      "learning_rate": 3.0881386085006e-06,
      "loss": 1.732,
      "step": 1455600
    },
    {
      "epoch": 56.29810109448118,
      "grad_norm": 12.0276460647583,
      "learning_rate": 3.084915754599013e-06,
      "loss": 1.7885,
      "step": 1455700
    },
    {
      "epoch": 56.30196851916309,
      "grad_norm": 11.300875663757324,
      "learning_rate": 3.0816929006974255e-06,
      "loss": 1.7858,
      "step": 1455800
    },
    {
      "epoch": 56.305835943845,
      "grad_norm": 10.401246070861816,
      "learning_rate": 3.0784700467958385e-06,
      "loss": 1.8195,
      "step": 1455900
    },
    {
      "epoch": 56.3097033685269,
      "grad_norm": 16.828519821166992,
      "learning_rate": 3.075247192894252e-06,
      "loss": 1.7937,
      "step": 1456000
    },
    {
      "epoch": 56.313570793208804,
      "grad_norm": 12.474615097045898,
      "learning_rate": 3.072024338992665e-06,
      "loss": 1.7615,
      "step": 1456100
    },
    {
      "epoch": 56.317438217890704,
      "grad_norm": 14.59939193725586,
      "learning_rate": 3.068801485091078e-06,
      "loss": 1.8373,
      "step": 1456200
    },
    {
      "epoch": 56.32130564257261,
      "grad_norm": 10.258516311645508,
      "learning_rate": 3.065578631189491e-06,
      "loss": 1.7979,
      "step": 1456300
    },
    {
      "epoch": 56.32517306725452,
      "grad_norm": 14.279889106750488,
      "learning_rate": 3.0623557772879045e-06,
      "loss": 1.727,
      "step": 1456400
    },
    {
      "epoch": 56.32904049193642,
      "grad_norm": 9.456317901611328,
      "learning_rate": 3.059132923386317e-06,
      "loss": 1.7315,
      "step": 1456500
    },
    {
      "epoch": 56.332907916618325,
      "grad_norm": 12.33102798461914,
      "learning_rate": 3.05591006948473e-06,
      "loss": 1.7732,
      "step": 1456600
    },
    {
      "epoch": 56.336775341300225,
      "grad_norm": 21.268592834472656,
      "learning_rate": 3.052687215583143e-06,
      "loss": 1.7384,
      "step": 1456700
    },
    {
      "epoch": 56.34064276598213,
      "grad_norm": 15.757286071777344,
      "learning_rate": 3.0494643616815566e-06,
      "loss": 1.7734,
      "step": 1456800
    },
    {
      "epoch": 56.34451019066404,
      "grad_norm": 10.774052619934082,
      "learning_rate": 3.0462415077799696e-06,
      "loss": 1.8321,
      "step": 1456900
    },
    {
      "epoch": 56.34837761534594,
      "grad_norm": 9.766436576843262,
      "learning_rate": 3.0430186538783826e-06,
      "loss": 1.7633,
      "step": 1457000
    },
    {
      "epoch": 56.352245040027846,
      "grad_norm": 16.86405372619629,
      "learning_rate": 3.0397957999767956e-06,
      "loss": 1.8177,
      "step": 1457100
    },
    {
      "epoch": 56.35611246470975,
      "grad_norm": 13.27717399597168,
      "learning_rate": 3.0365729460752087e-06,
      "loss": 1.7639,
      "step": 1457200
    },
    {
      "epoch": 56.35997988939165,
      "grad_norm": 15.564798355102539,
      "learning_rate": 3.0333500921736217e-06,
      "loss": 1.7102,
      "step": 1457300
    },
    {
      "epoch": 56.36384731407356,
      "grad_norm": 11.881753921508789,
      "learning_rate": 3.0301272382720347e-06,
      "loss": 1.7504,
      "step": 1457400
    },
    {
      "epoch": 56.36771473875546,
      "grad_norm": 13.549381256103516,
      "learning_rate": 3.0269043843704477e-06,
      "loss": 1.8662,
      "step": 1457500
    },
    {
      "epoch": 56.37158216343737,
      "grad_norm": 13.377103805541992,
      "learning_rate": 3.0236815304688608e-06,
      "loss": 1.8137,
      "step": 1457600
    },
    {
      "epoch": 56.375449588119274,
      "grad_norm": 12.57387924194336,
      "learning_rate": 3.0204586765672742e-06,
      "loss": 1.803,
      "step": 1457700
    },
    {
      "epoch": 56.379317012801174,
      "grad_norm": 13.862236022949219,
      "learning_rate": 3.0172358226656872e-06,
      "loss": 1.6957,
      "step": 1457800
    },
    {
      "epoch": 56.38318443748308,
      "grad_norm": 15.84546184539795,
      "learning_rate": 3.0140129687641003e-06,
      "loss": 1.6964,
      "step": 1457900
    },
    {
      "epoch": 56.38705186216498,
      "grad_norm": 17.340076446533203,
      "learning_rate": 3.010790114862513e-06,
      "loss": 1.7622,
      "step": 1458000
    },
    {
      "epoch": 56.39091928684689,
      "grad_norm": 12.563282012939453,
      "learning_rate": 3.0075672609609263e-06,
      "loss": 1.8013,
      "step": 1458100
    },
    {
      "epoch": 56.394786711528795,
      "grad_norm": 12.73156452178955,
      "learning_rate": 3.0043444070593393e-06,
      "loss": 1.7257,
      "step": 1458200
    },
    {
      "epoch": 56.398654136210695,
      "grad_norm": 15.062986373901367,
      "learning_rate": 3.0011215531577524e-06,
      "loss": 1.8512,
      "step": 1458300
    },
    {
      "epoch": 56.4025215608926,
      "grad_norm": 10.090434074401855,
      "learning_rate": 2.9978986992561654e-06,
      "loss": 1.8006,
      "step": 1458400
    },
    {
      "epoch": 56.40638898557451,
      "grad_norm": 11.01050090789795,
      "learning_rate": 2.9946758453545784e-06,
      "loss": 1.8246,
      "step": 1458500
    },
    {
      "epoch": 56.41025641025641,
      "grad_norm": 8.550976753234863,
      "learning_rate": 2.991452991452992e-06,
      "loss": 1.7471,
      "step": 1458600
    },
    {
      "epoch": 56.414123834938316,
      "grad_norm": 10.07262897491455,
      "learning_rate": 2.9882301375514045e-06,
      "loss": 1.7114,
      "step": 1458700
    },
    {
      "epoch": 56.417991259620216,
      "grad_norm": 10.903496742248535,
      "learning_rate": 2.9850072836498175e-06,
      "loss": 1.7747,
      "step": 1458800
    },
    {
      "epoch": 56.42185868430212,
      "grad_norm": 12.425941467285156,
      "learning_rate": 2.9817844297482305e-06,
      "loss": 1.7615,
      "step": 1458900
    },
    {
      "epoch": 56.42572610898403,
      "grad_norm": 11.627327919006348,
      "learning_rate": 2.978561575846644e-06,
      "loss": 1.7111,
      "step": 1459000
    },
    {
      "epoch": 56.42959353366593,
      "grad_norm": 11.614033699035645,
      "learning_rate": 2.975338721945057e-06,
      "loss": 1.7196,
      "step": 1459100
    },
    {
      "epoch": 56.43346095834784,
      "grad_norm": 11.532346725463867,
      "learning_rate": 2.97211586804347e-06,
      "loss": 1.8296,
      "step": 1459200
    },
    {
      "epoch": 56.437328383029744,
      "grad_norm": 10.960908889770508,
      "learning_rate": 2.968893014141883e-06,
      "loss": 1.7931,
      "step": 1459300
    },
    {
      "epoch": 56.441195807711644,
      "grad_norm": 14.336421012878418,
      "learning_rate": 2.9656701602402965e-06,
      "loss": 1.7456,
      "step": 1459400
    },
    {
      "epoch": 56.44506323239355,
      "grad_norm": 8.616092681884766,
      "learning_rate": 2.962447306338709e-06,
      "loss": 1.8033,
      "step": 1459500
    },
    {
      "epoch": 56.44893065707545,
      "grad_norm": 14.081612586975098,
      "learning_rate": 2.959224452437122e-06,
      "loss": 1.7589,
      "step": 1459600
    },
    {
      "epoch": 56.45279808175736,
      "grad_norm": 12.18377685546875,
      "learning_rate": 2.956001598535535e-06,
      "loss": 1.7999,
      "step": 1459700
    },
    {
      "epoch": 56.456665506439265,
      "grad_norm": 10.906332015991211,
      "learning_rate": 2.9527787446339486e-06,
      "loss": 1.6825,
      "step": 1459800
    },
    {
      "epoch": 56.460532931121165,
      "grad_norm": 12.733750343322754,
      "learning_rate": 2.9495558907323616e-06,
      "loss": 1.8089,
      "step": 1459900
    },
    {
      "epoch": 56.46440035580307,
      "grad_norm": 12.422065734863281,
      "learning_rate": 2.9463330368307746e-06,
      "loss": 1.8143,
      "step": 1460000
    },
    {
      "epoch": 56.46826778048497,
      "grad_norm": 15.864084243774414,
      "learning_rate": 2.9431101829291876e-06,
      "loss": 1.8025,
      "step": 1460100
    },
    {
      "epoch": 56.47213520516688,
      "grad_norm": 11.24868392944336,
      "learning_rate": 2.9398873290276007e-06,
      "loss": 1.7902,
      "step": 1460200
    },
    {
      "epoch": 56.476002629848786,
      "grad_norm": 13.136061668395996,
      "learning_rate": 2.9366644751260137e-06,
      "loss": 1.6222,
      "step": 1460300
    },
    {
      "epoch": 56.479870054530686,
      "grad_norm": 19.446592330932617,
      "learning_rate": 2.9334416212244267e-06,
      "loss": 1.7771,
      "step": 1460400
    },
    {
      "epoch": 56.48373747921259,
      "grad_norm": 20.04693031311035,
      "learning_rate": 2.9302187673228397e-06,
      "loss": 1.8295,
      "step": 1460500
    },
    {
      "epoch": 56.4876049038945,
      "grad_norm": 11.359211921691895,
      "learning_rate": 2.9269959134212528e-06,
      "loss": 1.7729,
      "step": 1460600
    },
    {
      "epoch": 56.4914723285764,
      "grad_norm": 9.816817283630371,
      "learning_rate": 2.9237730595196662e-06,
      "loss": 1.6789,
      "step": 1460700
    },
    {
      "epoch": 56.49533975325831,
      "grad_norm": 11.32325553894043,
      "learning_rate": 2.9205502056180792e-06,
      "loss": 1.7898,
      "step": 1460800
    },
    {
      "epoch": 56.49920717794021,
      "grad_norm": 13.619956016540527,
      "learning_rate": 2.9173273517164923e-06,
      "loss": 1.8027,
      "step": 1460900
    },
    {
      "epoch": 56.503074602622114,
      "grad_norm": 12.162675857543945,
      "learning_rate": 2.914104497814905e-06,
      "loss": 1.7607,
      "step": 1461000
    },
    {
      "epoch": 56.50694202730402,
      "grad_norm": 15.332483291625977,
      "learning_rate": 2.9108816439133183e-06,
      "loss": 1.8024,
      "step": 1461100
    },
    {
      "epoch": 56.51080945198592,
      "grad_norm": 16.06932258605957,
      "learning_rate": 2.9076587900117313e-06,
      "loss": 1.6999,
      "step": 1461200
    },
    {
      "epoch": 56.51467687666783,
      "grad_norm": 12.267035484313965,
      "learning_rate": 2.9044359361101444e-06,
      "loss": 1.763,
      "step": 1461300
    },
    {
      "epoch": 56.51854430134973,
      "grad_norm": 12.042790412902832,
      "learning_rate": 2.9012130822085574e-06,
      "loss": 1.7833,
      "step": 1461400
    },
    {
      "epoch": 56.522411726031635,
      "grad_norm": 14.836709976196289,
      "learning_rate": 2.897990228306971e-06,
      "loss": 1.7691,
      "step": 1461500
    },
    {
      "epoch": 56.52627915071354,
      "grad_norm": 11.748348236083984,
      "learning_rate": 2.894767374405384e-06,
      "loss": 1.768,
      "step": 1461600
    },
    {
      "epoch": 56.53014657539544,
      "grad_norm": 13.955110549926758,
      "learning_rate": 2.8915445205037965e-06,
      "loss": 1.7579,
      "step": 1461700
    },
    {
      "epoch": 56.53401400007735,
      "grad_norm": 13.610247611999512,
      "learning_rate": 2.8883216666022095e-06,
      "loss": 1.6863,
      "step": 1461800
    },
    {
      "epoch": 56.537881424759256,
      "grad_norm": 10.557548522949219,
      "learning_rate": 2.885098812700623e-06,
      "loss": 1.6151,
      "step": 1461900
    },
    {
      "epoch": 56.541748849441156,
      "grad_norm": 13.11409854888916,
      "learning_rate": 2.881875958799036e-06,
      "loss": 1.8253,
      "step": 1462000
    },
    {
      "epoch": 56.54561627412306,
      "grad_norm": 12.1467866897583,
      "learning_rate": 2.878653104897449e-06,
      "loss": 1.8282,
      "step": 1462100
    },
    {
      "epoch": 56.54948369880496,
      "grad_norm": 10.644318580627441,
      "learning_rate": 2.875430250995862e-06,
      "loss": 1.8204,
      "step": 1462200
    },
    {
      "epoch": 56.55335112348687,
      "grad_norm": 10.455322265625,
      "learning_rate": 2.872207397094275e-06,
      "loss": 1.8067,
      "step": 1462300
    },
    {
      "epoch": 56.55721854816878,
      "grad_norm": 19.676820755004883,
      "learning_rate": 2.868984543192688e-06,
      "loss": 1.7409,
      "step": 1462400
    },
    {
      "epoch": 56.56108597285068,
      "grad_norm": 17.227203369140625,
      "learning_rate": 2.865761689291101e-06,
      "loss": 1.7829,
      "step": 1462500
    },
    {
      "epoch": 56.564953397532584,
      "grad_norm": 13.542972564697266,
      "learning_rate": 2.862538835389514e-06,
      "loss": 1.7539,
      "step": 1462600
    },
    {
      "epoch": 56.56882082221449,
      "grad_norm": 13.026914596557617,
      "learning_rate": 2.859315981487927e-06,
      "loss": 1.756,
      "step": 1462700
    },
    {
      "epoch": 56.57268824689639,
      "grad_norm": 12.060192108154297,
      "learning_rate": 2.8560931275863406e-06,
      "loss": 1.7482,
      "step": 1462800
    },
    {
      "epoch": 56.5765556715783,
      "grad_norm": 13.052177429199219,
      "learning_rate": 2.8528702736847536e-06,
      "loss": 1.8292,
      "step": 1462900
    },
    {
      "epoch": 56.5804230962602,
      "grad_norm": 12.039582252502441,
      "learning_rate": 2.8496474197831666e-06,
      "loss": 1.7406,
      "step": 1463000
    },
    {
      "epoch": 56.584290520942105,
      "grad_norm": 14.257843971252441,
      "learning_rate": 2.8464245658815796e-06,
      "loss": 1.8257,
      "step": 1463100
    },
    {
      "epoch": 56.58815794562401,
      "grad_norm": 11.80867862701416,
      "learning_rate": 2.8432017119799927e-06,
      "loss": 1.776,
      "step": 1463200
    },
    {
      "epoch": 56.59202537030591,
      "grad_norm": 12.945752143859863,
      "learning_rate": 2.8399788580784057e-06,
      "loss": 1.7476,
      "step": 1463300
    },
    {
      "epoch": 56.59589279498782,
      "grad_norm": 11.500782012939453,
      "learning_rate": 2.8367560041768187e-06,
      "loss": 1.798,
      "step": 1463400
    },
    {
      "epoch": 56.59976021966972,
      "grad_norm": 12.101377487182617,
      "learning_rate": 2.8335331502752317e-06,
      "loss": 1.7335,
      "step": 1463500
    },
    {
      "epoch": 56.603627644351626,
      "grad_norm": 15.297269821166992,
      "learning_rate": 2.8303102963736448e-06,
      "loss": 1.7252,
      "step": 1463600
    },
    {
      "epoch": 56.60749506903353,
      "grad_norm": 13.003724098205566,
      "learning_rate": 2.8270874424720582e-06,
      "loss": 1.7451,
      "step": 1463700
    },
    {
      "epoch": 56.61136249371543,
      "grad_norm": 17.422080993652344,
      "learning_rate": 2.8238645885704712e-06,
      "loss": 1.754,
      "step": 1463800
    },
    {
      "epoch": 56.61522991839734,
      "grad_norm": 11.293266296386719,
      "learning_rate": 2.820641734668884e-06,
      "loss": 1.7525,
      "step": 1463900
    },
    {
      "epoch": 56.61909734307925,
      "grad_norm": 13.453927993774414,
      "learning_rate": 2.8174188807672973e-06,
      "loss": 1.7338,
      "step": 1464000
    },
    {
      "epoch": 56.62296476776115,
      "grad_norm": 12.346810340881348,
      "learning_rate": 2.8141960268657103e-06,
      "loss": 1.8118,
      "step": 1464100
    },
    {
      "epoch": 56.626832192443054,
      "grad_norm": 14.4017333984375,
      "learning_rate": 2.8109731729641233e-06,
      "loss": 1.8075,
      "step": 1464200
    },
    {
      "epoch": 56.630699617124954,
      "grad_norm": 10.019025802612305,
      "learning_rate": 2.8077503190625364e-06,
      "loss": 1.671,
      "step": 1464300
    },
    {
      "epoch": 56.63456704180686,
      "grad_norm": 6.96505880355835,
      "learning_rate": 2.8045274651609494e-06,
      "loss": 1.8082,
      "step": 1464400
    },
    {
      "epoch": 56.63843446648877,
      "grad_norm": 10.320178031921387,
      "learning_rate": 2.801304611259363e-06,
      "loss": 1.7128,
      "step": 1464500
    },
    {
      "epoch": 56.64230189117067,
      "grad_norm": 14.486790657043457,
      "learning_rate": 2.7980817573577754e-06,
      "loss": 1.8127,
      "step": 1464600
    },
    {
      "epoch": 56.646169315852575,
      "grad_norm": 12.081387519836426,
      "learning_rate": 2.7948589034561885e-06,
      "loss": 1.774,
      "step": 1464700
    },
    {
      "epoch": 56.650036740534475,
      "grad_norm": 11.60936164855957,
      "learning_rate": 2.7916360495546015e-06,
      "loss": 1.7849,
      "step": 1464800
    },
    {
      "epoch": 56.65390416521638,
      "grad_norm": 13.953619003295898,
      "learning_rate": 2.788413195653015e-06,
      "loss": 1.8701,
      "step": 1464900
    },
    {
      "epoch": 56.65777158989829,
      "grad_norm": 14.978863716125488,
      "learning_rate": 2.785190341751428e-06,
      "loss": 1.7948,
      "step": 1465000
    },
    {
      "epoch": 56.66163901458019,
      "grad_norm": 13.032431602478027,
      "learning_rate": 2.781967487849841e-06,
      "loss": 1.7418,
      "step": 1465100
    },
    {
      "epoch": 56.665506439262096,
      "grad_norm": 11.380794525146484,
      "learning_rate": 2.778744633948254e-06,
      "loss": 1.747,
      "step": 1465200
    },
    {
      "epoch": 56.669373863944,
      "grad_norm": 11.398351669311523,
      "learning_rate": 2.775521780046667e-06,
      "loss": 1.8335,
      "step": 1465300
    },
    {
      "epoch": 56.6732412886259,
      "grad_norm": 13.410728454589844,
      "learning_rate": 2.77229892614508e-06,
      "loss": 1.6934,
      "step": 1465400
    },
    {
      "epoch": 56.67710871330781,
      "grad_norm": 8.953217506408691,
      "learning_rate": 2.769076072243493e-06,
      "loss": 1.7128,
      "step": 1465500
    },
    {
      "epoch": 56.68097613798971,
      "grad_norm": 11.565899848937988,
      "learning_rate": 2.765853218341906e-06,
      "loss": 1.7499,
      "step": 1465600
    },
    {
      "epoch": 56.68484356267162,
      "grad_norm": 12.145441055297852,
      "learning_rate": 2.762630364440319e-06,
      "loss": 1.833,
      "step": 1465700
    },
    {
      "epoch": 56.688710987353524,
      "grad_norm": 14.726686477661133,
      "learning_rate": 2.7594075105387326e-06,
      "loss": 1.7574,
      "step": 1465800
    },
    {
      "epoch": 56.692578412035424,
      "grad_norm": 11.848028182983398,
      "learning_rate": 2.7561846566371456e-06,
      "loss": 1.7236,
      "step": 1465900
    },
    {
      "epoch": 56.69644583671733,
      "grad_norm": 11.945123672485352,
      "learning_rate": 2.7529618027355586e-06,
      "loss": 1.8146,
      "step": 1466000
    },
    {
      "epoch": 56.70031326139923,
      "grad_norm": 11.036459922790527,
      "learning_rate": 2.7497389488339712e-06,
      "loss": 1.7005,
      "step": 1466100
    },
    {
      "epoch": 56.70418068608114,
      "grad_norm": 12.704421997070312,
      "learning_rate": 2.7465160949323847e-06,
      "loss": 1.7508,
      "step": 1466200
    },
    {
      "epoch": 56.708048110763045,
      "grad_norm": 10.34304141998291,
      "learning_rate": 2.7432932410307977e-06,
      "loss": 1.8042,
      "step": 1466300
    },
    {
      "epoch": 56.711915535444945,
      "grad_norm": 14.381312370300293,
      "learning_rate": 2.7400703871292107e-06,
      "loss": 1.7419,
      "step": 1466400
    },
    {
      "epoch": 56.71578296012685,
      "grad_norm": 11.93026351928711,
      "learning_rate": 2.7368475332276237e-06,
      "loss": 1.6832,
      "step": 1466500
    },
    {
      "epoch": 56.71965038480876,
      "grad_norm": 13.201955795288086,
      "learning_rate": 2.733624679326037e-06,
      "loss": 1.802,
      "step": 1466600
    },
    {
      "epoch": 56.72351780949066,
      "grad_norm": 11.504955291748047,
      "learning_rate": 2.7304018254244502e-06,
      "loss": 1.7429,
      "step": 1466700
    },
    {
      "epoch": 56.727385234172566,
      "grad_norm": 12.315654754638672,
      "learning_rate": 2.7271789715228632e-06,
      "loss": 1.7274,
      "step": 1466800
    },
    {
      "epoch": 56.731252658854466,
      "grad_norm": 13.321191787719727,
      "learning_rate": 2.723956117621276e-06,
      "loss": 1.7632,
      "step": 1466900
    },
    {
      "epoch": 56.73512008353637,
      "grad_norm": 15.049320220947266,
      "learning_rate": 2.7207332637196893e-06,
      "loss": 1.7113,
      "step": 1467000
    },
    {
      "epoch": 56.73898750821828,
      "grad_norm": 10.855801582336426,
      "learning_rate": 2.7175104098181023e-06,
      "loss": 1.8187,
      "step": 1467100
    },
    {
      "epoch": 56.74285493290018,
      "grad_norm": 12.844284057617188,
      "learning_rate": 2.7142875559165153e-06,
      "loss": 1.7412,
      "step": 1467200
    },
    {
      "epoch": 56.74672235758209,
      "grad_norm": 11.7100191116333,
      "learning_rate": 2.7110647020149284e-06,
      "loss": 1.7537,
      "step": 1467300
    },
    {
      "epoch": 56.750589782263994,
      "grad_norm": 16.01280403137207,
      "learning_rate": 2.7078418481133414e-06,
      "loss": 1.8022,
      "step": 1467400
    },
    {
      "epoch": 56.754457206945894,
      "grad_norm": 11.5223970413208,
      "learning_rate": 2.704618994211755e-06,
      "loss": 1.7929,
      "step": 1467500
    },
    {
      "epoch": 56.7583246316278,
      "grad_norm": 3.1728029251098633,
      "learning_rate": 2.7013961403101674e-06,
      "loss": 1.75,
      "step": 1467600
    },
    {
      "epoch": 56.7621920563097,
      "grad_norm": 12.811116218566895,
      "learning_rate": 2.6981732864085805e-06,
      "loss": 1.8142,
      "step": 1467700
    },
    {
      "epoch": 56.76605948099161,
      "grad_norm": 17.80108070373535,
      "learning_rate": 2.6949504325069935e-06,
      "loss": 1.7644,
      "step": 1467800
    },
    {
      "epoch": 56.769926905673515,
      "grad_norm": 11.03990364074707,
      "learning_rate": 2.691727578605407e-06,
      "loss": 1.8333,
      "step": 1467900
    },
    {
      "epoch": 56.773794330355415,
      "grad_norm": 13.653130531311035,
      "learning_rate": 2.68850472470382e-06,
      "loss": 1.8647,
      "step": 1468000
    },
    {
      "epoch": 56.77766175503732,
      "grad_norm": 9.994874000549316,
      "learning_rate": 2.685281870802233e-06,
      "loss": 1.7391,
      "step": 1468100
    },
    {
      "epoch": 56.78152917971922,
      "grad_norm": 12.43333625793457,
      "learning_rate": 2.682059016900646e-06,
      "loss": 1.7452,
      "step": 1468200
    },
    {
      "epoch": 56.78539660440113,
      "grad_norm": 14.992788314819336,
      "learning_rate": 2.678836162999059e-06,
      "loss": 1.7677,
      "step": 1468300
    },
    {
      "epoch": 56.789264029083036,
      "grad_norm": 14.515074729919434,
      "learning_rate": 2.675613309097472e-06,
      "loss": 1.8635,
      "step": 1468400
    },
    {
      "epoch": 56.793131453764936,
      "grad_norm": 19.460920333862305,
      "learning_rate": 2.672390455195885e-06,
      "loss": 1.8112,
      "step": 1468500
    },
    {
      "epoch": 56.79699887844684,
      "grad_norm": 11.000142097473145,
      "learning_rate": 2.669167601294298e-06,
      "loss": 1.7899,
      "step": 1468600
    },
    {
      "epoch": 56.80086630312875,
      "grad_norm": 13.792076110839844,
      "learning_rate": 2.6659447473927115e-06,
      "loss": 1.6593,
      "step": 1468700
    },
    {
      "epoch": 56.80473372781065,
      "grad_norm": 12.471864700317383,
      "learning_rate": 2.6627218934911246e-06,
      "loss": 1.6657,
      "step": 1468800
    },
    {
      "epoch": 56.80860115249256,
      "grad_norm": 10.361978530883789,
      "learning_rate": 2.6594990395895376e-06,
      "loss": 1.8798,
      "step": 1468900
    },
    {
      "epoch": 56.81246857717446,
      "grad_norm": 14.574631690979004,
      "learning_rate": 2.6562761856879506e-06,
      "loss": 1.7487,
      "step": 1469000
    },
    {
      "epoch": 56.816336001856364,
      "grad_norm": 11.271883964538574,
      "learning_rate": 2.6530533317863636e-06,
      "loss": 1.7839,
      "step": 1469100
    },
    {
      "epoch": 56.82020342653827,
      "grad_norm": 10.74767017364502,
      "learning_rate": 2.6498304778847767e-06,
      "loss": 1.7175,
      "step": 1469200
    },
    {
      "epoch": 56.82407085122017,
      "grad_norm": 13.296945571899414,
      "learning_rate": 2.6466076239831897e-06,
      "loss": 1.842,
      "step": 1469300
    },
    {
      "epoch": 56.82793827590208,
      "grad_norm": 10.295055389404297,
      "learning_rate": 2.6433847700816027e-06,
      "loss": 1.7151,
      "step": 1469400
    },
    {
      "epoch": 56.831805700583985,
      "grad_norm": 14.80812931060791,
      "learning_rate": 2.6401619161800157e-06,
      "loss": 1.8223,
      "step": 1469500
    },
    {
      "epoch": 56.835673125265885,
      "grad_norm": 14.87075138092041,
      "learning_rate": 2.636939062278429e-06,
      "loss": 1.7174,
      "step": 1469600
    },
    {
      "epoch": 56.83954054994779,
      "grad_norm": 14.471831321716309,
      "learning_rate": 2.6337162083768422e-06,
      "loss": 1.8396,
      "step": 1469700
    },
    {
      "epoch": 56.84340797462969,
      "grad_norm": 13.112441062927246,
      "learning_rate": 2.630493354475255e-06,
      "loss": 1.7819,
      "step": 1469800
    },
    {
      "epoch": 56.8472753993116,
      "grad_norm": 11.298707962036133,
      "learning_rate": 2.627270500573668e-06,
      "loss": 1.7676,
      "step": 1469900
    },
    {
      "epoch": 56.851142823993506,
      "grad_norm": 12.826746940612793,
      "learning_rate": 2.6240476466720813e-06,
      "loss": 1.7345,
      "step": 1470000
    },
    {
      "epoch": 56.855010248675406,
      "grad_norm": 11.545025825500488,
      "learning_rate": 2.6208247927704943e-06,
      "loss": 1.7749,
      "step": 1470100
    },
    {
      "epoch": 56.85887767335731,
      "grad_norm": 12.922104835510254,
      "learning_rate": 2.6176019388689073e-06,
      "loss": 1.679,
      "step": 1470200
    },
    {
      "epoch": 56.86274509803921,
      "grad_norm": 12.490913391113281,
      "learning_rate": 2.6143790849673204e-06,
      "loss": 1.7761,
      "step": 1470300
    },
    {
      "epoch": 56.86661252272112,
      "grad_norm": 15.811981201171875,
      "learning_rate": 2.6111562310657334e-06,
      "loss": 1.7628,
      "step": 1470400
    },
    {
      "epoch": 56.87047994740303,
      "grad_norm": 13.75168514251709,
      "learning_rate": 2.607933377164147e-06,
      "loss": 1.7829,
      "step": 1470500
    },
    {
      "epoch": 56.87434737208493,
      "grad_norm": 13.909109115600586,
      "learning_rate": 2.6047105232625594e-06,
      "loss": 1.8794,
      "step": 1470600
    },
    {
      "epoch": 56.878214796766834,
      "grad_norm": 12.173370361328125,
      "learning_rate": 2.6014876693609725e-06,
      "loss": 1.7664,
      "step": 1470700
    },
    {
      "epoch": 56.88208222144874,
      "grad_norm": 15.026314735412598,
      "learning_rate": 2.5982648154593855e-06,
      "loss": 1.7674,
      "step": 1470800
    },
    {
      "epoch": 56.88594964613064,
      "grad_norm": 13.699912071228027,
      "learning_rate": 2.595041961557799e-06,
      "loss": 1.8064,
      "step": 1470900
    },
    {
      "epoch": 56.88981707081255,
      "grad_norm": 12.544690132141113,
      "learning_rate": 2.591819107656212e-06,
      "loss": 1.7334,
      "step": 1471000
    },
    {
      "epoch": 56.89368449549445,
      "grad_norm": 14.000018119812012,
      "learning_rate": 2.588596253754625e-06,
      "loss": 1.7782,
      "step": 1471100
    },
    {
      "epoch": 56.897551920176355,
      "grad_norm": 14.762595176696777,
      "learning_rate": 2.585373399853038e-06,
      "loss": 1.742,
      "step": 1471200
    },
    {
      "epoch": 56.90141934485826,
      "grad_norm": 13.441641807556152,
      "learning_rate": 2.582150545951451e-06,
      "loss": 1.6652,
      "step": 1471300
    },
    {
      "epoch": 56.90528676954016,
      "grad_norm": 12.445173263549805,
      "learning_rate": 2.578927692049864e-06,
      "loss": 1.7325,
      "step": 1471400
    },
    {
      "epoch": 56.90915419422207,
      "grad_norm": 16.131595611572266,
      "learning_rate": 2.575704838148277e-06,
      "loss": 1.7795,
      "step": 1471500
    },
    {
      "epoch": 56.91302161890397,
      "grad_norm": 11.150794982910156,
      "learning_rate": 2.57248198424669e-06,
      "loss": 1.6987,
      "step": 1471600
    },
    {
      "epoch": 56.916889043585876,
      "grad_norm": 10.8482084274292,
      "learning_rate": 2.5692591303451035e-06,
      "loss": 1.7802,
      "step": 1471700
    },
    {
      "epoch": 56.92075646826778,
      "grad_norm": 18.147571563720703,
      "learning_rate": 2.5660362764435166e-06,
      "loss": 1.7355,
      "step": 1471800
    },
    {
      "epoch": 56.92462389294968,
      "grad_norm": 12.980368614196777,
      "learning_rate": 2.5628134225419296e-06,
      "loss": 1.7767,
      "step": 1471900
    },
    {
      "epoch": 56.92849131763159,
      "grad_norm": 14.269923210144043,
      "learning_rate": 2.559590568640342e-06,
      "loss": 1.7447,
      "step": 1472000
    },
    {
      "epoch": 56.9323587423135,
      "grad_norm": 13.299859046936035,
      "learning_rate": 2.5563677147387556e-06,
      "loss": 1.7904,
      "step": 1472100
    },
    {
      "epoch": 56.9362261669954,
      "grad_norm": 11.906378746032715,
      "learning_rate": 2.5531448608371687e-06,
      "loss": 1.7552,
      "step": 1472200
    },
    {
      "epoch": 56.940093591677304,
      "grad_norm": 12.10067367553711,
      "learning_rate": 2.5499220069355817e-06,
      "loss": 1.6829,
      "step": 1472300
    },
    {
      "epoch": 56.943961016359204,
      "grad_norm": 11.612959861755371,
      "learning_rate": 2.5466991530339947e-06,
      "loss": 1.7981,
      "step": 1472400
    },
    {
      "epoch": 56.94782844104111,
      "grad_norm": 11.29985523223877,
      "learning_rate": 2.5434762991324077e-06,
      "loss": 1.7074,
      "step": 1472500
    },
    {
      "epoch": 56.95169586572302,
      "grad_norm": 10.772992134094238,
      "learning_rate": 2.540253445230821e-06,
      "loss": 1.802,
      "step": 1472600
    },
    {
      "epoch": 56.95556329040492,
      "grad_norm": 12.183570861816406,
      "learning_rate": 2.5370305913292342e-06,
      "loss": 1.7025,
      "step": 1472700
    },
    {
      "epoch": 56.959430715086825,
      "grad_norm": 14.245054244995117,
      "learning_rate": 2.533807737427647e-06,
      "loss": 1.8162,
      "step": 1472800
    },
    {
      "epoch": 56.963298139768725,
      "grad_norm": 16.76199722290039,
      "learning_rate": 2.53058488352606e-06,
      "loss": 1.7969,
      "step": 1472900
    },
    {
      "epoch": 56.96716556445063,
      "grad_norm": 16.44721221923828,
      "learning_rate": 2.5273620296244733e-06,
      "loss": 1.8378,
      "step": 1473000
    },
    {
      "epoch": 56.97103298913254,
      "grad_norm": 12.770987510681152,
      "learning_rate": 2.5241391757228863e-06,
      "loss": 1.8361,
      "step": 1473100
    },
    {
      "epoch": 56.97490041381444,
      "grad_norm": 12.70578384399414,
      "learning_rate": 2.5209163218212993e-06,
      "loss": 1.7891,
      "step": 1473200
    },
    {
      "epoch": 56.978767838496346,
      "grad_norm": 15.63721752166748,
      "learning_rate": 2.5176934679197124e-06,
      "loss": 1.7558,
      "step": 1473300
    },
    {
      "epoch": 56.98263526317825,
      "grad_norm": 14.258934020996094,
      "learning_rate": 2.514470614018126e-06,
      "loss": 1.8234,
      "step": 1473400
    },
    {
      "epoch": 56.98650268786015,
      "grad_norm": 7.614616394042969,
      "learning_rate": 2.5112477601165384e-06,
      "loss": 1.7246,
      "step": 1473500
    },
    {
      "epoch": 56.99037011254206,
      "grad_norm": 17.452302932739258,
      "learning_rate": 2.5080249062149514e-06,
      "loss": 1.7553,
      "step": 1473600
    },
    {
      "epoch": 56.99423753722396,
      "grad_norm": 6.236837387084961,
      "learning_rate": 2.5048020523133645e-06,
      "loss": 1.762,
      "step": 1473700
    },
    {
      "epoch": 56.99810496190587,
      "grad_norm": 13.121986389160156,
      "learning_rate": 2.501579198411778e-06,
      "loss": 1.6949,
      "step": 1473800
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.736085295677185,
      "eval_runtime": 2.8206,
      "eval_samples_per_second": 482.528,
      "eval_steps_per_second": 482.528,
      "step": 1473849
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.5663163661956787,
      "eval_runtime": 55.6287,
      "eval_samples_per_second": 464.814,
      "eval_steps_per_second": 464.814,
      "step": 1473849
    },
    {
      "epoch": 57.001972386587774,
      "grad_norm": 16.147132873535156,
      "learning_rate": 2.498356344510191e-06,
      "loss": 1.7306,
      "step": 1473900
    },
    {
      "epoch": 57.005839811269674,
      "grad_norm": 10.868666648864746,
      "learning_rate": 2.495133490608604e-06,
      "loss": 1.7454,
      "step": 1474000
    },
    {
      "epoch": 57.00970723595158,
      "grad_norm": 14.931415557861328,
      "learning_rate": 2.491910636707017e-06,
      "loss": 1.7992,
      "step": 1474100
    },
    {
      "epoch": 57.01357466063349,
      "grad_norm": 11.865175247192383,
      "learning_rate": 2.48868778280543e-06,
      "loss": 1.817,
      "step": 1474200
    },
    {
      "epoch": 57.01744208531539,
      "grad_norm": 9.824352264404297,
      "learning_rate": 2.485464928903843e-06,
      "loss": 1.8518,
      "step": 1474300
    },
    {
      "epoch": 57.021309509997295,
      "grad_norm": 11.702325820922852,
      "learning_rate": 2.482242075002256e-06,
      "loss": 1.7531,
      "step": 1474400
    },
    {
      "epoch": 57.025176934679195,
      "grad_norm": 11.414217948913574,
      "learning_rate": 2.479019221100669e-06,
      "loss": 1.7339,
      "step": 1474500
    },
    {
      "epoch": 57.0290443593611,
      "grad_norm": 13.948732376098633,
      "learning_rate": 2.475796367199082e-06,
      "loss": 1.7734,
      "step": 1474600
    },
    {
      "epoch": 57.03291178404301,
      "grad_norm": 12.352262496948242,
      "learning_rate": 2.4725735132974955e-06,
      "loss": 1.7977,
      "step": 1474700
    },
    {
      "epoch": 57.03677920872491,
      "grad_norm": 15.349311828613281,
      "learning_rate": 2.4693506593959086e-06,
      "loss": 1.8201,
      "step": 1474800
    },
    {
      "epoch": 57.040646633406816,
      "grad_norm": 13.602495193481445,
      "learning_rate": 2.4661278054943216e-06,
      "loss": 1.7919,
      "step": 1474900
    },
    {
      "epoch": 57.044514058088716,
      "grad_norm": 14.581683158874512,
      "learning_rate": 2.462904951592734e-06,
      "loss": 1.7892,
      "step": 1475000
    },
    {
      "epoch": 57.04838148277062,
      "grad_norm": 12.69848918914795,
      "learning_rate": 2.4596820976911476e-06,
      "loss": 1.683,
      "step": 1475100
    },
    {
      "epoch": 57.05224890745253,
      "grad_norm": 13.621903419494629,
      "learning_rate": 2.4564592437895607e-06,
      "loss": 1.8833,
      "step": 1475200
    },
    {
      "epoch": 57.05611633213443,
      "grad_norm": 12.755743026733398,
      "learning_rate": 2.4532363898879737e-06,
      "loss": 1.7822,
      "step": 1475300
    },
    {
      "epoch": 57.05998375681634,
      "grad_norm": 18.042104721069336,
      "learning_rate": 2.4500135359863867e-06,
      "loss": 1.6986,
      "step": 1475400
    },
    {
      "epoch": 57.063851181498244,
      "grad_norm": 12.59714126586914,
      "learning_rate": 2.4467906820847997e-06,
      "loss": 1.8489,
      "step": 1475500
    },
    {
      "epoch": 57.067718606180144,
      "grad_norm": 11.787662506103516,
      "learning_rate": 2.443567828183213e-06,
      "loss": 1.752,
      "step": 1475600
    },
    {
      "epoch": 57.07158603086205,
      "grad_norm": 12.157357215881348,
      "learning_rate": 2.440344974281626e-06,
      "loss": 1.6731,
      "step": 1475700
    },
    {
      "epoch": 57.07545345554395,
      "grad_norm": 12.411126136779785,
      "learning_rate": 2.437122120380039e-06,
      "loss": 1.8514,
      "step": 1475800
    },
    {
      "epoch": 57.07932088022586,
      "grad_norm": 11.75812816619873,
      "learning_rate": 2.433899266478452e-06,
      "loss": 1.7032,
      "step": 1475900
    },
    {
      "epoch": 57.083188304907765,
      "grad_norm": 11.360275268554688,
      "learning_rate": 2.4306764125768653e-06,
      "loss": 1.8004,
      "step": 1476000
    },
    {
      "epoch": 57.087055729589665,
      "grad_norm": 16.53725814819336,
      "learning_rate": 2.4274535586752783e-06,
      "loss": 1.7659,
      "step": 1476100
    },
    {
      "epoch": 57.09092315427157,
      "grad_norm": 16.196928024291992,
      "learning_rate": 2.4242307047736913e-06,
      "loss": 1.6958,
      "step": 1476200
    },
    {
      "epoch": 57.09479057895347,
      "grad_norm": 10.638442039489746,
      "learning_rate": 2.4210078508721044e-06,
      "loss": 1.7198,
      "step": 1476300
    },
    {
      "epoch": 57.09865800363538,
      "grad_norm": 11.729133605957031,
      "learning_rate": 2.417784996970518e-06,
      "loss": 1.7616,
      "step": 1476400
    },
    {
      "epoch": 57.102525428317286,
      "grad_norm": 12.948352813720703,
      "learning_rate": 2.4145621430689304e-06,
      "loss": 1.7521,
      "step": 1476500
    },
    {
      "epoch": 57.106392852999186,
      "grad_norm": 12.2743501663208,
      "learning_rate": 2.4113392891673434e-06,
      "loss": 1.8892,
      "step": 1476600
    },
    {
      "epoch": 57.11026027768109,
      "grad_norm": 14.872983932495117,
      "learning_rate": 2.4081164352657565e-06,
      "loss": 1.7235,
      "step": 1476700
    },
    {
      "epoch": 57.114127702363,
      "grad_norm": 15.758549690246582,
      "learning_rate": 2.40489358136417e-06,
      "loss": 1.7515,
      "step": 1476800
    },
    {
      "epoch": 57.1179951270449,
      "grad_norm": 18.573902130126953,
      "learning_rate": 2.401670727462583e-06,
      "loss": 1.7986,
      "step": 1476900
    },
    {
      "epoch": 57.12186255172681,
      "grad_norm": 11.66939640045166,
      "learning_rate": 2.398447873560996e-06,
      "loss": 1.6645,
      "step": 1477000
    },
    {
      "epoch": 57.12572997640871,
      "grad_norm": 6.493224620819092,
      "learning_rate": 2.395225019659409e-06,
      "loss": 1.6726,
      "step": 1477100
    },
    {
      "epoch": 57.129597401090614,
      "grad_norm": 13.408535957336426,
      "learning_rate": 2.392002165757822e-06,
      "loss": 1.7821,
      "step": 1477200
    },
    {
      "epoch": 57.13346482577252,
      "grad_norm": 12.252734184265137,
      "learning_rate": 2.388779311856235e-06,
      "loss": 1.8047,
      "step": 1477300
    },
    {
      "epoch": 57.13733225045442,
      "grad_norm": 18.945837020874023,
      "learning_rate": 2.385556457954648e-06,
      "loss": 1.7931,
      "step": 1477400
    },
    {
      "epoch": 57.14119967513633,
      "grad_norm": 16.748966217041016,
      "learning_rate": 2.382333604053061e-06,
      "loss": 1.8408,
      "step": 1477500
    },
    {
      "epoch": 57.14506709981823,
      "grad_norm": 11.922602653503418,
      "learning_rate": 2.379110750151474e-06,
      "loss": 1.7056,
      "step": 1477600
    },
    {
      "epoch": 57.148934524500135,
      "grad_norm": 11.239411354064941,
      "learning_rate": 2.3758878962498876e-06,
      "loss": 1.6839,
      "step": 1477700
    },
    {
      "epoch": 57.15280194918204,
      "grad_norm": 12.208231925964355,
      "learning_rate": 2.3726650423483006e-06,
      "loss": 1.7777,
      "step": 1477800
    },
    {
      "epoch": 57.15666937386394,
      "grad_norm": 9.664615631103516,
      "learning_rate": 2.369442188446713e-06,
      "loss": 1.7319,
      "step": 1477900
    },
    {
      "epoch": 57.16053679854585,
      "grad_norm": 12.975350379943848,
      "learning_rate": 2.366219334545126e-06,
      "loss": 1.8059,
      "step": 1478000
    },
    {
      "epoch": 57.164404223227756,
      "grad_norm": 16.09931755065918,
      "learning_rate": 2.3629964806435396e-06,
      "loss": 1.7217,
      "step": 1478100
    },
    {
      "epoch": 57.168271647909656,
      "grad_norm": 11.091346740722656,
      "learning_rate": 2.3597736267419527e-06,
      "loss": 1.7954,
      "step": 1478200
    },
    {
      "epoch": 57.17213907259156,
      "grad_norm": 11.960286140441895,
      "learning_rate": 2.3565507728403657e-06,
      "loss": 1.6972,
      "step": 1478300
    },
    {
      "epoch": 57.17600649727346,
      "grad_norm": 11.952728271484375,
      "learning_rate": 2.3533279189387787e-06,
      "loss": 1.7576,
      "step": 1478400
    },
    {
      "epoch": 57.17987392195537,
      "grad_norm": 11.501974105834961,
      "learning_rate": 2.350105065037192e-06,
      "loss": 1.8108,
      "step": 1478500
    },
    {
      "epoch": 57.18374134663728,
      "grad_norm": 11.37645149230957,
      "learning_rate": 2.346882211135605e-06,
      "loss": 1.7016,
      "step": 1478600
    },
    {
      "epoch": 57.18760877131918,
      "grad_norm": 11.39609432220459,
      "learning_rate": 2.343659357234018e-06,
      "loss": 1.7278,
      "step": 1478700
    },
    {
      "epoch": 57.191476196001084,
      "grad_norm": 9.597518920898438,
      "learning_rate": 2.340436503332431e-06,
      "loss": 1.7949,
      "step": 1478800
    },
    {
      "epoch": 57.19534362068299,
      "grad_norm": 15.592405319213867,
      "learning_rate": 2.3372136494308443e-06,
      "loss": 1.8324,
      "step": 1478900
    },
    {
      "epoch": 57.19921104536489,
      "grad_norm": 13.117836952209473,
      "learning_rate": 2.3339907955292573e-06,
      "loss": 1.7753,
      "step": 1479000
    },
    {
      "epoch": 57.2030784700468,
      "grad_norm": 14.54638957977295,
      "learning_rate": 2.3307679416276703e-06,
      "loss": 1.8524,
      "step": 1479100
    },
    {
      "epoch": 57.2069458947287,
      "grad_norm": 11.298063278198242,
      "learning_rate": 2.3275450877260833e-06,
      "loss": 1.8043,
      "step": 1479200
    },
    {
      "epoch": 57.210813319410605,
      "grad_norm": 11.620079040527344,
      "learning_rate": 2.3243222338244964e-06,
      "loss": 1.7389,
      "step": 1479300
    },
    {
      "epoch": 57.21468074409251,
      "grad_norm": 13.480389595031738,
      "learning_rate": 2.3210993799229094e-06,
      "loss": 1.7205,
      "step": 1479400
    },
    {
      "epoch": 57.21854816877441,
      "grad_norm": 18.941904067993164,
      "learning_rate": 2.3178765260213224e-06,
      "loss": 1.7799,
      "step": 1479500
    },
    {
      "epoch": 57.22241559345632,
      "grad_norm": 17.158597946166992,
      "learning_rate": 2.3146536721197354e-06,
      "loss": 1.7175,
      "step": 1479600
    },
    {
      "epoch": 57.22628301813822,
      "grad_norm": 12.441545486450195,
      "learning_rate": 2.3114308182181485e-06,
      "loss": 1.8258,
      "step": 1479700
    },
    {
      "epoch": 57.230150442820126,
      "grad_norm": 11.870674133300781,
      "learning_rate": 2.308207964316562e-06,
      "loss": 1.8339,
      "step": 1479800
    },
    {
      "epoch": 57.23401786750203,
      "grad_norm": 13.054170608520508,
      "learning_rate": 2.304985110414975e-06,
      "loss": 1.6623,
      "step": 1479900
    },
    {
      "epoch": 57.23788529218393,
      "grad_norm": 11.744939804077148,
      "learning_rate": 2.301762256513388e-06,
      "loss": 1.7374,
      "step": 1480000
    },
    {
      "epoch": 57.24175271686584,
      "grad_norm": 14.794344902038574,
      "learning_rate": 2.2985394026118006e-06,
      "loss": 1.7455,
      "step": 1480100
    },
    {
      "epoch": 57.24562014154775,
      "grad_norm": 11.998922348022461,
      "learning_rate": 2.295316548710214e-06,
      "loss": 1.7929,
      "step": 1480200
    },
    {
      "epoch": 57.24948756622965,
      "grad_norm": 12.601815223693848,
      "learning_rate": 2.292093694808627e-06,
      "loss": 1.8108,
      "step": 1480300
    },
    {
      "epoch": 57.253354990911554,
      "grad_norm": 13.182451248168945,
      "learning_rate": 2.28887084090704e-06,
      "loss": 1.7005,
      "step": 1480400
    },
    {
      "epoch": 57.25722241559345,
      "grad_norm": 10.91770076751709,
      "learning_rate": 2.285647987005453e-06,
      "loss": 1.8576,
      "step": 1480500
    },
    {
      "epoch": 57.26108984027536,
      "grad_norm": 13.126187324523926,
      "learning_rate": 2.282425133103866e-06,
      "loss": 1.8122,
      "step": 1480600
    },
    {
      "epoch": 57.26495726495727,
      "grad_norm": 11.953254699707031,
      "learning_rate": 2.2792022792022796e-06,
      "loss": 1.7591,
      "step": 1480700
    },
    {
      "epoch": 57.26882468963917,
      "grad_norm": 18.566253662109375,
      "learning_rate": 2.2759794253006926e-06,
      "loss": 1.8271,
      "step": 1480800
    },
    {
      "epoch": 57.272692114321075,
      "grad_norm": 13.292970657348633,
      "learning_rate": 2.272756571399105e-06,
      "loss": 1.8187,
      "step": 1480900
    },
    {
      "epoch": 57.276559539002974,
      "grad_norm": 13.2534761428833,
      "learning_rate": 2.2695337174975186e-06,
      "loss": 1.8345,
      "step": 1481000
    },
    {
      "epoch": 57.28042696368488,
      "grad_norm": 14.0892333984375,
      "learning_rate": 2.2663108635959316e-06,
      "loss": 1.7309,
      "step": 1481100
    },
    {
      "epoch": 57.28429438836679,
      "grad_norm": 13.675808906555176,
      "learning_rate": 2.2630880096943447e-06,
      "loss": 1.8391,
      "step": 1481200
    },
    {
      "epoch": 57.28816181304869,
      "grad_norm": 11.263239860534668,
      "learning_rate": 2.2598651557927577e-06,
      "loss": 1.7437,
      "step": 1481300
    },
    {
      "epoch": 57.292029237730596,
      "grad_norm": 11.638564109802246,
      "learning_rate": 2.2566423018911707e-06,
      "loss": 1.7961,
      "step": 1481400
    },
    {
      "epoch": 57.2958966624125,
      "grad_norm": 14.280851364135742,
      "learning_rate": 2.253419447989584e-06,
      "loss": 1.7453,
      "step": 1481500
    },
    {
      "epoch": 57.2997640870944,
      "grad_norm": 12.33591365814209,
      "learning_rate": 2.2501965940879968e-06,
      "loss": 1.772,
      "step": 1481600
    },
    {
      "epoch": 57.30363151177631,
      "grad_norm": 11.412943840026855,
      "learning_rate": 2.24697374018641e-06,
      "loss": 1.7818,
      "step": 1481700
    },
    {
      "epoch": 57.30749893645821,
      "grad_norm": 8.650483131408691,
      "learning_rate": 2.243750886284823e-06,
      "loss": 1.7767,
      "step": 1481800
    },
    {
      "epoch": 57.31136636114012,
      "grad_norm": 14.079337120056152,
      "learning_rate": 2.2405280323832363e-06,
      "loss": 1.8155,
      "step": 1481900
    },
    {
      "epoch": 57.31523378582202,
      "grad_norm": 13.382377624511719,
      "learning_rate": 2.2373051784816493e-06,
      "loss": 1.8199,
      "step": 1482000
    },
    {
      "epoch": 57.31910121050392,
      "grad_norm": 8.09601879119873,
      "learning_rate": 2.2340823245800623e-06,
      "loss": 1.766,
      "step": 1482100
    },
    {
      "epoch": 57.32296863518583,
      "grad_norm": 9.739034652709961,
      "learning_rate": 2.2308594706784753e-06,
      "loss": 1.7731,
      "step": 1482200
    },
    {
      "epoch": 57.32683605986774,
      "grad_norm": 10.862833976745605,
      "learning_rate": 2.2276366167768884e-06,
      "loss": 1.7929,
      "step": 1482300
    },
    {
      "epoch": 57.33070348454964,
      "grad_norm": 10.984067916870117,
      "learning_rate": 2.2244137628753014e-06,
      "loss": 1.8428,
      "step": 1482400
    },
    {
      "epoch": 57.334570909231545,
      "grad_norm": 14.13499641418457,
      "learning_rate": 2.2211909089737144e-06,
      "loss": 1.766,
      "step": 1482500
    },
    {
      "epoch": 57.338438333913444,
      "grad_norm": 17.339780807495117,
      "learning_rate": 2.2179680550721274e-06,
      "loss": 1.7062,
      "step": 1482600
    },
    {
      "epoch": 57.34230575859535,
      "grad_norm": 12.722954750061035,
      "learning_rate": 2.2147452011705405e-06,
      "loss": 1.7559,
      "step": 1482700
    },
    {
      "epoch": 57.34617318327726,
      "grad_norm": 12.706979751586914,
      "learning_rate": 2.211522347268954e-06,
      "loss": 1.5695,
      "step": 1482800
    },
    {
      "epoch": 57.35004060795916,
      "grad_norm": 13.572190284729004,
      "learning_rate": 2.208299493367367e-06,
      "loss": 1.8213,
      "step": 1482900
    },
    {
      "epoch": 57.353908032641066,
      "grad_norm": 13.604645729064941,
      "learning_rate": 2.20507663946578e-06,
      "loss": 1.7587,
      "step": 1483000
    },
    {
      "epoch": 57.357775457322965,
      "grad_norm": 9.867042541503906,
      "learning_rate": 2.2018537855641926e-06,
      "loss": 1.7969,
      "step": 1483100
    },
    {
      "epoch": 57.36164288200487,
      "grad_norm": 12.137365341186523,
      "learning_rate": 2.198630931662606e-06,
      "loss": 1.7383,
      "step": 1483200
    },
    {
      "epoch": 57.36551030668678,
      "grad_norm": 14.258390426635742,
      "learning_rate": 2.195408077761019e-06,
      "loss": 1.8417,
      "step": 1483300
    },
    {
      "epoch": 57.36937773136868,
      "grad_norm": 14.49953842163086,
      "learning_rate": 2.192185223859432e-06,
      "loss": 1.7797,
      "step": 1483400
    },
    {
      "epoch": 57.37324515605059,
      "grad_norm": 14.541244506835938,
      "learning_rate": 2.188962369957845e-06,
      "loss": 1.7296,
      "step": 1483500
    },
    {
      "epoch": 57.37711258073249,
      "grad_norm": 12.25273609161377,
      "learning_rate": 2.1857395160562585e-06,
      "loss": 1.7184,
      "step": 1483600
    },
    {
      "epoch": 57.38098000541439,
      "grad_norm": 10.66959285736084,
      "learning_rate": 2.1825166621546716e-06,
      "loss": 1.806,
      "step": 1483700
    },
    {
      "epoch": 57.3848474300963,
      "grad_norm": 15.286795616149902,
      "learning_rate": 2.179293808253084e-06,
      "loss": 1.839,
      "step": 1483800
    },
    {
      "epoch": 57.3887148547782,
      "grad_norm": 10.095257759094238,
      "learning_rate": 2.176070954351497e-06,
      "loss": 1.6771,
      "step": 1483900
    },
    {
      "epoch": 57.39258227946011,
      "grad_norm": 12.478616714477539,
      "learning_rate": 2.1728481004499106e-06,
      "loss": 1.7382,
      "step": 1484000
    },
    {
      "epoch": 57.396449704142015,
      "grad_norm": 12.267273902893066,
      "learning_rate": 2.1696252465483236e-06,
      "loss": 1.7837,
      "step": 1484100
    },
    {
      "epoch": 57.400317128823914,
      "grad_norm": 15.229735374450684,
      "learning_rate": 2.1664023926467367e-06,
      "loss": 1.7526,
      "step": 1484200
    },
    {
      "epoch": 57.40418455350582,
      "grad_norm": 13.27691650390625,
      "learning_rate": 2.1631795387451497e-06,
      "loss": 1.8151,
      "step": 1484300
    },
    {
      "epoch": 57.40805197818772,
      "grad_norm": 11.754189491271973,
      "learning_rate": 2.1599566848435627e-06,
      "loss": 1.7548,
      "step": 1484400
    },
    {
      "epoch": 57.41191940286963,
      "grad_norm": 11.495369911193848,
      "learning_rate": 2.156733830941976e-06,
      "loss": 1.6978,
      "step": 1484500
    },
    {
      "epoch": 57.415786827551536,
      "grad_norm": 15.035002708435059,
      "learning_rate": 2.1535109770403888e-06,
      "loss": 1.7827,
      "step": 1484600
    },
    {
      "epoch": 57.419654252233435,
      "grad_norm": 15.23346996307373,
      "learning_rate": 2.150288123138802e-06,
      "loss": 1.7979,
      "step": 1484700
    },
    {
      "epoch": 57.42352167691534,
      "grad_norm": 11.75914192199707,
      "learning_rate": 2.147065269237215e-06,
      "loss": 1.713,
      "step": 1484800
    },
    {
      "epoch": 57.42738910159725,
      "grad_norm": 14.518296241760254,
      "learning_rate": 2.1438424153356283e-06,
      "loss": 1.7089,
      "step": 1484900
    },
    {
      "epoch": 57.43125652627915,
      "grad_norm": 12.9213285446167,
      "learning_rate": 2.1406195614340413e-06,
      "loss": 1.8116,
      "step": 1485000
    },
    {
      "epoch": 57.43512395096106,
      "grad_norm": 10.74556827545166,
      "learning_rate": 2.1373967075324543e-06,
      "loss": 1.7231,
      "step": 1485100
    },
    {
      "epoch": 57.438991375642956,
      "grad_norm": 13.440943717956543,
      "learning_rate": 2.1341738536308673e-06,
      "loss": 1.7667,
      "step": 1485200
    },
    {
      "epoch": 57.44285880032486,
      "grad_norm": 14.570393562316895,
      "learning_rate": 2.1309509997292804e-06,
      "loss": 1.7479,
      "step": 1485300
    },
    {
      "epoch": 57.44672622500677,
      "grad_norm": 15.055510520935059,
      "learning_rate": 2.1277281458276934e-06,
      "loss": 1.8057,
      "step": 1485400
    },
    {
      "epoch": 57.45059364968867,
      "grad_norm": 13.64481258392334,
      "learning_rate": 2.1245052919261064e-06,
      "loss": 1.7796,
      "step": 1485500
    },
    {
      "epoch": 57.45446107437058,
      "grad_norm": 14.82013988494873,
      "learning_rate": 2.1212824380245194e-06,
      "loss": 1.7543,
      "step": 1485600
    },
    {
      "epoch": 57.458328499052485,
      "grad_norm": 12.632064819335938,
      "learning_rate": 2.118059584122933e-06,
      "loss": 1.6853,
      "step": 1485700
    },
    {
      "epoch": 57.462195923734384,
      "grad_norm": 11.995317459106445,
      "learning_rate": 2.114836730221346e-06,
      "loss": 1.8424,
      "step": 1485800
    },
    {
      "epoch": 57.46606334841629,
      "grad_norm": 15.700861930847168,
      "learning_rate": 2.111613876319759e-06,
      "loss": 1.7748,
      "step": 1485900
    },
    {
      "epoch": 57.46993077309819,
      "grad_norm": 14.630168914794922,
      "learning_rate": 2.1083910224181715e-06,
      "loss": 1.7802,
      "step": 1486000
    },
    {
      "epoch": 57.4737981977801,
      "grad_norm": 11.326947212219238,
      "learning_rate": 2.105168168516585e-06,
      "loss": 1.675,
      "step": 1486100
    },
    {
      "epoch": 57.477665622462006,
      "grad_norm": 8.93798828125,
      "learning_rate": 2.101945314614998e-06,
      "loss": 1.7747,
      "step": 1486200
    },
    {
      "epoch": 57.481533047143905,
      "grad_norm": 15.33698844909668,
      "learning_rate": 2.098722460713411e-06,
      "loss": 1.9018,
      "step": 1486300
    },
    {
      "epoch": 57.48540047182581,
      "grad_norm": 11.2998685836792,
      "learning_rate": 2.095499606811824e-06,
      "loss": 1.7247,
      "step": 1486400
    },
    {
      "epoch": 57.48926789650771,
      "grad_norm": 14.474842071533203,
      "learning_rate": 2.092276752910237e-06,
      "loss": 1.7997,
      "step": 1486500
    },
    {
      "epoch": 57.49313532118962,
      "grad_norm": 13.677680015563965,
      "learning_rate": 2.0890538990086505e-06,
      "loss": 1.8324,
      "step": 1486600
    },
    {
      "epoch": 57.49700274587153,
      "grad_norm": 12.245891571044922,
      "learning_rate": 2.0858310451070636e-06,
      "loss": 1.6361,
      "step": 1486700
    },
    {
      "epoch": 57.500870170553426,
      "grad_norm": 15.820358276367188,
      "learning_rate": 2.082608191205476e-06,
      "loss": 1.7747,
      "step": 1486800
    },
    {
      "epoch": 57.50473759523533,
      "grad_norm": 11.516241073608398,
      "learning_rate": 2.079385337303889e-06,
      "loss": 1.7522,
      "step": 1486900
    },
    {
      "epoch": 57.50860501991724,
      "grad_norm": 15.81053638458252,
      "learning_rate": 2.0761624834023026e-06,
      "loss": 1.7405,
      "step": 1487000
    },
    {
      "epoch": 57.51247244459914,
      "grad_norm": 14.999218940734863,
      "learning_rate": 2.0729396295007156e-06,
      "loss": 1.8205,
      "step": 1487100
    },
    {
      "epoch": 57.51633986928105,
      "grad_norm": 14.50689697265625,
      "learning_rate": 2.0697167755991287e-06,
      "loss": 1.7956,
      "step": 1487200
    },
    {
      "epoch": 57.52020729396295,
      "grad_norm": 13.054408073425293,
      "learning_rate": 2.0664939216975417e-06,
      "loss": 1.782,
      "step": 1487300
    },
    {
      "epoch": 57.524074718644854,
      "grad_norm": 7.35090446472168,
      "learning_rate": 2.0632710677959547e-06,
      "loss": 1.7958,
      "step": 1487400
    },
    {
      "epoch": 57.52794214332676,
      "grad_norm": 13.945822715759277,
      "learning_rate": 2.0600482138943677e-06,
      "loss": 1.8079,
      "step": 1487500
    },
    {
      "epoch": 57.53180956800866,
      "grad_norm": 12.923233032226562,
      "learning_rate": 2.0568253599927808e-06,
      "loss": 1.6816,
      "step": 1487600
    },
    {
      "epoch": 57.53567699269057,
      "grad_norm": 12.797322273254395,
      "learning_rate": 2.053602506091194e-06,
      "loss": 1.7665,
      "step": 1487700
    },
    {
      "epoch": 57.53954441737247,
      "grad_norm": 12.310715675354004,
      "learning_rate": 2.050379652189607e-06,
      "loss": 1.799,
      "step": 1487800
    },
    {
      "epoch": 57.543411842054375,
      "grad_norm": 14.557676315307617,
      "learning_rate": 2.0471567982880203e-06,
      "loss": 1.6988,
      "step": 1487900
    },
    {
      "epoch": 57.54727926673628,
      "grad_norm": 13.530840873718262,
      "learning_rate": 2.0439339443864333e-06,
      "loss": 1.7306,
      "step": 1488000
    },
    {
      "epoch": 57.55114669141818,
      "grad_norm": 12.162397384643555,
      "learning_rate": 2.0407110904848463e-06,
      "loss": 1.8068,
      "step": 1488100
    },
    {
      "epoch": 57.55501411610009,
      "grad_norm": 10.335543632507324,
      "learning_rate": 2.0374882365832593e-06,
      "loss": 1.6881,
      "step": 1488200
    },
    {
      "epoch": 57.558881540782,
      "grad_norm": 11.69293212890625,
      "learning_rate": 2.0342653826816724e-06,
      "loss": 1.8335,
      "step": 1488300
    },
    {
      "epoch": 57.562748965463896,
      "grad_norm": 14.559928894042969,
      "learning_rate": 2.0310425287800854e-06,
      "loss": 1.7813,
      "step": 1488400
    },
    {
      "epoch": 57.5666163901458,
      "grad_norm": 16.08456802368164,
      "learning_rate": 2.0278196748784984e-06,
      "loss": 1.8447,
      "step": 1488500
    },
    {
      "epoch": 57.5704838148277,
      "grad_norm": 12.95449161529541,
      "learning_rate": 2.0245968209769114e-06,
      "loss": 1.6969,
      "step": 1488600
    },
    {
      "epoch": 57.57435123950961,
      "grad_norm": 13.492959022521973,
      "learning_rate": 2.021373967075325e-06,
      "loss": 1.729,
      "step": 1488700
    },
    {
      "epoch": 57.57821866419152,
      "grad_norm": 15.488764762878418,
      "learning_rate": 2.018151113173738e-06,
      "loss": 1.8212,
      "step": 1488800
    },
    {
      "epoch": 57.58208608887342,
      "grad_norm": 12.345624923706055,
      "learning_rate": 2.014928259272151e-06,
      "loss": 1.7401,
      "step": 1488900
    },
    {
      "epoch": 57.585953513555324,
      "grad_norm": 13.155009269714355,
      "learning_rate": 2.0117054053705635e-06,
      "loss": 1.7453,
      "step": 1489000
    },
    {
      "epoch": 57.589820938237224,
      "grad_norm": 14.761555671691895,
      "learning_rate": 2.008482551468977e-06,
      "loss": 1.7601,
      "step": 1489100
    },
    {
      "epoch": 57.59368836291913,
      "grad_norm": 12.580360412597656,
      "learning_rate": 2.00525969756739e-06,
      "loss": 1.7457,
      "step": 1489200
    },
    {
      "epoch": 57.59755578760104,
      "grad_norm": 13.330986022949219,
      "learning_rate": 2.002036843665803e-06,
      "loss": 1.8342,
      "step": 1489300
    },
    {
      "epoch": 57.60142321228294,
      "grad_norm": 13.196304321289062,
      "learning_rate": 1.998813989764216e-06,
      "loss": 1.755,
      "step": 1489400
    },
    {
      "epoch": 57.605290636964845,
      "grad_norm": 10.170896530151367,
      "learning_rate": 1.995591135862629e-06,
      "loss": 1.7772,
      "step": 1489500
    },
    {
      "epoch": 57.60915806164675,
      "grad_norm": 13.7609224319458,
      "learning_rate": 1.9923682819610425e-06,
      "loss": 1.674,
      "step": 1489600
    },
    {
      "epoch": 57.61302548632865,
      "grad_norm": 7.400845050811768,
      "learning_rate": 1.989145428059455e-06,
      "loss": 1.7914,
      "step": 1489700
    },
    {
      "epoch": 57.61689291101056,
      "grad_norm": 12.46967601776123,
      "learning_rate": 1.985922574157868e-06,
      "loss": 1.8571,
      "step": 1489800
    },
    {
      "epoch": 57.62076033569246,
      "grad_norm": 14.408354759216309,
      "learning_rate": 1.982699720256281e-06,
      "loss": 1.6777,
      "step": 1489900
    },
    {
      "epoch": 57.624627760374366,
      "grad_norm": 13.445893287658691,
      "learning_rate": 1.9794768663546946e-06,
      "loss": 1.7838,
      "step": 1490000
    },
    {
      "epoch": 57.62849518505627,
      "grad_norm": 11.12714958190918,
      "learning_rate": 1.9762540124531076e-06,
      "loss": 1.7509,
      "step": 1490100
    },
    {
      "epoch": 57.63236260973817,
      "grad_norm": 12.889681816101074,
      "learning_rate": 1.9730311585515207e-06,
      "loss": 1.703,
      "step": 1490200
    },
    {
      "epoch": 57.63623003442008,
      "grad_norm": 10.367640495300293,
      "learning_rate": 1.9698083046499337e-06,
      "loss": 1.7288,
      "step": 1490300
    },
    {
      "epoch": 57.64009745910199,
      "grad_norm": 14.532309532165527,
      "learning_rate": 1.966585450748347e-06,
      "loss": 1.7412,
      "step": 1490400
    },
    {
      "epoch": 57.64396488378389,
      "grad_norm": 16.096040725708008,
      "learning_rate": 1.9633625968467597e-06,
      "loss": 1.755,
      "step": 1490500
    },
    {
      "epoch": 57.647832308465794,
      "grad_norm": 13.176779747009277,
      "learning_rate": 1.9601397429451728e-06,
      "loss": 1.7352,
      "step": 1490600
    },
    {
      "epoch": 57.651699733147694,
      "grad_norm": 9.539238929748535,
      "learning_rate": 1.956916889043586e-06,
      "loss": 1.775,
      "step": 1490700
    },
    {
      "epoch": 57.6555671578296,
      "grad_norm": 14.51073932647705,
      "learning_rate": 1.9536940351419992e-06,
      "loss": 1.8477,
      "step": 1490800
    },
    {
      "epoch": 57.65943458251151,
      "grad_norm": 11.906364440917969,
      "learning_rate": 1.9504711812404123e-06,
      "loss": 1.8071,
      "step": 1490900
    },
    {
      "epoch": 57.66330200719341,
      "grad_norm": 15.106340408325195,
      "learning_rate": 1.9472483273388253e-06,
      "loss": 1.7981,
      "step": 1491000
    },
    {
      "epoch": 57.667169431875315,
      "grad_norm": 12.549162864685059,
      "learning_rate": 1.9440254734372383e-06,
      "loss": 1.7832,
      "step": 1491100
    },
    {
      "epoch": 57.671036856557215,
      "grad_norm": 9.910725593566895,
      "learning_rate": 1.9408026195356513e-06,
      "loss": 1.7397,
      "step": 1491200
    },
    {
      "epoch": 57.67490428123912,
      "grad_norm": 16.88506507873535,
      "learning_rate": 1.9375797656340644e-06,
      "loss": 1.7471,
      "step": 1491300
    },
    {
      "epoch": 57.67877170592103,
      "grad_norm": 12.152656555175781,
      "learning_rate": 1.9343569117324774e-06,
      "loss": 1.8019,
      "step": 1491400
    },
    {
      "epoch": 57.68263913060293,
      "grad_norm": 12.977659225463867,
      "learning_rate": 1.9311340578308904e-06,
      "loss": 1.7361,
      "step": 1491500
    },
    {
      "epoch": 57.686506555284836,
      "grad_norm": 11.458828926086426,
      "learning_rate": 1.9279112039293034e-06,
      "loss": 1.8321,
      "step": 1491600
    },
    {
      "epoch": 57.69037397996674,
      "grad_norm": 11.23841381072998,
      "learning_rate": 1.924688350027717e-06,
      "loss": 1.8347,
      "step": 1491700
    },
    {
      "epoch": 57.69424140464864,
      "grad_norm": 12.882987976074219,
      "learning_rate": 1.92146549612613e-06,
      "loss": 1.7719,
      "step": 1491800
    },
    {
      "epoch": 57.69810882933055,
      "grad_norm": 12.412651062011719,
      "learning_rate": 1.9182426422245425e-06,
      "loss": 1.7388,
      "step": 1491900
    },
    {
      "epoch": 57.70197625401245,
      "grad_norm": 18.227636337280273,
      "learning_rate": 1.9150197883229555e-06,
      "loss": 1.7845,
      "step": 1492000
    },
    {
      "epoch": 57.70584367869436,
      "grad_norm": 12.834049224853516,
      "learning_rate": 1.911796934421369e-06,
      "loss": 1.7791,
      "step": 1492100
    },
    {
      "epoch": 57.709711103376264,
      "grad_norm": 15.281408309936523,
      "learning_rate": 1.908574080519782e-06,
      "loss": 1.8431,
      "step": 1492200
    },
    {
      "epoch": 57.713578528058164,
      "grad_norm": 10.445427894592285,
      "learning_rate": 1.905351226618195e-06,
      "loss": 1.6711,
      "step": 1492300
    },
    {
      "epoch": 57.71744595274007,
      "grad_norm": 14.068010330200195,
      "learning_rate": 1.9021283727166083e-06,
      "loss": 1.7338,
      "step": 1492400
    },
    {
      "epoch": 57.72131337742197,
      "grad_norm": 16.65802001953125,
      "learning_rate": 1.8989055188150213e-06,
      "loss": 1.6984,
      "step": 1492500
    },
    {
      "epoch": 57.72518080210388,
      "grad_norm": 11.333449363708496,
      "learning_rate": 1.8956826649134343e-06,
      "loss": 1.8068,
      "step": 1492600
    },
    {
      "epoch": 57.729048226785785,
      "grad_norm": 9.594292640686035,
      "learning_rate": 1.8924598110118471e-06,
      "loss": 1.7816,
      "step": 1492700
    },
    {
      "epoch": 57.732915651467685,
      "grad_norm": 11.10936164855957,
      "learning_rate": 1.8892369571102604e-06,
      "loss": 1.8496,
      "step": 1492800
    },
    {
      "epoch": 57.73678307614959,
      "grad_norm": 13.336689949035645,
      "learning_rate": 1.8860141032086734e-06,
      "loss": 1.7656,
      "step": 1492900
    },
    {
      "epoch": 57.7406505008315,
      "grad_norm": 10.564553260803223,
      "learning_rate": 1.8827912493070864e-06,
      "loss": 1.7325,
      "step": 1493000
    },
    {
      "epoch": 57.7445179255134,
      "grad_norm": 13.989605903625488,
      "learning_rate": 1.8795683954054996e-06,
      "loss": 1.7666,
      "step": 1493100
    },
    {
      "epoch": 57.748385350195306,
      "grad_norm": 13.099761009216309,
      "learning_rate": 1.8763455415039127e-06,
      "loss": 1.7742,
      "step": 1493200
    },
    {
      "epoch": 57.752252774877206,
      "grad_norm": 13.976967811584473,
      "learning_rate": 1.873122687602326e-06,
      "loss": 1.7696,
      "step": 1493300
    },
    {
      "epoch": 57.75612019955911,
      "grad_norm": 13.877107620239258,
      "learning_rate": 1.8698998337007385e-06,
      "loss": 1.7252,
      "step": 1493400
    },
    {
      "epoch": 57.75998762424102,
      "grad_norm": 14.115187644958496,
      "learning_rate": 1.8666769797991517e-06,
      "loss": 1.7837,
      "step": 1493500
    },
    {
      "epoch": 57.76385504892292,
      "grad_norm": 10.449602127075195,
      "learning_rate": 1.8634541258975648e-06,
      "loss": 1.7011,
      "step": 1493600
    },
    {
      "epoch": 57.76772247360483,
      "grad_norm": 13.84739875793457,
      "learning_rate": 1.860231271995978e-06,
      "loss": 1.7592,
      "step": 1493700
    },
    {
      "epoch": 57.771589898286734,
      "grad_norm": 10.225302696228027,
      "learning_rate": 1.857008418094391e-06,
      "loss": 1.7186,
      "step": 1493800
    },
    {
      "epoch": 57.775457322968634,
      "grad_norm": 17.315786361694336,
      "learning_rate": 1.8537855641928043e-06,
      "loss": 1.7334,
      "step": 1493900
    },
    {
      "epoch": 57.77932474765054,
      "grad_norm": 12.204853057861328,
      "learning_rate": 1.8505627102912173e-06,
      "loss": 1.723,
      "step": 1494000
    },
    {
      "epoch": 57.78319217233244,
      "grad_norm": 12.975722312927246,
      "learning_rate": 1.8473398563896303e-06,
      "loss": 1.7372,
      "step": 1494100
    },
    {
      "epoch": 57.78705959701435,
      "grad_norm": 11.414331436157227,
      "learning_rate": 1.8441170024880431e-06,
      "loss": 1.8656,
      "step": 1494200
    },
    {
      "epoch": 57.790927021696255,
      "grad_norm": 14.952810287475586,
      "learning_rate": 1.8408941485864564e-06,
      "loss": 1.7989,
      "step": 1494300
    },
    {
      "epoch": 57.794794446378155,
      "grad_norm": 11.41573715209961,
      "learning_rate": 1.8376712946848694e-06,
      "loss": 1.7528,
      "step": 1494400
    },
    {
      "epoch": 57.79866187106006,
      "grad_norm": 10.841598510742188,
      "learning_rate": 1.8344484407832824e-06,
      "loss": 1.8132,
      "step": 1494500
    },
    {
      "epoch": 57.80252929574196,
      "grad_norm": 8.560717582702637,
      "learning_rate": 1.8312255868816956e-06,
      "loss": 1.8505,
      "step": 1494600
    },
    {
      "epoch": 57.80639672042387,
      "grad_norm": 13.139135360717773,
      "learning_rate": 1.8280027329801087e-06,
      "loss": 1.7789,
      "step": 1494700
    },
    {
      "epoch": 57.810264145105776,
      "grad_norm": 11.00044059753418,
      "learning_rate": 1.824779879078522e-06,
      "loss": 1.7844,
      "step": 1494800
    },
    {
      "epoch": 57.814131569787676,
      "grad_norm": 10.557194709777832,
      "learning_rate": 1.8215570251769347e-06,
      "loss": 1.7829,
      "step": 1494900
    },
    {
      "epoch": 57.81799899446958,
      "grad_norm": 14.622011184692383,
      "learning_rate": 1.8183341712753477e-06,
      "loss": 1.6884,
      "step": 1495000
    },
    {
      "epoch": 57.82186641915149,
      "grad_norm": 14.643988609313965,
      "learning_rate": 1.8151113173737608e-06,
      "loss": 1.773,
      "step": 1495100
    },
    {
      "epoch": 57.82573384383339,
      "grad_norm": 12.274682998657227,
      "learning_rate": 1.811888463472174e-06,
      "loss": 1.695,
      "step": 1495200
    },
    {
      "epoch": 57.8296012685153,
      "grad_norm": 12.499421119689941,
      "learning_rate": 1.808665609570587e-06,
      "loss": 1.733,
      "step": 1495300
    },
    {
      "epoch": 57.8334686931972,
      "grad_norm": 13.240656852722168,
      "learning_rate": 1.8054427556690003e-06,
      "loss": 1.7749,
      "step": 1495400
    },
    {
      "epoch": 57.837336117879104,
      "grad_norm": 13.434892654418945,
      "learning_rate": 1.8022199017674133e-06,
      "loss": 1.8657,
      "step": 1495500
    },
    {
      "epoch": 57.84120354256101,
      "grad_norm": 12.517635345458984,
      "learning_rate": 1.798997047865826e-06,
      "loss": 1.8209,
      "step": 1495600
    },
    {
      "epoch": 57.84507096724291,
      "grad_norm": 9.891840934753418,
      "learning_rate": 1.7957741939642391e-06,
      "loss": 1.7311,
      "step": 1495700
    },
    {
      "epoch": 57.84893839192482,
      "grad_norm": 19.62824821472168,
      "learning_rate": 1.7925513400626524e-06,
      "loss": 1.7351,
      "step": 1495800
    },
    {
      "epoch": 57.85280581660672,
      "grad_norm": 13.19918155670166,
      "learning_rate": 1.7893284861610654e-06,
      "loss": 1.6891,
      "step": 1495900
    },
    {
      "epoch": 57.856673241288625,
      "grad_norm": 16.173954010009766,
      "learning_rate": 1.7861056322594786e-06,
      "loss": 1.8046,
      "step": 1496000
    },
    {
      "epoch": 57.86054066597053,
      "grad_norm": 17.593830108642578,
      "learning_rate": 1.7828827783578916e-06,
      "loss": 1.819,
      "step": 1496100
    },
    {
      "epoch": 57.86440809065243,
      "grad_norm": 12.638303756713867,
      "learning_rate": 1.7796599244563047e-06,
      "loss": 1.7926,
      "step": 1496200
    },
    {
      "epoch": 57.86827551533434,
      "grad_norm": 10.674487113952637,
      "learning_rate": 1.776437070554718e-06,
      "loss": 1.662,
      "step": 1496300
    },
    {
      "epoch": 57.872142940016246,
      "grad_norm": 14.013175010681152,
      "learning_rate": 1.7732142166531307e-06,
      "loss": 1.885,
      "step": 1496400
    },
    {
      "epoch": 57.876010364698146,
      "grad_norm": 13.218707084655762,
      "learning_rate": 1.7699913627515437e-06,
      "loss": 1.7455,
      "step": 1496500
    },
    {
      "epoch": 57.87987778938005,
      "grad_norm": 11.518774032592773,
      "learning_rate": 1.7667685088499568e-06,
      "loss": 1.7046,
      "step": 1496600
    },
    {
      "epoch": 57.88374521406195,
      "grad_norm": 11.092867851257324,
      "learning_rate": 1.76354565494837e-06,
      "loss": 1.863,
      "step": 1496700
    },
    {
      "epoch": 57.88761263874386,
      "grad_norm": 11.146800994873047,
      "learning_rate": 1.760322801046783e-06,
      "loss": 1.7166,
      "step": 1496800
    },
    {
      "epoch": 57.89148006342577,
      "grad_norm": 9.955353736877441,
      "learning_rate": 1.7570999471451963e-06,
      "loss": 1.8047,
      "step": 1496900
    },
    {
      "epoch": 57.89534748810767,
      "grad_norm": 11.13222885131836,
      "learning_rate": 1.7538770932436093e-06,
      "loss": 1.7245,
      "step": 1497000
    },
    {
      "epoch": 57.899214912789574,
      "grad_norm": 15.070050239562988,
      "learning_rate": 1.750654239342022e-06,
      "loss": 1.8553,
      "step": 1497100
    },
    {
      "epoch": 57.903082337471474,
      "grad_norm": 10.889023780822754,
      "learning_rate": 1.7474313854404351e-06,
      "loss": 1.7476,
      "step": 1497200
    },
    {
      "epoch": 57.90694976215338,
      "grad_norm": 15.199156761169434,
      "learning_rate": 1.7442085315388484e-06,
      "loss": 1.7186,
      "step": 1497300
    },
    {
      "epoch": 57.91081718683529,
      "grad_norm": 12.298824310302734,
      "learning_rate": 1.7409856776372614e-06,
      "loss": 1.783,
      "step": 1497400
    },
    {
      "epoch": 57.91468461151719,
      "grad_norm": 11.827486038208008,
      "learning_rate": 1.7377628237356746e-06,
      "loss": 1.747,
      "step": 1497500
    },
    {
      "epoch": 57.918552036199095,
      "grad_norm": 12.268050193786621,
      "learning_rate": 1.7345399698340876e-06,
      "loss": 1.7408,
      "step": 1497600
    },
    {
      "epoch": 57.922419460881,
      "grad_norm": 11.642216682434082,
      "learning_rate": 1.7313171159325007e-06,
      "loss": 1.8057,
      "step": 1497700
    },
    {
      "epoch": 57.9262868855629,
      "grad_norm": 11.120037078857422,
      "learning_rate": 1.7280942620309135e-06,
      "loss": 1.7791,
      "step": 1497800
    },
    {
      "epoch": 57.93015431024481,
      "grad_norm": 12.368073463439941,
      "learning_rate": 1.7248714081293267e-06,
      "loss": 1.777,
      "step": 1497900
    },
    {
      "epoch": 57.93402173492671,
      "grad_norm": 12.927793502807617,
      "learning_rate": 1.7216485542277397e-06,
      "loss": 1.8191,
      "step": 1498000
    },
    {
      "epoch": 57.937889159608616,
      "grad_norm": 11.812087059020996,
      "learning_rate": 1.7184257003261528e-06,
      "loss": 1.7756,
      "step": 1498100
    },
    {
      "epoch": 57.94175658429052,
      "grad_norm": 14.893970489501953,
      "learning_rate": 1.715202846424566e-06,
      "loss": 1.7982,
      "step": 1498200
    },
    {
      "epoch": 57.94562400897242,
      "grad_norm": 17.61166000366211,
      "learning_rate": 1.711979992522979e-06,
      "loss": 1.7363,
      "step": 1498300
    },
    {
      "epoch": 57.94949143365433,
      "grad_norm": 11.99815845489502,
      "learning_rate": 1.7087571386213923e-06,
      "loss": 1.8162,
      "step": 1498400
    },
    {
      "epoch": 57.95335885833624,
      "grad_norm": 13.40185832977295,
      "learning_rate": 1.7055342847198053e-06,
      "loss": 1.8145,
      "step": 1498500
    },
    {
      "epoch": 57.95722628301814,
      "grad_norm": 13.354619979858398,
      "learning_rate": 1.702311430818218e-06,
      "loss": 1.7318,
      "step": 1498600
    },
    {
      "epoch": 57.961093707700044,
      "grad_norm": 13.806987762451172,
      "learning_rate": 1.6990885769166311e-06,
      "loss": 1.7233,
      "step": 1498700
    },
    {
      "epoch": 57.964961132381944,
      "grad_norm": 13.654314041137695,
      "learning_rate": 1.6958657230150444e-06,
      "loss": 1.7373,
      "step": 1498800
    },
    {
      "epoch": 57.96882855706385,
      "grad_norm": 13.8620023727417,
      "learning_rate": 1.6926428691134574e-06,
      "loss": 1.8018,
      "step": 1498900
    },
    {
      "epoch": 57.97269598174576,
      "grad_norm": 11.649847984313965,
      "learning_rate": 1.6894200152118706e-06,
      "loss": 1.6991,
      "step": 1499000
    },
    {
      "epoch": 57.97656340642766,
      "grad_norm": 16.08882713317871,
      "learning_rate": 1.6861971613102836e-06,
      "loss": 1.7787,
      "step": 1499100
    },
    {
      "epoch": 57.980430831109565,
      "grad_norm": 15.22986888885498,
      "learning_rate": 1.6829743074086967e-06,
      "loss": 1.7924,
      "step": 1499200
    },
    {
      "epoch": 57.984298255791465,
      "grad_norm": 25.723011016845703,
      "learning_rate": 1.6797514535071095e-06,
      "loss": 1.7819,
      "step": 1499300
    },
    {
      "epoch": 57.98816568047337,
      "grad_norm": 8.40137004852295,
      "learning_rate": 1.6765285996055227e-06,
      "loss": 1.7707,
      "step": 1499400
    },
    {
      "epoch": 57.99203310515528,
      "grad_norm": 11.571431159973145,
      "learning_rate": 1.6733057457039357e-06,
      "loss": 1.7857,
      "step": 1499500
    },
    {
      "epoch": 57.99590052983718,
      "grad_norm": 12.863353729248047,
      "learning_rate": 1.670082891802349e-06,
      "loss": 1.7533,
      "step": 1499600
    },
    {
      "epoch": 57.999767954519086,
      "grad_norm": 11.963094711303711,
      "learning_rate": 1.666860037900762e-06,
      "loss": 1.7634,
      "step": 1499700
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.7354907989501953,
      "eval_runtime": 2.977,
      "eval_samples_per_second": 457.166,
      "eval_steps_per_second": 457.166,
      "step": 1499706
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.5650230646133423,
      "eval_runtime": 55.6297,
      "eval_samples_per_second": 464.805,
      "eval_steps_per_second": 464.805,
      "step": 1499706
    },
    {
      "epoch": 58.00363537920099,
      "grad_norm": 11.581363677978516,
      "learning_rate": 1.663637183999175e-06,
      "loss": 1.7048,
      "step": 1499800
    },
    {
      "epoch": 58.00750280388289,
      "grad_norm": 10.192642211914062,
      "learning_rate": 1.6604143300975883e-06,
      "loss": 1.7302,
      "step": 1499900
    },
    {
      "epoch": 58.0113702285648,
      "grad_norm": 9.949310302734375,
      "learning_rate": 1.6571914761960013e-06,
      "loss": 1.7087,
      "step": 1500000
    },
    {
      "epoch": 58.0152376532467,
      "grad_norm": 9.527289390563965,
      "learning_rate": 1.653968622294414e-06,
      "loss": 1.6899,
      "step": 1500100
    },
    {
      "epoch": 58.01910507792861,
      "grad_norm": 12.30306625366211,
      "learning_rate": 1.6507457683928271e-06,
      "loss": 1.6864,
      "step": 1500200
    },
    {
      "epoch": 58.022972502610514,
      "grad_norm": 12.334952354431152,
      "learning_rate": 1.6475229144912404e-06,
      "loss": 1.8033,
      "step": 1500300
    },
    {
      "epoch": 58.026839927292414,
      "grad_norm": 22.351329803466797,
      "learning_rate": 1.6443000605896534e-06,
      "loss": 1.7834,
      "step": 1500400
    },
    {
      "epoch": 58.03070735197432,
      "grad_norm": 13.223100662231445,
      "learning_rate": 1.6410772066880666e-06,
      "loss": 1.7236,
      "step": 1500500
    },
    {
      "epoch": 58.03457477665622,
      "grad_norm": 10.281159400939941,
      "learning_rate": 1.6378543527864796e-06,
      "loss": 1.7484,
      "step": 1500600
    },
    {
      "epoch": 58.03844220133813,
      "grad_norm": 11.133625030517578,
      "learning_rate": 1.6346314988848929e-06,
      "loss": 1.8072,
      "step": 1500700
    },
    {
      "epoch": 58.042309626020035,
      "grad_norm": 11.895648956298828,
      "learning_rate": 1.6314086449833055e-06,
      "loss": 1.7543,
      "step": 1500800
    },
    {
      "epoch": 58.046177050701935,
      "grad_norm": 11.800376892089844,
      "learning_rate": 1.6281857910817187e-06,
      "loss": 1.7458,
      "step": 1500900
    },
    {
      "epoch": 58.05004447538384,
      "grad_norm": 12.230219841003418,
      "learning_rate": 1.6249629371801317e-06,
      "loss": 1.7974,
      "step": 1501000
    },
    {
      "epoch": 58.05391190006575,
      "grad_norm": 11.197286605834961,
      "learning_rate": 1.621740083278545e-06,
      "loss": 1.7649,
      "step": 1501100
    },
    {
      "epoch": 58.05777932474765,
      "grad_norm": 13.089644432067871,
      "learning_rate": 1.618517229376958e-06,
      "loss": 1.7099,
      "step": 1501200
    },
    {
      "epoch": 58.061646749429556,
      "grad_norm": 15.845632553100586,
      "learning_rate": 1.615294375475371e-06,
      "loss": 1.7095,
      "step": 1501300
    },
    {
      "epoch": 58.065514174111456,
      "grad_norm": 11.346949577331543,
      "learning_rate": 1.6120715215737843e-06,
      "loss": 1.6817,
      "step": 1501400
    },
    {
      "epoch": 58.06938159879336,
      "grad_norm": 11.591022491455078,
      "learning_rate": 1.608848667672197e-06,
      "loss": 1.7042,
      "step": 1501500
    },
    {
      "epoch": 58.07324902347527,
      "grad_norm": 7.557753086090088,
      "learning_rate": 1.60562581377061e-06,
      "loss": 1.7777,
      "step": 1501600
    },
    {
      "epoch": 58.07711644815717,
      "grad_norm": 14.018815994262695,
      "learning_rate": 1.6024029598690231e-06,
      "loss": 1.7145,
      "step": 1501700
    },
    {
      "epoch": 58.08098387283908,
      "grad_norm": 12.153533935546875,
      "learning_rate": 1.5991801059674364e-06,
      "loss": 1.8314,
      "step": 1501800
    },
    {
      "epoch": 58.084851297520984,
      "grad_norm": 13.160552024841309,
      "learning_rate": 1.5959572520658494e-06,
      "loss": 1.7056,
      "step": 1501900
    },
    {
      "epoch": 58.088718722202884,
      "grad_norm": 11.132683753967285,
      "learning_rate": 1.5927343981642626e-06,
      "loss": 1.7368,
      "step": 1502000
    },
    {
      "epoch": 58.09258614688479,
      "grad_norm": 10.411028861999512,
      "learning_rate": 1.5895115442626757e-06,
      "loss": 1.8869,
      "step": 1502100
    },
    {
      "epoch": 58.09645357156669,
      "grad_norm": 17.656869888305664,
      "learning_rate": 1.5862886903610889e-06,
      "loss": 1.6721,
      "step": 1502200
    },
    {
      "epoch": 58.1003209962486,
      "grad_norm": 11.2647066116333,
      "learning_rate": 1.5830658364595015e-06,
      "loss": 1.7422,
      "step": 1502300
    },
    {
      "epoch": 58.104188420930505,
      "grad_norm": 10.797904968261719,
      "learning_rate": 1.5798429825579147e-06,
      "loss": 1.7525,
      "step": 1502400
    },
    {
      "epoch": 58.108055845612405,
      "grad_norm": 13.894351959228516,
      "learning_rate": 1.5766201286563277e-06,
      "loss": 1.664,
      "step": 1502500
    },
    {
      "epoch": 58.11192327029431,
      "grad_norm": 13.84957218170166,
      "learning_rate": 1.573397274754741e-06,
      "loss": 1.7184,
      "step": 1502600
    },
    {
      "epoch": 58.11579069497621,
      "grad_norm": 12.766459465026855,
      "learning_rate": 1.570174420853154e-06,
      "loss": 1.7697,
      "step": 1502700
    },
    {
      "epoch": 58.11965811965812,
      "grad_norm": 16.27541732788086,
      "learning_rate": 1.566951566951567e-06,
      "loss": 1.8234,
      "step": 1502800
    },
    {
      "epoch": 58.123525544340026,
      "grad_norm": 8.98643970489502,
      "learning_rate": 1.5637287130499803e-06,
      "loss": 1.7821,
      "step": 1502900
    },
    {
      "epoch": 58.127392969021926,
      "grad_norm": 12.451883316040039,
      "learning_rate": 1.560505859148393e-06,
      "loss": 1.7123,
      "step": 1503000
    },
    {
      "epoch": 58.13126039370383,
      "grad_norm": 15.358940124511719,
      "learning_rate": 1.5572830052468063e-06,
      "loss": 1.7765,
      "step": 1503100
    },
    {
      "epoch": 58.13512781838574,
      "grad_norm": 12.692964553833008,
      "learning_rate": 1.5540601513452191e-06,
      "loss": 1.7082,
      "step": 1503200
    },
    {
      "epoch": 58.13899524306764,
      "grad_norm": 11.16566276550293,
      "learning_rate": 1.5508372974436324e-06,
      "loss": 1.7901,
      "step": 1503300
    },
    {
      "epoch": 58.14286266774955,
      "grad_norm": 12.944453239440918,
      "learning_rate": 1.5476144435420454e-06,
      "loss": 1.7677,
      "step": 1503400
    },
    {
      "epoch": 58.14673009243145,
      "grad_norm": 17.233922958374023,
      "learning_rate": 1.5443915896404586e-06,
      "loss": 1.7844,
      "step": 1503500
    },
    {
      "epoch": 58.150597517113354,
      "grad_norm": 13.19460391998291,
      "learning_rate": 1.5411687357388714e-06,
      "loss": 1.7929,
      "step": 1503600
    },
    {
      "epoch": 58.15446494179526,
      "grad_norm": 10.015914916992188,
      "learning_rate": 1.5379458818372847e-06,
      "loss": 1.7553,
      "step": 1503700
    },
    {
      "epoch": 58.15833236647716,
      "grad_norm": 11.297858238220215,
      "learning_rate": 1.5347230279356977e-06,
      "loss": 1.7367,
      "step": 1503800
    },
    {
      "epoch": 58.16219979115907,
      "grad_norm": 9.760468482971191,
      "learning_rate": 1.5315001740341107e-06,
      "loss": 1.6959,
      "step": 1503900
    },
    {
      "epoch": 58.16606721584097,
      "grad_norm": 12.403098106384277,
      "learning_rate": 1.5282773201325237e-06,
      "loss": 1.8218,
      "step": 1504000
    },
    {
      "epoch": 58.169934640522875,
      "grad_norm": 12.79401683807373,
      "learning_rate": 1.525054466230937e-06,
      "loss": 1.7306,
      "step": 1504100
    },
    {
      "epoch": 58.17380206520478,
      "grad_norm": 13.756614685058594,
      "learning_rate": 1.52183161232935e-06,
      "loss": 1.721,
      "step": 1504200
    },
    {
      "epoch": 58.17766948988668,
      "grad_norm": 12.984597206115723,
      "learning_rate": 1.518608758427763e-06,
      "loss": 1.7613,
      "step": 1504300
    },
    {
      "epoch": 58.18153691456859,
      "grad_norm": 11.958207130432129,
      "learning_rate": 1.515385904526176e-06,
      "loss": 1.7792,
      "step": 1504400
    },
    {
      "epoch": 58.185404339250496,
      "grad_norm": 10.308526039123535,
      "learning_rate": 1.5121630506245893e-06,
      "loss": 1.7343,
      "step": 1504500
    },
    {
      "epoch": 58.189271763932396,
      "grad_norm": 11.467867851257324,
      "learning_rate": 1.5089401967230023e-06,
      "loss": 1.6995,
      "step": 1504600
    },
    {
      "epoch": 58.1931391886143,
      "grad_norm": 15.863072395324707,
      "learning_rate": 1.5057173428214153e-06,
      "loss": 1.6425,
      "step": 1504700
    },
    {
      "epoch": 58.1970066132962,
      "grad_norm": 18.66728401184082,
      "learning_rate": 1.5024944889198284e-06,
      "loss": 1.7409,
      "step": 1504800
    },
    {
      "epoch": 58.20087403797811,
      "grad_norm": 11.371938705444336,
      "learning_rate": 1.4992716350182414e-06,
      "loss": 1.9232,
      "step": 1504900
    },
    {
      "epoch": 58.20474146266002,
      "grad_norm": 17.634628295898438,
      "learning_rate": 1.4960487811166544e-06,
      "loss": 1.7805,
      "step": 1505000
    },
    {
      "epoch": 58.20860888734192,
      "grad_norm": 10.365184783935547,
      "learning_rate": 1.4928259272150674e-06,
      "loss": 1.7153,
      "step": 1505100
    },
    {
      "epoch": 58.212476312023824,
      "grad_norm": 13.655953407287598,
      "learning_rate": 1.4896030733134807e-06,
      "loss": 1.7554,
      "step": 1505200
    },
    {
      "epoch": 58.216343736705724,
      "grad_norm": 12.109935760498047,
      "learning_rate": 1.4863802194118937e-06,
      "loss": 1.7808,
      "step": 1505300
    },
    {
      "epoch": 58.22021116138763,
      "grad_norm": 13.790766716003418,
      "learning_rate": 1.4831573655103067e-06,
      "loss": 1.8268,
      "step": 1505400
    },
    {
      "epoch": 58.22407858606954,
      "grad_norm": 21.975095748901367,
      "learning_rate": 1.4799345116087197e-06,
      "loss": 1.7188,
      "step": 1505500
    },
    {
      "epoch": 58.22794601075144,
      "grad_norm": 13.596038818359375,
      "learning_rate": 1.476711657707133e-06,
      "loss": 1.73,
      "step": 1505600
    },
    {
      "epoch": 58.231813435433345,
      "grad_norm": 9.56702709197998,
      "learning_rate": 1.473488803805546e-06,
      "loss": 1.6637,
      "step": 1505700
    },
    {
      "epoch": 58.23568086011525,
      "grad_norm": 13.474373817443848,
      "learning_rate": 1.470265949903959e-06,
      "loss": 1.7756,
      "step": 1505800
    },
    {
      "epoch": 58.23954828479715,
      "grad_norm": 11.263068199157715,
      "learning_rate": 1.467043096002372e-06,
      "loss": 1.7544,
      "step": 1505900
    },
    {
      "epoch": 58.24341570947906,
      "grad_norm": 11.532524108886719,
      "learning_rate": 1.4638202421007853e-06,
      "loss": 1.6859,
      "step": 1506000
    },
    {
      "epoch": 58.24728313416096,
      "grad_norm": 11.32223892211914,
      "learning_rate": 1.460597388199198e-06,
      "loss": 1.7658,
      "step": 1506100
    },
    {
      "epoch": 58.251150558842866,
      "grad_norm": 11.683966636657715,
      "learning_rate": 1.4573745342976113e-06,
      "loss": 1.6871,
      "step": 1506200
    },
    {
      "epoch": 58.25501798352477,
      "grad_norm": 13.023151397705078,
      "learning_rate": 1.4541516803960244e-06,
      "loss": 1.8761,
      "step": 1506300
    },
    {
      "epoch": 58.25888540820667,
      "grad_norm": 12.713061332702637,
      "learning_rate": 1.4509288264944374e-06,
      "loss": 1.7439,
      "step": 1506400
    },
    {
      "epoch": 58.26275283288858,
      "grad_norm": 11.437206268310547,
      "learning_rate": 1.4477059725928504e-06,
      "loss": 1.6213,
      "step": 1506500
    },
    {
      "epoch": 58.26662025757049,
      "grad_norm": 15.641090393066406,
      "learning_rate": 1.4444831186912634e-06,
      "loss": 1.7319,
      "step": 1506600
    },
    {
      "epoch": 58.27048768225239,
      "grad_norm": 13.172778129577637,
      "learning_rate": 1.4412602647896767e-06,
      "loss": 1.7358,
      "step": 1506700
    },
    {
      "epoch": 58.274355106934294,
      "grad_norm": 11.635334968566895,
      "learning_rate": 1.4380374108880897e-06,
      "loss": 1.7361,
      "step": 1506800
    },
    {
      "epoch": 58.278222531616194,
      "grad_norm": 13.39240837097168,
      "learning_rate": 1.4348145569865027e-06,
      "loss": 1.7353,
      "step": 1506900
    },
    {
      "epoch": 58.2820899562981,
      "grad_norm": 11.892733573913574,
      "learning_rate": 1.4315917030849157e-06,
      "loss": 1.7741,
      "step": 1507000
    },
    {
      "epoch": 58.28595738098001,
      "grad_norm": 13.902693748474121,
      "learning_rate": 1.428368849183329e-06,
      "loss": 1.7662,
      "step": 1507100
    },
    {
      "epoch": 58.28982480566191,
      "grad_norm": 16.016944885253906,
      "learning_rate": 1.4251459952817418e-06,
      "loss": 1.7318,
      "step": 1507200
    },
    {
      "epoch": 58.293692230343815,
      "grad_norm": 5.086320400238037,
      "learning_rate": 1.421923141380155e-06,
      "loss": 1.7484,
      "step": 1507300
    },
    {
      "epoch": 58.297559655025715,
      "grad_norm": 11.252400398254395,
      "learning_rate": 1.418700287478568e-06,
      "loss": 1.6879,
      "step": 1507400
    },
    {
      "epoch": 58.30142707970762,
      "grad_norm": 18.206411361694336,
      "learning_rate": 1.4154774335769813e-06,
      "loss": 1.7318,
      "step": 1507500
    },
    {
      "epoch": 58.30529450438953,
      "grad_norm": 13.6953763961792,
      "learning_rate": 1.412254579675394e-06,
      "loss": 1.9315,
      "step": 1507600
    },
    {
      "epoch": 58.30916192907143,
      "grad_norm": 13.853398323059082,
      "learning_rate": 1.4090317257738073e-06,
      "loss": 1.7039,
      "step": 1507700
    },
    {
      "epoch": 58.313029353753336,
      "grad_norm": 11.78610897064209,
      "learning_rate": 1.4058088718722204e-06,
      "loss": 1.7467,
      "step": 1507800
    },
    {
      "epoch": 58.31689677843524,
      "grad_norm": 8.019078254699707,
      "learning_rate": 1.4025860179706334e-06,
      "loss": 1.7628,
      "step": 1507900
    },
    {
      "epoch": 58.32076420311714,
      "grad_norm": 13.22179889678955,
      "learning_rate": 1.3993631640690464e-06,
      "loss": 1.856,
      "step": 1508000
    },
    {
      "epoch": 58.32463162779905,
      "grad_norm": 17.414592742919922,
      "learning_rate": 1.3961403101674597e-06,
      "loss": 1.8264,
      "step": 1508100
    },
    {
      "epoch": 58.32849905248095,
      "grad_norm": 16.87261962890625,
      "learning_rate": 1.3929174562658727e-06,
      "loss": 1.8191,
      "step": 1508200
    },
    {
      "epoch": 58.33236647716286,
      "grad_norm": 12.334267616271973,
      "learning_rate": 1.3896946023642857e-06,
      "loss": 1.7333,
      "step": 1508300
    },
    {
      "epoch": 58.336233901844764,
      "grad_norm": 15.65925121307373,
      "learning_rate": 1.3864717484626987e-06,
      "loss": 1.7663,
      "step": 1508400
    },
    {
      "epoch": 58.340101326526664,
      "grad_norm": 16.996917724609375,
      "learning_rate": 1.3832488945611117e-06,
      "loss": 1.6635,
      "step": 1508500
    },
    {
      "epoch": 58.34396875120857,
      "grad_norm": 12.09785270690918,
      "learning_rate": 1.380026040659525e-06,
      "loss": 1.7984,
      "step": 1508600
    },
    {
      "epoch": 58.34783617589047,
      "grad_norm": 12.892647743225098,
      "learning_rate": 1.3768031867579378e-06,
      "loss": 1.6794,
      "step": 1508700
    },
    {
      "epoch": 58.35170360057238,
      "grad_norm": 12.215906143188477,
      "learning_rate": 1.373580332856351e-06,
      "loss": 1.7145,
      "step": 1508800
    },
    {
      "epoch": 58.355571025254285,
      "grad_norm": 14.096261978149414,
      "learning_rate": 1.370357478954764e-06,
      "loss": 1.8351,
      "step": 1508900
    },
    {
      "epoch": 58.359438449936185,
      "grad_norm": 14.142788887023926,
      "learning_rate": 1.3671346250531773e-06,
      "loss": 1.7413,
      "step": 1509000
    },
    {
      "epoch": 58.36330587461809,
      "grad_norm": 13.410861015319824,
      "learning_rate": 1.36391177115159e-06,
      "loss": 1.7037,
      "step": 1509100
    },
    {
      "epoch": 58.3671732993,
      "grad_norm": 11.370466232299805,
      "learning_rate": 1.3606889172500033e-06,
      "loss": 1.7848,
      "step": 1509200
    },
    {
      "epoch": 58.3710407239819,
      "grad_norm": 9.757612228393555,
      "learning_rate": 1.3574660633484164e-06,
      "loss": 1.7594,
      "step": 1509300
    },
    {
      "epoch": 58.374908148663806,
      "grad_norm": 12.233163833618164,
      "learning_rate": 1.3542432094468296e-06,
      "loss": 1.7916,
      "step": 1509400
    },
    {
      "epoch": 58.378775573345706,
      "grad_norm": 12.829708099365234,
      "learning_rate": 1.3510203555452424e-06,
      "loss": 1.7591,
      "step": 1509500
    },
    {
      "epoch": 58.38264299802761,
      "grad_norm": 11.728450775146484,
      "learning_rate": 1.3477975016436557e-06,
      "loss": 1.6855,
      "step": 1509600
    },
    {
      "epoch": 58.38651042270952,
      "grad_norm": 11.256152153015137,
      "learning_rate": 1.3445746477420687e-06,
      "loss": 1.8318,
      "step": 1509700
    },
    {
      "epoch": 58.39037784739142,
      "grad_norm": 15.307027816772461,
      "learning_rate": 1.3413517938404817e-06,
      "loss": 1.7769,
      "step": 1509800
    },
    {
      "epoch": 58.39424527207333,
      "grad_norm": 11.599090576171875,
      "learning_rate": 1.3381289399388947e-06,
      "loss": 1.7499,
      "step": 1509900
    },
    {
      "epoch": 58.398112696755234,
      "grad_norm": 11.904656410217285,
      "learning_rate": 1.3349060860373077e-06,
      "loss": 1.7152,
      "step": 1510000
    },
    {
      "epoch": 58.401980121437134,
      "grad_norm": 11.46395492553711,
      "learning_rate": 1.331683232135721e-06,
      "loss": 1.8178,
      "step": 1510100
    },
    {
      "epoch": 58.40584754611904,
      "grad_norm": 19.04294776916504,
      "learning_rate": 1.3284603782341338e-06,
      "loss": 1.8576,
      "step": 1510200
    },
    {
      "epoch": 58.40971497080094,
      "grad_norm": 11.497089385986328,
      "learning_rate": 1.325237524332547e-06,
      "loss": 1.7232,
      "step": 1510300
    },
    {
      "epoch": 58.41358239548285,
      "grad_norm": 12.525320053100586,
      "learning_rate": 1.32201467043096e-06,
      "loss": 1.7524,
      "step": 1510400
    },
    {
      "epoch": 58.417449820164755,
      "grad_norm": 14.389984130859375,
      "learning_rate": 1.3187918165293733e-06,
      "loss": 1.7954,
      "step": 1510500
    },
    {
      "epoch": 58.421317244846655,
      "grad_norm": 13.6856107711792,
      "learning_rate": 1.3155689626277861e-06,
      "loss": 1.7342,
      "step": 1510600
    },
    {
      "epoch": 58.42518466952856,
      "grad_norm": 11.910859107971191,
      "learning_rate": 1.3123461087261993e-06,
      "loss": 1.7056,
      "step": 1510700
    },
    {
      "epoch": 58.42905209421046,
      "grad_norm": 11.215580940246582,
      "learning_rate": 1.3091232548246124e-06,
      "loss": 1.7184,
      "step": 1510800
    },
    {
      "epoch": 58.43291951889237,
      "grad_norm": 14.99501895904541,
      "learning_rate": 1.3059004009230254e-06,
      "loss": 1.8518,
      "step": 1510900
    },
    {
      "epoch": 58.436786943574276,
      "grad_norm": 12.770380973815918,
      "learning_rate": 1.3026775470214384e-06,
      "loss": 1.6786,
      "step": 1511000
    },
    {
      "epoch": 58.440654368256176,
      "grad_norm": 9.555977821350098,
      "learning_rate": 1.2994546931198517e-06,
      "loss": 1.7093,
      "step": 1511100
    },
    {
      "epoch": 58.44452179293808,
      "grad_norm": 12.252988815307617,
      "learning_rate": 1.2962318392182647e-06,
      "loss": 1.7289,
      "step": 1511200
    },
    {
      "epoch": 58.44838921761999,
      "grad_norm": 13.522124290466309,
      "learning_rate": 1.2930089853166777e-06,
      "loss": 1.8198,
      "step": 1511300
    },
    {
      "epoch": 58.45225664230189,
      "grad_norm": 12.710667610168457,
      "learning_rate": 1.2897861314150907e-06,
      "loss": 1.6904,
      "step": 1511400
    },
    {
      "epoch": 58.4561240669838,
      "grad_norm": 13.354097366333008,
      "learning_rate": 1.2865632775135037e-06,
      "loss": 1.7673,
      "step": 1511500
    },
    {
      "epoch": 58.4599914916657,
      "grad_norm": 9.873708724975586,
      "learning_rate": 1.283340423611917e-06,
      "loss": 1.7662,
      "step": 1511600
    },
    {
      "epoch": 58.463858916347604,
      "grad_norm": 16.111047744750977,
      "learning_rate": 1.2801175697103298e-06,
      "loss": 1.793,
      "step": 1511700
    },
    {
      "epoch": 58.46772634102951,
      "grad_norm": 9.112009048461914,
      "learning_rate": 1.276894715808743e-06,
      "loss": 1.7138,
      "step": 1511800
    },
    {
      "epoch": 58.47159376571141,
      "grad_norm": 13.650408744812012,
      "learning_rate": 1.273671861907156e-06,
      "loss": 1.7922,
      "step": 1511900
    },
    {
      "epoch": 58.47546119039332,
      "grad_norm": 12.629450798034668,
      "learning_rate": 1.270449008005569e-06,
      "loss": 1.7475,
      "step": 1512000
    },
    {
      "epoch": 58.47932861507522,
      "grad_norm": 11.062141418457031,
      "learning_rate": 1.2672261541039821e-06,
      "loss": 1.7856,
      "step": 1512100
    },
    {
      "epoch": 58.483196039757125,
      "grad_norm": 12.473013877868652,
      "learning_rate": 1.2640033002023953e-06,
      "loss": 1.6934,
      "step": 1512200
    },
    {
      "epoch": 58.48706346443903,
      "grad_norm": 15.728224754333496,
      "learning_rate": 1.2607804463008084e-06,
      "loss": 1.8302,
      "step": 1512300
    },
    {
      "epoch": 58.49093088912093,
      "grad_norm": 11.180341720581055,
      "learning_rate": 1.2575575923992214e-06,
      "loss": 1.7482,
      "step": 1512400
    },
    {
      "epoch": 58.49479831380284,
      "grad_norm": 13.581830978393555,
      "learning_rate": 1.2543347384976344e-06,
      "loss": 1.7904,
      "step": 1512500
    },
    {
      "epoch": 58.498665738484746,
      "grad_norm": 13.623809814453125,
      "learning_rate": 1.2511118845960477e-06,
      "loss": 1.8782,
      "step": 1512600
    },
    {
      "epoch": 58.502533163166646,
      "grad_norm": 10.287591934204102,
      "learning_rate": 1.2478890306944607e-06,
      "loss": 1.7863,
      "step": 1512700
    },
    {
      "epoch": 58.50640058784855,
      "grad_norm": 9.388197898864746,
      "learning_rate": 1.2446661767928737e-06,
      "loss": 1.7849,
      "step": 1512800
    },
    {
      "epoch": 58.51026801253045,
      "grad_norm": 13.67353630065918,
      "learning_rate": 1.2414433228912867e-06,
      "loss": 1.7626,
      "step": 1512900
    },
    {
      "epoch": 58.51413543721236,
      "grad_norm": 10.832413673400879,
      "learning_rate": 1.2382204689897e-06,
      "loss": 1.7402,
      "step": 1513000
    },
    {
      "epoch": 58.51800286189427,
      "grad_norm": 7.475948333740234,
      "learning_rate": 1.2349976150881128e-06,
      "loss": 1.7341,
      "step": 1513100
    },
    {
      "epoch": 58.52187028657617,
      "grad_norm": 12.491442680358887,
      "learning_rate": 1.231774761186526e-06,
      "loss": 1.8002,
      "step": 1513200
    },
    {
      "epoch": 58.525737711258074,
      "grad_norm": 15.96251392364502,
      "learning_rate": 1.228551907284939e-06,
      "loss": 1.8073,
      "step": 1513300
    },
    {
      "epoch": 58.529605135939974,
      "grad_norm": 12.149499893188477,
      "learning_rate": 1.225329053383352e-06,
      "loss": 1.7705,
      "step": 1513400
    },
    {
      "epoch": 58.53347256062188,
      "grad_norm": 11.539695739746094,
      "learning_rate": 1.222106199481765e-06,
      "loss": 1.8258,
      "step": 1513500
    },
    {
      "epoch": 58.53733998530379,
      "grad_norm": 12.1417236328125,
      "learning_rate": 1.2188833455801781e-06,
      "loss": 1.7058,
      "step": 1513600
    },
    {
      "epoch": 58.54120740998569,
      "grad_norm": 13.503387451171875,
      "learning_rate": 1.2156604916785913e-06,
      "loss": 1.8415,
      "step": 1513700
    },
    {
      "epoch": 58.545074834667595,
      "grad_norm": 16.411638259887695,
      "learning_rate": 1.2124376377770044e-06,
      "loss": 1.76,
      "step": 1513800
    },
    {
      "epoch": 58.5489422593495,
      "grad_norm": 11.107605934143066,
      "learning_rate": 1.2092147838754174e-06,
      "loss": 1.7082,
      "step": 1513900
    },
    {
      "epoch": 58.5528096840314,
      "grad_norm": 10.596229553222656,
      "learning_rate": 1.2059919299738304e-06,
      "loss": 1.7557,
      "step": 1514000
    },
    {
      "epoch": 58.55667710871331,
      "grad_norm": 10.267504692077637,
      "learning_rate": 1.2027690760722437e-06,
      "loss": 1.6866,
      "step": 1514100
    },
    {
      "epoch": 58.56054453339521,
      "grad_norm": 11.407279014587402,
      "learning_rate": 1.1995462221706567e-06,
      "loss": 1.7556,
      "step": 1514200
    },
    {
      "epoch": 58.564411958077116,
      "grad_norm": 12.275370597839355,
      "learning_rate": 1.1963233682690697e-06,
      "loss": 1.7608,
      "step": 1514300
    },
    {
      "epoch": 58.56827938275902,
      "grad_norm": 10.594115257263184,
      "learning_rate": 1.1931005143674827e-06,
      "loss": 1.7707,
      "step": 1514400
    },
    {
      "epoch": 58.57214680744092,
      "grad_norm": 13.020902633666992,
      "learning_rate": 1.189877660465896e-06,
      "loss": 1.831,
      "step": 1514500
    },
    {
      "epoch": 58.57601423212283,
      "grad_norm": 13.025641441345215,
      "learning_rate": 1.1866548065643088e-06,
      "loss": 1.8068,
      "step": 1514600
    },
    {
      "epoch": 58.57988165680474,
      "grad_norm": 11.378202438354492,
      "learning_rate": 1.183431952662722e-06,
      "loss": 1.7575,
      "step": 1514700
    },
    {
      "epoch": 58.58374908148664,
      "grad_norm": 11.030293464660645,
      "learning_rate": 1.180209098761135e-06,
      "loss": 1.7913,
      "step": 1514800
    },
    {
      "epoch": 58.587616506168544,
      "grad_norm": 14.825980186462402,
      "learning_rate": 1.176986244859548e-06,
      "loss": 1.7546,
      "step": 1514900
    },
    {
      "epoch": 58.591483930850444,
      "grad_norm": 11.783014297485352,
      "learning_rate": 1.173763390957961e-06,
      "loss": 1.7772,
      "step": 1515000
    },
    {
      "epoch": 58.59535135553235,
      "grad_norm": 13.992681503295898,
      "learning_rate": 1.1705405370563741e-06,
      "loss": 1.7904,
      "step": 1515100
    },
    {
      "epoch": 58.59921878021426,
      "grad_norm": 11.184328079223633,
      "learning_rate": 1.1673176831547873e-06,
      "loss": 1.7425,
      "step": 1515200
    },
    {
      "epoch": 58.60308620489616,
      "grad_norm": 12.891958236694336,
      "learning_rate": 1.1640948292532004e-06,
      "loss": 1.8253,
      "step": 1515300
    },
    {
      "epoch": 58.606953629578065,
      "grad_norm": 12.891251564025879,
      "learning_rate": 1.1608719753516134e-06,
      "loss": 1.8007,
      "step": 1515400
    },
    {
      "epoch": 58.610821054259965,
      "grad_norm": 13.157455444335938,
      "learning_rate": 1.1576491214500264e-06,
      "loss": 1.7581,
      "step": 1515500
    },
    {
      "epoch": 58.61468847894187,
      "grad_norm": 13.986754417419434,
      "learning_rate": 1.1544262675484397e-06,
      "loss": 1.7608,
      "step": 1515600
    },
    {
      "epoch": 58.61855590362378,
      "grad_norm": 13.946233749389648,
      "learning_rate": 1.1512034136468525e-06,
      "loss": 1.7847,
      "step": 1515700
    },
    {
      "epoch": 58.62242332830568,
      "grad_norm": 15.17150592803955,
      "learning_rate": 1.1479805597452657e-06,
      "loss": 1.7626,
      "step": 1515800
    },
    {
      "epoch": 58.626290752987586,
      "grad_norm": 11.06628704071045,
      "learning_rate": 1.1447577058436787e-06,
      "loss": 1.7686,
      "step": 1515900
    },
    {
      "epoch": 58.63015817766949,
      "grad_norm": 9.362346649169922,
      "learning_rate": 1.141534851942092e-06,
      "loss": 1.7942,
      "step": 1516000
    },
    {
      "epoch": 58.63402560235139,
      "grad_norm": 11.368179321289062,
      "learning_rate": 1.1383119980405048e-06,
      "loss": 1.6911,
      "step": 1516100
    },
    {
      "epoch": 58.6378930270333,
      "grad_norm": 13.333027839660645,
      "learning_rate": 1.135089144138918e-06,
      "loss": 1.7361,
      "step": 1516200
    },
    {
      "epoch": 58.6417604517152,
      "grad_norm": 12.297121047973633,
      "learning_rate": 1.131866290237331e-06,
      "loss": 1.7377,
      "step": 1516300
    },
    {
      "epoch": 58.64562787639711,
      "grad_norm": 14.589611053466797,
      "learning_rate": 1.128643436335744e-06,
      "loss": 1.7625,
      "step": 1516400
    },
    {
      "epoch": 58.649495301079014,
      "grad_norm": 13.554483413696289,
      "learning_rate": 1.125420582434157e-06,
      "loss": 1.8105,
      "step": 1516500
    },
    {
      "epoch": 58.653362725760914,
      "grad_norm": 11.585253715515137,
      "learning_rate": 1.1221977285325703e-06,
      "loss": 1.8385,
      "step": 1516600
    },
    {
      "epoch": 58.65723015044282,
      "grad_norm": 16.76940155029297,
      "learning_rate": 1.1189748746309833e-06,
      "loss": 1.7289,
      "step": 1516700
    },
    {
      "epoch": 58.66109757512472,
      "grad_norm": 18.474079132080078,
      "learning_rate": 1.1157520207293964e-06,
      "loss": 1.8093,
      "step": 1516800
    },
    {
      "epoch": 58.66496499980663,
      "grad_norm": 11.406521797180176,
      "learning_rate": 1.1125291668278094e-06,
      "loss": 1.8704,
      "step": 1516900
    },
    {
      "epoch": 58.668832424488535,
      "grad_norm": 12.26057243347168,
      "learning_rate": 1.1093063129262224e-06,
      "loss": 1.8566,
      "step": 1517000
    },
    {
      "epoch": 58.672699849170435,
      "grad_norm": 12.05752182006836,
      "learning_rate": 1.1060834590246357e-06,
      "loss": 1.8163,
      "step": 1517100
    },
    {
      "epoch": 58.67656727385234,
      "grad_norm": 12.30772876739502,
      "learning_rate": 1.1028606051230485e-06,
      "loss": 1.7625,
      "step": 1517200
    },
    {
      "epoch": 58.68043469853425,
      "grad_norm": 15.340195655822754,
      "learning_rate": 1.0996377512214617e-06,
      "loss": 1.8673,
      "step": 1517300
    },
    {
      "epoch": 58.68430212321615,
      "grad_norm": 11.58895492553711,
      "learning_rate": 1.0964148973198747e-06,
      "loss": 1.7437,
      "step": 1517400
    },
    {
      "epoch": 58.688169547898056,
      "grad_norm": 12.555767059326172,
      "learning_rate": 1.093192043418288e-06,
      "loss": 1.7776,
      "step": 1517500
    },
    {
      "epoch": 58.692036972579956,
      "grad_norm": 13.569825172424316,
      "learning_rate": 1.0899691895167008e-06,
      "loss": 1.7885,
      "step": 1517600
    },
    {
      "epoch": 58.69590439726186,
      "grad_norm": 14.238391876220703,
      "learning_rate": 1.086746335615114e-06,
      "loss": 1.924,
      "step": 1517700
    },
    {
      "epoch": 58.69977182194377,
      "grad_norm": 13.644487380981445,
      "learning_rate": 1.083523481713527e-06,
      "loss": 1.8583,
      "step": 1517800
    },
    {
      "epoch": 58.70363924662567,
      "grad_norm": 8.162195205688477,
      "learning_rate": 1.08030062781194e-06,
      "loss": 1.653,
      "step": 1517900
    },
    {
      "epoch": 58.70750667130758,
      "grad_norm": 14.892970085144043,
      "learning_rate": 1.077077773910353e-06,
      "loss": 1.7243,
      "step": 1518000
    },
    {
      "epoch": 58.711374095989484,
      "grad_norm": 9.485713005065918,
      "learning_rate": 1.0738549200087663e-06,
      "loss": 1.7855,
      "step": 1518100
    },
    {
      "epoch": 58.715241520671384,
      "grad_norm": 12.079087257385254,
      "learning_rate": 1.0706320661071793e-06,
      "loss": 1.7872,
      "step": 1518200
    },
    {
      "epoch": 58.71910894535329,
      "grad_norm": 13.334515571594238,
      "learning_rate": 1.0674092122055924e-06,
      "loss": 1.7393,
      "step": 1518300
    },
    {
      "epoch": 58.72297637003519,
      "grad_norm": 12.485602378845215,
      "learning_rate": 1.0641863583040054e-06,
      "loss": 1.8762,
      "step": 1518400
    },
    {
      "epoch": 58.7268437947171,
      "grad_norm": 12.71357536315918,
      "learning_rate": 1.0609635044024184e-06,
      "loss": 1.7418,
      "step": 1518500
    },
    {
      "epoch": 58.730711219399005,
      "grad_norm": 13.594747543334961,
      "learning_rate": 1.0577406505008317e-06,
      "loss": 1.7577,
      "step": 1518600
    },
    {
      "epoch": 58.734578644080905,
      "grad_norm": 14.146345138549805,
      "learning_rate": 1.0545177965992445e-06,
      "loss": 1.88,
      "step": 1518700
    },
    {
      "epoch": 58.73844606876281,
      "grad_norm": 16.455625534057617,
      "learning_rate": 1.0512949426976577e-06,
      "loss": 1.7655,
      "step": 1518800
    },
    {
      "epoch": 58.74231349344471,
      "grad_norm": 13.02850341796875,
      "learning_rate": 1.0480720887960707e-06,
      "loss": 1.7628,
      "step": 1518900
    },
    {
      "epoch": 58.74618091812662,
      "grad_norm": 14.129910469055176,
      "learning_rate": 1.044849234894484e-06,
      "loss": 1.7777,
      "step": 1519000
    },
    {
      "epoch": 58.750048342808526,
      "grad_norm": 13.468159675598145,
      "learning_rate": 1.0416263809928968e-06,
      "loss": 1.8212,
      "step": 1519100
    },
    {
      "epoch": 58.753915767490426,
      "grad_norm": 13.917505264282227,
      "learning_rate": 1.03840352709131e-06,
      "loss": 1.7779,
      "step": 1519200
    },
    {
      "epoch": 58.75778319217233,
      "grad_norm": 11.409377098083496,
      "learning_rate": 1.035180673189723e-06,
      "loss": 1.7983,
      "step": 1519300
    },
    {
      "epoch": 58.76165061685424,
      "grad_norm": 12.106040000915527,
      "learning_rate": 1.031957819288136e-06,
      "loss": 1.746,
      "step": 1519400
    },
    {
      "epoch": 58.76551804153614,
      "grad_norm": 13.021714210510254,
      "learning_rate": 1.028734965386549e-06,
      "loss": 1.8011,
      "step": 1519500
    },
    {
      "epoch": 58.76938546621805,
      "grad_norm": 16.198436737060547,
      "learning_rate": 1.0255121114849623e-06,
      "loss": 1.7784,
      "step": 1519600
    },
    {
      "epoch": 58.77325289089995,
      "grad_norm": 12.79466438293457,
      "learning_rate": 1.0222892575833753e-06,
      "loss": 1.7397,
      "step": 1519700
    },
    {
      "epoch": 58.777120315581854,
      "grad_norm": 14.589022636413574,
      "learning_rate": 1.0190664036817884e-06,
      "loss": 1.8413,
      "step": 1519800
    },
    {
      "epoch": 58.78098774026376,
      "grad_norm": 10.82413387298584,
      "learning_rate": 1.0158435497802014e-06,
      "loss": 1.7592,
      "step": 1519900
    },
    {
      "epoch": 58.78485516494566,
      "grad_norm": 12.2799654006958,
      "learning_rate": 1.0126206958786144e-06,
      "loss": 1.8241,
      "step": 1520000
    },
    {
      "epoch": 58.78872258962757,
      "grad_norm": 12.971179008483887,
      "learning_rate": 1.0093978419770277e-06,
      "loss": 1.6768,
      "step": 1520100
    },
    {
      "epoch": 58.79259001430947,
      "grad_norm": 10.243490219116211,
      "learning_rate": 1.0061749880754405e-06,
      "loss": 1.7136,
      "step": 1520200
    },
    {
      "epoch": 58.796457438991375,
      "grad_norm": 10.932695388793945,
      "learning_rate": 1.0029521341738537e-06,
      "loss": 1.853,
      "step": 1520300
    },
    {
      "epoch": 58.80032486367328,
      "grad_norm": 14.413111686706543,
      "learning_rate": 9.997292802722667e-07,
      "loss": 1.7641,
      "step": 1520400
    },
    {
      "epoch": 58.80419228835518,
      "grad_norm": 13.12447452545166,
      "learning_rate": 9.965064263706797e-07,
      "loss": 1.7064,
      "step": 1520500
    },
    {
      "epoch": 58.80805971303709,
      "grad_norm": 12.672008514404297,
      "learning_rate": 9.932835724690928e-07,
      "loss": 1.7758,
      "step": 1520600
    },
    {
      "epoch": 58.811927137718996,
      "grad_norm": 15.852242469787598,
      "learning_rate": 9.90060718567506e-07,
      "loss": 1.81,
      "step": 1520700
    },
    {
      "epoch": 58.815794562400896,
      "grad_norm": 13.381996154785156,
      "learning_rate": 9.86837864665919e-07,
      "loss": 1.684,
      "step": 1520800
    },
    {
      "epoch": 58.8196619870828,
      "grad_norm": 6.255518913269043,
      "learning_rate": 9.83615010764332e-07,
      "loss": 1.7551,
      "step": 1520900
    },
    {
      "epoch": 58.8235294117647,
      "grad_norm": 18.39891815185547,
      "learning_rate": 9.80392156862745e-07,
      "loss": 1.8307,
      "step": 1521000
    },
    {
      "epoch": 58.82739683644661,
      "grad_norm": 7.870040416717529,
      "learning_rate": 9.771693029611583e-07,
      "loss": 1.776,
      "step": 1521100
    },
    {
      "epoch": 58.83126426112852,
      "grad_norm": 9.300217628479004,
      "learning_rate": 9.739464490595713e-07,
      "loss": 1.6842,
      "step": 1521200
    },
    {
      "epoch": 58.83513168581042,
      "grad_norm": 14.299905776977539,
      "learning_rate": 9.707235951579844e-07,
      "loss": 1.8724,
      "step": 1521300
    },
    {
      "epoch": 58.838999110492324,
      "grad_norm": 9.847412109375,
      "learning_rate": 9.675007412563974e-07,
      "loss": 1.7445,
      "step": 1521400
    },
    {
      "epoch": 58.842866535174224,
      "grad_norm": 15.253448486328125,
      "learning_rate": 9.642778873548106e-07,
      "loss": 1.7371,
      "step": 1521500
    },
    {
      "epoch": 58.84673395985613,
      "grad_norm": 11.256295204162598,
      "learning_rate": 9.610550334532234e-07,
      "loss": 1.8227,
      "step": 1521600
    },
    {
      "epoch": 58.85060138453804,
      "grad_norm": 14.188264846801758,
      "learning_rate": 9.578321795516367e-07,
      "loss": 1.6792,
      "step": 1521700
    },
    {
      "epoch": 58.85446880921994,
      "grad_norm": 11.099913597106934,
      "learning_rate": 9.546093256500497e-07,
      "loss": 1.7211,
      "step": 1521800
    },
    {
      "epoch": 58.858336233901845,
      "grad_norm": 13.732398986816406,
      "learning_rate": 9.513864717484628e-07,
      "loss": 1.868,
      "step": 1521900
    },
    {
      "epoch": 58.86220365858375,
      "grad_norm": 15.116315841674805,
      "learning_rate": 9.481636178468757e-07,
      "loss": 1.8358,
      "step": 1522000
    },
    {
      "epoch": 58.86607108326565,
      "grad_norm": 13.198740005493164,
      "learning_rate": 9.449407639452889e-07,
      "loss": 1.8687,
      "step": 1522100
    },
    {
      "epoch": 58.86993850794756,
      "grad_norm": 14.181427001953125,
      "learning_rate": 9.41717910043702e-07,
      "loss": 1.7905,
      "step": 1522200
    },
    {
      "epoch": 58.87380593262946,
      "grad_norm": 10.930665016174316,
      "learning_rate": 9.384950561421151e-07,
      "loss": 1.709,
      "step": 1522300
    },
    {
      "epoch": 58.877673357311366,
      "grad_norm": 11.777223587036133,
      "learning_rate": 9.352722022405281e-07,
      "loss": 1.7961,
      "step": 1522400
    },
    {
      "epoch": 58.88154078199327,
      "grad_norm": 13.760896682739258,
      "learning_rate": 9.320493483389412e-07,
      "loss": 1.768,
      "step": 1522500
    },
    {
      "epoch": 58.88540820667517,
      "grad_norm": 13.671219825744629,
      "learning_rate": 9.288264944373542e-07,
      "loss": 1.8132,
      "step": 1522600
    },
    {
      "epoch": 58.88927563135708,
      "grad_norm": 12.752664566040039,
      "learning_rate": 9.256036405357672e-07,
      "loss": 1.7632,
      "step": 1522700
    },
    {
      "epoch": 58.89314305603899,
      "grad_norm": 10.161407470703125,
      "learning_rate": 9.223807866341803e-07,
      "loss": 1.6677,
      "step": 1522800
    },
    {
      "epoch": 58.89701048072089,
      "grad_norm": 13.742206573486328,
      "learning_rate": 9.191579327325934e-07,
      "loss": 1.7484,
      "step": 1522900
    },
    {
      "epoch": 58.900877905402794,
      "grad_norm": 10.593060493469238,
      "learning_rate": 9.159350788310065e-07,
      "loss": 1.7422,
      "step": 1523000
    },
    {
      "epoch": 58.904745330084694,
      "grad_norm": 11.770695686340332,
      "learning_rate": 9.127122249294194e-07,
      "loss": 1.7217,
      "step": 1523100
    },
    {
      "epoch": 58.9086127547666,
      "grad_norm": 15.668746948242188,
      "learning_rate": 9.094893710278326e-07,
      "loss": 1.7099,
      "step": 1523200
    },
    {
      "epoch": 58.91248017944851,
      "grad_norm": 12.053155899047852,
      "learning_rate": 9.062665171262457e-07,
      "loss": 1.7279,
      "step": 1523300
    },
    {
      "epoch": 58.91634760413041,
      "grad_norm": 11.027528762817383,
      "learning_rate": 9.030436632246588e-07,
      "loss": 1.7206,
      "step": 1523400
    },
    {
      "epoch": 58.920215028812315,
      "grad_norm": 12.47795581817627,
      "learning_rate": 8.998208093230717e-07,
      "loss": 1.7375,
      "step": 1523500
    },
    {
      "epoch": 58.924082453494215,
      "grad_norm": 15.478119850158691,
      "learning_rate": 8.965979554214849e-07,
      "loss": 1.7724,
      "step": 1523600
    },
    {
      "epoch": 58.92794987817612,
      "grad_norm": 12.675012588500977,
      "learning_rate": 8.93375101519898e-07,
      "loss": 1.8447,
      "step": 1523700
    },
    {
      "epoch": 58.93181730285803,
      "grad_norm": 14.040556907653809,
      "learning_rate": 8.901522476183109e-07,
      "loss": 1.8064,
      "step": 1523800
    },
    {
      "epoch": 58.93568472753993,
      "grad_norm": 14.5030517578125,
      "learning_rate": 8.869293937167241e-07,
      "loss": 1.8626,
      "step": 1523900
    },
    {
      "epoch": 58.939552152221836,
      "grad_norm": 13.387717247009277,
      "learning_rate": 8.837065398151372e-07,
      "loss": 1.7558,
      "step": 1524000
    },
    {
      "epoch": 58.94341957690374,
      "grad_norm": 12.83082389831543,
      "learning_rate": 8.804836859135502e-07,
      "loss": 1.6834,
      "step": 1524100
    },
    {
      "epoch": 58.94728700158564,
      "grad_norm": 10.655227661132812,
      "learning_rate": 8.772608320119632e-07,
      "loss": 1.7575,
      "step": 1524200
    },
    {
      "epoch": 58.95115442626755,
      "grad_norm": 11.268531799316406,
      "learning_rate": 8.740379781103764e-07,
      "loss": 1.811,
      "step": 1524300
    },
    {
      "epoch": 58.95502185094945,
      "grad_norm": 11.280776977539062,
      "learning_rate": 8.708151242087894e-07,
      "loss": 1.8227,
      "step": 1524400
    },
    {
      "epoch": 58.95888927563136,
      "grad_norm": 8.53370475769043,
      "learning_rate": 8.675922703072025e-07,
      "loss": 1.868,
      "step": 1524500
    },
    {
      "epoch": 58.962756700313264,
      "grad_norm": 13.274477005004883,
      "learning_rate": 8.643694164056154e-07,
      "loss": 1.726,
      "step": 1524600
    },
    {
      "epoch": 58.966624124995164,
      "grad_norm": 15.609621047973633,
      "learning_rate": 8.611465625040286e-07,
      "loss": 1.7315,
      "step": 1524700
    },
    {
      "epoch": 58.97049154967707,
      "grad_norm": 11.139883041381836,
      "learning_rate": 8.579237086024417e-07,
      "loss": 1.7585,
      "step": 1524800
    },
    {
      "epoch": 58.97435897435897,
      "grad_norm": 11.123920440673828,
      "learning_rate": 8.547008547008548e-07,
      "loss": 1.7689,
      "step": 1524900
    },
    {
      "epoch": 58.97822639904088,
      "grad_norm": 7.5789971351623535,
      "learning_rate": 8.514780007992677e-07,
      "loss": 1.8229,
      "step": 1525000
    },
    {
      "epoch": 58.982093823722785,
      "grad_norm": 15.901342391967773,
      "learning_rate": 8.482551468976809e-07,
      "loss": 1.7533,
      "step": 1525100
    },
    {
      "epoch": 58.985961248404685,
      "grad_norm": 13.846895217895508,
      "learning_rate": 8.45032292996094e-07,
      "loss": 1.8624,
      "step": 1525200
    },
    {
      "epoch": 58.98982867308659,
      "grad_norm": 17.935020446777344,
      "learning_rate": 8.418094390945069e-07,
      "loss": 1.7434,
      "step": 1525300
    },
    {
      "epoch": 58.9936960977685,
      "grad_norm": 11.12527847290039,
      "learning_rate": 8.385865851929201e-07,
      "loss": 1.7712,
      "step": 1525400
    },
    {
      "epoch": 58.9975635224504,
      "grad_norm": 9.029988288879395,
      "learning_rate": 8.353637312913332e-07,
      "loss": 1.7698,
      "step": 1525500
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.7341625690460205,
      "eval_runtime": 2.9362,
      "eval_samples_per_second": 463.528,
      "eval_steps_per_second": 463.528,
      "step": 1525563
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.5634316205978394,
      "eval_runtime": 56.6998,
      "eval_samples_per_second": 456.034,
      "eval_steps_per_second": 456.034,
      "step": 1525563
    },
    {
      "epoch": 59.001430947132306,
      "grad_norm": 12.1786470413208,
      "learning_rate": 8.321408773897463e-07,
      "loss": 1.7552,
      "step": 1525600
    },
    {
      "epoch": 59.005298371814206,
      "grad_norm": 13.130943298339844,
      "learning_rate": 8.289180234881592e-07,
      "loss": 1.6961,
      "step": 1525700
    },
    {
      "epoch": 59.00916579649611,
      "grad_norm": 10.504623413085938,
      "learning_rate": 8.256951695865724e-07,
      "loss": 1.726,
      "step": 1525800
    },
    {
      "epoch": 59.01303322117802,
      "grad_norm": 11.185611724853516,
      "learning_rate": 8.224723156849854e-07,
      "loss": 1.793,
      "step": 1525900
    },
    {
      "epoch": 59.01690064585992,
      "grad_norm": 10.815563201904297,
      "learning_rate": 8.192494617833985e-07,
      "loss": 1.7493,
      "step": 1526000
    },
    {
      "epoch": 59.02076807054183,
      "grad_norm": 11.817290306091309,
      "learning_rate": 8.160266078818115e-07,
      "loss": 1.7251,
      "step": 1526100
    },
    {
      "epoch": 59.024635495223734,
      "grad_norm": 15.63811206817627,
      "learning_rate": 8.128037539802246e-07,
      "loss": 1.7954,
      "step": 1526200
    },
    {
      "epoch": 59.028502919905634,
      "grad_norm": 11.742593765258789,
      "learning_rate": 8.095809000786377e-07,
      "loss": 1.7876,
      "step": 1526300
    },
    {
      "epoch": 59.03237034458754,
      "grad_norm": 11.85618782043457,
      "learning_rate": 8.063580461770506e-07,
      "loss": 1.8114,
      "step": 1526400
    },
    {
      "epoch": 59.03623776926944,
      "grad_norm": 13.878512382507324,
      "learning_rate": 8.031351922754638e-07,
      "loss": 1.7733,
      "step": 1526500
    },
    {
      "epoch": 59.04010519395135,
      "grad_norm": 15.500096321105957,
      "learning_rate": 7.999123383738769e-07,
      "loss": 1.8103,
      "step": 1526600
    },
    {
      "epoch": 59.043972618633255,
      "grad_norm": 16.545001983642578,
      "learning_rate": 7.9668948447229e-07,
      "loss": 1.7561,
      "step": 1526700
    },
    {
      "epoch": 59.047840043315155,
      "grad_norm": 14.434377670288086,
      "learning_rate": 7.934666305707029e-07,
      "loss": 1.744,
      "step": 1526800
    },
    {
      "epoch": 59.05170746799706,
      "grad_norm": 13.33955192565918,
      "learning_rate": 7.902437766691161e-07,
      "loss": 1.7127,
      "step": 1526900
    },
    {
      "epoch": 59.05557489267896,
      "grad_norm": 10.356626510620117,
      "learning_rate": 7.870209227675292e-07,
      "loss": 1.6943,
      "step": 1527000
    },
    {
      "epoch": 59.05944231736087,
      "grad_norm": 12.706413269042969,
      "learning_rate": 7.837980688659423e-07,
      "loss": 1.6869,
      "step": 1527100
    },
    {
      "epoch": 59.063309742042776,
      "grad_norm": 13.415149688720703,
      "learning_rate": 7.805752149643553e-07,
      "loss": 1.8192,
      "step": 1527200
    },
    {
      "epoch": 59.067177166724676,
      "grad_norm": 13.846057891845703,
      "learning_rate": 7.773523610627684e-07,
      "loss": 1.8423,
      "step": 1527300
    },
    {
      "epoch": 59.07104459140658,
      "grad_norm": 13.353096008300781,
      "learning_rate": 7.741295071611814e-07,
      "loss": 1.7201,
      "step": 1527400
    },
    {
      "epoch": 59.07491201608849,
      "grad_norm": 10.99959945678711,
      "learning_rate": 7.709066532595945e-07,
      "loss": 1.7754,
      "step": 1527500
    },
    {
      "epoch": 59.07877944077039,
      "grad_norm": 12.738154411315918,
      "learning_rate": 7.676837993580075e-07,
      "loss": 1.826,
      "step": 1527600
    },
    {
      "epoch": 59.0826468654523,
      "grad_norm": 18.71672248840332,
      "learning_rate": 7.644609454564206e-07,
      "loss": 1.8128,
      "step": 1527700
    },
    {
      "epoch": 59.0865142901342,
      "grad_norm": 11.389311790466309,
      "learning_rate": 7.612380915548336e-07,
      "loss": 1.7847,
      "step": 1527800
    },
    {
      "epoch": 59.090381714816104,
      "grad_norm": 11.812240600585938,
      "learning_rate": 7.580152376532467e-07,
      "loss": 1.7749,
      "step": 1527900
    },
    {
      "epoch": 59.09424913949801,
      "grad_norm": 10.337937355041504,
      "learning_rate": 7.547923837516598e-07,
      "loss": 1.8028,
      "step": 1528000
    },
    {
      "epoch": 59.09811656417991,
      "grad_norm": 9.760369300842285,
      "learning_rate": 7.515695298500729e-07,
      "loss": 1.7202,
      "step": 1528100
    },
    {
      "epoch": 59.10198398886182,
      "grad_norm": 12.312322616577148,
      "learning_rate": 7.483466759484859e-07,
      "loss": 1.7807,
      "step": 1528200
    },
    {
      "epoch": 59.10585141354372,
      "grad_norm": 12.047316551208496,
      "learning_rate": 7.45123822046899e-07,
      "loss": 1.7665,
      "step": 1528300
    },
    {
      "epoch": 59.109718838225625,
      "grad_norm": 11.882485389709473,
      "learning_rate": 7.419009681453121e-07,
      "loss": 1.7772,
      "step": 1528400
    },
    {
      "epoch": 59.11358626290753,
      "grad_norm": 11.753297805786133,
      "learning_rate": 7.386781142437251e-07,
      "loss": 1.7656,
      "step": 1528500
    },
    {
      "epoch": 59.11745368758943,
      "grad_norm": 11.240204811096191,
      "learning_rate": 7.354552603421382e-07,
      "loss": 1.7469,
      "step": 1528600
    },
    {
      "epoch": 59.12132111227134,
      "grad_norm": 13.507783889770508,
      "learning_rate": 7.322324064405512e-07,
      "loss": 1.7958,
      "step": 1528700
    },
    {
      "epoch": 59.125188536953246,
      "grad_norm": 9.823150634765625,
      "learning_rate": 7.290095525389644e-07,
      "loss": 1.829,
      "step": 1528800
    },
    {
      "epoch": 59.129055961635146,
      "grad_norm": 16.56669807434082,
      "learning_rate": 7.257866986373774e-07,
      "loss": 1.8088,
      "step": 1528900
    },
    {
      "epoch": 59.13292338631705,
      "grad_norm": 13.220708847045898,
      "learning_rate": 7.225638447357905e-07,
      "loss": 1.8175,
      "step": 1529000
    },
    {
      "epoch": 59.13679081099895,
      "grad_norm": 16.606271743774414,
      "learning_rate": 7.193409908342035e-07,
      "loss": 1.8046,
      "step": 1529100
    },
    {
      "epoch": 59.14065823568086,
      "grad_norm": 7.922431945800781,
      "learning_rate": 7.161181369326167e-07,
      "loss": 1.85,
      "step": 1529200
    },
    {
      "epoch": 59.14452566036277,
      "grad_norm": 12.60785961151123,
      "learning_rate": 7.128952830310297e-07,
      "loss": 1.7958,
      "step": 1529300
    },
    {
      "epoch": 59.14839308504467,
      "grad_norm": 10.834390640258789,
      "learning_rate": 7.096724291294427e-07,
      "loss": 1.7367,
      "step": 1529400
    },
    {
      "epoch": 59.152260509726574,
      "grad_norm": 11.014579772949219,
      "learning_rate": 7.064495752278558e-07,
      "loss": 1.8242,
      "step": 1529500
    },
    {
      "epoch": 59.156127934408474,
      "grad_norm": 12.907022476196289,
      "learning_rate": 7.032267213262689e-07,
      "loss": 1.7184,
      "step": 1529600
    },
    {
      "epoch": 59.15999535909038,
      "grad_norm": 9.846797943115234,
      "learning_rate": 7.000038674246819e-07,
      "loss": 1.7605,
      "step": 1529700
    },
    {
      "epoch": 59.16386278377229,
      "grad_norm": 11.899335861206055,
      "learning_rate": 6.967810135230949e-07,
      "loss": 1.8066,
      "step": 1529800
    },
    {
      "epoch": 59.16773020845419,
      "grad_norm": 12.635775566101074,
      "learning_rate": 6.935581596215081e-07,
      "loss": 1.7346,
      "step": 1529900
    },
    {
      "epoch": 59.171597633136095,
      "grad_norm": 12.546467781066895,
      "learning_rate": 6.903353057199211e-07,
      "loss": 1.769,
      "step": 1530000
    },
    {
      "epoch": 59.175465057818,
      "grad_norm": 16.371458053588867,
      "learning_rate": 6.871124518183342e-07,
      "loss": 1.7559,
      "step": 1530100
    },
    {
      "epoch": 59.1793324824999,
      "grad_norm": 13.050322532653809,
      "learning_rate": 6.838895979167472e-07,
      "loss": 1.7131,
      "step": 1530200
    },
    {
      "epoch": 59.18319990718181,
      "grad_norm": 16.445003509521484,
      "learning_rate": 6.806667440151604e-07,
      "loss": 1.8026,
      "step": 1530300
    },
    {
      "epoch": 59.18706733186371,
      "grad_norm": 12.754127502441406,
      "learning_rate": 6.774438901135734e-07,
      "loss": 1.786,
      "step": 1530400
    },
    {
      "epoch": 59.190934756545616,
      "grad_norm": 18.02880859375,
      "learning_rate": 6.742210362119865e-07,
      "loss": 1.6668,
      "step": 1530500
    },
    {
      "epoch": 59.19480218122752,
      "grad_norm": 13.859811782836914,
      "learning_rate": 6.709981823103995e-07,
      "loss": 1.7605,
      "step": 1530600
    },
    {
      "epoch": 59.19866960590942,
      "grad_norm": 14.090205192565918,
      "learning_rate": 6.677753284088127e-07,
      "loss": 1.7731,
      "step": 1530700
    },
    {
      "epoch": 59.20253703059133,
      "grad_norm": 12.790582656860352,
      "learning_rate": 6.645524745072257e-07,
      "loss": 1.6972,
      "step": 1530800
    },
    {
      "epoch": 59.20640445527324,
      "grad_norm": 11.32308578491211,
      "learning_rate": 6.613296206056387e-07,
      "loss": 1.8338,
      "step": 1530900
    },
    {
      "epoch": 59.21027187995514,
      "grad_norm": 15.538301467895508,
      "learning_rate": 6.581067667040519e-07,
      "loss": 1.7397,
      "step": 1531000
    },
    {
      "epoch": 59.214139304637044,
      "grad_norm": 11.072500228881836,
      "learning_rate": 6.548839128024649e-07,
      "loss": 1.6736,
      "step": 1531100
    },
    {
      "epoch": 59.218006729318944,
      "grad_norm": 13.46949577331543,
      "learning_rate": 6.516610589008779e-07,
      "loss": 1.8099,
      "step": 1531200
    },
    {
      "epoch": 59.22187415400085,
      "grad_norm": 12.458498001098633,
      "learning_rate": 6.484382049992909e-07,
      "loss": 1.7803,
      "step": 1531300
    },
    {
      "epoch": 59.22574157868276,
      "grad_norm": 9.73681640625,
      "learning_rate": 6.452153510977041e-07,
      "loss": 1.7143,
      "step": 1531400
    },
    {
      "epoch": 59.22960900336466,
      "grad_norm": 12.609735488891602,
      "learning_rate": 6.419924971961171e-07,
      "loss": 1.7276,
      "step": 1531500
    },
    {
      "epoch": 59.233476428046565,
      "grad_norm": 14.16350269317627,
      "learning_rate": 6.387696432945302e-07,
      "loss": 1.6914,
      "step": 1531600
    },
    {
      "epoch": 59.237343852728465,
      "grad_norm": 12.863372802734375,
      "learning_rate": 6.355467893929432e-07,
      "loss": 1.6975,
      "step": 1531700
    },
    {
      "epoch": 59.24121127741037,
      "grad_norm": 12.398211479187012,
      "learning_rate": 6.323239354913564e-07,
      "loss": 1.7942,
      "step": 1531800
    },
    {
      "epoch": 59.24507870209228,
      "grad_norm": 12.968941688537598,
      "learning_rate": 6.291010815897694e-07,
      "loss": 1.7817,
      "step": 1531900
    },
    {
      "epoch": 59.24894612677418,
      "grad_norm": 11.269630432128906,
      "learning_rate": 6.258782276881825e-07,
      "loss": 1.7579,
      "step": 1532000
    },
    {
      "epoch": 59.252813551456086,
      "grad_norm": 11.215438842773438,
      "learning_rate": 6.226553737865955e-07,
      "loss": 1.8989,
      "step": 1532100
    },
    {
      "epoch": 59.25668097613799,
      "grad_norm": 16.106891632080078,
      "learning_rate": 6.194325198850086e-07,
      "loss": 1.7778,
      "step": 1532200
    },
    {
      "epoch": 59.26054840081989,
      "grad_norm": 19.122648239135742,
      "learning_rate": 6.162096659834217e-07,
      "loss": 1.728,
      "step": 1532300
    },
    {
      "epoch": 59.2644158255018,
      "grad_norm": 11.23240852355957,
      "learning_rate": 6.129868120818347e-07,
      "loss": 1.8197,
      "step": 1532400
    },
    {
      "epoch": 59.2682832501837,
      "grad_norm": 15.26301383972168,
      "learning_rate": 6.097639581802479e-07,
      "loss": 1.7672,
      "step": 1532500
    },
    {
      "epoch": 59.27215067486561,
      "grad_norm": 15.131118774414062,
      "learning_rate": 6.065411042786609e-07,
      "loss": 1.7986,
      "step": 1532600
    },
    {
      "epoch": 59.276018099547514,
      "grad_norm": 12.548354148864746,
      "learning_rate": 6.03318250377074e-07,
      "loss": 1.7746,
      "step": 1532700
    },
    {
      "epoch": 59.279885524229414,
      "grad_norm": 15.132206916809082,
      "learning_rate": 6.00095396475487e-07,
      "loss": 1.7838,
      "step": 1532800
    },
    {
      "epoch": 59.28375294891132,
      "grad_norm": 12.565102577209473,
      "learning_rate": 5.968725425739001e-07,
      "loss": 1.8006,
      "step": 1532900
    },
    {
      "epoch": 59.28762037359322,
      "grad_norm": 14.52939224243164,
      "learning_rate": 5.936496886723131e-07,
      "loss": 1.7335,
      "step": 1533000
    },
    {
      "epoch": 59.29148779827513,
      "grad_norm": 13.962708473205566,
      "learning_rate": 5.904268347707262e-07,
      "loss": 1.763,
      "step": 1533100
    },
    {
      "epoch": 59.295355222957035,
      "grad_norm": 11.927994728088379,
      "learning_rate": 5.872039808691392e-07,
      "loss": 1.7342,
      "step": 1533200
    },
    {
      "epoch": 59.299222647638935,
      "grad_norm": 9.822840690612793,
      "learning_rate": 5.839811269675523e-07,
      "loss": 1.8331,
      "step": 1533300
    },
    {
      "epoch": 59.30309007232084,
      "grad_norm": 11.381296157836914,
      "learning_rate": 5.807582730659654e-07,
      "loss": 1.8038,
      "step": 1533400
    },
    {
      "epoch": 59.30695749700275,
      "grad_norm": 15.128786087036133,
      "learning_rate": 5.775354191643784e-07,
      "loss": 1.7652,
      "step": 1533500
    },
    {
      "epoch": 59.31082492168465,
      "grad_norm": 11.75301742553711,
      "learning_rate": 5.743125652627915e-07,
      "loss": 1.7444,
      "step": 1533600
    },
    {
      "epoch": 59.314692346366556,
      "grad_norm": 12.693711280822754,
      "learning_rate": 5.710897113612046e-07,
      "loss": 1.7715,
      "step": 1533700
    },
    {
      "epoch": 59.318559771048456,
      "grad_norm": 13.719744682312012,
      "learning_rate": 5.678668574596177e-07,
      "loss": 1.7828,
      "step": 1533800
    },
    {
      "epoch": 59.32242719573036,
      "grad_norm": 14.850118637084961,
      "learning_rate": 5.646440035580307e-07,
      "loss": 1.7121,
      "step": 1533900
    },
    {
      "epoch": 59.32629462041227,
      "grad_norm": 12.983086585998535,
      "learning_rate": 5.614211496564439e-07,
      "loss": 1.7799,
      "step": 1534000
    },
    {
      "epoch": 59.33016204509417,
      "grad_norm": 11.868478775024414,
      "learning_rate": 5.581982957548569e-07,
      "loss": 1.8263,
      "step": 1534100
    },
    {
      "epoch": 59.33402946977608,
      "grad_norm": 11.125128746032715,
      "learning_rate": 5.5497544185327e-07,
      "loss": 1.7641,
      "step": 1534200
    },
    {
      "epoch": 59.337896894457984,
      "grad_norm": 12.853250503540039,
      "learning_rate": 5.51752587951683e-07,
      "loss": 1.7044,
      "step": 1534300
    },
    {
      "epoch": 59.341764319139884,
      "grad_norm": 15.501460075378418,
      "learning_rate": 5.485297340500961e-07,
      "loss": 1.7393,
      "step": 1534400
    },
    {
      "epoch": 59.34563174382179,
      "grad_norm": 11.06659984588623,
      "learning_rate": 5.453068801485092e-07,
      "loss": 1.7895,
      "step": 1534500
    },
    {
      "epoch": 59.34949916850369,
      "grad_norm": 17.707983016967773,
      "learning_rate": 5.420840262469222e-07,
      "loss": 1.8489,
      "step": 1534600
    },
    {
      "epoch": 59.3533665931856,
      "grad_norm": 14.799728393554688,
      "learning_rate": 5.388611723453352e-07,
      "loss": 1.7785,
      "step": 1534700
    },
    {
      "epoch": 59.357234017867505,
      "grad_norm": 13.266850471496582,
      "learning_rate": 5.356383184437483e-07,
      "loss": 1.6841,
      "step": 1534800
    },
    {
      "epoch": 59.361101442549405,
      "grad_norm": 12.432260513305664,
      "learning_rate": 5.324154645421614e-07,
      "loss": 1.8416,
      "step": 1534900
    },
    {
      "epoch": 59.36496886723131,
      "grad_norm": 12.708489418029785,
      "learning_rate": 5.291926106405744e-07,
      "loss": 1.7427,
      "step": 1535000
    },
    {
      "epoch": 59.36883629191321,
      "grad_norm": 9.455739974975586,
      "learning_rate": 5.259697567389875e-07,
      "loss": 1.7071,
      "step": 1535100
    },
    {
      "epoch": 59.37270371659512,
      "grad_norm": 11.872902870178223,
      "learning_rate": 5.227469028374006e-07,
      "loss": 1.7868,
      "step": 1535200
    },
    {
      "epoch": 59.376571141277026,
      "grad_norm": 14.679123878479004,
      "learning_rate": 5.195240489358137e-07,
      "loss": 1.7341,
      "step": 1535300
    },
    {
      "epoch": 59.380438565958926,
      "grad_norm": 11.561528205871582,
      "learning_rate": 5.163011950342267e-07,
      "loss": 1.7637,
      "step": 1535400
    },
    {
      "epoch": 59.38430599064083,
      "grad_norm": 11.676589965820312,
      "learning_rate": 5.130783411326399e-07,
      "loss": 1.6569,
      "step": 1535500
    },
    {
      "epoch": 59.38817341532274,
      "grad_norm": 13.300209999084473,
      "learning_rate": 5.098554872310529e-07,
      "loss": 1.8594,
      "step": 1535600
    },
    {
      "epoch": 59.39204084000464,
      "grad_norm": 19.470918655395508,
      "learning_rate": 5.066326333294659e-07,
      "loss": 1.7416,
      "step": 1535700
    },
    {
      "epoch": 59.39590826468655,
      "grad_norm": 12.535628318786621,
      "learning_rate": 5.03409779427879e-07,
      "loss": 1.7029,
      "step": 1535800
    },
    {
      "epoch": 59.39977568936845,
      "grad_norm": 11.280194282531738,
      "learning_rate": 5.001869255262921e-07,
      "loss": 1.8022,
      "step": 1535900
    },
    {
      "epoch": 59.403643114050354,
      "grad_norm": 13.787442207336426,
      "learning_rate": 4.969640716247052e-07,
      "loss": 1.7722,
      "step": 1536000
    },
    {
      "epoch": 59.40751053873226,
      "grad_norm": 10.040081977844238,
      "learning_rate": 4.937412177231182e-07,
      "loss": 1.7274,
      "step": 1536100
    },
    {
      "epoch": 59.41137796341416,
      "grad_norm": 12.670670509338379,
      "learning_rate": 4.905183638215312e-07,
      "loss": 1.7286,
      "step": 1536200
    },
    {
      "epoch": 59.41524538809607,
      "grad_norm": 10.630837440490723,
      "learning_rate": 4.872955099199443e-07,
      "loss": 1.7525,
      "step": 1536300
    },
    {
      "epoch": 59.41911281277797,
      "grad_norm": 12.713841438293457,
      "learning_rate": 4.840726560183574e-07,
      "loss": 1.7808,
      "step": 1536400
    },
    {
      "epoch": 59.422980237459875,
      "grad_norm": 12.209473609924316,
      "learning_rate": 4.808498021167704e-07,
      "loss": 1.7115,
      "step": 1536500
    },
    {
      "epoch": 59.42684766214178,
      "grad_norm": 12.40175724029541,
      "learning_rate": 4.776269482151835e-07,
      "loss": 1.7569,
      "step": 1536600
    },
    {
      "epoch": 59.43071508682368,
      "grad_norm": 12.23889446258545,
      "learning_rate": 4.7440409431359657e-07,
      "loss": 1.7778,
      "step": 1536700
    },
    {
      "epoch": 59.43458251150559,
      "grad_norm": 12.635141372680664,
      "learning_rate": 4.7118124041200965e-07,
      "loss": 1.7449,
      "step": 1536800
    },
    {
      "epoch": 59.438449936187496,
      "grad_norm": 11.630352973937988,
      "learning_rate": 4.6795838651042273e-07,
      "loss": 1.7201,
      "step": 1536900
    },
    {
      "epoch": 59.442317360869396,
      "grad_norm": 12.730409622192383,
      "learning_rate": 4.6473553260883575e-07,
      "loss": 1.7445,
      "step": 1537000
    },
    {
      "epoch": 59.4461847855513,
      "grad_norm": 9.25357437133789,
      "learning_rate": 4.615126787072489e-07,
      "loss": 1.6859,
      "step": 1537100
    },
    {
      "epoch": 59.4500522102332,
      "grad_norm": 12.47636890411377,
      "learning_rate": 4.582898248056619e-07,
      "loss": 1.799,
      "step": 1537200
    },
    {
      "epoch": 59.45391963491511,
      "grad_norm": 14.645602226257324,
      "learning_rate": 4.5506697090407504e-07,
      "loss": 1.7429,
      "step": 1537300
    },
    {
      "epoch": 59.45778705959702,
      "grad_norm": 11.512481689453125,
      "learning_rate": 4.5184411700248806e-07,
      "loss": 1.7753,
      "step": 1537400
    },
    {
      "epoch": 59.46165448427892,
      "grad_norm": 14.200976371765137,
      "learning_rate": 4.4862126310090114e-07,
      "loss": 1.825,
      "step": 1537500
    },
    {
      "epoch": 59.465521908960824,
      "grad_norm": 14.14843463897705,
      "learning_rate": 4.4539840919931416e-07,
      "loss": 1.8229,
      "step": 1537600
    },
    {
      "epoch": 59.46938933364273,
      "grad_norm": 11.499358177185059,
      "learning_rate": 4.421755552977273e-07,
      "loss": 1.811,
      "step": 1537700
    },
    {
      "epoch": 59.47325675832463,
      "grad_norm": 11.520811080932617,
      "learning_rate": 4.389527013961403e-07,
      "loss": 1.7486,
      "step": 1537800
    },
    {
      "epoch": 59.47712418300654,
      "grad_norm": 15.933989524841309,
      "learning_rate": 4.3572984749455345e-07,
      "loss": 1.8466,
      "step": 1537900
    },
    {
      "epoch": 59.48099160768844,
      "grad_norm": 9.679655075073242,
      "learning_rate": 4.3250699359296647e-07,
      "loss": 1.7568,
      "step": 1538000
    },
    {
      "epoch": 59.484859032370345,
      "grad_norm": 11.133797645568848,
      "learning_rate": 4.292841396913795e-07,
      "loss": 1.7122,
      "step": 1538100
    },
    {
      "epoch": 59.48872645705225,
      "grad_norm": 14.1553373336792,
      "learning_rate": 4.260612857897926e-07,
      "loss": 1.7695,
      "step": 1538200
    },
    {
      "epoch": 59.49259388173415,
      "grad_norm": 16.6137752532959,
      "learning_rate": 4.2283843188820565e-07,
      "loss": 1.7461,
      "step": 1538300
    },
    {
      "epoch": 59.49646130641606,
      "grad_norm": 12.281121253967285,
      "learning_rate": 4.1961557798661873e-07,
      "loss": 1.7232,
      "step": 1538400
    },
    {
      "epoch": 59.50032873109796,
      "grad_norm": 12.918030738830566,
      "learning_rate": 4.1639272408503175e-07,
      "loss": 1.7271,
      "step": 1538500
    },
    {
      "epoch": 59.504196155779866,
      "grad_norm": 12.538386344909668,
      "learning_rate": 4.131698701834449e-07,
      "loss": 1.707,
      "step": 1538600
    },
    {
      "epoch": 59.50806358046177,
      "grad_norm": 11.317193031311035,
      "learning_rate": 4.099470162818579e-07,
      "loss": 1.8047,
      "step": 1538700
    },
    {
      "epoch": 59.51193100514367,
      "grad_norm": 13.675867080688477,
      "learning_rate": 4.0672416238027104e-07,
      "loss": 1.7032,
      "step": 1538800
    },
    {
      "epoch": 59.51579842982558,
      "grad_norm": 14.513216972351074,
      "learning_rate": 4.0350130847868406e-07,
      "loss": 1.6944,
      "step": 1538900
    },
    {
      "epoch": 59.51966585450749,
      "grad_norm": 12.360503196716309,
      "learning_rate": 4.002784545770972e-07,
      "loss": 1.7573,
      "step": 1539000
    },
    {
      "epoch": 59.52353327918939,
      "grad_norm": 14.640981674194336,
      "learning_rate": 3.970556006755102e-07,
      "loss": 1.759,
      "step": 1539100
    },
    {
      "epoch": 59.527400703871294,
      "grad_norm": 11.917487144470215,
      "learning_rate": 3.9383274677392324e-07,
      "loss": 1.8085,
      "step": 1539200
    },
    {
      "epoch": 59.531268128553194,
      "grad_norm": 12.193391799926758,
      "learning_rate": 3.906098928723363e-07,
      "loss": 1.7659,
      "step": 1539300
    },
    {
      "epoch": 59.5351355532351,
      "grad_norm": 9.358193397521973,
      "learning_rate": 3.873870389707494e-07,
      "loss": 1.7875,
      "step": 1539400
    },
    {
      "epoch": 59.53900297791701,
      "grad_norm": 14.084517478942871,
      "learning_rate": 3.8416418506916247e-07,
      "loss": 1.7033,
      "step": 1539500
    },
    {
      "epoch": 59.54287040259891,
      "grad_norm": 11.200608253479004,
      "learning_rate": 3.8094133116757555e-07,
      "loss": 1.7535,
      "step": 1539600
    },
    {
      "epoch": 59.546737827280815,
      "grad_norm": 13.073521614074707,
      "learning_rate": 3.777184772659886e-07,
      "loss": 1.7428,
      "step": 1539700
    },
    {
      "epoch": 59.550605251962715,
      "grad_norm": 13.221680641174316,
      "learning_rate": 3.7449562336440165e-07,
      "loss": 1.7587,
      "step": 1539800
    },
    {
      "epoch": 59.55447267664462,
      "grad_norm": 12.764360427856445,
      "learning_rate": 3.7127276946281473e-07,
      "loss": 1.7834,
      "step": 1539900
    },
    {
      "epoch": 59.55834010132653,
      "grad_norm": 12.085403442382812,
      "learning_rate": 3.680499155612278e-07,
      "loss": 1.804,
      "step": 1540000
    },
    {
      "epoch": 59.56220752600843,
      "grad_norm": 11.794309616088867,
      "learning_rate": 3.648270616596409e-07,
      "loss": 1.7549,
      "step": 1540100
    },
    {
      "epoch": 59.566074950690336,
      "grad_norm": 12.795470237731934,
      "learning_rate": 3.616042077580539e-07,
      "loss": 1.8046,
      "step": 1540200
    },
    {
      "epoch": 59.56994237537224,
      "grad_norm": 13.958695411682129,
      "learning_rate": 3.58381353856467e-07,
      "loss": 1.6794,
      "step": 1540300
    },
    {
      "epoch": 59.57380980005414,
      "grad_norm": 15.80964183807373,
      "learning_rate": 3.5515849995488006e-07,
      "loss": 1.7534,
      "step": 1540400
    },
    {
      "epoch": 59.57767722473605,
      "grad_norm": 12.193629264831543,
      "learning_rate": 3.5193564605329314e-07,
      "loss": 1.7829,
      "step": 1540500
    },
    {
      "epoch": 59.58154464941795,
      "grad_norm": 12.187127113342285,
      "learning_rate": 3.487127921517062e-07,
      "loss": 1.7655,
      "step": 1540600
    },
    {
      "epoch": 59.58541207409986,
      "grad_norm": 15.277414321899414,
      "learning_rate": 3.454899382501193e-07,
      "loss": 1.7524,
      "step": 1540700
    },
    {
      "epoch": 59.589279498781764,
      "grad_norm": 11.545618057250977,
      "learning_rate": 3.4226708434853237e-07,
      "loss": 1.7485,
      "step": 1540800
    },
    {
      "epoch": 59.593146923463664,
      "grad_norm": 11.898778915405273,
      "learning_rate": 3.390442304469454e-07,
      "loss": 1.7563,
      "step": 1540900
    },
    {
      "epoch": 59.59701434814557,
      "grad_norm": 13.213932037353516,
      "learning_rate": 3.358213765453584e-07,
      "loss": 1.7682,
      "step": 1541000
    },
    {
      "epoch": 59.60088177282748,
      "grad_norm": 11.581742286682129,
      "learning_rate": 3.325985226437715e-07,
      "loss": 1.8017,
      "step": 1541100
    },
    {
      "epoch": 59.60474919750938,
      "grad_norm": 11.735200881958008,
      "learning_rate": 3.2937566874218457e-07,
      "loss": 1.7436,
      "step": 1541200
    },
    {
      "epoch": 59.608616622191285,
      "grad_norm": 14.844428062438965,
      "learning_rate": 3.2615281484059765e-07,
      "loss": 1.8329,
      "step": 1541300
    },
    {
      "epoch": 59.612484046873185,
      "grad_norm": 14.552515029907227,
      "learning_rate": 3.2292996093901073e-07,
      "loss": 1.725,
      "step": 1541400
    },
    {
      "epoch": 59.61635147155509,
      "grad_norm": 10.586341857910156,
      "learning_rate": 3.197071070374238e-07,
      "loss": 1.7471,
      "step": 1541500
    },
    {
      "epoch": 59.620218896237,
      "grad_norm": 10.104703903198242,
      "learning_rate": 3.164842531358369e-07,
      "loss": 1.7428,
      "step": 1541600
    },
    {
      "epoch": 59.6240863209189,
      "grad_norm": 9.306665420532227,
      "learning_rate": 3.1326139923424996e-07,
      "loss": 1.6785,
      "step": 1541700
    },
    {
      "epoch": 59.627953745600806,
      "grad_norm": 13.73372745513916,
      "learning_rate": 3.10038545332663e-07,
      "loss": 1.795,
      "step": 1541800
    },
    {
      "epoch": 59.631821170282706,
      "grad_norm": 12.03709602355957,
      "learning_rate": 3.0681569143107606e-07,
      "loss": 1.7379,
      "step": 1541900
    },
    {
      "epoch": 59.63568859496461,
      "grad_norm": 15.153231620788574,
      "learning_rate": 3.0359283752948914e-07,
      "loss": 1.7571,
      "step": 1542000
    },
    {
      "epoch": 59.63955601964652,
      "grad_norm": 12.83735179901123,
      "learning_rate": 3.0036998362790216e-07,
      "loss": 1.6997,
      "step": 1542100
    },
    {
      "epoch": 59.64342344432842,
      "grad_norm": 13.363349914550781,
      "learning_rate": 2.9714712972631524e-07,
      "loss": 1.6968,
      "step": 1542200
    },
    {
      "epoch": 59.64729086901033,
      "grad_norm": 15.149291038513184,
      "learning_rate": 2.939242758247283e-07,
      "loss": 1.7231,
      "step": 1542300
    },
    {
      "epoch": 59.651158293692234,
      "grad_norm": 16.722171783447266,
      "learning_rate": 2.907014219231414e-07,
      "loss": 1.7755,
      "step": 1542400
    },
    {
      "epoch": 59.655025718374134,
      "grad_norm": 11.123370170593262,
      "learning_rate": 2.8747856802155447e-07,
      "loss": 1.7257,
      "step": 1542500
    },
    {
      "epoch": 59.65889314305604,
      "grad_norm": 9.58456802368164,
      "learning_rate": 2.8425571411996755e-07,
      "loss": 1.7357,
      "step": 1542600
    },
    {
      "epoch": 59.66276056773794,
      "grad_norm": 14.170247077941895,
      "learning_rate": 2.8103286021838057e-07,
      "loss": 1.8628,
      "step": 1542700
    },
    {
      "epoch": 59.66662799241985,
      "grad_norm": 12.502555847167969,
      "learning_rate": 2.7781000631679365e-07,
      "loss": 1.8043,
      "step": 1542800
    },
    {
      "epoch": 59.670495417101755,
      "grad_norm": 12.71595573425293,
      "learning_rate": 2.7458715241520673e-07,
      "loss": 1.7996,
      "step": 1542900
    },
    {
      "epoch": 59.674362841783655,
      "grad_norm": 10.268766403198242,
      "learning_rate": 2.713642985136198e-07,
      "loss": 1.7159,
      "step": 1543000
    },
    {
      "epoch": 59.67823026646556,
      "grad_norm": 14.611793518066406,
      "learning_rate": 2.681414446120329e-07,
      "loss": 1.8231,
      "step": 1543100
    },
    {
      "epoch": 59.68209769114746,
      "grad_norm": 10.747712135314941,
      "learning_rate": 2.6491859071044596e-07,
      "loss": 1.8252,
      "step": 1543200
    },
    {
      "epoch": 59.68596511582937,
      "grad_norm": 11.57513427734375,
      "learning_rate": 2.61695736808859e-07,
      "loss": 1.7546,
      "step": 1543300
    },
    {
      "epoch": 59.689832540511276,
      "grad_norm": 9.726263999938965,
      "learning_rate": 2.5847288290727206e-07,
      "loss": 1.7684,
      "step": 1543400
    },
    {
      "epoch": 59.693699965193176,
      "grad_norm": 13.67393970489502,
      "learning_rate": 2.5525002900568514e-07,
      "loss": 1.7767,
      "step": 1543500
    },
    {
      "epoch": 59.69756738987508,
      "grad_norm": 13.98772144317627,
      "learning_rate": 2.5202717510409816e-07,
      "loss": 1.7331,
      "step": 1543600
    },
    {
      "epoch": 59.70143481455699,
      "grad_norm": 11.893433570861816,
      "learning_rate": 2.4880432120251124e-07,
      "loss": 1.8524,
      "step": 1543700
    },
    {
      "epoch": 59.70530223923889,
      "grad_norm": 10.596195220947266,
      "learning_rate": 2.455814673009243e-07,
      "loss": 1.8039,
      "step": 1543800
    },
    {
      "epoch": 59.7091696639208,
      "grad_norm": 13.803390502929688,
      "learning_rate": 2.423586133993374e-07,
      "loss": 1.7138,
      "step": 1543900
    },
    {
      "epoch": 59.7130370886027,
      "grad_norm": 12.40602970123291,
      "learning_rate": 2.3913575949775047e-07,
      "loss": 1.7698,
      "step": 1544000
    },
    {
      "epoch": 59.716904513284604,
      "grad_norm": 11.317479133605957,
      "learning_rate": 2.3591290559616352e-07,
      "loss": 1.7838,
      "step": 1544100
    },
    {
      "epoch": 59.72077193796651,
      "grad_norm": 11.90977668762207,
      "learning_rate": 2.326900516945766e-07,
      "loss": 1.6664,
      "step": 1544200
    },
    {
      "epoch": 59.72463936264841,
      "grad_norm": 11.175431251525879,
      "learning_rate": 2.2946719779298968e-07,
      "loss": 1.7101,
      "step": 1544300
    },
    {
      "epoch": 59.72850678733032,
      "grad_norm": 14.436012268066406,
      "learning_rate": 2.2624434389140275e-07,
      "loss": 1.6864,
      "step": 1544400
    },
    {
      "epoch": 59.73237421201222,
      "grad_norm": 12.844205856323242,
      "learning_rate": 2.2302148998981578e-07,
      "loss": 1.7232,
      "step": 1544500
    },
    {
      "epoch": 59.736241636694125,
      "grad_norm": 8.903828620910645,
      "learning_rate": 2.1979863608822886e-07,
      "loss": 1.7066,
      "step": 1544600
    },
    {
      "epoch": 59.74010906137603,
      "grad_norm": 10.393778800964355,
      "learning_rate": 2.165757821866419e-07,
      "loss": 1.7895,
      "step": 1544700
    },
    {
      "epoch": 59.74397648605793,
      "grad_norm": 10.784405708312988,
      "learning_rate": 2.1335292828505498e-07,
      "loss": 1.6278,
      "step": 1544800
    },
    {
      "epoch": 59.74784391073984,
      "grad_norm": 12.540298461914062,
      "learning_rate": 2.1013007438346806e-07,
      "loss": 1.8179,
      "step": 1544900
    },
    {
      "epoch": 59.751711335421746,
      "grad_norm": 12.866747856140137,
      "learning_rate": 2.069072204818811e-07,
      "loss": 1.7408,
      "step": 1545000
    },
    {
      "epoch": 59.755578760103646,
      "grad_norm": 9.825105667114258,
      "learning_rate": 2.036843665802942e-07,
      "loss": 1.7136,
      "step": 1545100
    },
    {
      "epoch": 59.75944618478555,
      "grad_norm": 12.444645881652832,
      "learning_rate": 2.0046151267870727e-07,
      "loss": 1.7872,
      "step": 1545200
    },
    {
      "epoch": 59.76331360946745,
      "grad_norm": 12.688603401184082,
      "learning_rate": 1.9723865877712034e-07,
      "loss": 1.7651,
      "step": 1545300
    },
    {
      "epoch": 59.76718103414936,
      "grad_norm": 15.36589241027832,
      "learning_rate": 1.940158048755334e-07,
      "loss": 1.8181,
      "step": 1545400
    },
    {
      "epoch": 59.77104845883127,
      "grad_norm": 15.43438720703125,
      "learning_rate": 1.9079295097394645e-07,
      "loss": 1.8626,
      "step": 1545500
    },
    {
      "epoch": 59.77491588351317,
      "grad_norm": 15.457608222961426,
      "learning_rate": 1.8757009707235952e-07,
      "loss": 1.7427,
      "step": 1545600
    },
    {
      "epoch": 59.778783308195074,
      "grad_norm": 15.755608558654785,
      "learning_rate": 1.843472431707726e-07,
      "loss": 1.7726,
      "step": 1545700
    },
    {
      "epoch": 59.782650732876974,
      "grad_norm": 15.650447845458984,
      "learning_rate": 1.8112438926918568e-07,
      "loss": 1.7339,
      "step": 1545800
    },
    {
      "epoch": 59.78651815755888,
      "grad_norm": 11.348740577697754,
      "learning_rate": 1.7790153536759873e-07,
      "loss": 1.8081,
      "step": 1545900
    },
    {
      "epoch": 59.79038558224079,
      "grad_norm": 9.974440574645996,
      "learning_rate": 1.7467868146601178e-07,
      "loss": 1.7701,
      "step": 1546000
    },
    {
      "epoch": 59.79425300692269,
      "grad_norm": 11.625072479248047,
      "learning_rate": 1.7145582756442486e-07,
      "loss": 1.7726,
      "step": 1546100
    },
    {
      "epoch": 59.798120431604595,
      "grad_norm": 9.76367473602295,
      "learning_rate": 1.6823297366283793e-07,
      "loss": 1.6688,
      "step": 1546200
    },
    {
      "epoch": 59.8019878562865,
      "grad_norm": 9.163336753845215,
      "learning_rate": 1.6501011976125098e-07,
      "loss": 1.7363,
      "step": 1546300
    },
    {
      "epoch": 59.8058552809684,
      "grad_norm": 13.266440391540527,
      "learning_rate": 1.6178726585966406e-07,
      "loss": 1.7686,
      "step": 1546400
    },
    {
      "epoch": 59.80972270565031,
      "grad_norm": 14.12766170501709,
      "learning_rate": 1.5856441195807714e-07,
      "loss": 1.8122,
      "step": 1546500
    },
    {
      "epoch": 59.81359013033221,
      "grad_norm": 12.718765258789062,
      "learning_rate": 1.553415580564902e-07,
      "loss": 1.7137,
      "step": 1546600
    },
    {
      "epoch": 59.817457555014116,
      "grad_norm": 10.846253395080566,
      "learning_rate": 1.5211870415490324e-07,
      "loss": 1.8933,
      "step": 1546700
    },
    {
      "epoch": 59.82132497969602,
      "grad_norm": 11.411291122436523,
      "learning_rate": 1.4889585025331632e-07,
      "loss": 1.7237,
      "step": 1546800
    },
    {
      "epoch": 59.82519240437792,
      "grad_norm": 12.49885082244873,
      "learning_rate": 1.456729963517294e-07,
      "loss": 1.7112,
      "step": 1546900
    },
    {
      "epoch": 59.82905982905983,
      "grad_norm": 14.922088623046875,
      "learning_rate": 1.4245014245014247e-07,
      "loss": 1.6393,
      "step": 1547000
    },
    {
      "epoch": 59.83292725374174,
      "grad_norm": 11.847541809082031,
      "learning_rate": 1.3922728854855552e-07,
      "loss": 1.6867,
      "step": 1547100
    },
    {
      "epoch": 59.83679467842364,
      "grad_norm": 13.984365463256836,
      "learning_rate": 1.3600443464696857e-07,
      "loss": 1.7598,
      "step": 1547200
    },
    {
      "epoch": 59.840662103105544,
      "grad_norm": 10.269301414489746,
      "learning_rate": 1.3278158074538165e-07,
      "loss": 1.785,
      "step": 1547300
    },
    {
      "epoch": 59.844529527787444,
      "grad_norm": 13.181182861328125,
      "learning_rate": 1.2955872684379473e-07,
      "loss": 1.742,
      "step": 1547400
    },
    {
      "epoch": 59.84839695246935,
      "grad_norm": 11.247147560119629,
      "learning_rate": 1.263358729422078e-07,
      "loss": 1.8012,
      "step": 1547500
    },
    {
      "epoch": 59.85226437715126,
      "grad_norm": 12.181609153747559,
      "learning_rate": 1.2311301904062086e-07,
      "loss": 1.7731,
      "step": 1547600
    },
    {
      "epoch": 59.85613180183316,
      "grad_norm": 15.052053451538086,
      "learning_rate": 1.198901651390339e-07,
      "loss": 1.8152,
      "step": 1547700
    },
    {
      "epoch": 59.859999226515065,
      "grad_norm": 12.767983436584473,
      "learning_rate": 1.1666731123744698e-07,
      "loss": 1.7126,
      "step": 1547800
    },
    {
      "epoch": 59.863866651196965,
      "grad_norm": 15.270853042602539,
      "learning_rate": 1.1344445733586005e-07,
      "loss": 1.806,
      "step": 1547900
    },
    {
      "epoch": 59.86773407587887,
      "grad_norm": 13.501716613769531,
      "learning_rate": 1.1022160343427313e-07,
      "loss": 1.7317,
      "step": 1548000
    },
    {
      "epoch": 59.87160150056078,
      "grad_norm": 11.603912353515625,
      "learning_rate": 1.0699874953268619e-07,
      "loss": 1.7502,
      "step": 1548100
    },
    {
      "epoch": 59.87546892524268,
      "grad_norm": 17.142114639282227,
      "learning_rate": 1.0377589563109927e-07,
      "loss": 1.6969,
      "step": 1548200
    },
    {
      "epoch": 59.879336349924586,
      "grad_norm": 13.485283851623535,
      "learning_rate": 1.0055304172951232e-07,
      "loss": 1.7076,
      "step": 1548300
    },
    {
      "epoch": 59.88320377460649,
      "grad_norm": 14.475685119628906,
      "learning_rate": 9.73301878279254e-08,
      "loss": 1.7286,
      "step": 1548400
    },
    {
      "epoch": 59.88707119928839,
      "grad_norm": 12.57640552520752,
      "learning_rate": 9.410733392633846e-08,
      "loss": 1.8198,
      "step": 1548500
    },
    {
      "epoch": 59.8909386239703,
      "grad_norm": 13.926739692687988,
      "learning_rate": 9.088448002475152e-08,
      "loss": 1.8743,
      "step": 1548600
    },
    {
      "epoch": 59.8948060486522,
      "grad_norm": 11.11225414276123,
      "learning_rate": 8.766162612316459e-08,
      "loss": 1.8066,
      "step": 1548700
    },
    {
      "epoch": 59.89867347333411,
      "grad_norm": 15.306804656982422,
      "learning_rate": 8.443877222157765e-08,
      "loss": 1.7495,
      "step": 1548800
    },
    {
      "epoch": 59.902540898016014,
      "grad_norm": 13.058866500854492,
      "learning_rate": 8.121591831999073e-08,
      "loss": 1.711,
      "step": 1548900
    },
    {
      "epoch": 59.906408322697914,
      "grad_norm": 10.747845649719238,
      "learning_rate": 7.799306441840379e-08,
      "loss": 1.7368,
      "step": 1549000
    },
    {
      "epoch": 59.91027574737982,
      "grad_norm": 16.53852653503418,
      "learning_rate": 7.477021051681686e-08,
      "loss": 1.7777,
      "step": 1549100
    },
    {
      "epoch": 59.91414317206173,
      "grad_norm": 10.887845993041992,
      "learning_rate": 7.154735661522992e-08,
      "loss": 1.7505,
      "step": 1549200
    },
    {
      "epoch": 59.91801059674363,
      "grad_norm": 10.561173439025879,
      "learning_rate": 6.8324502713643e-08,
      "loss": 1.7817,
      "step": 1549300
    },
    {
      "epoch": 59.921878021425535,
      "grad_norm": 12.12525463104248,
      "learning_rate": 6.510164881205605e-08,
      "loss": 1.7614,
      "step": 1549400
    },
    {
      "epoch": 59.925745446107435,
      "grad_norm": 15.05998420715332,
      "learning_rate": 6.187879491046913e-08,
      "loss": 1.7588,
      "step": 1549500
    },
    {
      "epoch": 59.92961287078934,
      "grad_norm": 14.333524703979492,
      "learning_rate": 5.8655941008882196e-08,
      "loss": 1.7506,
      "step": 1549600
    },
    {
      "epoch": 59.93348029547125,
      "grad_norm": 8.588041305541992,
      "learning_rate": 5.5433087107295254e-08,
      "loss": 1.7738,
      "step": 1549700
    },
    {
      "epoch": 59.93734772015315,
      "grad_norm": 13.330684661865234,
      "learning_rate": 5.2210233205708324e-08,
      "loss": 1.8197,
      "step": 1549800
    },
    {
      "epoch": 59.941215144835056,
      "grad_norm": 12.13268756866455,
      "learning_rate": 4.8987379304121395e-08,
      "loss": 1.7447,
      "step": 1549900
    },
    {
      "epoch": 59.945082569516956,
      "grad_norm": 8.19629192352295,
      "learning_rate": 4.576452540253445e-08,
      "loss": 1.8428,
      "step": 1550000
    },
    {
      "epoch": 59.94894999419886,
      "grad_norm": 13.139833450317383,
      "learning_rate": 4.254167150094752e-08,
      "loss": 1.8387,
      "step": 1550100
    },
    {
      "epoch": 59.95281741888077,
      "grad_norm": 12.307991027832031,
      "learning_rate": 3.931881759936059e-08,
      "loss": 1.8004,
      "step": 1550200
    },
    {
      "epoch": 59.95668484356267,
      "grad_norm": 11.96689224243164,
      "learning_rate": 3.609596369777365e-08,
      "loss": 1.7321,
      "step": 1550300
    },
    {
      "epoch": 59.96055226824458,
      "grad_norm": 16.76715660095215,
      "learning_rate": 3.287310979618672e-08,
      "loss": 1.7713,
      "step": 1550400
    },
    {
      "epoch": 59.964419692926484,
      "grad_norm": 14.738798141479492,
      "learning_rate": 2.9650255894599786e-08,
      "loss": 1.7621,
      "step": 1550500
    },
    {
      "epoch": 59.968287117608384,
      "grad_norm": 11.435529708862305,
      "learning_rate": 2.6427401993012853e-08,
      "loss": 1.747,
      "step": 1550600
    },
    {
      "epoch": 59.97215454229029,
      "grad_norm": 14.352448463439941,
      "learning_rate": 2.320454809142592e-08,
      "loss": 1.7444,
      "step": 1550700
    },
    {
      "epoch": 59.97602196697219,
      "grad_norm": 17.422197341918945,
      "learning_rate": 1.9981694189838988e-08,
      "loss": 1.7503,
      "step": 1550800
    },
    {
      "epoch": 59.9798893916541,
      "grad_norm": 10.698921203613281,
      "learning_rate": 1.6758840288252055e-08,
      "loss": 1.8094,
      "step": 1550900
    },
    {
      "epoch": 59.983756816336005,
      "grad_norm": 13.559167861938477,
      "learning_rate": 1.3535986386665119e-08,
      "loss": 1.69,
      "step": 1551000
    },
    {
      "epoch": 59.987624241017905,
      "grad_norm": 11.751595497131348,
      "learning_rate": 1.0313132485078186e-08,
      "loss": 1.7517,
      "step": 1551100
    },
    {
      "epoch": 59.99149166569981,
      "grad_norm": 13.390835762023926,
      "learning_rate": 7.090278583491253e-09,
      "loss": 1.7711,
      "step": 1551200
    },
    {
      "epoch": 59.99535909038171,
      "grad_norm": 10.673938751220703,
      "learning_rate": 3.86742468190432e-09,
      "loss": 1.7495,
      "step": 1551300
    },
    {
      "epoch": 59.99922651506362,
      "grad_norm": 16.186655044555664,
      "learning_rate": 6.445707803173867e-10,
      "loss": 1.8038,
      "step": 1551400
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.7337985038757324,
      "eval_runtime": 3.0568,
      "eval_samples_per_second": 445.241,
      "eval_steps_per_second": 445.241,
      "step": 1551420
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.5630828142166138,
      "eval_runtime": 55.546,
      "eval_samples_per_second": 465.506,
      "eval_steps_per_second": 465.506,
      "step": 1551420
    }
  ],
  "logging_steps": 100,
  "max_steps": 1551420,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 960031603814400.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
