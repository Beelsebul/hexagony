{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 60.0,
  "eval_steps": 500,
  "global_step": 765960,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007833307222309259,
      "grad_norm": 11.212727546691895,
      "learning_rate": 4.999347224398141e-05,
      "loss": 3.9825,
      "step": 100
    },
    {
      "epoch": 0.015666614444618518,
      "grad_norm": 8.532801628112793,
      "learning_rate": 4.998694448796282e-05,
      "loss": 2.9893,
      "step": 200
    },
    {
      "epoch": 0.023499921666927777,
      "grad_norm": 8.257346153259277,
      "learning_rate": 4.9980416731944226e-05,
      "loss": 2.7089,
      "step": 300
    },
    {
      "epoch": 0.031333228889237036,
      "grad_norm": 7.489764213562012,
      "learning_rate": 4.997388897592564e-05,
      "loss": 2.6916,
      "step": 400
    },
    {
      "epoch": 0.039166536111546295,
      "grad_norm": 8.02414321899414,
      "learning_rate": 4.996736121990705e-05,
      "loss": 2.7025,
      "step": 500
    },
    {
      "epoch": 0.046999843333855554,
      "grad_norm": 6.332596302032471,
      "learning_rate": 4.996083346388846e-05,
      "loss": 2.5461,
      "step": 600
    },
    {
      "epoch": 0.05483315055616481,
      "grad_norm": 6.801485061645508,
      "learning_rate": 4.995430570786986e-05,
      "loss": 2.5705,
      "step": 700
    },
    {
      "epoch": 0.06266645777847407,
      "grad_norm": 5.651359558105469,
      "learning_rate": 4.9947777951851276e-05,
      "loss": 2.5193,
      "step": 800
    },
    {
      "epoch": 0.07049976500078334,
      "grad_norm": 6.3678765296936035,
      "learning_rate": 4.994125019583268e-05,
      "loss": 2.3928,
      "step": 900
    },
    {
      "epoch": 0.07833307222309259,
      "grad_norm": 5.983728408813477,
      "learning_rate": 4.9934722439814094e-05,
      "loss": 2.4901,
      "step": 1000
    },
    {
      "epoch": 0.08616637944540186,
      "grad_norm": 4.899207592010498,
      "learning_rate": 4.99281946837955e-05,
      "loss": 2.511,
      "step": 1100
    },
    {
      "epoch": 0.09399968666771111,
      "grad_norm": 7.56265115737915,
      "learning_rate": 4.992166692777691e-05,
      "loss": 2.3821,
      "step": 1200
    },
    {
      "epoch": 0.10183299389002037,
      "grad_norm": 6.646008491516113,
      "learning_rate": 4.991513917175832e-05,
      "loss": 2.4605,
      "step": 1300
    },
    {
      "epoch": 0.10966630111232963,
      "grad_norm": 6.035210609436035,
      "learning_rate": 4.9908611415739724e-05,
      "loss": 2.435,
      "step": 1400
    },
    {
      "epoch": 0.11749960833463889,
      "grad_norm": 6.403013706207275,
      "learning_rate": 4.9902083659721136e-05,
      "loss": 2.469,
      "step": 1500
    },
    {
      "epoch": 0.12533291555694814,
      "grad_norm": 4.885438919067383,
      "learning_rate": 4.989555590370255e-05,
      "loss": 2.3893,
      "step": 1600
    },
    {
      "epoch": 0.1331662227792574,
      "grad_norm": 6.433860778808594,
      "learning_rate": 4.9889028147683955e-05,
      "loss": 2.3104,
      "step": 1700
    },
    {
      "epoch": 0.14099953000156668,
      "grad_norm": 7.488671779632568,
      "learning_rate": 4.988250039166537e-05,
      "loss": 2.4168,
      "step": 1800
    },
    {
      "epoch": 0.1488328372238759,
      "grad_norm": 7.049365043640137,
      "learning_rate": 4.987597263564677e-05,
      "loss": 2.3866,
      "step": 1900
    },
    {
      "epoch": 0.15666614444618518,
      "grad_norm": 6.394885540008545,
      "learning_rate": 4.986944487962818e-05,
      "loss": 2.359,
      "step": 2000
    },
    {
      "epoch": 0.16449945166849445,
      "grad_norm": 4.979330539703369,
      "learning_rate": 4.986291712360959e-05,
      "loss": 2.33,
      "step": 2100
    },
    {
      "epoch": 0.1723327588908037,
      "grad_norm": 6.175715923309326,
      "learning_rate": 4.9856389367591e-05,
      "loss": 2.2873,
      "step": 2200
    },
    {
      "epoch": 0.18016606611311295,
      "grad_norm": 6.070090293884277,
      "learning_rate": 4.984986161157241e-05,
      "loss": 2.388,
      "step": 2300
    },
    {
      "epoch": 0.18799937333542222,
      "grad_norm": 5.342574119567871,
      "learning_rate": 4.984333385555382e-05,
      "loss": 2.2315,
      "step": 2400
    },
    {
      "epoch": 0.19583268055773148,
      "grad_norm": 5.177824974060059,
      "learning_rate": 4.983680609953523e-05,
      "loss": 2.3711,
      "step": 2500
    },
    {
      "epoch": 0.20366598778004075,
      "grad_norm": 4.736539840698242,
      "learning_rate": 4.9830278343516634e-05,
      "loss": 2.4076,
      "step": 2600
    },
    {
      "epoch": 0.21149929500234999,
      "grad_norm": 6.12066650390625,
      "learning_rate": 4.982375058749804e-05,
      "loss": 2.3468,
      "step": 2700
    },
    {
      "epoch": 0.21933260222465925,
      "grad_norm": 5.341080188751221,
      "learning_rate": 4.981722283147945e-05,
      "loss": 2.331,
      "step": 2800
    },
    {
      "epoch": 0.22716590944696852,
      "grad_norm": 5.833035945892334,
      "learning_rate": 4.9810695075460865e-05,
      "loss": 2.3268,
      "step": 2900
    },
    {
      "epoch": 0.23499921666927778,
      "grad_norm": 4.931703090667725,
      "learning_rate": 4.980416731944227e-05,
      "loss": 2.3861,
      "step": 3000
    },
    {
      "epoch": 0.24283252389158702,
      "grad_norm": 7.029495716094971,
      "learning_rate": 4.979763956342368e-05,
      "loss": 2.3208,
      "step": 3100
    },
    {
      "epoch": 0.2506658311138963,
      "grad_norm": 6.58861780166626,
      "learning_rate": 4.979111180740509e-05,
      "loss": 2.3718,
      "step": 3200
    },
    {
      "epoch": 0.2584991383362055,
      "grad_norm": 5.514246940612793,
      "learning_rate": 4.9784584051386494e-05,
      "loss": 2.2004,
      "step": 3300
    },
    {
      "epoch": 0.2663324455585148,
      "grad_norm": 5.459942817687988,
      "learning_rate": 4.977805629536791e-05,
      "loss": 2.3212,
      "step": 3400
    },
    {
      "epoch": 0.27416575278082406,
      "grad_norm": 6.756868362426758,
      "learning_rate": 4.977152853934931e-05,
      "loss": 2.2719,
      "step": 3500
    },
    {
      "epoch": 0.28199906000313335,
      "grad_norm": 5.054433822631836,
      "learning_rate": 4.9765000783330725e-05,
      "loss": 2.2989,
      "step": 3600
    },
    {
      "epoch": 0.2898323672254426,
      "grad_norm": 5.98095703125,
      "learning_rate": 4.975847302731214e-05,
      "loss": 2.3207,
      "step": 3700
    },
    {
      "epoch": 0.2976656744477518,
      "grad_norm": 5.8603386878967285,
      "learning_rate": 4.9751945271293544e-05,
      "loss": 2.2025,
      "step": 3800
    },
    {
      "epoch": 0.3054989816700611,
      "grad_norm": 4.7171125411987305,
      "learning_rate": 4.974541751527495e-05,
      "loss": 2.2189,
      "step": 3900
    },
    {
      "epoch": 0.31333228889237036,
      "grad_norm": 5.640927791595459,
      "learning_rate": 4.973888975925636e-05,
      "loss": 2.3326,
      "step": 4000
    },
    {
      "epoch": 0.3211655961146796,
      "grad_norm": 6.180228233337402,
      "learning_rate": 4.973236200323777e-05,
      "loss": 2.2897,
      "step": 4100
    },
    {
      "epoch": 0.3289989033369889,
      "grad_norm": 4.780956268310547,
      "learning_rate": 4.972583424721918e-05,
      "loss": 2.2752,
      "step": 4200
    },
    {
      "epoch": 0.33683221055929813,
      "grad_norm": 5.220983505249023,
      "learning_rate": 4.9719306491200586e-05,
      "loss": 2.252,
      "step": 4300
    },
    {
      "epoch": 0.3446655177816074,
      "grad_norm": 5.104160308837891,
      "learning_rate": 4.9712778735182e-05,
      "loss": 2.2667,
      "step": 4400
    },
    {
      "epoch": 0.35249882500391666,
      "grad_norm": 5.4735517501831055,
      "learning_rate": 4.9706250979163404e-05,
      "loss": 2.3106,
      "step": 4500
    },
    {
      "epoch": 0.3603321322262259,
      "grad_norm": 5.224542140960693,
      "learning_rate": 4.969972322314481e-05,
      "loss": 2.2883,
      "step": 4600
    },
    {
      "epoch": 0.3681654394485352,
      "grad_norm": 4.710848331451416,
      "learning_rate": 4.969319546712622e-05,
      "loss": 2.2628,
      "step": 4700
    },
    {
      "epoch": 0.37599874667084443,
      "grad_norm": 6.841795921325684,
      "learning_rate": 4.9686667711107635e-05,
      "loss": 2.239,
      "step": 4800
    },
    {
      "epoch": 0.38383205389315367,
      "grad_norm": 5.2074055671691895,
      "learning_rate": 4.968013995508904e-05,
      "loss": 2.2285,
      "step": 4900
    },
    {
      "epoch": 0.39166536111546296,
      "grad_norm": 5.826254844665527,
      "learning_rate": 4.9673612199070453e-05,
      "loss": 2.2121,
      "step": 5000
    },
    {
      "epoch": 0.3994986683377722,
      "grad_norm": 6.542318820953369,
      "learning_rate": 4.966708444305186e-05,
      "loss": 2.2621,
      "step": 5100
    },
    {
      "epoch": 0.4073319755600815,
      "grad_norm": 4.999856948852539,
      "learning_rate": 4.9660556687033265e-05,
      "loss": 2.265,
      "step": 5200
    },
    {
      "epoch": 0.41516528278239073,
      "grad_norm": 5.920478820800781,
      "learning_rate": 4.965402893101468e-05,
      "loss": 2.3019,
      "step": 5300
    },
    {
      "epoch": 0.42299859000469997,
      "grad_norm": 4.125137805938721,
      "learning_rate": 4.964750117499608e-05,
      "loss": 2.192,
      "step": 5400
    },
    {
      "epoch": 0.43083189722700926,
      "grad_norm": 5.34205436706543,
      "learning_rate": 4.9640973418977496e-05,
      "loss": 2.2812,
      "step": 5500
    },
    {
      "epoch": 0.4386652044493185,
      "grad_norm": 5.266794681549072,
      "learning_rate": 4.963444566295891e-05,
      "loss": 2.2238,
      "step": 5600
    },
    {
      "epoch": 0.44649851167162774,
      "grad_norm": 5.967224597930908,
      "learning_rate": 4.9627917906940314e-05,
      "loss": 2.3115,
      "step": 5700
    },
    {
      "epoch": 0.45433181889393703,
      "grad_norm": 5.045041561126709,
      "learning_rate": 4.962139015092172e-05,
      "loss": 2.1978,
      "step": 5800
    },
    {
      "epoch": 0.46216512611624627,
      "grad_norm": 4.907043933868408,
      "learning_rate": 4.9614862394903126e-05,
      "loss": 2.196,
      "step": 5900
    },
    {
      "epoch": 0.46999843333855557,
      "grad_norm": 4.809208869934082,
      "learning_rate": 4.960833463888454e-05,
      "loss": 2.2574,
      "step": 6000
    },
    {
      "epoch": 0.4778317405608648,
      "grad_norm": 5.539190769195557,
      "learning_rate": 4.960180688286595e-05,
      "loss": 2.1489,
      "step": 6100
    },
    {
      "epoch": 0.48566504778317404,
      "grad_norm": 5.846864700317383,
      "learning_rate": 4.9595279126847357e-05,
      "loss": 2.244,
      "step": 6200
    },
    {
      "epoch": 0.49349835500548334,
      "grad_norm": 4.680606365203857,
      "learning_rate": 4.958875137082877e-05,
      "loss": 2.1937,
      "step": 6300
    },
    {
      "epoch": 0.5013316622277926,
      "grad_norm": 5.21718168258667,
      "learning_rate": 4.9582223614810175e-05,
      "loss": 2.1571,
      "step": 6400
    },
    {
      "epoch": 0.5091649694501018,
      "grad_norm": 6.3120551109313965,
      "learning_rate": 4.957569585879158e-05,
      "loss": 2.2198,
      "step": 6500
    },
    {
      "epoch": 0.516998276672411,
      "grad_norm": 5.230154514312744,
      "learning_rate": 4.956916810277299e-05,
      "loss": 2.1273,
      "step": 6600
    },
    {
      "epoch": 0.5248315838947204,
      "grad_norm": 5.517388820648193,
      "learning_rate": 4.95626403467544e-05,
      "loss": 2.2634,
      "step": 6700
    },
    {
      "epoch": 0.5326648911170296,
      "grad_norm": 7.19775390625,
      "learning_rate": 4.955611259073581e-05,
      "loss": 2.24,
      "step": 6800
    },
    {
      "epoch": 0.5404981983393389,
      "grad_norm": 5.995786190032959,
      "learning_rate": 4.9549584834717224e-05,
      "loss": 2.2224,
      "step": 6900
    },
    {
      "epoch": 0.5483315055616481,
      "grad_norm": 5.831862926483154,
      "learning_rate": 4.954305707869863e-05,
      "loss": 2.2066,
      "step": 7000
    },
    {
      "epoch": 0.5561648127839574,
      "grad_norm": 5.1995720863342285,
      "learning_rate": 4.9536529322680036e-05,
      "loss": 2.2987,
      "step": 7100
    },
    {
      "epoch": 0.5639981200062667,
      "grad_norm": 7.116579532623291,
      "learning_rate": 4.953000156666145e-05,
      "loss": 2.1729,
      "step": 7200
    },
    {
      "epoch": 0.5718314272285759,
      "grad_norm": 6.507894039154053,
      "learning_rate": 4.9523473810642854e-05,
      "loss": 2.2042,
      "step": 7300
    },
    {
      "epoch": 0.5796647344508852,
      "grad_norm": 6.134331226348877,
      "learning_rate": 4.9516946054624266e-05,
      "loss": 2.154,
      "step": 7400
    },
    {
      "epoch": 0.5874980416731944,
      "grad_norm": 6.062495231628418,
      "learning_rate": 4.951041829860568e-05,
      "loss": 2.1693,
      "step": 7500
    },
    {
      "epoch": 0.5953313488955037,
      "grad_norm": 6.351763725280762,
      "learning_rate": 4.9503890542587085e-05,
      "loss": 2.2225,
      "step": 7600
    },
    {
      "epoch": 0.6031646561178129,
      "grad_norm": 6.139704704284668,
      "learning_rate": 4.949736278656849e-05,
      "loss": 2.0942,
      "step": 7700
    },
    {
      "epoch": 0.6109979633401222,
      "grad_norm": 6.887972831726074,
      "learning_rate": 4.9490835030549896e-05,
      "loss": 2.185,
      "step": 7800
    },
    {
      "epoch": 0.6188312705624315,
      "grad_norm": 6.069847106933594,
      "learning_rate": 4.948430727453131e-05,
      "loss": 2.1868,
      "step": 7900
    },
    {
      "epoch": 0.6266645777847407,
      "grad_norm": 5.3646416664123535,
      "learning_rate": 4.947777951851272e-05,
      "loss": 2.1302,
      "step": 8000
    },
    {
      "epoch": 0.63449788500705,
      "grad_norm": 5.377294540405273,
      "learning_rate": 4.947125176249413e-05,
      "loss": 2.1772,
      "step": 8100
    },
    {
      "epoch": 0.6423311922293592,
      "grad_norm": 4.671091079711914,
      "learning_rate": 4.946472400647554e-05,
      "loss": 2.1292,
      "step": 8200
    },
    {
      "epoch": 0.6501644994516685,
      "grad_norm": 6.026116847991943,
      "learning_rate": 4.9458196250456945e-05,
      "loss": 2.1192,
      "step": 8300
    },
    {
      "epoch": 0.6579978066739778,
      "grad_norm": 6.355044841766357,
      "learning_rate": 4.945166849443835e-05,
      "loss": 2.165,
      "step": 8400
    },
    {
      "epoch": 0.665831113896287,
      "grad_norm": 5.947581768035889,
      "learning_rate": 4.9445140738419764e-05,
      "loss": 2.2652,
      "step": 8500
    },
    {
      "epoch": 0.6736644211185963,
      "grad_norm": 5.236397743225098,
      "learning_rate": 4.943861298240117e-05,
      "loss": 2.2877,
      "step": 8600
    },
    {
      "epoch": 0.6814977283409055,
      "grad_norm": 6.39498233795166,
      "learning_rate": 4.943208522638258e-05,
      "loss": 2.1415,
      "step": 8700
    },
    {
      "epoch": 0.6893310355632148,
      "grad_norm": 5.526889801025391,
      "learning_rate": 4.9425557470363995e-05,
      "loss": 2.1701,
      "step": 8800
    },
    {
      "epoch": 0.6971643427855241,
      "grad_norm": 5.563882827758789,
      "learning_rate": 4.94190297143454e-05,
      "loss": 2.1499,
      "step": 8900
    },
    {
      "epoch": 0.7049976500078333,
      "grad_norm": 4.730083465576172,
      "learning_rate": 4.9412501958326806e-05,
      "loss": 2.2495,
      "step": 9000
    },
    {
      "epoch": 0.7128309572301426,
      "grad_norm": 5.803686618804932,
      "learning_rate": 4.940597420230822e-05,
      "loss": 2.1496,
      "step": 9100
    },
    {
      "epoch": 0.7206642644524518,
      "grad_norm": 8.344736099243164,
      "learning_rate": 4.9399446446289624e-05,
      "loss": 2.0937,
      "step": 9200
    },
    {
      "epoch": 0.728497571674761,
      "grad_norm": 5.4411797523498535,
      "learning_rate": 4.939291869027104e-05,
      "loss": 2.1534,
      "step": 9300
    },
    {
      "epoch": 0.7363308788970704,
      "grad_norm": 5.37543249130249,
      "learning_rate": 4.938639093425244e-05,
      "loss": 2.1288,
      "step": 9400
    },
    {
      "epoch": 0.7441641861193796,
      "grad_norm": 6.616916179656982,
      "learning_rate": 4.9379863178233855e-05,
      "loss": 2.1779,
      "step": 9500
    },
    {
      "epoch": 0.7519974933416889,
      "grad_norm": 6.237786293029785,
      "learning_rate": 4.937333542221526e-05,
      "loss": 2.1594,
      "step": 9600
    },
    {
      "epoch": 0.7598308005639981,
      "grad_norm": 6.505497932434082,
      "learning_rate": 4.936680766619667e-05,
      "loss": 2.1934,
      "step": 9700
    },
    {
      "epoch": 0.7676641077863073,
      "grad_norm": 6.148593902587891,
      "learning_rate": 4.936027991017808e-05,
      "loss": 2.0975,
      "step": 9800
    },
    {
      "epoch": 0.7754974150086167,
      "grad_norm": 6.343632221221924,
      "learning_rate": 4.9353752154159485e-05,
      "loss": 2.1901,
      "step": 9900
    },
    {
      "epoch": 0.7833307222309259,
      "grad_norm": 5.76759147644043,
      "learning_rate": 4.93472243981409e-05,
      "loss": 2.291,
      "step": 10000
    },
    {
      "epoch": 0.7911640294532352,
      "grad_norm": 6.058598041534424,
      "learning_rate": 4.934069664212231e-05,
      "loss": 2.2547,
      "step": 10100
    },
    {
      "epoch": 0.7989973366755444,
      "grad_norm": 6.284708499908447,
      "learning_rate": 4.9334168886103716e-05,
      "loss": 2.1592,
      "step": 10200
    },
    {
      "epoch": 0.8068306438978536,
      "grad_norm": 6.2775750160217285,
      "learning_rate": 4.932764113008512e-05,
      "loss": 2.067,
      "step": 10300
    },
    {
      "epoch": 0.814663951120163,
      "grad_norm": 5.635641574859619,
      "learning_rate": 4.9321113374066534e-05,
      "loss": 2.1379,
      "step": 10400
    },
    {
      "epoch": 0.8224972583424722,
      "grad_norm": 5.398134708404541,
      "learning_rate": 4.931458561804794e-05,
      "loss": 2.1187,
      "step": 10500
    },
    {
      "epoch": 0.8303305655647815,
      "grad_norm": 5.384355068206787,
      "learning_rate": 4.930805786202935e-05,
      "loss": 2.1711,
      "step": 10600
    },
    {
      "epoch": 0.8381638727870907,
      "grad_norm": 6.249190807342529,
      "learning_rate": 4.9301530106010765e-05,
      "loss": 2.1661,
      "step": 10700
    },
    {
      "epoch": 0.8459971800093999,
      "grad_norm": 5.340925216674805,
      "learning_rate": 4.929500234999217e-05,
      "loss": 2.1187,
      "step": 10800
    },
    {
      "epoch": 0.8538304872317092,
      "grad_norm": 5.101469993591309,
      "learning_rate": 4.928847459397358e-05,
      "loss": 2.0743,
      "step": 10900
    },
    {
      "epoch": 0.8616637944540185,
      "grad_norm": 4.930428981781006,
      "learning_rate": 4.928194683795498e-05,
      "loss": 2.1182,
      "step": 11000
    },
    {
      "epoch": 0.8694971016763278,
      "grad_norm": 9.59282398223877,
      "learning_rate": 4.9275419081936395e-05,
      "loss": 2.2413,
      "step": 11100
    },
    {
      "epoch": 0.877330408898637,
      "grad_norm": 4.74672269821167,
      "learning_rate": 4.926889132591781e-05,
      "loss": 2.2167,
      "step": 11200
    },
    {
      "epoch": 0.8851637161209462,
      "grad_norm": 4.669819355010986,
      "learning_rate": 4.926236356989921e-05,
      "loss": 2.225,
      "step": 11300
    },
    {
      "epoch": 0.8929970233432555,
      "grad_norm": 5.583201885223389,
      "learning_rate": 4.9255835813880626e-05,
      "loss": 2.097,
      "step": 11400
    },
    {
      "epoch": 0.9008303305655648,
      "grad_norm": 7.006921768188477,
      "learning_rate": 4.924930805786203e-05,
      "loss": 2.2709,
      "step": 11500
    },
    {
      "epoch": 0.9086636377878741,
      "grad_norm": 4.6969194412231445,
      "learning_rate": 4.924278030184344e-05,
      "loss": 2.122,
      "step": 11600
    },
    {
      "epoch": 0.9164969450101833,
      "grad_norm": 7.37421178817749,
      "learning_rate": 4.923625254582485e-05,
      "loss": 2.0835,
      "step": 11700
    },
    {
      "epoch": 0.9243302522324925,
      "grad_norm": 6.504129409790039,
      "learning_rate": 4.9229724789806256e-05,
      "loss": 2.1198,
      "step": 11800
    },
    {
      "epoch": 0.9321635594548018,
      "grad_norm": 5.969969749450684,
      "learning_rate": 4.922319703378767e-05,
      "loss": 2.0943,
      "step": 11900
    },
    {
      "epoch": 0.9399968666771111,
      "grad_norm": 5.7581787109375,
      "learning_rate": 4.921666927776908e-05,
      "loss": 2.1548,
      "step": 12000
    },
    {
      "epoch": 0.9478301738994204,
      "grad_norm": 4.423715114593506,
      "learning_rate": 4.921014152175049e-05,
      "loss": 2.239,
      "step": 12100
    },
    {
      "epoch": 0.9556634811217296,
      "grad_norm": 4.635471343994141,
      "learning_rate": 4.920361376573189e-05,
      "loss": 2.1932,
      "step": 12200
    },
    {
      "epoch": 0.9634967883440388,
      "grad_norm": 7.249829292297363,
      "learning_rate": 4.9197086009713305e-05,
      "loss": 2.074,
      "step": 12300
    },
    {
      "epoch": 0.9713300955663481,
      "grad_norm": 5.26556396484375,
      "learning_rate": 4.919055825369471e-05,
      "loss": 2.1473,
      "step": 12400
    },
    {
      "epoch": 0.9791634027886573,
      "grad_norm": 5.4101738929748535,
      "learning_rate": 4.918403049767612e-05,
      "loss": 2.1763,
      "step": 12500
    },
    {
      "epoch": 0.9869967100109667,
      "grad_norm": 5.1334991455078125,
      "learning_rate": 4.9177502741657536e-05,
      "loss": 2.1057,
      "step": 12600
    },
    {
      "epoch": 0.9948300172332759,
      "grad_norm": 6.1203107833862305,
      "learning_rate": 4.917097498563894e-05,
      "loss": 2.1508,
      "step": 12700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.0372188091278076,
      "eval_runtime": 1.5542,
      "eval_samples_per_second": 432.372,
      "eval_steps_per_second": 432.372,
      "step": 12766
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9230916500091553,
      "eval_runtime": 29.3993,
      "eval_samples_per_second": 434.229,
      "eval_steps_per_second": 434.229,
      "step": 12766
    },
    {
      "epoch": 1.0026633244555851,
      "grad_norm": 5.0751166343688965,
      "learning_rate": 4.916444722962035e-05,
      "loss": 2.1002,
      "step": 12800
    },
    {
      "epoch": 1.0104966316778945,
      "grad_norm": 5.668735504150391,
      "learning_rate": 4.915791947360175e-05,
      "loss": 2.1333,
      "step": 12900
    },
    {
      "epoch": 1.0183299389002036,
      "grad_norm": 5.750729084014893,
      "learning_rate": 4.9151391717583166e-05,
      "loss": 2.0843,
      "step": 13000
    },
    {
      "epoch": 1.026163246122513,
      "grad_norm": 5.4186577796936035,
      "learning_rate": 4.914486396156457e-05,
      "loss": 2.0577,
      "step": 13100
    },
    {
      "epoch": 1.033996553344822,
      "grad_norm": 5.838184833526611,
      "learning_rate": 4.9138336205545984e-05,
      "loss": 2.0276,
      "step": 13200
    },
    {
      "epoch": 1.0418298605671314,
      "grad_norm": 5.636327266693115,
      "learning_rate": 4.9131808449527396e-05,
      "loss": 2.1095,
      "step": 13300
    },
    {
      "epoch": 1.0496631677894408,
      "grad_norm": 6.606537342071533,
      "learning_rate": 4.91252806935088e-05,
      "loss": 2.1122,
      "step": 13400
    },
    {
      "epoch": 1.05749647501175,
      "grad_norm": 6.112626552581787,
      "learning_rate": 4.911875293749021e-05,
      "loss": 2.057,
      "step": 13500
    },
    {
      "epoch": 1.0653297822340593,
      "grad_norm": 5.237815856933594,
      "learning_rate": 4.911222518147162e-05,
      "loss": 2.1032,
      "step": 13600
    },
    {
      "epoch": 1.0731630894563684,
      "grad_norm": 5.657760143280029,
      "learning_rate": 4.9105697425453026e-05,
      "loss": 2.1093,
      "step": 13700
    },
    {
      "epoch": 1.0809963966786778,
      "grad_norm": 6.598867893218994,
      "learning_rate": 4.909916966943444e-05,
      "loss": 2.0938,
      "step": 13800
    },
    {
      "epoch": 1.088829703900987,
      "grad_norm": 7.081791400909424,
      "learning_rate": 4.909264191341585e-05,
      "loss": 2.1089,
      "step": 13900
    },
    {
      "epoch": 1.0966630111232962,
      "grad_norm": 4.200961112976074,
      "learning_rate": 4.908611415739726e-05,
      "loss": 2.0771,
      "step": 14000
    },
    {
      "epoch": 1.1044963183456056,
      "grad_norm": 6.001265048980713,
      "learning_rate": 4.907958640137866e-05,
      "loss": 2.0891,
      "step": 14100
    },
    {
      "epoch": 1.1123296255679147,
      "grad_norm": 5.770054817199707,
      "learning_rate": 4.9073058645360076e-05,
      "loss": 2.0551,
      "step": 14200
    },
    {
      "epoch": 1.120162932790224,
      "grad_norm": 5.298604488372803,
      "learning_rate": 4.906653088934148e-05,
      "loss": 2.0979,
      "step": 14300
    },
    {
      "epoch": 1.1279962400125334,
      "grad_norm": 4.9695587158203125,
      "learning_rate": 4.9060003133322894e-05,
      "loss": 2.0699,
      "step": 14400
    },
    {
      "epoch": 1.1358295472348425,
      "grad_norm": 6.071866512298584,
      "learning_rate": 4.90534753773043e-05,
      "loss": 1.993,
      "step": 14500
    },
    {
      "epoch": 1.1436628544571519,
      "grad_norm": 6.302634239196777,
      "learning_rate": 4.904694762128571e-05,
      "loss": 2.1163,
      "step": 14600
    },
    {
      "epoch": 1.151496161679461,
      "grad_norm": 8.206295013427734,
      "learning_rate": 4.904041986526712e-05,
      "loss": 2.1248,
      "step": 14700
    },
    {
      "epoch": 1.1593294689017704,
      "grad_norm": 6.130761623382568,
      "learning_rate": 4.9033892109248524e-05,
      "loss": 2.1437,
      "step": 14800
    },
    {
      "epoch": 1.1671627761240795,
      "grad_norm": 4.534794330596924,
      "learning_rate": 4.9027364353229936e-05,
      "loss": 2.0734,
      "step": 14900
    },
    {
      "epoch": 1.1749960833463888,
      "grad_norm": 8.074286460876465,
      "learning_rate": 4.902083659721134e-05,
      "loss": 2.0449,
      "step": 15000
    },
    {
      "epoch": 1.1828293905686982,
      "grad_norm": 7.182647228240967,
      "learning_rate": 4.9014308841192755e-05,
      "loss": 2.0722,
      "step": 15100
    },
    {
      "epoch": 1.1906626977910073,
      "grad_norm": 5.598883152008057,
      "learning_rate": 4.900778108517417e-05,
      "loss": 2.1134,
      "step": 15200
    },
    {
      "epoch": 1.1984960050133167,
      "grad_norm": 5.934550762176514,
      "learning_rate": 4.900125332915557e-05,
      "loss": 2.0977,
      "step": 15300
    },
    {
      "epoch": 1.206329312235626,
      "grad_norm": 6.848305702209473,
      "learning_rate": 4.899472557313698e-05,
      "loss": 2.1197,
      "step": 15400
    },
    {
      "epoch": 1.2141626194579351,
      "grad_norm": 4.682607173919678,
      "learning_rate": 4.898819781711839e-05,
      "loss": 2.0575,
      "step": 15500
    },
    {
      "epoch": 1.2219959266802445,
      "grad_norm": 6.62583589553833,
      "learning_rate": 4.89816700610998e-05,
      "loss": 2.1651,
      "step": 15600
    },
    {
      "epoch": 1.2298292339025536,
      "grad_norm": 4.632841110229492,
      "learning_rate": 4.897514230508121e-05,
      "loss": 2.063,
      "step": 15700
    },
    {
      "epoch": 1.237662541124863,
      "grad_norm": 7.264548301696777,
      "learning_rate": 4.896861454906262e-05,
      "loss": 2.1698,
      "step": 15800
    },
    {
      "epoch": 1.245495848347172,
      "grad_norm": 4.9100422859191895,
      "learning_rate": 4.896208679304403e-05,
      "loss": 2.138,
      "step": 15900
    },
    {
      "epoch": 1.2533291555694814,
      "grad_norm": 4.81561279296875,
      "learning_rate": 4.8955559037025434e-05,
      "loss": 2.161,
      "step": 16000
    },
    {
      "epoch": 1.2611624627917908,
      "grad_norm": 5.511953353881836,
      "learning_rate": 4.894903128100684e-05,
      "loss": 2.1274,
      "step": 16100
    },
    {
      "epoch": 1.2689957700141,
      "grad_norm": 6.033578872680664,
      "learning_rate": 4.894250352498825e-05,
      "loss": 2.1412,
      "step": 16200
    },
    {
      "epoch": 1.2768290772364093,
      "grad_norm": 5.319091796875,
      "learning_rate": 4.893597576896966e-05,
      "loss": 2.172,
      "step": 16300
    },
    {
      "epoch": 1.2846623844587186,
      "grad_norm": 4.527461528778076,
      "learning_rate": 4.892944801295107e-05,
      "loss": 2.1188,
      "step": 16400
    },
    {
      "epoch": 1.2924956916810277,
      "grad_norm": 5.479573726654053,
      "learning_rate": 4.892292025693248e-05,
      "loss": 2.1157,
      "step": 16500
    },
    {
      "epoch": 1.3003289989033369,
      "grad_norm": 4.94338846206665,
      "learning_rate": 4.891639250091389e-05,
      "loss": 2.1628,
      "step": 16600
    },
    {
      "epoch": 1.3081623061256462,
      "grad_norm": 5.25357723236084,
      "learning_rate": 4.8909864744895294e-05,
      "loss": 2.0437,
      "step": 16700
    },
    {
      "epoch": 1.3159956133479556,
      "grad_norm": 5.027772426605225,
      "learning_rate": 4.890333698887671e-05,
      "loss": 2.0603,
      "step": 16800
    },
    {
      "epoch": 1.3238289205702647,
      "grad_norm": 5.633784294128418,
      "learning_rate": 4.889680923285811e-05,
      "loss": 2.0999,
      "step": 16900
    },
    {
      "epoch": 1.331662227792574,
      "grad_norm": 6.154983997344971,
      "learning_rate": 4.8890281476839525e-05,
      "loss": 2.0704,
      "step": 17000
    },
    {
      "epoch": 1.3394955350148834,
      "grad_norm": 9.395544052124023,
      "learning_rate": 4.888375372082094e-05,
      "loss": 2.1534,
      "step": 17100
    },
    {
      "epoch": 1.3473288422371925,
      "grad_norm": 5.835675239562988,
      "learning_rate": 4.8877225964802343e-05,
      "loss": 2.0935,
      "step": 17200
    },
    {
      "epoch": 1.3551621494595019,
      "grad_norm": 5.541331768035889,
      "learning_rate": 4.887069820878375e-05,
      "loss": 2.0051,
      "step": 17300
    },
    {
      "epoch": 1.362995456681811,
      "grad_norm": 4.498076438903809,
      "learning_rate": 4.886417045276516e-05,
      "loss": 2.041,
      "step": 17400
    },
    {
      "epoch": 1.3708287639041203,
      "grad_norm": 4.479153156280518,
      "learning_rate": 4.885764269674657e-05,
      "loss": 2.0581,
      "step": 17500
    },
    {
      "epoch": 1.3786620711264295,
      "grad_norm": 4.575009822845459,
      "learning_rate": 4.885111494072798e-05,
      "loss": 2.1636,
      "step": 17600
    },
    {
      "epoch": 1.3864953783487388,
      "grad_norm": 5.2126240730285645,
      "learning_rate": 4.8844587184709386e-05,
      "loss": 2.0737,
      "step": 17700
    },
    {
      "epoch": 1.3943286855710482,
      "grad_norm": 5.993738174438477,
      "learning_rate": 4.88380594286908e-05,
      "loss": 2.2095,
      "step": 17800
    },
    {
      "epoch": 1.4021619927933573,
      "grad_norm": 5.589296817779541,
      "learning_rate": 4.8831531672672204e-05,
      "loss": 1.9755,
      "step": 17900
    },
    {
      "epoch": 1.4099953000156666,
      "grad_norm": 5.6782546043396,
      "learning_rate": 4.882500391665361e-05,
      "loss": 2.0214,
      "step": 18000
    },
    {
      "epoch": 1.417828607237976,
      "grad_norm": 6.691398620605469,
      "learning_rate": 4.881847616063502e-05,
      "loss": 2.0355,
      "step": 18100
    },
    {
      "epoch": 1.4256619144602851,
      "grad_norm": 5.254960060119629,
      "learning_rate": 4.881194840461643e-05,
      "loss": 2.0958,
      "step": 18200
    },
    {
      "epoch": 1.4334952216825945,
      "grad_norm": 8.178653717041016,
      "learning_rate": 4.880542064859784e-05,
      "loss": 2.0073,
      "step": 18300
    },
    {
      "epoch": 1.4413285289049036,
      "grad_norm": 4.594842910766602,
      "learning_rate": 4.879889289257925e-05,
      "loss": 2.1704,
      "step": 18400
    },
    {
      "epoch": 1.449161836127213,
      "grad_norm": 4.646774768829346,
      "learning_rate": 4.879236513656066e-05,
      "loss": 2.0735,
      "step": 18500
    },
    {
      "epoch": 1.456995143349522,
      "grad_norm": 6.627148628234863,
      "learning_rate": 4.8785837380542065e-05,
      "loss": 2.1065,
      "step": 18600
    },
    {
      "epoch": 1.4648284505718314,
      "grad_norm": 5.558542728424072,
      "learning_rate": 4.877930962452348e-05,
      "loss": 2.0622,
      "step": 18700
    },
    {
      "epoch": 1.4726617577941408,
      "grad_norm": 5.315141201019287,
      "learning_rate": 4.877278186850488e-05,
      "loss": 2.0288,
      "step": 18800
    },
    {
      "epoch": 1.48049506501645,
      "grad_norm": 5.176080703735352,
      "learning_rate": 4.8766254112486296e-05,
      "loss": 2.1854,
      "step": 18900
    },
    {
      "epoch": 1.4883283722387592,
      "grad_norm": 5.39152193069458,
      "learning_rate": 4.875972635646771e-05,
      "loss": 2.0042,
      "step": 19000
    },
    {
      "epoch": 1.4961616794610686,
      "grad_norm": 7.476358890533447,
      "learning_rate": 4.8753198600449114e-05,
      "loss": 2.069,
      "step": 19100
    },
    {
      "epoch": 1.5039949866833777,
      "grad_norm": 6.1028900146484375,
      "learning_rate": 4.874667084443052e-05,
      "loss": 2.1605,
      "step": 19200
    },
    {
      "epoch": 1.5118282939056868,
      "grad_norm": 6.791190147399902,
      "learning_rate": 4.874014308841193e-05,
      "loss": 2.138,
      "step": 19300
    },
    {
      "epoch": 1.5196616011279962,
      "grad_norm": 4.782421112060547,
      "learning_rate": 4.873361533239334e-05,
      "loss": 2.0488,
      "step": 19400
    },
    {
      "epoch": 1.5274949083503055,
      "grad_norm": 5.900424957275391,
      "learning_rate": 4.8727087576374744e-05,
      "loss": 2.071,
      "step": 19500
    },
    {
      "epoch": 1.5353282155726147,
      "grad_norm": 4.4757914543151855,
      "learning_rate": 4.8720559820356156e-05,
      "loss": 2.049,
      "step": 19600
    },
    {
      "epoch": 1.543161522794924,
      "grad_norm": 6.631092548370361,
      "learning_rate": 4.871403206433757e-05,
      "loss": 2.0358,
      "step": 19700
    },
    {
      "epoch": 1.5509948300172334,
      "grad_norm": 11.515556335449219,
      "learning_rate": 4.8707504308318975e-05,
      "loss": 2.0668,
      "step": 19800
    },
    {
      "epoch": 1.5588281372395425,
      "grad_norm": 6.6526947021484375,
      "learning_rate": 4.870097655230038e-05,
      "loss": 2.0365,
      "step": 19900
    },
    {
      "epoch": 1.5666614444618518,
      "grad_norm": 4.757647514343262,
      "learning_rate": 4.869444879628179e-05,
      "loss": 2.0549,
      "step": 20000
    },
    {
      "epoch": 1.5744947516841612,
      "grad_norm": 4.662642478942871,
      "learning_rate": 4.86879210402632e-05,
      "loss": 2.1469,
      "step": 20100
    },
    {
      "epoch": 1.5823280589064703,
      "grad_norm": 5.379243850708008,
      "learning_rate": 4.868139328424461e-05,
      "loss": 2.0092,
      "step": 20200
    },
    {
      "epoch": 1.5901613661287795,
      "grad_norm": 7.488030433654785,
      "learning_rate": 4.8674865528226024e-05,
      "loss": 2.1034,
      "step": 20300
    },
    {
      "epoch": 1.5979946733510888,
      "grad_norm": 3.4778220653533936,
      "learning_rate": 4.866833777220743e-05,
      "loss": 1.9796,
      "step": 20400
    },
    {
      "epoch": 1.6058279805733982,
      "grad_norm": 5.560220718383789,
      "learning_rate": 4.8661810016188835e-05,
      "loss": 2.0135,
      "step": 20500
    },
    {
      "epoch": 1.6136612877957073,
      "grad_norm": 6.431575775146484,
      "learning_rate": 4.865528226017025e-05,
      "loss": 2.0515,
      "step": 20600
    },
    {
      "epoch": 1.6214945950180166,
      "grad_norm": 5.014174938201904,
      "learning_rate": 4.8648754504151654e-05,
      "loss": 1.9808,
      "step": 20700
    },
    {
      "epoch": 1.629327902240326,
      "grad_norm": 5.600538730621338,
      "learning_rate": 4.8642226748133066e-05,
      "loss": 2.0347,
      "step": 20800
    },
    {
      "epoch": 1.637161209462635,
      "grad_norm": 6.0922393798828125,
      "learning_rate": 4.863569899211448e-05,
      "loss": 2.0599,
      "step": 20900
    },
    {
      "epoch": 1.6449945166849442,
      "grad_norm": 7.769165992736816,
      "learning_rate": 4.8629171236095885e-05,
      "loss": 2.0741,
      "step": 21000
    },
    {
      "epoch": 1.6528278239072538,
      "grad_norm": 8.548449516296387,
      "learning_rate": 4.862264348007729e-05,
      "loss": 2.0281,
      "step": 21100
    },
    {
      "epoch": 1.660661131129563,
      "grad_norm": 4.955871105194092,
      "learning_rate": 4.8616115724058696e-05,
      "loss": 2.069,
      "step": 21200
    },
    {
      "epoch": 1.668494438351872,
      "grad_norm": 4.4734110832214355,
      "learning_rate": 4.860958796804011e-05,
      "loss": 2.092,
      "step": 21300
    },
    {
      "epoch": 1.6763277455741814,
      "grad_norm": 4.814508438110352,
      "learning_rate": 4.8603060212021514e-05,
      "loss": 2.1476,
      "step": 21400
    },
    {
      "epoch": 1.6841610527964908,
      "grad_norm": 4.603963375091553,
      "learning_rate": 4.859653245600293e-05,
      "loss": 2.0197,
      "step": 21500
    },
    {
      "epoch": 1.6919943600187999,
      "grad_norm": 5.248845100402832,
      "learning_rate": 4.859000469998434e-05,
      "loss": 2.1338,
      "step": 21600
    },
    {
      "epoch": 1.6998276672411092,
      "grad_norm": 4.445164680480957,
      "learning_rate": 4.8583476943965745e-05,
      "loss": 2.0639,
      "step": 21700
    },
    {
      "epoch": 1.7076609744634186,
      "grad_norm": 6.130378246307373,
      "learning_rate": 4.857694918794715e-05,
      "loss": 2.0716,
      "step": 21800
    },
    {
      "epoch": 1.7154942816857277,
      "grad_norm": 5.682661533355713,
      "learning_rate": 4.8570421431928564e-05,
      "loss": 2.0083,
      "step": 21900
    },
    {
      "epoch": 1.7233275889080368,
      "grad_norm": 5.7828826904296875,
      "learning_rate": 4.856389367590997e-05,
      "loss": 2.0802,
      "step": 22000
    },
    {
      "epoch": 1.7311608961303462,
      "grad_norm": 4.37761926651001,
      "learning_rate": 4.855736591989138e-05,
      "loss": 2.0758,
      "step": 22100
    },
    {
      "epoch": 1.7389942033526555,
      "grad_norm": 6.18450403213501,
      "learning_rate": 4.8550838163872794e-05,
      "loss": 2.0799,
      "step": 22200
    },
    {
      "epoch": 1.7468275105749647,
      "grad_norm": 8.31511116027832,
      "learning_rate": 4.85443104078542e-05,
      "loss": 2.1036,
      "step": 22300
    },
    {
      "epoch": 1.754660817797274,
      "grad_norm": 5.135018348693848,
      "learning_rate": 4.8537782651835606e-05,
      "loss": 1.996,
      "step": 22400
    },
    {
      "epoch": 1.7624941250195834,
      "grad_norm": 6.485657215118408,
      "learning_rate": 4.853125489581702e-05,
      "loss": 2.0739,
      "step": 22500
    },
    {
      "epoch": 1.7703274322418925,
      "grad_norm": 5.331279277801514,
      "learning_rate": 4.8524727139798424e-05,
      "loss": 2.1012,
      "step": 22600
    },
    {
      "epoch": 1.7781607394642018,
      "grad_norm": 4.162642955780029,
      "learning_rate": 4.851819938377983e-05,
      "loss": 2.0452,
      "step": 22700
    },
    {
      "epoch": 1.7859940466865112,
      "grad_norm": 4.054581642150879,
      "learning_rate": 4.851167162776124e-05,
      "loss": 1.9554,
      "step": 22800
    },
    {
      "epoch": 1.7938273539088203,
      "grad_norm": 5.379858493804932,
      "learning_rate": 4.8505143871742655e-05,
      "loss": 2.0669,
      "step": 22900
    },
    {
      "epoch": 1.8016606611311294,
      "grad_norm": 5.060847759246826,
      "learning_rate": 4.849861611572406e-05,
      "loss": 2.1849,
      "step": 23000
    },
    {
      "epoch": 1.8094939683534388,
      "grad_norm": 7.357667446136475,
      "learning_rate": 4.849208835970547e-05,
      "loss": 2.0163,
      "step": 23100
    },
    {
      "epoch": 1.8173272755757481,
      "grad_norm": 6.001011848449707,
      "learning_rate": 4.848556060368688e-05,
      "loss": 1.991,
      "step": 23200
    },
    {
      "epoch": 1.8251605827980573,
      "grad_norm": 5.358302116394043,
      "learning_rate": 4.8479032847668285e-05,
      "loss": 2.0667,
      "step": 23300
    },
    {
      "epoch": 1.8329938900203666,
      "grad_norm": 8.502008438110352,
      "learning_rate": 4.84725050916497e-05,
      "loss": 2.0055,
      "step": 23400
    },
    {
      "epoch": 1.840827197242676,
      "grad_norm": 5.255362510681152,
      "learning_rate": 4.846597733563111e-05,
      "loss": 2.0922,
      "step": 23500
    },
    {
      "epoch": 1.848660504464985,
      "grad_norm": 4.739783763885498,
      "learning_rate": 4.8459449579612516e-05,
      "loss": 2.0699,
      "step": 23600
    },
    {
      "epoch": 1.8564938116872942,
      "grad_norm": 5.759664058685303,
      "learning_rate": 4.845292182359392e-05,
      "loss": 2.0514,
      "step": 23700
    },
    {
      "epoch": 1.8643271189096038,
      "grad_norm": 6.953304290771484,
      "learning_rate": 4.8446394067575334e-05,
      "loss": 2.0606,
      "step": 23800
    },
    {
      "epoch": 1.872160426131913,
      "grad_norm": 5.364686489105225,
      "learning_rate": 4.843986631155674e-05,
      "loss": 2.0643,
      "step": 23900
    },
    {
      "epoch": 1.879993733354222,
      "grad_norm": 8.645956039428711,
      "learning_rate": 4.843333855553815e-05,
      "loss": 2.0604,
      "step": 24000
    },
    {
      "epoch": 1.8878270405765314,
      "grad_norm": 5.612048625946045,
      "learning_rate": 4.8426810799519565e-05,
      "loss": 1.9698,
      "step": 24100
    },
    {
      "epoch": 1.8956603477988407,
      "grad_norm": 5.10699987411499,
      "learning_rate": 4.842028304350097e-05,
      "loss": 2.0228,
      "step": 24200
    },
    {
      "epoch": 1.9034936550211499,
      "grad_norm": 5.128702640533447,
      "learning_rate": 4.8413755287482377e-05,
      "loss": 2.0461,
      "step": 24300
    },
    {
      "epoch": 1.9113269622434592,
      "grad_norm": 5.2049126625061035,
      "learning_rate": 4.840722753146379e-05,
      "loss": 1.9541,
      "step": 24400
    },
    {
      "epoch": 1.9191602694657686,
      "grad_norm": 4.81820011138916,
      "learning_rate": 4.8400699775445195e-05,
      "loss": 2.0284,
      "step": 24500
    },
    {
      "epoch": 1.9269935766880777,
      "grad_norm": 4.779738426208496,
      "learning_rate": 4.83941720194266e-05,
      "loss": 2.0499,
      "step": 24600
    },
    {
      "epoch": 1.9348268839103868,
      "grad_norm": 5.044183731079102,
      "learning_rate": 4.838764426340801e-05,
      "loss": 2.0332,
      "step": 24700
    },
    {
      "epoch": 1.9426601911326964,
      "grad_norm": 7.374741077423096,
      "learning_rate": 4.8381116507389426e-05,
      "loss": 2.1881,
      "step": 24800
    },
    {
      "epoch": 1.9504934983550055,
      "grad_norm": 6.062929630279541,
      "learning_rate": 4.837458875137083e-05,
      "loss": 1.986,
      "step": 24900
    },
    {
      "epoch": 1.9583268055773146,
      "grad_norm": 5.477476596832275,
      "learning_rate": 4.836806099535224e-05,
      "loss": 2.0565,
      "step": 25000
    },
    {
      "epoch": 1.966160112799624,
      "grad_norm": 4.674515724182129,
      "learning_rate": 4.836153323933365e-05,
      "loss": 2.0824,
      "step": 25100
    },
    {
      "epoch": 1.9739934200219333,
      "grad_norm": 7.829503536224365,
      "learning_rate": 4.8355005483315056e-05,
      "loss": 1.9818,
      "step": 25200
    },
    {
      "epoch": 1.9818267272442425,
      "grad_norm": 6.371964454650879,
      "learning_rate": 4.834847772729647e-05,
      "loss": 2.058,
      "step": 25300
    },
    {
      "epoch": 1.9896600344665518,
      "grad_norm": 4.77095890045166,
      "learning_rate": 4.834194997127788e-05,
      "loss": 1.9499,
      "step": 25400
    },
    {
      "epoch": 1.9974933416888612,
      "grad_norm": 5.919712543487549,
      "learning_rate": 4.8335422215259286e-05,
      "loss": 2.0417,
      "step": 25500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9682326316833496,
      "eval_runtime": 1.5588,
      "eval_samples_per_second": 431.095,
      "eval_steps_per_second": 431.095,
      "step": 25532
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8292429447174072,
      "eval_runtime": 29.6437,
      "eval_samples_per_second": 430.648,
      "eval_steps_per_second": 430.648,
      "step": 25532
    },
    {
      "epoch": 2.0053266489111703,
      "grad_norm": 4.956715106964111,
      "learning_rate": 4.832889445924069e-05,
      "loss": 1.9676,
      "step": 25600
    },
    {
      "epoch": 2.0131599561334794,
      "grad_norm": 6.0833868980407715,
      "learning_rate": 4.8322366703222105e-05,
      "loss": 2.0324,
      "step": 25700
    },
    {
      "epoch": 2.020993263355789,
      "grad_norm": 5.0463972091674805,
      "learning_rate": 4.831583894720351e-05,
      "loss": 2.0027,
      "step": 25800
    },
    {
      "epoch": 2.028826570578098,
      "grad_norm": 6.313888072967529,
      "learning_rate": 4.8309311191184916e-05,
      "loss": 1.9809,
      "step": 25900
    },
    {
      "epoch": 2.0366598778004072,
      "grad_norm": 4.784383773803711,
      "learning_rate": 4.830278343516633e-05,
      "loss": 1.9509,
      "step": 26000
    },
    {
      "epoch": 2.0444931850227164,
      "grad_norm": 9.98560619354248,
      "learning_rate": 4.829625567914774e-05,
      "loss": 2.0782,
      "step": 26100
    },
    {
      "epoch": 2.052326492245026,
      "grad_norm": 5.609538555145264,
      "learning_rate": 4.828972792312915e-05,
      "loss": 1.9829,
      "step": 26200
    },
    {
      "epoch": 2.060159799467335,
      "grad_norm": 4.29588508605957,
      "learning_rate": 4.828320016711055e-05,
      "loss": 1.9364,
      "step": 26300
    },
    {
      "epoch": 2.067993106689644,
      "grad_norm": 7.396958827972412,
      "learning_rate": 4.8276672411091966e-05,
      "loss": 1.9371,
      "step": 26400
    },
    {
      "epoch": 2.0758264139119538,
      "grad_norm": 4.850846290588379,
      "learning_rate": 4.827014465507337e-05,
      "loss": 1.9644,
      "step": 26500
    },
    {
      "epoch": 2.083659721134263,
      "grad_norm": 5.9106340408325195,
      "learning_rate": 4.8263616899054784e-05,
      "loss": 2.0763,
      "step": 26600
    },
    {
      "epoch": 2.091493028356572,
      "grad_norm": 5.569774150848389,
      "learning_rate": 4.8257089143036196e-05,
      "loss": 1.9892,
      "step": 26700
    },
    {
      "epoch": 2.0993263355788816,
      "grad_norm": 5.24359130859375,
      "learning_rate": 4.82505613870176e-05,
      "loss": 2.0564,
      "step": 26800
    },
    {
      "epoch": 2.1071596428011907,
      "grad_norm": 6.056571006774902,
      "learning_rate": 4.824403363099901e-05,
      "loss": 1.9443,
      "step": 26900
    },
    {
      "epoch": 2.1149929500235,
      "grad_norm": 5.908640384674072,
      "learning_rate": 4.823750587498042e-05,
      "loss": 1.9522,
      "step": 27000
    },
    {
      "epoch": 2.122826257245809,
      "grad_norm": 4.811688423156738,
      "learning_rate": 4.8230978118961826e-05,
      "loss": 1.9587,
      "step": 27100
    },
    {
      "epoch": 2.1306595644681185,
      "grad_norm": 5.831846714019775,
      "learning_rate": 4.822445036294324e-05,
      "loss": 2.0341,
      "step": 27200
    },
    {
      "epoch": 2.1384928716904277,
      "grad_norm": 4.825645923614502,
      "learning_rate": 4.821792260692465e-05,
      "loss": 2.0187,
      "step": 27300
    },
    {
      "epoch": 2.146326178912737,
      "grad_norm": 5.339398384094238,
      "learning_rate": 4.821139485090606e-05,
      "loss": 2.0403,
      "step": 27400
    },
    {
      "epoch": 2.1541594861350464,
      "grad_norm": 5.163570404052734,
      "learning_rate": 4.820486709488746e-05,
      "loss": 1.9884,
      "step": 27500
    },
    {
      "epoch": 2.1619927933573555,
      "grad_norm": 7.403613090515137,
      "learning_rate": 4.8198339338868875e-05,
      "loss": 1.9626,
      "step": 27600
    },
    {
      "epoch": 2.1698261005796646,
      "grad_norm": 5.470179080963135,
      "learning_rate": 4.819181158285028e-05,
      "loss": 1.9624,
      "step": 27700
    },
    {
      "epoch": 2.177659407801974,
      "grad_norm": 6.433256149291992,
      "learning_rate": 4.818528382683169e-05,
      "loss": 2.0203,
      "step": 27800
    },
    {
      "epoch": 2.1854927150242833,
      "grad_norm": 5.997132301330566,
      "learning_rate": 4.81787560708131e-05,
      "loss": 2.0261,
      "step": 27900
    },
    {
      "epoch": 2.1933260222465925,
      "grad_norm": 8.363478660583496,
      "learning_rate": 4.817222831479451e-05,
      "loss": 2.0132,
      "step": 28000
    },
    {
      "epoch": 2.2011593294689016,
      "grad_norm": 4.480154037475586,
      "learning_rate": 4.816570055877592e-05,
      "loss": 2.1867,
      "step": 28100
    },
    {
      "epoch": 2.208992636691211,
      "grad_norm": 6.3734283447265625,
      "learning_rate": 4.8159172802757324e-05,
      "loss": 1.9691,
      "step": 28200
    },
    {
      "epoch": 2.2168259439135203,
      "grad_norm": 5.815783977508545,
      "learning_rate": 4.8152645046738736e-05,
      "loss": 2.0501,
      "step": 28300
    },
    {
      "epoch": 2.2246592511358294,
      "grad_norm": 6.3873820304870605,
      "learning_rate": 4.814611729072014e-05,
      "loss": 1.9647,
      "step": 28400
    },
    {
      "epoch": 2.232492558358139,
      "grad_norm": 4.823578834533691,
      "learning_rate": 4.8139589534701554e-05,
      "loss": 1.9751,
      "step": 28500
    },
    {
      "epoch": 2.240325865580448,
      "grad_norm": 5.917789936065674,
      "learning_rate": 4.813306177868297e-05,
      "loss": 2.0691,
      "step": 28600
    },
    {
      "epoch": 2.2481591728027572,
      "grad_norm": 5.736191272735596,
      "learning_rate": 4.812653402266437e-05,
      "loss": 1.9929,
      "step": 28700
    },
    {
      "epoch": 2.255992480025067,
      "grad_norm": 7.168615341186523,
      "learning_rate": 4.812000626664578e-05,
      "loss": 2.022,
      "step": 28800
    },
    {
      "epoch": 2.263825787247376,
      "grad_norm": 6.1214375495910645,
      "learning_rate": 4.811347851062719e-05,
      "loss": 2.0128,
      "step": 28900
    },
    {
      "epoch": 2.271659094469685,
      "grad_norm": 5.320870876312256,
      "learning_rate": 4.81069507546086e-05,
      "loss": 1.9958,
      "step": 29000
    },
    {
      "epoch": 2.279492401691994,
      "grad_norm": 6.409520149230957,
      "learning_rate": 4.810042299859e-05,
      "loss": 2.0455,
      "step": 29100
    },
    {
      "epoch": 2.2873257089143038,
      "grad_norm": 4.990448951721191,
      "learning_rate": 4.8093895242571415e-05,
      "loss": 2.0285,
      "step": 29200
    },
    {
      "epoch": 2.295159016136613,
      "grad_norm": 5.951266288757324,
      "learning_rate": 4.808736748655283e-05,
      "loss": 1.9554,
      "step": 29300
    },
    {
      "epoch": 2.302992323358922,
      "grad_norm": 4.2607340812683105,
      "learning_rate": 4.8080839730534233e-05,
      "loss": 1.9707,
      "step": 29400
    },
    {
      "epoch": 2.3108256305812316,
      "grad_norm": 6.685838222503662,
      "learning_rate": 4.807431197451564e-05,
      "loss": 1.915,
      "step": 29500
    },
    {
      "epoch": 2.3186589378035407,
      "grad_norm": 5.788713455200195,
      "learning_rate": 4.806778421849705e-05,
      "loss": 2.0092,
      "step": 29600
    },
    {
      "epoch": 2.32649224502585,
      "grad_norm": 6.85596227645874,
      "learning_rate": 4.806125646247846e-05,
      "loss": 2.0123,
      "step": 29700
    },
    {
      "epoch": 2.334325552248159,
      "grad_norm": 5.773847579956055,
      "learning_rate": 4.805472870645987e-05,
      "loss": 2.0376,
      "step": 29800
    },
    {
      "epoch": 2.3421588594704685,
      "grad_norm": 6.644999980926514,
      "learning_rate": 4.804820095044128e-05,
      "loss": 2.0137,
      "step": 29900
    },
    {
      "epoch": 2.3499921666927777,
      "grad_norm": 6.724207401275635,
      "learning_rate": 4.804167319442269e-05,
      "loss": 1.9977,
      "step": 30000
    },
    {
      "epoch": 2.357825473915087,
      "grad_norm": 6.436797618865967,
      "learning_rate": 4.8035145438404094e-05,
      "loss": 2.0807,
      "step": 30100
    },
    {
      "epoch": 2.3656587811373964,
      "grad_norm": 6.98540735244751,
      "learning_rate": 4.802861768238551e-05,
      "loss": 2.0581,
      "step": 30200
    },
    {
      "epoch": 2.3734920883597055,
      "grad_norm": 5.060544013977051,
      "learning_rate": 4.802208992636691e-05,
      "loss": 2.0563,
      "step": 30300
    },
    {
      "epoch": 2.3813253955820146,
      "grad_norm": 4.797931671142578,
      "learning_rate": 4.8015562170348325e-05,
      "loss": 1.9933,
      "step": 30400
    },
    {
      "epoch": 2.389158702804324,
      "grad_norm": 4.037655830383301,
      "learning_rate": 4.800903441432974e-05,
      "loss": 2.0633,
      "step": 30500
    },
    {
      "epoch": 2.3969920100266333,
      "grad_norm": 6.205372333526611,
      "learning_rate": 4.800250665831114e-05,
      "loss": 2.0843,
      "step": 30600
    },
    {
      "epoch": 2.4048253172489424,
      "grad_norm": 5.356622695922852,
      "learning_rate": 4.799597890229255e-05,
      "loss": 2.0485,
      "step": 30700
    },
    {
      "epoch": 2.412658624471252,
      "grad_norm": 6.683080673217773,
      "learning_rate": 4.798945114627396e-05,
      "loss": 1.9445,
      "step": 30800
    },
    {
      "epoch": 2.420491931693561,
      "grad_norm": 6.742157459259033,
      "learning_rate": 4.798292339025537e-05,
      "loss": 2.0611,
      "step": 30900
    },
    {
      "epoch": 2.4283252389158703,
      "grad_norm": 6.667940616607666,
      "learning_rate": 4.797639563423677e-05,
      "loss": 2.0516,
      "step": 31000
    },
    {
      "epoch": 2.4361585461381794,
      "grad_norm": 6.315928936004639,
      "learning_rate": 4.7969867878218186e-05,
      "loss": 2.0515,
      "step": 31100
    },
    {
      "epoch": 2.443991853360489,
      "grad_norm": 7.533237934112549,
      "learning_rate": 4.79633401221996e-05,
      "loss": 1.9439,
      "step": 31200
    },
    {
      "epoch": 2.451825160582798,
      "grad_norm": 6.637378692626953,
      "learning_rate": 4.7956812366181004e-05,
      "loss": 2.001,
      "step": 31300
    },
    {
      "epoch": 2.459658467805107,
      "grad_norm": 6.103724479675293,
      "learning_rate": 4.795028461016241e-05,
      "loss": 1.966,
      "step": 31400
    },
    {
      "epoch": 2.4674917750274163,
      "grad_norm": 6.856485843658447,
      "learning_rate": 4.794375685414382e-05,
      "loss": 1.9277,
      "step": 31500
    },
    {
      "epoch": 2.475325082249726,
      "grad_norm": 5.3789191246032715,
      "learning_rate": 4.793722909812523e-05,
      "loss": 1.9467,
      "step": 31600
    },
    {
      "epoch": 2.483158389472035,
      "grad_norm": 5.6793107986450195,
      "learning_rate": 4.793070134210664e-05,
      "loss": 2.0068,
      "step": 31700
    },
    {
      "epoch": 2.490991696694344,
      "grad_norm": 5.616949558258057,
      "learning_rate": 4.792417358608805e-05,
      "loss": 2.0014,
      "step": 31800
    },
    {
      "epoch": 2.4988250039166537,
      "grad_norm": 6.198630332946777,
      "learning_rate": 4.791764583006946e-05,
      "loss": 1.9925,
      "step": 31900
    },
    {
      "epoch": 2.506658311138963,
      "grad_norm": 4.247830390930176,
      "learning_rate": 4.7911118074050865e-05,
      "loss": 2.0204,
      "step": 32000
    },
    {
      "epoch": 2.514491618361272,
      "grad_norm": 5.22157621383667,
      "learning_rate": 4.790459031803228e-05,
      "loss": 1.9429,
      "step": 32100
    },
    {
      "epoch": 2.5223249255835816,
      "grad_norm": 7.527088642120361,
      "learning_rate": 4.789806256201368e-05,
      "loss": 1.9489,
      "step": 32200
    },
    {
      "epoch": 2.5301582328058907,
      "grad_norm": 6.106583118438721,
      "learning_rate": 4.789153480599509e-05,
      "loss": 2.0856,
      "step": 32300
    },
    {
      "epoch": 2.5379915400282,
      "grad_norm": 5.41178560256958,
      "learning_rate": 4.78850070499765e-05,
      "loss": 2.0504,
      "step": 32400
    },
    {
      "epoch": 2.5458248472505094,
      "grad_norm": 5.812557697296143,
      "learning_rate": 4.7878479293957914e-05,
      "loss": 2.1138,
      "step": 32500
    },
    {
      "epoch": 2.5536581544728185,
      "grad_norm": 5.192811965942383,
      "learning_rate": 4.787195153793932e-05,
      "loss": 1.8674,
      "step": 32600
    },
    {
      "epoch": 2.5614914616951276,
      "grad_norm": 3.6733973026275635,
      "learning_rate": 4.786542378192073e-05,
      "loss": 1.8791,
      "step": 32700
    },
    {
      "epoch": 2.569324768917437,
      "grad_norm": 4.873044967651367,
      "learning_rate": 4.785889602590214e-05,
      "loss": 1.9772,
      "step": 32800
    },
    {
      "epoch": 2.5771580761397463,
      "grad_norm": 4.917568683624268,
      "learning_rate": 4.7852368269883544e-05,
      "loss": 2.0662,
      "step": 32900
    },
    {
      "epoch": 2.5849913833620555,
      "grad_norm": 5.627414226531982,
      "learning_rate": 4.7845840513864956e-05,
      "loss": 2.0826,
      "step": 33000
    },
    {
      "epoch": 2.5928246905843646,
      "grad_norm": 6.773957252502441,
      "learning_rate": 4.783931275784637e-05,
      "loss": 1.9911,
      "step": 33100
    },
    {
      "epoch": 2.6006579978066737,
      "grad_norm": 5.58717155456543,
      "learning_rate": 4.7832785001827775e-05,
      "loss": 2.0147,
      "step": 33200
    },
    {
      "epoch": 2.6084913050289833,
      "grad_norm": 7.165770053863525,
      "learning_rate": 4.782625724580918e-05,
      "loss": 2.0749,
      "step": 33300
    },
    {
      "epoch": 2.6163246122512924,
      "grad_norm": 5.430645942687988,
      "learning_rate": 4.781972948979059e-05,
      "loss": 1.9568,
      "step": 33400
    },
    {
      "epoch": 2.6241579194736016,
      "grad_norm": 8.350567817687988,
      "learning_rate": 4.7813201733772e-05,
      "loss": 2.0864,
      "step": 33500
    },
    {
      "epoch": 2.631991226695911,
      "grad_norm": 5.517162322998047,
      "learning_rate": 4.780667397775341e-05,
      "loss": 2.0282,
      "step": 33600
    },
    {
      "epoch": 2.6398245339182203,
      "grad_norm": 5.979674339294434,
      "learning_rate": 4.7800146221734824e-05,
      "loss": 1.9788,
      "step": 33700
    },
    {
      "epoch": 2.6476578411405294,
      "grad_norm": 5.542837142944336,
      "learning_rate": 4.779361846571623e-05,
      "loss": 2.0963,
      "step": 33800
    },
    {
      "epoch": 2.655491148362839,
      "grad_norm": 6.5974907875061035,
      "learning_rate": 4.7787090709697635e-05,
      "loss": 1.989,
      "step": 33900
    },
    {
      "epoch": 2.663324455585148,
      "grad_norm": 5.6348419189453125,
      "learning_rate": 4.778056295367905e-05,
      "loss": 2.0454,
      "step": 34000
    },
    {
      "epoch": 2.671157762807457,
      "grad_norm": 4.841152191162109,
      "learning_rate": 4.7774035197660454e-05,
      "loss": 1.9571,
      "step": 34100
    },
    {
      "epoch": 2.6789910700297668,
      "grad_norm": 6.591876029968262,
      "learning_rate": 4.776750744164186e-05,
      "loss": 1.9076,
      "step": 34200
    },
    {
      "epoch": 2.686824377252076,
      "grad_norm": 5.685513496398926,
      "learning_rate": 4.776097968562327e-05,
      "loss": 2.0055,
      "step": 34300
    },
    {
      "epoch": 2.694657684474385,
      "grad_norm": 4.808432102203369,
      "learning_rate": 4.7754451929604684e-05,
      "loss": 2.0043,
      "step": 34400
    },
    {
      "epoch": 2.7024909916966946,
      "grad_norm": 4.614828586578369,
      "learning_rate": 4.774792417358609e-05,
      "loss": 1.9383,
      "step": 34500
    },
    {
      "epoch": 2.7103242989190037,
      "grad_norm": 6.294888973236084,
      "learning_rate": 4.7741396417567496e-05,
      "loss": 2.0633,
      "step": 34600
    },
    {
      "epoch": 2.718157606141313,
      "grad_norm": 4.882060527801514,
      "learning_rate": 4.773486866154891e-05,
      "loss": 2.0896,
      "step": 34700
    },
    {
      "epoch": 2.725990913363622,
      "grad_norm": 4.6175923347473145,
      "learning_rate": 4.7728340905530314e-05,
      "loss": 1.9979,
      "step": 34800
    },
    {
      "epoch": 2.733824220585931,
      "grad_norm": 4.753927707672119,
      "learning_rate": 4.772181314951173e-05,
      "loss": 2.022,
      "step": 34900
    },
    {
      "epoch": 2.7416575278082407,
      "grad_norm": 7.088729381561279,
      "learning_rate": 4.771528539349314e-05,
      "loss": 1.9966,
      "step": 35000
    },
    {
      "epoch": 2.74949083503055,
      "grad_norm": 7.213872909545898,
      "learning_rate": 4.7708757637474545e-05,
      "loss": 1.9133,
      "step": 35100
    },
    {
      "epoch": 2.757324142252859,
      "grad_norm": 6.114412307739258,
      "learning_rate": 4.770222988145595e-05,
      "loss": 1.9282,
      "step": 35200
    },
    {
      "epoch": 2.7651574494751685,
      "grad_norm": 6.732129096984863,
      "learning_rate": 4.7695702125437363e-05,
      "loss": 1.9902,
      "step": 35300
    },
    {
      "epoch": 2.7729907566974776,
      "grad_norm": 4.161372661590576,
      "learning_rate": 4.768917436941877e-05,
      "loss": 1.9981,
      "step": 35400
    },
    {
      "epoch": 2.7808240639197868,
      "grad_norm": 4.917385578155518,
      "learning_rate": 4.7682646613400175e-05,
      "loss": 1.9519,
      "step": 35500
    },
    {
      "epoch": 2.7886573711420963,
      "grad_norm": 5.709962844848633,
      "learning_rate": 4.767611885738159e-05,
      "loss": 1.9421,
      "step": 35600
    },
    {
      "epoch": 2.7964906783644055,
      "grad_norm": 6.820308685302734,
      "learning_rate": 4.7669591101363e-05,
      "loss": 1.9564,
      "step": 35700
    },
    {
      "epoch": 2.8043239855867146,
      "grad_norm": 7.355522632598877,
      "learning_rate": 4.7663063345344406e-05,
      "loss": 1.9187,
      "step": 35800
    },
    {
      "epoch": 2.812157292809024,
      "grad_norm": 6.3507819175720215,
      "learning_rate": 4.765653558932582e-05,
      "loss": 2.0307,
      "step": 35900
    },
    {
      "epoch": 2.8199906000313333,
      "grad_norm": 7.466729164123535,
      "learning_rate": 4.7650007833307224e-05,
      "loss": 2.0357,
      "step": 36000
    },
    {
      "epoch": 2.8278239072536424,
      "grad_norm": 4.6971635818481445,
      "learning_rate": 4.764348007728863e-05,
      "loss": 1.9698,
      "step": 36100
    },
    {
      "epoch": 2.835657214475952,
      "grad_norm": 4.549051761627197,
      "learning_rate": 4.763695232127004e-05,
      "loss": 2.0015,
      "step": 36200
    },
    {
      "epoch": 2.843490521698261,
      "grad_norm": 5.877121448516846,
      "learning_rate": 4.7630424565251455e-05,
      "loss": 2.018,
      "step": 36300
    },
    {
      "epoch": 2.8513238289205702,
      "grad_norm": 8.058822631835938,
      "learning_rate": 4.762389680923286e-05,
      "loss": 1.876,
      "step": 36400
    },
    {
      "epoch": 2.8591571361428794,
      "grad_norm": 4.521383762359619,
      "learning_rate": 4.7617369053214267e-05,
      "loss": 2.0694,
      "step": 36500
    },
    {
      "epoch": 2.866990443365189,
      "grad_norm": 5.928529262542725,
      "learning_rate": 4.761084129719568e-05,
      "loss": 1.9333,
      "step": 36600
    },
    {
      "epoch": 2.874823750587498,
      "grad_norm": 11.358901977539062,
      "learning_rate": 4.7604313541177085e-05,
      "loss": 2.0171,
      "step": 36700
    },
    {
      "epoch": 2.882657057809807,
      "grad_norm": 5.5014142990112305,
      "learning_rate": 4.75977857851585e-05,
      "loss": 1.9602,
      "step": 36800
    },
    {
      "epoch": 2.8904903650321163,
      "grad_norm": 6.109546184539795,
      "learning_rate": 4.759125802913991e-05,
      "loss": 1.932,
      "step": 36900
    },
    {
      "epoch": 2.898323672254426,
      "grad_norm": 4.324086666107178,
      "learning_rate": 4.7584730273121316e-05,
      "loss": 1.9322,
      "step": 37000
    },
    {
      "epoch": 2.906156979476735,
      "grad_norm": 4.886047840118408,
      "learning_rate": 4.757820251710272e-05,
      "loss": 2.1013,
      "step": 37100
    },
    {
      "epoch": 2.913990286699044,
      "grad_norm": 5.158526420593262,
      "learning_rate": 4.7571674761084134e-05,
      "loss": 2.0161,
      "step": 37200
    },
    {
      "epoch": 2.9218235939213537,
      "grad_norm": 5.1924357414245605,
      "learning_rate": 4.756514700506554e-05,
      "loss": 1.9915,
      "step": 37300
    },
    {
      "epoch": 2.929656901143663,
      "grad_norm": 6.991426467895508,
      "learning_rate": 4.7558619249046946e-05,
      "loss": 2.0017,
      "step": 37400
    },
    {
      "epoch": 2.937490208365972,
      "grad_norm": 4.638895034790039,
      "learning_rate": 4.755209149302836e-05,
      "loss": 1.9874,
      "step": 37500
    },
    {
      "epoch": 2.9453235155882815,
      "grad_norm": 6.8662285804748535,
      "learning_rate": 4.754556373700977e-05,
      "loss": 1.993,
      "step": 37600
    },
    {
      "epoch": 2.9531568228105907,
      "grad_norm": 6.392449855804443,
      "learning_rate": 4.7539035980991176e-05,
      "loss": 2.0101,
      "step": 37700
    },
    {
      "epoch": 2.9609901300329,
      "grad_norm": 5.90269660949707,
      "learning_rate": 4.753250822497259e-05,
      "loss": 2.0232,
      "step": 37800
    },
    {
      "epoch": 2.9688234372552094,
      "grad_norm": 4.132229804992676,
      "learning_rate": 4.7525980468953995e-05,
      "loss": 1.9923,
      "step": 37900
    },
    {
      "epoch": 2.9766567444775185,
      "grad_norm": 7.163891792297363,
      "learning_rate": 4.75194527129354e-05,
      "loss": 1.9155,
      "step": 38000
    },
    {
      "epoch": 2.9844900516998276,
      "grad_norm": 6.164093017578125,
      "learning_rate": 4.751292495691681e-05,
      "loss": 1.9892,
      "step": 38100
    },
    {
      "epoch": 2.992323358922137,
      "grad_norm": 6.985508918762207,
      "learning_rate": 4.7506397200898226e-05,
      "loss": 1.9933,
      "step": 38200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9308656454086304,
      "eval_runtime": 1.6317,
      "eval_samples_per_second": 411.832,
      "eval_steps_per_second": 411.832,
      "step": 38298
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.7737061977386475,
      "eval_runtime": 29.5642,
      "eval_samples_per_second": 431.806,
      "eval_steps_per_second": 431.806,
      "step": 38298
    },
    {
      "epoch": 3.0001566661444463,
      "grad_norm": 4.71837043762207,
      "learning_rate": 4.749986944487963e-05,
      "loss": 1.9454,
      "step": 38300
    },
    {
      "epoch": 3.0079899733667554,
      "grad_norm": 6.827297210693359,
      "learning_rate": 4.749334168886104e-05,
      "loss": 1.9541,
      "step": 38400
    },
    {
      "epoch": 3.0158232805890646,
      "grad_norm": 5.8941731452941895,
      "learning_rate": 4.748681393284245e-05,
      "loss": 1.9098,
      "step": 38500
    },
    {
      "epoch": 3.023656587811374,
      "grad_norm": 5.896299362182617,
      "learning_rate": 4.7480286176823855e-05,
      "loss": 1.9399,
      "step": 38600
    },
    {
      "epoch": 3.0314898950336833,
      "grad_norm": 6.600996971130371,
      "learning_rate": 4.747375842080526e-05,
      "loss": 1.954,
      "step": 38700
    },
    {
      "epoch": 3.0393232022559924,
      "grad_norm": 4.572072505950928,
      "learning_rate": 4.7467230664786674e-05,
      "loss": 2.0279,
      "step": 38800
    },
    {
      "epoch": 3.0471565094783015,
      "grad_norm": 7.400931358337402,
      "learning_rate": 4.7460702908768086e-05,
      "loss": 1.9829,
      "step": 38900
    },
    {
      "epoch": 3.054989816700611,
      "grad_norm": 5.475318431854248,
      "learning_rate": 4.745417515274949e-05,
      "loss": 1.9506,
      "step": 39000
    },
    {
      "epoch": 3.06282312392292,
      "grad_norm": 5.531495571136475,
      "learning_rate": 4.7447647396730905e-05,
      "loss": 1.9295,
      "step": 39100
    },
    {
      "epoch": 3.0706564311452293,
      "grad_norm": 9.218501091003418,
      "learning_rate": 4.744111964071231e-05,
      "loss": 1.9554,
      "step": 39200
    },
    {
      "epoch": 3.078489738367539,
      "grad_norm": 6.808801651000977,
      "learning_rate": 4.7434591884693716e-05,
      "loss": 1.9075,
      "step": 39300
    },
    {
      "epoch": 3.086323045589848,
      "grad_norm": 6.3780436515808105,
      "learning_rate": 4.742806412867513e-05,
      "loss": 2.0222,
      "step": 39400
    },
    {
      "epoch": 3.094156352812157,
      "grad_norm": 5.410760879516602,
      "learning_rate": 4.742153637265654e-05,
      "loss": 2.0452,
      "step": 39500
    },
    {
      "epoch": 3.1019896600344667,
      "grad_norm": 6.26101016998291,
      "learning_rate": 4.741500861663795e-05,
      "loss": 2.076,
      "step": 39600
    },
    {
      "epoch": 3.109822967256776,
      "grad_norm": 6.662065505981445,
      "learning_rate": 4.740848086061935e-05,
      "loss": 1.9925,
      "step": 39700
    },
    {
      "epoch": 3.117656274479085,
      "grad_norm": 5.016234397888184,
      "learning_rate": 4.7401953104600765e-05,
      "loss": 1.956,
      "step": 39800
    },
    {
      "epoch": 3.125489581701394,
      "grad_norm": 4.996872901916504,
      "learning_rate": 4.739542534858217e-05,
      "loss": 1.9667,
      "step": 39900
    },
    {
      "epoch": 3.1333228889237037,
      "grad_norm": 5.3936076164245605,
      "learning_rate": 4.7388897592563584e-05,
      "loss": 1.9443,
      "step": 40000
    },
    {
      "epoch": 3.141156196146013,
      "grad_norm": 6.536532402038574,
      "learning_rate": 4.7382369836544996e-05,
      "loss": 1.9469,
      "step": 40100
    },
    {
      "epoch": 3.148989503368322,
      "grad_norm": 4.788793087005615,
      "learning_rate": 4.73758420805264e-05,
      "loss": 2.0187,
      "step": 40200
    },
    {
      "epoch": 3.1568228105906315,
      "grad_norm": 5.456810474395752,
      "learning_rate": 4.736931432450781e-05,
      "loss": 1.8999,
      "step": 40300
    },
    {
      "epoch": 3.1646561178129406,
      "grad_norm": 4.265789031982422,
      "learning_rate": 4.736278656848922e-05,
      "loss": 1.9226,
      "step": 40400
    },
    {
      "epoch": 3.1724894250352498,
      "grad_norm": 4.336578369140625,
      "learning_rate": 4.7356258812470626e-05,
      "loss": 1.9178,
      "step": 40500
    },
    {
      "epoch": 3.1803227322575593,
      "grad_norm": 5.5399169921875,
      "learning_rate": 4.734973105645203e-05,
      "loss": 1.974,
      "step": 40600
    },
    {
      "epoch": 3.1881560394798685,
      "grad_norm": 6.467173099517822,
      "learning_rate": 4.7343203300433444e-05,
      "loss": 1.9221,
      "step": 40700
    },
    {
      "epoch": 3.1959893467021776,
      "grad_norm": 4.970762252807617,
      "learning_rate": 4.733667554441486e-05,
      "loss": 2.0256,
      "step": 40800
    },
    {
      "epoch": 3.2038226539244867,
      "grad_norm": 8.345017433166504,
      "learning_rate": 4.733014778839626e-05,
      "loss": 1.9723,
      "step": 40900
    },
    {
      "epoch": 3.2116559611467963,
      "grad_norm": 7.943288803100586,
      "learning_rate": 4.7323620032377675e-05,
      "loss": 1.9314,
      "step": 41000
    },
    {
      "epoch": 3.2194892683691054,
      "grad_norm": 6.0038299560546875,
      "learning_rate": 4.731709227635908e-05,
      "loss": 2.0186,
      "step": 41100
    },
    {
      "epoch": 3.2273225755914146,
      "grad_norm": 5.47782039642334,
      "learning_rate": 4.731056452034049e-05,
      "loss": 2.0056,
      "step": 41200
    },
    {
      "epoch": 3.235155882813724,
      "grad_norm": 4.304720401763916,
      "learning_rate": 4.73040367643219e-05,
      "loss": 1.9505,
      "step": 41300
    },
    {
      "epoch": 3.2429891900360333,
      "grad_norm": 4.22036600112915,
      "learning_rate": 4.729750900830331e-05,
      "loss": 1.8751,
      "step": 41400
    },
    {
      "epoch": 3.2508224972583424,
      "grad_norm": 5.216179847717285,
      "learning_rate": 4.729098125228472e-05,
      "loss": 2.0604,
      "step": 41500
    },
    {
      "epoch": 3.258655804480652,
      "grad_norm": 5.288343906402588,
      "learning_rate": 4.7284453496266123e-05,
      "loss": 1.9989,
      "step": 41600
    },
    {
      "epoch": 3.266489111702961,
      "grad_norm": 5.22849178314209,
      "learning_rate": 4.7277925740247536e-05,
      "loss": 1.894,
      "step": 41700
    },
    {
      "epoch": 3.27432241892527,
      "grad_norm": 6.649212837219238,
      "learning_rate": 4.727139798422894e-05,
      "loss": 1.986,
      "step": 41800
    },
    {
      "epoch": 3.2821557261475793,
      "grad_norm": 7.335737228393555,
      "learning_rate": 4.726487022821035e-05,
      "loss": 1.8219,
      "step": 41900
    },
    {
      "epoch": 3.289989033369889,
      "grad_norm": 5.4919962882995605,
      "learning_rate": 4.725834247219176e-05,
      "loss": 1.9594,
      "step": 42000
    },
    {
      "epoch": 3.297822340592198,
      "grad_norm": 6.60817289352417,
      "learning_rate": 4.725181471617317e-05,
      "loss": 1.9767,
      "step": 42100
    },
    {
      "epoch": 3.305655647814507,
      "grad_norm": 5.820912837982178,
      "learning_rate": 4.724528696015458e-05,
      "loss": 1.9479,
      "step": 42200
    },
    {
      "epoch": 3.3134889550368167,
      "grad_norm": 6.2367353439331055,
      "learning_rate": 4.723875920413599e-05,
      "loss": 1.9475,
      "step": 42300
    },
    {
      "epoch": 3.321322262259126,
      "grad_norm": 5.570112705230713,
      "learning_rate": 4.72322314481174e-05,
      "loss": 1.9777,
      "step": 42400
    },
    {
      "epoch": 3.329155569481435,
      "grad_norm": 7.750535488128662,
      "learning_rate": 4.72257036920988e-05,
      "loss": 2.0387,
      "step": 42500
    },
    {
      "epoch": 3.336988876703744,
      "grad_norm": 4.526148319244385,
      "learning_rate": 4.7219175936080215e-05,
      "loss": 1.9969,
      "step": 42600
    },
    {
      "epoch": 3.3448221839260537,
      "grad_norm": 5.7679595947265625,
      "learning_rate": 4.721264818006163e-05,
      "loss": 2.1117,
      "step": 42700
    },
    {
      "epoch": 3.352655491148363,
      "grad_norm": 4.421351909637451,
      "learning_rate": 4.720612042404303e-05,
      "loss": 1.9378,
      "step": 42800
    },
    {
      "epoch": 3.360488798370672,
      "grad_norm": 4.917328357696533,
      "learning_rate": 4.7199592668024446e-05,
      "loss": 2.0271,
      "step": 42900
    },
    {
      "epoch": 3.3683221055929815,
      "grad_norm": 5.629764080047607,
      "learning_rate": 4.719306491200585e-05,
      "loss": 2.0095,
      "step": 43000
    },
    {
      "epoch": 3.3761554128152906,
      "grad_norm": 4.906534194946289,
      "learning_rate": 4.718653715598726e-05,
      "loss": 1.9294,
      "step": 43100
    },
    {
      "epoch": 3.3839887200375998,
      "grad_norm": 5.091347694396973,
      "learning_rate": 4.718000939996867e-05,
      "loss": 1.9457,
      "step": 43200
    },
    {
      "epoch": 3.3918220272599093,
      "grad_norm": 7.737907886505127,
      "learning_rate": 4.717348164395008e-05,
      "loss": 1.9561,
      "step": 43300
    },
    {
      "epoch": 3.3996553344822185,
      "grad_norm": 6.352792263031006,
      "learning_rate": 4.716695388793149e-05,
      "loss": 1.9972,
      "step": 43400
    },
    {
      "epoch": 3.4074886417045276,
      "grad_norm": 4.957119941711426,
      "learning_rate": 4.7160426131912894e-05,
      "loss": 1.9454,
      "step": 43500
    },
    {
      "epoch": 3.415321948926837,
      "grad_norm": 5.323812961578369,
      "learning_rate": 4.7153898375894307e-05,
      "loss": 1.8836,
      "step": 43600
    },
    {
      "epoch": 3.4231552561491463,
      "grad_norm": 4.882335186004639,
      "learning_rate": 4.714737061987571e-05,
      "loss": 1.9786,
      "step": 43700
    },
    {
      "epoch": 3.4309885633714554,
      "grad_norm": 6.844223976135254,
      "learning_rate": 4.714084286385712e-05,
      "loss": 1.8762,
      "step": 43800
    },
    {
      "epoch": 3.4388218705937645,
      "grad_norm": 7.915201187133789,
      "learning_rate": 4.713431510783853e-05,
      "loss": 1.9572,
      "step": 43900
    },
    {
      "epoch": 3.446655177816074,
      "grad_norm": 5.57480525970459,
      "learning_rate": 4.712778735181994e-05,
      "loss": 1.977,
      "step": 44000
    },
    {
      "epoch": 3.4544884850383832,
      "grad_norm": 5.633299827575684,
      "learning_rate": 4.712125959580135e-05,
      "loss": 1.9833,
      "step": 44100
    },
    {
      "epoch": 3.4623217922606924,
      "grad_norm": 5.884884834289551,
      "learning_rate": 4.711473183978276e-05,
      "loss": 1.9232,
      "step": 44200
    },
    {
      "epoch": 3.4701550994830015,
      "grad_norm": 6.852656841278076,
      "learning_rate": 4.710820408376417e-05,
      "loss": 1.8884,
      "step": 44300
    },
    {
      "epoch": 3.477988406705311,
      "grad_norm": 4.929096221923828,
      "learning_rate": 4.710167632774557e-05,
      "loss": 1.9171,
      "step": 44400
    },
    {
      "epoch": 3.48582171392762,
      "grad_norm": 9.42967414855957,
      "learning_rate": 4.7095148571726986e-05,
      "loss": 1.9422,
      "step": 44500
    },
    {
      "epoch": 3.4936550211499293,
      "grad_norm": 6.042068958282471,
      "learning_rate": 4.70886208157084e-05,
      "loss": 1.9774,
      "step": 44600
    },
    {
      "epoch": 3.501488328372239,
      "grad_norm": 5.580682754516602,
      "learning_rate": 4.7082093059689804e-05,
      "loss": 2.0254,
      "step": 44700
    },
    {
      "epoch": 3.509321635594548,
      "grad_norm": 5.984923839569092,
      "learning_rate": 4.707556530367121e-05,
      "loss": 1.8428,
      "step": 44800
    },
    {
      "epoch": 3.517154942816857,
      "grad_norm": 5.0874457359313965,
      "learning_rate": 4.706903754765262e-05,
      "loss": 2.009,
      "step": 44900
    },
    {
      "epoch": 3.5249882500391667,
      "grad_norm": 4.572721481323242,
      "learning_rate": 4.706250979163403e-05,
      "loss": 1.9483,
      "step": 45000
    },
    {
      "epoch": 3.532821557261476,
      "grad_norm": 4.6263885498046875,
      "learning_rate": 4.7055982035615434e-05,
      "loss": 1.9743,
      "step": 45100
    },
    {
      "epoch": 3.540654864483785,
      "grad_norm": 4.157665252685547,
      "learning_rate": 4.7049454279596846e-05,
      "loss": 1.9869,
      "step": 45200
    },
    {
      "epoch": 3.5484881717060945,
      "grad_norm": 6.196304798126221,
      "learning_rate": 4.704292652357826e-05,
      "loss": 1.9913,
      "step": 45300
    },
    {
      "epoch": 3.5563214789284037,
      "grad_norm": 4.359049320220947,
      "learning_rate": 4.7036398767559665e-05,
      "loss": 1.9091,
      "step": 45400
    },
    {
      "epoch": 3.564154786150713,
      "grad_norm": 5.654540538787842,
      "learning_rate": 4.702987101154108e-05,
      "loss": 1.9933,
      "step": 45500
    },
    {
      "epoch": 3.5719880933730224,
      "grad_norm": 5.765282154083252,
      "learning_rate": 4.702334325552248e-05,
      "loss": 1.9193,
      "step": 45600
    },
    {
      "epoch": 3.5798214005953315,
      "grad_norm": 5.346955299377441,
      "learning_rate": 4.701681549950389e-05,
      "loss": 1.9482,
      "step": 45700
    },
    {
      "epoch": 3.5876547078176406,
      "grad_norm": 6.98417854309082,
      "learning_rate": 4.70102877434853e-05,
      "loss": 1.9541,
      "step": 45800
    },
    {
      "epoch": 3.5954880150399497,
      "grad_norm": 6.327020168304443,
      "learning_rate": 4.7003759987466714e-05,
      "loss": 1.9467,
      "step": 45900
    },
    {
      "epoch": 3.603321322262259,
      "grad_norm": 4.51192045211792,
      "learning_rate": 4.699723223144812e-05,
      "loss": 2.0142,
      "step": 46000
    },
    {
      "epoch": 3.6111546294845684,
      "grad_norm": 7.7480292320251465,
      "learning_rate": 4.699070447542953e-05,
      "loss": 1.9955,
      "step": 46100
    },
    {
      "epoch": 3.6189879367068776,
      "grad_norm": 4.622944355010986,
      "learning_rate": 4.698417671941094e-05,
      "loss": 1.9033,
      "step": 46200
    },
    {
      "epoch": 3.6268212439291867,
      "grad_norm": 5.313327312469482,
      "learning_rate": 4.6977648963392344e-05,
      "loss": 1.8744,
      "step": 46300
    },
    {
      "epoch": 3.6346545511514963,
      "grad_norm": 4.7374372482299805,
      "learning_rate": 4.6971121207373756e-05,
      "loss": 1.9038,
      "step": 46400
    },
    {
      "epoch": 3.6424878583738054,
      "grad_norm": 6.3743414878845215,
      "learning_rate": 4.696459345135517e-05,
      "loss": 1.9426,
      "step": 46500
    },
    {
      "epoch": 3.6503211655961145,
      "grad_norm": 7.207611083984375,
      "learning_rate": 4.6958065695336574e-05,
      "loss": 1.9636,
      "step": 46600
    },
    {
      "epoch": 3.658154472818424,
      "grad_norm": 7.885278701782227,
      "learning_rate": 4.695153793931798e-05,
      "loss": 2.0858,
      "step": 46700
    },
    {
      "epoch": 3.6659877800407332,
      "grad_norm": 6.004594802856445,
      "learning_rate": 4.694501018329939e-05,
      "loss": 1.8529,
      "step": 46800
    },
    {
      "epoch": 3.6738210872630424,
      "grad_norm": 7.749959468841553,
      "learning_rate": 4.69384824272808e-05,
      "loss": 2.0034,
      "step": 46900
    },
    {
      "epoch": 3.681654394485352,
      "grad_norm": 6.782040119171143,
      "learning_rate": 4.6931954671262204e-05,
      "loss": 1.9533,
      "step": 47000
    },
    {
      "epoch": 3.689487701707661,
      "grad_norm": 5.979499816894531,
      "learning_rate": 4.692542691524362e-05,
      "loss": 1.9701,
      "step": 47100
    },
    {
      "epoch": 3.69732100892997,
      "grad_norm": 4.491099834442139,
      "learning_rate": 4.691889915922503e-05,
      "loss": 1.9537,
      "step": 47200
    },
    {
      "epoch": 3.7051543161522797,
      "grad_norm": 4.501776695251465,
      "learning_rate": 4.6912371403206435e-05,
      "loss": 1.9596,
      "step": 47300
    },
    {
      "epoch": 3.712987623374589,
      "grad_norm": 7.332306861877441,
      "learning_rate": 4.690584364718785e-05,
      "loss": 1.8826,
      "step": 47400
    },
    {
      "epoch": 3.720820930596898,
      "grad_norm": 6.043405055999756,
      "learning_rate": 4.6899315891169253e-05,
      "loss": 1.9794,
      "step": 47500
    },
    {
      "epoch": 3.728654237819207,
      "grad_norm": 4.257876396179199,
      "learning_rate": 4.689278813515066e-05,
      "loss": 1.8999,
      "step": 47600
    },
    {
      "epoch": 3.7364875450415163,
      "grad_norm": 5.383195400238037,
      "learning_rate": 4.688626037913207e-05,
      "loss": 1.9912,
      "step": 47700
    },
    {
      "epoch": 3.744320852263826,
      "grad_norm": 4.772984027862549,
      "learning_rate": 4.6879732623113484e-05,
      "loss": 1.8628,
      "step": 47800
    },
    {
      "epoch": 3.752154159486135,
      "grad_norm": 5.122155666351318,
      "learning_rate": 4.687320486709489e-05,
      "loss": 1.9416,
      "step": 47900
    },
    {
      "epoch": 3.759987466708444,
      "grad_norm": 5.505286693572998,
      "learning_rate": 4.6866677111076296e-05,
      "loss": 1.9208,
      "step": 48000
    },
    {
      "epoch": 3.7678207739307537,
      "grad_norm": 5.426303863525391,
      "learning_rate": 4.686014935505771e-05,
      "loss": 2.0288,
      "step": 48100
    },
    {
      "epoch": 3.775654081153063,
      "grad_norm": 5.433131694793701,
      "learning_rate": 4.6853621599039114e-05,
      "loss": 1.906,
      "step": 48200
    },
    {
      "epoch": 3.783487388375372,
      "grad_norm": 4.359324932098389,
      "learning_rate": 4.684709384302052e-05,
      "loss": 1.9416,
      "step": 48300
    },
    {
      "epoch": 3.7913206955976815,
      "grad_norm": 5.202821731567383,
      "learning_rate": 4.684056608700193e-05,
      "loss": 1.9863,
      "step": 48400
    },
    {
      "epoch": 3.7991540028199906,
      "grad_norm": 10.237892150878906,
      "learning_rate": 4.6834038330983345e-05,
      "loss": 1.8781,
      "step": 48500
    },
    {
      "epoch": 3.8069873100422997,
      "grad_norm": 5.0815863609313965,
      "learning_rate": 4.682751057496475e-05,
      "loss": 1.9329,
      "step": 48600
    },
    {
      "epoch": 3.8148206172646093,
      "grad_norm": 5.60310173034668,
      "learning_rate": 4.682098281894616e-05,
      "loss": 1.9885,
      "step": 48700
    },
    {
      "epoch": 3.8226539244869184,
      "grad_norm": 7.459204196929932,
      "learning_rate": 4.681445506292757e-05,
      "loss": 1.9542,
      "step": 48800
    },
    {
      "epoch": 3.8304872317092276,
      "grad_norm": 4.815457344055176,
      "learning_rate": 4.6807927306908975e-05,
      "loss": 1.8925,
      "step": 48900
    },
    {
      "epoch": 3.838320538931537,
      "grad_norm": 5.704172134399414,
      "learning_rate": 4.680139955089039e-05,
      "loss": 1.947,
      "step": 49000
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 4.554837703704834,
      "learning_rate": 4.67948717948718e-05,
      "loss": 2.0066,
      "step": 49100
    },
    {
      "epoch": 3.8539871533761554,
      "grad_norm": 5.504068374633789,
      "learning_rate": 4.6788344038853206e-05,
      "loss": 1.9304,
      "step": 49200
    },
    {
      "epoch": 3.8618204605984645,
      "grad_norm": 5.584444046020508,
      "learning_rate": 4.678181628283462e-05,
      "loss": 1.8726,
      "step": 49300
    },
    {
      "epoch": 3.869653767820774,
      "grad_norm": 5.503060817718506,
      "learning_rate": 4.6775288526816024e-05,
      "loss": 1.9907,
      "step": 49400
    },
    {
      "epoch": 3.877487075043083,
      "grad_norm": 8.496488571166992,
      "learning_rate": 4.676876077079743e-05,
      "loss": 1.8778,
      "step": 49500
    },
    {
      "epoch": 3.8853203822653923,
      "grad_norm": 6.42442512512207,
      "learning_rate": 4.676223301477884e-05,
      "loss": 1.9611,
      "step": 49600
    },
    {
      "epoch": 3.8931536894877015,
      "grad_norm": 5.710604190826416,
      "learning_rate": 4.6755705258760255e-05,
      "loss": 2.0196,
      "step": 49700
    },
    {
      "epoch": 3.900986996710011,
      "grad_norm": 8.356612205505371,
      "learning_rate": 4.674917750274166e-05,
      "loss": 1.9161,
      "step": 49800
    },
    {
      "epoch": 3.90882030393232,
      "grad_norm": 5.904664039611816,
      "learning_rate": 4.6742649746723066e-05,
      "loss": 1.913,
      "step": 49900
    },
    {
      "epoch": 3.9166536111546293,
      "grad_norm": 6.736499309539795,
      "learning_rate": 4.673612199070448e-05,
      "loss": 1.9872,
      "step": 50000
    },
    {
      "epoch": 3.924486918376939,
      "grad_norm": 5.354021072387695,
      "learning_rate": 4.6729594234685885e-05,
      "loss": 1.8889,
      "step": 50100
    },
    {
      "epoch": 3.932320225599248,
      "grad_norm": 5.013912677764893,
      "learning_rate": 4.672306647866729e-05,
      "loss": 1.9472,
      "step": 50200
    },
    {
      "epoch": 3.940153532821557,
      "grad_norm": 5.841207027435303,
      "learning_rate": 4.67165387226487e-05,
      "loss": 1.9716,
      "step": 50300
    },
    {
      "epoch": 3.9479868400438667,
      "grad_norm": 4.6704182624816895,
      "learning_rate": 4.6710010966630116e-05,
      "loss": 1.9037,
      "step": 50400
    },
    {
      "epoch": 3.955820147266176,
      "grad_norm": 7.427650451660156,
      "learning_rate": 4.670348321061152e-05,
      "loss": 1.9087,
      "step": 50500
    },
    {
      "epoch": 3.963653454488485,
      "grad_norm": 4.9594831466674805,
      "learning_rate": 4.6696955454592934e-05,
      "loss": 1.9626,
      "step": 50600
    },
    {
      "epoch": 3.9714867617107945,
      "grad_norm": 5.944599151611328,
      "learning_rate": 4.669042769857434e-05,
      "loss": 2.0118,
      "step": 50700
    },
    {
      "epoch": 3.9793200689331036,
      "grad_norm": 4.139528274536133,
      "learning_rate": 4.6683899942555745e-05,
      "loss": 2.0068,
      "step": 50800
    },
    {
      "epoch": 3.9871533761554128,
      "grad_norm": 5.537788391113281,
      "learning_rate": 4.667737218653716e-05,
      "loss": 1.9819,
      "step": 50900
    },
    {
      "epoch": 3.9949866833777223,
      "grad_norm": 5.763659477233887,
      "learning_rate": 4.667084443051857e-05,
      "loss": 1.9438,
      "step": 51000
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9062670469284058,
      "eval_runtime": 1.5629,
      "eval_samples_per_second": 429.977,
      "eval_steps_per_second": 429.977,
      "step": 51064
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.7356221675872803,
      "eval_runtime": 29.5107,
      "eval_samples_per_second": 432.588,
      "eval_steps_per_second": 432.588,
      "step": 51064
    },
    {
      "epoch": 4.002819990600031,
      "grad_norm": 4.682296276092529,
      "learning_rate": 4.6664316674499976e-05,
      "loss": 1.9427,
      "step": 51100
    },
    {
      "epoch": 4.010653297822341,
      "grad_norm": 4.486672401428223,
      "learning_rate": 4.665778891848139e-05,
      "loss": 1.9667,
      "step": 51200
    },
    {
      "epoch": 4.01848660504465,
      "grad_norm": 6.315978050231934,
      "learning_rate": 4.6651261162462795e-05,
      "loss": 1.9603,
      "step": 51300
    },
    {
      "epoch": 4.026319912266959,
      "grad_norm": 4.4069318771362305,
      "learning_rate": 4.66447334064442e-05,
      "loss": 1.8579,
      "step": 51400
    },
    {
      "epoch": 4.034153219489268,
      "grad_norm": 5.5392327308654785,
      "learning_rate": 4.6638205650425606e-05,
      "loss": 1.9958,
      "step": 51500
    },
    {
      "epoch": 4.041986526711578,
      "grad_norm": 5.733121871948242,
      "learning_rate": 4.663167789440702e-05,
      "loss": 1.923,
      "step": 51600
    },
    {
      "epoch": 4.049819833933887,
      "grad_norm": 6.856019973754883,
      "learning_rate": 4.662515013838843e-05,
      "loss": 1.9173,
      "step": 51700
    },
    {
      "epoch": 4.057653141156196,
      "grad_norm": 4.823547840118408,
      "learning_rate": 4.661862238236984e-05,
      "loss": 1.8553,
      "step": 51800
    },
    {
      "epoch": 4.065486448378506,
      "grad_norm": 5.923846244812012,
      "learning_rate": 4.661209462635125e-05,
      "loss": 1.8098,
      "step": 51900
    },
    {
      "epoch": 4.0733197556008145,
      "grad_norm": 5.309613227844238,
      "learning_rate": 4.6605566870332655e-05,
      "loss": 1.9002,
      "step": 52000
    },
    {
      "epoch": 4.081153062823124,
      "grad_norm": 6.950382232666016,
      "learning_rate": 4.659903911431406e-05,
      "loss": 1.967,
      "step": 52100
    },
    {
      "epoch": 4.088986370045433,
      "grad_norm": 5.522065162658691,
      "learning_rate": 4.6592511358295474e-05,
      "loss": 2.019,
      "step": 52200
    },
    {
      "epoch": 4.096819677267742,
      "grad_norm": 4.045872211456299,
      "learning_rate": 4.6585983602276886e-05,
      "loss": 1.9536,
      "step": 52300
    },
    {
      "epoch": 4.104652984490052,
      "grad_norm": 6.016124248504639,
      "learning_rate": 4.657945584625829e-05,
      "loss": 1.9552,
      "step": 52400
    },
    {
      "epoch": 4.112486291712361,
      "grad_norm": 5.175796031951904,
      "learning_rate": 4.6572928090239705e-05,
      "loss": 1.9139,
      "step": 52500
    },
    {
      "epoch": 4.12031959893467,
      "grad_norm": 6.461360454559326,
      "learning_rate": 4.656640033422111e-05,
      "loss": 1.8909,
      "step": 52600
    },
    {
      "epoch": 4.12815290615698,
      "grad_norm": 5.948416709899902,
      "learning_rate": 4.6559872578202516e-05,
      "loss": 1.8501,
      "step": 52700
    },
    {
      "epoch": 4.135986213379288,
      "grad_norm": 8.311089515686035,
      "learning_rate": 4.655334482218393e-05,
      "loss": 1.8687,
      "step": 52800
    },
    {
      "epoch": 4.143819520601598,
      "grad_norm": 7.44896125793457,
      "learning_rate": 4.654681706616534e-05,
      "loss": 1.8767,
      "step": 52900
    },
    {
      "epoch": 4.1516528278239075,
      "grad_norm": 5.780789852142334,
      "learning_rate": 4.654028931014675e-05,
      "loss": 1.9328,
      "step": 53000
    },
    {
      "epoch": 4.159486135046216,
      "grad_norm": 5.17604398727417,
      "learning_rate": 4.653376155412815e-05,
      "loss": 1.8619,
      "step": 53100
    },
    {
      "epoch": 4.167319442268526,
      "grad_norm": 6.063261032104492,
      "learning_rate": 4.6527233798109565e-05,
      "loss": 1.9141,
      "step": 53200
    },
    {
      "epoch": 4.175152749490835,
      "grad_norm": 5.493602752685547,
      "learning_rate": 4.652070604209097e-05,
      "loss": 1.9,
      "step": 53300
    },
    {
      "epoch": 4.182986056713144,
      "grad_norm": 7.300305366516113,
      "learning_rate": 4.651417828607238e-05,
      "loss": 1.9755,
      "step": 53400
    },
    {
      "epoch": 4.190819363935454,
      "grad_norm": 5.117328643798828,
      "learning_rate": 4.650765053005379e-05,
      "loss": 1.9377,
      "step": 53500
    },
    {
      "epoch": 4.198652671157763,
      "grad_norm": 3.475623607635498,
      "learning_rate": 4.65011227740352e-05,
      "loss": 1.9542,
      "step": 53600
    },
    {
      "epoch": 4.206485978380072,
      "grad_norm": 3.75846791267395,
      "learning_rate": 4.649459501801661e-05,
      "loss": 1.813,
      "step": 53700
    },
    {
      "epoch": 4.2143192856023814,
      "grad_norm": 4.692771911621094,
      "learning_rate": 4.648806726199802e-05,
      "loss": 1.8531,
      "step": 53800
    },
    {
      "epoch": 4.22215259282469,
      "grad_norm": 4.991825103759766,
      "learning_rate": 4.6481539505979426e-05,
      "loss": 1.9071,
      "step": 53900
    },
    {
      "epoch": 4.229985900047,
      "grad_norm": 4.9339799880981445,
      "learning_rate": 4.647501174996083e-05,
      "loss": 2.0171,
      "step": 54000
    },
    {
      "epoch": 4.237819207269309,
      "grad_norm": 4.915694713592529,
      "learning_rate": 4.6468483993942244e-05,
      "loss": 1.8866,
      "step": 54100
    },
    {
      "epoch": 4.245652514491618,
      "grad_norm": 6.411978244781494,
      "learning_rate": 4.646195623792366e-05,
      "loss": 1.8487,
      "step": 54200
    },
    {
      "epoch": 4.2534858217139275,
      "grad_norm": 5.393667221069336,
      "learning_rate": 4.645542848190506e-05,
      "loss": 1.9677,
      "step": 54300
    },
    {
      "epoch": 4.261319128936237,
      "grad_norm": 5.058690547943115,
      "learning_rate": 4.6448900725886475e-05,
      "loss": 1.9368,
      "step": 54400
    },
    {
      "epoch": 4.269152436158546,
      "grad_norm": 5.145897388458252,
      "learning_rate": 4.644237296986788e-05,
      "loss": 1.9319,
      "step": 54500
    },
    {
      "epoch": 4.276985743380855,
      "grad_norm": 5.182901382446289,
      "learning_rate": 4.643584521384929e-05,
      "loss": 1.8872,
      "step": 54600
    },
    {
      "epoch": 4.284819050603165,
      "grad_norm": 6.572621822357178,
      "learning_rate": 4.64293174578307e-05,
      "loss": 1.9014,
      "step": 54700
    },
    {
      "epoch": 4.292652357825474,
      "grad_norm": 4.106717586517334,
      "learning_rate": 4.6422789701812105e-05,
      "loss": 2.0906,
      "step": 54800
    },
    {
      "epoch": 4.300485665047783,
      "grad_norm": 6.750456809997559,
      "learning_rate": 4.641626194579352e-05,
      "loss": 1.9524,
      "step": 54900
    },
    {
      "epoch": 4.308318972270093,
      "grad_norm": 6.986253261566162,
      "learning_rate": 4.640973418977492e-05,
      "loss": 1.9699,
      "step": 55000
    },
    {
      "epoch": 4.316152279492401,
      "grad_norm": 5.7419352531433105,
      "learning_rate": 4.6403206433756336e-05,
      "loss": 1.9456,
      "step": 55100
    },
    {
      "epoch": 4.323985586714711,
      "grad_norm": 5.918210506439209,
      "learning_rate": 4.639667867773774e-05,
      "loss": 1.9581,
      "step": 55200
    },
    {
      "epoch": 4.331818893937021,
      "grad_norm": 5.062831878662109,
      "learning_rate": 4.639015092171915e-05,
      "loss": 1.925,
      "step": 55300
    },
    {
      "epoch": 4.339652201159329,
      "grad_norm": 5.151716232299805,
      "learning_rate": 4.638362316570056e-05,
      "loss": 1.8695,
      "step": 55400
    },
    {
      "epoch": 4.347485508381639,
      "grad_norm": 5.7987446784973145,
      "learning_rate": 4.637709540968197e-05,
      "loss": 1.8639,
      "step": 55500
    },
    {
      "epoch": 4.355318815603948,
      "grad_norm": 7.6174635887146,
      "learning_rate": 4.637056765366338e-05,
      "loss": 1.9659,
      "step": 55600
    },
    {
      "epoch": 4.363152122826257,
      "grad_norm": 5.3916707038879395,
      "learning_rate": 4.636403989764479e-05,
      "loss": 1.8736,
      "step": 55700
    },
    {
      "epoch": 4.370985430048567,
      "grad_norm": 5.753052711486816,
      "learning_rate": 4.6357512141626197e-05,
      "loss": 1.9308,
      "step": 55800
    },
    {
      "epoch": 4.378818737270876,
      "grad_norm": 6.1557183265686035,
      "learning_rate": 4.63509843856076e-05,
      "loss": 1.824,
      "step": 55900
    },
    {
      "epoch": 4.386652044493185,
      "grad_norm": 5.866579055786133,
      "learning_rate": 4.6344456629589015e-05,
      "loss": 2.0019,
      "step": 56000
    },
    {
      "epoch": 4.3944853517154945,
      "grad_norm": 5.820334434509277,
      "learning_rate": 4.633792887357043e-05,
      "loss": 1.9735,
      "step": 56100
    },
    {
      "epoch": 4.402318658937803,
      "grad_norm": 6.242558002471924,
      "learning_rate": 4.633140111755183e-05,
      "loss": 1.9792,
      "step": 56200
    },
    {
      "epoch": 4.410151966160113,
      "grad_norm": 5.907978057861328,
      "learning_rate": 4.6324873361533246e-05,
      "loss": 1.8854,
      "step": 56300
    },
    {
      "epoch": 4.417985273382422,
      "grad_norm": 9.965494155883789,
      "learning_rate": 4.631834560551465e-05,
      "loss": 1.8564,
      "step": 56400
    },
    {
      "epoch": 4.425818580604731,
      "grad_norm": 6.046030044555664,
      "learning_rate": 4.631181784949606e-05,
      "loss": 1.9924,
      "step": 56500
    },
    {
      "epoch": 4.433651887827041,
      "grad_norm": 5.239116668701172,
      "learning_rate": 4.630529009347746e-05,
      "loss": 1.8684,
      "step": 56600
    },
    {
      "epoch": 4.44148519504935,
      "grad_norm": 8.218957901000977,
      "learning_rate": 4.6298762337458876e-05,
      "loss": 1.9512,
      "step": 56700
    },
    {
      "epoch": 4.449318502271659,
      "grad_norm": 6.270647048950195,
      "learning_rate": 4.629223458144029e-05,
      "loss": 1.9866,
      "step": 56800
    },
    {
      "epoch": 4.457151809493968,
      "grad_norm": 5.395991802215576,
      "learning_rate": 4.6285706825421694e-05,
      "loss": 1.8806,
      "step": 56900
    },
    {
      "epoch": 4.464985116716278,
      "grad_norm": 6.8945465087890625,
      "learning_rate": 4.6279179069403106e-05,
      "loss": 1.8517,
      "step": 57000
    },
    {
      "epoch": 4.472818423938587,
      "grad_norm": 6.832829475402832,
      "learning_rate": 4.627265131338451e-05,
      "loss": 2.001,
      "step": 57100
    },
    {
      "epoch": 4.480651731160896,
      "grad_norm": 5.179531574249268,
      "learning_rate": 4.626612355736592e-05,
      "loss": 1.8356,
      "step": 57200
    },
    {
      "epoch": 4.488485038383206,
      "grad_norm": 6.830272197723389,
      "learning_rate": 4.625959580134733e-05,
      "loss": 1.9768,
      "step": 57300
    },
    {
      "epoch": 4.4963183456055145,
      "grad_norm": 5.425405502319336,
      "learning_rate": 4.625306804532874e-05,
      "loss": 1.9979,
      "step": 57400
    },
    {
      "epoch": 4.504151652827824,
      "grad_norm": 5.522280693054199,
      "learning_rate": 4.624654028931015e-05,
      "loss": 2.0128,
      "step": 57500
    },
    {
      "epoch": 4.511984960050134,
      "grad_norm": 4.542263984680176,
      "learning_rate": 4.624001253329156e-05,
      "loss": 1.8458,
      "step": 57600
    },
    {
      "epoch": 4.519818267272442,
      "grad_norm": 4.077132701873779,
      "learning_rate": 4.623348477727297e-05,
      "loss": 1.9053,
      "step": 57700
    },
    {
      "epoch": 4.527651574494752,
      "grad_norm": 6.2385478019714355,
      "learning_rate": 4.622695702125437e-05,
      "loss": 1.8144,
      "step": 57800
    },
    {
      "epoch": 4.5354848817170605,
      "grad_norm": 6.882685661315918,
      "learning_rate": 4.6220429265235785e-05,
      "loss": 2.0257,
      "step": 57900
    },
    {
      "epoch": 4.54331818893937,
      "grad_norm": 5.108600616455078,
      "learning_rate": 4.621390150921719e-05,
      "loss": 1.8651,
      "step": 58000
    },
    {
      "epoch": 4.55115149616168,
      "grad_norm": 5.5733184814453125,
      "learning_rate": 4.6207373753198604e-05,
      "loss": 1.873,
      "step": 58100
    },
    {
      "epoch": 4.558984803383988,
      "grad_norm": 9.31253433227539,
      "learning_rate": 4.620084599718001e-05,
      "loss": 1.8719,
      "step": 58200
    },
    {
      "epoch": 4.566818110606298,
      "grad_norm": 6.2545881271362305,
      "learning_rate": 4.619431824116142e-05,
      "loss": 1.9489,
      "step": 58300
    },
    {
      "epoch": 4.5746514178286075,
      "grad_norm": 5.337741374969482,
      "learning_rate": 4.618779048514283e-05,
      "loss": 1.9216,
      "step": 58400
    },
    {
      "epoch": 4.582484725050916,
      "grad_norm": 6.741903781890869,
      "learning_rate": 4.6181262729124234e-05,
      "loss": 1.9131,
      "step": 58500
    },
    {
      "epoch": 4.590318032273226,
      "grad_norm": 5.090508460998535,
      "learning_rate": 4.6174734973105646e-05,
      "loss": 1.9017,
      "step": 58600
    },
    {
      "epoch": 4.598151339495535,
      "grad_norm": 6.200201511383057,
      "learning_rate": 4.616820721708706e-05,
      "loss": 1.9494,
      "step": 58700
    },
    {
      "epoch": 4.605984646717844,
      "grad_norm": 5.844675064086914,
      "learning_rate": 4.6161679461068464e-05,
      "loss": 1.9748,
      "step": 58800
    },
    {
      "epoch": 4.613817953940154,
      "grad_norm": 4.830227375030518,
      "learning_rate": 4.615515170504988e-05,
      "loss": 1.9538,
      "step": 58900
    },
    {
      "epoch": 4.621651261162463,
      "grad_norm": 5.7162041664123535,
      "learning_rate": 4.614862394903128e-05,
      "loss": 2.01,
      "step": 59000
    },
    {
      "epoch": 4.629484568384772,
      "grad_norm": 5.148288726806641,
      "learning_rate": 4.614209619301269e-05,
      "loss": 1.8995,
      "step": 59100
    },
    {
      "epoch": 4.637317875607081,
      "grad_norm": 6.013646125793457,
      "learning_rate": 4.61355684369941e-05,
      "loss": 1.9465,
      "step": 59200
    },
    {
      "epoch": 4.645151182829391,
      "grad_norm": 5.152865886688232,
      "learning_rate": 4.6129040680975514e-05,
      "loss": 1.9264,
      "step": 59300
    },
    {
      "epoch": 4.6529844900517,
      "grad_norm": 5.245610237121582,
      "learning_rate": 4.612251292495692e-05,
      "loss": 1.9632,
      "step": 59400
    },
    {
      "epoch": 4.660817797274009,
      "grad_norm": 5.361746788024902,
      "learning_rate": 4.611598516893833e-05,
      "loss": 1.8813,
      "step": 59500
    },
    {
      "epoch": 4.668651104496318,
      "grad_norm": 7.104550838470459,
      "learning_rate": 4.610945741291974e-05,
      "loss": 1.9949,
      "step": 59600
    },
    {
      "epoch": 4.6764844117186275,
      "grad_norm": 4.114593505859375,
      "learning_rate": 4.6102929656901143e-05,
      "loss": 1.8911,
      "step": 59700
    },
    {
      "epoch": 4.684317718940937,
      "grad_norm": 5.124051094055176,
      "learning_rate": 4.609640190088255e-05,
      "loss": 1.9385,
      "step": 59800
    },
    {
      "epoch": 4.692151026163246,
      "grad_norm": 6.6019463539123535,
      "learning_rate": 4.608987414486396e-05,
      "loss": 1.8552,
      "step": 59900
    },
    {
      "epoch": 4.699984333385555,
      "grad_norm": 6.8896260261535645,
      "learning_rate": 4.6083346388845374e-05,
      "loss": 1.9293,
      "step": 60000
    },
    {
      "epoch": 4.707817640607865,
      "grad_norm": 7.15289831161499,
      "learning_rate": 4.607681863282678e-05,
      "loss": 1.9729,
      "step": 60100
    },
    {
      "epoch": 4.715650947830174,
      "grad_norm": 7.214371204376221,
      "learning_rate": 4.607029087680819e-05,
      "loss": 1.9755,
      "step": 60200
    },
    {
      "epoch": 4.723484255052483,
      "grad_norm": 3.9515655040740967,
      "learning_rate": 4.60637631207896e-05,
      "loss": 1.8788,
      "step": 60300
    },
    {
      "epoch": 4.731317562274793,
      "grad_norm": 5.759751319885254,
      "learning_rate": 4.6057235364771004e-05,
      "loss": 1.9233,
      "step": 60400
    },
    {
      "epoch": 4.739150869497101,
      "grad_norm": 7.08296012878418,
      "learning_rate": 4.605070760875242e-05,
      "loss": 1.9443,
      "step": 60500
    },
    {
      "epoch": 4.746984176719411,
      "grad_norm": 14.06915283203125,
      "learning_rate": 4.604417985273383e-05,
      "loss": 1.9507,
      "step": 60600
    },
    {
      "epoch": 4.7548174839417205,
      "grad_norm": 5.3184590339660645,
      "learning_rate": 4.6037652096715235e-05,
      "loss": 1.9979,
      "step": 60700
    },
    {
      "epoch": 4.762650791164029,
      "grad_norm": 5.14134407043457,
      "learning_rate": 4.603112434069665e-05,
      "loss": 1.8395,
      "step": 60800
    },
    {
      "epoch": 4.770484098386339,
      "grad_norm": 6.269712448120117,
      "learning_rate": 4.602459658467805e-05,
      "loss": 1.9229,
      "step": 60900
    },
    {
      "epoch": 4.778317405608648,
      "grad_norm": 6.645703315734863,
      "learning_rate": 4.601806882865946e-05,
      "loss": 1.8732,
      "step": 61000
    },
    {
      "epoch": 4.786150712830957,
      "grad_norm": 6.284276008605957,
      "learning_rate": 4.601154107264087e-05,
      "loss": 1.9075,
      "step": 61100
    },
    {
      "epoch": 4.793984020053267,
      "grad_norm": 5.265667915344238,
      "learning_rate": 4.600501331662228e-05,
      "loss": 1.9624,
      "step": 61200
    },
    {
      "epoch": 4.801817327275575,
      "grad_norm": 4.947025775909424,
      "learning_rate": 4.599848556060369e-05,
      "loss": 1.8546,
      "step": 61300
    },
    {
      "epoch": 4.809650634497885,
      "grad_norm": 7.294427394866943,
      "learning_rate": 4.59919578045851e-05,
      "loss": 1.8659,
      "step": 61400
    },
    {
      "epoch": 4.8174839417201945,
      "grad_norm": 5.107466697692871,
      "learning_rate": 4.598543004856651e-05,
      "loss": 1.8558,
      "step": 61500
    },
    {
      "epoch": 4.825317248942504,
      "grad_norm": 6.689131736755371,
      "learning_rate": 4.5978902292547914e-05,
      "loss": 1.8987,
      "step": 61600
    },
    {
      "epoch": 4.833150556164813,
      "grad_norm": 5.275065898895264,
      "learning_rate": 4.597237453652932e-05,
      "loss": 1.9791,
      "step": 61700
    },
    {
      "epoch": 4.840983863387122,
      "grad_norm": 6.329781532287598,
      "learning_rate": 4.596584678051073e-05,
      "loss": 1.8599,
      "step": 61800
    },
    {
      "epoch": 4.848817170609431,
      "grad_norm": 6.932374477386475,
      "learning_rate": 4.5959319024492145e-05,
      "loss": 1.9529,
      "step": 61900
    },
    {
      "epoch": 4.8566504778317405,
      "grad_norm": 4.901656627655029,
      "learning_rate": 4.595279126847355e-05,
      "loss": 1.934,
      "step": 62000
    },
    {
      "epoch": 4.86448378505405,
      "grad_norm": 5.701454162597656,
      "learning_rate": 4.594626351245496e-05,
      "loss": 2.0212,
      "step": 62100
    },
    {
      "epoch": 4.872317092276359,
      "grad_norm": 5.976922512054443,
      "learning_rate": 4.593973575643637e-05,
      "loss": 1.8913,
      "step": 62200
    },
    {
      "epoch": 4.880150399498668,
      "grad_norm": 5.360473155975342,
      "learning_rate": 4.5933208000417775e-05,
      "loss": 1.9559,
      "step": 62300
    },
    {
      "epoch": 4.887983706720978,
      "grad_norm": 5.4633636474609375,
      "learning_rate": 4.592668024439919e-05,
      "loss": 1.9695,
      "step": 62400
    },
    {
      "epoch": 4.895817013943287,
      "grad_norm": 5.9654541015625,
      "learning_rate": 4.59201524883806e-05,
      "loss": 1.8877,
      "step": 62500
    },
    {
      "epoch": 4.903650321165596,
      "grad_norm": 5.210245609283447,
      "learning_rate": 4.5913624732362006e-05,
      "loss": 1.8867,
      "step": 62600
    },
    {
      "epoch": 4.911483628387906,
      "grad_norm": 6.63082218170166,
      "learning_rate": 4.590709697634342e-05,
      "loss": 1.8633,
      "step": 62700
    },
    {
      "epoch": 4.919316935610214,
      "grad_norm": 9.140241622924805,
      "learning_rate": 4.5900569220324824e-05,
      "loss": 1.9133,
      "step": 62800
    },
    {
      "epoch": 4.927150242832524,
      "grad_norm": 5.8448262214660645,
      "learning_rate": 4.589404146430623e-05,
      "loss": 2.0202,
      "step": 62900
    },
    {
      "epoch": 4.934983550054833,
      "grad_norm": 6.5186638832092285,
      "learning_rate": 4.588751370828764e-05,
      "loss": 1.9557,
      "step": 63000
    },
    {
      "epoch": 4.942816857277142,
      "grad_norm": 5.960698127746582,
      "learning_rate": 4.588098595226905e-05,
      "loss": 1.8694,
      "step": 63100
    },
    {
      "epoch": 4.950650164499452,
      "grad_norm": 4.942166328430176,
      "learning_rate": 4.587445819625046e-05,
      "loss": 1.9531,
      "step": 63200
    },
    {
      "epoch": 4.958483471721761,
      "grad_norm": 5.316283226013184,
      "learning_rate": 4.5867930440231866e-05,
      "loss": 1.9169,
      "step": 63300
    },
    {
      "epoch": 4.96631677894407,
      "grad_norm": 4.604250907897949,
      "learning_rate": 4.586140268421328e-05,
      "loss": 1.9192,
      "step": 63400
    },
    {
      "epoch": 4.97415008616638,
      "grad_norm": 6.887375831604004,
      "learning_rate": 4.5854874928194685e-05,
      "loss": 1.8348,
      "step": 63500
    },
    {
      "epoch": 4.981983393388688,
      "grad_norm": 4.117638111114502,
      "learning_rate": 4.584834717217609e-05,
      "loss": 1.897,
      "step": 63600
    },
    {
      "epoch": 4.989816700610998,
      "grad_norm": 5.269729137420654,
      "learning_rate": 4.58418194161575e-05,
      "loss": 1.904,
      "step": 63700
    },
    {
      "epoch": 4.9976500078333075,
      "grad_norm": 6.229546546936035,
      "learning_rate": 4.5835291660138915e-05,
      "loss": 2.0407,
      "step": 63800
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.887791633605957,
      "eval_runtime": 1.5669,
      "eval_samples_per_second": 428.872,
      "eval_steps_per_second": 428.872,
      "step": 63830
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.702575922012329,
      "eval_runtime": 30.2363,
      "eval_samples_per_second": 422.207,
      "eval_steps_per_second": 422.207,
      "step": 63830
    },
    {
      "epoch": 5.005483315055616,
      "grad_norm": 5.600771427154541,
      "learning_rate": 4.582876390412032e-05,
      "loss": 1.935,
      "step": 63900
    },
    {
      "epoch": 5.013316622277926,
      "grad_norm": 6.443646430969238,
      "learning_rate": 4.5822236148101734e-05,
      "loss": 1.9226,
      "step": 64000
    },
    {
      "epoch": 5.021149929500235,
      "grad_norm": 5.455080032348633,
      "learning_rate": 4.581570839208314e-05,
      "loss": 1.9011,
      "step": 64100
    },
    {
      "epoch": 5.028983236722544,
      "grad_norm": 6.934269905090332,
      "learning_rate": 4.5809180636064545e-05,
      "loss": 1.7965,
      "step": 64200
    },
    {
      "epoch": 5.036816543944854,
      "grad_norm": 5.929367542266846,
      "learning_rate": 4.580265288004596e-05,
      "loss": 1.9052,
      "step": 64300
    },
    {
      "epoch": 5.044649851167163,
      "grad_norm": 6.150493144989014,
      "learning_rate": 4.5796125124027364e-05,
      "loss": 1.8918,
      "step": 64400
    },
    {
      "epoch": 5.052483158389472,
      "grad_norm": 4.854221343994141,
      "learning_rate": 4.5789597368008776e-05,
      "loss": 1.8937,
      "step": 64500
    },
    {
      "epoch": 5.060316465611781,
      "grad_norm": 5.408780097961426,
      "learning_rate": 4.578306961199019e-05,
      "loss": 1.8815,
      "step": 64600
    },
    {
      "epoch": 5.068149772834091,
      "grad_norm": 5.893254280090332,
      "learning_rate": 4.5776541855971595e-05,
      "loss": 1.8779,
      "step": 64700
    },
    {
      "epoch": 5.0759830800564,
      "grad_norm": 5.06292724609375,
      "learning_rate": 4.5770014099953e-05,
      "loss": 1.9188,
      "step": 64800
    },
    {
      "epoch": 5.083816387278709,
      "grad_norm": 7.553474426269531,
      "learning_rate": 4.5763486343934406e-05,
      "loss": 1.9316,
      "step": 64900
    },
    {
      "epoch": 5.091649694501018,
      "grad_norm": 4.3263726234436035,
      "learning_rate": 4.575695858791582e-05,
      "loss": 1.8698,
      "step": 65000
    },
    {
      "epoch": 5.0994830017233275,
      "grad_norm": 7.646966457366943,
      "learning_rate": 4.575043083189723e-05,
      "loss": 1.9506,
      "step": 65100
    },
    {
      "epoch": 5.107316308945637,
      "grad_norm": 5.434436798095703,
      "learning_rate": 4.574390307587864e-05,
      "loss": 1.8808,
      "step": 65200
    },
    {
      "epoch": 5.115149616167946,
      "grad_norm": 4.3540496826171875,
      "learning_rate": 4.573737531986005e-05,
      "loss": 1.9483,
      "step": 65300
    },
    {
      "epoch": 5.122982923390255,
      "grad_norm": 5.354203224182129,
      "learning_rate": 4.5730847563841455e-05,
      "loss": 1.8662,
      "step": 65400
    },
    {
      "epoch": 5.130816230612565,
      "grad_norm": 5.441896915435791,
      "learning_rate": 4.572431980782286e-05,
      "loss": 1.8777,
      "step": 65500
    },
    {
      "epoch": 5.1386495378348735,
      "grad_norm": 6.659382343292236,
      "learning_rate": 4.5717792051804274e-05,
      "loss": 1.8277,
      "step": 65600
    },
    {
      "epoch": 5.146482845057183,
      "grad_norm": 4.921355247497559,
      "learning_rate": 4.5711264295785686e-05,
      "loss": 1.8916,
      "step": 65700
    },
    {
      "epoch": 5.154316152279493,
      "grad_norm": 4.897595405578613,
      "learning_rate": 4.570473653976709e-05,
      "loss": 1.8875,
      "step": 65800
    },
    {
      "epoch": 5.162149459501801,
      "grad_norm": 6.255789756774902,
      "learning_rate": 4.5698208783748504e-05,
      "loss": 1.8527,
      "step": 65900
    },
    {
      "epoch": 5.169982766724111,
      "grad_norm": 6.8086957931518555,
      "learning_rate": 4.569168102772991e-05,
      "loss": 1.9098,
      "step": 66000
    },
    {
      "epoch": 5.1778160739464205,
      "grad_norm": 5.299683570861816,
      "learning_rate": 4.5685153271711316e-05,
      "loss": 1.9828,
      "step": 66100
    },
    {
      "epoch": 5.185649381168729,
      "grad_norm": 5.1725664138793945,
      "learning_rate": 4.567862551569273e-05,
      "loss": 1.8466,
      "step": 66200
    },
    {
      "epoch": 5.193482688391039,
      "grad_norm": 5.0987019538879395,
      "learning_rate": 4.5672097759674134e-05,
      "loss": 1.8908,
      "step": 66300
    },
    {
      "epoch": 5.201315995613348,
      "grad_norm": 4.919559955596924,
      "learning_rate": 4.566557000365555e-05,
      "loss": 2.0093,
      "step": 66400
    },
    {
      "epoch": 5.209149302835657,
      "grad_norm": 5.747039794921875,
      "learning_rate": 4.565904224763696e-05,
      "loss": 1.8493,
      "step": 66500
    },
    {
      "epoch": 5.216982610057967,
      "grad_norm": 5.48103141784668,
      "learning_rate": 4.5652514491618365e-05,
      "loss": 1.9229,
      "step": 66600
    },
    {
      "epoch": 5.224815917280276,
      "grad_norm": 5.359640598297119,
      "learning_rate": 4.564598673559977e-05,
      "loss": 1.9952,
      "step": 66700
    },
    {
      "epoch": 5.232649224502585,
      "grad_norm": 5.240274906158447,
      "learning_rate": 4.563945897958118e-05,
      "loss": 1.8044,
      "step": 66800
    },
    {
      "epoch": 5.240482531724894,
      "grad_norm": 4.183546543121338,
      "learning_rate": 4.563293122356259e-05,
      "loss": 1.8818,
      "step": 66900
    },
    {
      "epoch": 5.248315838947203,
      "grad_norm": 6.646308422088623,
      "learning_rate": 4.5626403467544e-05,
      "loss": 1.8445,
      "step": 67000
    },
    {
      "epoch": 5.256149146169513,
      "grad_norm": 6.826442718505859,
      "learning_rate": 4.561987571152541e-05,
      "loss": 1.86,
      "step": 67100
    },
    {
      "epoch": 5.263982453391822,
      "grad_norm": 6.704695701599121,
      "learning_rate": 4.561334795550682e-05,
      "loss": 1.8853,
      "step": 67200
    },
    {
      "epoch": 5.271815760614131,
      "grad_norm": 5.933998107910156,
      "learning_rate": 4.5606820199488226e-05,
      "loss": 1.9344,
      "step": 67300
    },
    {
      "epoch": 5.2796490678364405,
      "grad_norm": 7.057545185089111,
      "learning_rate": 4.560029244346963e-05,
      "loss": 1.9056,
      "step": 67400
    },
    {
      "epoch": 5.28748237505875,
      "grad_norm": 5.401801586151123,
      "learning_rate": 4.5593764687451044e-05,
      "loss": 1.8086,
      "step": 67500
    },
    {
      "epoch": 5.295315682281059,
      "grad_norm": 4.966821670532227,
      "learning_rate": 4.558723693143245e-05,
      "loss": 1.9055,
      "step": 67600
    },
    {
      "epoch": 5.303148989503368,
      "grad_norm": 3.606818675994873,
      "learning_rate": 4.558070917541386e-05,
      "loss": 1.8938,
      "step": 67700
    },
    {
      "epoch": 5.310982296725678,
      "grad_norm": 8.620888710021973,
      "learning_rate": 4.5574181419395275e-05,
      "loss": 1.9357,
      "step": 67800
    },
    {
      "epoch": 5.318815603947987,
      "grad_norm": 7.306026458740234,
      "learning_rate": 4.556765366337668e-05,
      "loss": 1.8952,
      "step": 67900
    },
    {
      "epoch": 5.326648911170296,
      "grad_norm": 7.648766040802002,
      "learning_rate": 4.5561125907358087e-05,
      "loss": 1.9453,
      "step": 68000
    },
    {
      "epoch": 5.334482218392606,
      "grad_norm": 4.501398086547852,
      "learning_rate": 4.55545981513395e-05,
      "loss": 1.8233,
      "step": 68100
    },
    {
      "epoch": 5.342315525614914,
      "grad_norm": 7.2727179527282715,
      "learning_rate": 4.5548070395320905e-05,
      "loss": 1.9893,
      "step": 68200
    },
    {
      "epoch": 5.350148832837224,
      "grad_norm": 5.732663631439209,
      "learning_rate": 4.554154263930232e-05,
      "loss": 1.9724,
      "step": 68300
    },
    {
      "epoch": 5.3579821400595335,
      "grad_norm": 5.835651874542236,
      "learning_rate": 4.553501488328372e-05,
      "loss": 1.8474,
      "step": 68400
    },
    {
      "epoch": 5.365815447281842,
      "grad_norm": 7.587038040161133,
      "learning_rate": 4.5528487127265136e-05,
      "loss": 1.9414,
      "step": 68500
    },
    {
      "epoch": 5.373648754504152,
      "grad_norm": 5.598766326904297,
      "learning_rate": 4.552195937124654e-05,
      "loss": 1.9637,
      "step": 68600
    },
    {
      "epoch": 5.3814820617264605,
      "grad_norm": 5.71624755859375,
      "learning_rate": 4.551543161522795e-05,
      "loss": 1.8619,
      "step": 68700
    },
    {
      "epoch": 5.38931536894877,
      "grad_norm": 7.4430460929870605,
      "learning_rate": 4.550890385920936e-05,
      "loss": 1.7922,
      "step": 68800
    },
    {
      "epoch": 5.39714867617108,
      "grad_norm": 6.891990661621094,
      "learning_rate": 4.550237610319077e-05,
      "loss": 1.9693,
      "step": 68900
    },
    {
      "epoch": 5.404981983393388,
      "grad_norm": 5.690829277038574,
      "learning_rate": 4.549584834717218e-05,
      "loss": 1.8825,
      "step": 69000
    },
    {
      "epoch": 5.412815290615698,
      "grad_norm": 6.564517498016357,
      "learning_rate": 4.548932059115359e-05,
      "loss": 1.9255,
      "step": 69100
    },
    {
      "epoch": 5.4206485978380075,
      "grad_norm": 4.142265796661377,
      "learning_rate": 4.5482792835134996e-05,
      "loss": 1.8044,
      "step": 69200
    },
    {
      "epoch": 5.428481905060316,
      "grad_norm": 6.037435531616211,
      "learning_rate": 4.54762650791164e-05,
      "loss": 1.9674,
      "step": 69300
    },
    {
      "epoch": 5.436315212282626,
      "grad_norm": 7.010925769805908,
      "learning_rate": 4.5469737323097815e-05,
      "loss": 1.8722,
      "step": 69400
    },
    {
      "epoch": 5.444148519504935,
      "grad_norm": 5.636247158050537,
      "learning_rate": 4.546320956707922e-05,
      "loss": 1.9114,
      "step": 69500
    },
    {
      "epoch": 5.451981826727244,
      "grad_norm": 4.039790153503418,
      "learning_rate": 4.545668181106063e-05,
      "loss": 1.8786,
      "step": 69600
    },
    {
      "epoch": 5.4598151339495535,
      "grad_norm": 6.404234886169434,
      "learning_rate": 4.5450154055042046e-05,
      "loss": 1.8563,
      "step": 69700
    },
    {
      "epoch": 5.467648441171863,
      "grad_norm": 5.398358345031738,
      "learning_rate": 4.544362629902345e-05,
      "loss": 1.9283,
      "step": 69800
    },
    {
      "epoch": 5.475481748394172,
      "grad_norm": 5.791173934936523,
      "learning_rate": 4.543709854300486e-05,
      "loss": 1.8793,
      "step": 69900
    },
    {
      "epoch": 5.483315055616481,
      "grad_norm": 6.158846855163574,
      "learning_rate": 4.543057078698626e-05,
      "loss": 1.7924,
      "step": 70000
    },
    {
      "epoch": 5.491148362838791,
      "grad_norm": 5.4583845138549805,
      "learning_rate": 4.5424043030967675e-05,
      "loss": 1.8845,
      "step": 70100
    },
    {
      "epoch": 5.4989816700611,
      "grad_norm": 6.21219539642334,
      "learning_rate": 4.541751527494909e-05,
      "loss": 1.8887,
      "step": 70200
    },
    {
      "epoch": 5.506814977283409,
      "grad_norm": 8.372376441955566,
      "learning_rate": 4.5410987518930494e-05,
      "loss": 1.8139,
      "step": 70300
    },
    {
      "epoch": 5.514648284505718,
      "grad_norm": 7.00941276550293,
      "learning_rate": 4.5404459762911906e-05,
      "loss": 1.8675,
      "step": 70400
    },
    {
      "epoch": 5.522481591728027,
      "grad_norm": 4.742746829986572,
      "learning_rate": 4.539793200689331e-05,
      "loss": 1.8918,
      "step": 70500
    },
    {
      "epoch": 5.530314898950337,
      "grad_norm": 6.677756309509277,
      "learning_rate": 4.539140425087472e-05,
      "loss": 1.875,
      "step": 70600
    },
    {
      "epoch": 5.538148206172647,
      "grad_norm": 7.896949291229248,
      "learning_rate": 4.538487649485613e-05,
      "loss": 1.8136,
      "step": 70700
    },
    {
      "epoch": 5.545981513394955,
      "grad_norm": 4.754403591156006,
      "learning_rate": 4.5378348738837536e-05,
      "loss": 1.8955,
      "step": 70800
    },
    {
      "epoch": 5.553814820617265,
      "grad_norm": 5.432519435882568,
      "learning_rate": 4.537182098281895e-05,
      "loss": 2.0187,
      "step": 70900
    },
    {
      "epoch": 5.5616481278395735,
      "grad_norm": 4.657122611999512,
      "learning_rate": 4.536529322680036e-05,
      "loss": 1.946,
      "step": 71000
    },
    {
      "epoch": 5.569481435061883,
      "grad_norm": 4.324944972991943,
      "learning_rate": 4.535876547078177e-05,
      "loss": 1.8696,
      "step": 71100
    },
    {
      "epoch": 5.577314742284193,
      "grad_norm": 7.061258316040039,
      "learning_rate": 4.535223771476317e-05,
      "loss": 1.8311,
      "step": 71200
    },
    {
      "epoch": 5.585148049506501,
      "grad_norm": 5.45520544052124,
      "learning_rate": 4.5345709958744585e-05,
      "loss": 1.8566,
      "step": 71300
    },
    {
      "epoch": 5.592981356728811,
      "grad_norm": 4.767122745513916,
      "learning_rate": 4.533918220272599e-05,
      "loss": 1.8435,
      "step": 71400
    },
    {
      "epoch": 5.6008146639511205,
      "grad_norm": 5.340298652648926,
      "learning_rate": 4.5332654446707404e-05,
      "loss": 1.9563,
      "step": 71500
    },
    {
      "epoch": 5.608647971173429,
      "grad_norm": 5.625802516937256,
      "learning_rate": 4.532612669068881e-05,
      "loss": 1.9684,
      "step": 71600
    },
    {
      "epoch": 5.616481278395739,
      "grad_norm": 7.5631608963012695,
      "learning_rate": 4.531959893467022e-05,
      "loss": 1.8776,
      "step": 71700
    },
    {
      "epoch": 5.624314585618048,
      "grad_norm": 5.935674667358398,
      "learning_rate": 4.531307117865163e-05,
      "loss": 1.9426,
      "step": 71800
    },
    {
      "epoch": 5.632147892840357,
      "grad_norm": 6.5003814697265625,
      "learning_rate": 4.5306543422633033e-05,
      "loss": 1.8498,
      "step": 71900
    },
    {
      "epoch": 5.639981200062667,
      "grad_norm": 7.131296157836914,
      "learning_rate": 4.5300015666614446e-05,
      "loss": 1.8783,
      "step": 72000
    },
    {
      "epoch": 5.647814507284975,
      "grad_norm": 8.901071548461914,
      "learning_rate": 4.529348791059586e-05,
      "loss": 1.9159,
      "step": 72100
    },
    {
      "epoch": 5.655647814507285,
      "grad_norm": 8.6394681930542,
      "learning_rate": 4.5286960154577264e-05,
      "loss": 1.9344,
      "step": 72200
    },
    {
      "epoch": 5.663481121729594,
      "grad_norm": 5.705524444580078,
      "learning_rate": 4.528043239855868e-05,
      "loss": 1.8859,
      "step": 72300
    },
    {
      "epoch": 5.671314428951904,
      "grad_norm": 7.2422966957092285,
      "learning_rate": 4.527390464254008e-05,
      "loss": 1.8975,
      "step": 72400
    },
    {
      "epoch": 5.679147736174213,
      "grad_norm": 4.9586334228515625,
      "learning_rate": 4.526737688652149e-05,
      "loss": 1.8525,
      "step": 72500
    },
    {
      "epoch": 5.686981043396522,
      "grad_norm": 5.4080424308776855,
      "learning_rate": 4.52608491305029e-05,
      "loss": 1.8563,
      "step": 72600
    },
    {
      "epoch": 5.694814350618831,
      "grad_norm": 7.263432025909424,
      "learning_rate": 4.525432137448431e-05,
      "loss": 1.8556,
      "step": 72700
    },
    {
      "epoch": 5.7026476578411405,
      "grad_norm": 6.674436092376709,
      "learning_rate": 4.524779361846572e-05,
      "loss": 1.9137,
      "step": 72800
    },
    {
      "epoch": 5.71048096506345,
      "grad_norm": 5.618369102478027,
      "learning_rate": 4.524126586244713e-05,
      "loss": 1.9748,
      "step": 72900
    },
    {
      "epoch": 5.718314272285759,
      "grad_norm": 5.5680131912231445,
      "learning_rate": 4.523473810642854e-05,
      "loss": 1.8776,
      "step": 73000
    },
    {
      "epoch": 5.726147579508068,
      "grad_norm": 4.932480812072754,
      "learning_rate": 4.522821035040994e-05,
      "loss": 1.9374,
      "step": 73100
    },
    {
      "epoch": 5.733980886730378,
      "grad_norm": 6.050424575805664,
      "learning_rate": 4.5221682594391356e-05,
      "loss": 1.8749,
      "step": 73200
    },
    {
      "epoch": 5.7418141939526866,
      "grad_norm": 5.044398784637451,
      "learning_rate": 4.521515483837276e-05,
      "loss": 1.9595,
      "step": 73300
    },
    {
      "epoch": 5.749647501174996,
      "grad_norm": 6.321298122406006,
      "learning_rate": 4.5208627082354174e-05,
      "loss": 1.8997,
      "step": 73400
    },
    {
      "epoch": 5.757480808397306,
      "grad_norm": 6.952830791473389,
      "learning_rate": 4.520209932633558e-05,
      "loss": 1.8644,
      "step": 73500
    },
    {
      "epoch": 5.765314115619614,
      "grad_norm": 6.654380798339844,
      "learning_rate": 4.519557157031699e-05,
      "loss": 1.8438,
      "step": 73600
    },
    {
      "epoch": 5.773147422841924,
      "grad_norm": 3.6539366245269775,
      "learning_rate": 4.51890438142984e-05,
      "loss": 1.9061,
      "step": 73700
    },
    {
      "epoch": 5.780980730064233,
      "grad_norm": 5.1861724853515625,
      "learning_rate": 4.5182516058279804e-05,
      "loss": 1.9048,
      "step": 73800
    },
    {
      "epoch": 5.788814037286542,
      "grad_norm": 5.063031196594238,
      "learning_rate": 4.5175988302261217e-05,
      "loss": 1.8967,
      "step": 73900
    },
    {
      "epoch": 5.796647344508852,
      "grad_norm": 8.382394790649414,
      "learning_rate": 4.516946054624262e-05,
      "loss": 1.8584,
      "step": 74000
    },
    {
      "epoch": 5.804480651731161,
      "grad_norm": 5.049184799194336,
      "learning_rate": 4.5162932790224035e-05,
      "loss": 1.9625,
      "step": 74100
    },
    {
      "epoch": 5.81231395895347,
      "grad_norm": 4.87591552734375,
      "learning_rate": 4.515640503420545e-05,
      "loss": 1.7828,
      "step": 74200
    },
    {
      "epoch": 5.82014726617578,
      "grad_norm": 8.377922058105469,
      "learning_rate": 4.514987727818685e-05,
      "loss": 1.8788,
      "step": 74300
    },
    {
      "epoch": 5.827980573398088,
      "grad_norm": 4.499776840209961,
      "learning_rate": 4.514334952216826e-05,
      "loss": 1.8631,
      "step": 74400
    },
    {
      "epoch": 5.835813880620398,
      "grad_norm": 4.0130391120910645,
      "learning_rate": 4.513682176614967e-05,
      "loss": 1.9487,
      "step": 74500
    },
    {
      "epoch": 5.843647187842707,
      "grad_norm": 6.10228967666626,
      "learning_rate": 4.513029401013108e-05,
      "loss": 1.8764,
      "step": 74600
    },
    {
      "epoch": 5.851480495065016,
      "grad_norm": 8.328259468078613,
      "learning_rate": 4.512376625411249e-05,
      "loss": 1.9531,
      "step": 74700
    },
    {
      "epoch": 5.859313802287326,
      "grad_norm": 6.023227691650391,
      "learning_rate": 4.51172384980939e-05,
      "loss": 1.9075,
      "step": 74800
    },
    {
      "epoch": 5.867147109509635,
      "grad_norm": 7.19274377822876,
      "learning_rate": 4.511071074207531e-05,
      "loss": 2.0023,
      "step": 74900
    },
    {
      "epoch": 5.874980416731944,
      "grad_norm": 7.0920610427856445,
      "learning_rate": 4.5104182986056714e-05,
      "loss": 1.9575,
      "step": 75000
    },
    {
      "epoch": 5.8828137239542535,
      "grad_norm": 5.015237331390381,
      "learning_rate": 4.509765523003812e-05,
      "loss": 1.961,
      "step": 75100
    },
    {
      "epoch": 5.890647031176563,
      "grad_norm": 4.915661811828613,
      "learning_rate": 4.509112747401953e-05,
      "loss": 1.899,
      "step": 75200
    },
    {
      "epoch": 5.898480338398872,
      "grad_norm": 5.353658676147461,
      "learning_rate": 4.5084599718000945e-05,
      "loss": 1.9284,
      "step": 75300
    },
    {
      "epoch": 5.906313645621181,
      "grad_norm": 5.407726287841797,
      "learning_rate": 4.507807196198235e-05,
      "loss": 1.8413,
      "step": 75400
    },
    {
      "epoch": 5.914146952843491,
      "grad_norm": 4.849395275115967,
      "learning_rate": 4.507154420596376e-05,
      "loss": 2.0297,
      "step": 75500
    },
    {
      "epoch": 5.9219802600658,
      "grad_norm": 5.436452865600586,
      "learning_rate": 4.506501644994517e-05,
      "loss": 1.9062,
      "step": 75600
    },
    {
      "epoch": 5.929813567288109,
      "grad_norm": 4.789425849914551,
      "learning_rate": 4.5058488693926575e-05,
      "loss": 1.8785,
      "step": 75700
    },
    {
      "epoch": 5.937646874510419,
      "grad_norm": 5.453388214111328,
      "learning_rate": 4.505196093790799e-05,
      "loss": 1.9378,
      "step": 75800
    },
    {
      "epoch": 5.945480181732727,
      "grad_norm": 4.318851470947266,
      "learning_rate": 4.504543318188939e-05,
      "loss": 1.9641,
      "step": 75900
    },
    {
      "epoch": 5.953313488955037,
      "grad_norm": 4.906707763671875,
      "learning_rate": 4.5038905425870805e-05,
      "loss": 1.9586,
      "step": 76000
    },
    {
      "epoch": 5.961146796177346,
      "grad_norm": 6.004552364349365,
      "learning_rate": 4.503237766985222e-05,
      "loss": 1.9885,
      "step": 76100
    },
    {
      "epoch": 5.968980103399655,
      "grad_norm": 5.327851295471191,
      "learning_rate": 4.5025849913833624e-05,
      "loss": 1.8806,
      "step": 76200
    },
    {
      "epoch": 5.976813410621965,
      "grad_norm": 6.537343502044678,
      "learning_rate": 4.501932215781503e-05,
      "loss": 1.8031,
      "step": 76300
    },
    {
      "epoch": 5.9846467178442735,
      "grad_norm": 4.867403030395508,
      "learning_rate": 4.501279440179644e-05,
      "loss": 1.9032,
      "step": 76400
    },
    {
      "epoch": 5.992480025066583,
      "grad_norm": 5.894906044006348,
      "learning_rate": 4.500626664577785e-05,
      "loss": 1.8303,
      "step": 76500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.8637175559997559,
      "eval_runtime": 1.6008,
      "eval_samples_per_second": 419.803,
      "eval_steps_per_second": 419.803,
      "step": 76596
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.671884536743164,
      "eval_runtime": 29.292,
      "eval_samples_per_second": 435.818,
      "eval_steps_per_second": 435.818,
      "step": 76596
    },
    {
      "epoch": 6.000313332288893,
      "grad_norm": 4.6981682777404785,
      "learning_rate": 4.499973888975926e-05,
      "loss": 2.021,
      "step": 76600
    },
    {
      "epoch": 6.008146639511201,
      "grad_norm": 4.996584415435791,
      "learning_rate": 4.4993211133740666e-05,
      "loss": 1.8909,
      "step": 76700
    },
    {
      "epoch": 6.015979946733511,
      "grad_norm": 6.48821496963501,
      "learning_rate": 4.498668337772208e-05,
      "loss": 1.8476,
      "step": 76800
    },
    {
      "epoch": 6.0238132539558205,
      "grad_norm": 5.963455677032471,
      "learning_rate": 4.4980155621703485e-05,
      "loss": 1.9769,
      "step": 76900
    },
    {
      "epoch": 6.031646561178129,
      "grad_norm": 5.7415618896484375,
      "learning_rate": 4.497362786568489e-05,
      "loss": 1.8617,
      "step": 77000
    },
    {
      "epoch": 6.039479868400439,
      "grad_norm": 6.2971062660217285,
      "learning_rate": 4.49671001096663e-05,
      "loss": 1.8899,
      "step": 77100
    },
    {
      "epoch": 6.047313175622748,
      "grad_norm": 5.384761810302734,
      "learning_rate": 4.496057235364771e-05,
      "loss": 1.9077,
      "step": 77200
    },
    {
      "epoch": 6.055146482845057,
      "grad_norm": 4.920450210571289,
      "learning_rate": 4.495404459762912e-05,
      "loss": 1.8299,
      "step": 77300
    },
    {
      "epoch": 6.0629797900673665,
      "grad_norm": 5.452466011047363,
      "learning_rate": 4.4947516841610534e-05,
      "loss": 1.831,
      "step": 77400
    },
    {
      "epoch": 6.070813097289676,
      "grad_norm": 4.944118976593018,
      "learning_rate": 4.494098908559194e-05,
      "loss": 1.8727,
      "step": 77500
    },
    {
      "epoch": 6.078646404511985,
      "grad_norm": 6.0324273109436035,
      "learning_rate": 4.4934461329573345e-05,
      "loss": 1.8748,
      "step": 77600
    },
    {
      "epoch": 6.086479711734294,
      "grad_norm": 5.827927112579346,
      "learning_rate": 4.492793357355476e-05,
      "loss": 1.8066,
      "step": 77700
    },
    {
      "epoch": 6.094313018956603,
      "grad_norm": 7.283755779266357,
      "learning_rate": 4.4921405817536164e-05,
      "loss": 1.8996,
      "step": 77800
    },
    {
      "epoch": 6.102146326178913,
      "grad_norm": 4.935957431793213,
      "learning_rate": 4.4914878061517576e-05,
      "loss": 1.8754,
      "step": 77900
    },
    {
      "epoch": 6.109979633401222,
      "grad_norm": 6.064892292022705,
      "learning_rate": 4.490835030549899e-05,
      "loss": 1.8693,
      "step": 78000
    },
    {
      "epoch": 6.117812940623531,
      "grad_norm": 6.227753162384033,
      "learning_rate": 4.4901822549480394e-05,
      "loss": 1.8722,
      "step": 78100
    },
    {
      "epoch": 6.12564624784584,
      "grad_norm": 6.025210380554199,
      "learning_rate": 4.48952947934618e-05,
      "loss": 1.886,
      "step": 78200
    },
    {
      "epoch": 6.13347955506815,
      "grad_norm": 4.52567720413208,
      "learning_rate": 4.488876703744321e-05,
      "loss": 1.8503,
      "step": 78300
    },
    {
      "epoch": 6.141312862290459,
      "grad_norm": 6.126389980316162,
      "learning_rate": 4.488223928142462e-05,
      "loss": 1.8663,
      "step": 78400
    },
    {
      "epoch": 6.149146169512768,
      "grad_norm": 6.753966808319092,
      "learning_rate": 4.487571152540603e-05,
      "loss": 1.8685,
      "step": 78500
    },
    {
      "epoch": 6.156979476735078,
      "grad_norm": 6.60678768157959,
      "learning_rate": 4.486918376938744e-05,
      "loss": 1.9705,
      "step": 78600
    },
    {
      "epoch": 6.1648127839573865,
      "grad_norm": 5.0402512550354,
      "learning_rate": 4.486265601336885e-05,
      "loss": 1.8965,
      "step": 78700
    },
    {
      "epoch": 6.172646091179696,
      "grad_norm": 4.840661525726318,
      "learning_rate": 4.4856128257350255e-05,
      "loss": 1.8473,
      "step": 78800
    },
    {
      "epoch": 6.180479398402006,
      "grad_norm": 7.3352813720703125,
      "learning_rate": 4.484960050133166e-05,
      "loss": 1.8799,
      "step": 78900
    },
    {
      "epoch": 6.188312705624314,
      "grad_norm": 5.125923156738281,
      "learning_rate": 4.4843072745313073e-05,
      "loss": 1.9095,
      "step": 79000
    },
    {
      "epoch": 6.196146012846624,
      "grad_norm": 7.110525608062744,
      "learning_rate": 4.483654498929448e-05,
      "loss": 1.926,
      "step": 79100
    },
    {
      "epoch": 6.2039793200689335,
      "grad_norm": 5.895996570587158,
      "learning_rate": 4.483001723327589e-05,
      "loss": 1.8947,
      "step": 79200
    },
    {
      "epoch": 6.211812627291242,
      "grad_norm": 5.219064235687256,
      "learning_rate": 4.4823489477257304e-05,
      "loss": 1.9447,
      "step": 79300
    },
    {
      "epoch": 6.219645934513552,
      "grad_norm": 5.121913909912109,
      "learning_rate": 4.481696172123871e-05,
      "loss": 1.8544,
      "step": 79400
    },
    {
      "epoch": 6.22747924173586,
      "grad_norm": 5.9367828369140625,
      "learning_rate": 4.4810433965220116e-05,
      "loss": 1.8796,
      "step": 79500
    },
    {
      "epoch": 6.23531254895817,
      "grad_norm": 6.27785062789917,
      "learning_rate": 4.480390620920153e-05,
      "loss": 1.8006,
      "step": 79600
    },
    {
      "epoch": 6.24314585618048,
      "grad_norm": 6.6412553787231445,
      "learning_rate": 4.4797378453182934e-05,
      "loss": 1.848,
      "step": 79700
    },
    {
      "epoch": 6.250979163402788,
      "grad_norm": 5.343756675720215,
      "learning_rate": 4.479085069716435e-05,
      "loss": 1.7406,
      "step": 79800
    },
    {
      "epoch": 6.258812470625098,
      "grad_norm": 4.685309410095215,
      "learning_rate": 4.478432294114576e-05,
      "loss": 1.8171,
      "step": 79900
    },
    {
      "epoch": 6.266645777847407,
      "grad_norm": 6.208586692810059,
      "learning_rate": 4.4777795185127165e-05,
      "loss": 1.8954,
      "step": 80000
    },
    {
      "epoch": 6.274479085069716,
      "grad_norm": 5.555669784545898,
      "learning_rate": 4.477126742910857e-05,
      "loss": 1.8053,
      "step": 80100
    },
    {
      "epoch": 6.282312392292026,
      "grad_norm": 5.601414680480957,
      "learning_rate": 4.4764739673089977e-05,
      "loss": 1.8984,
      "step": 80200
    },
    {
      "epoch": 6.290145699514335,
      "grad_norm": 4.984330177307129,
      "learning_rate": 4.475821191707139e-05,
      "loss": 1.902,
      "step": 80300
    },
    {
      "epoch": 6.297979006736644,
      "grad_norm": 5.439050674438477,
      "learning_rate": 4.4751684161052795e-05,
      "loss": 1.9358,
      "step": 80400
    },
    {
      "epoch": 6.3058123139589535,
      "grad_norm": 7.090550422668457,
      "learning_rate": 4.474515640503421e-05,
      "loss": 1.8137,
      "step": 80500
    },
    {
      "epoch": 6.313645621181263,
      "grad_norm": 5.519496917724609,
      "learning_rate": 4.473862864901562e-05,
      "loss": 1.8686,
      "step": 80600
    },
    {
      "epoch": 6.321478928403572,
      "grad_norm": 5.085290431976318,
      "learning_rate": 4.4732100892997026e-05,
      "loss": 1.9634,
      "step": 80700
    },
    {
      "epoch": 6.329312235625881,
      "grad_norm": 6.263561725616455,
      "learning_rate": 4.472557313697843e-05,
      "loss": 1.9113,
      "step": 80800
    },
    {
      "epoch": 6.337145542848191,
      "grad_norm": 5.808189392089844,
      "learning_rate": 4.4719045380959844e-05,
      "loss": 1.7946,
      "step": 80900
    },
    {
      "epoch": 6.3449788500704996,
      "grad_norm": 6.062119483947754,
      "learning_rate": 4.471251762494125e-05,
      "loss": 1.9163,
      "step": 81000
    },
    {
      "epoch": 6.352812157292809,
      "grad_norm": 6.078279495239258,
      "learning_rate": 4.470598986892266e-05,
      "loss": 1.8413,
      "step": 81100
    },
    {
      "epoch": 6.360645464515119,
      "grad_norm": 5.323707103729248,
      "learning_rate": 4.4699462112904075e-05,
      "loss": 1.9095,
      "step": 81200
    },
    {
      "epoch": 6.368478771737427,
      "grad_norm": 5.163199424743652,
      "learning_rate": 4.469293435688548e-05,
      "loss": 1.9101,
      "step": 81300
    },
    {
      "epoch": 6.376312078959737,
      "grad_norm": 6.206531047821045,
      "learning_rate": 4.4686406600866886e-05,
      "loss": 1.8056,
      "step": 81400
    },
    {
      "epoch": 6.3841453861820465,
      "grad_norm": 4.390842437744141,
      "learning_rate": 4.46798788448483e-05,
      "loss": 1.7967,
      "step": 81500
    },
    {
      "epoch": 6.391978693404355,
      "grad_norm": 6.6760077476501465,
      "learning_rate": 4.4673351088829705e-05,
      "loss": 1.8669,
      "step": 81600
    },
    {
      "epoch": 6.399812000626665,
      "grad_norm": 6.280661106109619,
      "learning_rate": 4.466682333281112e-05,
      "loss": 1.8415,
      "step": 81700
    },
    {
      "epoch": 6.4076453078489735,
      "grad_norm": 6.198777675628662,
      "learning_rate": 4.466029557679252e-05,
      "loss": 1.8525,
      "step": 81800
    },
    {
      "epoch": 6.415478615071283,
      "grad_norm": 4.922205924987793,
      "learning_rate": 4.4653767820773936e-05,
      "loss": 1.8992,
      "step": 81900
    },
    {
      "epoch": 6.423311922293593,
      "grad_norm": 5.59372091293335,
      "learning_rate": 4.464724006475534e-05,
      "loss": 1.8593,
      "step": 82000
    },
    {
      "epoch": 6.431145229515901,
      "grad_norm": 5.1591267585754395,
      "learning_rate": 4.464071230873675e-05,
      "loss": 1.8505,
      "step": 82100
    },
    {
      "epoch": 6.438978536738211,
      "grad_norm": 6.018515110015869,
      "learning_rate": 4.463418455271816e-05,
      "loss": 1.8956,
      "step": 82200
    },
    {
      "epoch": 6.44681184396052,
      "grad_norm": 5.70229434967041,
      "learning_rate": 4.4627656796699565e-05,
      "loss": 1.8439,
      "step": 82300
    },
    {
      "epoch": 6.454645151182829,
      "grad_norm": 6.891598701477051,
      "learning_rate": 4.462112904068098e-05,
      "loss": 1.8471,
      "step": 82400
    },
    {
      "epoch": 6.462478458405139,
      "grad_norm": 6.290090084075928,
      "learning_rate": 4.461460128466239e-05,
      "loss": 1.8341,
      "step": 82500
    },
    {
      "epoch": 6.470311765627448,
      "grad_norm": 4.899143218994141,
      "learning_rate": 4.4608073528643796e-05,
      "loss": 1.844,
      "step": 82600
    },
    {
      "epoch": 6.478145072849757,
      "grad_norm": 4.735135555267334,
      "learning_rate": 4.46015457726252e-05,
      "loss": 1.9847,
      "step": 82700
    },
    {
      "epoch": 6.4859783800720665,
      "grad_norm": 6.045490741729736,
      "learning_rate": 4.4595018016606615e-05,
      "loss": 1.9247,
      "step": 82800
    },
    {
      "epoch": 6.493811687294376,
      "grad_norm": 4.586584568023682,
      "learning_rate": 4.458849026058802e-05,
      "loss": 1.9812,
      "step": 82900
    },
    {
      "epoch": 6.501644994516685,
      "grad_norm": 4.301980018615723,
      "learning_rate": 4.458196250456943e-05,
      "loss": 1.8984,
      "step": 83000
    },
    {
      "epoch": 6.509478301738994,
      "grad_norm": 7.8770904541015625,
      "learning_rate": 4.4575434748550845e-05,
      "loss": 1.91,
      "step": 83100
    },
    {
      "epoch": 6.517311608961304,
      "grad_norm": 5.774523735046387,
      "learning_rate": 4.456890699253225e-05,
      "loss": 1.8607,
      "step": 83200
    },
    {
      "epoch": 6.525144916183613,
      "grad_norm": 7.388970375061035,
      "learning_rate": 4.456237923651366e-05,
      "loss": 1.7967,
      "step": 83300
    },
    {
      "epoch": 6.532978223405922,
      "grad_norm": 5.7206854820251465,
      "learning_rate": 4.455585148049506e-05,
      "loss": 1.8737,
      "step": 83400
    },
    {
      "epoch": 6.540811530628231,
      "grad_norm": 6.354691028594971,
      "learning_rate": 4.4549323724476475e-05,
      "loss": 1.7646,
      "step": 83500
    },
    {
      "epoch": 6.54864483785054,
      "grad_norm": 5.123215198516846,
      "learning_rate": 4.454279596845788e-05,
      "loss": 1.8588,
      "step": 83600
    },
    {
      "epoch": 6.55647814507285,
      "grad_norm": 5.507336139678955,
      "learning_rate": 4.4536268212439294e-05,
      "loss": 1.7943,
      "step": 83700
    },
    {
      "epoch": 6.564311452295159,
      "grad_norm": 6.358090877532959,
      "learning_rate": 4.4529740456420706e-05,
      "loss": 1.8464,
      "step": 83800
    },
    {
      "epoch": 6.572144759517468,
      "grad_norm": 5.2022552490234375,
      "learning_rate": 4.452321270040211e-05,
      "loss": 1.8775,
      "step": 83900
    },
    {
      "epoch": 6.579978066739778,
      "grad_norm": 7.506966590881348,
      "learning_rate": 4.451668494438352e-05,
      "loss": 1.8324,
      "step": 84000
    },
    {
      "epoch": 6.5878113739620865,
      "grad_norm": 6.4472975730896,
      "learning_rate": 4.451015718836493e-05,
      "loss": 1.8913,
      "step": 84100
    },
    {
      "epoch": 6.595644681184396,
      "grad_norm": 8.17690658569336,
      "learning_rate": 4.4503629432346336e-05,
      "loss": 1.8236,
      "step": 84200
    },
    {
      "epoch": 6.603477988406706,
      "grad_norm": 5.309209823608398,
      "learning_rate": 4.449710167632775e-05,
      "loss": 1.7984,
      "step": 84300
    },
    {
      "epoch": 6.611311295629014,
      "grad_norm": 5.527920246124268,
      "learning_rate": 4.449057392030916e-05,
      "loss": 1.8901,
      "step": 84400
    },
    {
      "epoch": 6.619144602851324,
      "grad_norm": 6.9484100341796875,
      "learning_rate": 4.448404616429057e-05,
      "loss": 1.7733,
      "step": 84500
    },
    {
      "epoch": 6.6269779100736335,
      "grad_norm": 5.21791410446167,
      "learning_rate": 4.447751840827197e-05,
      "loss": 1.881,
      "step": 84600
    },
    {
      "epoch": 6.634811217295942,
      "grad_norm": 7.507216930389404,
      "learning_rate": 4.4470990652253385e-05,
      "loss": 1.8601,
      "step": 84700
    },
    {
      "epoch": 6.642644524518252,
      "grad_norm": 6.91985559463501,
      "learning_rate": 4.446446289623479e-05,
      "loss": 1.8747,
      "step": 84800
    },
    {
      "epoch": 6.650477831740561,
      "grad_norm": 8.089137077331543,
      "learning_rate": 4.4457935140216203e-05,
      "loss": 1.8139,
      "step": 84900
    },
    {
      "epoch": 6.65831113896287,
      "grad_norm": 4.91157865524292,
      "learning_rate": 4.4451407384197616e-05,
      "loss": 1.9148,
      "step": 85000
    },
    {
      "epoch": 6.6661444461851795,
      "grad_norm": 5.550230503082275,
      "learning_rate": 4.444487962817902e-05,
      "loss": 1.9574,
      "step": 85100
    },
    {
      "epoch": 6.673977753407488,
      "grad_norm": 4.860698223114014,
      "learning_rate": 4.443835187216043e-05,
      "loss": 1.8816,
      "step": 85200
    },
    {
      "epoch": 6.681811060629798,
      "grad_norm": 5.743381023406982,
      "learning_rate": 4.443182411614183e-05,
      "loss": 1.8804,
      "step": 85300
    },
    {
      "epoch": 6.689644367852107,
      "grad_norm": 6.773608207702637,
      "learning_rate": 4.4425296360123246e-05,
      "loss": 1.8999,
      "step": 85400
    },
    {
      "epoch": 6.697477675074416,
      "grad_norm": 4.79278564453125,
      "learning_rate": 4.441876860410465e-05,
      "loss": 1.8275,
      "step": 85500
    },
    {
      "epoch": 6.705310982296726,
      "grad_norm": 7.077473163604736,
      "learning_rate": 4.4412240848086064e-05,
      "loss": 1.8465,
      "step": 85600
    },
    {
      "epoch": 6.713144289519035,
      "grad_norm": 4.5769171714782715,
      "learning_rate": 4.440571309206748e-05,
      "loss": 1.9029,
      "step": 85700
    },
    {
      "epoch": 6.720977596741344,
      "grad_norm": 5.701836109161377,
      "learning_rate": 4.439918533604888e-05,
      "loss": 1.8985,
      "step": 85800
    },
    {
      "epoch": 6.7288109039636534,
      "grad_norm": 5.02486515045166,
      "learning_rate": 4.439265758003029e-05,
      "loss": 1.8765,
      "step": 85900
    },
    {
      "epoch": 6.736644211185963,
      "grad_norm": 7.100706577301025,
      "learning_rate": 4.43861298240117e-05,
      "loss": 1.9461,
      "step": 86000
    },
    {
      "epoch": 6.744477518408272,
      "grad_norm": 4.313997268676758,
      "learning_rate": 4.4379602067993107e-05,
      "loss": 1.8756,
      "step": 86100
    },
    {
      "epoch": 6.752310825630581,
      "grad_norm": 7.346149921417236,
      "learning_rate": 4.437307431197452e-05,
      "loss": 1.8293,
      "step": 86200
    },
    {
      "epoch": 6.760144132852891,
      "grad_norm": 4.732449054718018,
      "learning_rate": 4.436654655595593e-05,
      "loss": 1.9084,
      "step": 86300
    },
    {
      "epoch": 6.7679774400751995,
      "grad_norm": 6.479621887207031,
      "learning_rate": 4.436001879993734e-05,
      "loss": 1.8648,
      "step": 86400
    },
    {
      "epoch": 6.775810747297509,
      "grad_norm": 5.220284461975098,
      "learning_rate": 4.435349104391874e-05,
      "loss": 1.8716,
      "step": 86500
    },
    {
      "epoch": 6.783644054519819,
      "grad_norm": 4.690433025360107,
      "learning_rate": 4.4346963287900156e-05,
      "loss": 1.9381,
      "step": 86600
    },
    {
      "epoch": 6.791477361742127,
      "grad_norm": 5.617800712585449,
      "learning_rate": 4.434043553188156e-05,
      "loss": 1.941,
      "step": 86700
    },
    {
      "epoch": 6.799310668964437,
      "grad_norm": 5.8689374923706055,
      "learning_rate": 4.433390777586297e-05,
      "loss": 1.851,
      "step": 86800
    },
    {
      "epoch": 6.807143976186746,
      "grad_norm": 6.054765701293945,
      "learning_rate": 4.432738001984438e-05,
      "loss": 1.9751,
      "step": 86900
    },
    {
      "epoch": 6.814977283409055,
      "grad_norm": 5.000360012054443,
      "learning_rate": 4.432085226382579e-05,
      "loss": 1.8608,
      "step": 87000
    },
    {
      "epoch": 6.822810590631365,
      "grad_norm": 5.22677755355835,
      "learning_rate": 4.43143245078072e-05,
      "loss": 1.8535,
      "step": 87100
    },
    {
      "epoch": 6.830643897853674,
      "grad_norm": 5.817208290100098,
      "learning_rate": 4.4307796751788604e-05,
      "loss": 1.8716,
      "step": 87200
    },
    {
      "epoch": 6.838477205075983,
      "grad_norm": 7.099733829498291,
      "learning_rate": 4.4301268995770016e-05,
      "loss": 1.8356,
      "step": 87300
    },
    {
      "epoch": 6.846310512298293,
      "grad_norm": 5.901979446411133,
      "learning_rate": 4.429474123975142e-05,
      "loss": 1.878,
      "step": 87400
    },
    {
      "epoch": 6.854143819520601,
      "grad_norm": 5.227420806884766,
      "learning_rate": 4.4288213483732835e-05,
      "loss": 1.8907,
      "step": 87500
    },
    {
      "epoch": 6.861977126742911,
      "grad_norm": 7.060606002807617,
      "learning_rate": 4.428168572771425e-05,
      "loss": 1.8883,
      "step": 87600
    },
    {
      "epoch": 6.86981043396522,
      "grad_norm": 5.214378356933594,
      "learning_rate": 4.427515797169565e-05,
      "loss": 1.9582,
      "step": 87700
    },
    {
      "epoch": 6.877643741187529,
      "grad_norm": 5.846185207366943,
      "learning_rate": 4.426863021567706e-05,
      "loss": 1.8709,
      "step": 87800
    },
    {
      "epoch": 6.885477048409839,
      "grad_norm": 4.4560136795043945,
      "learning_rate": 4.426210245965847e-05,
      "loss": 1.8978,
      "step": 87900
    },
    {
      "epoch": 6.893310355632148,
      "grad_norm": 4.953562259674072,
      "learning_rate": 4.425557470363988e-05,
      "loss": 1.9628,
      "step": 88000
    },
    {
      "epoch": 6.901143662854457,
      "grad_norm": 5.913660526275635,
      "learning_rate": 4.424904694762129e-05,
      "loss": 1.8434,
      "step": 88100
    },
    {
      "epoch": 6.9089769700767665,
      "grad_norm": 5.55129337310791,
      "learning_rate": 4.42425191916027e-05,
      "loss": 1.8288,
      "step": 88200
    },
    {
      "epoch": 6.916810277299076,
      "grad_norm": 6.972144603729248,
      "learning_rate": 4.423599143558411e-05,
      "loss": 1.8668,
      "step": 88300
    },
    {
      "epoch": 6.924643584521385,
      "grad_norm": 6.684669017791748,
      "learning_rate": 4.4229463679565514e-05,
      "loss": 1.9157,
      "step": 88400
    },
    {
      "epoch": 6.932476891743694,
      "grad_norm": 5.540720462799072,
      "learning_rate": 4.422293592354692e-05,
      "loss": 1.9574,
      "step": 88500
    },
    {
      "epoch": 6.940310198966003,
      "grad_norm": 4.601508617401123,
      "learning_rate": 4.421640816752833e-05,
      "loss": 1.9283,
      "step": 88600
    },
    {
      "epoch": 6.948143506188313,
      "grad_norm": 7.017576217651367,
      "learning_rate": 4.420988041150974e-05,
      "loss": 1.9402,
      "step": 88700
    },
    {
      "epoch": 6.955976813410622,
      "grad_norm": 6.629973411560059,
      "learning_rate": 4.420335265549115e-05,
      "loss": 1.9294,
      "step": 88800
    },
    {
      "epoch": 6.963810120632932,
      "grad_norm": 5.227345943450928,
      "learning_rate": 4.419682489947256e-05,
      "loss": 1.8768,
      "step": 88900
    },
    {
      "epoch": 6.97164342785524,
      "grad_norm": 4.455121040344238,
      "learning_rate": 4.419029714345397e-05,
      "loss": 1.838,
      "step": 89000
    },
    {
      "epoch": 6.97947673507755,
      "grad_norm": 6.182623863220215,
      "learning_rate": 4.4183769387435374e-05,
      "loss": 1.8547,
      "step": 89100
    },
    {
      "epoch": 6.987310042299859,
      "grad_norm": 5.491194725036621,
      "learning_rate": 4.417724163141679e-05,
      "loss": 1.9027,
      "step": 89200
    },
    {
      "epoch": 6.995143349522168,
      "grad_norm": 6.010458469390869,
      "learning_rate": 4.417071387539819e-05,
      "loss": 1.8572,
      "step": 89300
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8570473194122314,
      "eval_runtime": 1.5535,
      "eval_samples_per_second": 432.57,
      "eval_steps_per_second": 432.57,
      "step": 89362
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.6516971588134766,
      "eval_runtime": 29.7641,
      "eval_samples_per_second": 428.906,
      "eval_steps_per_second": 428.906,
      "step": 89362
    },
    {
      "epoch": 7.002976656744478,
      "grad_norm": 6.751969814300537,
      "learning_rate": 4.4164186119379605e-05,
      "loss": 1.9133,
      "step": 89400
    },
    {
      "epoch": 7.0108099639667865,
      "grad_norm": 7.411643981933594,
      "learning_rate": 4.415765836336102e-05,
      "loss": 1.8407,
      "step": 89500
    },
    {
      "epoch": 7.018643271189096,
      "grad_norm": 6.234311580657959,
      "learning_rate": 4.4151130607342424e-05,
      "loss": 1.8982,
      "step": 89600
    },
    {
      "epoch": 7.026476578411406,
      "grad_norm": 5.784567832946777,
      "learning_rate": 4.414460285132383e-05,
      "loss": 1.8182,
      "step": 89700
    },
    {
      "epoch": 7.034309885633714,
      "grad_norm": 5.041339874267578,
      "learning_rate": 4.413807509530524e-05,
      "loss": 1.9138,
      "step": 89800
    },
    {
      "epoch": 7.042143192856024,
      "grad_norm": 5.870672702789307,
      "learning_rate": 4.413154733928665e-05,
      "loss": 1.8451,
      "step": 89900
    },
    {
      "epoch": 7.049976500078333,
      "grad_norm": 6.481844902038574,
      "learning_rate": 4.4125019583268054e-05,
      "loss": 1.7903,
      "step": 90000
    },
    {
      "epoch": 7.057809807300642,
      "grad_norm": 5.230164527893066,
      "learning_rate": 4.411849182724947e-05,
      "loss": 1.9145,
      "step": 90100
    },
    {
      "epoch": 7.065643114522952,
      "grad_norm": 3.9884371757507324,
      "learning_rate": 4.411196407123088e-05,
      "loss": 1.8624,
      "step": 90200
    },
    {
      "epoch": 7.073476421745261,
      "grad_norm": 5.528879642486572,
      "learning_rate": 4.4105436315212284e-05,
      "loss": 1.9076,
      "step": 90300
    },
    {
      "epoch": 7.08130972896757,
      "grad_norm": 5.004423141479492,
      "learning_rate": 4.409890855919369e-05,
      "loss": 1.8684,
      "step": 90400
    },
    {
      "epoch": 7.0891430361898795,
      "grad_norm": 7.04168176651001,
      "learning_rate": 4.40923808031751e-05,
      "loss": 1.8535,
      "step": 90500
    },
    {
      "epoch": 7.096976343412188,
      "grad_norm": 5.207037925720215,
      "learning_rate": 4.408585304715651e-05,
      "loss": 1.7862,
      "step": 90600
    },
    {
      "epoch": 7.104809650634498,
      "grad_norm": 5.042391777038574,
      "learning_rate": 4.407932529113792e-05,
      "loss": 1.8332,
      "step": 90700
    },
    {
      "epoch": 7.112642957856807,
      "grad_norm": 7.24307107925415,
      "learning_rate": 4.4072797535119334e-05,
      "loss": 1.848,
      "step": 90800
    },
    {
      "epoch": 7.120476265079116,
      "grad_norm": 5.238840103149414,
      "learning_rate": 4.406626977910074e-05,
      "loss": 1.8798,
      "step": 90900
    },
    {
      "epoch": 7.128309572301426,
      "grad_norm": 4.772665023803711,
      "learning_rate": 4.4059742023082145e-05,
      "loss": 1.8228,
      "step": 91000
    },
    {
      "epoch": 7.136142879523735,
      "grad_norm": 6.234663963317871,
      "learning_rate": 4.405321426706356e-05,
      "loss": 1.7509,
      "step": 91100
    },
    {
      "epoch": 7.143976186746044,
      "grad_norm": 5.15786600112915,
      "learning_rate": 4.404668651104496e-05,
      "loss": 1.815,
      "step": 91200
    },
    {
      "epoch": 7.151809493968353,
      "grad_norm": 5.473668575286865,
      "learning_rate": 4.4040158755026376e-05,
      "loss": 1.7994,
      "step": 91300
    },
    {
      "epoch": 7.159642801190663,
      "grad_norm": 5.785516738891602,
      "learning_rate": 4.403363099900779e-05,
      "loss": 1.8139,
      "step": 91400
    },
    {
      "epoch": 7.167476108412972,
      "grad_norm": 5.821121692657471,
      "learning_rate": 4.4027103242989194e-05,
      "loss": 1.8656,
      "step": 91500
    },
    {
      "epoch": 7.175309415635281,
      "grad_norm": 6.034173965454102,
      "learning_rate": 4.40205754869706e-05,
      "loss": 1.8906,
      "step": 91600
    },
    {
      "epoch": 7.183142722857591,
      "grad_norm": 5.971675395965576,
      "learning_rate": 4.401404773095201e-05,
      "loss": 1.7818,
      "step": 91700
    },
    {
      "epoch": 7.1909760300798995,
      "grad_norm": 7.538678169250488,
      "learning_rate": 4.400751997493342e-05,
      "loss": 1.8386,
      "step": 91800
    },
    {
      "epoch": 7.198809337302209,
      "grad_norm": 4.664353847503662,
      "learning_rate": 4.4000992218914824e-05,
      "loss": 1.8705,
      "step": 91900
    },
    {
      "epoch": 7.206642644524519,
      "grad_norm": 4.673062801361084,
      "learning_rate": 4.399446446289624e-05,
      "loss": 1.8616,
      "step": 92000
    },
    {
      "epoch": 7.214475951746827,
      "grad_norm": 6.157116889953613,
      "learning_rate": 4.398793670687765e-05,
      "loss": 1.8918,
      "step": 92100
    },
    {
      "epoch": 7.222309258969137,
      "grad_norm": 5.751270771026611,
      "learning_rate": 4.3981408950859055e-05,
      "loss": 1.9273,
      "step": 92200
    },
    {
      "epoch": 7.2301425661914465,
      "grad_norm": 6.838949680328369,
      "learning_rate": 4.397488119484046e-05,
      "loss": 1.8544,
      "step": 92300
    },
    {
      "epoch": 7.237975873413755,
      "grad_norm": 4.46440315246582,
      "learning_rate": 4.396835343882187e-05,
      "loss": 1.8983,
      "step": 92400
    },
    {
      "epoch": 7.245809180636065,
      "grad_norm": 4.631138324737549,
      "learning_rate": 4.396182568280328e-05,
      "loss": 1.9719,
      "step": 92500
    },
    {
      "epoch": 7.253642487858373,
      "grad_norm": 4.160382270812988,
      "learning_rate": 4.395529792678469e-05,
      "loss": 1.7984,
      "step": 92600
    },
    {
      "epoch": 7.261475795080683,
      "grad_norm": 5.075967788696289,
      "learning_rate": 4.3948770170766104e-05,
      "loss": 1.9028,
      "step": 92700
    },
    {
      "epoch": 7.2693091023029925,
      "grad_norm": 6.841681957244873,
      "learning_rate": 4.394224241474751e-05,
      "loss": 1.8782,
      "step": 92800
    },
    {
      "epoch": 7.277142409525301,
      "grad_norm": 6.842599391937256,
      "learning_rate": 4.3935714658728916e-05,
      "loss": 1.9245,
      "step": 92900
    },
    {
      "epoch": 7.284975716747611,
      "grad_norm": 4.100218772888184,
      "learning_rate": 4.392918690271033e-05,
      "loss": 1.8654,
      "step": 93000
    },
    {
      "epoch": 7.29280902396992,
      "grad_norm": 6.0857110023498535,
      "learning_rate": 4.3922659146691734e-05,
      "loss": 1.849,
      "step": 93100
    },
    {
      "epoch": 7.300642331192229,
      "grad_norm": 6.139772415161133,
      "learning_rate": 4.391613139067314e-05,
      "loss": 1.8755,
      "step": 93200
    },
    {
      "epoch": 7.308475638414539,
      "grad_norm": 6.348038673400879,
      "learning_rate": 4.390960363465456e-05,
      "loss": 1.7824,
      "step": 93300
    },
    {
      "epoch": 7.316308945636848,
      "grad_norm": 7.804041385650635,
      "learning_rate": 4.3903075878635965e-05,
      "loss": 1.8265,
      "step": 93400
    },
    {
      "epoch": 7.324142252859157,
      "grad_norm": 8.34299087524414,
      "learning_rate": 4.389654812261737e-05,
      "loss": 1.949,
      "step": 93500
    },
    {
      "epoch": 7.3319755600814664,
      "grad_norm": 5.967693328857422,
      "learning_rate": 4.3890020366598776e-05,
      "loss": 1.9078,
      "step": 93600
    },
    {
      "epoch": 7.339808867303776,
      "grad_norm": 6.918959617614746,
      "learning_rate": 4.388349261058019e-05,
      "loss": 1.9078,
      "step": 93700
    },
    {
      "epoch": 7.347642174526085,
      "grad_norm": 5.3902788162231445,
      "learning_rate": 4.3876964854561595e-05,
      "loss": 1.7646,
      "step": 93800
    },
    {
      "epoch": 7.355475481748394,
      "grad_norm": 6.219506740570068,
      "learning_rate": 4.387043709854301e-05,
      "loss": 1.8643,
      "step": 93900
    },
    {
      "epoch": 7.363308788970704,
      "grad_norm": 7.52510404586792,
      "learning_rate": 4.386390934252442e-05,
      "loss": 1.8298,
      "step": 94000
    },
    {
      "epoch": 7.3711420961930125,
      "grad_norm": 4.947326183319092,
      "learning_rate": 4.3857381586505826e-05,
      "loss": 1.8117,
      "step": 94100
    },
    {
      "epoch": 7.378975403415322,
      "grad_norm": 5.541248321533203,
      "learning_rate": 4.385085383048723e-05,
      "loss": 1.8432,
      "step": 94200
    },
    {
      "epoch": 7.386808710637631,
      "grad_norm": 5.899682998657227,
      "learning_rate": 4.3844326074468644e-05,
      "loss": 1.7979,
      "step": 94300
    },
    {
      "epoch": 7.39464201785994,
      "grad_norm": 6.900928974151611,
      "learning_rate": 4.383779831845005e-05,
      "loss": 1.889,
      "step": 94400
    },
    {
      "epoch": 7.40247532508225,
      "grad_norm": 4.888333797454834,
      "learning_rate": 4.383127056243146e-05,
      "loss": 1.835,
      "step": 94500
    },
    {
      "epoch": 7.410308632304559,
      "grad_norm": 6.2675347328186035,
      "learning_rate": 4.3824742806412875e-05,
      "loss": 1.7909,
      "step": 94600
    },
    {
      "epoch": 7.418141939526868,
      "grad_norm": 5.5127387046813965,
      "learning_rate": 4.381821505039428e-05,
      "loss": 1.8685,
      "step": 94700
    },
    {
      "epoch": 7.425975246749178,
      "grad_norm": 6.539482593536377,
      "learning_rate": 4.3811687294375686e-05,
      "loss": 1.8486,
      "step": 94800
    },
    {
      "epoch": 7.433808553971486,
      "grad_norm": 9.06519603729248,
      "learning_rate": 4.38051595383571e-05,
      "loss": 1.8594,
      "step": 94900
    },
    {
      "epoch": 7.441641861193796,
      "grad_norm": 4.362765312194824,
      "learning_rate": 4.3798631782338505e-05,
      "loss": 1.7599,
      "step": 95000
    },
    {
      "epoch": 7.449475168416106,
      "grad_norm": 6.198741912841797,
      "learning_rate": 4.379210402631991e-05,
      "loss": 1.8517,
      "step": 95100
    },
    {
      "epoch": 7.457308475638414,
      "grad_norm": 5.778425693511963,
      "learning_rate": 4.378557627030132e-05,
      "loss": 1.9033,
      "step": 95200
    },
    {
      "epoch": 7.465141782860724,
      "grad_norm": 6.2917609214782715,
      "learning_rate": 4.3779048514282735e-05,
      "loss": 1.8027,
      "step": 95300
    },
    {
      "epoch": 7.472975090083033,
      "grad_norm": 6.588283538818359,
      "learning_rate": 4.377252075826414e-05,
      "loss": 1.8861,
      "step": 95400
    },
    {
      "epoch": 7.480808397305342,
      "grad_norm": 4.525031089782715,
      "learning_rate": 4.376599300224555e-05,
      "loss": 1.8051,
      "step": 95500
    },
    {
      "epoch": 7.488641704527652,
      "grad_norm": 5.262133598327637,
      "learning_rate": 4.375946524622696e-05,
      "loss": 1.9038,
      "step": 95600
    },
    {
      "epoch": 7.496475011749961,
      "grad_norm": 6.696945667266846,
      "learning_rate": 4.3752937490208365e-05,
      "loss": 1.9203,
      "step": 95700
    },
    {
      "epoch": 7.50430831897227,
      "grad_norm": 7.319892406463623,
      "learning_rate": 4.374640973418978e-05,
      "loss": 1.8776,
      "step": 95800
    },
    {
      "epoch": 7.5121416261945795,
      "grad_norm": 6.029181957244873,
      "learning_rate": 4.373988197817119e-05,
      "loss": 1.8604,
      "step": 95900
    },
    {
      "epoch": 7.519974933416888,
      "grad_norm": 5.76232385635376,
      "learning_rate": 4.3733354222152596e-05,
      "loss": 1.8301,
      "step": 96000
    },
    {
      "epoch": 7.527808240639198,
      "grad_norm": 5.882856369018555,
      "learning_rate": 4.3726826466134e-05,
      "loss": 1.8591,
      "step": 96100
    },
    {
      "epoch": 7.535641547861507,
      "grad_norm": 5.65313720703125,
      "learning_rate": 4.3720298710115414e-05,
      "loss": 1.8381,
      "step": 96200
    },
    {
      "epoch": 7.543474855083817,
      "grad_norm": 5.662196159362793,
      "learning_rate": 4.371377095409682e-05,
      "loss": 1.8177,
      "step": 96300
    },
    {
      "epoch": 7.551308162306126,
      "grad_norm": 4.067558288574219,
      "learning_rate": 4.3707243198078226e-05,
      "loss": 1.9863,
      "step": 96400
    },
    {
      "epoch": 7.559141469528435,
      "grad_norm": 6.055860996246338,
      "learning_rate": 4.3700715442059645e-05,
      "loss": 1.8745,
      "step": 96500
    },
    {
      "epoch": 7.566974776750744,
      "grad_norm": 5.531948566436768,
      "learning_rate": 4.369418768604105e-05,
      "loss": 1.8604,
      "step": 96600
    },
    {
      "epoch": 7.574808083973053,
      "grad_norm": 5.336712837219238,
      "learning_rate": 4.368765993002246e-05,
      "loss": 1.848,
      "step": 96700
    },
    {
      "epoch": 7.582641391195363,
      "grad_norm": 6.798884868621826,
      "learning_rate": 4.368113217400387e-05,
      "loss": 1.8473,
      "step": 96800
    },
    {
      "epoch": 7.590474698417672,
      "grad_norm": 6.3278632164001465,
      "learning_rate": 4.3674604417985275e-05,
      "loss": 1.8109,
      "step": 96900
    },
    {
      "epoch": 7.598308005639981,
      "grad_norm": 10.563804626464844,
      "learning_rate": 4.366807666196668e-05,
      "loss": 1.8714,
      "step": 97000
    },
    {
      "epoch": 7.606141312862291,
      "grad_norm": 7.098834037780762,
      "learning_rate": 4.3661548905948093e-05,
      "loss": 1.8161,
      "step": 97100
    },
    {
      "epoch": 7.6139746200845995,
      "grad_norm": 5.358545303344727,
      "learning_rate": 4.3655021149929506e-05,
      "loss": 1.8279,
      "step": 97200
    },
    {
      "epoch": 7.621807927306909,
      "grad_norm": 6.020886421203613,
      "learning_rate": 4.364849339391091e-05,
      "loss": 1.8777,
      "step": 97300
    },
    {
      "epoch": 7.629641234529219,
      "grad_norm": 6.636059761047363,
      "learning_rate": 4.364196563789232e-05,
      "loss": 1.8473,
      "step": 97400
    },
    {
      "epoch": 7.637474541751527,
      "grad_norm": 6.450276851654053,
      "learning_rate": 4.363543788187373e-05,
      "loss": 1.7922,
      "step": 97500
    },
    {
      "epoch": 7.645307848973837,
      "grad_norm": 5.553572654724121,
      "learning_rate": 4.3628910125855136e-05,
      "loss": 1.8851,
      "step": 97600
    },
    {
      "epoch": 7.6531411561961455,
      "grad_norm": 5.6988725662231445,
      "learning_rate": 4.362238236983655e-05,
      "loss": 1.9769,
      "step": 97700
    },
    {
      "epoch": 7.660974463418455,
      "grad_norm": 6.680997848510742,
      "learning_rate": 4.361585461381796e-05,
      "loss": 1.7776,
      "step": 97800
    },
    {
      "epoch": 7.668807770640765,
      "grad_norm": 6.0361528396606445,
      "learning_rate": 4.360932685779937e-05,
      "loss": 1.8669,
      "step": 97900
    },
    {
      "epoch": 7.676641077863074,
      "grad_norm": 5.394024848937988,
      "learning_rate": 4.360279910178077e-05,
      "loss": 1.7904,
      "step": 98000
    },
    {
      "epoch": 7.684474385085383,
      "grad_norm": 3.872821807861328,
      "learning_rate": 4.3596271345762185e-05,
      "loss": 1.829,
      "step": 98100
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 4.703361988067627,
      "learning_rate": 4.358974358974359e-05,
      "loss": 1.8615,
      "step": 98200
    },
    {
      "epoch": 7.700140999530001,
      "grad_norm": 5.093423366546631,
      "learning_rate": 4.3583215833724997e-05,
      "loss": 1.8307,
      "step": 98300
    },
    {
      "epoch": 7.707974306752311,
      "grad_norm": 6.156128406524658,
      "learning_rate": 4.357668807770641e-05,
      "loss": 1.7634,
      "step": 98400
    },
    {
      "epoch": 7.71580761397462,
      "grad_norm": 5.095841407775879,
      "learning_rate": 4.357016032168782e-05,
      "loss": 1.9032,
      "step": 98500
    },
    {
      "epoch": 7.723640921196929,
      "grad_norm": 4.465944766998291,
      "learning_rate": 4.356363256566923e-05,
      "loss": 1.8127,
      "step": 98600
    },
    {
      "epoch": 7.731474228419239,
      "grad_norm": 6.451413631439209,
      "learning_rate": 4.355710480965063e-05,
      "loss": 1.8353,
      "step": 98700
    },
    {
      "epoch": 7.739307535641548,
      "grad_norm": 7.0208821296691895,
      "learning_rate": 4.3550577053632046e-05,
      "loss": 1.8903,
      "step": 98800
    },
    {
      "epoch": 7.747140842863857,
      "grad_norm": 5.442350387573242,
      "learning_rate": 4.354404929761345e-05,
      "loss": 1.8111,
      "step": 98900
    },
    {
      "epoch": 7.754974150086166,
      "grad_norm": 6.572093963623047,
      "learning_rate": 4.3537521541594864e-05,
      "loss": 1.8693,
      "step": 99000
    },
    {
      "epoch": 7.762807457308476,
      "grad_norm": 5.829384803771973,
      "learning_rate": 4.3530993785576277e-05,
      "loss": 1.8875,
      "step": 99100
    },
    {
      "epoch": 7.770640764530785,
      "grad_norm": 5.336715221405029,
      "learning_rate": 4.352446602955768e-05,
      "loss": 1.9432,
      "step": 99200
    },
    {
      "epoch": 7.778474071753094,
      "grad_norm": 7.579971790313721,
      "learning_rate": 4.351793827353909e-05,
      "loss": 1.905,
      "step": 99300
    },
    {
      "epoch": 7.786307378975403,
      "grad_norm": 6.617674827575684,
      "learning_rate": 4.35114105175205e-05,
      "loss": 1.8283,
      "step": 99400
    },
    {
      "epoch": 7.7941406861977125,
      "grad_norm": 3.594311237335205,
      "learning_rate": 4.3504882761501906e-05,
      "loss": 1.8618,
      "step": 99500
    },
    {
      "epoch": 7.801973993420022,
      "grad_norm": 5.375036239624023,
      "learning_rate": 4.349835500548331e-05,
      "loss": 1.8895,
      "step": 99600
    },
    {
      "epoch": 7.809807300642332,
      "grad_norm": 5.492892265319824,
      "learning_rate": 4.349182724946473e-05,
      "loss": 1.8679,
      "step": 99700
    },
    {
      "epoch": 7.81764060786464,
      "grad_norm": 6.624236106872559,
      "learning_rate": 4.348529949344614e-05,
      "loss": 1.8865,
      "step": 99800
    },
    {
      "epoch": 7.82547391508695,
      "grad_norm": 6.558599948883057,
      "learning_rate": 4.347877173742754e-05,
      "loss": 1.7852,
      "step": 99900
    },
    {
      "epoch": 7.833307222309259,
      "grad_norm": 5.5435357093811035,
      "learning_rate": 4.3472243981408956e-05,
      "loss": 1.941,
      "step": 100000
    },
    {
      "epoch": 7.841140529531568,
      "grad_norm": 4.778807640075684,
      "learning_rate": 4.346571622539036e-05,
      "loss": 1.8887,
      "step": 100100
    },
    {
      "epoch": 7.848973836753878,
      "grad_norm": 4.496356010437012,
      "learning_rate": 4.345918846937177e-05,
      "loss": 1.9336,
      "step": 100200
    },
    {
      "epoch": 7.856807143976186,
      "grad_norm": 6.451961994171143,
      "learning_rate": 4.345266071335318e-05,
      "loss": 1.7998,
      "step": 100300
    },
    {
      "epoch": 7.864640451198496,
      "grad_norm": 5.407769203186035,
      "learning_rate": 4.344613295733459e-05,
      "loss": 1.9042,
      "step": 100400
    },
    {
      "epoch": 7.8724737584208055,
      "grad_norm": 6.416059970855713,
      "learning_rate": 4.3439605201316e-05,
      "loss": 1.8562,
      "step": 100500
    },
    {
      "epoch": 7.880307065643114,
      "grad_norm": 5.565370082855225,
      "learning_rate": 4.3433077445297404e-05,
      "loss": 1.9291,
      "step": 100600
    },
    {
      "epoch": 7.888140372865424,
      "grad_norm": 5.134228706359863,
      "learning_rate": 4.3426549689278816e-05,
      "loss": 1.7733,
      "step": 100700
    },
    {
      "epoch": 7.895973680087733,
      "grad_norm": 6.479645729064941,
      "learning_rate": 4.342002193326022e-05,
      "loss": 1.9215,
      "step": 100800
    },
    {
      "epoch": 7.903806987310042,
      "grad_norm": 6.680212497711182,
      "learning_rate": 4.3413494177241635e-05,
      "loss": 1.9081,
      "step": 100900
    },
    {
      "epoch": 7.911640294532352,
      "grad_norm": 4.096212863922119,
      "learning_rate": 4.340696642122305e-05,
      "loss": 1.9474,
      "step": 101000
    },
    {
      "epoch": 7.919473601754661,
      "grad_norm": 5.471920967102051,
      "learning_rate": 4.340043866520445e-05,
      "loss": 1.767,
      "step": 101100
    },
    {
      "epoch": 7.92730690897697,
      "grad_norm": 7.538737773895264,
      "learning_rate": 4.339391090918586e-05,
      "loss": 1.7835,
      "step": 101200
    },
    {
      "epoch": 7.9351402161992795,
      "grad_norm": 6.5419158935546875,
      "learning_rate": 4.338738315316727e-05,
      "loss": 1.8109,
      "step": 101300
    },
    {
      "epoch": 7.942973523421589,
      "grad_norm": 4.525368690490723,
      "learning_rate": 4.338085539714868e-05,
      "loss": 1.8378,
      "step": 101400
    },
    {
      "epoch": 7.950806830643898,
      "grad_norm": 5.368637561798096,
      "learning_rate": 4.337432764113008e-05,
      "loss": 1.8787,
      "step": 101500
    },
    {
      "epoch": 7.958640137866207,
      "grad_norm": 5.2467942237854,
      "learning_rate": 4.3367799885111495e-05,
      "loss": 1.9009,
      "step": 101600
    },
    {
      "epoch": 7.966473445088516,
      "grad_norm": 5.946303844451904,
      "learning_rate": 4.336127212909291e-05,
      "loss": 1.8421,
      "step": 101700
    },
    {
      "epoch": 7.9743067523108255,
      "grad_norm": 6.197371482849121,
      "learning_rate": 4.3354744373074314e-05,
      "loss": 1.935,
      "step": 101800
    },
    {
      "epoch": 7.982140059533135,
      "grad_norm": 4.997814655303955,
      "learning_rate": 4.334821661705572e-05,
      "loss": 1.8936,
      "step": 101900
    },
    {
      "epoch": 7.989973366755444,
      "grad_norm": 6.540529727935791,
      "learning_rate": 4.334168886103713e-05,
      "loss": 1.9229,
      "step": 102000
    },
    {
      "epoch": 7.997806673977753,
      "grad_norm": 5.440306186676025,
      "learning_rate": 4.333516110501854e-05,
      "loss": 1.7684,
      "step": 102100
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8429114818572998,
      "eval_runtime": 1.5502,
      "eval_samples_per_second": 433.5,
      "eval_steps_per_second": 433.5,
      "step": 102128
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.6299039125442505,
      "eval_runtime": 29.2188,
      "eval_samples_per_second": 436.91,
      "eval_steps_per_second": 436.91,
      "step": 102128
    },
    {
      "epoch": 8.005639981200062,
      "grad_norm": 5.536896705627441,
      "learning_rate": 4.332863334899995e-05,
      "loss": 1.8364,
      "step": 102200
    },
    {
      "epoch": 8.013473288422372,
      "grad_norm": 12.776296615600586,
      "learning_rate": 4.332210559298136e-05,
      "loss": 1.8193,
      "step": 102300
    },
    {
      "epoch": 8.021306595644681,
      "grad_norm": 6.909684181213379,
      "learning_rate": 4.331557783696277e-05,
      "loss": 1.8312,
      "step": 102400
    },
    {
      "epoch": 8.02913990286699,
      "grad_norm": 5.961374282836914,
      "learning_rate": 4.3309050080944174e-05,
      "loss": 1.8649,
      "step": 102500
    },
    {
      "epoch": 8.0369732100893,
      "grad_norm": 4.474427223205566,
      "learning_rate": 4.330252232492559e-05,
      "loss": 1.7381,
      "step": 102600
    },
    {
      "epoch": 8.044806517311608,
      "grad_norm": 6.781212329864502,
      "learning_rate": 4.329599456890699e-05,
      "loss": 1.8984,
      "step": 102700
    },
    {
      "epoch": 8.052639824533918,
      "grad_norm": 5.45428991317749,
      "learning_rate": 4.32894668128884e-05,
      "loss": 1.8572,
      "step": 102800
    },
    {
      "epoch": 8.060473131756227,
      "grad_norm": 6.1973958015441895,
      "learning_rate": 4.328293905686982e-05,
      "loss": 1.8504,
      "step": 102900
    },
    {
      "epoch": 8.068306438978537,
      "grad_norm": 5.4176225662231445,
      "learning_rate": 4.3276411300851224e-05,
      "loss": 1.8655,
      "step": 103000
    },
    {
      "epoch": 8.076139746200846,
      "grad_norm": 5.414506435394287,
      "learning_rate": 4.326988354483263e-05,
      "loss": 1.8616,
      "step": 103100
    },
    {
      "epoch": 8.083973053423156,
      "grad_norm": 6.678903579711914,
      "learning_rate": 4.326335578881404e-05,
      "loss": 1.7047,
      "step": 103200
    },
    {
      "epoch": 8.091806360645464,
      "grad_norm": 5.0762834548950195,
      "learning_rate": 4.325682803279545e-05,
      "loss": 1.8095,
      "step": 103300
    },
    {
      "epoch": 8.099639667867773,
      "grad_norm": 5.958156108856201,
      "learning_rate": 4.325030027677685e-05,
      "loss": 1.768,
      "step": 103400
    },
    {
      "epoch": 8.107472975090083,
      "grad_norm": 5.085126876831055,
      "learning_rate": 4.3243772520758266e-05,
      "loss": 1.9063,
      "step": 103500
    },
    {
      "epoch": 8.115306282312392,
      "grad_norm": 4.8362627029418945,
      "learning_rate": 4.323724476473968e-05,
      "loss": 1.7904,
      "step": 103600
    },
    {
      "epoch": 8.123139589534702,
      "grad_norm": 7.429465293884277,
      "learning_rate": 4.3230717008721084e-05,
      "loss": 1.8987,
      "step": 103700
    },
    {
      "epoch": 8.130972896757012,
      "grad_norm": 4.685451984405518,
      "learning_rate": 4.322418925270249e-05,
      "loss": 1.8645,
      "step": 103800
    },
    {
      "epoch": 8.13880620397932,
      "grad_norm": 5.596375465393066,
      "learning_rate": 4.32176614966839e-05,
      "loss": 1.7889,
      "step": 103900
    },
    {
      "epoch": 8.146639511201629,
      "grad_norm": 5.9201178550720215,
      "learning_rate": 4.321113374066531e-05,
      "loss": 1.8614,
      "step": 104000
    },
    {
      "epoch": 8.154472818423939,
      "grad_norm": 7.975886821746826,
      "learning_rate": 4.320460598464672e-05,
      "loss": 1.7941,
      "step": 104100
    },
    {
      "epoch": 8.162306125646248,
      "grad_norm": 6.853901386260986,
      "learning_rate": 4.3198078228628133e-05,
      "loss": 1.8414,
      "step": 104200
    },
    {
      "epoch": 8.170139432868558,
      "grad_norm": 5.835506916046143,
      "learning_rate": 4.319155047260954e-05,
      "loss": 1.8689,
      "step": 104300
    },
    {
      "epoch": 8.177972740090865,
      "grad_norm": 5.77581262588501,
      "learning_rate": 4.3185022716590945e-05,
      "loss": 1.8019,
      "step": 104400
    },
    {
      "epoch": 8.185806047313175,
      "grad_norm": 5.5398268699646,
      "learning_rate": 4.317849496057236e-05,
      "loss": 1.7239,
      "step": 104500
    },
    {
      "epoch": 8.193639354535485,
      "grad_norm": 4.552420616149902,
      "learning_rate": 4.317196720455376e-05,
      "loss": 1.7963,
      "step": 104600
    },
    {
      "epoch": 8.201472661757794,
      "grad_norm": 4.878259658813477,
      "learning_rate": 4.316543944853517e-05,
      "loss": 1.8009,
      "step": 104700
    },
    {
      "epoch": 8.209305968980104,
      "grad_norm": 7.349928379058838,
      "learning_rate": 4.315891169251658e-05,
      "loss": 1.8536,
      "step": 104800
    },
    {
      "epoch": 8.217139276202413,
      "grad_norm": 5.326336860656738,
      "learning_rate": 4.3152383936497994e-05,
      "loss": 1.7524,
      "step": 104900
    },
    {
      "epoch": 8.224972583424721,
      "grad_norm": 8.22406005859375,
      "learning_rate": 4.31458561804794e-05,
      "loss": 1.7482,
      "step": 105000
    },
    {
      "epoch": 8.23280589064703,
      "grad_norm": 5.924548149108887,
      "learning_rate": 4.313932842446081e-05,
      "loss": 1.9044,
      "step": 105100
    },
    {
      "epoch": 8.24063919786934,
      "grad_norm": 4.030429840087891,
      "learning_rate": 4.313280066844222e-05,
      "loss": 1.8302,
      "step": 105200
    },
    {
      "epoch": 8.24847250509165,
      "grad_norm": 5.004240989685059,
      "learning_rate": 4.3126272912423624e-05,
      "loss": 1.8251,
      "step": 105300
    },
    {
      "epoch": 8.25630581231396,
      "grad_norm": 5.513988971710205,
      "learning_rate": 4.3119745156405037e-05,
      "loss": 1.8681,
      "step": 105400
    },
    {
      "epoch": 8.264139119536269,
      "grad_norm": 5.276856899261475,
      "learning_rate": 4.311321740038645e-05,
      "loss": 1.8129,
      "step": 105500
    },
    {
      "epoch": 8.271972426758577,
      "grad_norm": 6.780150890350342,
      "learning_rate": 4.3106689644367855e-05,
      "loss": 1.7976,
      "step": 105600
    },
    {
      "epoch": 8.279805733980886,
      "grad_norm": 6.111763000488281,
      "learning_rate": 4.310016188834926e-05,
      "loss": 1.7988,
      "step": 105700
    },
    {
      "epoch": 8.287639041203196,
      "grad_norm": 5.254818439483643,
      "learning_rate": 4.309363413233067e-05,
      "loss": 1.7425,
      "step": 105800
    },
    {
      "epoch": 8.295472348425506,
      "grad_norm": 4.149421691894531,
      "learning_rate": 4.308710637631208e-05,
      "loss": 1.7779,
      "step": 105900
    },
    {
      "epoch": 8.303305655647815,
      "grad_norm": 4.8884196281433105,
      "learning_rate": 4.3080578620293485e-05,
      "loss": 1.8974,
      "step": 106000
    },
    {
      "epoch": 8.311138962870125,
      "grad_norm": 8.55901050567627,
      "learning_rate": 4.3074050864274904e-05,
      "loss": 1.8741,
      "step": 106100
    },
    {
      "epoch": 8.318972270092432,
      "grad_norm": 7.182124614715576,
      "learning_rate": 4.306752310825631e-05,
      "loss": 1.8078,
      "step": 106200
    },
    {
      "epoch": 8.326805577314742,
      "grad_norm": 6.513416767120361,
      "learning_rate": 4.3060995352237716e-05,
      "loss": 1.846,
      "step": 106300
    },
    {
      "epoch": 8.334638884537052,
      "grad_norm": 6.507751941680908,
      "learning_rate": 4.305446759621913e-05,
      "loss": 1.7646,
      "step": 106400
    },
    {
      "epoch": 8.342472191759361,
      "grad_norm": 6.893202304840088,
      "learning_rate": 4.3047939840200534e-05,
      "loss": 1.8287,
      "step": 106500
    },
    {
      "epoch": 8.35030549898167,
      "grad_norm": 4.221508026123047,
      "learning_rate": 4.304141208418194e-05,
      "loss": 1.795,
      "step": 106600
    },
    {
      "epoch": 8.358138806203979,
      "grad_norm": 5.705471515655518,
      "learning_rate": 4.303488432816335e-05,
      "loss": 1.7672,
      "step": 106700
    },
    {
      "epoch": 8.365972113426288,
      "grad_norm": 6.981991767883301,
      "learning_rate": 4.3028356572144765e-05,
      "loss": 1.854,
      "step": 106800
    },
    {
      "epoch": 8.373805420648598,
      "grad_norm": 7.377749919891357,
      "learning_rate": 4.302182881612617e-05,
      "loss": 1.9001,
      "step": 106900
    },
    {
      "epoch": 8.381638727870907,
      "grad_norm": 5.499879837036133,
      "learning_rate": 4.3015301060107576e-05,
      "loss": 1.8293,
      "step": 107000
    },
    {
      "epoch": 8.389472035093217,
      "grad_norm": 5.0117669105529785,
      "learning_rate": 4.300877330408899e-05,
      "loss": 1.7963,
      "step": 107100
    },
    {
      "epoch": 8.397305342315526,
      "grad_norm": 5.954325199127197,
      "learning_rate": 4.3002245548070395e-05,
      "loss": 1.8417,
      "step": 107200
    },
    {
      "epoch": 8.405138649537834,
      "grad_norm": 4.859307289123535,
      "learning_rate": 4.299571779205181e-05,
      "loss": 1.8911,
      "step": 107300
    },
    {
      "epoch": 8.412971956760144,
      "grad_norm": 6.0663862228393555,
      "learning_rate": 4.298919003603322e-05,
      "loss": 1.8071,
      "step": 107400
    },
    {
      "epoch": 8.420805263982453,
      "grad_norm": 5.3269944190979,
      "learning_rate": 4.2982662280014625e-05,
      "loss": 1.7712,
      "step": 107500
    },
    {
      "epoch": 8.428638571204763,
      "grad_norm": 4.302248954772949,
      "learning_rate": 4.297613452399603e-05,
      "loss": 1.7905,
      "step": 107600
    },
    {
      "epoch": 8.436471878427072,
      "grad_norm": 5.678369045257568,
      "learning_rate": 4.2969606767977444e-05,
      "loss": 1.9493,
      "step": 107700
    },
    {
      "epoch": 8.44430518564938,
      "grad_norm": 4.822454929351807,
      "learning_rate": 4.296307901195885e-05,
      "loss": 1.9032,
      "step": 107800
    },
    {
      "epoch": 8.45213849287169,
      "grad_norm": 5.856539249420166,
      "learning_rate": 4.2956551255940255e-05,
      "loss": 1.8976,
      "step": 107900
    },
    {
      "epoch": 8.459971800094,
      "grad_norm": 6.965035438537598,
      "learning_rate": 4.295002349992167e-05,
      "loss": 1.7434,
      "step": 108000
    },
    {
      "epoch": 8.467805107316309,
      "grad_norm": 5.970279693603516,
      "learning_rate": 4.294349574390308e-05,
      "loss": 1.9314,
      "step": 108100
    },
    {
      "epoch": 8.475638414538619,
      "grad_norm": 4.570199966430664,
      "learning_rate": 4.2936967987884486e-05,
      "loss": 1.909,
      "step": 108200
    },
    {
      "epoch": 8.483471721760928,
      "grad_norm": 7.472957134246826,
      "learning_rate": 4.29304402318659e-05,
      "loss": 1.7464,
      "step": 108300
    },
    {
      "epoch": 8.491305028983236,
      "grad_norm": 7.027839183807373,
      "learning_rate": 4.2923912475847304e-05,
      "loss": 1.8996,
      "step": 108400
    },
    {
      "epoch": 8.499138336205545,
      "grad_norm": 5.041378498077393,
      "learning_rate": 4.291738471982871e-05,
      "loss": 1.8051,
      "step": 108500
    },
    {
      "epoch": 8.506971643427855,
      "grad_norm": 6.035407066345215,
      "learning_rate": 4.291085696381012e-05,
      "loss": 1.864,
      "step": 108600
    },
    {
      "epoch": 8.514804950650165,
      "grad_norm": 4.519113540649414,
      "learning_rate": 4.2904329207791535e-05,
      "loss": 1.8274,
      "step": 108700
    },
    {
      "epoch": 8.522638257872474,
      "grad_norm": 6.993194103240967,
      "learning_rate": 4.289780145177294e-05,
      "loss": 1.6818,
      "step": 108800
    },
    {
      "epoch": 8.530471565094784,
      "grad_norm": 5.515748023986816,
      "learning_rate": 4.289127369575435e-05,
      "loss": 1.8271,
      "step": 108900
    },
    {
      "epoch": 8.538304872317092,
      "grad_norm": 7.2710490226745605,
      "learning_rate": 4.288474593973576e-05,
      "loss": 1.8084,
      "step": 109000
    },
    {
      "epoch": 8.546138179539401,
      "grad_norm": 5.738241195678711,
      "learning_rate": 4.2878218183717165e-05,
      "loss": 1.8418,
      "step": 109100
    },
    {
      "epoch": 8.55397148676171,
      "grad_norm": 5.72908878326416,
      "learning_rate": 4.287169042769857e-05,
      "loss": 1.8627,
      "step": 109200
    },
    {
      "epoch": 8.56180479398402,
      "grad_norm": 7.634739875793457,
      "learning_rate": 4.286516267167999e-05,
      "loss": 1.8582,
      "step": 109300
    },
    {
      "epoch": 8.56963810120633,
      "grad_norm": 6.6090288162231445,
      "learning_rate": 4.2858634915661396e-05,
      "loss": 1.8387,
      "step": 109400
    },
    {
      "epoch": 8.57747140842864,
      "grad_norm": 7.975053787231445,
      "learning_rate": 4.28521071596428e-05,
      "loss": 1.8218,
      "step": 109500
    },
    {
      "epoch": 8.585304715650947,
      "grad_norm": 5.032892227172852,
      "learning_rate": 4.2845579403624214e-05,
      "loss": 1.9507,
      "step": 109600
    },
    {
      "epoch": 8.593138022873257,
      "grad_norm": 4.270137786865234,
      "learning_rate": 4.283905164760562e-05,
      "loss": 1.8783,
      "step": 109700
    },
    {
      "epoch": 8.600971330095566,
      "grad_norm": 5.828395843505859,
      "learning_rate": 4.2832523891587026e-05,
      "loss": 1.7932,
      "step": 109800
    },
    {
      "epoch": 8.608804637317876,
      "grad_norm": 6.116133689880371,
      "learning_rate": 4.282599613556844e-05,
      "loss": 1.7985,
      "step": 109900
    },
    {
      "epoch": 8.616637944540186,
      "grad_norm": 5.1578369140625,
      "learning_rate": 4.281946837954985e-05,
      "loss": 1.9109,
      "step": 110000
    },
    {
      "epoch": 8.624471251762493,
      "grad_norm": 5.225367069244385,
      "learning_rate": 4.281294062353126e-05,
      "loss": 1.8635,
      "step": 110100
    },
    {
      "epoch": 8.632304558984803,
      "grad_norm": 5.068044185638428,
      "learning_rate": 4.280641286751267e-05,
      "loss": 1.8467,
      "step": 110200
    },
    {
      "epoch": 8.640137866207112,
      "grad_norm": 7.6124114990234375,
      "learning_rate": 4.2799885111494075e-05,
      "loss": 1.8387,
      "step": 110300
    },
    {
      "epoch": 8.647971173429422,
      "grad_norm": 5.061441421508789,
      "learning_rate": 4.279335735547548e-05,
      "loss": 1.8688,
      "step": 110400
    },
    {
      "epoch": 8.655804480651732,
      "grad_norm": 6.564965724945068,
      "learning_rate": 4.278682959945689e-05,
      "loss": 1.8907,
      "step": 110500
    },
    {
      "epoch": 8.663637787874041,
      "grad_norm": 5.897773742675781,
      "learning_rate": 4.2780301843438306e-05,
      "loss": 1.8898,
      "step": 110600
    },
    {
      "epoch": 8.671471095096349,
      "grad_norm": 6.447495937347412,
      "learning_rate": 4.277377408741971e-05,
      "loss": 1.8117,
      "step": 110700
    },
    {
      "epoch": 8.679304402318659,
      "grad_norm": 3.829664945602417,
      "learning_rate": 4.276724633140112e-05,
      "loss": 1.8238,
      "step": 110800
    },
    {
      "epoch": 8.687137709540968,
      "grad_norm": 5.861354351043701,
      "learning_rate": 4.276071857538253e-05,
      "loss": 1.8962,
      "step": 110900
    },
    {
      "epoch": 8.694971016763278,
      "grad_norm": 7.466018199920654,
      "learning_rate": 4.2754190819363936e-05,
      "loss": 1.8054,
      "step": 111000
    },
    {
      "epoch": 8.702804323985587,
      "grad_norm": 5.115446090698242,
      "learning_rate": 4.274766306334534e-05,
      "loss": 1.8644,
      "step": 111100
    },
    {
      "epoch": 8.710637631207897,
      "grad_norm": 6.120887279510498,
      "learning_rate": 4.2741135307326754e-05,
      "loss": 1.8546,
      "step": 111200
    },
    {
      "epoch": 8.718470938430205,
      "grad_norm": 5.0038161277771,
      "learning_rate": 4.2734607551308167e-05,
      "loss": 1.8313,
      "step": 111300
    },
    {
      "epoch": 8.726304245652514,
      "grad_norm": 7.271187782287598,
      "learning_rate": 4.272807979528957e-05,
      "loss": 1.874,
      "step": 111400
    },
    {
      "epoch": 8.734137552874824,
      "grad_norm": 6.17939567565918,
      "learning_rate": 4.2721552039270985e-05,
      "loss": 1.8566,
      "step": 111500
    },
    {
      "epoch": 8.741970860097133,
      "grad_norm": 7.000115871429443,
      "learning_rate": 4.271502428325239e-05,
      "loss": 1.7943,
      "step": 111600
    },
    {
      "epoch": 8.749804167319443,
      "grad_norm": 6.3027663230896,
      "learning_rate": 4.2708496527233796e-05,
      "loss": 1.7694,
      "step": 111700
    },
    {
      "epoch": 8.757637474541752,
      "grad_norm": 6.142148017883301,
      "learning_rate": 4.270196877121521e-05,
      "loss": 1.8594,
      "step": 111800
    },
    {
      "epoch": 8.76547078176406,
      "grad_norm": 5.243221759796143,
      "learning_rate": 4.269544101519662e-05,
      "loss": 1.9596,
      "step": 111900
    },
    {
      "epoch": 8.77330408898637,
      "grad_norm": 5.422426700592041,
      "learning_rate": 4.268891325917803e-05,
      "loss": 1.8674,
      "step": 112000
    },
    {
      "epoch": 8.78113739620868,
      "grad_norm": 2.337768077850342,
      "learning_rate": 4.268238550315943e-05,
      "loss": 1.8364,
      "step": 112100
    },
    {
      "epoch": 8.788970703430989,
      "grad_norm": 4.389353275299072,
      "learning_rate": 4.2675857747140846e-05,
      "loss": 1.8678,
      "step": 112200
    },
    {
      "epoch": 8.796804010653299,
      "grad_norm": 6.902839183807373,
      "learning_rate": 4.266932999112225e-05,
      "loss": 1.9221,
      "step": 112300
    },
    {
      "epoch": 8.804637317875606,
      "grad_norm": 5.141385555267334,
      "learning_rate": 4.266280223510366e-05,
      "loss": 1.8777,
      "step": 112400
    },
    {
      "epoch": 8.812470625097916,
      "grad_norm": 5.064167022705078,
      "learning_rate": 4.2656274479085076e-05,
      "loss": 1.8565,
      "step": 112500
    },
    {
      "epoch": 8.820303932320225,
      "grad_norm": 6.385183334350586,
      "learning_rate": 4.264974672306648e-05,
      "loss": 1.8996,
      "step": 112600
    },
    {
      "epoch": 8.828137239542535,
      "grad_norm": 4.668848991394043,
      "learning_rate": 4.264321896704789e-05,
      "loss": 1.8325,
      "step": 112700
    },
    {
      "epoch": 8.835970546764845,
      "grad_norm": 7.952258586883545,
      "learning_rate": 4.26366912110293e-05,
      "loss": 1.9003,
      "step": 112800
    },
    {
      "epoch": 8.843803853987154,
      "grad_norm": 5.2834296226501465,
      "learning_rate": 4.2630163455010706e-05,
      "loss": 1.8409,
      "step": 112900
    },
    {
      "epoch": 8.851637161209462,
      "grad_norm": 5.597841739654541,
      "learning_rate": 4.262363569899211e-05,
      "loss": 1.8422,
      "step": 113000
    },
    {
      "epoch": 8.859470468431772,
      "grad_norm": 5.061986923217773,
      "learning_rate": 4.2617107942973525e-05,
      "loss": 1.8334,
      "step": 113100
    },
    {
      "epoch": 8.867303775654081,
      "grad_norm": 5.001499176025391,
      "learning_rate": 4.261058018695494e-05,
      "loss": 1.8847,
      "step": 113200
    },
    {
      "epoch": 8.87513708287639,
      "grad_norm": 4.2245635986328125,
      "learning_rate": 4.260405243093634e-05,
      "loss": 1.7917,
      "step": 113300
    },
    {
      "epoch": 8.8829703900987,
      "grad_norm": 6.460033893585205,
      "learning_rate": 4.2597524674917755e-05,
      "loss": 1.7774,
      "step": 113400
    },
    {
      "epoch": 8.890803697321008,
      "grad_norm": 5.543825149536133,
      "learning_rate": 4.259099691889916e-05,
      "loss": 1.8358,
      "step": 113500
    },
    {
      "epoch": 8.898637004543318,
      "grad_norm": 6.438249588012695,
      "learning_rate": 4.258446916288057e-05,
      "loss": 1.8756,
      "step": 113600
    },
    {
      "epoch": 8.906470311765627,
      "grad_norm": 5.167321681976318,
      "learning_rate": 4.257794140686198e-05,
      "loss": 1.9498,
      "step": 113700
    },
    {
      "epoch": 8.914303618987937,
      "grad_norm": 5.901218414306641,
      "learning_rate": 4.257141365084339e-05,
      "loss": 1.7176,
      "step": 113800
    },
    {
      "epoch": 8.922136926210246,
      "grad_norm": 6.652669429779053,
      "learning_rate": 4.25648858948248e-05,
      "loss": 1.8534,
      "step": 113900
    },
    {
      "epoch": 8.929970233432556,
      "grad_norm": 6.278481483459473,
      "learning_rate": 4.2558358138806204e-05,
      "loss": 1.8845,
      "step": 114000
    },
    {
      "epoch": 8.937803540654864,
      "grad_norm": 7.134505271911621,
      "learning_rate": 4.2551830382787616e-05,
      "loss": 1.8241,
      "step": 114100
    },
    {
      "epoch": 8.945636847877173,
      "grad_norm": 5.483834266662598,
      "learning_rate": 4.254530262676902e-05,
      "loss": 1.8541,
      "step": 114200
    },
    {
      "epoch": 8.953470155099483,
      "grad_norm": 4.663034915924072,
      "learning_rate": 4.253877487075043e-05,
      "loss": 1.7912,
      "step": 114300
    },
    {
      "epoch": 8.961303462321792,
      "grad_norm": 6.619333744049072,
      "learning_rate": 4.253224711473184e-05,
      "loss": 1.8558,
      "step": 114400
    },
    {
      "epoch": 8.969136769544102,
      "grad_norm": 6.316842079162598,
      "learning_rate": 4.252571935871325e-05,
      "loss": 1.9793,
      "step": 114500
    },
    {
      "epoch": 8.976970076766412,
      "grad_norm": 4.405378818511963,
      "learning_rate": 4.251919160269466e-05,
      "loss": 1.8363,
      "step": 114600
    },
    {
      "epoch": 8.98480338398872,
      "grad_norm": 5.392592906951904,
      "learning_rate": 4.251266384667607e-05,
      "loss": 1.8393,
      "step": 114700
    },
    {
      "epoch": 8.992636691211029,
      "grad_norm": 4.60967493057251,
      "learning_rate": 4.250613609065748e-05,
      "loss": 1.8744,
      "step": 114800
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8457894325256348,
      "eval_runtime": 1.5542,
      "eval_samples_per_second": 432.375,
      "eval_steps_per_second": 432.375,
      "step": 114894
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.623178243637085,
      "eval_runtime": 30.4944,
      "eval_samples_per_second": 418.634,
      "eval_steps_per_second": 418.634,
      "step": 114894
    },
    {
      "epoch": 9.000469998433339,
      "grad_norm": 6.331604480743408,
      "learning_rate": 4.249960833463888e-05,
      "loss": 1.8965,
      "step": 114900
    },
    {
      "epoch": 9.008303305655648,
      "grad_norm": 4.936544418334961,
      "learning_rate": 4.2493080578620295e-05,
      "loss": 1.8015,
      "step": 115000
    },
    {
      "epoch": 9.016136612877958,
      "grad_norm": 6.198299407958984,
      "learning_rate": 4.248655282260171e-05,
      "loss": 1.8651,
      "step": 115100
    },
    {
      "epoch": 9.023969920100267,
      "grad_norm": 4.9421281814575195,
      "learning_rate": 4.2480025066583114e-05,
      "loss": 1.7392,
      "step": 115200
    },
    {
      "epoch": 9.031803227322575,
      "grad_norm": 4.7904887199401855,
      "learning_rate": 4.2473497310564526e-05,
      "loss": 1.7533,
      "step": 115300
    },
    {
      "epoch": 9.039636534544885,
      "grad_norm": 5.463306903839111,
      "learning_rate": 4.246696955454593e-05,
      "loss": 1.8443,
      "step": 115400
    },
    {
      "epoch": 9.047469841767194,
      "grad_norm": 5.095639228820801,
      "learning_rate": 4.246044179852734e-05,
      "loss": 1.8894,
      "step": 115500
    },
    {
      "epoch": 9.055303148989504,
      "grad_norm": 4.661257743835449,
      "learning_rate": 4.245391404250874e-05,
      "loss": 1.7522,
      "step": 115600
    },
    {
      "epoch": 9.063136456211813,
      "grad_norm": 5.642051696777344,
      "learning_rate": 4.244738628649016e-05,
      "loss": 1.7871,
      "step": 115700
    },
    {
      "epoch": 9.070969763434121,
      "grad_norm": 6.757259368896484,
      "learning_rate": 4.244085853047157e-05,
      "loss": 1.8112,
      "step": 115800
    },
    {
      "epoch": 9.07880307065643,
      "grad_norm": 3.7882707118988037,
      "learning_rate": 4.2434330774452974e-05,
      "loss": 1.728,
      "step": 115900
    },
    {
      "epoch": 9.08663637787874,
      "grad_norm": 5.819206714630127,
      "learning_rate": 4.242780301843439e-05,
      "loss": 1.7742,
      "step": 116000
    },
    {
      "epoch": 9.09446968510105,
      "grad_norm": 6.6462297439575195,
      "learning_rate": 4.242127526241579e-05,
      "loss": 1.811,
      "step": 116100
    },
    {
      "epoch": 9.10230299232336,
      "grad_norm": 5.300759315490723,
      "learning_rate": 4.24147475063972e-05,
      "loss": 1.7795,
      "step": 116200
    },
    {
      "epoch": 9.110136299545669,
      "grad_norm": 5.04423713684082,
      "learning_rate": 4.240821975037861e-05,
      "loss": 1.7277,
      "step": 116300
    },
    {
      "epoch": 9.117969606767977,
      "grad_norm": 4.233832359313965,
      "learning_rate": 4.2401691994360023e-05,
      "loss": 1.8386,
      "step": 116400
    },
    {
      "epoch": 9.125802913990286,
      "grad_norm": 4.869755268096924,
      "learning_rate": 4.239516423834143e-05,
      "loss": 1.7286,
      "step": 116500
    },
    {
      "epoch": 9.133636221212596,
      "grad_norm": 5.349650859832764,
      "learning_rate": 4.238863648232284e-05,
      "loss": 1.784,
      "step": 116600
    },
    {
      "epoch": 9.141469528434905,
      "grad_norm": 5.177614688873291,
      "learning_rate": 4.238210872630425e-05,
      "loss": 1.8242,
      "step": 116700
    },
    {
      "epoch": 9.149302835657215,
      "grad_norm": 6.75508451461792,
      "learning_rate": 4.237558097028565e-05,
      "loss": 1.7754,
      "step": 116800
    },
    {
      "epoch": 9.157136142879525,
      "grad_norm": 4.826476573944092,
      "learning_rate": 4.2369053214267066e-05,
      "loss": 1.7573,
      "step": 116900
    },
    {
      "epoch": 9.164969450101832,
      "grad_norm": 4.505082130432129,
      "learning_rate": 4.236252545824848e-05,
      "loss": 1.8516,
      "step": 117000
    },
    {
      "epoch": 9.172802757324142,
      "grad_norm": 4.65993595123291,
      "learning_rate": 4.2355997702229884e-05,
      "loss": 1.8428,
      "step": 117100
    },
    {
      "epoch": 9.180636064546452,
      "grad_norm": 5.887964725494385,
      "learning_rate": 4.234946994621129e-05,
      "loss": 1.8189,
      "step": 117200
    },
    {
      "epoch": 9.188469371768761,
      "grad_norm": 4.856056213378906,
      "learning_rate": 4.23429421901927e-05,
      "loss": 1.7955,
      "step": 117300
    },
    {
      "epoch": 9.19630267899107,
      "grad_norm": 6.3948655128479,
      "learning_rate": 4.233641443417411e-05,
      "loss": 1.8492,
      "step": 117400
    },
    {
      "epoch": 9.204135986213378,
      "grad_norm": 6.154547691345215,
      "learning_rate": 4.2329886678155514e-05,
      "loss": 1.8504,
      "step": 117500
    },
    {
      "epoch": 9.211969293435688,
      "grad_norm": 6.569886684417725,
      "learning_rate": 4.2323358922136926e-05,
      "loss": 1.8284,
      "step": 117600
    },
    {
      "epoch": 9.219802600657998,
      "grad_norm": 6.191978931427002,
      "learning_rate": 4.231683116611834e-05,
      "loss": 1.8618,
      "step": 117700
    },
    {
      "epoch": 9.227635907880307,
      "grad_norm": 10.826593399047852,
      "learning_rate": 4.2310303410099745e-05,
      "loss": 1.8734,
      "step": 117800
    },
    {
      "epoch": 9.235469215102617,
      "grad_norm": 6.43772029876709,
      "learning_rate": 4.230377565408116e-05,
      "loss": 1.8809,
      "step": 117900
    },
    {
      "epoch": 9.243302522324926,
      "grad_norm": 5.524500370025635,
      "learning_rate": 4.229724789806256e-05,
      "loss": 1.8032,
      "step": 118000
    },
    {
      "epoch": 9.251135829547234,
      "grad_norm": 7.090282440185547,
      "learning_rate": 4.229072014204397e-05,
      "loss": 1.7903,
      "step": 118100
    },
    {
      "epoch": 9.258969136769544,
      "grad_norm": 9.640168190002441,
      "learning_rate": 4.228419238602538e-05,
      "loss": 1.769,
      "step": 118200
    },
    {
      "epoch": 9.266802443991853,
      "grad_norm": 4.804727554321289,
      "learning_rate": 4.2277664630006794e-05,
      "loss": 1.7656,
      "step": 118300
    },
    {
      "epoch": 9.274635751214163,
      "grad_norm": 6.946939945220947,
      "learning_rate": 4.22711368739882e-05,
      "loss": 1.8163,
      "step": 118400
    },
    {
      "epoch": 9.282469058436472,
      "grad_norm": 6.132902145385742,
      "learning_rate": 4.226460911796961e-05,
      "loss": 1.8254,
      "step": 118500
    },
    {
      "epoch": 9.290302365658782,
      "grad_norm": 5.386994361877441,
      "learning_rate": 4.225808136195102e-05,
      "loss": 1.8833,
      "step": 118600
    },
    {
      "epoch": 9.29813567288109,
      "grad_norm": 6.449494361877441,
      "learning_rate": 4.2251553605932424e-05,
      "loss": 1.7946,
      "step": 118700
    },
    {
      "epoch": 9.3059689801034,
      "grad_norm": 6.537110805511475,
      "learning_rate": 4.224502584991383e-05,
      "loss": 1.9286,
      "step": 118800
    },
    {
      "epoch": 9.313802287325709,
      "grad_norm": 7.732693195343018,
      "learning_rate": 4.223849809389525e-05,
      "loss": 1.7134,
      "step": 118900
    },
    {
      "epoch": 9.321635594548018,
      "grad_norm": 7.310647010803223,
      "learning_rate": 4.2231970337876655e-05,
      "loss": 1.7477,
      "step": 119000
    },
    {
      "epoch": 9.329468901770328,
      "grad_norm": 6.29002571105957,
      "learning_rate": 4.222544258185806e-05,
      "loss": 1.8669,
      "step": 119100
    },
    {
      "epoch": 9.337302208992636,
      "grad_norm": 5.421308994293213,
      "learning_rate": 4.221891482583947e-05,
      "loss": 1.8342,
      "step": 119200
    },
    {
      "epoch": 9.345135516214945,
      "grad_norm": 6.431489944458008,
      "learning_rate": 4.221238706982088e-05,
      "loss": 1.8636,
      "step": 119300
    },
    {
      "epoch": 9.352968823437255,
      "grad_norm": 5.663363456726074,
      "learning_rate": 4.2205859313802285e-05,
      "loss": 1.7567,
      "step": 119400
    },
    {
      "epoch": 9.360802130659565,
      "grad_norm": 5.176940441131592,
      "learning_rate": 4.21993315577837e-05,
      "loss": 1.8738,
      "step": 119500
    },
    {
      "epoch": 9.368635437881874,
      "grad_norm": 5.867221832275391,
      "learning_rate": 4.219280380176511e-05,
      "loss": 1.9632,
      "step": 119600
    },
    {
      "epoch": 9.376468745104184,
      "grad_norm": 6.777771949768066,
      "learning_rate": 4.2186276045746515e-05,
      "loss": 1.8051,
      "step": 119700
    },
    {
      "epoch": 9.384302052326492,
      "grad_norm": 5.9840497970581055,
      "learning_rate": 4.217974828972793e-05,
      "loss": 1.6999,
      "step": 119800
    },
    {
      "epoch": 9.392135359548801,
      "grad_norm": 6.610448837280273,
      "learning_rate": 4.2173220533709334e-05,
      "loss": 1.8011,
      "step": 119900
    },
    {
      "epoch": 9.39996866677111,
      "grad_norm": 6.288341999053955,
      "learning_rate": 4.216669277769074e-05,
      "loss": 1.7578,
      "step": 120000
    },
    {
      "epoch": 9.40780197399342,
      "grad_norm": 5.401524066925049,
      "learning_rate": 4.216016502167215e-05,
      "loss": 1.8925,
      "step": 120100
    },
    {
      "epoch": 9.41563528121573,
      "grad_norm": 5.862833023071289,
      "learning_rate": 4.2153637265653565e-05,
      "loss": 1.8867,
      "step": 120200
    },
    {
      "epoch": 9.42346858843804,
      "grad_norm": 7.03843355178833,
      "learning_rate": 4.214710950963497e-05,
      "loss": 1.7567,
      "step": 120300
    },
    {
      "epoch": 9.431301895660347,
      "grad_norm": 5.2581963539123535,
      "learning_rate": 4.214058175361638e-05,
      "loss": 1.8763,
      "step": 120400
    },
    {
      "epoch": 9.439135202882657,
      "grad_norm": 6.173208713531494,
      "learning_rate": 4.213405399759779e-05,
      "loss": 1.7985,
      "step": 120500
    },
    {
      "epoch": 9.446968510104966,
      "grad_norm": 6.4959797859191895,
      "learning_rate": 4.2127526241579194e-05,
      "loss": 1.7944,
      "step": 120600
    },
    {
      "epoch": 9.454801817327276,
      "grad_norm": 5.90585470199585,
      "learning_rate": 4.21209984855606e-05,
      "loss": 1.9372,
      "step": 120700
    },
    {
      "epoch": 9.462635124549585,
      "grad_norm": 4.107830047607422,
      "learning_rate": 4.211447072954201e-05,
      "loss": 1.817,
      "step": 120800
    },
    {
      "epoch": 9.470468431771893,
      "grad_norm": 5.051558971405029,
      "learning_rate": 4.2107942973523425e-05,
      "loss": 1.8632,
      "step": 120900
    },
    {
      "epoch": 9.478301738994203,
      "grad_norm": 5.139899253845215,
      "learning_rate": 4.210141521750483e-05,
      "loss": 1.8854,
      "step": 121000
    },
    {
      "epoch": 9.486135046216512,
      "grad_norm": 4.964175701141357,
      "learning_rate": 4.2094887461486244e-05,
      "loss": 1.8734,
      "step": 121100
    },
    {
      "epoch": 9.493968353438822,
      "grad_norm": 4.580449104309082,
      "learning_rate": 4.208835970546765e-05,
      "loss": 1.8988,
      "step": 121200
    },
    {
      "epoch": 9.501801660661132,
      "grad_norm": 5.908081531524658,
      "learning_rate": 4.2081831949449055e-05,
      "loss": 1.8128,
      "step": 121300
    },
    {
      "epoch": 9.509634967883441,
      "grad_norm": 5.513803958892822,
      "learning_rate": 4.207530419343047e-05,
      "loss": 1.9198,
      "step": 121400
    },
    {
      "epoch": 9.517468275105749,
      "grad_norm": 6.830859661102295,
      "learning_rate": 4.206877643741188e-05,
      "loss": 1.8074,
      "step": 121500
    },
    {
      "epoch": 9.525301582328058,
      "grad_norm": 6.480199813842773,
      "learning_rate": 4.2062248681393286e-05,
      "loss": 1.7599,
      "step": 121600
    },
    {
      "epoch": 9.533134889550368,
      "grad_norm": 6.220177173614502,
      "learning_rate": 4.20557209253747e-05,
      "loss": 1.8411,
      "step": 121700
    },
    {
      "epoch": 9.540968196772678,
      "grad_norm": 6.1622819900512695,
      "learning_rate": 4.2049193169356104e-05,
      "loss": 1.8577,
      "step": 121800
    },
    {
      "epoch": 9.548801503994987,
      "grad_norm": 7.14170503616333,
      "learning_rate": 4.204266541333751e-05,
      "loss": 1.847,
      "step": 121900
    },
    {
      "epoch": 9.556634811217297,
      "grad_norm": 6.053109169006348,
      "learning_rate": 4.203613765731892e-05,
      "loss": 1.8978,
      "step": 122000
    },
    {
      "epoch": 9.564468118439605,
      "grad_norm": 7.935424327850342,
      "learning_rate": 4.2029609901300335e-05,
      "loss": 1.9467,
      "step": 122100
    },
    {
      "epoch": 9.572301425661914,
      "grad_norm": 7.327477931976318,
      "learning_rate": 4.202308214528174e-05,
      "loss": 1.848,
      "step": 122200
    },
    {
      "epoch": 9.580134732884224,
      "grad_norm": 5.508328437805176,
      "learning_rate": 4.201655438926315e-05,
      "loss": 1.8037,
      "step": 122300
    },
    {
      "epoch": 9.587968040106533,
      "grad_norm": 5.795559883117676,
      "learning_rate": 4.201002663324456e-05,
      "loss": 1.8118,
      "step": 122400
    },
    {
      "epoch": 9.595801347328843,
      "grad_norm": 5.152476787567139,
      "learning_rate": 4.2003498877225965e-05,
      "loss": 1.7858,
      "step": 122500
    },
    {
      "epoch": 9.60363465455115,
      "grad_norm": 8.14554500579834,
      "learning_rate": 4.199697112120737e-05,
      "loss": 1.8379,
      "step": 122600
    },
    {
      "epoch": 9.61146796177346,
      "grad_norm": 5.596519947052002,
      "learning_rate": 4.199044336518878e-05,
      "loss": 1.8845,
      "step": 122700
    },
    {
      "epoch": 9.61930126899577,
      "grad_norm": 5.2008209228515625,
      "learning_rate": 4.1983915609170196e-05,
      "loss": 1.7667,
      "step": 122800
    },
    {
      "epoch": 9.62713457621808,
      "grad_norm": 5.778863430023193,
      "learning_rate": 4.19773878531516e-05,
      "loss": 1.8106,
      "step": 122900
    },
    {
      "epoch": 9.634967883440389,
      "grad_norm": 6.708373069763184,
      "learning_rate": 4.1970860097133014e-05,
      "loss": 1.9551,
      "step": 123000
    },
    {
      "epoch": 9.642801190662698,
      "grad_norm": 7.51605749130249,
      "learning_rate": 4.196433234111442e-05,
      "loss": 1.934,
      "step": 123100
    },
    {
      "epoch": 9.650634497885006,
      "grad_norm": 5.6760573387146,
      "learning_rate": 4.1957804585095826e-05,
      "loss": 1.868,
      "step": 123200
    },
    {
      "epoch": 9.658467805107316,
      "grad_norm": 5.59248685836792,
      "learning_rate": 4.195127682907724e-05,
      "loss": 1.8132,
      "step": 123300
    },
    {
      "epoch": 9.666301112329625,
      "grad_norm": 6.238458156585693,
      "learning_rate": 4.194474907305865e-05,
      "loss": 1.8635,
      "step": 123400
    },
    {
      "epoch": 9.674134419551935,
      "grad_norm": 5.283497333526611,
      "learning_rate": 4.1938221317040057e-05,
      "loss": 1.7722,
      "step": 123500
    },
    {
      "epoch": 9.681967726774245,
      "grad_norm": 8.21517276763916,
      "learning_rate": 4.193169356102147e-05,
      "loss": 1.7195,
      "step": 123600
    },
    {
      "epoch": 9.689801033996554,
      "grad_norm": 5.350949287414551,
      "learning_rate": 4.1925165805002875e-05,
      "loss": 1.8506,
      "step": 123700
    },
    {
      "epoch": 9.697634341218862,
      "grad_norm": 6.128211975097656,
      "learning_rate": 4.191863804898428e-05,
      "loss": 1.8033,
      "step": 123800
    },
    {
      "epoch": 9.705467648441171,
      "grad_norm": 8.270537376403809,
      "learning_rate": 4.1912110292965686e-05,
      "loss": 1.8523,
      "step": 123900
    },
    {
      "epoch": 9.713300955663481,
      "grad_norm": 5.266451358795166,
      "learning_rate": 4.19055825369471e-05,
      "loss": 1.8043,
      "step": 124000
    },
    {
      "epoch": 9.72113426288579,
      "grad_norm": 4.640086650848389,
      "learning_rate": 4.189905478092851e-05,
      "loss": 1.7905,
      "step": 124100
    },
    {
      "epoch": 9.7289675701081,
      "grad_norm": 6.83798360824585,
      "learning_rate": 4.189252702490992e-05,
      "loss": 1.8444,
      "step": 124200
    },
    {
      "epoch": 9.73680087733041,
      "grad_norm": 5.172757625579834,
      "learning_rate": 4.188599926889133e-05,
      "loss": 1.8542,
      "step": 124300
    },
    {
      "epoch": 9.744634184552718,
      "grad_norm": 5.844913959503174,
      "learning_rate": 4.1879471512872736e-05,
      "loss": 1.8145,
      "step": 124400
    },
    {
      "epoch": 9.752467491775027,
      "grad_norm": 4.50960636138916,
      "learning_rate": 4.187294375685414e-05,
      "loss": 1.7974,
      "step": 124500
    },
    {
      "epoch": 9.760300798997337,
      "grad_norm": 5.638892650604248,
      "learning_rate": 4.1866416000835554e-05,
      "loss": 1.7877,
      "step": 124600
    },
    {
      "epoch": 9.768134106219646,
      "grad_norm": 5.584195137023926,
      "learning_rate": 4.1859888244816966e-05,
      "loss": 1.8015,
      "step": 124700
    },
    {
      "epoch": 9.775967413441956,
      "grad_norm": 5.3608598709106445,
      "learning_rate": 4.185336048879837e-05,
      "loss": 1.8,
      "step": 124800
    },
    {
      "epoch": 9.783800720664264,
      "grad_norm": 7.968785762786865,
      "learning_rate": 4.1846832732779785e-05,
      "loss": 1.8608,
      "step": 124900
    },
    {
      "epoch": 9.791634027886573,
      "grad_norm": 4.269855499267578,
      "learning_rate": 4.184030497676119e-05,
      "loss": 1.7722,
      "step": 125000
    },
    {
      "epoch": 9.799467335108883,
      "grad_norm": 4.176350116729736,
      "learning_rate": 4.1833777220742596e-05,
      "loss": 1.8329,
      "step": 125100
    },
    {
      "epoch": 9.807300642331192,
      "grad_norm": 6.974916934967041,
      "learning_rate": 4.182724946472401e-05,
      "loss": 1.8529,
      "step": 125200
    },
    {
      "epoch": 9.815133949553502,
      "grad_norm": 5.277596950531006,
      "learning_rate": 4.182072170870542e-05,
      "loss": 1.8404,
      "step": 125300
    },
    {
      "epoch": 9.822967256775812,
      "grad_norm": 5.161667346954346,
      "learning_rate": 4.181419395268683e-05,
      "loss": 1.8276,
      "step": 125400
    },
    {
      "epoch": 9.83080056399812,
      "grad_norm": 5.410218715667725,
      "learning_rate": 4.180766619666823e-05,
      "loss": 1.8195,
      "step": 125500
    },
    {
      "epoch": 9.838633871220429,
      "grad_norm": 5.833362102508545,
      "learning_rate": 4.1801138440649645e-05,
      "loss": 1.8525,
      "step": 125600
    },
    {
      "epoch": 9.846467178442738,
      "grad_norm": 9.303934097290039,
      "learning_rate": 4.179461068463105e-05,
      "loss": 1.845,
      "step": 125700
    },
    {
      "epoch": 9.854300485665048,
      "grad_norm": 5.896316051483154,
      "learning_rate": 4.178808292861246e-05,
      "loss": 1.8826,
      "step": 125800
    },
    {
      "epoch": 9.862133792887358,
      "grad_norm": 4.627278804779053,
      "learning_rate": 4.178155517259387e-05,
      "loss": 1.9251,
      "step": 125900
    },
    {
      "epoch": 9.869967100109665,
      "grad_norm": 6.090358734130859,
      "learning_rate": 4.177502741657528e-05,
      "loss": 1.8952,
      "step": 126000
    },
    {
      "epoch": 9.877800407331975,
      "grad_norm": 8.742472648620605,
      "learning_rate": 4.176849966055669e-05,
      "loss": 1.827,
      "step": 126100
    },
    {
      "epoch": 9.885633714554285,
      "grad_norm": 6.163211345672607,
      "learning_rate": 4.17619719045381e-05,
      "loss": 1.8291,
      "step": 126200
    },
    {
      "epoch": 9.893467021776594,
      "grad_norm": 4.901070594787598,
      "learning_rate": 4.1755444148519506e-05,
      "loss": 1.8483,
      "step": 126300
    },
    {
      "epoch": 9.901300328998904,
      "grad_norm": 4.650735855102539,
      "learning_rate": 4.174891639250091e-05,
      "loss": 1.7248,
      "step": 126400
    },
    {
      "epoch": 9.909133636221213,
      "grad_norm": 5.962992191314697,
      "learning_rate": 4.1742388636482324e-05,
      "loss": 1.8822,
      "step": 126500
    },
    {
      "epoch": 9.916966943443521,
      "grad_norm": 6.551685333251953,
      "learning_rate": 4.173586088046374e-05,
      "loss": 1.8579,
      "step": 126600
    },
    {
      "epoch": 9.92480025066583,
      "grad_norm": 7.737500190734863,
      "learning_rate": 4.172933312444514e-05,
      "loss": 1.8164,
      "step": 126700
    },
    {
      "epoch": 9.93263355788814,
      "grad_norm": 6.172016620635986,
      "learning_rate": 4.1722805368426555e-05,
      "loss": 1.7574,
      "step": 126800
    },
    {
      "epoch": 9.94046686511045,
      "grad_norm": 5.4554924964904785,
      "learning_rate": 4.171627761240796e-05,
      "loss": 1.843,
      "step": 126900
    },
    {
      "epoch": 9.94830017233276,
      "grad_norm": 6.165257453918457,
      "learning_rate": 4.170974985638937e-05,
      "loss": 1.8224,
      "step": 127000
    },
    {
      "epoch": 9.956133479555069,
      "grad_norm": 6.770727634429932,
      "learning_rate": 4.170322210037078e-05,
      "loss": 1.8547,
      "step": 127100
    },
    {
      "epoch": 9.963966786777377,
      "grad_norm": 5.492369174957275,
      "learning_rate": 4.1696694344352185e-05,
      "loss": 1.841,
      "step": 127200
    },
    {
      "epoch": 9.971800093999686,
      "grad_norm": 5.657352447509766,
      "learning_rate": 4.16901665883336e-05,
      "loss": 1.8574,
      "step": 127300
    },
    {
      "epoch": 9.979633401221996,
      "grad_norm": 5.8249711990356445,
      "learning_rate": 4.1683638832315004e-05,
      "loss": 1.7617,
      "step": 127400
    },
    {
      "epoch": 9.987466708444305,
      "grad_norm": 6.467479705810547,
      "learning_rate": 4.1677111076296416e-05,
      "loss": 1.8164,
      "step": 127500
    },
    {
      "epoch": 9.995300015666615,
      "grad_norm": 7.343998432159424,
      "learning_rate": 4.167058332027782e-05,
      "loss": 1.9381,
      "step": 127600
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8351103067398071,
      "eval_runtime": 2.9175,
      "eval_samples_per_second": 230.338,
      "eval_steps_per_second": 230.338,
      "step": 127660
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.6022944450378418,
      "eval_runtime": 57.0868,
      "eval_samples_per_second": 223.624,
      "eval_steps_per_second": 223.624,
      "step": 127660
    },
    {
      "epoch": 10.003133322888925,
      "grad_norm": 5.448051452636719,
      "learning_rate": 4.166405556425923e-05,
      "loss": 1.7976,
      "step": 127700
    },
    {
      "epoch": 10.010966630111232,
      "grad_norm": 7.80128812789917,
      "learning_rate": 4.165752780824064e-05,
      "loss": 1.7726,
      "step": 127800
    },
    {
      "epoch": 10.018799937333542,
      "grad_norm": 5.237933158874512,
      "learning_rate": 4.165100005222205e-05,
      "loss": 1.7527,
      "step": 127900
    },
    {
      "epoch": 10.026633244555851,
      "grad_norm": 5.41697359085083,
      "learning_rate": 4.164447229620346e-05,
      "loss": 1.8053,
      "step": 128000
    },
    {
      "epoch": 10.034466551778161,
      "grad_norm": 5.115617275238037,
      "learning_rate": 4.163794454018487e-05,
      "loss": 1.7317,
      "step": 128100
    },
    {
      "epoch": 10.04229985900047,
      "grad_norm": 6.391078472137451,
      "learning_rate": 4.163141678416628e-05,
      "loss": 1.8164,
      "step": 128200
    },
    {
      "epoch": 10.050133166222778,
      "grad_norm": 7.125184535980225,
      "learning_rate": 4.162488902814768e-05,
      "loss": 1.7543,
      "step": 128300
    },
    {
      "epoch": 10.057966473445088,
      "grad_norm": 6.082248210906982,
      "learning_rate": 4.1618361272129095e-05,
      "loss": 1.839,
      "step": 128400
    },
    {
      "epoch": 10.065799780667398,
      "grad_norm": 6.9702630043029785,
      "learning_rate": 4.161183351611051e-05,
      "loss": 1.807,
      "step": 128500
    },
    {
      "epoch": 10.073633087889707,
      "grad_norm": 4.258270740509033,
      "learning_rate": 4.160530576009191e-05,
      "loss": 1.7603,
      "step": 128600
    },
    {
      "epoch": 10.081466395112017,
      "grad_norm": 4.5046515464782715,
      "learning_rate": 4.1598778004073326e-05,
      "loss": 1.781,
      "step": 128700
    },
    {
      "epoch": 10.089299702334326,
      "grad_norm": 5.874749183654785,
      "learning_rate": 4.159225024805473e-05,
      "loss": 1.7831,
      "step": 128800
    },
    {
      "epoch": 10.097133009556634,
      "grad_norm": 5.515130043029785,
      "learning_rate": 4.158572249203614e-05,
      "loss": 1.8558,
      "step": 128900
    },
    {
      "epoch": 10.104966316778944,
      "grad_norm": 3.90545916557312,
      "learning_rate": 4.157919473601754e-05,
      "loss": 1.9146,
      "step": 129000
    },
    {
      "epoch": 10.112799624001253,
      "grad_norm": 5.2228102684021,
      "learning_rate": 4.1572666979998956e-05,
      "loss": 1.8499,
      "step": 129100
    },
    {
      "epoch": 10.120632931223563,
      "grad_norm": 6.0339884757995605,
      "learning_rate": 4.156613922398037e-05,
      "loss": 1.8463,
      "step": 129200
    },
    {
      "epoch": 10.128466238445872,
      "grad_norm": 5.748325347900391,
      "learning_rate": 4.1559611467961774e-05,
      "loss": 1.765,
      "step": 129300
    },
    {
      "epoch": 10.136299545668182,
      "grad_norm": 5.690452575683594,
      "learning_rate": 4.155308371194319e-05,
      "loss": 1.7292,
      "step": 129400
    },
    {
      "epoch": 10.14413285289049,
      "grad_norm": 5.372961521148682,
      "learning_rate": 4.154655595592459e-05,
      "loss": 1.8393,
      "step": 129500
    },
    {
      "epoch": 10.1519661601128,
      "grad_norm": 6.26536226272583,
      "learning_rate": 4.1540028199906e-05,
      "loss": 1.9759,
      "step": 129600
    },
    {
      "epoch": 10.159799467335109,
      "grad_norm": 5.118535995483398,
      "learning_rate": 4.153350044388741e-05,
      "loss": 1.7941,
      "step": 129700
    },
    {
      "epoch": 10.167632774557418,
      "grad_norm": 4.641465663909912,
      "learning_rate": 4.152697268786882e-05,
      "loss": 1.7874,
      "step": 129800
    },
    {
      "epoch": 10.175466081779728,
      "grad_norm": 7.055355548858643,
      "learning_rate": 4.152044493185023e-05,
      "loss": 1.8315,
      "step": 129900
    },
    {
      "epoch": 10.183299389002036,
      "grad_norm": 5.3546600341796875,
      "learning_rate": 4.151391717583164e-05,
      "loss": 1.8291,
      "step": 130000
    },
    {
      "epoch": 10.191132696224345,
      "grad_norm": 6.1261982917785645,
      "learning_rate": 4.150738941981305e-05,
      "loss": 1.7576,
      "step": 130100
    },
    {
      "epoch": 10.198966003446655,
      "grad_norm": 9.239001274108887,
      "learning_rate": 4.150086166379445e-05,
      "loss": 1.9053,
      "step": 130200
    },
    {
      "epoch": 10.206799310668965,
      "grad_norm": 6.49295711517334,
      "learning_rate": 4.1494333907775866e-05,
      "loss": 1.8542,
      "step": 130300
    },
    {
      "epoch": 10.214632617891274,
      "grad_norm": 6.399125099182129,
      "learning_rate": 4.148780615175727e-05,
      "loss": 1.8835,
      "step": 130400
    },
    {
      "epoch": 10.222465925113584,
      "grad_norm": 5.898252487182617,
      "learning_rate": 4.1481278395738684e-05,
      "loss": 1.718,
      "step": 130500
    },
    {
      "epoch": 10.230299232335891,
      "grad_norm": 6.2490739822387695,
      "learning_rate": 4.147475063972009e-05,
      "loss": 1.7578,
      "step": 130600
    },
    {
      "epoch": 10.238132539558201,
      "grad_norm": 7.881723880767822,
      "learning_rate": 4.14682228837015e-05,
      "loss": 1.8341,
      "step": 130700
    },
    {
      "epoch": 10.24596584678051,
      "grad_norm": 5.761693477630615,
      "learning_rate": 4.146169512768291e-05,
      "loss": 1.8613,
      "step": 130800
    },
    {
      "epoch": 10.25379915400282,
      "grad_norm": 12.221567153930664,
      "learning_rate": 4.1455167371664314e-05,
      "loss": 1.7272,
      "step": 130900
    },
    {
      "epoch": 10.26163246122513,
      "grad_norm": 5.7360053062438965,
      "learning_rate": 4.1448639615645726e-05,
      "loss": 1.7932,
      "step": 131000
    },
    {
      "epoch": 10.26946576844744,
      "grad_norm": 6.751584529876709,
      "learning_rate": 4.144211185962714e-05,
      "loss": 1.7649,
      "step": 131100
    },
    {
      "epoch": 10.277299075669747,
      "grad_norm": 5.70153284072876,
      "learning_rate": 4.1435584103608545e-05,
      "loss": 1.7889,
      "step": 131200
    },
    {
      "epoch": 10.285132382892057,
      "grad_norm": 7.831987380981445,
      "learning_rate": 4.142905634758996e-05,
      "loss": 1.8591,
      "step": 131300
    },
    {
      "epoch": 10.292965690114366,
      "grad_norm": 6.9767680168151855,
      "learning_rate": 4.142252859157136e-05,
      "loss": 1.7678,
      "step": 131400
    },
    {
      "epoch": 10.300798997336676,
      "grad_norm": 5.212942123413086,
      "learning_rate": 4.141600083555277e-05,
      "loss": 1.805,
      "step": 131500
    },
    {
      "epoch": 10.308632304558985,
      "grad_norm": 6.707123279571533,
      "learning_rate": 4.140947307953418e-05,
      "loss": 1.758,
      "step": 131600
    },
    {
      "epoch": 10.316465611781293,
      "grad_norm": 4.482352256774902,
      "learning_rate": 4.1402945323515594e-05,
      "loss": 1.7797,
      "step": 131700
    },
    {
      "epoch": 10.324298919003603,
      "grad_norm": 5.1539082527160645,
      "learning_rate": 4.1396417567497e-05,
      "loss": 1.7999,
      "step": 131800
    },
    {
      "epoch": 10.332132226225912,
      "grad_norm": 5.7061285972595215,
      "learning_rate": 4.138988981147841e-05,
      "loss": 1.8676,
      "step": 131900
    },
    {
      "epoch": 10.339965533448222,
      "grad_norm": 6.79160213470459,
      "learning_rate": 4.138336205545982e-05,
      "loss": 1.8467,
      "step": 132000
    },
    {
      "epoch": 10.347798840670531,
      "grad_norm": 6.879742622375488,
      "learning_rate": 4.1376834299441224e-05,
      "loss": 1.8111,
      "step": 132100
    },
    {
      "epoch": 10.355632147892841,
      "grad_norm": 5.569178581237793,
      "learning_rate": 4.1370306543422636e-05,
      "loss": 1.822,
      "step": 132200
    },
    {
      "epoch": 10.363465455115149,
      "grad_norm": 6.883254528045654,
      "learning_rate": 4.136377878740404e-05,
      "loss": 1.8249,
      "step": 132300
    },
    {
      "epoch": 10.371298762337458,
      "grad_norm": 5.352935791015625,
      "learning_rate": 4.1357251031385455e-05,
      "loss": 1.8628,
      "step": 132400
    },
    {
      "epoch": 10.379132069559768,
      "grad_norm": 4.357451915740967,
      "learning_rate": 4.135072327536686e-05,
      "loss": 1.8711,
      "step": 132500
    },
    {
      "epoch": 10.386965376782078,
      "grad_norm": 8.56338882446289,
      "learning_rate": 4.134419551934827e-05,
      "loss": 1.8621,
      "step": 132600
    },
    {
      "epoch": 10.394798684004387,
      "grad_norm": 5.381176471710205,
      "learning_rate": 4.133766776332968e-05,
      "loss": 1.8331,
      "step": 132700
    },
    {
      "epoch": 10.402631991226697,
      "grad_norm": 5.006015777587891,
      "learning_rate": 4.1331140007311084e-05,
      "loss": 1.8534,
      "step": 132800
    },
    {
      "epoch": 10.410465298449004,
      "grad_norm": 5.5936808586120605,
      "learning_rate": 4.13246122512925e-05,
      "loss": 1.8797,
      "step": 132900
    },
    {
      "epoch": 10.418298605671314,
      "grad_norm": 5.127411365509033,
      "learning_rate": 4.131808449527391e-05,
      "loss": 1.7909,
      "step": 133000
    },
    {
      "epoch": 10.426131912893624,
      "grad_norm": 6.403457164764404,
      "learning_rate": 4.1311556739255315e-05,
      "loss": 1.7675,
      "step": 133100
    },
    {
      "epoch": 10.433965220115933,
      "grad_norm": 5.213257789611816,
      "learning_rate": 4.130502898323673e-05,
      "loss": 1.827,
      "step": 133200
    },
    {
      "epoch": 10.441798527338243,
      "grad_norm": 6.42577600479126,
      "learning_rate": 4.1298501227218134e-05,
      "loss": 1.7615,
      "step": 133300
    },
    {
      "epoch": 10.449631834560552,
      "grad_norm": 6.277506351470947,
      "learning_rate": 4.129197347119954e-05,
      "loss": 1.8344,
      "step": 133400
    },
    {
      "epoch": 10.45746514178286,
      "grad_norm": 5.725212097167969,
      "learning_rate": 4.128544571518095e-05,
      "loss": 1.8107,
      "step": 133500
    },
    {
      "epoch": 10.46529844900517,
      "grad_norm": 5.049173355102539,
      "learning_rate": 4.127891795916236e-05,
      "loss": 1.8493,
      "step": 133600
    },
    {
      "epoch": 10.47313175622748,
      "grad_norm": 6.295303821563721,
      "learning_rate": 4.127239020314377e-05,
      "loss": 1.7091,
      "step": 133700
    },
    {
      "epoch": 10.480965063449789,
      "grad_norm": 4.675457000732422,
      "learning_rate": 4.126586244712518e-05,
      "loss": 1.8689,
      "step": 133800
    },
    {
      "epoch": 10.488798370672098,
      "grad_norm": 7.600100994110107,
      "learning_rate": 4.125933469110659e-05,
      "loss": 1.8284,
      "step": 133900
    },
    {
      "epoch": 10.496631677894406,
      "grad_norm": 4.172295093536377,
      "learning_rate": 4.1252806935087994e-05,
      "loss": 1.7783,
      "step": 134000
    },
    {
      "epoch": 10.504464985116716,
      "grad_norm": 5.960208415985107,
      "learning_rate": 4.12462791790694e-05,
      "loss": 1.8871,
      "step": 134100
    },
    {
      "epoch": 10.512298292339025,
      "grad_norm": 6.744691848754883,
      "learning_rate": 4.123975142305081e-05,
      "loss": 1.8641,
      "step": 134200
    },
    {
      "epoch": 10.520131599561335,
      "grad_norm": 6.161773204803467,
      "learning_rate": 4.1233223667032225e-05,
      "loss": 1.8972,
      "step": 134300
    },
    {
      "epoch": 10.527964906783644,
      "grad_norm": 5.1605305671691895,
      "learning_rate": 4.122669591101363e-05,
      "loss": 1.8144,
      "step": 134400
    },
    {
      "epoch": 10.535798214005954,
      "grad_norm": 6.866400241851807,
      "learning_rate": 4.1220168154995043e-05,
      "loss": 1.8561,
      "step": 134500
    },
    {
      "epoch": 10.543631521228262,
      "grad_norm": 5.715341091156006,
      "learning_rate": 4.121364039897645e-05,
      "loss": 1.8752,
      "step": 134600
    },
    {
      "epoch": 10.551464828450571,
      "grad_norm": 5.64078950881958,
      "learning_rate": 4.1207112642957855e-05,
      "loss": 1.9046,
      "step": 134700
    },
    {
      "epoch": 10.559298135672881,
      "grad_norm": 5.361617565155029,
      "learning_rate": 4.120058488693927e-05,
      "loss": 1.7823,
      "step": 134800
    },
    {
      "epoch": 10.56713144289519,
      "grad_norm": 6.341736316680908,
      "learning_rate": 4.119405713092068e-05,
      "loss": 1.7846,
      "step": 134900
    },
    {
      "epoch": 10.5749647501175,
      "grad_norm": 5.731029033660889,
      "learning_rate": 4.1187529374902086e-05,
      "loss": 1.8081,
      "step": 135000
    },
    {
      "epoch": 10.582798057339808,
      "grad_norm": 5.48651647567749,
      "learning_rate": 4.11810016188835e-05,
      "loss": 1.7579,
      "step": 135100
    },
    {
      "epoch": 10.590631364562118,
      "grad_norm": 5.659520149230957,
      "learning_rate": 4.1174473862864904e-05,
      "loss": 1.7637,
      "step": 135200
    },
    {
      "epoch": 10.598464671784427,
      "grad_norm": 7.333294868469238,
      "learning_rate": 4.116794610684631e-05,
      "loss": 1.8511,
      "step": 135300
    },
    {
      "epoch": 10.606297979006737,
      "grad_norm": 6.013121128082275,
      "learning_rate": 4.116141835082772e-05,
      "loss": 1.7502,
      "step": 135400
    },
    {
      "epoch": 10.614131286229046,
      "grad_norm": 5.636725425720215,
      "learning_rate": 4.115489059480913e-05,
      "loss": 1.7904,
      "step": 135500
    },
    {
      "epoch": 10.621964593451356,
      "grad_norm": 6.980792999267578,
      "learning_rate": 4.114836283879054e-05,
      "loss": 1.7855,
      "step": 135600
    },
    {
      "epoch": 10.629797900673664,
      "grad_norm": 5.295610427856445,
      "learning_rate": 4.1141835082771947e-05,
      "loss": 1.7971,
      "step": 135700
    },
    {
      "epoch": 10.637631207895973,
      "grad_norm": 5.504050254821777,
      "learning_rate": 4.113530732675336e-05,
      "loss": 1.8653,
      "step": 135800
    },
    {
      "epoch": 10.645464515118283,
      "grad_norm": 5.201179504394531,
      "learning_rate": 4.1128779570734765e-05,
      "loss": 1.7698,
      "step": 135900
    },
    {
      "epoch": 10.653297822340592,
      "grad_norm": 6.029948711395264,
      "learning_rate": 4.112225181471617e-05,
      "loss": 1.8704,
      "step": 136000
    },
    {
      "epoch": 10.661131129562902,
      "grad_norm": 6.4137983322143555,
      "learning_rate": 4.111572405869758e-05,
      "loss": 1.903,
      "step": 136100
    },
    {
      "epoch": 10.668964436785211,
      "grad_norm": 4.929981708526611,
      "learning_rate": 4.1109196302678996e-05,
      "loss": 1.6755,
      "step": 136200
    },
    {
      "epoch": 10.67679774400752,
      "grad_norm": 6.284608840942383,
      "learning_rate": 4.11026685466604e-05,
      "loss": 1.7519,
      "step": 136300
    },
    {
      "epoch": 10.684631051229829,
      "grad_norm": 4.95701789855957,
      "learning_rate": 4.1096140790641814e-05,
      "loss": 1.7777,
      "step": 136400
    },
    {
      "epoch": 10.692464358452138,
      "grad_norm": 3.9064671993255615,
      "learning_rate": 4.108961303462322e-05,
      "loss": 1.7789,
      "step": 136500
    },
    {
      "epoch": 10.700297665674448,
      "grad_norm": 3.8450419902801514,
      "learning_rate": 4.1083085278604626e-05,
      "loss": 1.6623,
      "step": 136600
    },
    {
      "epoch": 10.708130972896758,
      "grad_norm": 5.638415813446045,
      "learning_rate": 4.107655752258604e-05,
      "loss": 1.8699,
      "step": 136700
    },
    {
      "epoch": 10.715964280119067,
      "grad_norm": 6.571872234344482,
      "learning_rate": 4.1070029766567444e-05,
      "loss": 1.8353,
      "step": 136800
    },
    {
      "epoch": 10.723797587341375,
      "grad_norm": 5.612318515777588,
      "learning_rate": 4.1063502010548856e-05,
      "loss": 1.8461,
      "step": 136900
    },
    {
      "epoch": 10.731630894563684,
      "grad_norm": 4.191009521484375,
      "learning_rate": 4.105697425453027e-05,
      "loss": 1.8984,
      "step": 137000
    },
    {
      "epoch": 10.739464201785994,
      "grad_norm": 5.16853141784668,
      "learning_rate": 4.1050446498511675e-05,
      "loss": 1.8146,
      "step": 137100
    },
    {
      "epoch": 10.747297509008304,
      "grad_norm": 6.409775733947754,
      "learning_rate": 4.104391874249308e-05,
      "loss": 1.8559,
      "step": 137200
    },
    {
      "epoch": 10.755130816230613,
      "grad_norm": 7.037043571472168,
      "learning_rate": 4.1037390986474486e-05,
      "loss": 1.8163,
      "step": 137300
    },
    {
      "epoch": 10.762964123452921,
      "grad_norm": 4.599850177764893,
      "learning_rate": 4.10308632304559e-05,
      "loss": 1.896,
      "step": 137400
    },
    {
      "epoch": 10.77079743067523,
      "grad_norm": 5.5130391120910645,
      "learning_rate": 4.102433547443731e-05,
      "loss": 1.9096,
      "step": 137500
    },
    {
      "epoch": 10.77863073789754,
      "grad_norm": 5.4946393966674805,
      "learning_rate": 4.101780771841872e-05,
      "loss": 1.8444,
      "step": 137600
    },
    {
      "epoch": 10.78646404511985,
      "grad_norm": 6.057858943939209,
      "learning_rate": 4.101127996240013e-05,
      "loss": 1.8366,
      "step": 137700
    },
    {
      "epoch": 10.79429735234216,
      "grad_norm": 7.897280216217041,
      "learning_rate": 4.1004752206381535e-05,
      "loss": 1.8477,
      "step": 137800
    },
    {
      "epoch": 10.802130659564469,
      "grad_norm": 5.9645819664001465,
      "learning_rate": 4.099822445036294e-05,
      "loss": 1.787,
      "step": 137900
    },
    {
      "epoch": 10.809963966786777,
      "grad_norm": 5.882702350616455,
      "learning_rate": 4.0991696694344354e-05,
      "loss": 1.7537,
      "step": 138000
    },
    {
      "epoch": 10.817797274009086,
      "grad_norm": 6.878829002380371,
      "learning_rate": 4.0985168938325766e-05,
      "loss": 1.8672,
      "step": 138100
    },
    {
      "epoch": 10.825630581231396,
      "grad_norm": 4.265238285064697,
      "learning_rate": 4.097864118230717e-05,
      "loss": 1.7693,
      "step": 138200
    },
    {
      "epoch": 10.833463888453705,
      "grad_norm": 6.687096118927002,
      "learning_rate": 4.0972113426288585e-05,
      "loss": 1.8101,
      "step": 138300
    },
    {
      "epoch": 10.841297195676015,
      "grad_norm": 7.675390720367432,
      "learning_rate": 4.096558567026999e-05,
      "loss": 1.8721,
      "step": 138400
    },
    {
      "epoch": 10.849130502898324,
      "grad_norm": 5.146328926086426,
      "learning_rate": 4.0959057914251396e-05,
      "loss": 1.7711,
      "step": 138500
    },
    {
      "epoch": 10.856963810120632,
      "grad_norm": 6.188024044036865,
      "learning_rate": 4.095253015823281e-05,
      "loss": 1.8587,
      "step": 138600
    },
    {
      "epoch": 10.864797117342942,
      "grad_norm": 6.320728778839111,
      "learning_rate": 4.0946002402214214e-05,
      "loss": 1.7554,
      "step": 138700
    },
    {
      "epoch": 10.872630424565251,
      "grad_norm": 5.183318138122559,
      "learning_rate": 4.093947464619563e-05,
      "loss": 1.7923,
      "step": 138800
    },
    {
      "epoch": 10.880463731787561,
      "grad_norm": 6.491761684417725,
      "learning_rate": 4.093294689017704e-05,
      "loss": 1.8062,
      "step": 138900
    },
    {
      "epoch": 10.88829703900987,
      "grad_norm": 5.208432674407959,
      "learning_rate": 4.0926419134158445e-05,
      "loss": 1.8067,
      "step": 139000
    },
    {
      "epoch": 10.89613034623218,
      "grad_norm": 6.596574783325195,
      "learning_rate": 4.091989137813985e-05,
      "loss": 1.8566,
      "step": 139100
    },
    {
      "epoch": 10.903963653454488,
      "grad_norm": 6.425262451171875,
      "learning_rate": 4.091336362212126e-05,
      "loss": 1.8174,
      "step": 139200
    },
    {
      "epoch": 10.911796960676797,
      "grad_norm": 5.092564582824707,
      "learning_rate": 4.090683586610267e-05,
      "loss": 1.8682,
      "step": 139300
    },
    {
      "epoch": 10.919630267899107,
      "grad_norm": 4.598048210144043,
      "learning_rate": 4.090030811008408e-05,
      "loss": 1.7998,
      "step": 139400
    },
    {
      "epoch": 10.927463575121417,
      "grad_norm": 8.025739669799805,
      "learning_rate": 4.089378035406549e-05,
      "loss": 1.9103,
      "step": 139500
    },
    {
      "epoch": 10.935296882343726,
      "grad_norm": 5.954856872558594,
      "learning_rate": 4.08872525980469e-05,
      "loss": 1.8703,
      "step": 139600
    },
    {
      "epoch": 10.943130189566034,
      "grad_norm": 6.904559135437012,
      "learning_rate": 4.0880724842028306e-05,
      "loss": 1.7642,
      "step": 139700
    },
    {
      "epoch": 10.950963496788344,
      "grad_norm": 6.9494242668151855,
      "learning_rate": 4.087419708600971e-05,
      "loss": 1.8449,
      "step": 139800
    },
    {
      "epoch": 10.958796804010653,
      "grad_norm": 4.73665189743042,
      "learning_rate": 4.0867669329991124e-05,
      "loss": 1.661,
      "step": 139900
    },
    {
      "epoch": 10.966630111232963,
      "grad_norm": 5.895406246185303,
      "learning_rate": 4.086114157397253e-05,
      "loss": 1.7299,
      "step": 140000
    },
    {
      "epoch": 10.974463418455272,
      "grad_norm": 5.435400485992432,
      "learning_rate": 4.085461381795394e-05,
      "loss": 1.7683,
      "step": 140100
    },
    {
      "epoch": 10.982296725677582,
      "grad_norm": 6.117864608764648,
      "learning_rate": 4.0848086061935355e-05,
      "loss": 1.7961,
      "step": 140200
    },
    {
      "epoch": 10.99013003289989,
      "grad_norm": 5.236634731292725,
      "learning_rate": 4.084155830591676e-05,
      "loss": 1.7379,
      "step": 140300
    },
    {
      "epoch": 10.9979633401222,
      "grad_norm": 7.199580669403076,
      "learning_rate": 4.083503054989817e-05,
      "loss": 1.8518,
      "step": 140400
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.819706678390503,
      "eval_runtime": 1.5479,
      "eval_samples_per_second": 434.132,
      "eval_steps_per_second": 434.132,
      "step": 140426
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.582922339439392,
      "eval_runtime": 28.8976,
      "eval_samples_per_second": 441.767,
      "eval_steps_per_second": 441.767,
      "step": 140426
    },
    {
      "epoch": 11.005796647344509,
      "grad_norm": 6.865828990936279,
      "learning_rate": 4.082850279387958e-05,
      "loss": 1.8398,
      "step": 140500
    },
    {
      "epoch": 11.013629954566818,
      "grad_norm": 4.296030521392822,
      "learning_rate": 4.0821975037860985e-05,
      "loss": 1.7371,
      "step": 140600
    },
    {
      "epoch": 11.021463261789128,
      "grad_norm": 6.55327033996582,
      "learning_rate": 4.08154472818424e-05,
      "loss": 1.7732,
      "step": 140700
    },
    {
      "epoch": 11.029296569011438,
      "grad_norm": 4.12326717376709,
      "learning_rate": 4.08089195258238e-05,
      "loss": 1.7452,
      "step": 140800
    },
    {
      "epoch": 11.037129876233745,
      "grad_norm": 6.661127090454102,
      "learning_rate": 4.0802391769805216e-05,
      "loss": 1.7186,
      "step": 140900
    },
    {
      "epoch": 11.044963183456055,
      "grad_norm": 4.872840404510498,
      "learning_rate": 4.079586401378662e-05,
      "loss": 1.7531,
      "step": 141000
    },
    {
      "epoch": 11.052796490678364,
      "grad_norm": 5.78319787979126,
      "learning_rate": 4.078933625776803e-05,
      "loss": 1.7074,
      "step": 141100
    },
    {
      "epoch": 11.060629797900674,
      "grad_norm": 6.225964069366455,
      "learning_rate": 4.078280850174944e-05,
      "loss": 1.85,
      "step": 141200
    },
    {
      "epoch": 11.068463105122984,
      "grad_norm": 5.310545921325684,
      "learning_rate": 4.077628074573085e-05,
      "loss": 1.716,
      "step": 141300
    },
    {
      "epoch": 11.076296412345291,
      "grad_norm": 6.5781941413879395,
      "learning_rate": 4.076975298971226e-05,
      "loss": 1.801,
      "step": 141400
    },
    {
      "epoch": 11.084129719567601,
      "grad_norm": 6.026918888092041,
      "learning_rate": 4.076322523369367e-05,
      "loss": 1.7175,
      "step": 141500
    },
    {
      "epoch": 11.09196302678991,
      "grad_norm": 5.33634090423584,
      "learning_rate": 4.075669747767508e-05,
      "loss": 1.8913,
      "step": 141600
    },
    {
      "epoch": 11.09979633401222,
      "grad_norm": 3.580319881439209,
      "learning_rate": 4.075016972165648e-05,
      "loss": 1.7581,
      "step": 141700
    },
    {
      "epoch": 11.10762964123453,
      "grad_norm": 5.3394365310668945,
      "learning_rate": 4.0743641965637895e-05,
      "loss": 1.7545,
      "step": 141800
    },
    {
      "epoch": 11.11546294845684,
      "grad_norm": 6.720604419708252,
      "learning_rate": 4.07371142096193e-05,
      "loss": 1.7362,
      "step": 141900
    },
    {
      "epoch": 11.123296255679147,
      "grad_norm": 5.692006587982178,
      "learning_rate": 4.073058645360071e-05,
      "loss": 1.7508,
      "step": 142000
    },
    {
      "epoch": 11.131129562901457,
      "grad_norm": 4.332099914550781,
      "learning_rate": 4.0724058697582126e-05,
      "loss": 1.8219,
      "step": 142100
    },
    {
      "epoch": 11.138962870123766,
      "grad_norm": 3.939582109451294,
      "learning_rate": 4.071753094156353e-05,
      "loss": 1.6832,
      "step": 142200
    },
    {
      "epoch": 11.146796177346076,
      "grad_norm": 4.854315757751465,
      "learning_rate": 4.071100318554494e-05,
      "loss": 1.8021,
      "step": 142300
    },
    {
      "epoch": 11.154629484568385,
      "grad_norm": 8.663337707519531,
      "learning_rate": 4.070447542952634e-05,
      "loss": 1.8632,
      "step": 142400
    },
    {
      "epoch": 11.162462791790695,
      "grad_norm": 6.87011194229126,
      "learning_rate": 4.0697947673507756e-05,
      "loss": 1.7946,
      "step": 142500
    },
    {
      "epoch": 11.170296099013003,
      "grad_norm": 5.655714988708496,
      "learning_rate": 4.069141991748917e-05,
      "loss": 1.7857,
      "step": 142600
    },
    {
      "epoch": 11.178129406235312,
      "grad_norm": 5.475968837738037,
      "learning_rate": 4.0684892161470574e-05,
      "loss": 1.811,
      "step": 142700
    },
    {
      "epoch": 11.185962713457622,
      "grad_norm": 5.876007556915283,
      "learning_rate": 4.0678364405451986e-05,
      "loss": 1.7726,
      "step": 142800
    },
    {
      "epoch": 11.193796020679931,
      "grad_norm": 5.970873832702637,
      "learning_rate": 4.067183664943339e-05,
      "loss": 1.8426,
      "step": 142900
    },
    {
      "epoch": 11.201629327902241,
      "grad_norm": 5.0345306396484375,
      "learning_rate": 4.06653088934148e-05,
      "loss": 1.786,
      "step": 143000
    },
    {
      "epoch": 11.209462635124549,
      "grad_norm": 4.879459381103516,
      "learning_rate": 4.065878113739621e-05,
      "loss": 1.9292,
      "step": 143100
    },
    {
      "epoch": 11.217295942346858,
      "grad_norm": 6.118165969848633,
      "learning_rate": 4.0652253381377616e-05,
      "loss": 1.7881,
      "step": 143200
    },
    {
      "epoch": 11.225129249569168,
      "grad_norm": 7.034386157989502,
      "learning_rate": 4.064572562535903e-05,
      "loss": 1.8119,
      "step": 143300
    },
    {
      "epoch": 11.232962556791477,
      "grad_norm": 6.010686874389648,
      "learning_rate": 4.063919786934044e-05,
      "loss": 1.7197,
      "step": 143400
    },
    {
      "epoch": 11.240795864013787,
      "grad_norm": 6.787762641906738,
      "learning_rate": 4.063267011332185e-05,
      "loss": 1.7533,
      "step": 143500
    },
    {
      "epoch": 11.248629171236097,
      "grad_norm": 5.76862096786499,
      "learning_rate": 4.062614235730325e-05,
      "loss": 1.7406,
      "step": 143600
    },
    {
      "epoch": 11.256462478458404,
      "grad_norm": 5.118156433105469,
      "learning_rate": 4.0619614601284666e-05,
      "loss": 1.8796,
      "step": 143700
    },
    {
      "epoch": 11.264295785680714,
      "grad_norm": 4.987014293670654,
      "learning_rate": 4.061308684526607e-05,
      "loss": 1.8035,
      "step": 143800
    },
    {
      "epoch": 11.272129092903024,
      "grad_norm": 6.648799896240234,
      "learning_rate": 4.0606559089247484e-05,
      "loss": 1.7923,
      "step": 143900
    },
    {
      "epoch": 11.279962400125333,
      "grad_norm": 6.054924011230469,
      "learning_rate": 4.060003133322889e-05,
      "loss": 1.7063,
      "step": 144000
    },
    {
      "epoch": 11.287795707347643,
      "grad_norm": 5.732301712036133,
      "learning_rate": 4.05935035772103e-05,
      "loss": 1.7981,
      "step": 144100
    },
    {
      "epoch": 11.295629014569952,
      "grad_norm": 5.220285892486572,
      "learning_rate": 4.058697582119171e-05,
      "loss": 1.7365,
      "step": 144200
    },
    {
      "epoch": 11.30346232179226,
      "grad_norm": 6.989729404449463,
      "learning_rate": 4.0580448065173114e-05,
      "loss": 1.7881,
      "step": 144300
    },
    {
      "epoch": 11.31129562901457,
      "grad_norm": 4.829203128814697,
      "learning_rate": 4.0573920309154526e-05,
      "loss": 1.7857,
      "step": 144400
    },
    {
      "epoch": 11.31912893623688,
      "grad_norm": 5.143613338470459,
      "learning_rate": 4.056739255313594e-05,
      "loss": 1.7984,
      "step": 144500
    },
    {
      "epoch": 11.326962243459189,
      "grad_norm": 6.517760276794434,
      "learning_rate": 4.0560864797117345e-05,
      "loss": 1.8058,
      "step": 144600
    },
    {
      "epoch": 11.334795550681498,
      "grad_norm": 6.360096454620361,
      "learning_rate": 4.055433704109876e-05,
      "loss": 1.7333,
      "step": 144700
    },
    {
      "epoch": 11.342628857903806,
      "grad_norm": 5.759018898010254,
      "learning_rate": 4.054780928508016e-05,
      "loss": 1.8392,
      "step": 144800
    },
    {
      "epoch": 11.350462165126116,
      "grad_norm": 4.188584804534912,
      "learning_rate": 4.054128152906157e-05,
      "loss": 1.7865,
      "step": 144900
    },
    {
      "epoch": 11.358295472348425,
      "grad_norm": 8.13226318359375,
      "learning_rate": 4.053475377304298e-05,
      "loss": 1.9026,
      "step": 145000
    },
    {
      "epoch": 11.366128779570735,
      "grad_norm": 7.292531490325928,
      "learning_rate": 4.052822601702439e-05,
      "loss": 1.7977,
      "step": 145100
    },
    {
      "epoch": 11.373962086793044,
      "grad_norm": 6.455923080444336,
      "learning_rate": 4.05216982610058e-05,
      "loss": 1.7741,
      "step": 145200
    },
    {
      "epoch": 11.381795394015354,
      "grad_norm": 6.882816791534424,
      "learning_rate": 4.051517050498721e-05,
      "loss": 1.7479,
      "step": 145300
    },
    {
      "epoch": 11.389628701237662,
      "grad_norm": 5.219517230987549,
      "learning_rate": 4.050864274896862e-05,
      "loss": 1.8045,
      "step": 145400
    },
    {
      "epoch": 11.397462008459971,
      "grad_norm": 5.637507438659668,
      "learning_rate": 4.0502114992950024e-05,
      "loss": 1.8578,
      "step": 145500
    },
    {
      "epoch": 11.405295315682281,
      "grad_norm": 5.20682430267334,
      "learning_rate": 4.0495587236931436e-05,
      "loss": 1.7612,
      "step": 145600
    },
    {
      "epoch": 11.41312862290459,
      "grad_norm": 4.717574119567871,
      "learning_rate": 4.048905948091284e-05,
      "loss": 1.8328,
      "step": 145700
    },
    {
      "epoch": 11.4209619301269,
      "grad_norm": 5.5832014083862305,
      "learning_rate": 4.0482531724894254e-05,
      "loss": 1.7683,
      "step": 145800
    },
    {
      "epoch": 11.42879523734921,
      "grad_norm": 6.429770469665527,
      "learning_rate": 4.047600396887566e-05,
      "loss": 1.7993,
      "step": 145900
    },
    {
      "epoch": 11.436628544571517,
      "grad_norm": 6.617941379547119,
      "learning_rate": 4.046947621285707e-05,
      "loss": 1.7143,
      "step": 146000
    },
    {
      "epoch": 11.444461851793827,
      "grad_norm": 5.2493743896484375,
      "learning_rate": 4.046294845683848e-05,
      "loss": 1.8799,
      "step": 146100
    },
    {
      "epoch": 11.452295159016137,
      "grad_norm": 7.384444713592529,
      "learning_rate": 4.0456420700819884e-05,
      "loss": 1.8048,
      "step": 146200
    },
    {
      "epoch": 11.460128466238446,
      "grad_norm": 7.414995193481445,
      "learning_rate": 4.04498929448013e-05,
      "loss": 1.8233,
      "step": 146300
    },
    {
      "epoch": 11.467961773460756,
      "grad_norm": 7.223167896270752,
      "learning_rate": 4.04433651887827e-05,
      "loss": 1.8569,
      "step": 146400
    },
    {
      "epoch": 11.475795080683064,
      "grad_norm": 5.120899677276611,
      "learning_rate": 4.0436837432764115e-05,
      "loss": 1.8506,
      "step": 146500
    },
    {
      "epoch": 11.483628387905373,
      "grad_norm": 7.958164691925049,
      "learning_rate": 4.043030967674553e-05,
      "loss": 1.8336,
      "step": 146600
    },
    {
      "epoch": 11.491461695127683,
      "grad_norm": 6.4943766593933105,
      "learning_rate": 4.0423781920726933e-05,
      "loss": 1.751,
      "step": 146700
    },
    {
      "epoch": 11.499295002349992,
      "grad_norm": 4.5727763175964355,
      "learning_rate": 4.041725416470834e-05,
      "loss": 1.8712,
      "step": 146800
    },
    {
      "epoch": 11.507128309572302,
      "grad_norm": 6.494313716888428,
      "learning_rate": 4.041072640868975e-05,
      "loss": 1.8129,
      "step": 146900
    },
    {
      "epoch": 11.514961616794611,
      "grad_norm": 6.648395538330078,
      "learning_rate": 4.040419865267116e-05,
      "loss": 1.8768,
      "step": 147000
    },
    {
      "epoch": 11.52279492401692,
      "grad_norm": 7.630270004272461,
      "learning_rate": 4.039767089665257e-05,
      "loss": 1.689,
      "step": 147100
    },
    {
      "epoch": 11.530628231239229,
      "grad_norm": 5.806258678436279,
      "learning_rate": 4.039114314063398e-05,
      "loss": 1.7355,
      "step": 147200
    },
    {
      "epoch": 11.538461538461538,
      "grad_norm": 5.4578857421875,
      "learning_rate": 4.038461538461539e-05,
      "loss": 1.8509,
      "step": 147300
    },
    {
      "epoch": 11.546294845683848,
      "grad_norm": 7.08149528503418,
      "learning_rate": 4.0378087628596794e-05,
      "loss": 1.7787,
      "step": 147400
    },
    {
      "epoch": 11.554128152906157,
      "grad_norm": 6.8232221603393555,
      "learning_rate": 4.03715598725782e-05,
      "loss": 1.7683,
      "step": 147500
    },
    {
      "epoch": 11.561961460128467,
      "grad_norm": 5.5927324295043945,
      "learning_rate": 4.036503211655961e-05,
      "loss": 1.7594,
      "step": 147600
    },
    {
      "epoch": 11.569794767350775,
      "grad_norm": 5.980330944061279,
      "learning_rate": 4.0358504360541025e-05,
      "loss": 1.7908,
      "step": 147700
    },
    {
      "epoch": 11.577628074573084,
      "grad_norm": 7.182455539703369,
      "learning_rate": 4.035197660452243e-05,
      "loss": 1.7697,
      "step": 147800
    },
    {
      "epoch": 11.585461381795394,
      "grad_norm": 6.695959091186523,
      "learning_rate": 4.034544884850384e-05,
      "loss": 1.9645,
      "step": 147900
    },
    {
      "epoch": 11.593294689017704,
      "grad_norm": 5.364059925079346,
      "learning_rate": 4.033892109248525e-05,
      "loss": 1.8035,
      "step": 148000
    },
    {
      "epoch": 11.601127996240013,
      "grad_norm": 8.689693450927734,
      "learning_rate": 4.0332393336466655e-05,
      "loss": 1.8138,
      "step": 148100
    },
    {
      "epoch": 11.608961303462323,
      "grad_norm": 4.586217403411865,
      "learning_rate": 4.032586558044807e-05,
      "loss": 1.8099,
      "step": 148200
    },
    {
      "epoch": 11.61679461068463,
      "grad_norm": 6.9250688552856445,
      "learning_rate": 4.031933782442947e-05,
      "loss": 1.7039,
      "step": 148300
    },
    {
      "epoch": 11.62462791790694,
      "grad_norm": 5.215100288391113,
      "learning_rate": 4.0312810068410886e-05,
      "loss": 1.7498,
      "step": 148400
    },
    {
      "epoch": 11.63246122512925,
      "grad_norm": 6.43070650100708,
      "learning_rate": 4.03062823123923e-05,
      "loss": 1.7169,
      "step": 148500
    },
    {
      "epoch": 11.64029453235156,
      "grad_norm": 7.56526517868042,
      "learning_rate": 4.0299754556373704e-05,
      "loss": 1.7959,
      "step": 148600
    },
    {
      "epoch": 11.648127839573869,
      "grad_norm": 5.995515823364258,
      "learning_rate": 4.029322680035511e-05,
      "loss": 1.8712,
      "step": 148700
    },
    {
      "epoch": 11.655961146796177,
      "grad_norm": 4.726562023162842,
      "learning_rate": 4.028669904433652e-05,
      "loss": 1.8523,
      "step": 148800
    },
    {
      "epoch": 11.663794454018486,
      "grad_norm": 6.42636251449585,
      "learning_rate": 4.028017128831793e-05,
      "loss": 1.7061,
      "step": 148900
    },
    {
      "epoch": 11.671627761240796,
      "grad_norm": 4.863781452178955,
      "learning_rate": 4.027364353229934e-05,
      "loss": 1.7778,
      "step": 149000
    },
    {
      "epoch": 11.679461068463105,
      "grad_norm": 6.920504093170166,
      "learning_rate": 4.0267115776280746e-05,
      "loss": 1.875,
      "step": 149100
    },
    {
      "epoch": 11.687294375685415,
      "grad_norm": 6.494094371795654,
      "learning_rate": 4.026058802026216e-05,
      "loss": 1.7875,
      "step": 149200
    },
    {
      "epoch": 11.695127682907724,
      "grad_norm": 4.404669761657715,
      "learning_rate": 4.0254060264243565e-05,
      "loss": 1.6733,
      "step": 149300
    },
    {
      "epoch": 11.702960990130032,
      "grad_norm": 6.504284381866455,
      "learning_rate": 4.024753250822497e-05,
      "loss": 1.7009,
      "step": 149400
    },
    {
      "epoch": 11.710794297352342,
      "grad_norm": 5.513644695281982,
      "learning_rate": 4.024100475220638e-05,
      "loss": 1.926,
      "step": 149500
    },
    {
      "epoch": 11.718627604574651,
      "grad_norm": 4.627826690673828,
      "learning_rate": 4.023447699618779e-05,
      "loss": 1.8408,
      "step": 149600
    },
    {
      "epoch": 11.726460911796961,
      "grad_norm": 6.623087406158447,
      "learning_rate": 4.02279492401692e-05,
      "loss": 1.7804,
      "step": 149700
    },
    {
      "epoch": 11.73429421901927,
      "grad_norm": 5.806681156158447,
      "learning_rate": 4.0221421484150614e-05,
      "loss": 1.8399,
      "step": 149800
    },
    {
      "epoch": 11.742127526241578,
      "grad_norm": 4.6492390632629395,
      "learning_rate": 4.021489372813202e-05,
      "loss": 1.8487,
      "step": 149900
    },
    {
      "epoch": 11.749960833463888,
      "grad_norm": 7.973198890686035,
      "learning_rate": 4.0208365972113425e-05,
      "loss": 1.8418,
      "step": 150000
    },
    {
      "epoch": 11.757794140686197,
      "grad_norm": 5.859240531921387,
      "learning_rate": 4.020183821609484e-05,
      "loss": 1.7727,
      "step": 150100
    },
    {
      "epoch": 11.765627447908507,
      "grad_norm": 5.6193528175354,
      "learning_rate": 4.0195310460076244e-05,
      "loss": 1.7617,
      "step": 150200
    },
    {
      "epoch": 11.773460755130817,
      "grad_norm": 5.815202713012695,
      "learning_rate": 4.0188782704057656e-05,
      "loss": 1.9163,
      "step": 150300
    },
    {
      "epoch": 11.781294062353126,
      "grad_norm": 6.60723352432251,
      "learning_rate": 4.018225494803907e-05,
      "loss": 1.8559,
      "step": 150400
    },
    {
      "epoch": 11.789127369575434,
      "grad_norm": 4.079840183258057,
      "learning_rate": 4.0175727192020475e-05,
      "loss": 1.7759,
      "step": 150500
    },
    {
      "epoch": 11.796960676797744,
      "grad_norm": 5.4511919021606445,
      "learning_rate": 4.016919943600188e-05,
      "loss": 1.7653,
      "step": 150600
    },
    {
      "epoch": 11.804793984020053,
      "grad_norm": 5.891178131103516,
      "learning_rate": 4.016267167998329e-05,
      "loss": 1.8273,
      "step": 150700
    },
    {
      "epoch": 11.812627291242363,
      "grad_norm": 5.408430099487305,
      "learning_rate": 4.01561439239647e-05,
      "loss": 1.8722,
      "step": 150800
    },
    {
      "epoch": 11.820460598464672,
      "grad_norm": 5.110166549682617,
      "learning_rate": 4.014961616794611e-05,
      "loss": 1.8338,
      "step": 150900
    },
    {
      "epoch": 11.828293905686982,
      "grad_norm": 7.54543399810791,
      "learning_rate": 4.014308841192752e-05,
      "loss": 1.73,
      "step": 151000
    },
    {
      "epoch": 11.83612721290929,
      "grad_norm": 5.36732816696167,
      "learning_rate": 4.013656065590893e-05,
      "loss": 1.8961,
      "step": 151100
    },
    {
      "epoch": 11.8439605201316,
      "grad_norm": 5.563138961791992,
      "learning_rate": 4.0130032899890335e-05,
      "loss": 1.7857,
      "step": 151200
    },
    {
      "epoch": 11.851793827353909,
      "grad_norm": 7.834105014801025,
      "learning_rate": 4.012350514387174e-05,
      "loss": 1.8338,
      "step": 151300
    },
    {
      "epoch": 11.859627134576218,
      "grad_norm": 6.715561389923096,
      "learning_rate": 4.0116977387853154e-05,
      "loss": 1.8487,
      "step": 151400
    },
    {
      "epoch": 11.867460441798528,
      "grad_norm": 6.1686296463012695,
      "learning_rate": 4.011044963183456e-05,
      "loss": 1.821,
      "step": 151500
    },
    {
      "epoch": 11.875293749020837,
      "grad_norm": 6.199481010437012,
      "learning_rate": 4.010392187581597e-05,
      "loss": 1.9887,
      "step": 151600
    },
    {
      "epoch": 11.883127056243145,
      "grad_norm": 6.6570258140563965,
      "learning_rate": 4.0097394119797384e-05,
      "loss": 1.8265,
      "step": 151700
    },
    {
      "epoch": 11.890960363465455,
      "grad_norm": 5.7306976318359375,
      "learning_rate": 4.009086636377879e-05,
      "loss": 1.7929,
      "step": 151800
    },
    {
      "epoch": 11.898793670687764,
      "grad_norm": 5.712580680847168,
      "learning_rate": 4.0084338607760196e-05,
      "loss": 1.9083,
      "step": 151900
    },
    {
      "epoch": 11.906626977910074,
      "grad_norm": 6.7840576171875,
      "learning_rate": 4.007781085174161e-05,
      "loss": 1.8039,
      "step": 152000
    },
    {
      "epoch": 11.914460285132384,
      "grad_norm": 6.267805099487305,
      "learning_rate": 4.0071283095723014e-05,
      "loss": 1.8412,
      "step": 152100
    },
    {
      "epoch": 11.922293592354691,
      "grad_norm": 5.946615695953369,
      "learning_rate": 4.006475533970443e-05,
      "loss": 1.8889,
      "step": 152200
    },
    {
      "epoch": 11.930126899577001,
      "grad_norm": 6.2327775955200195,
      "learning_rate": 4.005822758368584e-05,
      "loss": 1.7617,
      "step": 152300
    },
    {
      "epoch": 11.93796020679931,
      "grad_norm": 5.441863059997559,
      "learning_rate": 4.0051699827667245e-05,
      "loss": 1.9004,
      "step": 152400
    },
    {
      "epoch": 11.94579351402162,
      "grad_norm": 7.149537563323975,
      "learning_rate": 4.004517207164865e-05,
      "loss": 1.7968,
      "step": 152500
    },
    {
      "epoch": 11.95362682124393,
      "grad_norm": 6.05963134765625,
      "learning_rate": 4.003864431563006e-05,
      "loss": 1.7872,
      "step": 152600
    },
    {
      "epoch": 11.96146012846624,
      "grad_norm": 6.592235088348389,
      "learning_rate": 4.003211655961147e-05,
      "loss": 1.8564,
      "step": 152700
    },
    {
      "epoch": 11.969293435688547,
      "grad_norm": 5.911327838897705,
      "learning_rate": 4.0025588803592875e-05,
      "loss": 1.7452,
      "step": 152800
    },
    {
      "epoch": 11.977126742910857,
      "grad_norm": 4.45781135559082,
      "learning_rate": 4.001906104757429e-05,
      "loss": 1.7921,
      "step": 152900
    },
    {
      "epoch": 11.984960050133166,
      "grad_norm": 4.7825026512146,
      "learning_rate": 4.00125332915557e-05,
      "loss": 1.826,
      "step": 153000
    },
    {
      "epoch": 11.992793357355476,
      "grad_norm": 4.904825210571289,
      "learning_rate": 4.0006005535537106e-05,
      "loss": 1.8827,
      "step": 153100
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.8170627355575562,
      "eval_runtime": 1.542,
      "eval_samples_per_second": 435.793,
      "eval_steps_per_second": 435.793,
      "step": 153192
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.5745065212249756,
      "eval_runtime": 29.0831,
      "eval_samples_per_second": 438.949,
      "eval_steps_per_second": 438.949,
      "step": 153192
    },
    {
      "epoch": 12.000626664577785,
      "grad_norm": 7.472986698150635,
      "learning_rate": 3.999947777951851e-05,
      "loss": 1.8217,
      "step": 153200
    },
    {
      "epoch": 12.008459971800095,
      "grad_norm": 8.508971214294434,
      "learning_rate": 3.9992950023499924e-05,
      "loss": 1.8048,
      "step": 153300
    },
    {
      "epoch": 12.016293279022403,
      "grad_norm": 5.656716823577881,
      "learning_rate": 3.998642226748133e-05,
      "loss": 1.7818,
      "step": 153400
    },
    {
      "epoch": 12.024126586244712,
      "grad_norm": 6.002967834472656,
      "learning_rate": 3.997989451146274e-05,
      "loss": 1.7319,
      "step": 153500
    },
    {
      "epoch": 12.031959893467022,
      "grad_norm": 4.215631484985352,
      "learning_rate": 3.9973366755444155e-05,
      "loss": 1.7615,
      "step": 153600
    },
    {
      "epoch": 12.039793200689331,
      "grad_norm": 5.461215972900391,
      "learning_rate": 3.996683899942556e-05,
      "loss": 1.8265,
      "step": 153700
    },
    {
      "epoch": 12.047626507911641,
      "grad_norm": 5.2690534591674805,
      "learning_rate": 3.9960311243406967e-05,
      "loss": 1.7904,
      "step": 153800
    },
    {
      "epoch": 12.055459815133949,
      "grad_norm": 6.386595249176025,
      "learning_rate": 3.995378348738838e-05,
      "loss": 1.827,
      "step": 153900
    },
    {
      "epoch": 12.063293122356258,
      "grad_norm": 6.132356643676758,
      "learning_rate": 3.9947255731369785e-05,
      "loss": 1.7884,
      "step": 154000
    },
    {
      "epoch": 12.071126429578568,
      "grad_norm": 5.3042826652526855,
      "learning_rate": 3.99407279753512e-05,
      "loss": 1.797,
      "step": 154100
    },
    {
      "epoch": 12.078959736800877,
      "grad_norm": 5.581937789916992,
      "learning_rate": 3.99342002193326e-05,
      "loss": 1.81,
      "step": 154200
    },
    {
      "epoch": 12.086793044023187,
      "grad_norm": 6.345736980438232,
      "learning_rate": 3.9927672463314016e-05,
      "loss": 1.7326,
      "step": 154300
    },
    {
      "epoch": 12.094626351245497,
      "grad_norm": 7.0978288650512695,
      "learning_rate": 3.992114470729542e-05,
      "loss": 1.7638,
      "step": 154400
    },
    {
      "epoch": 12.102459658467804,
      "grad_norm": 6.029381275177002,
      "learning_rate": 3.991461695127683e-05,
      "loss": 1.8,
      "step": 154500
    },
    {
      "epoch": 12.110292965690114,
      "grad_norm": 6.036742687225342,
      "learning_rate": 3.990808919525824e-05,
      "loss": 1.7524,
      "step": 154600
    },
    {
      "epoch": 12.118126272912424,
      "grad_norm": 5.5211992263793945,
      "learning_rate": 3.9901561439239646e-05,
      "loss": 1.8018,
      "step": 154700
    },
    {
      "epoch": 12.125959580134733,
      "grad_norm": 5.6434736251831055,
      "learning_rate": 3.989503368322106e-05,
      "loss": 1.8512,
      "step": 154800
    },
    {
      "epoch": 12.133792887357043,
      "grad_norm": 5.468342304229736,
      "learning_rate": 3.988850592720247e-05,
      "loss": 1.8257,
      "step": 154900
    },
    {
      "epoch": 12.141626194579352,
      "grad_norm": 5.302687644958496,
      "learning_rate": 3.9881978171183876e-05,
      "loss": 1.8262,
      "step": 155000
    },
    {
      "epoch": 12.14945950180166,
      "grad_norm": 7.230998516082764,
      "learning_rate": 3.987545041516528e-05,
      "loss": 1.796,
      "step": 155100
    },
    {
      "epoch": 12.15729280902397,
      "grad_norm": 7.2706403732299805,
      "learning_rate": 3.9868922659146695e-05,
      "loss": 1.7024,
      "step": 155200
    },
    {
      "epoch": 12.16512611624628,
      "grad_norm": 5.739399433135986,
      "learning_rate": 3.98623949031281e-05,
      "loss": 1.7994,
      "step": 155300
    },
    {
      "epoch": 12.172959423468589,
      "grad_norm": 4.817228317260742,
      "learning_rate": 3.985586714710951e-05,
      "loss": 1.7789,
      "step": 155400
    },
    {
      "epoch": 12.180792730690898,
      "grad_norm": 5.633499622344971,
      "learning_rate": 3.9849339391090926e-05,
      "loss": 1.7959,
      "step": 155500
    },
    {
      "epoch": 12.188626037913206,
      "grad_norm": 5.072478771209717,
      "learning_rate": 3.984281163507233e-05,
      "loss": 1.7867,
      "step": 155600
    },
    {
      "epoch": 12.196459345135516,
      "grad_norm": 5.195344924926758,
      "learning_rate": 3.983628387905374e-05,
      "loss": 1.8632,
      "step": 155700
    },
    {
      "epoch": 12.204292652357825,
      "grad_norm": 5.395366191864014,
      "learning_rate": 3.982975612303514e-05,
      "loss": 1.8171,
      "step": 155800
    },
    {
      "epoch": 12.212125959580135,
      "grad_norm": 5.798166751861572,
      "learning_rate": 3.9823228367016556e-05,
      "loss": 1.8415,
      "step": 155900
    },
    {
      "epoch": 12.219959266802444,
      "grad_norm": 4.8104939460754395,
      "learning_rate": 3.981670061099796e-05,
      "loss": 1.7834,
      "step": 156000
    },
    {
      "epoch": 12.227792574024754,
      "grad_norm": 5.322709083557129,
      "learning_rate": 3.9810172854979374e-05,
      "loss": 1.7823,
      "step": 156100
    },
    {
      "epoch": 12.235625881247062,
      "grad_norm": 6.4907708168029785,
      "learning_rate": 3.9803645098960786e-05,
      "loss": 1.7398,
      "step": 156200
    },
    {
      "epoch": 12.243459188469371,
      "grad_norm": 5.4688520431518555,
      "learning_rate": 3.979711734294219e-05,
      "loss": 1.7807,
      "step": 156300
    },
    {
      "epoch": 12.25129249569168,
      "grad_norm": 5.186123847961426,
      "learning_rate": 3.97905895869236e-05,
      "loss": 1.8251,
      "step": 156400
    },
    {
      "epoch": 12.25912580291399,
      "grad_norm": 5.9453206062316895,
      "learning_rate": 3.978406183090501e-05,
      "loss": 1.7992,
      "step": 156500
    },
    {
      "epoch": 12.2669591101363,
      "grad_norm": 5.376611232757568,
      "learning_rate": 3.9777534074886416e-05,
      "loss": 1.7165,
      "step": 156600
    },
    {
      "epoch": 12.27479241735861,
      "grad_norm": 7.214023113250732,
      "learning_rate": 3.977100631886783e-05,
      "loss": 1.8318,
      "step": 156700
    },
    {
      "epoch": 12.282625724580917,
      "grad_norm": 5.099867343902588,
      "learning_rate": 3.976447856284924e-05,
      "loss": 1.7974,
      "step": 156800
    },
    {
      "epoch": 12.290459031803227,
      "grad_norm": 5.7923173904418945,
      "learning_rate": 3.975795080683065e-05,
      "loss": 1.8276,
      "step": 156900
    },
    {
      "epoch": 12.298292339025537,
      "grad_norm": 4.60364294052124,
      "learning_rate": 3.975142305081205e-05,
      "loss": 1.8561,
      "step": 157000
    },
    {
      "epoch": 12.306125646247846,
      "grad_norm": 9.692387580871582,
      "learning_rate": 3.9744895294793465e-05,
      "loss": 1.741,
      "step": 157100
    },
    {
      "epoch": 12.313958953470156,
      "grad_norm": 5.279097557067871,
      "learning_rate": 3.973836753877487e-05,
      "loss": 1.7159,
      "step": 157200
    },
    {
      "epoch": 12.321792260692465,
      "grad_norm": 5.967104911804199,
      "learning_rate": 3.9731839782756284e-05,
      "loss": 1.8295,
      "step": 157300
    },
    {
      "epoch": 12.329625567914773,
      "grad_norm": 3.258819341659546,
      "learning_rate": 3.9725312026737696e-05,
      "loss": 1.7111,
      "step": 157400
    },
    {
      "epoch": 12.337458875137083,
      "grad_norm": 5.379271030426025,
      "learning_rate": 3.97187842707191e-05,
      "loss": 1.8263,
      "step": 157500
    },
    {
      "epoch": 12.345292182359392,
      "grad_norm": 6.1590657234191895,
      "learning_rate": 3.971225651470051e-05,
      "loss": 1.7828,
      "step": 157600
    },
    {
      "epoch": 12.353125489581702,
      "grad_norm": 5.326923847198486,
      "learning_rate": 3.9705728758681914e-05,
      "loss": 1.7229,
      "step": 157700
    },
    {
      "epoch": 12.360958796804011,
      "grad_norm": 6.199974536895752,
      "learning_rate": 3.9699201002663326e-05,
      "loss": 1.7937,
      "step": 157800
    },
    {
      "epoch": 12.36879210402632,
      "grad_norm": 5.271490573883057,
      "learning_rate": 3.969267324664473e-05,
      "loss": 1.8102,
      "step": 157900
    },
    {
      "epoch": 12.376625411248629,
      "grad_norm": 6.035192966461182,
      "learning_rate": 3.9686145490626144e-05,
      "loss": 1.6768,
      "step": 158000
    },
    {
      "epoch": 12.384458718470938,
      "grad_norm": 6.176699161529541,
      "learning_rate": 3.967961773460756e-05,
      "loss": 1.8115,
      "step": 158100
    },
    {
      "epoch": 12.392292025693248,
      "grad_norm": 5.292426109313965,
      "learning_rate": 3.967308997858896e-05,
      "loss": 1.8564,
      "step": 158200
    },
    {
      "epoch": 12.400125332915557,
      "grad_norm": 6.047504425048828,
      "learning_rate": 3.966656222257037e-05,
      "loss": 1.7039,
      "step": 158300
    },
    {
      "epoch": 12.407958640137867,
      "grad_norm": 4.634655475616455,
      "learning_rate": 3.966003446655178e-05,
      "loss": 1.8654,
      "step": 158400
    },
    {
      "epoch": 12.415791947360175,
      "grad_norm": 5.086634159088135,
      "learning_rate": 3.965350671053319e-05,
      "loss": 1.7868,
      "step": 158500
    },
    {
      "epoch": 12.423625254582484,
      "grad_norm": 4.630693435668945,
      "learning_rate": 3.96469789545146e-05,
      "loss": 1.7036,
      "step": 158600
    },
    {
      "epoch": 12.431458561804794,
      "grad_norm": 9.242819786071777,
      "learning_rate": 3.964045119849601e-05,
      "loss": 1.7978,
      "step": 158700
    },
    {
      "epoch": 12.439291869027103,
      "grad_norm": 4.923623561859131,
      "learning_rate": 3.963392344247742e-05,
      "loss": 1.8398,
      "step": 158800
    },
    {
      "epoch": 12.447125176249413,
      "grad_norm": 5.658054351806641,
      "learning_rate": 3.9627395686458823e-05,
      "loss": 1.7849,
      "step": 158900
    },
    {
      "epoch": 12.45495848347172,
      "grad_norm": 5.4942193031311035,
      "learning_rate": 3.9620867930440236e-05,
      "loss": 1.7328,
      "step": 159000
    },
    {
      "epoch": 12.46279179069403,
      "grad_norm": 5.006661415100098,
      "learning_rate": 3.961434017442164e-05,
      "loss": 1.7196,
      "step": 159100
    },
    {
      "epoch": 12.47062509791634,
      "grad_norm": 7.248847007751465,
      "learning_rate": 3.960781241840305e-05,
      "loss": 1.8076,
      "step": 159200
    },
    {
      "epoch": 12.47845840513865,
      "grad_norm": 5.243509769439697,
      "learning_rate": 3.960128466238446e-05,
      "loss": 1.7638,
      "step": 159300
    },
    {
      "epoch": 12.48629171236096,
      "grad_norm": 6.170886993408203,
      "learning_rate": 3.959475690636587e-05,
      "loss": 1.834,
      "step": 159400
    },
    {
      "epoch": 12.494125019583269,
      "grad_norm": 5.82365083694458,
      "learning_rate": 3.958822915034728e-05,
      "loss": 1.8009,
      "step": 159500
    },
    {
      "epoch": 12.501958326805577,
      "grad_norm": 5.099502086639404,
      "learning_rate": 3.9581701394328684e-05,
      "loss": 1.8768,
      "step": 159600
    },
    {
      "epoch": 12.509791634027886,
      "grad_norm": 5.301076412200928,
      "learning_rate": 3.95751736383101e-05,
      "loss": 1.8081,
      "step": 159700
    },
    {
      "epoch": 12.517624941250196,
      "grad_norm": 6.824252128601074,
      "learning_rate": 3.95686458822915e-05,
      "loss": 1.7452,
      "step": 159800
    },
    {
      "epoch": 12.525458248472505,
      "grad_norm": 6.065732002258301,
      "learning_rate": 3.9562118126272915e-05,
      "loss": 1.7731,
      "step": 159900
    },
    {
      "epoch": 12.533291555694815,
      "grad_norm": 4.719038009643555,
      "learning_rate": 3.955559037025433e-05,
      "loss": 1.8138,
      "step": 160000
    },
    {
      "epoch": 12.541124862917124,
      "grad_norm": 5.965651988983154,
      "learning_rate": 3.954906261423573e-05,
      "loss": 1.8485,
      "step": 160100
    },
    {
      "epoch": 12.548958170139432,
      "grad_norm": 3.926166296005249,
      "learning_rate": 3.954253485821714e-05,
      "loss": 1.843,
      "step": 160200
    },
    {
      "epoch": 12.556791477361742,
      "grad_norm": 6.104245662689209,
      "learning_rate": 3.953600710219855e-05,
      "loss": 1.7943,
      "step": 160300
    },
    {
      "epoch": 12.564624784584051,
      "grad_norm": 4.336541175842285,
      "learning_rate": 3.952947934617996e-05,
      "loss": 1.7424,
      "step": 160400
    },
    {
      "epoch": 12.57245809180636,
      "grad_norm": 5.632030487060547,
      "learning_rate": 3.952295159016137e-05,
      "loss": 1.8082,
      "step": 160500
    },
    {
      "epoch": 12.58029139902867,
      "grad_norm": 4.69830846786499,
      "learning_rate": 3.951642383414278e-05,
      "loss": 1.7222,
      "step": 160600
    },
    {
      "epoch": 12.58812470625098,
      "grad_norm": 5.241939067840576,
      "learning_rate": 3.950989607812419e-05,
      "loss": 1.7512,
      "step": 160700
    },
    {
      "epoch": 12.595958013473288,
      "grad_norm": 5.79531192779541,
      "learning_rate": 3.9503368322105594e-05,
      "loss": 1.8057,
      "step": 160800
    },
    {
      "epoch": 12.603791320695597,
      "grad_norm": 9.75977611541748,
      "learning_rate": 3.9496840566087e-05,
      "loss": 1.8535,
      "step": 160900
    },
    {
      "epoch": 12.611624627917907,
      "grad_norm": 6.539089202880859,
      "learning_rate": 3.949031281006841e-05,
      "loss": 1.7509,
      "step": 161000
    },
    {
      "epoch": 12.619457935140217,
      "grad_norm": 6.353991985321045,
      "learning_rate": 3.948378505404982e-05,
      "loss": 1.7278,
      "step": 161100
    },
    {
      "epoch": 12.627291242362526,
      "grad_norm": 5.4681396484375,
      "learning_rate": 3.947725729803123e-05,
      "loss": 1.7441,
      "step": 161200
    },
    {
      "epoch": 12.635124549584834,
      "grad_norm": 7.925219535827637,
      "learning_rate": 3.947072954201264e-05,
      "loss": 1.8218,
      "step": 161300
    },
    {
      "epoch": 12.642957856807143,
      "grad_norm": 5.810291290283203,
      "learning_rate": 3.946420178599405e-05,
      "loss": 1.7605,
      "step": 161400
    },
    {
      "epoch": 12.650791164029453,
      "grad_norm": 4.275250434875488,
      "learning_rate": 3.9457674029975455e-05,
      "loss": 1.7657,
      "step": 161500
    },
    {
      "epoch": 12.658624471251763,
      "grad_norm": 6.526968479156494,
      "learning_rate": 3.945114627395687e-05,
      "loss": 1.8381,
      "step": 161600
    },
    {
      "epoch": 12.666457778474072,
      "grad_norm": 5.601296424865723,
      "learning_rate": 3.944461851793827e-05,
      "loss": 1.8771,
      "step": 161700
    },
    {
      "epoch": 12.674291085696382,
      "grad_norm": 6.493943691253662,
      "learning_rate": 3.9438090761919686e-05,
      "loss": 1.8115,
      "step": 161800
    },
    {
      "epoch": 12.68212439291869,
      "grad_norm": 6.10299825668335,
      "learning_rate": 3.94315630059011e-05,
      "loss": 1.7317,
      "step": 161900
    },
    {
      "epoch": 12.689957700140999,
      "grad_norm": 6.229762554168701,
      "learning_rate": 3.9425035249882504e-05,
      "loss": 1.6901,
      "step": 162000
    },
    {
      "epoch": 12.697791007363309,
      "grad_norm": 4.995145320892334,
      "learning_rate": 3.941850749386391e-05,
      "loss": 1.7654,
      "step": 162100
    },
    {
      "epoch": 12.705624314585618,
      "grad_norm": 4.850228786468506,
      "learning_rate": 3.941197973784532e-05,
      "loss": 1.7742,
      "step": 162200
    },
    {
      "epoch": 12.713457621807928,
      "grad_norm": 6.74994421005249,
      "learning_rate": 3.940545198182673e-05,
      "loss": 1.8295,
      "step": 162300
    },
    {
      "epoch": 12.721290929030237,
      "grad_norm": 6.882572650909424,
      "learning_rate": 3.9398924225808134e-05,
      "loss": 1.7439,
      "step": 162400
    },
    {
      "epoch": 12.729124236252545,
      "grad_norm": 4.624030113220215,
      "learning_rate": 3.939239646978955e-05,
      "loss": 1.8186,
      "step": 162500
    },
    {
      "epoch": 12.736957543474855,
      "grad_norm": 4.48698091506958,
      "learning_rate": 3.938586871377096e-05,
      "loss": 1.7824,
      "step": 162600
    },
    {
      "epoch": 12.744790850697164,
      "grad_norm": 6.667448043823242,
      "learning_rate": 3.9379340957752365e-05,
      "loss": 1.776,
      "step": 162700
    },
    {
      "epoch": 12.752624157919474,
      "grad_norm": 6.021874904632568,
      "learning_rate": 3.937281320173377e-05,
      "loss": 1.8003,
      "step": 162800
    },
    {
      "epoch": 12.760457465141783,
      "grad_norm": 6.9937052726745605,
      "learning_rate": 3.936628544571518e-05,
      "loss": 1.7226,
      "step": 162900
    },
    {
      "epoch": 12.768290772364093,
      "grad_norm": 7.4581170082092285,
      "learning_rate": 3.935975768969659e-05,
      "loss": 1.769,
      "step": 163000
    },
    {
      "epoch": 12.7761240795864,
      "grad_norm": 3.611159324645996,
      "learning_rate": 3.9353229933678e-05,
      "loss": 1.7769,
      "step": 163100
    },
    {
      "epoch": 12.78395738680871,
      "grad_norm": 6.34555721282959,
      "learning_rate": 3.9346702177659414e-05,
      "loss": 1.8213,
      "step": 163200
    },
    {
      "epoch": 12.79179069403102,
      "grad_norm": 6.909831523895264,
      "learning_rate": 3.934017442164082e-05,
      "loss": 1.729,
      "step": 163300
    },
    {
      "epoch": 12.79962400125333,
      "grad_norm": 6.043089389801025,
      "learning_rate": 3.9333646665622225e-05,
      "loss": 1.8206,
      "step": 163400
    },
    {
      "epoch": 12.80745730847564,
      "grad_norm": 5.228894233703613,
      "learning_rate": 3.932711890960364e-05,
      "loss": 1.9035,
      "step": 163500
    },
    {
      "epoch": 12.815290615697947,
      "grad_norm": 5.167004585266113,
      "learning_rate": 3.9320591153585044e-05,
      "loss": 1.7906,
      "step": 163600
    },
    {
      "epoch": 12.823123922920256,
      "grad_norm": 6.390994071960449,
      "learning_rate": 3.9314063397566456e-05,
      "loss": 1.7578,
      "step": 163700
    },
    {
      "epoch": 12.830957230142566,
      "grad_norm": 4.802485942840576,
      "learning_rate": 3.930753564154787e-05,
      "loss": 1.8457,
      "step": 163800
    },
    {
      "epoch": 12.838790537364876,
      "grad_norm": 4.985747814178467,
      "learning_rate": 3.9301007885529274e-05,
      "loss": 1.7962,
      "step": 163900
    },
    {
      "epoch": 12.846623844587185,
      "grad_norm": 4.734464645385742,
      "learning_rate": 3.929448012951068e-05,
      "loss": 1.8429,
      "step": 164000
    },
    {
      "epoch": 12.854457151809495,
      "grad_norm": 6.0311808586120605,
      "learning_rate": 3.928795237349209e-05,
      "loss": 1.7773,
      "step": 164100
    },
    {
      "epoch": 12.862290459031803,
      "grad_norm": 6.315135955810547,
      "learning_rate": 3.92814246174735e-05,
      "loss": 1.6962,
      "step": 164200
    },
    {
      "epoch": 12.870123766254112,
      "grad_norm": 5.12099552154541,
      "learning_rate": 3.9274896861454904e-05,
      "loss": 1.8151,
      "step": 164300
    },
    {
      "epoch": 12.877957073476422,
      "grad_norm": 4.3665995597839355,
      "learning_rate": 3.926836910543632e-05,
      "loss": 1.8347,
      "step": 164400
    },
    {
      "epoch": 12.885790380698731,
      "grad_norm": 5.616077423095703,
      "learning_rate": 3.926184134941773e-05,
      "loss": 1.789,
      "step": 164500
    },
    {
      "epoch": 12.89362368792104,
      "grad_norm": 5.364787578582764,
      "learning_rate": 3.9255313593399135e-05,
      "loss": 1.8033,
      "step": 164600
    },
    {
      "epoch": 12.901456995143349,
      "grad_norm": 7.06308126449585,
      "learning_rate": 3.924878583738054e-05,
      "loss": 1.7938,
      "step": 164700
    },
    {
      "epoch": 12.909290302365658,
      "grad_norm": 2.8423001766204834,
      "learning_rate": 3.9242258081361953e-05,
      "loss": 1.8095,
      "step": 164800
    },
    {
      "epoch": 12.917123609587968,
      "grad_norm": 5.690279960632324,
      "learning_rate": 3.923573032534336e-05,
      "loss": 1.8904,
      "step": 164900
    },
    {
      "epoch": 12.924956916810277,
      "grad_norm": 5.194606304168701,
      "learning_rate": 3.922920256932477e-05,
      "loss": 1.8902,
      "step": 165000
    },
    {
      "epoch": 12.932790224032587,
      "grad_norm": 5.979868412017822,
      "learning_rate": 3.9222674813306184e-05,
      "loss": 1.7599,
      "step": 165100
    },
    {
      "epoch": 12.940623531254897,
      "grad_norm": 5.3131303787231445,
      "learning_rate": 3.921614705728759e-05,
      "loss": 1.874,
      "step": 165200
    },
    {
      "epoch": 12.948456838477204,
      "grad_norm": 5.930403709411621,
      "learning_rate": 3.9209619301268996e-05,
      "loss": 1.7803,
      "step": 165300
    },
    {
      "epoch": 12.956290145699514,
      "grad_norm": 5.663864612579346,
      "learning_rate": 3.920309154525041e-05,
      "loss": 1.8244,
      "step": 165400
    },
    {
      "epoch": 12.964123452921823,
      "grad_norm": 5.287257671356201,
      "learning_rate": 3.9196563789231814e-05,
      "loss": 1.8502,
      "step": 165500
    },
    {
      "epoch": 12.971956760144133,
      "grad_norm": 5.534394264221191,
      "learning_rate": 3.919003603321322e-05,
      "loss": 1.8838,
      "step": 165600
    },
    {
      "epoch": 12.979790067366443,
      "grad_norm": 5.75678014755249,
      "learning_rate": 3.918350827719464e-05,
      "loss": 1.88,
      "step": 165700
    },
    {
      "epoch": 12.987623374588752,
      "grad_norm": 7.974686622619629,
      "learning_rate": 3.9176980521176045e-05,
      "loss": 1.8473,
      "step": 165800
    },
    {
      "epoch": 12.99545668181106,
      "grad_norm": 5.630237579345703,
      "learning_rate": 3.917045276515745e-05,
      "loss": 1.7882,
      "step": 165900
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.817994236946106,
      "eval_runtime": 3.0441,
      "eval_samples_per_second": 220.752,
      "eval_steps_per_second": 220.752,
      "step": 165958
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.5621907711029053,
      "eval_runtime": 43.0543,
      "eval_samples_per_second": 296.509,
      "eval_steps_per_second": 296.509,
      "step": 165958
    },
    {
      "epoch": 13.00328998903337,
      "grad_norm": 5.266663074493408,
      "learning_rate": 3.9163925009138857e-05,
      "loss": 1.7798,
      "step": 166000
    },
    {
      "epoch": 13.011123296255679,
      "grad_norm": 5.630505561828613,
      "learning_rate": 3.915739725312027e-05,
      "loss": 1.7553,
      "step": 166100
    },
    {
      "epoch": 13.018956603477989,
      "grad_norm": 5.875624656677246,
      "learning_rate": 3.9150869497101675e-05,
      "loss": 1.821,
      "step": 166200
    },
    {
      "epoch": 13.026789910700298,
      "grad_norm": 5.875820159912109,
      "learning_rate": 3.914434174108309e-05,
      "loss": 1.7544,
      "step": 166300
    },
    {
      "epoch": 13.034623217922608,
      "grad_norm": 6.154248237609863,
      "learning_rate": 3.91378139850645e-05,
      "loss": 1.7674,
      "step": 166400
    },
    {
      "epoch": 13.042456525144916,
      "grad_norm": 7.327063083648682,
      "learning_rate": 3.9131286229045906e-05,
      "loss": 1.6561,
      "step": 166500
    },
    {
      "epoch": 13.050289832367225,
      "grad_norm": 4.903303623199463,
      "learning_rate": 3.912475847302731e-05,
      "loss": 1.7545,
      "step": 166600
    },
    {
      "epoch": 13.058123139589535,
      "grad_norm": 4.621368885040283,
      "learning_rate": 3.9118230717008724e-05,
      "loss": 1.6389,
      "step": 166700
    },
    {
      "epoch": 13.065956446811844,
      "grad_norm": 7.009572505950928,
      "learning_rate": 3.911170296099013e-05,
      "loss": 1.7875,
      "step": 166800
    },
    {
      "epoch": 13.073789754034154,
      "grad_norm": 6.625912189483643,
      "learning_rate": 3.910517520497154e-05,
      "loss": 1.7364,
      "step": 166900
    },
    {
      "epoch": 13.081623061256462,
      "grad_norm": 5.998239994049072,
      "learning_rate": 3.9098647448952955e-05,
      "loss": 1.81,
      "step": 167000
    },
    {
      "epoch": 13.089456368478771,
      "grad_norm": 6.999018669128418,
      "learning_rate": 3.909211969293436e-05,
      "loss": 1.7819,
      "step": 167100
    },
    {
      "epoch": 13.09728967570108,
      "grad_norm": 6.671962261199951,
      "learning_rate": 3.9085591936915766e-05,
      "loss": 1.8246,
      "step": 167200
    },
    {
      "epoch": 13.10512298292339,
      "grad_norm": 6.677286624908447,
      "learning_rate": 3.907906418089718e-05,
      "loss": 1.7465,
      "step": 167300
    },
    {
      "epoch": 13.1129562901457,
      "grad_norm": 5.069451808929443,
      "learning_rate": 3.9072536424878585e-05,
      "loss": 1.8602,
      "step": 167400
    },
    {
      "epoch": 13.12078959736801,
      "grad_norm": 4.9645819664001465,
      "learning_rate": 3.906600866885999e-05,
      "loss": 1.809,
      "step": 167500
    },
    {
      "epoch": 13.128622904590317,
      "grad_norm": 5.525218963623047,
      "learning_rate": 3.90594809128414e-05,
      "loss": 1.6892,
      "step": 167600
    },
    {
      "epoch": 13.136456211812627,
      "grad_norm": 6.054967403411865,
      "learning_rate": 3.9052953156822816e-05,
      "loss": 1.7562,
      "step": 167700
    },
    {
      "epoch": 13.144289519034936,
      "grad_norm": 5.204529285430908,
      "learning_rate": 3.904642540080422e-05,
      "loss": 1.8347,
      "step": 167800
    },
    {
      "epoch": 13.152122826257246,
      "grad_norm": 7.511424541473389,
      "learning_rate": 3.903989764478563e-05,
      "loss": 1.744,
      "step": 167900
    },
    {
      "epoch": 13.159956133479556,
      "grad_norm": 7.032972812652588,
      "learning_rate": 3.903336988876704e-05,
      "loss": 1.7964,
      "step": 168000
    },
    {
      "epoch": 13.167789440701865,
      "grad_norm": 5.94240140914917,
      "learning_rate": 3.9026842132748445e-05,
      "loss": 1.775,
      "step": 168100
    },
    {
      "epoch": 13.175622747924173,
      "grad_norm": 4.5896830558776855,
      "learning_rate": 3.902031437672986e-05,
      "loss": 1.7976,
      "step": 168200
    },
    {
      "epoch": 13.183456055146483,
      "grad_norm": 6.807816028594971,
      "learning_rate": 3.901378662071127e-05,
      "loss": 1.8253,
      "step": 168300
    },
    {
      "epoch": 13.191289362368792,
      "grad_norm": 6.623965740203857,
      "learning_rate": 3.9007258864692676e-05,
      "loss": 1.7588,
      "step": 168400
    },
    {
      "epoch": 13.199122669591102,
      "grad_norm": 6.274345397949219,
      "learning_rate": 3.900073110867408e-05,
      "loss": 1.798,
      "step": 168500
    },
    {
      "epoch": 13.206955976813411,
      "grad_norm": 6.8089447021484375,
      "learning_rate": 3.8994203352655495e-05,
      "loss": 1.8248,
      "step": 168600
    },
    {
      "epoch": 13.214789284035719,
      "grad_norm": 5.02432107925415,
      "learning_rate": 3.89876755966369e-05,
      "loss": 1.8627,
      "step": 168700
    },
    {
      "epoch": 13.222622591258029,
      "grad_norm": 5.8776092529296875,
      "learning_rate": 3.8981147840618306e-05,
      "loss": 1.7685,
      "step": 168800
    },
    {
      "epoch": 13.230455898480338,
      "grad_norm": 4.252567768096924,
      "learning_rate": 3.8974620084599726e-05,
      "loss": 1.7389,
      "step": 168900
    },
    {
      "epoch": 13.238289205702648,
      "grad_norm": 4.598310470581055,
      "learning_rate": 3.896809232858113e-05,
      "loss": 1.6652,
      "step": 169000
    },
    {
      "epoch": 13.246122512924957,
      "grad_norm": 5.358697891235352,
      "learning_rate": 3.896156457256254e-05,
      "loss": 1.7719,
      "step": 169100
    },
    {
      "epoch": 13.253955820147267,
      "grad_norm": 5.66438627243042,
      "learning_rate": 3.895503681654395e-05,
      "loss": 1.7729,
      "step": 169200
    },
    {
      "epoch": 13.261789127369575,
      "grad_norm": 4.697834491729736,
      "learning_rate": 3.8948509060525355e-05,
      "loss": 1.8143,
      "step": 169300
    },
    {
      "epoch": 13.269622434591884,
      "grad_norm": 5.313333034515381,
      "learning_rate": 3.894198130450676e-05,
      "loss": 1.8518,
      "step": 169400
    },
    {
      "epoch": 13.277455741814194,
      "grad_norm": 6.63149881362915,
      "learning_rate": 3.8935453548488174e-05,
      "loss": 1.853,
      "step": 169500
    },
    {
      "epoch": 13.285289049036503,
      "grad_norm": 6.416861057281494,
      "learning_rate": 3.8928925792469586e-05,
      "loss": 1.8539,
      "step": 169600
    },
    {
      "epoch": 13.293122356258813,
      "grad_norm": 7.441574573516846,
      "learning_rate": 3.892239803645099e-05,
      "loss": 1.9043,
      "step": 169700
    },
    {
      "epoch": 13.300955663481123,
      "grad_norm": 5.251113414764404,
      "learning_rate": 3.89158702804324e-05,
      "loss": 1.6914,
      "step": 169800
    },
    {
      "epoch": 13.30878897070343,
      "grad_norm": 5.349427700042725,
      "learning_rate": 3.890934252441381e-05,
      "loss": 1.8887,
      "step": 169900
    },
    {
      "epoch": 13.31662227792574,
      "grad_norm": 6.552717685699463,
      "learning_rate": 3.8902814768395216e-05,
      "loss": 1.7973,
      "step": 170000
    },
    {
      "epoch": 13.32445558514805,
      "grad_norm": 4.79719877243042,
      "learning_rate": 3.889628701237663e-05,
      "loss": 1.9518,
      "step": 170100
    },
    {
      "epoch": 13.332288892370359,
      "grad_norm": 5.153458118438721,
      "learning_rate": 3.888975925635804e-05,
      "loss": 1.8101,
      "step": 170200
    },
    {
      "epoch": 13.340122199592669,
      "grad_norm": 7.216141223907471,
      "learning_rate": 3.888323150033945e-05,
      "loss": 1.7733,
      "step": 170300
    },
    {
      "epoch": 13.347955506814976,
      "grad_norm": 6.004995346069336,
      "learning_rate": 3.887670374432085e-05,
      "loss": 1.7714,
      "step": 170400
    },
    {
      "epoch": 13.355788814037286,
      "grad_norm": 6.432680130004883,
      "learning_rate": 3.8870175988302265e-05,
      "loss": 1.8833,
      "step": 170500
    },
    {
      "epoch": 13.363622121259596,
      "grad_norm": 5.083533763885498,
      "learning_rate": 3.886364823228367e-05,
      "loss": 1.7046,
      "step": 170600
    },
    {
      "epoch": 13.371455428481905,
      "grad_norm": 5.091850757598877,
      "learning_rate": 3.885712047626508e-05,
      "loss": 1.7609,
      "step": 170700
    },
    {
      "epoch": 13.379288735704215,
      "grad_norm": 6.500002861022949,
      "learning_rate": 3.885059272024649e-05,
      "loss": 1.8431,
      "step": 170800
    },
    {
      "epoch": 13.387122042926524,
      "grad_norm": 5.669638156890869,
      "learning_rate": 3.88440649642279e-05,
      "loss": 1.8653,
      "step": 170900
    },
    {
      "epoch": 13.394955350148832,
      "grad_norm": 5.268329620361328,
      "learning_rate": 3.883753720820931e-05,
      "loss": 1.7209,
      "step": 171000
    },
    {
      "epoch": 13.402788657371142,
      "grad_norm": 6.510091304779053,
      "learning_rate": 3.8831009452190713e-05,
      "loss": 1.7429,
      "step": 171100
    },
    {
      "epoch": 13.410621964593451,
      "grad_norm": 4.206093788146973,
      "learning_rate": 3.8824481696172126e-05,
      "loss": 1.7435,
      "step": 171200
    },
    {
      "epoch": 13.41845527181576,
      "grad_norm": 6.8232221603393555,
      "learning_rate": 3.881795394015353e-05,
      "loss": 1.7122,
      "step": 171300
    },
    {
      "epoch": 13.42628857903807,
      "grad_norm": 4.310111999511719,
      "learning_rate": 3.8811426184134944e-05,
      "loss": 1.7688,
      "step": 171400
    },
    {
      "epoch": 13.43412188626038,
      "grad_norm": 6.087651252746582,
      "learning_rate": 3.880489842811636e-05,
      "loss": 1.8946,
      "step": 171500
    },
    {
      "epoch": 13.441955193482688,
      "grad_norm": 5.331269264221191,
      "learning_rate": 3.879837067209776e-05,
      "loss": 1.7802,
      "step": 171600
    },
    {
      "epoch": 13.449788500704997,
      "grad_norm": 6.300383567810059,
      "learning_rate": 3.879184291607917e-05,
      "loss": 1.7234,
      "step": 171700
    },
    {
      "epoch": 13.457621807927307,
      "grad_norm": 5.834240913391113,
      "learning_rate": 3.878531516006058e-05,
      "loss": 1.8793,
      "step": 171800
    },
    {
      "epoch": 13.465455115149616,
      "grad_norm": 6.61797571182251,
      "learning_rate": 3.877878740404199e-05,
      "loss": 1.7536,
      "step": 171900
    },
    {
      "epoch": 13.473288422371926,
      "grad_norm": 4.9641194343566895,
      "learning_rate": 3.877225964802339e-05,
      "loss": 1.7773,
      "step": 172000
    },
    {
      "epoch": 13.481121729594234,
      "grad_norm": 5.803050518035889,
      "learning_rate": 3.876573189200481e-05,
      "loss": 1.8427,
      "step": 172100
    },
    {
      "epoch": 13.488955036816543,
      "grad_norm": 4.958302021026611,
      "learning_rate": 3.875920413598622e-05,
      "loss": 1.7889,
      "step": 172200
    },
    {
      "epoch": 13.496788344038853,
      "grad_norm": 6.3002848625183105,
      "learning_rate": 3.875267637996762e-05,
      "loss": 1.7566,
      "step": 172300
    },
    {
      "epoch": 13.504621651261163,
      "grad_norm": 6.9428911209106445,
      "learning_rate": 3.8746148623949036e-05,
      "loss": 1.7012,
      "step": 172400
    },
    {
      "epoch": 13.512454958483472,
      "grad_norm": 6.077476978302002,
      "learning_rate": 3.873962086793044e-05,
      "loss": 1.7264,
      "step": 172500
    },
    {
      "epoch": 13.520288265705782,
      "grad_norm": 8.016083717346191,
      "learning_rate": 3.873309311191185e-05,
      "loss": 1.7805,
      "step": 172600
    },
    {
      "epoch": 13.52812157292809,
      "grad_norm": 7.607161045074463,
      "learning_rate": 3.872656535589326e-05,
      "loss": 1.8536,
      "step": 172700
    },
    {
      "epoch": 13.535954880150399,
      "grad_norm": 6.0785393714904785,
      "learning_rate": 3.872003759987467e-05,
      "loss": 1.779,
      "step": 172800
    },
    {
      "epoch": 13.543788187372709,
      "grad_norm": 4.616727828979492,
      "learning_rate": 3.871350984385608e-05,
      "loss": 1.8274,
      "step": 172900
    },
    {
      "epoch": 13.551621494595018,
      "grad_norm": 5.701009750366211,
      "learning_rate": 3.8706982087837484e-05,
      "loss": 1.7616,
      "step": 173000
    },
    {
      "epoch": 13.559454801817328,
      "grad_norm": 6.388614654541016,
      "learning_rate": 3.8700454331818897e-05,
      "loss": 1.8614,
      "step": 173100
    },
    {
      "epoch": 13.567288109039637,
      "grad_norm": 7.51622200012207,
      "learning_rate": 3.86939265758003e-05,
      "loss": 1.7702,
      "step": 173200
    },
    {
      "epoch": 13.575121416261945,
      "grad_norm": 5.860733985900879,
      "learning_rate": 3.8687398819781715e-05,
      "loss": 1.8295,
      "step": 173300
    },
    {
      "epoch": 13.582954723484255,
      "grad_norm": 6.6533098220825195,
      "learning_rate": 3.868087106376313e-05,
      "loss": 1.6954,
      "step": 173400
    },
    {
      "epoch": 13.590788030706564,
      "grad_norm": 5.408716201782227,
      "learning_rate": 3.867434330774453e-05,
      "loss": 1.8606,
      "step": 173500
    },
    {
      "epoch": 13.598621337928874,
      "grad_norm": 12.012672424316406,
      "learning_rate": 3.866781555172594e-05,
      "loss": 1.7039,
      "step": 173600
    },
    {
      "epoch": 13.606454645151183,
      "grad_norm": 6.901054382324219,
      "learning_rate": 3.866128779570735e-05,
      "loss": 1.754,
      "step": 173700
    },
    {
      "epoch": 13.614287952373491,
      "grad_norm": 6.2922043800354,
      "learning_rate": 3.865476003968876e-05,
      "loss": 1.7326,
      "step": 173800
    },
    {
      "epoch": 13.6221212595958,
      "grad_norm": 4.69189453125,
      "learning_rate": 3.864823228367016e-05,
      "loss": 1.655,
      "step": 173900
    },
    {
      "epoch": 13.62995456681811,
      "grad_norm": 5.257896900177002,
      "learning_rate": 3.8641704527651576e-05,
      "loss": 1.7893,
      "step": 174000
    },
    {
      "epoch": 13.63778787404042,
      "grad_norm": 8.171941757202148,
      "learning_rate": 3.863517677163299e-05,
      "loss": 1.8587,
      "step": 174100
    },
    {
      "epoch": 13.64562118126273,
      "grad_norm": 5.023980617523193,
      "learning_rate": 3.8628649015614394e-05,
      "loss": 1.7646,
      "step": 174200
    },
    {
      "epoch": 13.653454488485039,
      "grad_norm": 5.1547160148620605,
      "learning_rate": 3.8622121259595806e-05,
      "loss": 1.7834,
      "step": 174300
    },
    {
      "epoch": 13.661287795707347,
      "grad_norm": 5.480454444885254,
      "learning_rate": 3.861559350357721e-05,
      "loss": 1.7064,
      "step": 174400
    },
    {
      "epoch": 13.669121102929656,
      "grad_norm": 9.833542823791504,
      "learning_rate": 3.860906574755862e-05,
      "loss": 1.7914,
      "step": 174500
    },
    {
      "epoch": 13.676954410151966,
      "grad_norm": 6.409991264343262,
      "learning_rate": 3.860253799154003e-05,
      "loss": 1.7014,
      "step": 174600
    },
    {
      "epoch": 13.684787717374276,
      "grad_norm": 4.7798871994018555,
      "learning_rate": 3.859601023552144e-05,
      "loss": 1.772,
      "step": 174700
    },
    {
      "epoch": 13.692621024596585,
      "grad_norm": 5.749242782592773,
      "learning_rate": 3.858948247950285e-05,
      "loss": 1.6848,
      "step": 174800
    },
    {
      "epoch": 13.700454331818895,
      "grad_norm": 5.366225242614746,
      "learning_rate": 3.8582954723484255e-05,
      "loss": 1.7557,
      "step": 174900
    },
    {
      "epoch": 13.708287639041203,
      "grad_norm": 6.479498386383057,
      "learning_rate": 3.857642696746567e-05,
      "loss": 1.9018,
      "step": 175000
    },
    {
      "epoch": 13.716120946263512,
      "grad_norm": 6.581515789031982,
      "learning_rate": 3.856989921144707e-05,
      "loss": 1.7926,
      "step": 175100
    },
    {
      "epoch": 13.723954253485822,
      "grad_norm": 4.5391621589660645,
      "learning_rate": 3.856337145542848e-05,
      "loss": 1.7948,
      "step": 175200
    },
    {
      "epoch": 13.731787560708131,
      "grad_norm": 5.7610764503479,
      "learning_rate": 3.85568436994099e-05,
      "loss": 1.7741,
      "step": 175300
    },
    {
      "epoch": 13.73962086793044,
      "grad_norm": 7.908602237701416,
      "learning_rate": 3.8550315943391304e-05,
      "loss": 1.7897,
      "step": 175400
    },
    {
      "epoch": 13.74745417515275,
      "grad_norm": 6.208255767822266,
      "learning_rate": 3.854378818737271e-05,
      "loss": 1.8557,
      "step": 175500
    },
    {
      "epoch": 13.755287482375058,
      "grad_norm": 6.651513576507568,
      "learning_rate": 3.853726043135412e-05,
      "loss": 1.7889,
      "step": 175600
    },
    {
      "epoch": 13.763120789597368,
      "grad_norm": 7.1534647941589355,
      "learning_rate": 3.853073267533553e-05,
      "loss": 1.865,
      "step": 175700
    },
    {
      "epoch": 13.770954096819677,
      "grad_norm": 4.202343940734863,
      "learning_rate": 3.8524204919316934e-05,
      "loss": 1.8602,
      "step": 175800
    },
    {
      "epoch": 13.778787404041987,
      "grad_norm": 4.959968090057373,
      "learning_rate": 3.8517677163298346e-05,
      "loss": 1.7459,
      "step": 175900
    },
    {
      "epoch": 13.786620711264296,
      "grad_norm": 6.567507266998291,
      "learning_rate": 3.851114940727976e-05,
      "loss": 1.8377,
      "step": 176000
    },
    {
      "epoch": 13.794454018486604,
      "grad_norm": 5.250039577484131,
      "learning_rate": 3.8504621651261164e-05,
      "loss": 1.7524,
      "step": 176100
    },
    {
      "epoch": 13.802287325708914,
      "grad_norm": 5.858899116516113,
      "learning_rate": 3.849809389524257e-05,
      "loss": 1.7391,
      "step": 176200
    },
    {
      "epoch": 13.810120632931223,
      "grad_norm": 6.650721549987793,
      "learning_rate": 3.849156613922398e-05,
      "loss": 1.7283,
      "step": 176300
    },
    {
      "epoch": 13.817953940153533,
      "grad_norm": 4.462439060211182,
      "learning_rate": 3.848503838320539e-05,
      "loss": 1.8027,
      "step": 176400
    },
    {
      "epoch": 13.825787247375843,
      "grad_norm": 4.684603691101074,
      "learning_rate": 3.84785106271868e-05,
      "loss": 1.8112,
      "step": 176500
    },
    {
      "epoch": 13.833620554598152,
      "grad_norm": 6.157642364501953,
      "learning_rate": 3.8471982871168214e-05,
      "loss": 1.8216,
      "step": 176600
    },
    {
      "epoch": 13.84145386182046,
      "grad_norm": 7.079420566558838,
      "learning_rate": 3.846545511514962e-05,
      "loss": 1.5962,
      "step": 176700
    },
    {
      "epoch": 13.84928716904277,
      "grad_norm": 4.819849967956543,
      "learning_rate": 3.8458927359131025e-05,
      "loss": 1.7528,
      "step": 176800
    },
    {
      "epoch": 13.857120476265079,
      "grad_norm": 5.2928547859191895,
      "learning_rate": 3.845239960311244e-05,
      "loss": 1.8235,
      "step": 176900
    },
    {
      "epoch": 13.864953783487389,
      "grad_norm": 6.002670764923096,
      "learning_rate": 3.8445871847093843e-05,
      "loss": 1.8042,
      "step": 177000
    },
    {
      "epoch": 13.872787090709698,
      "grad_norm": 5.923577308654785,
      "learning_rate": 3.843934409107525e-05,
      "loss": 1.8029,
      "step": 177100
    },
    {
      "epoch": 13.880620397932006,
      "grad_norm": 6.271401882171631,
      "learning_rate": 3.843281633505666e-05,
      "loss": 1.7799,
      "step": 177200
    },
    {
      "epoch": 13.888453705154316,
      "grad_norm": 6.752296447753906,
      "learning_rate": 3.8426288579038074e-05,
      "loss": 1.7159,
      "step": 177300
    },
    {
      "epoch": 13.896287012376625,
      "grad_norm": 4.326335430145264,
      "learning_rate": 3.841976082301948e-05,
      "loss": 1.7427,
      "step": 177400
    },
    {
      "epoch": 13.904120319598935,
      "grad_norm": 6.977465629577637,
      "learning_rate": 3.841323306700089e-05,
      "loss": 1.7109,
      "step": 177500
    },
    {
      "epoch": 13.911953626821244,
      "grad_norm": 4.250268936157227,
      "learning_rate": 3.84067053109823e-05,
      "loss": 1.9166,
      "step": 177600
    },
    {
      "epoch": 13.919786934043554,
      "grad_norm": 5.682535171508789,
      "learning_rate": 3.8400177554963704e-05,
      "loss": 1.7819,
      "step": 177700
    },
    {
      "epoch": 13.927620241265862,
      "grad_norm": 5.333357334136963,
      "learning_rate": 3.839364979894512e-05,
      "loss": 1.7732,
      "step": 177800
    },
    {
      "epoch": 13.935453548488171,
      "grad_norm": 6.662428379058838,
      "learning_rate": 3.838712204292653e-05,
      "loss": 1.6839,
      "step": 177900
    },
    {
      "epoch": 13.94328685571048,
      "grad_norm": 5.350479602813721,
      "learning_rate": 3.8380594286907935e-05,
      "loss": 1.7324,
      "step": 178000
    },
    {
      "epoch": 13.95112016293279,
      "grad_norm": 5.81145715713501,
      "learning_rate": 3.837406653088934e-05,
      "loss": 1.8267,
      "step": 178100
    },
    {
      "epoch": 13.9589534701551,
      "grad_norm": 4.960825443267822,
      "learning_rate": 3.836753877487075e-05,
      "loss": 1.842,
      "step": 178200
    },
    {
      "epoch": 13.96678677737741,
      "grad_norm": 6.276289463043213,
      "learning_rate": 3.836101101885216e-05,
      "loss": 1.782,
      "step": 178300
    },
    {
      "epoch": 13.974620084599717,
      "grad_norm": 5.230931282043457,
      "learning_rate": 3.8354483262833565e-05,
      "loss": 1.7614,
      "step": 178400
    },
    {
      "epoch": 13.982453391822027,
      "grad_norm": 5.7049689292907715,
      "learning_rate": 3.8347955506814984e-05,
      "loss": 1.8171,
      "step": 178500
    },
    {
      "epoch": 13.990286699044336,
      "grad_norm": 8.150074005126953,
      "learning_rate": 3.834142775079639e-05,
      "loss": 1.8037,
      "step": 178600
    },
    {
      "epoch": 13.998120006266646,
      "grad_norm": 6.382721424102783,
      "learning_rate": 3.8334899994777796e-05,
      "loss": 1.7503,
      "step": 178700
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8135408163070679,
      "eval_runtime": 2.9201,
      "eval_samples_per_second": 230.132,
      "eval_steps_per_second": 230.132,
      "step": 178724
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.5488107204437256,
      "eval_runtime": 55.4106,
      "eval_samples_per_second": 230.389,
      "eval_steps_per_second": 230.389,
      "step": 178724
    },
    {
      "epoch": 14.005953313488956,
      "grad_norm": 6.599225044250488,
      "learning_rate": 3.832837223875921e-05,
      "loss": 1.7656,
      "step": 178800
    },
    {
      "epoch": 14.013786620711265,
      "grad_norm": 5.153632640838623,
      "learning_rate": 3.8321844482740614e-05,
      "loss": 1.7645,
      "step": 178900
    },
    {
      "epoch": 14.021619927933573,
      "grad_norm": 7.8830084800720215,
      "learning_rate": 3.831531672672202e-05,
      "loss": 1.7409,
      "step": 179000
    },
    {
      "epoch": 14.029453235155882,
      "grad_norm": 7.661324501037598,
      "learning_rate": 3.830878897070343e-05,
      "loss": 1.8186,
      "step": 179100
    },
    {
      "epoch": 14.037286542378192,
      "grad_norm": 5.420350074768066,
      "learning_rate": 3.8302261214684845e-05,
      "loss": 1.7955,
      "step": 179200
    },
    {
      "epoch": 14.045119849600502,
      "grad_norm": 6.988455295562744,
      "learning_rate": 3.829573345866625e-05,
      "loss": 1.7721,
      "step": 179300
    },
    {
      "epoch": 14.052953156822811,
      "grad_norm": 4.435516357421875,
      "learning_rate": 3.8289205702647656e-05,
      "loss": 1.866,
      "step": 179400
    },
    {
      "epoch": 14.060786464045119,
      "grad_norm": 7.832305908203125,
      "learning_rate": 3.828267794662907e-05,
      "loss": 1.7178,
      "step": 179500
    },
    {
      "epoch": 14.068619771267429,
      "grad_norm": 5.45150899887085,
      "learning_rate": 3.8276150190610475e-05,
      "loss": 1.7837,
      "step": 179600
    },
    {
      "epoch": 14.076453078489738,
      "grad_norm": 5.017021656036377,
      "learning_rate": 3.826962243459189e-05,
      "loss": 1.6672,
      "step": 179700
    },
    {
      "epoch": 14.084286385712048,
      "grad_norm": 5.807113170623779,
      "learning_rate": 3.82630946785733e-05,
      "loss": 1.8096,
      "step": 179800
    },
    {
      "epoch": 14.092119692934357,
      "grad_norm": 6.028566360473633,
      "learning_rate": 3.8256566922554706e-05,
      "loss": 1.8036,
      "step": 179900
    },
    {
      "epoch": 14.099953000156667,
      "grad_norm": 6.856255054473877,
      "learning_rate": 3.825003916653611e-05,
      "loss": 1.7453,
      "step": 180000
    },
    {
      "epoch": 14.107786307378975,
      "grad_norm": 6.18766975402832,
      "learning_rate": 3.8243511410517524e-05,
      "loss": 1.7193,
      "step": 180100
    },
    {
      "epoch": 14.115619614601284,
      "grad_norm": 5.26162052154541,
      "learning_rate": 3.823698365449893e-05,
      "loss": 1.8412,
      "step": 180200
    },
    {
      "epoch": 14.123452921823594,
      "grad_norm": 6.25535774230957,
      "learning_rate": 3.8230455898480335e-05,
      "loss": 1.812,
      "step": 180300
    },
    {
      "epoch": 14.131286229045903,
      "grad_norm": 6.1708526611328125,
      "learning_rate": 3.822392814246175e-05,
      "loss": 1.8084,
      "step": 180400
    },
    {
      "epoch": 14.139119536268213,
      "grad_norm": 5.341518402099609,
      "learning_rate": 3.821740038644316e-05,
      "loss": 1.7881,
      "step": 180500
    },
    {
      "epoch": 14.146952843490523,
      "grad_norm": 8.291083335876465,
      "learning_rate": 3.8210872630424566e-05,
      "loss": 1.7873,
      "step": 180600
    },
    {
      "epoch": 14.15478615071283,
      "grad_norm": 6.062406063079834,
      "learning_rate": 3.820434487440598e-05,
      "loss": 1.7473,
      "step": 180700
    },
    {
      "epoch": 14.16261945793514,
      "grad_norm": 4.009702682495117,
      "learning_rate": 3.8197817118387385e-05,
      "loss": 1.8071,
      "step": 180800
    },
    {
      "epoch": 14.17045276515745,
      "grad_norm": 6.240551471710205,
      "learning_rate": 3.819128936236879e-05,
      "loss": 1.7736,
      "step": 180900
    },
    {
      "epoch": 14.178286072379759,
      "grad_norm": 4.813060283660889,
      "learning_rate": 3.81847616063502e-05,
      "loss": 1.7191,
      "step": 181000
    },
    {
      "epoch": 14.186119379602069,
      "grad_norm": 6.194219589233398,
      "learning_rate": 3.8178233850331616e-05,
      "loss": 1.7411,
      "step": 181100
    },
    {
      "epoch": 14.193952686824376,
      "grad_norm": 6.706739902496338,
      "learning_rate": 3.817170609431302e-05,
      "loss": 1.7631,
      "step": 181200
    },
    {
      "epoch": 14.201785994046686,
      "grad_norm": 6.138462066650391,
      "learning_rate": 3.816517833829443e-05,
      "loss": 1.7144,
      "step": 181300
    },
    {
      "epoch": 14.209619301268996,
      "grad_norm": 6.594637870788574,
      "learning_rate": 3.815865058227584e-05,
      "loss": 1.8938,
      "step": 181400
    },
    {
      "epoch": 14.217452608491305,
      "grad_norm": 4.6312150955200195,
      "learning_rate": 3.8152122826257245e-05,
      "loss": 1.765,
      "step": 181500
    },
    {
      "epoch": 14.225285915713615,
      "grad_norm": 6.4310526847839355,
      "learning_rate": 3.814559507023865e-05,
      "loss": 1.6491,
      "step": 181600
    },
    {
      "epoch": 14.233119222935924,
      "grad_norm": 7.169839859008789,
      "learning_rate": 3.813906731422007e-05,
      "loss": 1.7094,
      "step": 181700
    },
    {
      "epoch": 14.240952530158232,
      "grad_norm": 6.758798122406006,
      "learning_rate": 3.8132539558201476e-05,
      "loss": 1.6624,
      "step": 181800
    },
    {
      "epoch": 14.248785837380542,
      "grad_norm": 5.482122898101807,
      "learning_rate": 3.812601180218288e-05,
      "loss": 1.7613,
      "step": 181900
    },
    {
      "epoch": 14.256619144602851,
      "grad_norm": 4.8564229011535645,
      "learning_rate": 3.8119484046164295e-05,
      "loss": 1.7829,
      "step": 182000
    },
    {
      "epoch": 14.26445245182516,
      "grad_norm": 5.716516494750977,
      "learning_rate": 3.81129562901457e-05,
      "loss": 1.714,
      "step": 182100
    },
    {
      "epoch": 14.27228575904747,
      "grad_norm": 5.199207305908203,
      "learning_rate": 3.8106428534127106e-05,
      "loss": 1.7543,
      "step": 182200
    },
    {
      "epoch": 14.28011906626978,
      "grad_norm": 5.484793186187744,
      "learning_rate": 3.809990077810852e-05,
      "loss": 1.8003,
      "step": 182300
    },
    {
      "epoch": 14.287952373492088,
      "grad_norm": 5.968092918395996,
      "learning_rate": 3.809337302208993e-05,
      "loss": 1.8526,
      "step": 182400
    },
    {
      "epoch": 14.295785680714397,
      "grad_norm": 9.399559020996094,
      "learning_rate": 3.808684526607134e-05,
      "loss": 1.7246,
      "step": 182500
    },
    {
      "epoch": 14.303618987936707,
      "grad_norm": 4.845795631408691,
      "learning_rate": 3.808031751005275e-05,
      "loss": 1.6893,
      "step": 182600
    },
    {
      "epoch": 14.311452295159016,
      "grad_norm": 8.606964111328125,
      "learning_rate": 3.8073789754034155e-05,
      "loss": 1.8143,
      "step": 182700
    },
    {
      "epoch": 14.319285602381326,
      "grad_norm": 6.669313430786133,
      "learning_rate": 3.806726199801556e-05,
      "loss": 1.6675,
      "step": 182800
    },
    {
      "epoch": 14.327118909603634,
      "grad_norm": 5.629894256591797,
      "learning_rate": 3.8060734241996974e-05,
      "loss": 1.759,
      "step": 182900
    },
    {
      "epoch": 14.334952216825943,
      "grad_norm": 5.764263153076172,
      "learning_rate": 3.8054206485978386e-05,
      "loss": 1.7448,
      "step": 183000
    },
    {
      "epoch": 14.342785524048253,
      "grad_norm": 5.345448017120361,
      "learning_rate": 3.804767872995979e-05,
      "loss": 1.715,
      "step": 183100
    },
    {
      "epoch": 14.350618831270562,
      "grad_norm": 4.108423709869385,
      "learning_rate": 3.80411509739412e-05,
      "loss": 1.8029,
      "step": 183200
    },
    {
      "epoch": 14.358452138492872,
      "grad_norm": 5.743382930755615,
      "learning_rate": 3.803462321792261e-05,
      "loss": 1.7455,
      "step": 183300
    },
    {
      "epoch": 14.366285445715182,
      "grad_norm": 4.6700758934021,
      "learning_rate": 3.8028095461904016e-05,
      "loss": 1.7784,
      "step": 183400
    },
    {
      "epoch": 14.37411875293749,
      "grad_norm": 5.113062858581543,
      "learning_rate": 3.802156770588542e-05,
      "loss": 1.7767,
      "step": 183500
    },
    {
      "epoch": 14.381952060159799,
      "grad_norm": 3.7438130378723145,
      "learning_rate": 3.8015039949866834e-05,
      "loss": 1.7054,
      "step": 183600
    },
    {
      "epoch": 14.389785367382109,
      "grad_norm": 6.032365798950195,
      "learning_rate": 3.800851219384825e-05,
      "loss": 1.7892,
      "step": 183700
    },
    {
      "epoch": 14.397618674604418,
      "grad_norm": 4.407660484313965,
      "learning_rate": 3.800198443782965e-05,
      "loss": 1.7804,
      "step": 183800
    },
    {
      "epoch": 14.405451981826728,
      "grad_norm": 5.203427314758301,
      "learning_rate": 3.7995456681811065e-05,
      "loss": 1.8304,
      "step": 183900
    },
    {
      "epoch": 14.413285289049037,
      "grad_norm": 4.649906158447266,
      "learning_rate": 3.798892892579247e-05,
      "loss": 1.753,
      "step": 184000
    },
    {
      "epoch": 14.421118596271345,
      "grad_norm": 5.142425537109375,
      "learning_rate": 3.798240116977388e-05,
      "loss": 1.863,
      "step": 184100
    },
    {
      "epoch": 14.428951903493655,
      "grad_norm": 6.617918968200684,
      "learning_rate": 3.797587341375529e-05,
      "loss": 1.6789,
      "step": 184200
    },
    {
      "epoch": 14.436785210715964,
      "grad_norm": 5.201137542724609,
      "learning_rate": 3.79693456577367e-05,
      "loss": 1.8062,
      "step": 184300
    },
    {
      "epoch": 14.444618517938274,
      "grad_norm": 5.846580982208252,
      "learning_rate": 3.796281790171811e-05,
      "loss": 1.7833,
      "step": 184400
    },
    {
      "epoch": 14.452451825160583,
      "grad_norm": 9.17229175567627,
      "learning_rate": 3.795629014569951e-05,
      "loss": 1.7426,
      "step": 184500
    },
    {
      "epoch": 14.460285132382893,
      "grad_norm": 6.0675530433654785,
      "learning_rate": 3.7949762389680926e-05,
      "loss": 1.7314,
      "step": 184600
    },
    {
      "epoch": 14.4681184396052,
      "grad_norm": 9.093511581420898,
      "learning_rate": 3.794323463366233e-05,
      "loss": 1.7806,
      "step": 184700
    },
    {
      "epoch": 14.47595174682751,
      "grad_norm": 4.444270133972168,
      "learning_rate": 3.793670687764374e-05,
      "loss": 1.8126,
      "step": 184800
    },
    {
      "epoch": 14.48378505404982,
      "grad_norm": 6.590786457061768,
      "learning_rate": 3.793017912162516e-05,
      "loss": 1.7967,
      "step": 184900
    },
    {
      "epoch": 14.49161836127213,
      "grad_norm": 6.0653204917907715,
      "learning_rate": 3.792365136560656e-05,
      "loss": 1.7137,
      "step": 185000
    },
    {
      "epoch": 14.499451668494439,
      "grad_norm": 9.089346885681152,
      "learning_rate": 3.791712360958797e-05,
      "loss": 1.8181,
      "step": 185100
    },
    {
      "epoch": 14.507284975716747,
      "grad_norm": 6.1041460037231445,
      "learning_rate": 3.791059585356938e-05,
      "loss": 1.6321,
      "step": 185200
    },
    {
      "epoch": 14.515118282939056,
      "grad_norm": 5.1976165771484375,
      "learning_rate": 3.7904068097550787e-05,
      "loss": 1.7828,
      "step": 185300
    },
    {
      "epoch": 14.522951590161366,
      "grad_norm": 6.9470906257629395,
      "learning_rate": 3.789754034153219e-05,
      "loss": 1.7697,
      "step": 185400
    },
    {
      "epoch": 14.530784897383676,
      "grad_norm": 6.324594020843506,
      "learning_rate": 3.7891012585513605e-05,
      "loss": 1.7206,
      "step": 185500
    },
    {
      "epoch": 14.538618204605985,
      "grad_norm": 5.928624629974365,
      "learning_rate": 3.788448482949502e-05,
      "loss": 1.7916,
      "step": 185600
    },
    {
      "epoch": 14.546451511828295,
      "grad_norm": 5.088656425476074,
      "learning_rate": 3.787795707347642e-05,
      "loss": 1.7733,
      "step": 185700
    },
    {
      "epoch": 14.554284819050602,
      "grad_norm": 4.161291599273682,
      "learning_rate": 3.7871429317457836e-05,
      "loss": 1.7026,
      "step": 185800
    },
    {
      "epoch": 14.562118126272912,
      "grad_norm": 6.035133361816406,
      "learning_rate": 3.786490156143924e-05,
      "loss": 1.7751,
      "step": 185900
    },
    {
      "epoch": 14.569951433495222,
      "grad_norm": 6.937087535858154,
      "learning_rate": 3.785837380542065e-05,
      "loss": 1.7479,
      "step": 186000
    },
    {
      "epoch": 14.577784740717531,
      "grad_norm": 6.413511276245117,
      "learning_rate": 3.785184604940206e-05,
      "loss": 1.7425,
      "step": 186100
    },
    {
      "epoch": 14.58561804793984,
      "grad_norm": 6.7800517082214355,
      "learning_rate": 3.784531829338347e-05,
      "loss": 1.8044,
      "step": 186200
    },
    {
      "epoch": 14.593451355162149,
      "grad_norm": 5.898059844970703,
      "learning_rate": 3.783879053736488e-05,
      "loss": 1.8332,
      "step": 186300
    },
    {
      "epoch": 14.601284662384458,
      "grad_norm": 5.610795497894287,
      "learning_rate": 3.7832262781346284e-05,
      "loss": 1.797,
      "step": 186400
    },
    {
      "epoch": 14.609117969606768,
      "grad_norm": 5.881318092346191,
      "learning_rate": 3.7825735025327696e-05,
      "loss": 1.8041,
      "step": 186500
    },
    {
      "epoch": 14.616951276829077,
      "grad_norm": 6.361812114715576,
      "learning_rate": 3.78192072693091e-05,
      "loss": 1.6895,
      "step": 186600
    },
    {
      "epoch": 14.624784584051387,
      "grad_norm": 7.070523738861084,
      "learning_rate": 3.781267951329051e-05,
      "loss": 1.8082,
      "step": 186700
    },
    {
      "epoch": 14.632617891273696,
      "grad_norm": 4.611704349517822,
      "learning_rate": 3.780615175727192e-05,
      "loss": 1.7666,
      "step": 186800
    },
    {
      "epoch": 14.640451198496004,
      "grad_norm": 6.324965000152588,
      "learning_rate": 3.779962400125333e-05,
      "loss": 1.74,
      "step": 186900
    },
    {
      "epoch": 14.648284505718314,
      "grad_norm": 5.680453777313232,
      "learning_rate": 3.779309624523474e-05,
      "loss": 1.6851,
      "step": 187000
    },
    {
      "epoch": 14.656117812940623,
      "grad_norm": 7.821564197540283,
      "learning_rate": 3.778656848921615e-05,
      "loss": 1.8092,
      "step": 187100
    },
    {
      "epoch": 14.663951120162933,
      "grad_norm": 5.294644832611084,
      "learning_rate": 3.778004073319756e-05,
      "loss": 1.7418,
      "step": 187200
    },
    {
      "epoch": 14.671784427385242,
      "grad_norm": 5.456587314605713,
      "learning_rate": 3.777351297717896e-05,
      "loss": 1.8756,
      "step": 187300
    },
    {
      "epoch": 14.679617734607552,
      "grad_norm": 4.2578558921813965,
      "learning_rate": 3.7766985221160375e-05,
      "loss": 1.8036,
      "step": 187400
    },
    {
      "epoch": 14.68745104182986,
      "grad_norm": 5.945379257202148,
      "learning_rate": 3.776045746514179e-05,
      "loss": 1.7986,
      "step": 187500
    },
    {
      "epoch": 14.69528434905217,
      "grad_norm": 5.89130973815918,
      "learning_rate": 3.7753929709123194e-05,
      "loss": 1.7647,
      "step": 187600
    },
    {
      "epoch": 14.703117656274479,
      "grad_norm": 5.600010871887207,
      "learning_rate": 3.7747401953104606e-05,
      "loss": 1.7774,
      "step": 187700
    },
    {
      "epoch": 14.710950963496789,
      "grad_norm": 6.939831733703613,
      "learning_rate": 3.774087419708601e-05,
      "loss": 1.7806,
      "step": 187800
    },
    {
      "epoch": 14.718784270719098,
      "grad_norm": 6.346406936645508,
      "learning_rate": 3.773434644106742e-05,
      "loss": 1.7714,
      "step": 187900
    },
    {
      "epoch": 14.726617577941408,
      "grad_norm": 6.160155773162842,
      "learning_rate": 3.7727818685048824e-05,
      "loss": 1.6578,
      "step": 188000
    },
    {
      "epoch": 14.734450885163715,
      "grad_norm": 6.223787784576416,
      "learning_rate": 3.772129092903024e-05,
      "loss": 1.8447,
      "step": 188100
    },
    {
      "epoch": 14.742284192386025,
      "grad_norm": 4.597732067108154,
      "learning_rate": 3.771476317301165e-05,
      "loss": 1.7205,
      "step": 188200
    },
    {
      "epoch": 14.750117499608335,
      "grad_norm": 6.294755935668945,
      "learning_rate": 3.7708235416993054e-05,
      "loss": 1.8172,
      "step": 188300
    },
    {
      "epoch": 14.757950806830644,
      "grad_norm": 6.051687240600586,
      "learning_rate": 3.770170766097447e-05,
      "loss": 1.7946,
      "step": 188400
    },
    {
      "epoch": 14.765784114052954,
      "grad_norm": 5.567879676818848,
      "learning_rate": 3.769517990495587e-05,
      "loss": 1.8755,
      "step": 188500
    },
    {
      "epoch": 14.773617421275262,
      "grad_norm": 7.091922283172607,
      "learning_rate": 3.768865214893728e-05,
      "loss": 1.8157,
      "step": 188600
    },
    {
      "epoch": 14.781450728497571,
      "grad_norm": 5.8578925132751465,
      "learning_rate": 3.768212439291869e-05,
      "loss": 1.7593,
      "step": 188700
    },
    {
      "epoch": 14.78928403571988,
      "grad_norm": 5.877974510192871,
      "learning_rate": 3.7675596636900104e-05,
      "loss": 1.8126,
      "step": 188800
    },
    {
      "epoch": 14.79711734294219,
      "grad_norm": 5.150978088378906,
      "learning_rate": 3.766906888088151e-05,
      "loss": 1.8558,
      "step": 188900
    },
    {
      "epoch": 14.8049506501645,
      "grad_norm": 8.474013328552246,
      "learning_rate": 3.766254112486292e-05,
      "loss": 1.6979,
      "step": 189000
    },
    {
      "epoch": 14.81278395738681,
      "grad_norm": 5.331034183502197,
      "learning_rate": 3.765601336884433e-05,
      "loss": 1.7797,
      "step": 189100
    },
    {
      "epoch": 14.820617264609117,
      "grad_norm": 5.129733085632324,
      "learning_rate": 3.7649485612825733e-05,
      "loss": 1.7436,
      "step": 189200
    },
    {
      "epoch": 14.828450571831427,
      "grad_norm": 6.9717230796813965,
      "learning_rate": 3.7642957856807146e-05,
      "loss": 1.8174,
      "step": 189300
    },
    {
      "epoch": 14.836283879053736,
      "grad_norm": 4.363758087158203,
      "learning_rate": 3.763643010078856e-05,
      "loss": 1.7849,
      "step": 189400
    },
    {
      "epoch": 14.844117186276046,
      "grad_norm": 4.9564619064331055,
      "learning_rate": 3.7629902344769964e-05,
      "loss": 1.8397,
      "step": 189500
    },
    {
      "epoch": 14.851950493498355,
      "grad_norm": 6.413949012756348,
      "learning_rate": 3.762337458875137e-05,
      "loss": 1.748,
      "step": 189600
    },
    {
      "epoch": 14.859783800720665,
      "grad_norm": 6.846943378448486,
      "learning_rate": 3.761684683273278e-05,
      "loss": 1.7716,
      "step": 189700
    },
    {
      "epoch": 14.867617107942973,
      "grad_norm": 6.0085272789001465,
      "learning_rate": 3.761031907671419e-05,
      "loss": 1.7655,
      "step": 189800
    },
    {
      "epoch": 14.875450415165282,
      "grad_norm": 6.693475246429443,
      "learning_rate": 3.7603791320695594e-05,
      "loss": 1.8326,
      "step": 189900
    },
    {
      "epoch": 14.883283722387592,
      "grad_norm": 6.613133430480957,
      "learning_rate": 3.759726356467701e-05,
      "loss": 1.7824,
      "step": 190000
    },
    {
      "epoch": 14.891117029609902,
      "grad_norm": 6.439784049987793,
      "learning_rate": 3.759073580865842e-05,
      "loss": 1.7966,
      "step": 190100
    },
    {
      "epoch": 14.898950336832211,
      "grad_norm": 5.586026668548584,
      "learning_rate": 3.7584208052639825e-05,
      "loss": 1.7465,
      "step": 190200
    },
    {
      "epoch": 14.90678364405452,
      "grad_norm": 5.8662614822387695,
      "learning_rate": 3.757768029662124e-05,
      "loss": 1.8326,
      "step": 190300
    },
    {
      "epoch": 14.914616951276829,
      "grad_norm": 5.488516330718994,
      "learning_rate": 3.757115254060264e-05,
      "loss": 1.7804,
      "step": 190400
    },
    {
      "epoch": 14.922450258499138,
      "grad_norm": 9.323443412780762,
      "learning_rate": 3.756462478458405e-05,
      "loss": 1.7626,
      "step": 190500
    },
    {
      "epoch": 14.930283565721448,
      "grad_norm": 4.298690319061279,
      "learning_rate": 3.755809702856546e-05,
      "loss": 1.9199,
      "step": 190600
    },
    {
      "epoch": 14.938116872943757,
      "grad_norm": 5.659178733825684,
      "learning_rate": 3.7551569272546874e-05,
      "loss": 1.7929,
      "step": 190700
    },
    {
      "epoch": 14.945950180166067,
      "grad_norm": 6.975569725036621,
      "learning_rate": 3.754504151652828e-05,
      "loss": 1.7965,
      "step": 190800
    },
    {
      "epoch": 14.953783487388375,
      "grad_norm": 5.9642109870910645,
      "learning_rate": 3.753851376050969e-05,
      "loss": 1.778,
      "step": 190900
    },
    {
      "epoch": 14.961616794610684,
      "grad_norm": 4.234434127807617,
      "learning_rate": 3.75319860044911e-05,
      "loss": 1.6493,
      "step": 191000
    },
    {
      "epoch": 14.969450101832994,
      "grad_norm": 4.878813743591309,
      "learning_rate": 3.7525458248472504e-05,
      "loss": 1.8782,
      "step": 191100
    },
    {
      "epoch": 14.977283409055303,
      "grad_norm": 5.77919340133667,
      "learning_rate": 3.751893049245391e-05,
      "loss": 1.7457,
      "step": 191200
    },
    {
      "epoch": 14.985116716277613,
      "grad_norm": 7.656811237335205,
      "learning_rate": 3.751240273643533e-05,
      "loss": 1.7816,
      "step": 191300
    },
    {
      "epoch": 14.992950023499922,
      "grad_norm": 5.7168145179748535,
      "learning_rate": 3.7505874980416735e-05,
      "loss": 1.8838,
      "step": 191400
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.811212420463562,
      "eval_runtime": 2.8782,
      "eval_samples_per_second": 233.476,
      "eval_steps_per_second": 233.476,
      "step": 191490
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.541782021522522,
      "eval_runtime": 55.1076,
      "eval_samples_per_second": 231.656,
      "eval_steps_per_second": 231.656,
      "step": 191490
    },
    {
      "epoch": 15.00078333072223,
      "grad_norm": 6.053223609924316,
      "learning_rate": 3.749934722439814e-05,
      "loss": 1.7469,
      "step": 191500
    },
    {
      "epoch": 15.00861663794454,
      "grad_norm": 6.655354976654053,
      "learning_rate": 3.749281946837955e-05,
      "loss": 1.7557,
      "step": 191600
    },
    {
      "epoch": 15.01644994516685,
      "grad_norm": 6.324655532836914,
      "learning_rate": 3.748629171236096e-05,
      "loss": 1.7444,
      "step": 191700
    },
    {
      "epoch": 15.024283252389159,
      "grad_norm": 7.557750225067139,
      "learning_rate": 3.7479763956342365e-05,
      "loss": 1.708,
      "step": 191800
    },
    {
      "epoch": 15.032116559611469,
      "grad_norm": 5.614170074462891,
      "learning_rate": 3.747323620032378e-05,
      "loss": 1.706,
      "step": 191900
    },
    {
      "epoch": 15.039949866833778,
      "grad_norm": 4.582817554473877,
      "learning_rate": 3.746670844430519e-05,
      "loss": 1.7969,
      "step": 192000
    },
    {
      "epoch": 15.047783174056086,
      "grad_norm": 4.535934925079346,
      "learning_rate": 3.7460180688286596e-05,
      "loss": 1.7428,
      "step": 192100
    },
    {
      "epoch": 15.055616481278395,
      "grad_norm": 6.916611671447754,
      "learning_rate": 3.745365293226801e-05,
      "loss": 1.8045,
      "step": 192200
    },
    {
      "epoch": 15.063449788500705,
      "grad_norm": 5.2937140464782715,
      "learning_rate": 3.7447125176249414e-05,
      "loss": 1.7221,
      "step": 192300
    },
    {
      "epoch": 15.071283095723015,
      "grad_norm": 6.211253643035889,
      "learning_rate": 3.744059742023082e-05,
      "loss": 1.7489,
      "step": 192400
    },
    {
      "epoch": 15.079116402945324,
      "grad_norm": 6.449056625366211,
      "learning_rate": 3.743406966421223e-05,
      "loss": 1.753,
      "step": 192500
    },
    {
      "epoch": 15.086949710167632,
      "grad_norm": 5.910313129425049,
      "learning_rate": 3.7427541908193645e-05,
      "loss": 1.7945,
      "step": 192600
    },
    {
      "epoch": 15.094783017389942,
      "grad_norm": 5.79087495803833,
      "learning_rate": 3.742101415217505e-05,
      "loss": 1.7745,
      "step": 192700
    },
    {
      "epoch": 15.102616324612251,
      "grad_norm": 7.9059929847717285,
      "learning_rate": 3.741448639615646e-05,
      "loss": 1.7985,
      "step": 192800
    },
    {
      "epoch": 15.11044963183456,
      "grad_norm": 4.259535789489746,
      "learning_rate": 3.740795864013787e-05,
      "loss": 1.6808,
      "step": 192900
    },
    {
      "epoch": 15.11828293905687,
      "grad_norm": 5.887284278869629,
      "learning_rate": 3.7401430884119275e-05,
      "loss": 1.7181,
      "step": 193000
    },
    {
      "epoch": 15.12611624627918,
      "grad_norm": 3.834562301635742,
      "learning_rate": 3.739490312810068e-05,
      "loss": 1.7392,
      "step": 193100
    },
    {
      "epoch": 15.133949553501488,
      "grad_norm": 6.519359588623047,
      "learning_rate": 3.738837537208209e-05,
      "loss": 1.6925,
      "step": 193200
    },
    {
      "epoch": 15.141782860723797,
      "grad_norm": 6.56142520904541,
      "learning_rate": 3.7381847616063505e-05,
      "loss": 1.7416,
      "step": 193300
    },
    {
      "epoch": 15.149616167946107,
      "grad_norm": 5.418981075286865,
      "learning_rate": 3.737531986004491e-05,
      "loss": 1.7149,
      "step": 193400
    },
    {
      "epoch": 15.157449475168416,
      "grad_norm": 5.6343092918396,
      "learning_rate": 3.7368792104026324e-05,
      "loss": 1.7881,
      "step": 193500
    },
    {
      "epoch": 15.165282782390726,
      "grad_norm": 5.105031967163086,
      "learning_rate": 3.736226434800773e-05,
      "loss": 1.8066,
      "step": 193600
    },
    {
      "epoch": 15.173116089613035,
      "grad_norm": 4.121769428253174,
      "learning_rate": 3.7355736591989135e-05,
      "loss": 1.7177,
      "step": 193700
    },
    {
      "epoch": 15.180949396835343,
      "grad_norm": 6.1790008544921875,
      "learning_rate": 3.734920883597055e-05,
      "loss": 1.7635,
      "step": 193800
    },
    {
      "epoch": 15.188782704057653,
      "grad_norm": 6.184379577636719,
      "learning_rate": 3.734268107995196e-05,
      "loss": 1.7182,
      "step": 193900
    },
    {
      "epoch": 15.196616011279962,
      "grad_norm": 6.058042049407959,
      "learning_rate": 3.7336153323933366e-05,
      "loss": 1.7814,
      "step": 194000
    },
    {
      "epoch": 15.204449318502272,
      "grad_norm": 6.393146991729736,
      "learning_rate": 3.732962556791478e-05,
      "loss": 1.81,
      "step": 194100
    },
    {
      "epoch": 15.212282625724582,
      "grad_norm": 5.829782009124756,
      "learning_rate": 3.7323097811896185e-05,
      "loss": 1.7771,
      "step": 194200
    },
    {
      "epoch": 15.22011593294689,
      "grad_norm": 5.938756465911865,
      "learning_rate": 3.731657005587759e-05,
      "loss": 1.755,
      "step": 194300
    },
    {
      "epoch": 15.227949240169199,
      "grad_norm": 6.927833557128906,
      "learning_rate": 3.7310042299859e-05,
      "loss": 1.7742,
      "step": 194400
    },
    {
      "epoch": 15.235782547391509,
      "grad_norm": 7.480703353881836,
      "learning_rate": 3.7303514543840415e-05,
      "loss": 1.6887,
      "step": 194500
    },
    {
      "epoch": 15.243615854613818,
      "grad_norm": 7.951548099517822,
      "learning_rate": 3.729698678782182e-05,
      "loss": 1.8603,
      "step": 194600
    },
    {
      "epoch": 15.251449161836128,
      "grad_norm": 7.059107303619385,
      "learning_rate": 3.729045903180323e-05,
      "loss": 1.8111,
      "step": 194700
    },
    {
      "epoch": 15.259282469058437,
      "grad_norm": 6.03836727142334,
      "learning_rate": 3.728393127578464e-05,
      "loss": 1.8273,
      "step": 194800
    },
    {
      "epoch": 15.267115776280745,
      "grad_norm": 5.193647384643555,
      "learning_rate": 3.7277403519766045e-05,
      "loss": 1.792,
      "step": 194900
    },
    {
      "epoch": 15.274949083503055,
      "grad_norm": 6.354994773864746,
      "learning_rate": 3.727087576374745e-05,
      "loss": 1.7422,
      "step": 195000
    },
    {
      "epoch": 15.282782390725364,
      "grad_norm": 5.129561901092529,
      "learning_rate": 3.7264348007728864e-05,
      "loss": 1.8527,
      "step": 195100
    },
    {
      "epoch": 15.290615697947674,
      "grad_norm": 6.533755302429199,
      "learning_rate": 3.7257820251710276e-05,
      "loss": 1.7682,
      "step": 195200
    },
    {
      "epoch": 15.298449005169983,
      "grad_norm": 5.83456563949585,
      "learning_rate": 3.725129249569168e-05,
      "loss": 1.8479,
      "step": 195300
    },
    {
      "epoch": 15.306282312392293,
      "grad_norm": 6.500914096832275,
      "learning_rate": 3.7244764739673094e-05,
      "loss": 1.7017,
      "step": 195400
    },
    {
      "epoch": 15.3141156196146,
      "grad_norm": 5.067908763885498,
      "learning_rate": 3.72382369836545e-05,
      "loss": 1.745,
      "step": 195500
    },
    {
      "epoch": 15.32194892683691,
      "grad_norm": 5.260638236999512,
      "learning_rate": 3.7231709227635906e-05,
      "loss": 1.7957,
      "step": 195600
    },
    {
      "epoch": 15.32978223405922,
      "grad_norm": 6.4827351570129395,
      "learning_rate": 3.722518147161732e-05,
      "loss": 1.7254,
      "step": 195700
    },
    {
      "epoch": 15.33761554128153,
      "grad_norm": 5.416661262512207,
      "learning_rate": 3.721865371559873e-05,
      "loss": 1.809,
      "step": 195800
    },
    {
      "epoch": 15.345448848503839,
      "grad_norm": 6.1988420486450195,
      "learning_rate": 3.721212595958014e-05,
      "loss": 1.729,
      "step": 195900
    },
    {
      "epoch": 15.353282155726147,
      "grad_norm": 6.855597496032715,
      "learning_rate": 3.720559820356155e-05,
      "loss": 1.7768,
      "step": 196000
    },
    {
      "epoch": 15.361115462948456,
      "grad_norm": 5.375616550445557,
      "learning_rate": 3.7199070447542955e-05,
      "loss": 1.8245,
      "step": 196100
    },
    {
      "epoch": 15.368948770170766,
      "grad_norm": 6.570098876953125,
      "learning_rate": 3.719254269152436e-05,
      "loss": 1.8002,
      "step": 196200
    },
    {
      "epoch": 15.376782077393075,
      "grad_norm": 2.5269062519073486,
      "learning_rate": 3.718601493550577e-05,
      "loss": 1.7325,
      "step": 196300
    },
    {
      "epoch": 15.384615384615385,
      "grad_norm": 6.965487480163574,
      "learning_rate": 3.717948717948718e-05,
      "loss": 1.8332,
      "step": 196400
    },
    {
      "epoch": 15.392448691837695,
      "grad_norm": 4.638038635253906,
      "learning_rate": 3.717295942346859e-05,
      "loss": 1.6867,
      "step": 196500
    },
    {
      "epoch": 15.400281999060002,
      "grad_norm": 6.532465934753418,
      "learning_rate": 3.716643166745e-05,
      "loss": 1.7047,
      "step": 196600
    },
    {
      "epoch": 15.408115306282312,
      "grad_norm": 6.6660075187683105,
      "learning_rate": 3.715990391143141e-05,
      "loss": 1.7873,
      "step": 196700
    },
    {
      "epoch": 15.415948613504622,
      "grad_norm": 5.198484420776367,
      "learning_rate": 3.7153376155412816e-05,
      "loss": 1.7463,
      "step": 196800
    },
    {
      "epoch": 15.423781920726931,
      "grad_norm": 4.630198955535889,
      "learning_rate": 3.714684839939422e-05,
      "loss": 1.8612,
      "step": 196900
    },
    {
      "epoch": 15.43161522794924,
      "grad_norm": 6.747849464416504,
      "learning_rate": 3.7140320643375634e-05,
      "loss": 1.7995,
      "step": 197000
    },
    {
      "epoch": 15.43944853517155,
      "grad_norm": 5.98021125793457,
      "learning_rate": 3.713379288735705e-05,
      "loss": 1.7491,
      "step": 197100
    },
    {
      "epoch": 15.447281842393858,
      "grad_norm": 6.16839075088501,
      "learning_rate": 3.712726513133845e-05,
      "loss": 1.8219,
      "step": 197200
    },
    {
      "epoch": 15.455115149616168,
      "grad_norm": 6.133669376373291,
      "learning_rate": 3.7120737375319865e-05,
      "loss": 1.7647,
      "step": 197300
    },
    {
      "epoch": 15.462948456838477,
      "grad_norm": 6.240448474884033,
      "learning_rate": 3.711420961930127e-05,
      "loss": 1.7605,
      "step": 197400
    },
    {
      "epoch": 15.470781764060787,
      "grad_norm": 5.458439350128174,
      "learning_rate": 3.7107681863282677e-05,
      "loss": 1.8013,
      "step": 197500
    },
    {
      "epoch": 15.478615071283096,
      "grad_norm": 7.32832670211792,
      "learning_rate": 3.710115410726409e-05,
      "loss": 1.7511,
      "step": 197600
    },
    {
      "epoch": 15.486448378505404,
      "grad_norm": 5.401645183563232,
      "learning_rate": 3.70946263512455e-05,
      "loss": 1.7556,
      "step": 197700
    },
    {
      "epoch": 15.494281685727714,
      "grad_norm": 5.209949493408203,
      "learning_rate": 3.708809859522691e-05,
      "loss": 1.7898,
      "step": 197800
    },
    {
      "epoch": 15.502114992950023,
      "grad_norm": 5.968122959136963,
      "learning_rate": 3.708157083920831e-05,
      "loss": 1.7406,
      "step": 197900
    },
    {
      "epoch": 15.509948300172333,
      "grad_norm": 8.55923843383789,
      "learning_rate": 3.7075043083189726e-05,
      "loss": 1.6993,
      "step": 198000
    },
    {
      "epoch": 15.517781607394642,
      "grad_norm": 5.745644569396973,
      "learning_rate": 3.706851532717113e-05,
      "loss": 1.7927,
      "step": 198100
    },
    {
      "epoch": 15.525614914616952,
      "grad_norm": 4.776991844177246,
      "learning_rate": 3.706198757115254e-05,
      "loss": 1.8608,
      "step": 198200
    },
    {
      "epoch": 15.53344822183926,
      "grad_norm": 4.876277923583984,
      "learning_rate": 3.705545981513395e-05,
      "loss": 1.6554,
      "step": 198300
    },
    {
      "epoch": 15.54128152906157,
      "grad_norm": 5.913731098175049,
      "learning_rate": 3.704893205911536e-05,
      "loss": 1.7789,
      "step": 198400
    },
    {
      "epoch": 15.549114836283879,
      "grad_norm": 6.104668617248535,
      "learning_rate": 3.704240430309677e-05,
      "loss": 1.6921,
      "step": 198500
    },
    {
      "epoch": 15.556948143506188,
      "grad_norm": 7.309671401977539,
      "learning_rate": 3.703587654707818e-05,
      "loss": 1.7332,
      "step": 198600
    },
    {
      "epoch": 15.564781450728498,
      "grad_norm": 6.616021156311035,
      "learning_rate": 3.7029348791059586e-05,
      "loss": 1.757,
      "step": 198700
    },
    {
      "epoch": 15.572614757950808,
      "grad_norm": 6.208805561065674,
      "learning_rate": 3.702282103504099e-05,
      "loss": 1.791,
      "step": 198800
    },
    {
      "epoch": 15.580448065173115,
      "grad_norm": 5.761775493621826,
      "learning_rate": 3.7016293279022405e-05,
      "loss": 1.6624,
      "step": 198900
    },
    {
      "epoch": 15.588281372395425,
      "grad_norm": 5.541751861572266,
      "learning_rate": 3.700976552300382e-05,
      "loss": 1.6959,
      "step": 199000
    },
    {
      "epoch": 15.596114679617735,
      "grad_norm": 5.66705322265625,
      "learning_rate": 3.700323776698522e-05,
      "loss": 1.7629,
      "step": 199100
    },
    {
      "epoch": 15.603947986840044,
      "grad_norm": 5.419274806976318,
      "learning_rate": 3.6996710010966636e-05,
      "loss": 1.7746,
      "step": 199200
    },
    {
      "epoch": 15.611781294062354,
      "grad_norm": 6.369231224060059,
      "learning_rate": 3.699018225494804e-05,
      "loss": 1.788,
      "step": 199300
    },
    {
      "epoch": 15.619614601284663,
      "grad_norm": 7.321242809295654,
      "learning_rate": 3.698365449892945e-05,
      "loss": 1.7729,
      "step": 199400
    },
    {
      "epoch": 15.627447908506971,
      "grad_norm": 5.602992534637451,
      "learning_rate": 3.697712674291086e-05,
      "loss": 1.8165,
      "step": 199500
    },
    {
      "epoch": 15.63528121572928,
      "grad_norm": 5.642589569091797,
      "learning_rate": 3.6970598986892265e-05,
      "loss": 1.7525,
      "step": 199600
    },
    {
      "epoch": 15.64311452295159,
      "grad_norm": 5.390480995178223,
      "learning_rate": 3.696407123087368e-05,
      "loss": 1.9027,
      "step": 199700
    },
    {
      "epoch": 15.6509478301739,
      "grad_norm": 7.5556840896606445,
      "learning_rate": 3.6957543474855084e-05,
      "loss": 1.8245,
      "step": 199800
    },
    {
      "epoch": 15.65878113739621,
      "grad_norm": 6.624140739440918,
      "learning_rate": 3.6951015718836496e-05,
      "loss": 1.7159,
      "step": 199900
    },
    {
      "epoch": 15.666614444618517,
      "grad_norm": 6.0968427658081055,
      "learning_rate": 3.69444879628179e-05,
      "loss": 1.6886,
      "step": 200000
    },
    {
      "epoch": 15.674447751840827,
      "grad_norm": 6.435976505279541,
      "learning_rate": 3.693796020679931e-05,
      "loss": 1.7756,
      "step": 200100
    },
    {
      "epoch": 15.682281059063136,
      "grad_norm": 5.732990741729736,
      "learning_rate": 3.693143245078072e-05,
      "loss": 1.7949,
      "step": 200200
    },
    {
      "epoch": 15.690114366285446,
      "grad_norm": 4.598323822021484,
      "learning_rate": 3.692490469476213e-05,
      "loss": 1.7645,
      "step": 200300
    },
    {
      "epoch": 15.697947673507755,
      "grad_norm": 4.385626316070557,
      "learning_rate": 3.691837693874354e-05,
      "loss": 1.7812,
      "step": 200400
    },
    {
      "epoch": 15.705780980730065,
      "grad_norm": 8.887057304382324,
      "learning_rate": 3.691184918272495e-05,
      "loss": 1.8227,
      "step": 200500
    },
    {
      "epoch": 15.713614287952373,
      "grad_norm": 6.180454254150391,
      "learning_rate": 3.690532142670636e-05,
      "loss": 1.8328,
      "step": 200600
    },
    {
      "epoch": 15.721447595174682,
      "grad_norm": 5.015071868896484,
      "learning_rate": 3.689879367068776e-05,
      "loss": 1.8054,
      "step": 200700
    },
    {
      "epoch": 15.729280902396992,
      "grad_norm": 4.403515338897705,
      "learning_rate": 3.6892265914669175e-05,
      "loss": 1.7892,
      "step": 200800
    },
    {
      "epoch": 15.737114209619302,
      "grad_norm": 6.019563674926758,
      "learning_rate": 3.688573815865059e-05,
      "loss": 1.7201,
      "step": 200900
    },
    {
      "epoch": 15.744947516841611,
      "grad_norm": 5.757798194885254,
      "learning_rate": 3.6879210402631994e-05,
      "loss": 1.8243,
      "step": 201000
    },
    {
      "epoch": 15.752780824063919,
      "grad_norm": 5.10944938659668,
      "learning_rate": 3.6872682646613406e-05,
      "loss": 1.7426,
      "step": 201100
    },
    {
      "epoch": 15.760614131286228,
      "grad_norm": 3.5380303859710693,
      "learning_rate": 3.686615489059481e-05,
      "loss": 1.7731,
      "step": 201200
    },
    {
      "epoch": 15.768447438508538,
      "grad_norm": 6.894423484802246,
      "learning_rate": 3.685962713457622e-05,
      "loss": 1.7634,
      "step": 201300
    },
    {
      "epoch": 15.776280745730848,
      "grad_norm": 4.9919281005859375,
      "learning_rate": 3.6853099378557623e-05,
      "loss": 1.7481,
      "step": 201400
    },
    {
      "epoch": 15.784114052953157,
      "grad_norm": 5.856508255004883,
      "learning_rate": 3.6846571622539036e-05,
      "loss": 1.8859,
      "step": 201500
    },
    {
      "epoch": 15.791947360175467,
      "grad_norm": 5.117400646209717,
      "learning_rate": 3.684004386652045e-05,
      "loss": 1.7121,
      "step": 201600
    },
    {
      "epoch": 15.799780667397775,
      "grad_norm": 5.421803951263428,
      "learning_rate": 3.6833516110501854e-05,
      "loss": 1.7171,
      "step": 201700
    },
    {
      "epoch": 15.807613974620084,
      "grad_norm": 5.521327018737793,
      "learning_rate": 3.682698835448327e-05,
      "loss": 1.7073,
      "step": 201800
    },
    {
      "epoch": 15.815447281842394,
      "grad_norm": 7.051358699798584,
      "learning_rate": 3.682046059846467e-05,
      "loss": 1.7413,
      "step": 201900
    },
    {
      "epoch": 15.823280589064703,
      "grad_norm": 7.158949851989746,
      "learning_rate": 3.681393284244608e-05,
      "loss": 1.8337,
      "step": 202000
    },
    {
      "epoch": 15.831113896287013,
      "grad_norm": 5.053206920623779,
      "learning_rate": 3.680740508642749e-05,
      "loss": 1.7982,
      "step": 202100
    },
    {
      "epoch": 15.838947203509322,
      "grad_norm": 6.717484474182129,
      "learning_rate": 3.6800877330408903e-05,
      "loss": 1.7516,
      "step": 202200
    },
    {
      "epoch": 15.84678051073163,
      "grad_norm": 6.685000419616699,
      "learning_rate": 3.679434957439031e-05,
      "loss": 1.7541,
      "step": 202300
    },
    {
      "epoch": 15.85461381795394,
      "grad_norm": 6.3043694496154785,
      "learning_rate": 3.678782181837172e-05,
      "loss": 1.8228,
      "step": 202400
    },
    {
      "epoch": 15.86244712517625,
      "grad_norm": 4.9641432762146,
      "learning_rate": 3.678129406235313e-05,
      "loss": 1.8126,
      "step": 202500
    },
    {
      "epoch": 15.870280432398559,
      "grad_norm": 5.809316635131836,
      "learning_rate": 3.677476630633453e-05,
      "loss": 1.7312,
      "step": 202600
    },
    {
      "epoch": 15.878113739620868,
      "grad_norm": 7.886962413787842,
      "learning_rate": 3.6768238550315946e-05,
      "loss": 1.7293,
      "step": 202700
    },
    {
      "epoch": 15.885947046843178,
      "grad_norm": 5.752218246459961,
      "learning_rate": 3.676171079429735e-05,
      "loss": 1.7334,
      "step": 202800
    },
    {
      "epoch": 15.893780354065486,
      "grad_norm": 6.022613525390625,
      "learning_rate": 3.6755183038278764e-05,
      "loss": 1.7968,
      "step": 202900
    },
    {
      "epoch": 15.901613661287795,
      "grad_norm": 8.7583589553833,
      "learning_rate": 3.674865528226017e-05,
      "loss": 1.7849,
      "step": 203000
    },
    {
      "epoch": 15.909446968510105,
      "grad_norm": 5.407005310058594,
      "learning_rate": 3.674212752624158e-05,
      "loss": 1.8006,
      "step": 203100
    },
    {
      "epoch": 15.917280275732415,
      "grad_norm": 5.973493576049805,
      "learning_rate": 3.673559977022299e-05,
      "loss": 1.7107,
      "step": 203200
    },
    {
      "epoch": 15.925113582954724,
      "grad_norm": 7.9872145652771,
      "learning_rate": 3.6729072014204394e-05,
      "loss": 1.8075,
      "step": 203300
    },
    {
      "epoch": 15.932946890177032,
      "grad_norm": 7.9034342765808105,
      "learning_rate": 3.6722544258185807e-05,
      "loss": 1.7652,
      "step": 203400
    },
    {
      "epoch": 15.940780197399341,
      "grad_norm": 5.072513103485107,
      "learning_rate": 3.671601650216722e-05,
      "loss": 1.7304,
      "step": 203500
    },
    {
      "epoch": 15.948613504621651,
      "grad_norm": 6.833290100097656,
      "learning_rate": 3.6709488746148625e-05,
      "loss": 1.691,
      "step": 203600
    },
    {
      "epoch": 15.95644681184396,
      "grad_norm": 6.00002384185791,
      "learning_rate": 3.670296099013004e-05,
      "loss": 1.7283,
      "step": 203700
    },
    {
      "epoch": 15.96428011906627,
      "grad_norm": 5.362172603607178,
      "learning_rate": 3.669643323411144e-05,
      "loss": 1.7987,
      "step": 203800
    },
    {
      "epoch": 15.97211342628858,
      "grad_norm": 6.6461381912231445,
      "learning_rate": 3.668990547809285e-05,
      "loss": 1.8209,
      "step": 203900
    },
    {
      "epoch": 15.979946733510888,
      "grad_norm": 5.189323425292969,
      "learning_rate": 3.668337772207426e-05,
      "loss": 1.7706,
      "step": 204000
    },
    {
      "epoch": 15.987780040733197,
      "grad_norm": 8.280928611755371,
      "learning_rate": 3.6676849966055674e-05,
      "loss": 1.7567,
      "step": 204100
    },
    {
      "epoch": 15.995613347955507,
      "grad_norm": 6.27910041809082,
      "learning_rate": 3.667032221003708e-05,
      "loss": 1.7539,
      "step": 204200
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.8107103109359741,
      "eval_runtime": 2.9173,
      "eval_samples_per_second": 230.354,
      "eval_steps_per_second": 230.354,
      "step": 204256
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.5338646173477173,
      "eval_runtime": 55.1842,
      "eval_samples_per_second": 231.334,
      "eval_steps_per_second": 231.334,
      "step": 204256
    },
    {
      "epoch": 16.003446655177815,
      "grad_norm": 4.811611652374268,
      "learning_rate": 3.666379445401849e-05,
      "loss": 1.8154,
      "step": 204300
    },
    {
      "epoch": 16.011279962400124,
      "grad_norm": 6.742312908172607,
      "learning_rate": 3.66572666979999e-05,
      "loss": 1.8218,
      "step": 204400
    },
    {
      "epoch": 16.019113269622434,
      "grad_norm": 5.689087867736816,
      "learning_rate": 3.6650738941981304e-05,
      "loss": 1.6833,
      "step": 204500
    },
    {
      "epoch": 16.026946576844743,
      "grad_norm": 5.006751537322998,
      "learning_rate": 3.6644211185962716e-05,
      "loss": 1.7295,
      "step": 204600
    },
    {
      "epoch": 16.034779884067053,
      "grad_norm": 4.1511454582214355,
      "learning_rate": 3.663768342994412e-05,
      "loss": 1.7789,
      "step": 204700
    },
    {
      "epoch": 16.042613191289362,
      "grad_norm": 6.151676177978516,
      "learning_rate": 3.6631155673925535e-05,
      "loss": 1.6856,
      "step": 204800
    },
    {
      "epoch": 16.050446498511672,
      "grad_norm": 7.6806182861328125,
      "learning_rate": 3.662462791790694e-05,
      "loss": 1.76,
      "step": 204900
    },
    {
      "epoch": 16.05827980573398,
      "grad_norm": 5.866674423217773,
      "learning_rate": 3.661810016188835e-05,
      "loss": 1.7018,
      "step": 205000
    },
    {
      "epoch": 16.06611311295629,
      "grad_norm": 6.336557388305664,
      "learning_rate": 3.661157240586976e-05,
      "loss": 1.6901,
      "step": 205100
    },
    {
      "epoch": 16.0739464201786,
      "grad_norm": 8.388619422912598,
      "learning_rate": 3.6605044649851165e-05,
      "loss": 1.7751,
      "step": 205200
    },
    {
      "epoch": 16.08177972740091,
      "grad_norm": 6.674232006072998,
      "learning_rate": 3.659851689383258e-05,
      "loss": 1.7149,
      "step": 205300
    },
    {
      "epoch": 16.089613034623216,
      "grad_norm": 7.487720966339111,
      "learning_rate": 3.659198913781399e-05,
      "loss": 1.7491,
      "step": 205400
    },
    {
      "epoch": 16.097446341845526,
      "grad_norm": 5.011544227600098,
      "learning_rate": 3.6585461381795395e-05,
      "loss": 1.729,
      "step": 205500
    },
    {
      "epoch": 16.105279649067835,
      "grad_norm": 5.299734115600586,
      "learning_rate": 3.657893362577681e-05,
      "loss": 1.7065,
      "step": 205600
    },
    {
      "epoch": 16.113112956290145,
      "grad_norm": 8.070409774780273,
      "learning_rate": 3.6572405869758214e-05,
      "loss": 1.6901,
      "step": 205700
    },
    {
      "epoch": 16.120946263512455,
      "grad_norm": 5.734541416168213,
      "learning_rate": 3.656587811373962e-05,
      "loss": 1.7489,
      "step": 205800
    },
    {
      "epoch": 16.128779570734764,
      "grad_norm": 4.356464862823486,
      "learning_rate": 3.655935035772103e-05,
      "loss": 1.7983,
      "step": 205900
    },
    {
      "epoch": 16.136612877957074,
      "grad_norm": 5.2182488441467285,
      "learning_rate": 3.655282260170244e-05,
      "loss": 1.8008,
      "step": 206000
    },
    {
      "epoch": 16.144446185179383,
      "grad_norm": 5.041721820831299,
      "learning_rate": 3.654629484568385e-05,
      "loss": 1.7886,
      "step": 206100
    },
    {
      "epoch": 16.152279492401693,
      "grad_norm": 5.971475601196289,
      "learning_rate": 3.653976708966526e-05,
      "loss": 1.861,
      "step": 206200
    },
    {
      "epoch": 16.160112799624002,
      "grad_norm": 5.981611251831055,
      "learning_rate": 3.653323933364667e-05,
      "loss": 1.7223,
      "step": 206300
    },
    {
      "epoch": 16.167946106846312,
      "grad_norm": 8.282758712768555,
      "learning_rate": 3.6526711577628075e-05,
      "loss": 1.7839,
      "step": 206400
    },
    {
      "epoch": 16.17577941406862,
      "grad_norm": 5.7262282371521,
      "learning_rate": 3.652018382160948e-05,
      "loss": 1.8545,
      "step": 206500
    },
    {
      "epoch": 16.183612721290928,
      "grad_norm": 4.793965816497803,
      "learning_rate": 3.651365606559089e-05,
      "loss": 1.7944,
      "step": 206600
    },
    {
      "epoch": 16.191446028513237,
      "grad_norm": 4.774863243103027,
      "learning_rate": 3.6507128309572305e-05,
      "loss": 1.7912,
      "step": 206700
    },
    {
      "epoch": 16.199279335735547,
      "grad_norm": 5.105541706085205,
      "learning_rate": 3.650060055355371e-05,
      "loss": 1.7206,
      "step": 206800
    },
    {
      "epoch": 16.207112642957856,
      "grad_norm": 7.930432319641113,
      "learning_rate": 3.6494072797535124e-05,
      "loss": 1.7652,
      "step": 206900
    },
    {
      "epoch": 16.214945950180166,
      "grad_norm": 5.250123023986816,
      "learning_rate": 3.648754504151653e-05,
      "loss": 1.7008,
      "step": 207000
    },
    {
      "epoch": 16.222779257402475,
      "grad_norm": 9.622422218322754,
      "learning_rate": 3.6481017285497935e-05,
      "loss": 1.7869,
      "step": 207100
    },
    {
      "epoch": 16.230612564624785,
      "grad_norm": 5.016424655914307,
      "learning_rate": 3.647448952947935e-05,
      "loss": 1.8304,
      "step": 207200
    },
    {
      "epoch": 16.238445871847095,
      "grad_norm": 5.7318925857543945,
      "learning_rate": 3.646796177346076e-05,
      "loss": 1.7496,
      "step": 207300
    },
    {
      "epoch": 16.246279179069404,
      "grad_norm": 7.061828136444092,
      "learning_rate": 3.6461434017442166e-05,
      "loss": 1.8062,
      "step": 207400
    },
    {
      "epoch": 16.254112486291714,
      "grad_norm": 5.448342800140381,
      "learning_rate": 3.645490626142358e-05,
      "loss": 1.7237,
      "step": 207500
    },
    {
      "epoch": 16.261945793514023,
      "grad_norm": 6.50455379486084,
      "learning_rate": 3.6448378505404984e-05,
      "loss": 1.7855,
      "step": 207600
    },
    {
      "epoch": 16.26977910073633,
      "grad_norm": 8.6051025390625,
      "learning_rate": 3.644185074938639e-05,
      "loss": 1.7513,
      "step": 207700
    },
    {
      "epoch": 16.27761240795864,
      "grad_norm": 5.113335609436035,
      "learning_rate": 3.64353229933678e-05,
      "loss": 1.759,
      "step": 207800
    },
    {
      "epoch": 16.28544571518095,
      "grad_norm": 5.830021858215332,
      "learning_rate": 3.642879523734921e-05,
      "loss": 1.9013,
      "step": 207900
    },
    {
      "epoch": 16.293279022403258,
      "grad_norm": 6.667901515960693,
      "learning_rate": 3.642226748133062e-05,
      "loss": 1.8034,
      "step": 208000
    },
    {
      "epoch": 16.301112329625568,
      "grad_norm": 6.52297306060791,
      "learning_rate": 3.641573972531203e-05,
      "loss": 1.733,
      "step": 208100
    },
    {
      "epoch": 16.308945636847877,
      "grad_norm": 8.116792678833008,
      "learning_rate": 3.640921196929344e-05,
      "loss": 1.8105,
      "step": 208200
    },
    {
      "epoch": 16.316778944070187,
      "grad_norm": 5.65287446975708,
      "learning_rate": 3.6402684213274845e-05,
      "loss": 1.7423,
      "step": 208300
    },
    {
      "epoch": 16.324612251292496,
      "grad_norm": 6.1610331535339355,
      "learning_rate": 3.639615645725625e-05,
      "loss": 1.6946,
      "step": 208400
    },
    {
      "epoch": 16.332445558514806,
      "grad_norm": 6.3987202644348145,
      "learning_rate": 3.6389628701237663e-05,
      "loss": 1.8039,
      "step": 208500
    },
    {
      "epoch": 16.340278865737115,
      "grad_norm": 6.947781562805176,
      "learning_rate": 3.6383100945219076e-05,
      "loss": 1.7957,
      "step": 208600
    },
    {
      "epoch": 16.348112172959425,
      "grad_norm": 4.394397735595703,
      "learning_rate": 3.637657318920048e-05,
      "loss": 1.6905,
      "step": 208700
    },
    {
      "epoch": 16.35594548018173,
      "grad_norm": 2.9684760570526123,
      "learning_rate": 3.6370045433181894e-05,
      "loss": 1.7273,
      "step": 208800
    },
    {
      "epoch": 16.36377878740404,
      "grad_norm": 5.987583160400391,
      "learning_rate": 3.63635176771633e-05,
      "loss": 1.8354,
      "step": 208900
    },
    {
      "epoch": 16.37161209462635,
      "grad_norm": 6.167445659637451,
      "learning_rate": 3.6356989921144706e-05,
      "loss": 1.7154,
      "step": 209000
    },
    {
      "epoch": 16.37944540184866,
      "grad_norm": 6.4841108322143555,
      "learning_rate": 3.635046216512612e-05,
      "loss": 1.8346,
      "step": 209100
    },
    {
      "epoch": 16.38727870907097,
      "grad_norm": 6.459695816040039,
      "learning_rate": 3.6343934409107524e-05,
      "loss": 1.7708,
      "step": 209200
    },
    {
      "epoch": 16.39511201629328,
      "grad_norm": 9.090181350708008,
      "learning_rate": 3.633740665308894e-05,
      "loss": 1.7581,
      "step": 209300
    },
    {
      "epoch": 16.40294532351559,
      "grad_norm": 6.399292469024658,
      "learning_rate": 3.633087889707035e-05,
      "loss": 1.7884,
      "step": 209400
    },
    {
      "epoch": 16.410778630737898,
      "grad_norm": 6.855001449584961,
      "learning_rate": 3.6324351141051755e-05,
      "loss": 1.8048,
      "step": 209500
    },
    {
      "epoch": 16.418611937960208,
      "grad_norm": 5.3944549560546875,
      "learning_rate": 3.631782338503316e-05,
      "loss": 1.7648,
      "step": 209600
    },
    {
      "epoch": 16.426445245182517,
      "grad_norm": 6.403278350830078,
      "learning_rate": 3.6311295629014567e-05,
      "loss": 1.7853,
      "step": 209700
    },
    {
      "epoch": 16.434278552404827,
      "grad_norm": 6.412417888641357,
      "learning_rate": 3.630476787299598e-05,
      "loss": 1.7862,
      "step": 209800
    },
    {
      "epoch": 16.442111859627136,
      "grad_norm": 6.230274677276611,
      "learning_rate": 3.629824011697739e-05,
      "loss": 1.796,
      "step": 209900
    },
    {
      "epoch": 16.449945166849442,
      "grad_norm": 7.067429542541504,
      "learning_rate": 3.62917123609588e-05,
      "loss": 1.8587,
      "step": 210000
    },
    {
      "epoch": 16.457778474071752,
      "grad_norm": 5.3102569580078125,
      "learning_rate": 3.628518460494021e-05,
      "loss": 1.7174,
      "step": 210100
    },
    {
      "epoch": 16.46561178129406,
      "grad_norm": 5.560426712036133,
      "learning_rate": 3.6278656848921616e-05,
      "loss": 1.7336,
      "step": 210200
    },
    {
      "epoch": 16.47344508851637,
      "grad_norm": 5.3431878089904785,
      "learning_rate": 3.627212909290302e-05,
      "loss": 1.7921,
      "step": 210300
    },
    {
      "epoch": 16.48127839573868,
      "grad_norm": 6.581270217895508,
      "learning_rate": 3.6265601336884434e-05,
      "loss": 1.7648,
      "step": 210400
    },
    {
      "epoch": 16.48911170296099,
      "grad_norm": 7.088765621185303,
      "learning_rate": 3.6259073580865847e-05,
      "loss": 1.7631,
      "step": 210500
    },
    {
      "epoch": 16.4969450101833,
      "grad_norm": 8.740514755249023,
      "learning_rate": 3.625254582484725e-05,
      "loss": 1.7291,
      "step": 210600
    },
    {
      "epoch": 16.50477831740561,
      "grad_norm": 5.456023216247559,
      "learning_rate": 3.6246018068828665e-05,
      "loss": 1.7615,
      "step": 210700
    },
    {
      "epoch": 16.51261162462792,
      "grad_norm": 8.0753812789917,
      "learning_rate": 3.623949031281007e-05,
      "loss": 1.704,
      "step": 210800
    },
    {
      "epoch": 16.52044493185023,
      "grad_norm": 6.6194634437561035,
      "learning_rate": 3.6232962556791476e-05,
      "loss": 1.7754,
      "step": 210900
    },
    {
      "epoch": 16.528278239072538,
      "grad_norm": 6.760044574737549,
      "learning_rate": 3.622643480077289e-05,
      "loss": 1.7036,
      "step": 211000
    },
    {
      "epoch": 16.536111546294844,
      "grad_norm": 8.619796752929688,
      "learning_rate": 3.6219907044754295e-05,
      "loss": 1.7165,
      "step": 211100
    },
    {
      "epoch": 16.543944853517154,
      "grad_norm": 9.622093200683594,
      "learning_rate": 3.621337928873571e-05,
      "loss": 1.697,
      "step": 211200
    },
    {
      "epoch": 16.551778160739463,
      "grad_norm": 4.929566860198975,
      "learning_rate": 3.620685153271712e-05,
      "loss": 1.7235,
      "step": 211300
    },
    {
      "epoch": 16.559611467961773,
      "grad_norm": 6.6589484214782715,
      "learning_rate": 3.6200323776698526e-05,
      "loss": 1.7595,
      "step": 211400
    },
    {
      "epoch": 16.567444775184082,
      "grad_norm": 5.572627067565918,
      "learning_rate": 3.619379602067993e-05,
      "loss": 1.7708,
      "step": 211500
    },
    {
      "epoch": 16.575278082406392,
      "grad_norm": 7.870030403137207,
      "learning_rate": 3.618726826466134e-05,
      "loss": 1.7042,
      "step": 211600
    },
    {
      "epoch": 16.5831113896287,
      "grad_norm": 6.477043151855469,
      "learning_rate": 3.618074050864275e-05,
      "loss": 1.7351,
      "step": 211700
    },
    {
      "epoch": 16.59094469685101,
      "grad_norm": 7.418919563293457,
      "learning_rate": 3.617421275262416e-05,
      "loss": 1.784,
      "step": 211800
    },
    {
      "epoch": 16.59877800407332,
      "grad_norm": 7.160482883453369,
      "learning_rate": 3.616768499660557e-05,
      "loss": 1.764,
      "step": 211900
    },
    {
      "epoch": 16.60661131129563,
      "grad_norm": 5.923701763153076,
      "learning_rate": 3.616115724058698e-05,
      "loss": 1.7096,
      "step": 212000
    },
    {
      "epoch": 16.61444461851794,
      "grad_norm": 7.799725532531738,
      "learning_rate": 3.6154629484568386e-05,
      "loss": 1.8111,
      "step": 212100
    },
    {
      "epoch": 16.62227792574025,
      "grad_norm": 6.047443866729736,
      "learning_rate": 3.614810172854979e-05,
      "loss": 1.6725,
      "step": 212200
    },
    {
      "epoch": 16.630111232962555,
      "grad_norm": 6.104653358459473,
      "learning_rate": 3.6141573972531205e-05,
      "loss": 1.6697,
      "step": 212300
    },
    {
      "epoch": 16.637944540184865,
      "grad_norm": 6.265138626098633,
      "learning_rate": 3.613504621651261e-05,
      "loss": 1.8127,
      "step": 212400
    },
    {
      "epoch": 16.645777847407174,
      "grad_norm": 5.863502502441406,
      "learning_rate": 3.612851846049402e-05,
      "loss": 1.7837,
      "step": 212500
    },
    {
      "epoch": 16.653611154629484,
      "grad_norm": 7.00968074798584,
      "learning_rate": 3.6121990704475435e-05,
      "loss": 1.7001,
      "step": 212600
    },
    {
      "epoch": 16.661444461851794,
      "grad_norm": 5.291994094848633,
      "learning_rate": 3.611546294845684e-05,
      "loss": 1.7849,
      "step": 212700
    },
    {
      "epoch": 16.669277769074103,
      "grad_norm": 7.914438247680664,
      "learning_rate": 3.610893519243825e-05,
      "loss": 1.6946,
      "step": 212800
    },
    {
      "epoch": 16.677111076296413,
      "grad_norm": 5.541163921356201,
      "learning_rate": 3.610240743641966e-05,
      "loss": 1.8371,
      "step": 212900
    },
    {
      "epoch": 16.684944383518722,
      "grad_norm": 5.294741153717041,
      "learning_rate": 3.6095879680401065e-05,
      "loss": 1.76,
      "step": 213000
    },
    {
      "epoch": 16.692777690741032,
      "grad_norm": 5.569924354553223,
      "learning_rate": 3.608935192438248e-05,
      "loss": 1.8254,
      "step": 213100
    },
    {
      "epoch": 16.70061099796334,
      "grad_norm": 5.2567949295043945,
      "learning_rate": 3.6082824168363884e-05,
      "loss": 1.7162,
      "step": 213200
    },
    {
      "epoch": 16.70844430518565,
      "grad_norm": 5.864222526550293,
      "learning_rate": 3.6076296412345296e-05,
      "loss": 1.8357,
      "step": 213300
    },
    {
      "epoch": 16.716277612407957,
      "grad_norm": 5.546104907989502,
      "learning_rate": 3.60697686563267e-05,
      "loss": 1.8032,
      "step": 213400
    },
    {
      "epoch": 16.724110919630267,
      "grad_norm": 5.489272594451904,
      "learning_rate": 3.606324090030811e-05,
      "loss": 1.8138,
      "step": 213500
    },
    {
      "epoch": 16.731944226852576,
      "grad_norm": 6.366621494293213,
      "learning_rate": 3.605671314428952e-05,
      "loss": 1.7261,
      "step": 213600
    },
    {
      "epoch": 16.739777534074886,
      "grad_norm": 6.36863374710083,
      "learning_rate": 3.605018538827093e-05,
      "loss": 1.8008,
      "step": 213700
    },
    {
      "epoch": 16.747610841297195,
      "grad_norm": 4.8822126388549805,
      "learning_rate": 3.604365763225234e-05,
      "loss": 1.6615,
      "step": 213800
    },
    {
      "epoch": 16.755444148519505,
      "grad_norm": 5.588169574737549,
      "learning_rate": 3.603712987623375e-05,
      "loss": 1.7513,
      "step": 213900
    },
    {
      "epoch": 16.763277455741814,
      "grad_norm": 6.08656644821167,
      "learning_rate": 3.603060212021516e-05,
      "loss": 1.6939,
      "step": 214000
    },
    {
      "epoch": 16.771110762964124,
      "grad_norm": 6.459090232849121,
      "learning_rate": 3.602407436419656e-05,
      "loss": 1.7093,
      "step": 214100
    },
    {
      "epoch": 16.778944070186434,
      "grad_norm": 7.066156387329102,
      "learning_rate": 3.6017546608177975e-05,
      "loss": 1.7657,
      "step": 214200
    },
    {
      "epoch": 16.786777377408743,
      "grad_norm": 6.631536960601807,
      "learning_rate": 3.601101885215938e-05,
      "loss": 1.643,
      "step": 214300
    },
    {
      "epoch": 16.794610684631053,
      "grad_norm": 10.18819808959961,
      "learning_rate": 3.6004491096140793e-05,
      "loss": 1.7384,
      "step": 214400
    },
    {
      "epoch": 16.80244399185336,
      "grad_norm": 5.7740960121154785,
      "learning_rate": 3.5997963340122206e-05,
      "loss": 1.6374,
      "step": 214500
    },
    {
      "epoch": 16.81027729907567,
      "grad_norm": 7.955361366271973,
      "learning_rate": 3.599143558410361e-05,
      "loss": 1.8575,
      "step": 214600
    },
    {
      "epoch": 16.818110606297978,
      "grad_norm": 5.140440940856934,
      "learning_rate": 3.598490782808502e-05,
      "loss": 1.7548,
      "step": 214700
    },
    {
      "epoch": 16.825943913520288,
      "grad_norm": 5.166802883148193,
      "learning_rate": 3.597838007206642e-05,
      "loss": 1.7558,
      "step": 214800
    },
    {
      "epoch": 16.833777220742597,
      "grad_norm": 7.269299030303955,
      "learning_rate": 3.5971852316047836e-05,
      "loss": 1.808,
      "step": 214900
    },
    {
      "epoch": 16.841610527964907,
      "grad_norm": 6.439091682434082,
      "learning_rate": 3.596532456002925e-05,
      "loss": 1.7629,
      "step": 215000
    },
    {
      "epoch": 16.849443835187216,
      "grad_norm": 7.754214286804199,
      "learning_rate": 3.5958796804010654e-05,
      "loss": 1.8181,
      "step": 215100
    },
    {
      "epoch": 16.857277142409526,
      "grad_norm": 5.8961615562438965,
      "learning_rate": 3.595226904799207e-05,
      "loss": 1.7594,
      "step": 215200
    },
    {
      "epoch": 16.865110449631835,
      "grad_norm": 6.623708248138428,
      "learning_rate": 3.594574129197347e-05,
      "loss": 1.7465,
      "step": 215300
    },
    {
      "epoch": 16.872943756854145,
      "grad_norm": 5.353483200073242,
      "learning_rate": 3.593921353595488e-05,
      "loss": 1.76,
      "step": 215400
    },
    {
      "epoch": 16.880777064076455,
      "grad_norm": 4.827822208404541,
      "learning_rate": 3.593268577993629e-05,
      "loss": 1.7319,
      "step": 215500
    },
    {
      "epoch": 16.88861037129876,
      "grad_norm": 5.224318504333496,
      "learning_rate": 3.5926158023917697e-05,
      "loss": 1.756,
      "step": 215600
    },
    {
      "epoch": 16.89644367852107,
      "grad_norm": 6.830460071563721,
      "learning_rate": 3.591963026789911e-05,
      "loss": 1.8106,
      "step": 215700
    },
    {
      "epoch": 16.90427698574338,
      "grad_norm": 6.739404678344727,
      "learning_rate": 3.591310251188052e-05,
      "loss": 1.7978,
      "step": 215800
    },
    {
      "epoch": 16.91211029296569,
      "grad_norm": 5.53412389755249,
      "learning_rate": 3.590657475586193e-05,
      "loss": 1.7323,
      "step": 215900
    },
    {
      "epoch": 16.919943600188,
      "grad_norm": 5.794041156768799,
      "learning_rate": 3.590004699984333e-05,
      "loss": 1.807,
      "step": 216000
    },
    {
      "epoch": 16.92777690741031,
      "grad_norm": 5.809442520141602,
      "learning_rate": 3.5893519243824746e-05,
      "loss": 1.661,
      "step": 216100
    },
    {
      "epoch": 16.935610214632618,
      "grad_norm": 4.803861141204834,
      "learning_rate": 3.588699148780615e-05,
      "loss": 1.7631,
      "step": 216200
    },
    {
      "epoch": 16.943443521854928,
      "grad_norm": 5.4341888427734375,
      "learning_rate": 3.5880463731787564e-05,
      "loss": 1.7682,
      "step": 216300
    },
    {
      "epoch": 16.951276829077237,
      "grad_norm": 6.257254123687744,
      "learning_rate": 3.5873935975768977e-05,
      "loss": 1.718,
      "step": 216400
    },
    {
      "epoch": 16.959110136299547,
      "grad_norm": 7.942409992218018,
      "learning_rate": 3.586740821975038e-05,
      "loss": 1.7297,
      "step": 216500
    },
    {
      "epoch": 16.966943443521856,
      "grad_norm": 6.274515628814697,
      "learning_rate": 3.586088046373179e-05,
      "loss": 1.7452,
      "step": 216600
    },
    {
      "epoch": 16.974776750744166,
      "grad_norm": 6.428719997406006,
      "learning_rate": 3.5854352707713194e-05,
      "loss": 1.6634,
      "step": 216700
    },
    {
      "epoch": 16.982610057966472,
      "grad_norm": 6.121391773223877,
      "learning_rate": 3.5847824951694606e-05,
      "loss": 1.7538,
      "step": 216800
    },
    {
      "epoch": 16.99044336518878,
      "grad_norm": 5.165872573852539,
      "learning_rate": 3.584129719567602e-05,
      "loss": 1.8114,
      "step": 216900
    },
    {
      "epoch": 16.99827667241109,
      "grad_norm": 7.742476940155029,
      "learning_rate": 3.5834769439657425e-05,
      "loss": 1.8258,
      "step": 217000
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.8074190616607666,
      "eval_runtime": 2.9107,
      "eval_samples_per_second": 230.87,
      "eval_steps_per_second": 230.87,
      "step": 217022
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.526440978050232,
      "eval_runtime": 55.085,
      "eval_samples_per_second": 231.751,
      "eval_steps_per_second": 231.751,
      "step": 217022
    },
    {
      "epoch": 17.0061099796334,
      "grad_norm": 5.867500305175781,
      "learning_rate": 3.582824168363884e-05,
      "loss": 1.743,
      "step": 217100
    },
    {
      "epoch": 17.01394328685571,
      "grad_norm": 9.958295822143555,
      "learning_rate": 3.582171392762024e-05,
      "loss": 1.7622,
      "step": 217200
    },
    {
      "epoch": 17.02177659407802,
      "grad_norm": 7.066745758056641,
      "learning_rate": 3.581518617160165e-05,
      "loss": 1.7585,
      "step": 217300
    },
    {
      "epoch": 17.02960990130033,
      "grad_norm": 6.1962809562683105,
      "learning_rate": 3.580865841558306e-05,
      "loss": 1.6852,
      "step": 217400
    },
    {
      "epoch": 17.03744320852264,
      "grad_norm": 8.815403938293457,
      "learning_rate": 3.580213065956447e-05,
      "loss": 1.6637,
      "step": 217500
    },
    {
      "epoch": 17.04527651574495,
      "grad_norm": 4.977276802062988,
      "learning_rate": 3.579560290354588e-05,
      "loss": 1.7812,
      "step": 217600
    },
    {
      "epoch": 17.053109822967258,
      "grad_norm": 4.962502956390381,
      "learning_rate": 3.578907514752729e-05,
      "loss": 1.7234,
      "step": 217700
    },
    {
      "epoch": 17.060943130189568,
      "grad_norm": 5.326295375823975,
      "learning_rate": 3.57825473915087e-05,
      "loss": 1.7704,
      "step": 217800
    },
    {
      "epoch": 17.068776437411874,
      "grad_norm": 5.359574317932129,
      "learning_rate": 3.5776019635490104e-05,
      "loss": 1.7132,
      "step": 217900
    },
    {
      "epoch": 17.076609744634183,
      "grad_norm": 6.3573994636535645,
      "learning_rate": 3.5769491879471516e-05,
      "loss": 1.8236,
      "step": 218000
    },
    {
      "epoch": 17.084443051856493,
      "grad_norm": 6.601084232330322,
      "learning_rate": 3.576296412345292e-05,
      "loss": 1.6544,
      "step": 218100
    },
    {
      "epoch": 17.092276359078802,
      "grad_norm": 5.613276481628418,
      "learning_rate": 3.5756436367434335e-05,
      "loss": 1.7524,
      "step": 218200
    },
    {
      "epoch": 17.100109666301112,
      "grad_norm": 5.541235446929932,
      "learning_rate": 3.574990861141574e-05,
      "loss": 1.6887,
      "step": 218300
    },
    {
      "epoch": 17.10794297352342,
      "grad_norm": 10.319046020507812,
      "learning_rate": 3.574338085539715e-05,
      "loss": 1.6671,
      "step": 218400
    },
    {
      "epoch": 17.11577628074573,
      "grad_norm": 5.609225273132324,
      "learning_rate": 3.573685309937856e-05,
      "loss": 1.8122,
      "step": 218500
    },
    {
      "epoch": 17.12360958796804,
      "grad_norm": 6.140942573547363,
      "learning_rate": 3.5730325343359964e-05,
      "loss": 1.6991,
      "step": 218600
    },
    {
      "epoch": 17.13144289519035,
      "grad_norm": 6.7275590896606445,
      "learning_rate": 3.572379758734138e-05,
      "loss": 1.7271,
      "step": 218700
    },
    {
      "epoch": 17.13927620241266,
      "grad_norm": 4.009283065795898,
      "learning_rate": 3.571726983132278e-05,
      "loss": 1.7528,
      "step": 218800
    },
    {
      "epoch": 17.14710950963497,
      "grad_norm": 4.604769229888916,
      "learning_rate": 3.5710742075304195e-05,
      "loss": 1.7231,
      "step": 218900
    },
    {
      "epoch": 17.15494281685728,
      "grad_norm": 4.789886474609375,
      "learning_rate": 3.570421431928561e-05,
      "loss": 1.8195,
      "step": 219000
    },
    {
      "epoch": 17.162776124079585,
      "grad_norm": 7.522273540496826,
      "learning_rate": 3.5697686563267014e-05,
      "loss": 1.7598,
      "step": 219100
    },
    {
      "epoch": 17.170609431301894,
      "grad_norm": 5.578623294830322,
      "learning_rate": 3.569115880724842e-05,
      "loss": 1.7189,
      "step": 219200
    },
    {
      "epoch": 17.178442738524204,
      "grad_norm": 5.157683849334717,
      "learning_rate": 3.568463105122983e-05,
      "loss": 1.7727,
      "step": 219300
    },
    {
      "epoch": 17.186276045746514,
      "grad_norm": 6.282682418823242,
      "learning_rate": 3.567810329521124e-05,
      "loss": 1.7624,
      "step": 219400
    },
    {
      "epoch": 17.194109352968823,
      "grad_norm": 6.386591911315918,
      "learning_rate": 3.567157553919265e-05,
      "loss": 1.7103,
      "step": 219500
    },
    {
      "epoch": 17.201942660191133,
      "grad_norm": 5.931523323059082,
      "learning_rate": 3.566504778317406e-05,
      "loss": 1.731,
      "step": 219600
    },
    {
      "epoch": 17.209775967413442,
      "grad_norm": 6.14581823348999,
      "learning_rate": 3.565852002715547e-05,
      "loss": 1.7114,
      "step": 219700
    },
    {
      "epoch": 17.217609274635752,
      "grad_norm": 5.612311363220215,
      "learning_rate": 3.5651992271136874e-05,
      "loss": 1.7447,
      "step": 219800
    },
    {
      "epoch": 17.22544258185806,
      "grad_norm": 7.062870025634766,
      "learning_rate": 3.564546451511828e-05,
      "loss": 1.808,
      "step": 219900
    },
    {
      "epoch": 17.23327588908037,
      "grad_norm": 6.637063980102539,
      "learning_rate": 3.563893675909969e-05,
      "loss": 1.7683,
      "step": 220000
    },
    {
      "epoch": 17.24110919630268,
      "grad_norm": 6.830625057220459,
      "learning_rate": 3.5632409003081105e-05,
      "loss": 1.801,
      "step": 220100
    },
    {
      "epoch": 17.248942503524987,
      "grad_norm": 5.100767612457275,
      "learning_rate": 3.562588124706251e-05,
      "loss": 1.7172,
      "step": 220200
    },
    {
      "epoch": 17.256775810747296,
      "grad_norm": 6.995242118835449,
      "learning_rate": 3.5619353491043924e-05,
      "loss": 1.7426,
      "step": 220300
    },
    {
      "epoch": 17.264609117969606,
      "grad_norm": 6.449321269989014,
      "learning_rate": 3.561282573502533e-05,
      "loss": 1.6365,
      "step": 220400
    },
    {
      "epoch": 17.272442425191915,
      "grad_norm": 5.666999816894531,
      "learning_rate": 3.5606297979006735e-05,
      "loss": 1.7472,
      "step": 220500
    },
    {
      "epoch": 17.280275732414225,
      "grad_norm": 5.542220592498779,
      "learning_rate": 3.559977022298815e-05,
      "loss": 1.6503,
      "step": 220600
    },
    {
      "epoch": 17.288109039636534,
      "grad_norm": 5.244088172912598,
      "learning_rate": 3.5593242466969553e-05,
      "loss": 1.7377,
      "step": 220700
    },
    {
      "epoch": 17.295942346858844,
      "grad_norm": 4.031895637512207,
      "learning_rate": 3.5586714710950966e-05,
      "loss": 1.7649,
      "step": 220800
    },
    {
      "epoch": 17.303775654081154,
      "grad_norm": 5.66810417175293,
      "learning_rate": 3.558018695493238e-05,
      "loss": 1.6418,
      "step": 220900
    },
    {
      "epoch": 17.311608961303463,
      "grad_norm": 4.2136616706848145,
      "learning_rate": 3.5573659198913784e-05,
      "loss": 1.7848,
      "step": 221000
    },
    {
      "epoch": 17.319442268525773,
      "grad_norm": 5.788878440856934,
      "learning_rate": 3.556713144289519e-05,
      "loss": 1.6616,
      "step": 221100
    },
    {
      "epoch": 17.327275575748082,
      "grad_norm": 4.533950328826904,
      "learning_rate": 3.55606036868766e-05,
      "loss": 1.7118,
      "step": 221200
    },
    {
      "epoch": 17.33510888297039,
      "grad_norm": 8.252599716186523,
      "learning_rate": 3.555407593085801e-05,
      "loss": 1.7643,
      "step": 221300
    },
    {
      "epoch": 17.342942190192698,
      "grad_norm": 6.785632610321045,
      "learning_rate": 3.554754817483942e-05,
      "loss": 1.8027,
      "step": 221400
    },
    {
      "epoch": 17.350775497415007,
      "grad_norm": 6.496426105499268,
      "learning_rate": 3.554102041882083e-05,
      "loss": 1.6717,
      "step": 221500
    },
    {
      "epoch": 17.358608804637317,
      "grad_norm": 5.550962448120117,
      "learning_rate": 3.553449266280224e-05,
      "loss": 1.6985,
      "step": 221600
    },
    {
      "epoch": 17.366442111859627,
      "grad_norm": 7.054423809051514,
      "learning_rate": 3.5527964906783645e-05,
      "loss": 1.6768,
      "step": 221700
    },
    {
      "epoch": 17.374275419081936,
      "grad_norm": 6.5447797775268555,
      "learning_rate": 3.552143715076505e-05,
      "loss": 1.7488,
      "step": 221800
    },
    {
      "epoch": 17.382108726304246,
      "grad_norm": 6.679493427276611,
      "learning_rate": 3.551490939474646e-05,
      "loss": 1.7815,
      "step": 221900
    },
    {
      "epoch": 17.389942033526555,
      "grad_norm": 7.549587249755859,
      "learning_rate": 3.550838163872787e-05,
      "loss": 1.7306,
      "step": 222000
    },
    {
      "epoch": 17.397775340748865,
      "grad_norm": 5.102502822875977,
      "learning_rate": 3.550185388270928e-05,
      "loss": 1.8179,
      "step": 222100
    },
    {
      "epoch": 17.405608647971174,
      "grad_norm": 5.9424238204956055,
      "learning_rate": 3.5495326126690694e-05,
      "loss": 1.714,
      "step": 222200
    },
    {
      "epoch": 17.413441955193484,
      "grad_norm": 5.513970851898193,
      "learning_rate": 3.54887983706721e-05,
      "loss": 1.7834,
      "step": 222300
    },
    {
      "epoch": 17.421275262415794,
      "grad_norm": 6.553439140319824,
      "learning_rate": 3.5482270614653506e-05,
      "loss": 1.7399,
      "step": 222400
    },
    {
      "epoch": 17.4291085696381,
      "grad_norm": 8.019920349121094,
      "learning_rate": 3.547574285863492e-05,
      "loss": 1.7292,
      "step": 222500
    },
    {
      "epoch": 17.43694187686041,
      "grad_norm": 5.506383419036865,
      "learning_rate": 3.5469215102616324e-05,
      "loss": 1.7957,
      "step": 222600
    },
    {
      "epoch": 17.44477518408272,
      "grad_norm": 6.978243827819824,
      "learning_rate": 3.5462687346597737e-05,
      "loss": 1.8039,
      "step": 222700
    },
    {
      "epoch": 17.45260849130503,
      "grad_norm": 8.067498207092285,
      "learning_rate": 3.545615959057915e-05,
      "loss": 1.7544,
      "step": 222800
    },
    {
      "epoch": 17.460441798527338,
      "grad_norm": 6.044218063354492,
      "learning_rate": 3.5449631834560555e-05,
      "loss": 1.7133,
      "step": 222900
    },
    {
      "epoch": 17.468275105749647,
      "grad_norm": 7.153794288635254,
      "learning_rate": 3.544310407854196e-05,
      "loss": 1.7874,
      "step": 223000
    },
    {
      "epoch": 17.476108412971957,
      "grad_norm": 5.863954067230225,
      "learning_rate": 3.543657632252337e-05,
      "loss": 1.8288,
      "step": 223100
    },
    {
      "epoch": 17.483941720194267,
      "grad_norm": 7.915191173553467,
      "learning_rate": 3.543004856650478e-05,
      "loss": 1.7711,
      "step": 223200
    },
    {
      "epoch": 17.491775027416576,
      "grad_norm": 5.102607250213623,
      "learning_rate": 3.542352081048619e-05,
      "loss": 1.7999,
      "step": 223300
    },
    {
      "epoch": 17.499608334638886,
      "grad_norm": 8.000665664672852,
      "learning_rate": 3.54169930544676e-05,
      "loss": 1.7134,
      "step": 223400
    },
    {
      "epoch": 17.507441641861195,
      "grad_norm": 5.761415958404541,
      "learning_rate": 3.541046529844901e-05,
      "loss": 1.8352,
      "step": 223500
    },
    {
      "epoch": 17.5152749490835,
      "grad_norm": 4.904940605163574,
      "learning_rate": 3.5403937542430416e-05,
      "loss": 1.7134,
      "step": 223600
    },
    {
      "epoch": 17.52310825630581,
      "grad_norm": 5.240908622741699,
      "learning_rate": 3.539740978641182e-05,
      "loss": 1.663,
      "step": 223700
    },
    {
      "epoch": 17.53094156352812,
      "grad_norm": 7.442673206329346,
      "learning_rate": 3.5390882030393234e-05,
      "loss": 1.7957,
      "step": 223800
    },
    {
      "epoch": 17.53877487075043,
      "grad_norm": 6.245865345001221,
      "learning_rate": 3.538435427437464e-05,
      "loss": 1.7503,
      "step": 223900
    },
    {
      "epoch": 17.54660817797274,
      "grad_norm": 11.722403526306152,
      "learning_rate": 3.537782651835605e-05,
      "loss": 1.7575,
      "step": 224000
    },
    {
      "epoch": 17.55444148519505,
      "grad_norm": 6.802087306976318,
      "learning_rate": 3.5371298762337465e-05,
      "loss": 1.7743,
      "step": 224100
    },
    {
      "epoch": 17.56227479241736,
      "grad_norm": 6.312737941741943,
      "learning_rate": 3.536477100631887e-05,
      "loss": 1.6972,
      "step": 224200
    },
    {
      "epoch": 17.57010809963967,
      "grad_norm": 6.350368022918701,
      "learning_rate": 3.5358243250300276e-05,
      "loss": 1.7368,
      "step": 224300
    },
    {
      "epoch": 17.577941406861978,
      "grad_norm": 4.757238864898682,
      "learning_rate": 3.535171549428169e-05,
      "loss": 1.7972,
      "step": 224400
    },
    {
      "epoch": 17.585774714084287,
      "grad_norm": 7.394224643707275,
      "learning_rate": 3.5345187738263095e-05,
      "loss": 1.8101,
      "step": 224500
    },
    {
      "epoch": 17.593608021306597,
      "grad_norm": 6.355954170227051,
      "learning_rate": 3.533865998224451e-05,
      "loss": 1.7452,
      "step": 224600
    },
    {
      "epoch": 17.601441328528907,
      "grad_norm": 7.218839168548584,
      "learning_rate": 3.533213222622592e-05,
      "loss": 1.7882,
      "step": 224700
    },
    {
      "epoch": 17.609274635751213,
      "grad_norm": 5.406283378601074,
      "learning_rate": 3.5325604470207325e-05,
      "loss": 1.796,
      "step": 224800
    },
    {
      "epoch": 17.617107942973522,
      "grad_norm": 6.18172550201416,
      "learning_rate": 3.531907671418873e-05,
      "loss": 1.697,
      "step": 224900
    },
    {
      "epoch": 17.624941250195832,
      "grad_norm": 6.8213582038879395,
      "learning_rate": 3.531254895817014e-05,
      "loss": 1.7622,
      "step": 225000
    },
    {
      "epoch": 17.63277455741814,
      "grad_norm": 6.967050075531006,
      "learning_rate": 3.530602120215155e-05,
      "loss": 1.7064,
      "step": 225100
    },
    {
      "epoch": 17.64060786464045,
      "grad_norm": 3.9523940086364746,
      "learning_rate": 3.5299493446132955e-05,
      "loss": 1.8305,
      "step": 225200
    },
    {
      "epoch": 17.64844117186276,
      "grad_norm": 6.345667362213135,
      "learning_rate": 3.529296569011437e-05,
      "loss": 1.7244,
      "step": 225300
    },
    {
      "epoch": 17.65627447908507,
      "grad_norm": 6.335032939910889,
      "learning_rate": 3.528643793409578e-05,
      "loss": 1.759,
      "step": 225400
    },
    {
      "epoch": 17.66410778630738,
      "grad_norm": 2.462846517562866,
      "learning_rate": 3.5279910178077186e-05,
      "loss": 1.7596,
      "step": 225500
    },
    {
      "epoch": 17.67194109352969,
      "grad_norm": 6.72595739364624,
      "learning_rate": 3.527338242205859e-05,
      "loss": 1.765,
      "step": 225600
    },
    {
      "epoch": 17.679774400752,
      "grad_norm": 7.250101089477539,
      "learning_rate": 3.5266854666040004e-05,
      "loss": 1.7631,
      "step": 225700
    },
    {
      "epoch": 17.68760770797431,
      "grad_norm": 7.581080913543701,
      "learning_rate": 3.526032691002141e-05,
      "loss": 1.6932,
      "step": 225800
    },
    {
      "epoch": 17.695441015196614,
      "grad_norm": 6.700854778289795,
      "learning_rate": 3.525379915400282e-05,
      "loss": 1.7993,
      "step": 225900
    },
    {
      "epoch": 17.703274322418924,
      "grad_norm": 5.516839027404785,
      "learning_rate": 3.5247271397984235e-05,
      "loss": 1.7013,
      "step": 226000
    },
    {
      "epoch": 17.711107629641234,
      "grad_norm": 6.540899753570557,
      "learning_rate": 3.524074364196564e-05,
      "loss": 1.7007,
      "step": 226100
    },
    {
      "epoch": 17.718940936863543,
      "grad_norm": 5.796587944030762,
      "learning_rate": 3.523421588594705e-05,
      "loss": 1.7614,
      "step": 226200
    },
    {
      "epoch": 17.726774244085853,
      "grad_norm": 7.933756351470947,
      "learning_rate": 3.522768812992846e-05,
      "loss": 1.7647,
      "step": 226300
    },
    {
      "epoch": 17.734607551308162,
      "grad_norm": 5.639045238494873,
      "learning_rate": 3.5221160373909865e-05,
      "loss": 1.8496,
      "step": 226400
    },
    {
      "epoch": 17.742440858530472,
      "grad_norm": 5.2811760902404785,
      "learning_rate": 3.521463261789128e-05,
      "loss": 1.7808,
      "step": 226500
    },
    {
      "epoch": 17.75027416575278,
      "grad_norm": 5.5543999671936035,
      "learning_rate": 3.5208104861872683e-05,
      "loss": 1.7189,
      "step": 226600
    },
    {
      "epoch": 17.75810747297509,
      "grad_norm": 4.082540035247803,
      "learning_rate": 3.5201577105854096e-05,
      "loss": 1.861,
      "step": 226700
    },
    {
      "epoch": 17.7659407801974,
      "grad_norm": 6.5045671463012695,
      "learning_rate": 3.51950493498355e-05,
      "loss": 1.8506,
      "step": 226800
    },
    {
      "epoch": 17.77377408741971,
      "grad_norm": 6.429298400878906,
      "learning_rate": 3.518852159381691e-05,
      "loss": 1.7589,
      "step": 226900
    },
    {
      "epoch": 17.781607394642016,
      "grad_norm": 7.948866367340088,
      "learning_rate": 3.518199383779832e-05,
      "loss": 1.7883,
      "step": 227000
    },
    {
      "epoch": 17.789440701864326,
      "grad_norm": 5.842198848724365,
      "learning_rate": 3.5175466081779726e-05,
      "loss": 1.7079,
      "step": 227100
    },
    {
      "epoch": 17.797274009086635,
      "grad_norm": 5.70416259765625,
      "learning_rate": 3.516893832576114e-05,
      "loss": 1.7365,
      "step": 227200
    },
    {
      "epoch": 17.805107316308945,
      "grad_norm": 6.437391757965088,
      "learning_rate": 3.516241056974255e-05,
      "loss": 1.6988,
      "step": 227300
    },
    {
      "epoch": 17.812940623531254,
      "grad_norm": 7.060565948486328,
      "learning_rate": 3.515588281372396e-05,
      "loss": 1.8023,
      "step": 227400
    },
    {
      "epoch": 17.820773930753564,
      "grad_norm": 5.969524383544922,
      "learning_rate": 3.514935505770536e-05,
      "loss": 1.6931,
      "step": 227500
    },
    {
      "epoch": 17.828607237975874,
      "grad_norm": 4.65082311630249,
      "learning_rate": 3.5142827301686775e-05,
      "loss": 1.7678,
      "step": 227600
    },
    {
      "epoch": 17.836440545198183,
      "grad_norm": 5.34796142578125,
      "learning_rate": 3.513629954566818e-05,
      "loss": 1.8058,
      "step": 227700
    },
    {
      "epoch": 17.844273852420493,
      "grad_norm": 6.193746566772461,
      "learning_rate": 3.512977178964959e-05,
      "loss": 1.7127,
      "step": 227800
    },
    {
      "epoch": 17.852107159642802,
      "grad_norm": 7.020761013031006,
      "learning_rate": 3.5123244033631006e-05,
      "loss": 1.7893,
      "step": 227900
    },
    {
      "epoch": 17.859940466865112,
      "grad_norm": 4.411492347717285,
      "learning_rate": 3.511671627761241e-05,
      "loss": 1.7174,
      "step": 228000
    },
    {
      "epoch": 17.86777377408742,
      "grad_norm": 7.030510902404785,
      "learning_rate": 3.511018852159382e-05,
      "loss": 1.7842,
      "step": 228100
    },
    {
      "epoch": 17.875607081309727,
      "grad_norm": 5.178274631500244,
      "learning_rate": 3.510366076557523e-05,
      "loss": 1.8139,
      "step": 228200
    },
    {
      "epoch": 17.883440388532037,
      "grad_norm": 4.284152507781982,
      "learning_rate": 3.5097133009556636e-05,
      "loss": 1.7312,
      "step": 228300
    },
    {
      "epoch": 17.891273695754347,
      "grad_norm": 6.6829609870910645,
      "learning_rate": 3.509060525353804e-05,
      "loss": 1.6764,
      "step": 228400
    },
    {
      "epoch": 17.899107002976656,
      "grad_norm": 5.081555366516113,
      "learning_rate": 3.5084077497519454e-05,
      "loss": 1.709,
      "step": 228500
    },
    {
      "epoch": 17.906940310198966,
      "grad_norm": 5.062013626098633,
      "learning_rate": 3.5077549741500867e-05,
      "loss": 1.7704,
      "step": 228600
    },
    {
      "epoch": 17.914773617421275,
      "grad_norm": 8.147147178649902,
      "learning_rate": 3.507102198548227e-05,
      "loss": 1.7522,
      "step": 228700
    },
    {
      "epoch": 17.922606924643585,
      "grad_norm": 6.503438472747803,
      "learning_rate": 3.506449422946368e-05,
      "loss": 1.7613,
      "step": 228800
    },
    {
      "epoch": 17.930440231865894,
      "grad_norm": 4.532250881195068,
      "learning_rate": 3.505796647344509e-05,
      "loss": 1.7546,
      "step": 228900
    },
    {
      "epoch": 17.938273539088204,
      "grad_norm": 5.065840244293213,
      "learning_rate": 3.5051438717426496e-05,
      "loss": 1.8148,
      "step": 229000
    },
    {
      "epoch": 17.946106846310514,
      "grad_norm": 4.491528511047363,
      "learning_rate": 3.504491096140791e-05,
      "loss": 1.7009,
      "step": 229100
    },
    {
      "epoch": 17.953940153532823,
      "grad_norm": 7.21796989440918,
      "learning_rate": 3.503838320538932e-05,
      "loss": 1.7933,
      "step": 229200
    },
    {
      "epoch": 17.96177346075513,
      "grad_norm": 4.949714183807373,
      "learning_rate": 3.503185544937073e-05,
      "loss": 1.6827,
      "step": 229300
    },
    {
      "epoch": 17.96960676797744,
      "grad_norm": 7.0593485832214355,
      "learning_rate": 3.502532769335213e-05,
      "loss": 1.7675,
      "step": 229400
    },
    {
      "epoch": 17.97744007519975,
      "grad_norm": 6.589061260223389,
      "learning_rate": 3.5018799937333546e-05,
      "loss": 1.8088,
      "step": 229500
    },
    {
      "epoch": 17.985273382422058,
      "grad_norm": 5.774905204772949,
      "learning_rate": 3.501227218131495e-05,
      "loss": 1.7058,
      "step": 229600
    },
    {
      "epoch": 17.993106689644367,
      "grad_norm": 6.30741548538208,
      "learning_rate": 3.5005744425296364e-05,
      "loss": 1.7727,
      "step": 229700
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.8048795461654663,
      "eval_runtime": 2.9097,
      "eval_samples_per_second": 230.951,
      "eval_steps_per_second": 230.951,
      "step": 229788
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.5199660062789917,
      "eval_runtime": 54.7794,
      "eval_samples_per_second": 233.044,
      "eval_steps_per_second": 233.044,
      "step": 229788
    },
    {
      "epoch": 18.000939996866677,
      "grad_norm": 6.0215535163879395,
      "learning_rate": 3.4999216669277776e-05,
      "loss": 1.7764,
      "step": 229800
    },
    {
      "epoch": 18.008773304088987,
      "grad_norm": 7.510375022888184,
      "learning_rate": 3.499268891325918e-05,
      "loss": 1.7475,
      "step": 229900
    },
    {
      "epoch": 18.016606611311296,
      "grad_norm": 5.10062837600708,
      "learning_rate": 3.498616115724059e-05,
      "loss": 1.7258,
      "step": 230000
    },
    {
      "epoch": 18.024439918533606,
      "grad_norm": 5.483680248260498,
      "learning_rate": 3.4979633401221994e-05,
      "loss": 1.7159,
      "step": 230100
    },
    {
      "epoch": 18.032273225755915,
      "grad_norm": 5.57388973236084,
      "learning_rate": 3.4973105645203406e-05,
      "loss": 1.6458,
      "step": 230200
    },
    {
      "epoch": 18.040106532978225,
      "grad_norm": 5.494202613830566,
      "learning_rate": 3.496657788918481e-05,
      "loss": 1.779,
      "step": 230300
    },
    {
      "epoch": 18.047939840200534,
      "grad_norm": 4.818127632141113,
      "learning_rate": 3.4960050133166225e-05,
      "loss": 1.7237,
      "step": 230400
    },
    {
      "epoch": 18.05577314742284,
      "grad_norm": 8.517640113830566,
      "learning_rate": 3.495352237714764e-05,
      "loss": 1.7016,
      "step": 230500
    },
    {
      "epoch": 18.06360645464515,
      "grad_norm": 6.573490619659424,
      "learning_rate": 3.494699462112904e-05,
      "loss": 1.7373,
      "step": 230600
    },
    {
      "epoch": 18.07143976186746,
      "grad_norm": 6.160923004150391,
      "learning_rate": 3.494046686511045e-05,
      "loss": 1.8475,
      "step": 230700
    },
    {
      "epoch": 18.07927306908977,
      "grad_norm": 7.162214279174805,
      "learning_rate": 3.493393910909186e-05,
      "loss": 1.732,
      "step": 230800
    },
    {
      "epoch": 18.08710637631208,
      "grad_norm": 5.927274703979492,
      "learning_rate": 3.492741135307327e-05,
      "loss": 1.7129,
      "step": 230900
    },
    {
      "epoch": 18.09493968353439,
      "grad_norm": 6.315054893493652,
      "learning_rate": 3.492088359705468e-05,
      "loss": 1.6813,
      "step": 231000
    },
    {
      "epoch": 18.102772990756698,
      "grad_norm": 4.788729190826416,
      "learning_rate": 3.491435584103609e-05,
      "loss": 1.7251,
      "step": 231100
    },
    {
      "epoch": 18.110606297979007,
      "grad_norm": 5.451242446899414,
      "learning_rate": 3.49078280850175e-05,
      "loss": 1.7387,
      "step": 231200
    },
    {
      "epoch": 18.118439605201317,
      "grad_norm": 6.335536956787109,
      "learning_rate": 3.4901300328998904e-05,
      "loss": 1.679,
      "step": 231300
    },
    {
      "epoch": 18.126272912423627,
      "grad_norm": 5.583285331726074,
      "learning_rate": 3.4894772572980316e-05,
      "loss": 1.712,
      "step": 231400
    },
    {
      "epoch": 18.134106219645936,
      "grad_norm": 4.300687313079834,
      "learning_rate": 3.488824481696172e-05,
      "loss": 1.6819,
      "step": 231500
    },
    {
      "epoch": 18.141939526868242,
      "grad_norm": 5.732565402984619,
      "learning_rate": 3.488171706094313e-05,
      "loss": 1.6955,
      "step": 231600
    },
    {
      "epoch": 18.14977283409055,
      "grad_norm": 5.556608200073242,
      "learning_rate": 3.487518930492454e-05,
      "loss": 1.6936,
      "step": 231700
    },
    {
      "epoch": 18.15760614131286,
      "grad_norm": 5.270289897918701,
      "learning_rate": 3.486866154890595e-05,
      "loss": 1.7416,
      "step": 231800
    },
    {
      "epoch": 18.16543944853517,
      "grad_norm": 4.722215175628662,
      "learning_rate": 3.486213379288736e-05,
      "loss": 1.7899,
      "step": 231900
    },
    {
      "epoch": 18.17327275575748,
      "grad_norm": 5.065710067749023,
      "learning_rate": 3.4855606036868764e-05,
      "loss": 1.7371,
      "step": 232000
    },
    {
      "epoch": 18.18110606297979,
      "grad_norm": 6.374650955200195,
      "learning_rate": 3.484907828085018e-05,
      "loss": 1.8537,
      "step": 232100
    },
    {
      "epoch": 18.1889393702021,
      "grad_norm": 5.559315204620361,
      "learning_rate": 3.484255052483158e-05,
      "loss": 1.7711,
      "step": 232200
    },
    {
      "epoch": 18.19677267742441,
      "grad_norm": 6.482900619506836,
      "learning_rate": 3.4836022768812995e-05,
      "loss": 1.6856,
      "step": 232300
    },
    {
      "epoch": 18.20460598464672,
      "grad_norm": 4.717644691467285,
      "learning_rate": 3.482949501279441e-05,
      "loss": 1.7159,
      "step": 232400
    },
    {
      "epoch": 18.21243929186903,
      "grad_norm": 4.798868179321289,
      "learning_rate": 3.4822967256775814e-05,
      "loss": 1.6717,
      "step": 232500
    },
    {
      "epoch": 18.220272599091338,
      "grad_norm": 7.769867897033691,
      "learning_rate": 3.481643950075722e-05,
      "loss": 1.7137,
      "step": 232600
    },
    {
      "epoch": 18.228105906313644,
      "grad_norm": 5.266343593597412,
      "learning_rate": 3.480991174473863e-05,
      "loss": 1.7225,
      "step": 232700
    },
    {
      "epoch": 18.235939213535953,
      "grad_norm": 5.403022289276123,
      "learning_rate": 3.480338398872004e-05,
      "loss": 1.7658,
      "step": 232800
    },
    {
      "epoch": 18.243772520758263,
      "grad_norm": 6.415067672729492,
      "learning_rate": 3.479685623270145e-05,
      "loss": 1.7816,
      "step": 232900
    },
    {
      "epoch": 18.251605827980573,
      "grad_norm": 6.662053108215332,
      "learning_rate": 3.479032847668286e-05,
      "loss": 1.7167,
      "step": 233000
    },
    {
      "epoch": 18.259439135202882,
      "grad_norm": 9.651708602905273,
      "learning_rate": 3.478380072066427e-05,
      "loss": 1.7516,
      "step": 233100
    },
    {
      "epoch": 18.26727244242519,
      "grad_norm": 6.7403883934021,
      "learning_rate": 3.4777272964645674e-05,
      "loss": 1.717,
      "step": 233200
    },
    {
      "epoch": 18.2751057496475,
      "grad_norm": 6.709950923919678,
      "learning_rate": 3.477074520862708e-05,
      "loss": 1.7949,
      "step": 233300
    },
    {
      "epoch": 18.28293905686981,
      "grad_norm": 8.327698707580566,
      "learning_rate": 3.476421745260849e-05,
      "loss": 1.6863,
      "step": 233400
    },
    {
      "epoch": 18.29077236409212,
      "grad_norm": 4.9331560134887695,
      "learning_rate": 3.47576896965899e-05,
      "loss": 1.6612,
      "step": 233500
    },
    {
      "epoch": 18.29860567131443,
      "grad_norm": 5.19831657409668,
      "learning_rate": 3.475116194057131e-05,
      "loss": 1.7411,
      "step": 233600
    },
    {
      "epoch": 18.30643897853674,
      "grad_norm": 6.919547080993652,
      "learning_rate": 3.4744634184552723e-05,
      "loss": 1.7881,
      "step": 233700
    },
    {
      "epoch": 18.31427228575905,
      "grad_norm": 6.436210632324219,
      "learning_rate": 3.473810642853413e-05,
      "loss": 1.7911,
      "step": 233800
    },
    {
      "epoch": 18.322105592981355,
      "grad_norm": 4.672531604766846,
      "learning_rate": 3.4731578672515535e-05,
      "loss": 1.7199,
      "step": 233900
    },
    {
      "epoch": 18.329938900203665,
      "grad_norm": 4.76377010345459,
      "learning_rate": 3.472505091649695e-05,
      "loss": 1.7408,
      "step": 234000
    },
    {
      "epoch": 18.337772207425974,
      "grad_norm": 7.681014060974121,
      "learning_rate": 3.471852316047835e-05,
      "loss": 1.7986,
      "step": 234100
    },
    {
      "epoch": 18.345605514648284,
      "grad_norm": 5.492066383361816,
      "learning_rate": 3.4711995404459766e-05,
      "loss": 1.6811,
      "step": 234200
    },
    {
      "epoch": 18.353438821870594,
      "grad_norm": 6.515786170959473,
      "learning_rate": 3.470546764844118e-05,
      "loss": 1.6989,
      "step": 234300
    },
    {
      "epoch": 18.361272129092903,
      "grad_norm": 5.506496906280518,
      "learning_rate": 3.4698939892422584e-05,
      "loss": 1.8024,
      "step": 234400
    },
    {
      "epoch": 18.369105436315213,
      "grad_norm": 5.923013687133789,
      "learning_rate": 3.469241213640399e-05,
      "loss": 1.7319,
      "step": 234500
    },
    {
      "epoch": 18.376938743537522,
      "grad_norm": 6.658286094665527,
      "learning_rate": 3.46858843803854e-05,
      "loss": 1.6851,
      "step": 234600
    },
    {
      "epoch": 18.384772050759832,
      "grad_norm": 7.867914199829102,
      "learning_rate": 3.467935662436681e-05,
      "loss": 1.7014,
      "step": 234700
    },
    {
      "epoch": 18.39260535798214,
      "grad_norm": 5.264836311340332,
      "learning_rate": 3.4672828868348214e-05,
      "loss": 1.8471,
      "step": 234800
    },
    {
      "epoch": 18.40043866520445,
      "grad_norm": 6.445237159729004,
      "learning_rate": 3.466630111232963e-05,
      "loss": 1.785,
      "step": 234900
    },
    {
      "epoch": 18.408271972426757,
      "grad_norm": 5.419398784637451,
      "learning_rate": 3.465977335631104e-05,
      "loss": 1.7574,
      "step": 235000
    },
    {
      "epoch": 18.416105279649067,
      "grad_norm": 6.040695667266846,
      "learning_rate": 3.4653245600292445e-05,
      "loss": 1.7756,
      "step": 235100
    },
    {
      "epoch": 18.423938586871376,
      "grad_norm": 6.367081165313721,
      "learning_rate": 3.464671784427385e-05,
      "loss": 1.6284,
      "step": 235200
    },
    {
      "epoch": 18.431771894093686,
      "grad_norm": 5.516140460968018,
      "learning_rate": 3.464019008825526e-05,
      "loss": 1.6705,
      "step": 235300
    },
    {
      "epoch": 18.439605201315995,
      "grad_norm": 7.1301188468933105,
      "learning_rate": 3.463366233223667e-05,
      "loss": 1.7791,
      "step": 235400
    },
    {
      "epoch": 18.447438508538305,
      "grad_norm": 3.892770767211914,
      "learning_rate": 3.462713457621808e-05,
      "loss": 1.8194,
      "step": 235500
    },
    {
      "epoch": 18.455271815760614,
      "grad_norm": 7.119263172149658,
      "learning_rate": 3.4620606820199494e-05,
      "loss": 1.7697,
      "step": 235600
    },
    {
      "epoch": 18.463105122982924,
      "grad_norm": 6.493659973144531,
      "learning_rate": 3.46140790641809e-05,
      "loss": 1.7415,
      "step": 235700
    },
    {
      "epoch": 18.470938430205234,
      "grad_norm": 5.361655235290527,
      "learning_rate": 3.4607551308162306e-05,
      "loss": 1.7637,
      "step": 235800
    },
    {
      "epoch": 18.478771737427543,
      "grad_norm": 5.567723751068115,
      "learning_rate": 3.460102355214372e-05,
      "loss": 1.7502,
      "step": 235900
    },
    {
      "epoch": 18.486605044649853,
      "grad_norm": 5.478850841522217,
      "learning_rate": 3.4594495796125124e-05,
      "loss": 1.7284,
      "step": 236000
    },
    {
      "epoch": 18.494438351872162,
      "grad_norm": 7.25941276550293,
      "learning_rate": 3.4587968040106536e-05,
      "loss": 1.688,
      "step": 236100
    },
    {
      "epoch": 18.50227165909447,
      "grad_norm": 7.040125846862793,
      "learning_rate": 3.458144028408795e-05,
      "loss": 1.8794,
      "step": 236200
    },
    {
      "epoch": 18.510104966316778,
      "grad_norm": 7.787398338317871,
      "learning_rate": 3.4574912528069355e-05,
      "loss": 1.7298,
      "step": 236300
    },
    {
      "epoch": 18.517938273539087,
      "grad_norm": 7.0454421043396,
      "learning_rate": 3.456838477205076e-05,
      "loss": 1.7532,
      "step": 236400
    },
    {
      "epoch": 18.525771580761397,
      "grad_norm": 7.0726823806762695,
      "learning_rate": 3.456185701603217e-05,
      "loss": 1.6776,
      "step": 236500
    },
    {
      "epoch": 18.533604887983707,
      "grad_norm": 5.864169597625732,
      "learning_rate": 3.455532926001358e-05,
      "loss": 1.8439,
      "step": 236600
    },
    {
      "epoch": 18.541438195206016,
      "grad_norm": 6.71726655960083,
      "learning_rate": 3.4548801503994985e-05,
      "loss": 1.7622,
      "step": 236700
    },
    {
      "epoch": 18.549271502428326,
      "grad_norm": 7.242330074310303,
      "learning_rate": 3.45422737479764e-05,
      "loss": 1.7427,
      "step": 236800
    },
    {
      "epoch": 18.557104809650635,
      "grad_norm": 7.227076530456543,
      "learning_rate": 3.453574599195781e-05,
      "loss": 1.8254,
      "step": 236900
    },
    {
      "epoch": 18.564938116872945,
      "grad_norm": 4.664150714874268,
      "learning_rate": 3.4529218235939215e-05,
      "loss": 1.7089,
      "step": 237000
    },
    {
      "epoch": 18.572771424095254,
      "grad_norm": 6.276473522186279,
      "learning_rate": 3.452269047992062e-05,
      "loss": 1.6856,
      "step": 237100
    },
    {
      "epoch": 18.580604731317564,
      "grad_norm": 6.758336544036865,
      "learning_rate": 3.4516162723902034e-05,
      "loss": 1.7073,
      "step": 237200
    },
    {
      "epoch": 18.58843803853987,
      "grad_norm": 6.248429298400879,
      "learning_rate": 3.450963496788344e-05,
      "loss": 1.7642,
      "step": 237300
    },
    {
      "epoch": 18.59627134576218,
      "grad_norm": 5.451976776123047,
      "learning_rate": 3.450310721186485e-05,
      "loss": 1.8172,
      "step": 237400
    },
    {
      "epoch": 18.60410465298449,
      "grad_norm": 6.224305629730225,
      "learning_rate": 3.4496579455846265e-05,
      "loss": 1.7438,
      "step": 237500
    },
    {
      "epoch": 18.6119379602068,
      "grad_norm": 6.5941362380981445,
      "learning_rate": 3.449005169982767e-05,
      "loss": 1.8053,
      "step": 237600
    },
    {
      "epoch": 18.61977126742911,
      "grad_norm": 4.737159252166748,
      "learning_rate": 3.4483523943809076e-05,
      "loss": 1.9023,
      "step": 237700
    },
    {
      "epoch": 18.627604574651418,
      "grad_norm": 5.64335298538208,
      "learning_rate": 3.447699618779049e-05,
      "loss": 1.6865,
      "step": 237800
    },
    {
      "epoch": 18.635437881873727,
      "grad_norm": 6.148643493652344,
      "learning_rate": 3.4470468431771894e-05,
      "loss": 1.7337,
      "step": 237900
    },
    {
      "epoch": 18.643271189096037,
      "grad_norm": 4.915271282196045,
      "learning_rate": 3.44639406757533e-05,
      "loss": 1.7282,
      "step": 238000
    },
    {
      "epoch": 18.651104496318347,
      "grad_norm": 4.676762580871582,
      "learning_rate": 3.445741291973472e-05,
      "loss": 1.6727,
      "step": 238100
    },
    {
      "epoch": 18.658937803540656,
      "grad_norm": 6.457875728607178,
      "learning_rate": 3.4450885163716125e-05,
      "loss": 1.8557,
      "step": 238200
    },
    {
      "epoch": 18.666771110762966,
      "grad_norm": 4.292997360229492,
      "learning_rate": 3.444435740769753e-05,
      "loss": 1.7449,
      "step": 238300
    },
    {
      "epoch": 18.67460441798527,
      "grad_norm": 9.127477645874023,
      "learning_rate": 3.443782965167894e-05,
      "loss": 1.6457,
      "step": 238400
    },
    {
      "epoch": 18.68243772520758,
      "grad_norm": 5.9289870262146,
      "learning_rate": 3.443130189566035e-05,
      "loss": 1.7304,
      "step": 238500
    },
    {
      "epoch": 18.69027103242989,
      "grad_norm": 5.530538558959961,
      "learning_rate": 3.4424774139641755e-05,
      "loss": 1.7337,
      "step": 238600
    },
    {
      "epoch": 18.6981043396522,
      "grad_norm": 5.51501989364624,
      "learning_rate": 3.441824638362317e-05,
      "loss": 1.7443,
      "step": 238700
    },
    {
      "epoch": 18.70593764687451,
      "grad_norm": 5.569181442260742,
      "learning_rate": 3.441171862760458e-05,
      "loss": 1.8503,
      "step": 238800
    },
    {
      "epoch": 18.71377095409682,
      "grad_norm": 8.044011116027832,
      "learning_rate": 3.4405190871585986e-05,
      "loss": 1.7628,
      "step": 238900
    },
    {
      "epoch": 18.72160426131913,
      "grad_norm": 6.040863990783691,
      "learning_rate": 3.439866311556739e-05,
      "loss": 1.7278,
      "step": 239000
    },
    {
      "epoch": 18.72943756854144,
      "grad_norm": 2.5943734645843506,
      "learning_rate": 3.4392135359548804e-05,
      "loss": 1.7777,
      "step": 239100
    },
    {
      "epoch": 18.73727087576375,
      "grad_norm": 4.920520782470703,
      "learning_rate": 3.438560760353021e-05,
      "loss": 1.7534,
      "step": 239200
    },
    {
      "epoch": 18.745104182986058,
      "grad_norm": 7.147196292877197,
      "learning_rate": 3.437907984751162e-05,
      "loss": 1.7617,
      "step": 239300
    },
    {
      "epoch": 18.752937490208367,
      "grad_norm": 4.314100742340088,
      "learning_rate": 3.4372552091493035e-05,
      "loss": 1.6193,
      "step": 239400
    },
    {
      "epoch": 18.760770797430673,
      "grad_norm": 5.26344633102417,
      "learning_rate": 3.436602433547444e-05,
      "loss": 1.7694,
      "step": 239500
    },
    {
      "epoch": 18.768604104652983,
      "grad_norm": 6.840044975280762,
      "learning_rate": 3.435949657945585e-05,
      "loss": 1.7502,
      "step": 239600
    },
    {
      "epoch": 18.776437411875293,
      "grad_norm": 6.690580368041992,
      "learning_rate": 3.435296882343726e-05,
      "loss": 1.7639,
      "step": 239700
    },
    {
      "epoch": 18.784270719097602,
      "grad_norm": 4.0626301765441895,
      "learning_rate": 3.4346441067418665e-05,
      "loss": 1.8237,
      "step": 239800
    },
    {
      "epoch": 18.79210402631991,
      "grad_norm": 6.595860958099365,
      "learning_rate": 3.433991331140007e-05,
      "loss": 1.8351,
      "step": 239900
    },
    {
      "epoch": 18.79993733354222,
      "grad_norm": 5.780875205993652,
      "learning_rate": 3.433338555538148e-05,
      "loss": 1.761,
      "step": 240000
    },
    {
      "epoch": 18.80777064076453,
      "grad_norm": 5.98366117477417,
      "learning_rate": 3.4326857799362896e-05,
      "loss": 1.7605,
      "step": 240100
    },
    {
      "epoch": 18.81560394798684,
      "grad_norm": 5.425118446350098,
      "learning_rate": 3.43203300433443e-05,
      "loss": 1.7509,
      "step": 240200
    },
    {
      "epoch": 18.82343725520915,
      "grad_norm": 5.917169094085693,
      "learning_rate": 3.431380228732571e-05,
      "loss": 1.7423,
      "step": 240300
    },
    {
      "epoch": 18.83127056243146,
      "grad_norm": 6.807645320892334,
      "learning_rate": 3.430727453130712e-05,
      "loss": 1.784,
      "step": 240400
    },
    {
      "epoch": 18.83910386965377,
      "grad_norm": 5.121989727020264,
      "learning_rate": 3.4300746775288526e-05,
      "loss": 1.8576,
      "step": 240500
    },
    {
      "epoch": 18.84693717687608,
      "grad_norm": 6.924696445465088,
      "learning_rate": 3.429421901926994e-05,
      "loss": 1.668,
      "step": 240600
    },
    {
      "epoch": 18.854770484098385,
      "grad_norm": 5.067995071411133,
      "learning_rate": 3.428769126325135e-05,
      "loss": 1.7214,
      "step": 240700
    },
    {
      "epoch": 18.862603791320694,
      "grad_norm": 8.81861400604248,
      "learning_rate": 3.4281163507232757e-05,
      "loss": 1.6509,
      "step": 240800
    },
    {
      "epoch": 18.870437098543004,
      "grad_norm": 5.917675971984863,
      "learning_rate": 3.427463575121416e-05,
      "loss": 1.8857,
      "step": 240900
    },
    {
      "epoch": 18.878270405765313,
      "grad_norm": 6.492614269256592,
      "learning_rate": 3.4268107995195575e-05,
      "loss": 1.7415,
      "step": 241000
    },
    {
      "epoch": 18.886103712987623,
      "grad_norm": 6.301731109619141,
      "learning_rate": 3.426158023917698e-05,
      "loss": 1.8166,
      "step": 241100
    },
    {
      "epoch": 18.893937020209933,
      "grad_norm": 5.703058242797852,
      "learning_rate": 3.4255052483158386e-05,
      "loss": 1.7204,
      "step": 241200
    },
    {
      "epoch": 18.901770327432242,
      "grad_norm": 5.367193222045898,
      "learning_rate": 3.4248524727139806e-05,
      "loss": 1.7166,
      "step": 241300
    },
    {
      "epoch": 18.90960363465455,
      "grad_norm": 6.521261215209961,
      "learning_rate": 3.424199697112121e-05,
      "loss": 1.7906,
      "step": 241400
    },
    {
      "epoch": 18.91743694187686,
      "grad_norm": 5.3453545570373535,
      "learning_rate": 3.423546921510262e-05,
      "loss": 1.7682,
      "step": 241500
    },
    {
      "epoch": 18.92527024909917,
      "grad_norm": 5.606189727783203,
      "learning_rate": 3.422894145908403e-05,
      "loss": 1.6863,
      "step": 241600
    },
    {
      "epoch": 18.93310355632148,
      "grad_norm": 5.400485038757324,
      "learning_rate": 3.4222413703065436e-05,
      "loss": 1.7872,
      "step": 241700
    },
    {
      "epoch": 18.940936863543786,
      "grad_norm": 7.405665397644043,
      "learning_rate": 3.421588594704684e-05,
      "loss": 1.7631,
      "step": 241800
    },
    {
      "epoch": 18.948770170766096,
      "grad_norm": 3.7364346981048584,
      "learning_rate": 3.4209358191028254e-05,
      "loss": 1.6168,
      "step": 241900
    },
    {
      "epoch": 18.956603477988406,
      "grad_norm": 4.426284313201904,
      "learning_rate": 3.4202830435009666e-05,
      "loss": 1.7022,
      "step": 242000
    },
    {
      "epoch": 18.964436785210715,
      "grad_norm": 4.255913734436035,
      "learning_rate": 3.419630267899107e-05,
      "loss": 1.7451,
      "step": 242100
    },
    {
      "epoch": 18.972270092433025,
      "grad_norm": 6.2428083419799805,
      "learning_rate": 3.418977492297248e-05,
      "loss": 1.7501,
      "step": 242200
    },
    {
      "epoch": 18.980103399655334,
      "grad_norm": 5.401393890380859,
      "learning_rate": 3.418324716695389e-05,
      "loss": 1.8203,
      "step": 242300
    },
    {
      "epoch": 18.987936706877644,
      "grad_norm": 5.792243480682373,
      "learning_rate": 3.4176719410935296e-05,
      "loss": 1.64,
      "step": 242400
    },
    {
      "epoch": 18.995770014099953,
      "grad_norm": 5.615912437438965,
      "learning_rate": 3.417019165491671e-05,
      "loss": 1.6175,
      "step": 242500
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.7939280271530151,
      "eval_runtime": 2.9011,
      "eval_samples_per_second": 231.639,
      "eval_steps_per_second": 231.639,
      "step": 242554
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.5038425922393799,
      "eval_runtime": 54.7854,
      "eval_samples_per_second": 233.018,
      "eval_steps_per_second": 233.018,
      "step": 242554
    },
    {
      "epoch": 19.003603321322263,
      "grad_norm": 5.3739848136901855,
      "learning_rate": 3.416366389889812e-05,
      "loss": 1.6582,
      "step": 242600
    },
    {
      "epoch": 19.011436628544573,
      "grad_norm": 7.044556140899658,
      "learning_rate": 3.415713614287953e-05,
      "loss": 1.7222,
      "step": 242700
    },
    {
      "epoch": 19.019269935766882,
      "grad_norm": 6.565251350402832,
      "learning_rate": 3.415060838686093e-05,
      "loss": 1.7679,
      "step": 242800
    },
    {
      "epoch": 19.02710324298919,
      "grad_norm": 5.932488441467285,
      "learning_rate": 3.4144080630842345e-05,
      "loss": 1.7201,
      "step": 242900
    },
    {
      "epoch": 19.034936550211498,
      "grad_norm": 5.709141254425049,
      "learning_rate": 3.413755287482375e-05,
      "loss": 1.7311,
      "step": 243000
    },
    {
      "epoch": 19.042769857433807,
      "grad_norm": 7.508702754974365,
      "learning_rate": 3.413102511880516e-05,
      "loss": 1.758,
      "step": 243100
    },
    {
      "epoch": 19.050603164656117,
      "grad_norm": 7.278385639190674,
      "learning_rate": 3.412449736278657e-05,
      "loss": 1.7319,
      "step": 243200
    },
    {
      "epoch": 19.058436471878426,
      "grad_norm": 6.046173572540283,
      "learning_rate": 3.411796960676798e-05,
      "loss": 1.7353,
      "step": 243300
    },
    {
      "epoch": 19.066269779100736,
      "grad_norm": 7.450201988220215,
      "learning_rate": 3.411144185074939e-05,
      "loss": 1.7458,
      "step": 243400
    },
    {
      "epoch": 19.074103086323046,
      "grad_norm": 6.625028610229492,
      "learning_rate": 3.4104914094730794e-05,
      "loss": 1.6948,
      "step": 243500
    },
    {
      "epoch": 19.081936393545355,
      "grad_norm": 6.905209064483643,
      "learning_rate": 3.4098386338712206e-05,
      "loss": 1.7294,
      "step": 243600
    },
    {
      "epoch": 19.089769700767665,
      "grad_norm": 5.7434163093566895,
      "learning_rate": 3.409185858269361e-05,
      "loss": 1.7025,
      "step": 243700
    },
    {
      "epoch": 19.097603007989974,
      "grad_norm": 6.25661039352417,
      "learning_rate": 3.4085330826675024e-05,
      "loss": 1.7395,
      "step": 243800
    },
    {
      "epoch": 19.105436315212284,
      "grad_norm": 6.480946063995361,
      "learning_rate": 3.407880307065644e-05,
      "loss": 1.6896,
      "step": 243900
    },
    {
      "epoch": 19.113269622434593,
      "grad_norm": 6.275344371795654,
      "learning_rate": 3.407227531463784e-05,
      "loss": 1.7238,
      "step": 244000
    },
    {
      "epoch": 19.1211029296569,
      "grad_norm": 6.159648895263672,
      "learning_rate": 3.406574755861925e-05,
      "loss": 1.7723,
      "step": 244100
    },
    {
      "epoch": 19.12893623687921,
      "grad_norm": 6.791623592376709,
      "learning_rate": 3.405921980260066e-05,
      "loss": 1.7069,
      "step": 244200
    },
    {
      "epoch": 19.13676954410152,
      "grad_norm": 6.500776767730713,
      "learning_rate": 3.405269204658207e-05,
      "loss": 1.6277,
      "step": 244300
    },
    {
      "epoch": 19.144602851323828,
      "grad_norm": 4.93089485168457,
      "learning_rate": 3.404616429056347e-05,
      "loss": 1.6862,
      "step": 244400
    },
    {
      "epoch": 19.152436158546138,
      "grad_norm": 5.186358451843262,
      "learning_rate": 3.403963653454489e-05,
      "loss": 1.7761,
      "step": 244500
    },
    {
      "epoch": 19.160269465768447,
      "grad_norm": 7.796111583709717,
      "learning_rate": 3.40331087785263e-05,
      "loss": 1.6514,
      "step": 244600
    },
    {
      "epoch": 19.168102772990757,
      "grad_norm": 4.905452251434326,
      "learning_rate": 3.4026581022507704e-05,
      "loss": 1.7627,
      "step": 244700
    },
    {
      "epoch": 19.175936080213067,
      "grad_norm": 3.9533491134643555,
      "learning_rate": 3.4020053266489116e-05,
      "loss": 1.7773,
      "step": 244800
    },
    {
      "epoch": 19.183769387435376,
      "grad_norm": 6.18442964553833,
      "learning_rate": 3.401352551047052e-05,
      "loss": 1.703,
      "step": 244900
    },
    {
      "epoch": 19.191602694657686,
      "grad_norm": 4.816236972808838,
      "learning_rate": 3.400699775445193e-05,
      "loss": 1.7217,
      "step": 245000
    },
    {
      "epoch": 19.199436001879995,
      "grad_norm": 6.23249626159668,
      "learning_rate": 3.400046999843334e-05,
      "loss": 1.7038,
      "step": 245100
    },
    {
      "epoch": 19.2072693091023,
      "grad_norm": 5.214019775390625,
      "learning_rate": 3.399394224241475e-05,
      "loss": 1.7275,
      "step": 245200
    },
    {
      "epoch": 19.21510261632461,
      "grad_norm": 5.766395092010498,
      "learning_rate": 3.398741448639616e-05,
      "loss": 1.828,
      "step": 245300
    },
    {
      "epoch": 19.22293592354692,
      "grad_norm": 5.953073024749756,
      "learning_rate": 3.3980886730377564e-05,
      "loss": 1.8221,
      "step": 245400
    },
    {
      "epoch": 19.23076923076923,
      "grad_norm": 4.668038845062256,
      "learning_rate": 3.397435897435898e-05,
      "loss": 1.7123,
      "step": 245500
    },
    {
      "epoch": 19.23860253799154,
      "grad_norm": 6.158665657043457,
      "learning_rate": 3.396783121834038e-05,
      "loss": 1.732,
      "step": 245600
    },
    {
      "epoch": 19.24643584521385,
      "grad_norm": 6.443846702575684,
      "learning_rate": 3.3961303462321795e-05,
      "loss": 1.7961,
      "step": 245700
    },
    {
      "epoch": 19.25426915243616,
      "grad_norm": 6.285325050354004,
      "learning_rate": 3.395477570630321e-05,
      "loss": 1.7208,
      "step": 245800
    },
    {
      "epoch": 19.26210245965847,
      "grad_norm": 7.9181060791015625,
      "learning_rate": 3.3948247950284613e-05,
      "loss": 1.7307,
      "step": 245900
    },
    {
      "epoch": 19.269935766880778,
      "grad_norm": 5.884255886077881,
      "learning_rate": 3.394172019426602e-05,
      "loss": 1.7137,
      "step": 246000
    },
    {
      "epoch": 19.277769074103087,
      "grad_norm": 6.010926246643066,
      "learning_rate": 3.393519243824743e-05,
      "loss": 1.7079,
      "step": 246100
    },
    {
      "epoch": 19.285602381325397,
      "grad_norm": 5.713217735290527,
      "learning_rate": 3.392866468222884e-05,
      "loss": 1.8045,
      "step": 246200
    },
    {
      "epoch": 19.293435688547707,
      "grad_norm": 6.976252555847168,
      "learning_rate": 3.392213692621024e-05,
      "loss": 1.7151,
      "step": 246300
    },
    {
      "epoch": 19.301268995770013,
      "grad_norm": 5.302755832672119,
      "learning_rate": 3.3915609170191656e-05,
      "loss": 1.6916,
      "step": 246400
    },
    {
      "epoch": 19.309102302992322,
      "grad_norm": 7.346316814422607,
      "learning_rate": 3.390908141417307e-05,
      "loss": 1.7163,
      "step": 246500
    },
    {
      "epoch": 19.31693561021463,
      "grad_norm": 8.633007049560547,
      "learning_rate": 3.3902553658154474e-05,
      "loss": 1.7181,
      "step": 246600
    },
    {
      "epoch": 19.32476891743694,
      "grad_norm": 5.898797512054443,
      "learning_rate": 3.389602590213589e-05,
      "loss": 1.7507,
      "step": 246700
    },
    {
      "epoch": 19.33260222465925,
      "grad_norm": 8.282666206359863,
      "learning_rate": 3.388949814611729e-05,
      "loss": 1.7187,
      "step": 246800
    },
    {
      "epoch": 19.34043553188156,
      "grad_norm": 5.593576908111572,
      "learning_rate": 3.38829703900987e-05,
      "loss": 1.6783,
      "step": 246900
    },
    {
      "epoch": 19.34826883910387,
      "grad_norm": 5.6634416580200195,
      "learning_rate": 3.387644263408011e-05,
      "loss": 1.7794,
      "step": 247000
    },
    {
      "epoch": 19.35610214632618,
      "grad_norm": 7.592442989349365,
      "learning_rate": 3.386991487806152e-05,
      "loss": 1.7617,
      "step": 247100
    },
    {
      "epoch": 19.36393545354849,
      "grad_norm": 5.245576858520508,
      "learning_rate": 3.386338712204293e-05,
      "loss": 1.7267,
      "step": 247200
    },
    {
      "epoch": 19.3717687607708,
      "grad_norm": 5.015330791473389,
      "learning_rate": 3.3856859366024335e-05,
      "loss": 1.7567,
      "step": 247300
    },
    {
      "epoch": 19.37960206799311,
      "grad_norm": 5.898990631103516,
      "learning_rate": 3.385033161000575e-05,
      "loss": 1.7712,
      "step": 247400
    },
    {
      "epoch": 19.387435375215414,
      "grad_norm": 6.527492046356201,
      "learning_rate": 3.384380385398715e-05,
      "loss": 1.6357,
      "step": 247500
    },
    {
      "epoch": 19.395268682437724,
      "grad_norm": 7.297234535217285,
      "learning_rate": 3.383727609796856e-05,
      "loss": 1.7617,
      "step": 247600
    },
    {
      "epoch": 19.403101989660033,
      "grad_norm": 5.584661483764648,
      "learning_rate": 3.383074834194998e-05,
      "loss": 1.6571,
      "step": 247700
    },
    {
      "epoch": 19.410935296882343,
      "grad_norm": 5.745275020599365,
      "learning_rate": 3.3824220585931384e-05,
      "loss": 1.7176,
      "step": 247800
    },
    {
      "epoch": 19.418768604104653,
      "grad_norm": 9.10781192779541,
      "learning_rate": 3.381769282991279e-05,
      "loss": 1.7058,
      "step": 247900
    },
    {
      "epoch": 19.426601911326962,
      "grad_norm": 5.759715557098389,
      "learning_rate": 3.38111650738942e-05,
      "loss": 1.7463,
      "step": 248000
    },
    {
      "epoch": 19.43443521854927,
      "grad_norm": 7.951811790466309,
      "learning_rate": 3.380463731787561e-05,
      "loss": 1.7668,
      "step": 248100
    },
    {
      "epoch": 19.44226852577158,
      "grad_norm": 6.209869384765625,
      "learning_rate": 3.3798109561857014e-05,
      "loss": 1.9285,
      "step": 248200
    },
    {
      "epoch": 19.45010183299389,
      "grad_norm": 5.743460655212402,
      "learning_rate": 3.3791581805838426e-05,
      "loss": 1.6941,
      "step": 248300
    },
    {
      "epoch": 19.4579351402162,
      "grad_norm": 6.0078935623168945,
      "learning_rate": 3.378505404981984e-05,
      "loss": 1.7289,
      "step": 248400
    },
    {
      "epoch": 19.46576844743851,
      "grad_norm": 6.998126029968262,
      "learning_rate": 3.3778526293801245e-05,
      "loss": 1.7543,
      "step": 248500
    },
    {
      "epoch": 19.47360175466082,
      "grad_norm": 14.848898887634277,
      "learning_rate": 3.377199853778265e-05,
      "loss": 1.8517,
      "step": 248600
    },
    {
      "epoch": 19.481435061883126,
      "grad_norm": 6.760620594024658,
      "learning_rate": 3.376547078176406e-05,
      "loss": 1.7387,
      "step": 248700
    },
    {
      "epoch": 19.489268369105435,
      "grad_norm": 6.152399063110352,
      "learning_rate": 3.375894302574547e-05,
      "loss": 1.7801,
      "step": 248800
    },
    {
      "epoch": 19.497101676327745,
      "grad_norm": 9.637773513793945,
      "learning_rate": 3.375241526972688e-05,
      "loss": 1.7047,
      "step": 248900
    },
    {
      "epoch": 19.504934983550054,
      "grad_norm": 8.00058364868164,
      "learning_rate": 3.3745887513708294e-05,
      "loss": 1.6427,
      "step": 249000
    },
    {
      "epoch": 19.512768290772364,
      "grad_norm": 6.216887474060059,
      "learning_rate": 3.37393597576897e-05,
      "loss": 1.7314,
      "step": 249100
    },
    {
      "epoch": 19.520601597994673,
      "grad_norm": 7.801874160766602,
      "learning_rate": 3.3732832001671105e-05,
      "loss": 1.6338,
      "step": 249200
    },
    {
      "epoch": 19.528434905216983,
      "grad_norm": 5.243853569030762,
      "learning_rate": 3.372630424565252e-05,
      "loss": 1.7313,
      "step": 249300
    },
    {
      "epoch": 19.536268212439293,
      "grad_norm": 5.4957170486450195,
      "learning_rate": 3.3719776489633924e-05,
      "loss": 1.8307,
      "step": 249400
    },
    {
      "epoch": 19.544101519661602,
      "grad_norm": 6.484165668487549,
      "learning_rate": 3.371324873361533e-05,
      "loss": 1.6934,
      "step": 249500
    },
    {
      "epoch": 19.55193482688391,
      "grad_norm": 5.145864963531494,
      "learning_rate": 3.370672097759674e-05,
      "loss": 1.7338,
      "step": 249600
    },
    {
      "epoch": 19.55976813410622,
      "grad_norm": 7.8228654861450195,
      "learning_rate": 3.3700193221578155e-05,
      "loss": 1.6389,
      "step": 249700
    },
    {
      "epoch": 19.567601441328527,
      "grad_norm": 5.795842170715332,
      "learning_rate": 3.369366546555956e-05,
      "loss": 1.8012,
      "step": 249800
    },
    {
      "epoch": 19.575434748550837,
      "grad_norm": 5.782533645629883,
      "learning_rate": 3.368713770954097e-05,
      "loss": 1.7879,
      "step": 249900
    },
    {
      "epoch": 19.583268055773146,
      "grad_norm": 5.145888328552246,
      "learning_rate": 3.368060995352238e-05,
      "loss": 1.8446,
      "step": 250000
    },
    {
      "epoch": 19.591101362995456,
      "grad_norm": 4.703640937805176,
      "learning_rate": 3.3674082197503784e-05,
      "loss": 1.6763,
      "step": 250100
    },
    {
      "epoch": 19.598934670217766,
      "grad_norm": 5.457275390625,
      "learning_rate": 3.36675544414852e-05,
      "loss": 1.7779,
      "step": 250200
    },
    {
      "epoch": 19.606767977440075,
      "grad_norm": 7.305327415466309,
      "learning_rate": 3.366102668546661e-05,
      "loss": 1.6899,
      "step": 250300
    },
    {
      "epoch": 19.614601284662385,
      "grad_norm": 5.461198329925537,
      "learning_rate": 3.3654498929448015e-05,
      "loss": 1.7908,
      "step": 250400
    },
    {
      "epoch": 19.622434591884694,
      "grad_norm": 6.031379222869873,
      "learning_rate": 3.364797117342942e-05,
      "loss": 1.7309,
      "step": 250500
    },
    {
      "epoch": 19.630267899107004,
      "grad_norm": 4.516569137573242,
      "learning_rate": 3.3641443417410834e-05,
      "loss": 1.8015,
      "step": 250600
    },
    {
      "epoch": 19.638101206329313,
      "grad_norm": 7.11091947555542,
      "learning_rate": 3.363491566139224e-05,
      "loss": 1.7621,
      "step": 250700
    },
    {
      "epoch": 19.645934513551623,
      "grad_norm": 6.715731143951416,
      "learning_rate": 3.3628387905373645e-05,
      "loss": 1.6993,
      "step": 250800
    },
    {
      "epoch": 19.65376782077393,
      "grad_norm": 5.3749494552612305,
      "learning_rate": 3.3621860149355064e-05,
      "loss": 1.7411,
      "step": 250900
    },
    {
      "epoch": 19.66160112799624,
      "grad_norm": 6.467195510864258,
      "learning_rate": 3.361533239333647e-05,
      "loss": 1.7837,
      "step": 251000
    },
    {
      "epoch": 19.669434435218548,
      "grad_norm": 5.393986225128174,
      "learning_rate": 3.3608804637317876e-05,
      "loss": 1.7546,
      "step": 251100
    },
    {
      "epoch": 19.677267742440858,
      "grad_norm": 5.547854423522949,
      "learning_rate": 3.360227688129929e-05,
      "loss": 1.7325,
      "step": 251200
    },
    {
      "epoch": 19.685101049663167,
      "grad_norm": 7.017618179321289,
      "learning_rate": 3.3595749125280694e-05,
      "loss": 1.6768,
      "step": 251300
    },
    {
      "epoch": 19.692934356885477,
      "grad_norm": 2.4632115364074707,
      "learning_rate": 3.35892213692621e-05,
      "loss": 1.7925,
      "step": 251400
    },
    {
      "epoch": 19.700767664107786,
      "grad_norm": 6.838687896728516,
      "learning_rate": 3.358269361324351e-05,
      "loss": 1.787,
      "step": 251500
    },
    {
      "epoch": 19.708600971330096,
      "grad_norm": 5.401732444763184,
      "learning_rate": 3.3576165857224925e-05,
      "loss": 1.8095,
      "step": 251600
    },
    {
      "epoch": 19.716434278552406,
      "grad_norm": 5.602209568023682,
      "learning_rate": 3.356963810120633e-05,
      "loss": 1.8124,
      "step": 251700
    },
    {
      "epoch": 19.724267585774715,
      "grad_norm": 6.150458812713623,
      "learning_rate": 3.356311034518774e-05,
      "loss": 1.719,
      "step": 251800
    },
    {
      "epoch": 19.732100892997025,
      "grad_norm": 6.166698455810547,
      "learning_rate": 3.355658258916915e-05,
      "loss": 1.6784,
      "step": 251900
    },
    {
      "epoch": 19.739934200219334,
      "grad_norm": 4.834582805633545,
      "learning_rate": 3.3550054833150555e-05,
      "loss": 1.7986,
      "step": 252000
    },
    {
      "epoch": 19.74776750744164,
      "grad_norm": 7.824340343475342,
      "learning_rate": 3.354352707713197e-05,
      "loss": 1.7723,
      "step": 252100
    },
    {
      "epoch": 19.75560081466395,
      "grad_norm": 4.679460048675537,
      "learning_rate": 3.353699932111338e-05,
      "loss": 1.8164,
      "step": 252200
    },
    {
      "epoch": 19.76343412188626,
      "grad_norm": 6.688159942626953,
      "learning_rate": 3.3530471565094786e-05,
      "loss": 1.7342,
      "step": 252300
    },
    {
      "epoch": 19.77126742910857,
      "grad_norm": 4.9435272216796875,
      "learning_rate": 3.352394380907619e-05,
      "loss": 1.6809,
      "step": 252400
    },
    {
      "epoch": 19.77910073633088,
      "grad_norm": 6.963136672973633,
      "learning_rate": 3.3517416053057604e-05,
      "loss": 1.7141,
      "step": 252500
    },
    {
      "epoch": 19.786934043553188,
      "grad_norm": 7.926363468170166,
      "learning_rate": 3.351088829703901e-05,
      "loss": 1.6815,
      "step": 252600
    },
    {
      "epoch": 19.794767350775498,
      "grad_norm": 6.364804267883301,
      "learning_rate": 3.3504360541020416e-05,
      "loss": 1.7378,
      "step": 252700
    },
    {
      "epoch": 19.802600657997807,
      "grad_norm": 6.192513942718506,
      "learning_rate": 3.349783278500183e-05,
      "loss": 1.7773,
      "step": 252800
    },
    {
      "epoch": 19.810433965220117,
      "grad_norm": 6.076749324798584,
      "learning_rate": 3.349130502898324e-05,
      "loss": 1.8005,
      "step": 252900
    },
    {
      "epoch": 19.818267272442426,
      "grad_norm": 6.108242988586426,
      "learning_rate": 3.3484777272964647e-05,
      "loss": 1.6791,
      "step": 253000
    },
    {
      "epoch": 19.826100579664736,
      "grad_norm": 5.019177436828613,
      "learning_rate": 3.347824951694606e-05,
      "loss": 1.7499,
      "step": 253100
    },
    {
      "epoch": 19.833933886887042,
      "grad_norm": 5.306890964508057,
      "learning_rate": 3.3471721760927465e-05,
      "loss": 1.7549,
      "step": 253200
    },
    {
      "epoch": 19.84176719410935,
      "grad_norm": 5.064974784851074,
      "learning_rate": 3.346519400490887e-05,
      "loss": 1.7388,
      "step": 253300
    },
    {
      "epoch": 19.84960050133166,
      "grad_norm": 8.133454322814941,
      "learning_rate": 3.345866624889028e-05,
      "loss": 1.7003,
      "step": 253400
    },
    {
      "epoch": 19.85743380855397,
      "grad_norm": 5.579736232757568,
      "learning_rate": 3.3452138492871696e-05,
      "loss": 1.7694,
      "step": 253500
    },
    {
      "epoch": 19.86526711577628,
      "grad_norm": 6.594063758850098,
      "learning_rate": 3.34456107368531e-05,
      "loss": 1.7579,
      "step": 253600
    },
    {
      "epoch": 19.87310042299859,
      "grad_norm": 6.65127420425415,
      "learning_rate": 3.343908298083451e-05,
      "loss": 1.7092,
      "step": 253700
    },
    {
      "epoch": 19.8809337302209,
      "grad_norm": 6.4828081130981445,
      "learning_rate": 3.343255522481592e-05,
      "loss": 1.7672,
      "step": 253800
    },
    {
      "epoch": 19.88876703744321,
      "grad_norm": 6.536229133605957,
      "learning_rate": 3.3426027468797326e-05,
      "loss": 1.6642,
      "step": 253900
    },
    {
      "epoch": 19.89660034466552,
      "grad_norm": 5.359438896179199,
      "learning_rate": 3.341949971277873e-05,
      "loss": 1.706,
      "step": 254000
    },
    {
      "epoch": 19.904433651887828,
      "grad_norm": 5.740692615509033,
      "learning_rate": 3.341297195676015e-05,
      "loss": 1.7508,
      "step": 254100
    },
    {
      "epoch": 19.912266959110138,
      "grad_norm": 6.365234375,
      "learning_rate": 3.3406444200741556e-05,
      "loss": 1.7333,
      "step": 254200
    },
    {
      "epoch": 19.920100266332447,
      "grad_norm": 6.264545917510986,
      "learning_rate": 3.339991644472296e-05,
      "loss": 1.7372,
      "step": 254300
    },
    {
      "epoch": 19.927933573554753,
      "grad_norm": 4.2061944007873535,
      "learning_rate": 3.3393388688704375e-05,
      "loss": 1.7523,
      "step": 254400
    },
    {
      "epoch": 19.935766880777063,
      "grad_norm": 6.012517929077148,
      "learning_rate": 3.338686093268578e-05,
      "loss": 1.7253,
      "step": 254500
    },
    {
      "epoch": 19.943600187999373,
      "grad_norm": 4.130316734313965,
      "learning_rate": 3.3380333176667186e-05,
      "loss": 1.7128,
      "step": 254600
    },
    {
      "epoch": 19.951433495221682,
      "grad_norm": 3.0387418270111084,
      "learning_rate": 3.33738054206486e-05,
      "loss": 1.7299,
      "step": 254700
    },
    {
      "epoch": 19.95926680244399,
      "grad_norm": 4.736509323120117,
      "learning_rate": 3.336727766463001e-05,
      "loss": 1.765,
      "step": 254800
    },
    {
      "epoch": 19.9671001096663,
      "grad_norm": 7.064998626708984,
      "learning_rate": 3.336074990861142e-05,
      "loss": 1.6962,
      "step": 254900
    },
    {
      "epoch": 19.97493341688861,
      "grad_norm": 8.744054794311523,
      "learning_rate": 3.335422215259283e-05,
      "loss": 1.7026,
      "step": 255000
    },
    {
      "epoch": 19.98276672411092,
      "grad_norm": 6.769168376922607,
      "learning_rate": 3.3347694396574235e-05,
      "loss": 1.6707,
      "step": 255100
    },
    {
      "epoch": 19.99060003133323,
      "grad_norm": 4.024702548980713,
      "learning_rate": 3.334116664055564e-05,
      "loss": 1.8013,
      "step": 255200
    },
    {
      "epoch": 19.99843333855554,
      "grad_norm": 7.184065341949463,
      "learning_rate": 3.3334638884537054e-05,
      "loss": 1.7281,
      "step": 255300
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.7975133657455444,
      "eval_runtime": 2.9645,
      "eval_samples_per_second": 226.686,
      "eval_steps_per_second": 226.686,
      "step": 255320
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.502272605895996,
      "eval_runtime": 54.9815,
      "eval_samples_per_second": 232.187,
      "eval_steps_per_second": 232.187,
      "step": 255320
    },
    {
      "epoch": 20.00626664577785,
      "grad_norm": 8.276839256286621,
      "learning_rate": 3.3328111128518466e-05,
      "loss": 1.69,
      "step": 255400
    },
    {
      "epoch": 20.014099953000155,
      "grad_norm": 6.270479202270508,
      "learning_rate": 3.332158337249987e-05,
      "loss": 1.8012,
      "step": 255500
    },
    {
      "epoch": 20.021933260222465,
      "grad_norm": 5.036230087280273,
      "learning_rate": 3.331505561648128e-05,
      "loss": 1.6904,
      "step": 255600
    },
    {
      "epoch": 20.029766567444774,
      "grad_norm": 6.343362331390381,
      "learning_rate": 3.330852786046269e-05,
      "loss": 1.783,
      "step": 255700
    },
    {
      "epoch": 20.037599874667084,
      "grad_norm": 5.666706562042236,
      "learning_rate": 3.3302000104444096e-05,
      "loss": 1.773,
      "step": 255800
    },
    {
      "epoch": 20.045433181889393,
      "grad_norm": 7.041997909545898,
      "learning_rate": 3.32954723484255e-05,
      "loss": 1.6985,
      "step": 255900
    },
    {
      "epoch": 20.053266489111703,
      "grad_norm": 5.737365245819092,
      "learning_rate": 3.3288944592406914e-05,
      "loss": 1.7919,
      "step": 256000
    },
    {
      "epoch": 20.061099796334013,
      "grad_norm": 7.627339839935303,
      "learning_rate": 3.328241683638833e-05,
      "loss": 1.7208,
      "step": 256100
    },
    {
      "epoch": 20.068933103556322,
      "grad_norm": 6.505967140197754,
      "learning_rate": 3.327588908036973e-05,
      "loss": 1.689,
      "step": 256200
    },
    {
      "epoch": 20.07676641077863,
      "grad_norm": 5.520288467407227,
      "learning_rate": 3.3269361324351145e-05,
      "loss": 1.6989,
      "step": 256300
    },
    {
      "epoch": 20.08459971800094,
      "grad_norm": 7.131052017211914,
      "learning_rate": 3.326283356833255e-05,
      "loss": 1.666,
      "step": 256400
    },
    {
      "epoch": 20.09243302522325,
      "grad_norm": 3.642824649810791,
      "learning_rate": 3.325630581231396e-05,
      "loss": 1.7221,
      "step": 256500
    },
    {
      "epoch": 20.100266332445557,
      "grad_norm": 7.052626609802246,
      "learning_rate": 3.324977805629537e-05,
      "loss": 1.7573,
      "step": 256600
    },
    {
      "epoch": 20.108099639667866,
      "grad_norm": 6.227840900421143,
      "learning_rate": 3.324325030027678e-05,
      "loss": 1.7884,
      "step": 256700
    },
    {
      "epoch": 20.115932946890176,
      "grad_norm": 5.597837924957275,
      "learning_rate": 3.323672254425819e-05,
      "loss": 1.7322,
      "step": 256800
    },
    {
      "epoch": 20.123766254112486,
      "grad_norm": 5.7878804206848145,
      "learning_rate": 3.3230194788239594e-05,
      "loss": 1.6931,
      "step": 256900
    },
    {
      "epoch": 20.131599561334795,
      "grad_norm": 5.550678253173828,
      "learning_rate": 3.3223667032221006e-05,
      "loss": 1.7799,
      "step": 257000
    },
    {
      "epoch": 20.139432868557105,
      "grad_norm": 4.017091751098633,
      "learning_rate": 3.321713927620241e-05,
      "loss": 1.621,
      "step": 257100
    },
    {
      "epoch": 20.147266175779414,
      "grad_norm": 7.43173885345459,
      "learning_rate": 3.321061152018382e-05,
      "loss": 1.6751,
      "step": 257200
    },
    {
      "epoch": 20.155099483001724,
      "grad_norm": 6.947762489318848,
      "learning_rate": 3.320408376416524e-05,
      "loss": 1.7816,
      "step": 257300
    },
    {
      "epoch": 20.162932790224033,
      "grad_norm": 6.268804550170898,
      "learning_rate": 3.319755600814664e-05,
      "loss": 1.7516,
      "step": 257400
    },
    {
      "epoch": 20.170766097446343,
      "grad_norm": 6.336014747619629,
      "learning_rate": 3.319102825212805e-05,
      "loss": 1.7514,
      "step": 257500
    },
    {
      "epoch": 20.178599404668653,
      "grad_norm": 5.120447635650635,
      "learning_rate": 3.318450049610946e-05,
      "loss": 1.624,
      "step": 257600
    },
    {
      "epoch": 20.186432711890962,
      "grad_norm": 6.667529582977295,
      "learning_rate": 3.317797274009087e-05,
      "loss": 1.7138,
      "step": 257700
    },
    {
      "epoch": 20.194266019113268,
      "grad_norm": 5.7863945960998535,
      "learning_rate": 3.317144498407227e-05,
      "loss": 1.6822,
      "step": 257800
    },
    {
      "epoch": 20.202099326335578,
      "grad_norm": 4.778263568878174,
      "learning_rate": 3.3164917228053685e-05,
      "loss": 1.6868,
      "step": 257900
    },
    {
      "epoch": 20.209932633557887,
      "grad_norm": 5.346489906311035,
      "learning_rate": 3.31583894720351e-05,
      "loss": 1.5939,
      "step": 258000
    },
    {
      "epoch": 20.217765940780197,
      "grad_norm": 6.182313919067383,
      "learning_rate": 3.31518617160165e-05,
      "loss": 1.6823,
      "step": 258100
    },
    {
      "epoch": 20.225599248002506,
      "grad_norm": 5.410881519317627,
      "learning_rate": 3.3145333959997916e-05,
      "loss": 1.7054,
      "step": 258200
    },
    {
      "epoch": 20.233432555224816,
      "grad_norm": 5.930952072143555,
      "learning_rate": 3.313880620397932e-05,
      "loss": 1.6676,
      "step": 258300
    },
    {
      "epoch": 20.241265862447126,
      "grad_norm": 9.040253639221191,
      "learning_rate": 3.313227844796073e-05,
      "loss": 1.6974,
      "step": 258400
    },
    {
      "epoch": 20.249099169669435,
      "grad_norm": 6.9310526847839355,
      "learning_rate": 3.312575069194214e-05,
      "loss": 1.7561,
      "step": 258500
    },
    {
      "epoch": 20.256932476891745,
      "grad_norm": 7.081728935241699,
      "learning_rate": 3.311922293592355e-05,
      "loss": 1.6989,
      "step": 258600
    },
    {
      "epoch": 20.264765784114054,
      "grad_norm": 6.182151794433594,
      "learning_rate": 3.311269517990496e-05,
      "loss": 1.7395,
      "step": 258700
    },
    {
      "epoch": 20.272599091336364,
      "grad_norm": 6.326460838317871,
      "learning_rate": 3.3106167423886364e-05,
      "loss": 1.6931,
      "step": 258800
    },
    {
      "epoch": 20.28043239855867,
      "grad_norm": 5.701855659484863,
      "learning_rate": 3.309963966786778e-05,
      "loss": 1.7154,
      "step": 258900
    },
    {
      "epoch": 20.28826570578098,
      "grad_norm": 6.309967994689941,
      "learning_rate": 3.309311191184918e-05,
      "loss": 1.7468,
      "step": 259000
    },
    {
      "epoch": 20.29609901300329,
      "grad_norm": 6.850462913513184,
      "learning_rate": 3.308658415583059e-05,
      "loss": 1.7176,
      "step": 259100
    },
    {
      "epoch": 20.3039323202256,
      "grad_norm": 7.686995983123779,
      "learning_rate": 3.3080056399812e-05,
      "loss": 1.686,
      "step": 259200
    },
    {
      "epoch": 20.311765627447908,
      "grad_norm": 5.844062328338623,
      "learning_rate": 3.307352864379341e-05,
      "loss": 1.661,
      "step": 259300
    },
    {
      "epoch": 20.319598934670218,
      "grad_norm": 4.991491794586182,
      "learning_rate": 3.306700088777482e-05,
      "loss": 1.7675,
      "step": 259400
    },
    {
      "epoch": 20.327432241892527,
      "grad_norm": 6.090445041656494,
      "learning_rate": 3.306047313175623e-05,
      "loss": 1.7789,
      "step": 259500
    },
    {
      "epoch": 20.335265549114837,
      "grad_norm": 5.805224895477295,
      "learning_rate": 3.305394537573764e-05,
      "loss": 1.7087,
      "step": 259600
    },
    {
      "epoch": 20.343098856337146,
      "grad_norm": 5.950680732727051,
      "learning_rate": 3.304741761971904e-05,
      "loss": 1.7285,
      "step": 259700
    },
    {
      "epoch": 20.350932163559456,
      "grad_norm": 7.730785846710205,
      "learning_rate": 3.3040889863700456e-05,
      "loss": 1.6813,
      "step": 259800
    },
    {
      "epoch": 20.358765470781766,
      "grad_norm": 6.410564422607422,
      "learning_rate": 3.303436210768187e-05,
      "loss": 1.6773,
      "step": 259900
    },
    {
      "epoch": 20.36659877800407,
      "grad_norm": 7.810969829559326,
      "learning_rate": 3.3027834351663274e-05,
      "loss": 1.7211,
      "step": 260000
    },
    {
      "epoch": 20.37443208522638,
      "grad_norm": 9.017537117004395,
      "learning_rate": 3.3021306595644687e-05,
      "loss": 1.7145,
      "step": 260100
    },
    {
      "epoch": 20.38226539244869,
      "grad_norm": 6.4311418533325195,
      "learning_rate": 3.301477883962609e-05,
      "loss": 1.6928,
      "step": 260200
    },
    {
      "epoch": 20.390098699671,
      "grad_norm": 6.855362892150879,
      "learning_rate": 3.30082510836075e-05,
      "loss": 1.6667,
      "step": 260300
    },
    {
      "epoch": 20.39793200689331,
      "grad_norm": 6.655640602111816,
      "learning_rate": 3.3001723327588904e-05,
      "loss": 1.8064,
      "step": 260400
    },
    {
      "epoch": 20.40576531411562,
      "grad_norm": 5.659409999847412,
      "learning_rate": 3.299519557157032e-05,
      "loss": 1.7389,
      "step": 260500
    },
    {
      "epoch": 20.41359862133793,
      "grad_norm": 6.219272136688232,
      "learning_rate": 3.298866781555173e-05,
      "loss": 1.7418,
      "step": 260600
    },
    {
      "epoch": 20.42143192856024,
      "grad_norm": 5.95920991897583,
      "learning_rate": 3.2982140059533135e-05,
      "loss": 1.8215,
      "step": 260700
    },
    {
      "epoch": 20.429265235782548,
      "grad_norm": 6.041280269622803,
      "learning_rate": 3.297561230351455e-05,
      "loss": 1.6775,
      "step": 260800
    },
    {
      "epoch": 20.437098543004858,
      "grad_norm": 5.0312957763671875,
      "learning_rate": 3.296908454749595e-05,
      "loss": 1.7114,
      "step": 260900
    },
    {
      "epoch": 20.444931850227167,
      "grad_norm": 5.976183891296387,
      "learning_rate": 3.296255679147736e-05,
      "loss": 1.7136,
      "step": 261000
    },
    {
      "epoch": 20.452765157449477,
      "grad_norm": 4.691251754760742,
      "learning_rate": 3.295602903545877e-05,
      "loss": 1.6776,
      "step": 261100
    },
    {
      "epoch": 20.460598464671783,
      "grad_norm": 6.975634574890137,
      "learning_rate": 3.2949501279440184e-05,
      "loss": 1.7828,
      "step": 261200
    },
    {
      "epoch": 20.468431771894092,
      "grad_norm": 5.859082221984863,
      "learning_rate": 3.294297352342159e-05,
      "loss": 1.8016,
      "step": 261300
    },
    {
      "epoch": 20.476265079116402,
      "grad_norm": 5.399866580963135,
      "learning_rate": 3.2936445767403e-05,
      "loss": 1.6876,
      "step": 261400
    },
    {
      "epoch": 20.48409838633871,
      "grad_norm": 5.422239780426025,
      "learning_rate": 3.292991801138441e-05,
      "loss": 1.7781,
      "step": 261500
    },
    {
      "epoch": 20.49193169356102,
      "grad_norm": 8.48833179473877,
      "learning_rate": 3.2923390255365814e-05,
      "loss": 1.7407,
      "step": 261600
    },
    {
      "epoch": 20.49976500078333,
      "grad_norm": 6.402830123901367,
      "learning_rate": 3.2916862499347226e-05,
      "loss": 1.8362,
      "step": 261700
    },
    {
      "epoch": 20.50759830800564,
      "grad_norm": 5.712980270385742,
      "learning_rate": 3.291033474332864e-05,
      "loss": 1.7134,
      "step": 261800
    },
    {
      "epoch": 20.51543161522795,
      "grad_norm": 5.393313407897949,
      "learning_rate": 3.2903806987310045e-05,
      "loss": 1.7764,
      "step": 261900
    },
    {
      "epoch": 20.52326492245026,
      "grad_norm": 8.24570083618164,
      "learning_rate": 3.289727923129145e-05,
      "loss": 1.8508,
      "step": 262000
    },
    {
      "epoch": 20.53109822967257,
      "grad_norm": 5.88576602935791,
      "learning_rate": 3.289075147527286e-05,
      "loss": 1.7286,
      "step": 262100
    },
    {
      "epoch": 20.53893153689488,
      "grad_norm": 5.12619686126709,
      "learning_rate": 3.288422371925427e-05,
      "loss": 1.729,
      "step": 262200
    },
    {
      "epoch": 20.546764844117185,
      "grad_norm": 6.214330196380615,
      "learning_rate": 3.2877695963235674e-05,
      "loss": 1.743,
      "step": 262300
    },
    {
      "epoch": 20.554598151339494,
      "grad_norm": 7.216902732849121,
      "learning_rate": 3.287116820721709e-05,
      "loss": 1.7529,
      "step": 262400
    },
    {
      "epoch": 20.562431458561804,
      "grad_norm": 5.606049060821533,
      "learning_rate": 3.28646404511985e-05,
      "loss": 1.6893,
      "step": 262500
    },
    {
      "epoch": 20.570264765784113,
      "grad_norm": 7.171075820922852,
      "learning_rate": 3.2858112695179905e-05,
      "loss": 1.8081,
      "step": 262600
    },
    {
      "epoch": 20.578098073006423,
      "grad_norm": 6.687615871429443,
      "learning_rate": 3.285158493916132e-05,
      "loss": 1.8113,
      "step": 262700
    },
    {
      "epoch": 20.585931380228732,
      "grad_norm": 7.048247814178467,
      "learning_rate": 3.2845057183142724e-05,
      "loss": 1.7698,
      "step": 262800
    },
    {
      "epoch": 20.593764687451042,
      "grad_norm": 6.255409240722656,
      "learning_rate": 3.283852942712413e-05,
      "loss": 1.7523,
      "step": 262900
    },
    {
      "epoch": 20.60159799467335,
      "grad_norm": 6.291770935058594,
      "learning_rate": 3.283200167110554e-05,
      "loss": 1.7309,
      "step": 263000
    },
    {
      "epoch": 20.60943130189566,
      "grad_norm": 5.625386714935303,
      "learning_rate": 3.2825473915086954e-05,
      "loss": 1.6612,
      "step": 263100
    },
    {
      "epoch": 20.61726460911797,
      "grad_norm": 5.745880126953125,
      "learning_rate": 3.281894615906836e-05,
      "loss": 1.7352,
      "step": 263200
    },
    {
      "epoch": 20.62509791634028,
      "grad_norm": 5.476532936096191,
      "learning_rate": 3.281241840304977e-05,
      "loss": 1.6524,
      "step": 263300
    },
    {
      "epoch": 20.632931223562586,
      "grad_norm": 5.871772766113281,
      "learning_rate": 3.280589064703118e-05,
      "loss": 1.7117,
      "step": 263400
    },
    {
      "epoch": 20.640764530784896,
      "grad_norm": 7.668819427490234,
      "learning_rate": 3.2799362891012584e-05,
      "loss": 1.6482,
      "step": 263500
    },
    {
      "epoch": 20.648597838007205,
      "grad_norm": 5.965639114379883,
      "learning_rate": 3.279283513499399e-05,
      "loss": 1.7626,
      "step": 263600
    },
    {
      "epoch": 20.656431145229515,
      "grad_norm": 5.0637125968933105,
      "learning_rate": 3.278630737897541e-05,
      "loss": 1.7841,
      "step": 263700
    },
    {
      "epoch": 20.664264452451825,
      "grad_norm": 3.8619744777679443,
      "learning_rate": 3.2779779622956815e-05,
      "loss": 1.7225,
      "step": 263800
    },
    {
      "epoch": 20.672097759674134,
      "grad_norm": 6.547771453857422,
      "learning_rate": 3.277325186693822e-05,
      "loss": 1.6814,
      "step": 263900
    },
    {
      "epoch": 20.679931066896444,
      "grad_norm": 6.045133113861084,
      "learning_rate": 3.2766724110919633e-05,
      "loss": 1.7514,
      "step": 264000
    },
    {
      "epoch": 20.687764374118753,
      "grad_norm": 4.884887218475342,
      "learning_rate": 3.276019635490104e-05,
      "loss": 1.7237,
      "step": 264100
    },
    {
      "epoch": 20.695597681341063,
      "grad_norm": 3.2643609046936035,
      "learning_rate": 3.2753668598882445e-05,
      "loss": 1.6808,
      "step": 264200
    },
    {
      "epoch": 20.703430988563372,
      "grad_norm": 6.259937286376953,
      "learning_rate": 3.274714084286386e-05,
      "loss": 1.6965,
      "step": 264300
    },
    {
      "epoch": 20.711264295785682,
      "grad_norm": 4.8104166984558105,
      "learning_rate": 3.274061308684527e-05,
      "loss": 1.7688,
      "step": 264400
    },
    {
      "epoch": 20.71909760300799,
      "grad_norm": 7.2350029945373535,
      "learning_rate": 3.2734085330826676e-05,
      "loss": 1.6649,
      "step": 264500
    },
    {
      "epoch": 20.726930910230298,
      "grad_norm": 5.801260948181152,
      "learning_rate": 3.272755757480809e-05,
      "loss": 1.7281,
      "step": 264600
    },
    {
      "epoch": 20.734764217452607,
      "grad_norm": 7.134361267089844,
      "learning_rate": 3.2721029818789494e-05,
      "loss": 1.7334,
      "step": 264700
    },
    {
      "epoch": 20.742597524674917,
      "grad_norm": 5.0285234451293945,
      "learning_rate": 3.27145020627709e-05,
      "loss": 1.7892,
      "step": 264800
    },
    {
      "epoch": 20.750430831897226,
      "grad_norm": 5.316565990447998,
      "learning_rate": 3.270797430675231e-05,
      "loss": 1.7728,
      "step": 264900
    },
    {
      "epoch": 20.758264139119536,
      "grad_norm": 4.513759136199951,
      "learning_rate": 3.2701446550733725e-05,
      "loss": 1.6964,
      "step": 265000
    },
    {
      "epoch": 20.766097446341846,
      "grad_norm": 6.409020900726318,
      "learning_rate": 3.269491879471513e-05,
      "loss": 1.7,
      "step": 265100
    },
    {
      "epoch": 20.773930753564155,
      "grad_norm": 4.868933200836182,
      "learning_rate": 3.268839103869654e-05,
      "loss": 1.7522,
      "step": 265200
    },
    {
      "epoch": 20.781764060786465,
      "grad_norm": 6.694874286651611,
      "learning_rate": 3.268186328267795e-05,
      "loss": 1.7903,
      "step": 265300
    },
    {
      "epoch": 20.789597368008774,
      "grad_norm": 6.835532188415527,
      "learning_rate": 3.2675335526659355e-05,
      "loss": 1.6773,
      "step": 265400
    },
    {
      "epoch": 20.797430675231084,
      "grad_norm": 8.184743881225586,
      "learning_rate": 3.266880777064076e-05,
      "loss": 1.7852,
      "step": 265500
    },
    {
      "epoch": 20.805263982453393,
      "grad_norm": 5.806459426879883,
      "learning_rate": 3.266228001462217e-05,
      "loss": 1.7695,
      "step": 265600
    },
    {
      "epoch": 20.8130972896757,
      "grad_norm": 6.208354473114014,
      "learning_rate": 3.2655752258603586e-05,
      "loss": 1.6941,
      "step": 265700
    },
    {
      "epoch": 20.82093059689801,
      "grad_norm": 5.616569519042969,
      "learning_rate": 3.264922450258499e-05,
      "loss": 1.8163,
      "step": 265800
    },
    {
      "epoch": 20.82876390412032,
      "grad_norm": 6.088566780090332,
      "learning_rate": 3.2642696746566404e-05,
      "loss": 1.8038,
      "step": 265900
    },
    {
      "epoch": 20.836597211342628,
      "grad_norm": 7.701503276824951,
      "learning_rate": 3.263616899054781e-05,
      "loss": 1.7625,
      "step": 266000
    },
    {
      "epoch": 20.844430518564938,
      "grad_norm": 6.095677375793457,
      "learning_rate": 3.2629641234529216e-05,
      "loss": 1.6774,
      "step": 266100
    },
    {
      "epoch": 20.852263825787247,
      "grad_norm": 6.06840705871582,
      "learning_rate": 3.262311347851063e-05,
      "loss": 1.7274,
      "step": 266200
    },
    {
      "epoch": 20.860097133009557,
      "grad_norm": 6.277737140655518,
      "learning_rate": 3.261658572249204e-05,
      "loss": 1.7529,
      "step": 266300
    },
    {
      "epoch": 20.867930440231866,
      "grad_norm": 8.512782096862793,
      "learning_rate": 3.2610057966473446e-05,
      "loss": 1.7851,
      "step": 266400
    },
    {
      "epoch": 20.875763747454176,
      "grad_norm": 7.896122932434082,
      "learning_rate": 3.260353021045486e-05,
      "loss": 1.609,
      "step": 266500
    },
    {
      "epoch": 20.883597054676486,
      "grad_norm": 5.886838436126709,
      "learning_rate": 3.2597002454436265e-05,
      "loss": 1.744,
      "step": 266600
    },
    {
      "epoch": 20.891430361898795,
      "grad_norm": 6.285374164581299,
      "learning_rate": 3.259047469841767e-05,
      "loss": 1.7609,
      "step": 266700
    },
    {
      "epoch": 20.899263669121105,
      "grad_norm": 7.157028675079346,
      "learning_rate": 3.258394694239908e-05,
      "loss": 1.7232,
      "step": 266800
    },
    {
      "epoch": 20.90709697634341,
      "grad_norm": 8.099777221679688,
      "learning_rate": 3.2577419186380496e-05,
      "loss": 1.7478,
      "step": 266900
    },
    {
      "epoch": 20.91493028356572,
      "grad_norm": 5.7389068603515625,
      "learning_rate": 3.25708914303619e-05,
      "loss": 1.6607,
      "step": 267000
    },
    {
      "epoch": 20.92276359078803,
      "grad_norm": 5.389855861663818,
      "learning_rate": 3.256436367434331e-05,
      "loss": 1.7897,
      "step": 267100
    },
    {
      "epoch": 20.93059689801034,
      "grad_norm": 6.756251335144043,
      "learning_rate": 3.255783591832472e-05,
      "loss": 1.7568,
      "step": 267200
    },
    {
      "epoch": 20.93843020523265,
      "grad_norm": 5.912774085998535,
      "learning_rate": 3.2551308162306125e-05,
      "loss": 1.7267,
      "step": 267300
    },
    {
      "epoch": 20.94626351245496,
      "grad_norm": 7.1618242263793945,
      "learning_rate": 3.254478040628753e-05,
      "loss": 1.6913,
      "step": 267400
    },
    {
      "epoch": 20.954096819677268,
      "grad_norm": 8.013052940368652,
      "learning_rate": 3.2538252650268944e-05,
      "loss": 1.7882,
      "step": 267500
    },
    {
      "epoch": 20.961930126899578,
      "grad_norm": 4.935395240783691,
      "learning_rate": 3.2531724894250356e-05,
      "loss": 1.716,
      "step": 267600
    },
    {
      "epoch": 20.969763434121887,
      "grad_norm": 6.157013416290283,
      "learning_rate": 3.252519713823176e-05,
      "loss": 1.8256,
      "step": 267700
    },
    {
      "epoch": 20.977596741344197,
      "grad_norm": 4.0515007972717285,
      "learning_rate": 3.2518669382213175e-05,
      "loss": 1.7388,
      "step": 267800
    },
    {
      "epoch": 20.985430048566506,
      "grad_norm": 5.531866550445557,
      "learning_rate": 3.251214162619458e-05,
      "loss": 1.7687,
      "step": 267900
    },
    {
      "epoch": 20.993263355788812,
      "grad_norm": 6.996670722961426,
      "learning_rate": 3.2505613870175986e-05,
      "loss": 1.6498,
      "step": 268000
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.7874890565872192,
      "eval_runtime": 2.9357,
      "eval_samples_per_second": 228.907,
      "eval_steps_per_second": 228.907,
      "step": 268086
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.489292860031128,
      "eval_runtime": 55.831,
      "eval_samples_per_second": 228.654,
      "eval_steps_per_second": 228.654,
      "step": 268086
    },
    {
      "epoch": 21.001096663011122,
      "grad_norm": 6.498220443725586,
      "learning_rate": 3.24990861141574e-05,
      "loss": 1.825,
      "step": 268100
    },
    {
      "epoch": 21.00892997023343,
      "grad_norm": 6.29610013961792,
      "learning_rate": 3.249255835813881e-05,
      "loss": 1.7629,
      "step": 268200
    },
    {
      "epoch": 21.01676327745574,
      "grad_norm": 7.968236923217773,
      "learning_rate": 3.248603060212022e-05,
      "loss": 1.7141,
      "step": 268300
    },
    {
      "epoch": 21.02459658467805,
      "grad_norm": 8.382416725158691,
      "learning_rate": 3.247950284610163e-05,
      "loss": 1.6693,
      "step": 268400
    },
    {
      "epoch": 21.03242989190036,
      "grad_norm": 5.698066234588623,
      "learning_rate": 3.2472975090083035e-05,
      "loss": 1.7094,
      "step": 268500
    },
    {
      "epoch": 21.04026319912267,
      "grad_norm": 8.849953651428223,
      "learning_rate": 3.246644733406444e-05,
      "loss": 1.7142,
      "step": 268600
    },
    {
      "epoch": 21.04809650634498,
      "grad_norm": 6.918188095092773,
      "learning_rate": 3.245991957804585e-05,
      "loss": 1.7434,
      "step": 268700
    },
    {
      "epoch": 21.05592981356729,
      "grad_norm": 6.632404327392578,
      "learning_rate": 3.245339182202726e-05,
      "loss": 1.6803,
      "step": 268800
    },
    {
      "epoch": 21.0637631207896,
      "grad_norm": 7.146675109863281,
      "learning_rate": 3.244686406600867e-05,
      "loss": 1.7043,
      "step": 268900
    },
    {
      "epoch": 21.071596428011908,
      "grad_norm": 6.140374183654785,
      "learning_rate": 3.244033630999008e-05,
      "loss": 1.7219,
      "step": 269000
    },
    {
      "epoch": 21.079429735234214,
      "grad_norm": 5.942001819610596,
      "learning_rate": 3.243380855397149e-05,
      "loss": 1.6488,
      "step": 269100
    },
    {
      "epoch": 21.087263042456524,
      "grad_norm": 4.855989456176758,
      "learning_rate": 3.2427280797952896e-05,
      "loss": 1.6835,
      "step": 269200
    },
    {
      "epoch": 21.095096349678833,
      "grad_norm": 5.988896369934082,
      "learning_rate": 3.24207530419343e-05,
      "loss": 1.7541,
      "step": 269300
    },
    {
      "epoch": 21.102929656901143,
      "grad_norm": 5.3433685302734375,
      "learning_rate": 3.2414225285915714e-05,
      "loss": 1.7021,
      "step": 269400
    },
    {
      "epoch": 21.110762964123452,
      "grad_norm": 6.243994235992432,
      "learning_rate": 3.240769752989713e-05,
      "loss": 1.7586,
      "step": 269500
    },
    {
      "epoch": 21.118596271345762,
      "grad_norm": 6.994405269622803,
      "learning_rate": 3.240116977387853e-05,
      "loss": 1.7817,
      "step": 269600
    },
    {
      "epoch": 21.12642957856807,
      "grad_norm": 6.972924709320068,
      "learning_rate": 3.2394642017859945e-05,
      "loss": 1.7532,
      "step": 269700
    },
    {
      "epoch": 21.13426288579038,
      "grad_norm": 6.064153671264648,
      "learning_rate": 3.238811426184135e-05,
      "loss": 1.7495,
      "step": 269800
    },
    {
      "epoch": 21.14209619301269,
      "grad_norm": 5.517270088195801,
      "learning_rate": 3.238158650582276e-05,
      "loss": 1.7041,
      "step": 269900
    },
    {
      "epoch": 21.149929500235,
      "grad_norm": 4.202102184295654,
      "learning_rate": 3.237505874980417e-05,
      "loss": 1.7114,
      "step": 270000
    },
    {
      "epoch": 21.15776280745731,
      "grad_norm": 5.172712802886963,
      "learning_rate": 3.236853099378558e-05,
      "loss": 1.5926,
      "step": 270100
    },
    {
      "epoch": 21.16559611467962,
      "grad_norm": 5.716180324554443,
      "learning_rate": 3.236200323776699e-05,
      "loss": 1.753,
      "step": 270200
    },
    {
      "epoch": 21.173429421901925,
      "grad_norm": 5.003580570220947,
      "learning_rate": 3.23554754817484e-05,
      "loss": 1.756,
      "step": 270300
    },
    {
      "epoch": 21.181262729124235,
      "grad_norm": 6.041212558746338,
      "learning_rate": 3.2348947725729806e-05,
      "loss": 1.6997,
      "step": 270400
    },
    {
      "epoch": 21.189096036346545,
      "grad_norm": 5.740006923675537,
      "learning_rate": 3.234241996971121e-05,
      "loss": 1.5926,
      "step": 270500
    },
    {
      "epoch": 21.196929343568854,
      "grad_norm": 6.568294048309326,
      "learning_rate": 3.233589221369262e-05,
      "loss": 1.7194,
      "step": 270600
    },
    {
      "epoch": 21.204762650791164,
      "grad_norm": 6.5259904861450195,
      "learning_rate": 3.232936445767403e-05,
      "loss": 1.6577,
      "step": 270700
    },
    {
      "epoch": 21.212595958013473,
      "grad_norm": 6.139894008636475,
      "learning_rate": 3.232283670165544e-05,
      "loss": 1.7442,
      "step": 270800
    },
    {
      "epoch": 21.220429265235783,
      "grad_norm": 6.26597785949707,
      "learning_rate": 3.231630894563685e-05,
      "loss": 1.754,
      "step": 270900
    },
    {
      "epoch": 21.228262572458092,
      "grad_norm": 6.938931941986084,
      "learning_rate": 3.230978118961826e-05,
      "loss": 1.6171,
      "step": 271000
    },
    {
      "epoch": 21.236095879680402,
      "grad_norm": 6.5793280601501465,
      "learning_rate": 3.230325343359967e-05,
      "loss": 1.6967,
      "step": 271100
    },
    {
      "epoch": 21.24392918690271,
      "grad_norm": 5.986634731292725,
      "learning_rate": 3.229672567758107e-05,
      "loss": 1.6601,
      "step": 271200
    },
    {
      "epoch": 21.25176249412502,
      "grad_norm": 5.907602787017822,
      "learning_rate": 3.2290197921562485e-05,
      "loss": 1.7247,
      "step": 271300
    },
    {
      "epoch": 21.259595801347327,
      "grad_norm": 6.651430606842041,
      "learning_rate": 3.22836701655439e-05,
      "loss": 1.737,
      "step": 271400
    },
    {
      "epoch": 21.267429108569637,
      "grad_norm": 5.379238605499268,
      "learning_rate": 3.22771424095253e-05,
      "loss": 1.8314,
      "step": 271500
    },
    {
      "epoch": 21.275262415791946,
      "grad_norm": 6.36892557144165,
      "learning_rate": 3.2270614653506716e-05,
      "loss": 1.7156,
      "step": 271600
    },
    {
      "epoch": 21.283095723014256,
      "grad_norm": 8.245698928833008,
      "learning_rate": 3.226408689748812e-05,
      "loss": 1.6594,
      "step": 271700
    },
    {
      "epoch": 21.290929030236565,
      "grad_norm": 6.318088054656982,
      "learning_rate": 3.225755914146953e-05,
      "loss": 1.8312,
      "step": 271800
    },
    {
      "epoch": 21.298762337458875,
      "grad_norm": 6.849182605743408,
      "learning_rate": 3.225103138545094e-05,
      "loss": 1.7695,
      "step": 271900
    },
    {
      "epoch": 21.306595644681185,
      "grad_norm": 4.988706111907959,
      "learning_rate": 3.2244503629432346e-05,
      "loss": 1.7485,
      "step": 272000
    },
    {
      "epoch": 21.314428951903494,
      "grad_norm": 5.2397780418396,
      "learning_rate": 3.223797587341376e-05,
      "loss": 1.7155,
      "step": 272100
    },
    {
      "epoch": 21.322262259125804,
      "grad_norm": 6.9418230056762695,
      "learning_rate": 3.2231448117395164e-05,
      "loss": 1.6917,
      "step": 272200
    },
    {
      "epoch": 21.330095566348113,
      "grad_norm": 5.518104076385498,
      "learning_rate": 3.2224920361376577e-05,
      "loss": 1.834,
      "step": 272300
    },
    {
      "epoch": 21.337928873570423,
      "grad_norm": 6.619892597198486,
      "learning_rate": 3.221839260535798e-05,
      "loss": 1.7574,
      "step": 272400
    },
    {
      "epoch": 21.345762180792732,
      "grad_norm": 6.484167098999023,
      "learning_rate": 3.221186484933939e-05,
      "loss": 1.7769,
      "step": 272500
    },
    {
      "epoch": 21.35359548801504,
      "grad_norm": 6.791900634765625,
      "learning_rate": 3.22053370933208e-05,
      "loss": 1.748,
      "step": 272600
    },
    {
      "epoch": 21.361428795237348,
      "grad_norm": 5.781637191772461,
      "learning_rate": 3.219880933730221e-05,
      "loss": 1.7706,
      "step": 272700
    },
    {
      "epoch": 21.369262102459658,
      "grad_norm": 6.090129852294922,
      "learning_rate": 3.219228158128362e-05,
      "loss": 1.8353,
      "step": 272800
    },
    {
      "epoch": 21.377095409681967,
      "grad_norm": 6.317825794219971,
      "learning_rate": 3.218575382526503e-05,
      "loss": 1.7063,
      "step": 272900
    },
    {
      "epoch": 21.384928716904277,
      "grad_norm": 8.651288986206055,
      "learning_rate": 3.217922606924644e-05,
      "loss": 1.6395,
      "step": 273000
    },
    {
      "epoch": 21.392762024126586,
      "grad_norm": 5.876100540161133,
      "learning_rate": 3.217269831322784e-05,
      "loss": 1.7154,
      "step": 273100
    },
    {
      "epoch": 21.400595331348896,
      "grad_norm": 5.314919948577881,
      "learning_rate": 3.2166170557209256e-05,
      "loss": 1.7434,
      "step": 273200
    },
    {
      "epoch": 21.408428638571205,
      "grad_norm": 5.886397838592529,
      "learning_rate": 3.215964280119067e-05,
      "loss": 1.6461,
      "step": 273300
    },
    {
      "epoch": 21.416261945793515,
      "grad_norm": 5.513667583465576,
      "learning_rate": 3.2153115045172074e-05,
      "loss": 1.7892,
      "step": 273400
    },
    {
      "epoch": 21.424095253015825,
      "grad_norm": 6.7414774894714355,
      "learning_rate": 3.2146587289153486e-05,
      "loss": 1.6774,
      "step": 273500
    },
    {
      "epoch": 21.431928560238134,
      "grad_norm": 5.385015487670898,
      "learning_rate": 3.214005953313489e-05,
      "loss": 1.8324,
      "step": 273600
    },
    {
      "epoch": 21.43976186746044,
      "grad_norm": 6.409081935882568,
      "learning_rate": 3.21335317771163e-05,
      "loss": 1.6375,
      "step": 273700
    },
    {
      "epoch": 21.44759517468275,
      "grad_norm": 5.927212715148926,
      "learning_rate": 3.2127004021097704e-05,
      "loss": 1.7574,
      "step": 273800
    },
    {
      "epoch": 21.45542848190506,
      "grad_norm": 6.237218379974365,
      "learning_rate": 3.2120476265079116e-05,
      "loss": 1.6792,
      "step": 273900
    },
    {
      "epoch": 21.46326178912737,
      "grad_norm": 6.3103928565979,
      "learning_rate": 3.211394850906053e-05,
      "loss": 1.7332,
      "step": 274000
    },
    {
      "epoch": 21.47109509634968,
      "grad_norm": 6.632248401641846,
      "learning_rate": 3.2107420753041935e-05,
      "loss": 1.7,
      "step": 274100
    },
    {
      "epoch": 21.478928403571988,
      "grad_norm": 7.247546672821045,
      "learning_rate": 3.210089299702335e-05,
      "loss": 1.7016,
      "step": 274200
    },
    {
      "epoch": 21.486761710794298,
      "grad_norm": 6.956959247589111,
      "learning_rate": 3.209436524100475e-05,
      "loss": 1.7944,
      "step": 274300
    },
    {
      "epoch": 21.494595018016607,
      "grad_norm": 6.267390251159668,
      "learning_rate": 3.208783748498616e-05,
      "loss": 1.7192,
      "step": 274400
    },
    {
      "epoch": 21.502428325238917,
      "grad_norm": 5.211315631866455,
      "learning_rate": 3.208130972896757e-05,
      "loss": 1.8172,
      "step": 274500
    },
    {
      "epoch": 21.510261632461226,
      "grad_norm": 5.836353778839111,
      "learning_rate": 3.2074781972948984e-05,
      "loss": 1.6806,
      "step": 274600
    },
    {
      "epoch": 21.518094939683536,
      "grad_norm": 10.833849906921387,
      "learning_rate": 3.206825421693039e-05,
      "loss": 1.7821,
      "step": 274700
    },
    {
      "epoch": 21.525928246905842,
      "grad_norm": 6.397589206695557,
      "learning_rate": 3.20617264609118e-05,
      "loss": 1.6828,
      "step": 274800
    },
    {
      "epoch": 21.53376155412815,
      "grad_norm": 7.263945579528809,
      "learning_rate": 3.205519870489321e-05,
      "loss": 1.7281,
      "step": 274900
    },
    {
      "epoch": 21.54159486135046,
      "grad_norm": 6.081972599029541,
      "learning_rate": 3.2048670948874614e-05,
      "loss": 1.673,
      "step": 275000
    },
    {
      "epoch": 21.54942816857277,
      "grad_norm": 6.593804836273193,
      "learning_rate": 3.2042143192856026e-05,
      "loss": 1.7958,
      "step": 275100
    },
    {
      "epoch": 21.55726147579508,
      "grad_norm": 5.276115894317627,
      "learning_rate": 3.203561543683743e-05,
      "loss": 1.7353,
      "step": 275200
    },
    {
      "epoch": 21.56509478301739,
      "grad_norm": 6.343405723571777,
      "learning_rate": 3.2029087680818844e-05,
      "loss": 1.6745,
      "step": 275300
    },
    {
      "epoch": 21.5729280902397,
      "grad_norm": 5.246237277984619,
      "learning_rate": 3.202255992480025e-05,
      "loss": 1.6598,
      "step": 275400
    },
    {
      "epoch": 21.58076139746201,
      "grad_norm": 6.934438705444336,
      "learning_rate": 3.201603216878166e-05,
      "loss": 1.7297,
      "step": 275500
    },
    {
      "epoch": 21.58859470468432,
      "grad_norm": 6.25242280960083,
      "learning_rate": 3.200950441276307e-05,
      "loss": 1.7513,
      "step": 275600
    },
    {
      "epoch": 21.596428011906628,
      "grad_norm": 4.867473125457764,
      "learning_rate": 3.2002976656744474e-05,
      "loss": 1.7999,
      "step": 275700
    },
    {
      "epoch": 21.604261319128938,
      "grad_norm": 6.001058101654053,
      "learning_rate": 3.199644890072589e-05,
      "loss": 1.7633,
      "step": 275800
    },
    {
      "epoch": 21.612094626351244,
      "grad_norm": 7.566278457641602,
      "learning_rate": 3.19899211447073e-05,
      "loss": 1.7228,
      "step": 275900
    },
    {
      "epoch": 21.619927933573553,
      "grad_norm": 4.611124038696289,
      "learning_rate": 3.1983393388688705e-05,
      "loss": 1.6889,
      "step": 276000
    },
    {
      "epoch": 21.627761240795863,
      "grad_norm": 4.380817413330078,
      "learning_rate": 3.197686563267012e-05,
      "loss": 1.7101,
      "step": 276100
    },
    {
      "epoch": 21.635594548018172,
      "grad_norm": 5.543848991394043,
      "learning_rate": 3.1970337876651523e-05,
      "loss": 1.7588,
      "step": 276200
    },
    {
      "epoch": 21.643427855240482,
      "grad_norm": 6.8985209465026855,
      "learning_rate": 3.196381012063293e-05,
      "loss": 1.5822,
      "step": 276300
    },
    {
      "epoch": 21.65126116246279,
      "grad_norm": 8.37416934967041,
      "learning_rate": 3.195728236461434e-05,
      "loss": 1.7964,
      "step": 276400
    },
    {
      "epoch": 21.6590944696851,
      "grad_norm": 6.8648481369018555,
      "learning_rate": 3.1950754608595754e-05,
      "loss": 1.7364,
      "step": 276500
    },
    {
      "epoch": 21.66692777690741,
      "grad_norm": 5.927731513977051,
      "learning_rate": 3.194422685257716e-05,
      "loss": 1.7332,
      "step": 276600
    },
    {
      "epoch": 21.67476108412972,
      "grad_norm": 6.7824578285217285,
      "learning_rate": 3.193769909655857e-05,
      "loss": 1.7796,
      "step": 276700
    },
    {
      "epoch": 21.68259439135203,
      "grad_norm": 7.485147953033447,
      "learning_rate": 3.193117134053998e-05,
      "loss": 1.7839,
      "step": 276800
    },
    {
      "epoch": 21.69042769857434,
      "grad_norm": 6.2641520500183105,
      "learning_rate": 3.1924643584521384e-05,
      "loss": 1.7666,
      "step": 276900
    },
    {
      "epoch": 21.69826100579665,
      "grad_norm": 5.6529316902160645,
      "learning_rate": 3.19181158285028e-05,
      "loss": 1.719,
      "step": 277000
    },
    {
      "epoch": 21.706094313018955,
      "grad_norm": 5.246757507324219,
      "learning_rate": 3.19115880724842e-05,
      "loss": 1.694,
      "step": 277100
    },
    {
      "epoch": 21.713927620241265,
      "grad_norm": 6.405909538269043,
      "learning_rate": 3.1905060316465615e-05,
      "loss": 1.6736,
      "step": 277200
    },
    {
      "epoch": 21.721760927463574,
      "grad_norm": 4.552432537078857,
      "learning_rate": 3.189853256044702e-05,
      "loss": 1.6958,
      "step": 277300
    },
    {
      "epoch": 21.729594234685884,
      "grad_norm": 6.318755626678467,
      "learning_rate": 3.189200480442843e-05,
      "loss": 1.8134,
      "step": 277400
    },
    {
      "epoch": 21.737427541908193,
      "grad_norm": 5.004558563232422,
      "learning_rate": 3.188547704840984e-05,
      "loss": 1.7466,
      "step": 277500
    },
    {
      "epoch": 21.745260849130503,
      "grad_norm": 7.87637996673584,
      "learning_rate": 3.1878949292391245e-05,
      "loss": 1.698,
      "step": 277600
    },
    {
      "epoch": 21.753094156352812,
      "grad_norm": 7.0898966789245605,
      "learning_rate": 3.187242153637266e-05,
      "loss": 1.7401,
      "step": 277700
    },
    {
      "epoch": 21.760927463575122,
      "grad_norm": 5.057464122772217,
      "learning_rate": 3.186589378035407e-05,
      "loss": 1.7301,
      "step": 277800
    },
    {
      "epoch": 21.76876077079743,
      "grad_norm": 5.241635322570801,
      "learning_rate": 3.1859366024335476e-05,
      "loss": 1.7436,
      "step": 277900
    },
    {
      "epoch": 21.77659407801974,
      "grad_norm": 4.857478141784668,
      "learning_rate": 3.185283826831689e-05,
      "loss": 1.7062,
      "step": 278000
    },
    {
      "epoch": 21.78442738524205,
      "grad_norm": 8.543992042541504,
      "learning_rate": 3.1846310512298294e-05,
      "loss": 1.7632,
      "step": 278100
    },
    {
      "epoch": 21.79226069246436,
      "grad_norm": 5.546699047088623,
      "learning_rate": 3.18397827562797e-05,
      "loss": 1.7128,
      "step": 278200
    },
    {
      "epoch": 21.800093999686666,
      "grad_norm": 7.539425373077393,
      "learning_rate": 3.183325500026111e-05,
      "loss": 1.8382,
      "step": 278300
    },
    {
      "epoch": 21.807927306908976,
      "grad_norm": 6.150217056274414,
      "learning_rate": 3.182672724424252e-05,
      "loss": 1.6909,
      "step": 278400
    },
    {
      "epoch": 21.815760614131285,
      "grad_norm": 6.9527177810668945,
      "learning_rate": 3.182019948822393e-05,
      "loss": 1.7458,
      "step": 278500
    },
    {
      "epoch": 21.823593921353595,
      "grad_norm": 6.601832866668701,
      "learning_rate": 3.181367173220534e-05,
      "loss": 1.8011,
      "step": 278600
    },
    {
      "epoch": 21.831427228575905,
      "grad_norm": 5.6913957595825195,
      "learning_rate": 3.180714397618675e-05,
      "loss": 1.6718,
      "step": 278700
    },
    {
      "epoch": 21.839260535798214,
      "grad_norm": 5.397503852844238,
      "learning_rate": 3.1800616220168155e-05,
      "loss": 1.637,
      "step": 278800
    },
    {
      "epoch": 21.847093843020524,
      "grad_norm": 5.414619445800781,
      "learning_rate": 3.179408846414956e-05,
      "loss": 1.7214,
      "step": 278900
    },
    {
      "epoch": 21.854927150242833,
      "grad_norm": 6.157627105712891,
      "learning_rate": 3.178756070813097e-05,
      "loss": 1.7696,
      "step": 279000
    },
    {
      "epoch": 21.862760457465143,
      "grad_norm": 5.742382049560547,
      "learning_rate": 3.1781032952112386e-05,
      "loss": 1.8007,
      "step": 279100
    },
    {
      "epoch": 21.870593764687452,
      "grad_norm": 6.223041534423828,
      "learning_rate": 3.177450519609379e-05,
      "loss": 1.7485,
      "step": 279200
    },
    {
      "epoch": 21.878427071909762,
      "grad_norm": 5.468425750732422,
      "learning_rate": 3.1767977440075204e-05,
      "loss": 1.6858,
      "step": 279300
    },
    {
      "epoch": 21.886260379132068,
      "grad_norm": 4.920286655426025,
      "learning_rate": 3.176144968405661e-05,
      "loss": 1.7208,
      "step": 279400
    },
    {
      "epoch": 21.894093686354378,
      "grad_norm": 7.812067031860352,
      "learning_rate": 3.1754921928038015e-05,
      "loss": 1.7207,
      "step": 279500
    },
    {
      "epoch": 21.901926993576687,
      "grad_norm": 6.444197177886963,
      "learning_rate": 3.174839417201943e-05,
      "loss": 1.698,
      "step": 279600
    },
    {
      "epoch": 21.909760300798997,
      "grad_norm": 6.271984577178955,
      "learning_rate": 3.174186641600084e-05,
      "loss": 1.7164,
      "step": 279700
    },
    {
      "epoch": 21.917593608021306,
      "grad_norm": 6.6854095458984375,
      "learning_rate": 3.1735338659982246e-05,
      "loss": 1.6857,
      "step": 279800
    },
    {
      "epoch": 21.925426915243616,
      "grad_norm": 7.062386989593506,
      "learning_rate": 3.172881090396366e-05,
      "loss": 1.7468,
      "step": 279900
    },
    {
      "epoch": 21.933260222465925,
      "grad_norm": 8.704707145690918,
      "learning_rate": 3.1722283147945065e-05,
      "loss": 1.8028,
      "step": 280000
    },
    {
      "epoch": 21.941093529688235,
      "grad_norm": 4.7586894035339355,
      "learning_rate": 3.171575539192647e-05,
      "loss": 1.7066,
      "step": 280100
    },
    {
      "epoch": 21.948926836910545,
      "grad_norm": 7.534712314605713,
      "learning_rate": 3.170922763590788e-05,
      "loss": 1.7364,
      "step": 280200
    },
    {
      "epoch": 21.956760144132854,
      "grad_norm": 6.104002475738525,
      "learning_rate": 3.170269987988929e-05,
      "loss": 1.7201,
      "step": 280300
    },
    {
      "epoch": 21.964593451355164,
      "grad_norm": 6.845401287078857,
      "learning_rate": 3.16961721238707e-05,
      "loss": 1.6996,
      "step": 280400
    },
    {
      "epoch": 21.97242675857747,
      "grad_norm": 5.309467792510986,
      "learning_rate": 3.168964436785211e-05,
      "loss": 1.7035,
      "step": 280500
    },
    {
      "epoch": 21.98026006579978,
      "grad_norm": 6.005596160888672,
      "learning_rate": 3.168311661183352e-05,
      "loss": 1.6538,
      "step": 280600
    },
    {
      "epoch": 21.98809337302209,
      "grad_norm": 8.262129783630371,
      "learning_rate": 3.1676588855814925e-05,
      "loss": 1.7289,
      "step": 280700
    },
    {
      "epoch": 21.9959266802444,
      "grad_norm": 7.653488636016846,
      "learning_rate": 3.167006109979633e-05,
      "loss": 1.7204,
      "step": 280800
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.796416163444519,
      "eval_runtime": 2.9285,
      "eval_samples_per_second": 229.467,
      "eval_steps_per_second": 229.467,
      "step": 280852
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.486993670463562,
      "eval_runtime": 54.797,
      "eval_samples_per_second": 232.969,
      "eval_steps_per_second": 232.969,
      "step": 280852
    },
    {
      "epoch": 22.003759987466708,
      "grad_norm": 5.075614929199219,
      "learning_rate": 3.1663533343777744e-05,
      "loss": 1.6371,
      "step": 280900
    },
    {
      "epoch": 22.011593294689018,
      "grad_norm": 6.428805828094482,
      "learning_rate": 3.1657005587759156e-05,
      "loss": 1.6535,
      "step": 281000
    },
    {
      "epoch": 22.019426601911327,
      "grad_norm": 5.497672080993652,
      "learning_rate": 3.165047783174056e-05,
      "loss": 1.6386,
      "step": 281100
    },
    {
      "epoch": 22.027259909133637,
      "grad_norm": 7.50076961517334,
      "learning_rate": 3.1643950075721974e-05,
      "loss": 1.7025,
      "step": 281200
    },
    {
      "epoch": 22.035093216355946,
      "grad_norm": 7.886189937591553,
      "learning_rate": 3.163742231970338e-05,
      "loss": 1.7216,
      "step": 281300
    },
    {
      "epoch": 22.042926523578256,
      "grad_norm": 7.181524753570557,
      "learning_rate": 3.1630894563684786e-05,
      "loss": 1.7748,
      "step": 281400
    },
    {
      "epoch": 22.050759830800565,
      "grad_norm": 7.367747783660889,
      "learning_rate": 3.16243668076662e-05,
      "loss": 1.7563,
      "step": 281500
    },
    {
      "epoch": 22.058593138022875,
      "grad_norm": 5.3754563331604,
      "learning_rate": 3.1617839051647604e-05,
      "loss": 1.6841,
      "step": 281600
    },
    {
      "epoch": 22.06642644524518,
      "grad_norm": 7.173090934753418,
      "learning_rate": 3.161131129562902e-05,
      "loss": 1.6753,
      "step": 281700
    },
    {
      "epoch": 22.07425975246749,
      "grad_norm": 6.875965595245361,
      "learning_rate": 3.160478353961043e-05,
      "loss": 1.6927,
      "step": 281800
    },
    {
      "epoch": 22.0820930596898,
      "grad_norm": 4.310874938964844,
      "learning_rate": 3.1598255783591835e-05,
      "loss": 1.639,
      "step": 281900
    },
    {
      "epoch": 22.08992636691211,
      "grad_norm": 5.45348596572876,
      "learning_rate": 3.159172802757324e-05,
      "loss": 1.7678,
      "step": 282000
    },
    {
      "epoch": 22.09775967413442,
      "grad_norm": 4.8222737312316895,
      "learning_rate": 3.158520027155465e-05,
      "loss": 1.6863,
      "step": 282100
    },
    {
      "epoch": 22.10559298135673,
      "grad_norm": 5.408777713775635,
      "learning_rate": 3.157867251553606e-05,
      "loss": 1.6512,
      "step": 282200
    },
    {
      "epoch": 22.11342628857904,
      "grad_norm": 7.449512004852295,
      "learning_rate": 3.157214475951747e-05,
      "loss": 1.754,
      "step": 282300
    },
    {
      "epoch": 22.121259595801348,
      "grad_norm": 5.452033996582031,
      "learning_rate": 3.156561700349888e-05,
      "loss": 1.7647,
      "step": 282400
    },
    {
      "epoch": 22.129092903023658,
      "grad_norm": 5.526336193084717,
      "learning_rate": 3.155908924748029e-05,
      "loss": 1.7215,
      "step": 282500
    },
    {
      "epoch": 22.136926210245967,
      "grad_norm": 5.946174144744873,
      "learning_rate": 3.1552561491461696e-05,
      "loss": 1.6454,
      "step": 282600
    },
    {
      "epoch": 22.144759517468277,
      "grad_norm": 6.389965534210205,
      "learning_rate": 3.15460337354431e-05,
      "loss": 1.6979,
      "step": 282700
    },
    {
      "epoch": 22.152592824690583,
      "grad_norm": 6.24180269241333,
      "learning_rate": 3.1539505979424514e-05,
      "loss": 1.7907,
      "step": 282800
    },
    {
      "epoch": 22.160426131912892,
      "grad_norm": 6.925060749053955,
      "learning_rate": 3.153297822340593e-05,
      "loss": 1.6588,
      "step": 282900
    },
    {
      "epoch": 22.168259439135202,
      "grad_norm": 6.473648548126221,
      "learning_rate": 3.152645046738733e-05,
      "loss": 1.7742,
      "step": 283000
    },
    {
      "epoch": 22.17609274635751,
      "grad_norm": 4.914220809936523,
      "learning_rate": 3.1519922711368745e-05,
      "loss": 1.7868,
      "step": 283100
    },
    {
      "epoch": 22.18392605357982,
      "grad_norm": 5.988654136657715,
      "learning_rate": 3.151339495535015e-05,
      "loss": 1.842,
      "step": 283200
    },
    {
      "epoch": 22.19175936080213,
      "grad_norm": 6.695800304412842,
      "learning_rate": 3.1506867199331557e-05,
      "loss": 1.7349,
      "step": 283300
    },
    {
      "epoch": 22.19959266802444,
      "grad_norm": 5.193150043487549,
      "learning_rate": 3.150033944331297e-05,
      "loss": 1.7498,
      "step": 283400
    },
    {
      "epoch": 22.20742597524675,
      "grad_norm": 5.363056182861328,
      "learning_rate": 3.1493811687294375e-05,
      "loss": 1.7167,
      "step": 283500
    },
    {
      "epoch": 22.21525928246906,
      "grad_norm": 5.315351486206055,
      "learning_rate": 3.148728393127579e-05,
      "loss": 1.6818,
      "step": 283600
    },
    {
      "epoch": 22.22309258969137,
      "grad_norm": 7.252847194671631,
      "learning_rate": 3.14807561752572e-05,
      "loss": 1.6178,
      "step": 283700
    },
    {
      "epoch": 22.23092589691368,
      "grad_norm": 7.038151264190674,
      "learning_rate": 3.1474228419238606e-05,
      "loss": 1.6283,
      "step": 283800
    },
    {
      "epoch": 22.238759204135985,
      "grad_norm": 8.448234558105469,
      "learning_rate": 3.146770066322001e-05,
      "loss": 1.7512,
      "step": 283900
    },
    {
      "epoch": 22.246592511358294,
      "grad_norm": 5.312158584594727,
      "learning_rate": 3.146117290720142e-05,
      "loss": 1.6823,
      "step": 284000
    },
    {
      "epoch": 22.254425818580604,
      "grad_norm": 5.699090480804443,
      "learning_rate": 3.145464515118283e-05,
      "loss": 1.7148,
      "step": 284100
    },
    {
      "epoch": 22.262259125802913,
      "grad_norm": 6.098268508911133,
      "learning_rate": 3.144811739516424e-05,
      "loss": 1.6935,
      "step": 284200
    },
    {
      "epoch": 22.270092433025223,
      "grad_norm": 6.94838809967041,
      "learning_rate": 3.144158963914565e-05,
      "loss": 1.7404,
      "step": 284300
    },
    {
      "epoch": 22.277925740247532,
      "grad_norm": 7.38486909866333,
      "learning_rate": 3.143506188312706e-05,
      "loss": 1.7224,
      "step": 284400
    },
    {
      "epoch": 22.285759047469842,
      "grad_norm": 6.186670303344727,
      "learning_rate": 3.1428534127108466e-05,
      "loss": 1.7161,
      "step": 284500
    },
    {
      "epoch": 22.29359235469215,
      "grad_norm": 5.2508544921875,
      "learning_rate": 3.142200637108987e-05,
      "loss": 1.7497,
      "step": 284600
    },
    {
      "epoch": 22.30142566191446,
      "grad_norm": 6.4052510261535645,
      "learning_rate": 3.1415478615071285e-05,
      "loss": 1.6111,
      "step": 284700
    },
    {
      "epoch": 22.30925896913677,
      "grad_norm": 6.609836101531982,
      "learning_rate": 3.140895085905269e-05,
      "loss": 1.7287,
      "step": 284800
    },
    {
      "epoch": 22.31709227635908,
      "grad_norm": 6.094588756561279,
      "learning_rate": 3.14024231030341e-05,
      "loss": 1.7437,
      "step": 284900
    },
    {
      "epoch": 22.32492558358139,
      "grad_norm": 8.545272827148438,
      "learning_rate": 3.1395895347015516e-05,
      "loss": 1.7771,
      "step": 285000
    },
    {
      "epoch": 22.332758890803696,
      "grad_norm": 6.133181095123291,
      "learning_rate": 3.138936759099692e-05,
      "loss": 1.7167,
      "step": 285100
    },
    {
      "epoch": 22.340592198026005,
      "grad_norm": 6.263281345367432,
      "learning_rate": 3.138283983497833e-05,
      "loss": 1.7256,
      "step": 285200
    },
    {
      "epoch": 22.348425505248315,
      "grad_norm": 5.8871235847473145,
      "learning_rate": 3.137631207895974e-05,
      "loss": 1.6943,
      "step": 285300
    },
    {
      "epoch": 22.356258812470625,
      "grad_norm": 4.7085065841674805,
      "learning_rate": 3.1369784322941146e-05,
      "loss": 1.7675,
      "step": 285400
    },
    {
      "epoch": 22.364092119692934,
      "grad_norm": 6.012338638305664,
      "learning_rate": 3.136325656692256e-05,
      "loss": 1.7464,
      "step": 285500
    },
    {
      "epoch": 22.371925426915244,
      "grad_norm": 7.816368103027344,
      "learning_rate": 3.1356728810903964e-05,
      "loss": 1.7656,
      "step": 285600
    },
    {
      "epoch": 22.379758734137553,
      "grad_norm": 6.159769058227539,
      "learning_rate": 3.1350201054885376e-05,
      "loss": 1.6964,
      "step": 285700
    },
    {
      "epoch": 22.387592041359863,
      "grad_norm": 6.956391334533691,
      "learning_rate": 3.134367329886678e-05,
      "loss": 1.7897,
      "step": 285800
    },
    {
      "epoch": 22.395425348582172,
      "grad_norm": 5.527471542358398,
      "learning_rate": 3.133714554284819e-05,
      "loss": 1.6811,
      "step": 285900
    },
    {
      "epoch": 22.403258655804482,
      "grad_norm": 5.23825216293335,
      "learning_rate": 3.13306177868296e-05,
      "loss": 1.7006,
      "step": 286000
    },
    {
      "epoch": 22.41109196302679,
      "grad_norm": 5.707094192504883,
      "learning_rate": 3.132409003081101e-05,
      "loss": 1.7727,
      "step": 286100
    },
    {
      "epoch": 22.418925270249098,
      "grad_norm": 5.532491207122803,
      "learning_rate": 3.131756227479242e-05,
      "loss": 1.7564,
      "step": 286200
    },
    {
      "epoch": 22.426758577471407,
      "grad_norm": 5.032701015472412,
      "learning_rate": 3.131103451877383e-05,
      "loss": 1.7689,
      "step": 286300
    },
    {
      "epoch": 22.434591884693717,
      "grad_norm": 7.3272504806518555,
      "learning_rate": 3.130450676275524e-05,
      "loss": 1.649,
      "step": 286400
    },
    {
      "epoch": 22.442425191916026,
      "grad_norm": 6.005194187164307,
      "learning_rate": 3.129797900673664e-05,
      "loss": 1.7586,
      "step": 286500
    },
    {
      "epoch": 22.450258499138336,
      "grad_norm": 5.635223865509033,
      "learning_rate": 3.1291451250718055e-05,
      "loss": 1.743,
      "step": 286600
    },
    {
      "epoch": 22.458091806360645,
      "grad_norm": 6.150772571563721,
      "learning_rate": 3.128492349469946e-05,
      "loss": 1.7788,
      "step": 286700
    },
    {
      "epoch": 22.465925113582955,
      "grad_norm": 5.210879802703857,
      "learning_rate": 3.1278395738680874e-05,
      "loss": 1.6088,
      "step": 286800
    },
    {
      "epoch": 22.473758420805265,
      "grad_norm": 7.301809787750244,
      "learning_rate": 3.1271867982662286e-05,
      "loss": 1.7401,
      "step": 286900
    },
    {
      "epoch": 22.481591728027574,
      "grad_norm": 7.43552827835083,
      "learning_rate": 3.126534022664369e-05,
      "loss": 1.7725,
      "step": 287000
    },
    {
      "epoch": 22.489425035249884,
      "grad_norm": 6.345401763916016,
      "learning_rate": 3.12588124706251e-05,
      "loss": 1.7348,
      "step": 287100
    },
    {
      "epoch": 22.497258342472193,
      "grad_norm": 5.19627046585083,
      "learning_rate": 3.1252284714606504e-05,
      "loss": 1.6523,
      "step": 287200
    },
    {
      "epoch": 22.5050916496945,
      "grad_norm": 5.832611560821533,
      "learning_rate": 3.1245756958587916e-05,
      "loss": 1.7686,
      "step": 287300
    },
    {
      "epoch": 22.51292495691681,
      "grad_norm": 6.735921382904053,
      "learning_rate": 3.123922920256933e-05,
      "loss": 1.6811,
      "step": 287400
    },
    {
      "epoch": 22.52075826413912,
      "grad_norm": 5.223733901977539,
      "learning_rate": 3.1232701446550734e-05,
      "loss": 1.6284,
      "step": 287500
    },
    {
      "epoch": 22.528591571361428,
      "grad_norm": 5.274346828460693,
      "learning_rate": 3.122617369053215e-05,
      "loss": 1.6814,
      "step": 287600
    },
    {
      "epoch": 22.536424878583738,
      "grad_norm": 6.547363758087158,
      "learning_rate": 3.121964593451355e-05,
      "loss": 1.7338,
      "step": 287700
    },
    {
      "epoch": 22.544258185806047,
      "grad_norm": 3.3927981853485107,
      "learning_rate": 3.121311817849496e-05,
      "loss": 1.6307,
      "step": 287800
    },
    {
      "epoch": 22.552091493028357,
      "grad_norm": 5.443146705627441,
      "learning_rate": 3.120659042247637e-05,
      "loss": 1.7728,
      "step": 287900
    },
    {
      "epoch": 22.559924800250666,
      "grad_norm": 7.723045825958252,
      "learning_rate": 3.120006266645778e-05,
      "loss": 1.7113,
      "step": 288000
    },
    {
      "epoch": 22.567758107472976,
      "grad_norm": 5.968832492828369,
      "learning_rate": 3.119353491043919e-05,
      "loss": 1.7847,
      "step": 288100
    },
    {
      "epoch": 22.575591414695285,
      "grad_norm": 6.037859916687012,
      "learning_rate": 3.11870071544206e-05,
      "loss": 1.7192,
      "step": 288200
    },
    {
      "epoch": 22.583424721917595,
      "grad_norm": 5.251055717468262,
      "learning_rate": 3.118047939840201e-05,
      "loss": 1.6783,
      "step": 288300
    },
    {
      "epoch": 22.591258029139905,
      "grad_norm": 3.788506031036377,
      "learning_rate": 3.1173951642383413e-05,
      "loss": 1.8148,
      "step": 288400
    },
    {
      "epoch": 22.59909133636221,
      "grad_norm": 5.511443138122559,
      "learning_rate": 3.1167423886364826e-05,
      "loss": 1.7983,
      "step": 288500
    },
    {
      "epoch": 22.60692464358452,
      "grad_norm": 7.606346130371094,
      "learning_rate": 3.116089613034623e-05,
      "loss": 1.7105,
      "step": 288600
    },
    {
      "epoch": 22.61475795080683,
      "grad_norm": 6.5704026222229,
      "learning_rate": 3.1154368374327644e-05,
      "loss": 1.7561,
      "step": 288700
    },
    {
      "epoch": 22.62259125802914,
      "grad_norm": 6.232907295227051,
      "learning_rate": 3.114784061830906e-05,
      "loss": 1.8183,
      "step": 288800
    },
    {
      "epoch": 22.63042456525145,
      "grad_norm": 6.290155410766602,
      "learning_rate": 3.114131286229046e-05,
      "loss": 1.7817,
      "step": 288900
    },
    {
      "epoch": 22.63825787247376,
      "grad_norm": 8.385519981384277,
      "learning_rate": 3.113478510627187e-05,
      "loss": 1.7365,
      "step": 289000
    },
    {
      "epoch": 22.646091179696068,
      "grad_norm": 5.43294095993042,
      "learning_rate": 3.1128257350253274e-05,
      "loss": 1.739,
      "step": 289100
    },
    {
      "epoch": 22.653924486918378,
      "grad_norm": 6.1588568687438965,
      "learning_rate": 3.112172959423469e-05,
      "loss": 1.7077,
      "step": 289200
    },
    {
      "epoch": 22.661757794140687,
      "grad_norm": 5.275543212890625,
      "learning_rate": 3.11152018382161e-05,
      "loss": 1.7666,
      "step": 289300
    },
    {
      "epoch": 22.669591101362997,
      "grad_norm": 5.1952223777771,
      "learning_rate": 3.1108674082197505e-05,
      "loss": 1.7084,
      "step": 289400
    },
    {
      "epoch": 22.677424408585306,
      "grad_norm": 5.880126953125,
      "learning_rate": 3.110214632617892e-05,
      "loss": 1.6934,
      "step": 289500
    },
    {
      "epoch": 22.685257715807612,
      "grad_norm": 5.689599514007568,
      "learning_rate": 3.109561857016032e-05,
      "loss": 1.7055,
      "step": 289600
    },
    {
      "epoch": 22.693091023029922,
      "grad_norm": 7.1873555183410645,
      "learning_rate": 3.108909081414173e-05,
      "loss": 1.7625,
      "step": 289700
    },
    {
      "epoch": 22.70092433025223,
      "grad_norm": 6.6948089599609375,
      "learning_rate": 3.108256305812314e-05,
      "loss": 1.6645,
      "step": 289800
    },
    {
      "epoch": 22.70875763747454,
      "grad_norm": 6.384026050567627,
      "learning_rate": 3.107603530210455e-05,
      "loss": 1.7053,
      "step": 289900
    },
    {
      "epoch": 22.71659094469685,
      "grad_norm": 4.824746608734131,
      "learning_rate": 3.106950754608596e-05,
      "loss": 1.6598,
      "step": 290000
    },
    {
      "epoch": 22.72442425191916,
      "grad_norm": 5.716211795806885,
      "learning_rate": 3.106297979006737e-05,
      "loss": 1.7053,
      "step": 290100
    },
    {
      "epoch": 22.73225755914147,
      "grad_norm": 5.3874053955078125,
      "learning_rate": 3.105645203404878e-05,
      "loss": 1.6821,
      "step": 290200
    },
    {
      "epoch": 22.74009086636378,
      "grad_norm": 7.793765068054199,
      "learning_rate": 3.1049924278030184e-05,
      "loss": 1.7944,
      "step": 290300
    },
    {
      "epoch": 22.74792417358609,
      "grad_norm": 5.05717134475708,
      "learning_rate": 3.1043396522011597e-05,
      "loss": 1.7735,
      "step": 290400
    },
    {
      "epoch": 22.7557574808084,
      "grad_norm": 5.1755547523498535,
      "learning_rate": 3.1036868765993e-05,
      "loss": 1.7866,
      "step": 290500
    },
    {
      "epoch": 22.763590788030708,
      "grad_norm": 4.910680770874023,
      "learning_rate": 3.1030341009974415e-05,
      "loss": 1.7448,
      "step": 290600
    },
    {
      "epoch": 22.771424095253018,
      "grad_norm": 12.293148040771484,
      "learning_rate": 3.102381325395582e-05,
      "loss": 1.7106,
      "step": 290700
    },
    {
      "epoch": 22.779257402475324,
      "grad_norm": 6.824542045593262,
      "learning_rate": 3.101728549793723e-05,
      "loss": 1.733,
      "step": 290800
    },
    {
      "epoch": 22.787090709697633,
      "grad_norm": 5.4277238845825195,
      "learning_rate": 3.101075774191864e-05,
      "loss": 1.6681,
      "step": 290900
    },
    {
      "epoch": 22.794924016919943,
      "grad_norm": 6.8777384757995605,
      "learning_rate": 3.1004229985900045e-05,
      "loss": 1.7378,
      "step": 291000
    },
    {
      "epoch": 22.802757324142252,
      "grad_norm": 6.76235294342041,
      "learning_rate": 3.099770222988146e-05,
      "loss": 1.7198,
      "step": 291100
    },
    {
      "epoch": 22.810590631364562,
      "grad_norm": 6.2257890701293945,
      "learning_rate": 3.099117447386286e-05,
      "loss": 1.7234,
      "step": 291200
    },
    {
      "epoch": 22.81842393858687,
      "grad_norm": 7.096609115600586,
      "learning_rate": 3.0984646717844276e-05,
      "loss": 1.7065,
      "step": 291300
    },
    {
      "epoch": 22.82625724580918,
      "grad_norm": 6.190832138061523,
      "learning_rate": 3.097811896182569e-05,
      "loss": 1.7175,
      "step": 291400
    },
    {
      "epoch": 22.83409055303149,
      "grad_norm": 4.951663970947266,
      "learning_rate": 3.0971591205807094e-05,
      "loss": 1.607,
      "step": 291500
    },
    {
      "epoch": 22.8419238602538,
      "grad_norm": 6.779915809631348,
      "learning_rate": 3.09650634497885e-05,
      "loss": 1.7003,
      "step": 291600
    },
    {
      "epoch": 22.84975716747611,
      "grad_norm": 7.810608386993408,
      "learning_rate": 3.095853569376991e-05,
      "loss": 1.7238,
      "step": 291700
    },
    {
      "epoch": 22.85759047469842,
      "grad_norm": 5.517414569854736,
      "learning_rate": 3.095200793775132e-05,
      "loss": 1.7382,
      "step": 291800
    },
    {
      "epoch": 22.865423781920725,
      "grad_norm": 6.447437286376953,
      "learning_rate": 3.094548018173273e-05,
      "loss": 1.6786,
      "step": 291900
    },
    {
      "epoch": 22.873257089143035,
      "grad_norm": 4.786532402038574,
      "learning_rate": 3.093895242571414e-05,
      "loss": 1.7117,
      "step": 292000
    },
    {
      "epoch": 22.881090396365344,
      "grad_norm": 5.4620232582092285,
      "learning_rate": 3.093242466969555e-05,
      "loss": 1.7295,
      "step": 292100
    },
    {
      "epoch": 22.888923703587654,
      "grad_norm": 5.7434611320495605,
      "learning_rate": 3.0925896913676955e-05,
      "loss": 1.6481,
      "step": 292200
    },
    {
      "epoch": 22.896757010809964,
      "grad_norm": 4.982200622558594,
      "learning_rate": 3.091936915765836e-05,
      "loss": 1.8004,
      "step": 292300
    },
    {
      "epoch": 22.904590318032273,
      "grad_norm": 7.218204975128174,
      "learning_rate": 3.091284140163977e-05,
      "loss": 1.7166,
      "step": 292400
    },
    {
      "epoch": 22.912423625254583,
      "grad_norm": 6.1519060134887695,
      "learning_rate": 3.0906313645621185e-05,
      "loss": 1.768,
      "step": 292500
    },
    {
      "epoch": 22.920256932476892,
      "grad_norm": 7.460228443145752,
      "learning_rate": 3.089978588960259e-05,
      "loss": 1.7629,
      "step": 292600
    },
    {
      "epoch": 22.928090239699202,
      "grad_norm": 5.50368070602417,
      "learning_rate": 3.0893258133584004e-05,
      "loss": 1.6376,
      "step": 292700
    },
    {
      "epoch": 22.93592354692151,
      "grad_norm": 6.131769180297852,
      "learning_rate": 3.088673037756541e-05,
      "loss": 1.7354,
      "step": 292800
    },
    {
      "epoch": 22.94375685414382,
      "grad_norm": 5.207498073577881,
      "learning_rate": 3.0880202621546815e-05,
      "loss": 1.7149,
      "step": 292900
    },
    {
      "epoch": 22.951590161366127,
      "grad_norm": 5.570493221282959,
      "learning_rate": 3.087367486552823e-05,
      "loss": 1.7806,
      "step": 293000
    },
    {
      "epoch": 22.959423468588437,
      "grad_norm": 5.963842391967773,
      "learning_rate": 3.0867147109509634e-05,
      "loss": 1.7245,
      "step": 293100
    },
    {
      "epoch": 22.967256775810746,
      "grad_norm": 6.761998653411865,
      "learning_rate": 3.0860619353491046e-05,
      "loss": 1.6439,
      "step": 293200
    },
    {
      "epoch": 22.975090083033056,
      "grad_norm": 6.8936238288879395,
      "learning_rate": 3.085409159747246e-05,
      "loss": 1.7363,
      "step": 293300
    },
    {
      "epoch": 22.982923390255365,
      "grad_norm": 7.917041778564453,
      "learning_rate": 3.0847563841453864e-05,
      "loss": 1.7529,
      "step": 293400
    },
    {
      "epoch": 22.990756697477675,
      "grad_norm": 5.638868808746338,
      "learning_rate": 3.084103608543527e-05,
      "loss": 1.7109,
      "step": 293500
    },
    {
      "epoch": 22.998590004699984,
      "grad_norm": 6.24783182144165,
      "learning_rate": 3.083450832941668e-05,
      "loss": 1.7706,
      "step": 293600
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.7908343076705933,
      "eval_runtime": 2.9673,
      "eval_samples_per_second": 226.466,
      "eval_steps_per_second": 226.466,
      "step": 293618
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.4797625541687012,
      "eval_runtime": 54.8258,
      "eval_samples_per_second": 232.846,
      "eval_steps_per_second": 232.846,
      "step": 293618
    },
    {
      "epoch": 23.006423311922294,
      "grad_norm": 6.203405380249023,
      "learning_rate": 3.082798057339809e-05,
      "loss": 1.6727,
      "step": 293700
    },
    {
      "epoch": 23.014256619144604,
      "grad_norm": 4.735561370849609,
      "learning_rate": 3.08214528173795e-05,
      "loss": 1.6284,
      "step": 293800
    },
    {
      "epoch": 23.022089926366913,
      "grad_norm": 6.761630535125732,
      "learning_rate": 3.081492506136091e-05,
      "loss": 1.7265,
      "step": 293900
    },
    {
      "epoch": 23.029923233589223,
      "grad_norm": 7.825871467590332,
      "learning_rate": 3.080839730534232e-05,
      "loss": 1.747,
      "step": 294000
    },
    {
      "epoch": 23.037756540811532,
      "grad_norm": 7.924779415130615,
      "learning_rate": 3.0801869549323725e-05,
      "loss": 1.7012,
      "step": 294100
    },
    {
      "epoch": 23.04558984803384,
      "grad_norm": 5.302205562591553,
      "learning_rate": 3.079534179330513e-05,
      "loss": 1.719,
      "step": 294200
    },
    {
      "epoch": 23.053423155256148,
      "grad_norm": 5.0270891189575195,
      "learning_rate": 3.0788814037286543e-05,
      "loss": 1.6339,
      "step": 294300
    },
    {
      "epoch": 23.061256462478458,
      "grad_norm": 4.481873989105225,
      "learning_rate": 3.078228628126795e-05,
      "loss": 1.72,
      "step": 294400
    },
    {
      "epoch": 23.069089769700767,
      "grad_norm": 6.873178958892822,
      "learning_rate": 3.077575852524936e-05,
      "loss": 1.6457,
      "step": 294500
    },
    {
      "epoch": 23.076923076923077,
      "grad_norm": 8.194828987121582,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 1.7882,
      "step": 294600
    },
    {
      "epoch": 23.084756384145386,
      "grad_norm": 5.603765487670898,
      "learning_rate": 3.076270301321218e-05,
      "loss": 1.7313,
      "step": 294700
    },
    {
      "epoch": 23.092589691367696,
      "grad_norm": 4.840909481048584,
      "learning_rate": 3.0756175257193586e-05,
      "loss": 1.7158,
      "step": 294800
    },
    {
      "epoch": 23.100422998590005,
      "grad_norm": 6.605605125427246,
      "learning_rate": 3.0749647501175e-05,
      "loss": 1.654,
      "step": 294900
    },
    {
      "epoch": 23.108256305812315,
      "grad_norm": 7.837543487548828,
      "learning_rate": 3.0743119745156404e-05,
      "loss": 1.674,
      "step": 295000
    },
    {
      "epoch": 23.116089613034625,
      "grad_norm": 7.1040802001953125,
      "learning_rate": 3.073659198913782e-05,
      "loss": 1.6769,
      "step": 295100
    },
    {
      "epoch": 23.123922920256934,
      "grad_norm": 5.887062072753906,
      "learning_rate": 3.073006423311923e-05,
      "loss": 1.7426,
      "step": 295200
    },
    {
      "epoch": 23.13175622747924,
      "grad_norm": 5.3145952224731445,
      "learning_rate": 3.0723536477100635e-05,
      "loss": 1.7914,
      "step": 295300
    },
    {
      "epoch": 23.13958953470155,
      "grad_norm": 7.274645805358887,
      "learning_rate": 3.071700872108204e-05,
      "loss": 1.7187,
      "step": 295400
    },
    {
      "epoch": 23.14742284192386,
      "grad_norm": 5.462841510772705,
      "learning_rate": 3.071048096506345e-05,
      "loss": 1.657,
      "step": 295500
    },
    {
      "epoch": 23.15525614914617,
      "grad_norm": 5.289542198181152,
      "learning_rate": 3.070395320904486e-05,
      "loss": 1.7608,
      "step": 295600
    },
    {
      "epoch": 23.16308945636848,
      "grad_norm": 5.740218639373779,
      "learning_rate": 3.069742545302627e-05,
      "loss": 1.697,
      "step": 295700
    },
    {
      "epoch": 23.170922763590788,
      "grad_norm": 6.712703227996826,
      "learning_rate": 3.069089769700768e-05,
      "loss": 1.7569,
      "step": 295800
    },
    {
      "epoch": 23.178756070813098,
      "grad_norm": 6.407626628875732,
      "learning_rate": 3.068436994098909e-05,
      "loss": 1.748,
      "step": 295900
    },
    {
      "epoch": 23.186589378035407,
      "grad_norm": 7.237922668457031,
      "learning_rate": 3.0677842184970496e-05,
      "loss": 1.7255,
      "step": 296000
    },
    {
      "epoch": 23.194422685257717,
      "grad_norm": 5.256799221038818,
      "learning_rate": 3.06713144289519e-05,
      "loss": 1.6313,
      "step": 296100
    },
    {
      "epoch": 23.202255992480026,
      "grad_norm": 5.959982395172119,
      "learning_rate": 3.0664786672933314e-05,
      "loss": 1.626,
      "step": 296200
    },
    {
      "epoch": 23.210089299702336,
      "grad_norm": 7.917194366455078,
      "learning_rate": 3.065825891691472e-05,
      "loss": 1.6757,
      "step": 296300
    },
    {
      "epoch": 23.217922606924642,
      "grad_norm": 6.2602362632751465,
      "learning_rate": 3.065173116089613e-05,
      "loss": 1.7017,
      "step": 296400
    },
    {
      "epoch": 23.22575591414695,
      "grad_norm": 5.3945722579956055,
      "learning_rate": 3.0645203404877545e-05,
      "loss": 1.7152,
      "step": 296500
    },
    {
      "epoch": 23.23358922136926,
      "grad_norm": 7.0064287185668945,
      "learning_rate": 3.063867564885895e-05,
      "loss": 1.6887,
      "step": 296600
    },
    {
      "epoch": 23.24142252859157,
      "grad_norm": 6.880053997039795,
      "learning_rate": 3.0632147892840356e-05,
      "loss": 1.7236,
      "step": 296700
    },
    {
      "epoch": 23.24925583581388,
      "grad_norm": 8.169161796569824,
      "learning_rate": 3.062562013682177e-05,
      "loss": 1.7061,
      "step": 296800
    },
    {
      "epoch": 23.25708914303619,
      "grad_norm": 5.113714694976807,
      "learning_rate": 3.0619092380803175e-05,
      "loss": 1.6826,
      "step": 296900
    },
    {
      "epoch": 23.2649224502585,
      "grad_norm": 5.5930304527282715,
      "learning_rate": 3.061256462478459e-05,
      "loss": 1.6828,
      "step": 297000
    },
    {
      "epoch": 23.27275575748081,
      "grad_norm": 6.168318271636963,
      "learning_rate": 3.0606036868766e-05,
      "loss": 1.8054,
      "step": 297100
    },
    {
      "epoch": 23.28058906470312,
      "grad_norm": 6.020261764526367,
      "learning_rate": 3.0599509112747406e-05,
      "loss": 1.616,
      "step": 297200
    },
    {
      "epoch": 23.288422371925428,
      "grad_norm": 4.777341365814209,
      "learning_rate": 3.059298135672881e-05,
      "loss": 1.7022,
      "step": 297300
    },
    {
      "epoch": 23.296255679147738,
      "grad_norm": 5.193290710449219,
      "learning_rate": 3.058645360071022e-05,
      "loss": 1.6726,
      "step": 297400
    },
    {
      "epoch": 23.304088986370047,
      "grad_norm": 6.198397159576416,
      "learning_rate": 3.057992584469163e-05,
      "loss": 1.6402,
      "step": 297500
    },
    {
      "epoch": 23.311922293592353,
      "grad_norm": 5.102572441101074,
      "learning_rate": 3.0573398088673035e-05,
      "loss": 1.7386,
      "step": 297600
    },
    {
      "epoch": 23.319755600814663,
      "grad_norm": 4.514742374420166,
      "learning_rate": 3.056687033265445e-05,
      "loss": 1.7326,
      "step": 297700
    },
    {
      "epoch": 23.327588908036972,
      "grad_norm": 5.5421624183654785,
      "learning_rate": 3.056034257663586e-05,
      "loss": 1.74,
      "step": 297800
    },
    {
      "epoch": 23.335422215259282,
      "grad_norm": 2.7818007469177246,
      "learning_rate": 3.0553814820617266e-05,
      "loss": 1.6894,
      "step": 297900
    },
    {
      "epoch": 23.34325552248159,
      "grad_norm": 5.463536739349365,
      "learning_rate": 3.054728706459867e-05,
      "loss": 1.7736,
      "step": 298000
    },
    {
      "epoch": 23.3510888297039,
      "grad_norm": 6.351123809814453,
      "learning_rate": 3.0540759308580085e-05,
      "loss": 1.7374,
      "step": 298100
    },
    {
      "epoch": 23.35892213692621,
      "grad_norm": 4.6865315437316895,
      "learning_rate": 3.053423155256149e-05,
      "loss": 1.7105,
      "step": 298200
    },
    {
      "epoch": 23.36675544414852,
      "grad_norm": 5.657090187072754,
      "learning_rate": 3.05277037965429e-05,
      "loss": 1.8047,
      "step": 298300
    },
    {
      "epoch": 23.37458875137083,
      "grad_norm": 5.331414699554443,
      "learning_rate": 3.0521176040524316e-05,
      "loss": 1.6101,
      "step": 298400
    },
    {
      "epoch": 23.38242205859314,
      "grad_norm": 5.4861884117126465,
      "learning_rate": 3.051464828450572e-05,
      "loss": 1.7474,
      "step": 298500
    },
    {
      "epoch": 23.39025536581545,
      "grad_norm": 5.261168956756592,
      "learning_rate": 3.0508120528487127e-05,
      "loss": 1.7214,
      "step": 298600
    },
    {
      "epoch": 23.398088673037755,
      "grad_norm": 5.781796932220459,
      "learning_rate": 3.050159277246854e-05,
      "loss": 1.6958,
      "step": 298700
    },
    {
      "epoch": 23.405921980260064,
      "grad_norm": 5.308435440063477,
      "learning_rate": 3.049506501644995e-05,
      "loss": 1.6454,
      "step": 298800
    },
    {
      "epoch": 23.413755287482374,
      "grad_norm": 6.34563684463501,
      "learning_rate": 3.0488537260431355e-05,
      "loss": 1.7612,
      "step": 298900
    },
    {
      "epoch": 23.421588594704684,
      "grad_norm": 7.593147277832031,
      "learning_rate": 3.048200950441276e-05,
      "loss": 1.7095,
      "step": 299000
    },
    {
      "epoch": 23.429421901926993,
      "grad_norm": 5.767026424407959,
      "learning_rate": 3.0475481748394176e-05,
      "loss": 1.812,
      "step": 299100
    },
    {
      "epoch": 23.437255209149303,
      "grad_norm": 6.341987609863281,
      "learning_rate": 3.0468953992375582e-05,
      "loss": 1.7927,
      "step": 299200
    },
    {
      "epoch": 23.445088516371612,
      "grad_norm": 5.07133150100708,
      "learning_rate": 3.0462426236356988e-05,
      "loss": 1.6494,
      "step": 299300
    },
    {
      "epoch": 23.452921823593922,
      "grad_norm": 5.266814231872559,
      "learning_rate": 3.04558984803384e-05,
      "loss": 1.737,
      "step": 299400
    },
    {
      "epoch": 23.46075513081623,
      "grad_norm": 5.765791893005371,
      "learning_rate": 3.044937072431981e-05,
      "loss": 1.725,
      "step": 299500
    },
    {
      "epoch": 23.46858843803854,
      "grad_norm": 6.884072780609131,
      "learning_rate": 3.0442842968301215e-05,
      "loss": 1.6221,
      "step": 299600
    },
    {
      "epoch": 23.47642174526085,
      "grad_norm": 5.277459621429443,
      "learning_rate": 3.0436315212282628e-05,
      "loss": 1.7362,
      "step": 299700
    },
    {
      "epoch": 23.484255052483157,
      "grad_norm": 7.11579704284668,
      "learning_rate": 3.0429787456264037e-05,
      "loss": 1.6463,
      "step": 299800
    },
    {
      "epoch": 23.492088359705466,
      "grad_norm": 6.495741844177246,
      "learning_rate": 3.0423259700245443e-05,
      "loss": 1.7297,
      "step": 299900
    },
    {
      "epoch": 23.499921666927776,
      "grad_norm": 3.2123560905456543,
      "learning_rate": 3.0416731944226855e-05,
      "loss": 1.7504,
      "step": 300000
    },
    {
      "epoch": 23.507754974150085,
      "grad_norm": 6.991249084472656,
      "learning_rate": 3.0410204188208264e-05,
      "loss": 1.7479,
      "step": 300100
    },
    {
      "epoch": 23.515588281372395,
      "grad_norm": 5.439831733703613,
      "learning_rate": 3.040367643218967e-05,
      "loss": 1.678,
      "step": 300200
    },
    {
      "epoch": 23.523421588594704,
      "grad_norm": 7.9904398918151855,
      "learning_rate": 3.0397148676171083e-05,
      "loss": 1.7754,
      "step": 300300
    },
    {
      "epoch": 23.531254895817014,
      "grad_norm": 6.574087142944336,
      "learning_rate": 3.0390620920152492e-05,
      "loss": 1.7628,
      "step": 300400
    },
    {
      "epoch": 23.539088203039324,
      "grad_norm": 7.472548484802246,
      "learning_rate": 3.0384093164133898e-05,
      "loss": 1.664,
      "step": 300500
    },
    {
      "epoch": 23.546921510261633,
      "grad_norm": 4.922276020050049,
      "learning_rate": 3.037756540811531e-05,
      "loss": 1.6853,
      "step": 300600
    },
    {
      "epoch": 23.554754817483943,
      "grad_norm": 5.323575496673584,
      "learning_rate": 3.037103765209672e-05,
      "loss": 1.6024,
      "step": 300700
    },
    {
      "epoch": 23.562588124706252,
      "grad_norm": 6.477619647979736,
      "learning_rate": 3.0364509896078125e-05,
      "loss": 1.6889,
      "step": 300800
    },
    {
      "epoch": 23.570421431928562,
      "grad_norm": 7.700713157653809,
      "learning_rate": 3.035798214005953e-05,
      "loss": 1.7856,
      "step": 300900
    },
    {
      "epoch": 23.578254739150868,
      "grad_norm": 5.868094444274902,
      "learning_rate": 3.0351454384040943e-05,
      "loss": 1.6835,
      "step": 301000
    },
    {
      "epoch": 23.586088046373177,
      "grad_norm": 5.335492134094238,
      "learning_rate": 3.0344926628022353e-05,
      "loss": 1.7081,
      "step": 301100
    },
    {
      "epoch": 23.593921353595487,
      "grad_norm": 4.595608234405518,
      "learning_rate": 3.033839887200376e-05,
      "loss": 1.7867,
      "step": 301200
    },
    {
      "epoch": 23.601754660817797,
      "grad_norm": 6.529259204864502,
      "learning_rate": 3.033187111598517e-05,
      "loss": 1.7047,
      "step": 301300
    },
    {
      "epoch": 23.609587968040106,
      "grad_norm": 5.8115997314453125,
      "learning_rate": 3.032534335996658e-05,
      "loss": 1.7699,
      "step": 301400
    },
    {
      "epoch": 23.617421275262416,
      "grad_norm": 3.124951124191284,
      "learning_rate": 3.0318815603947986e-05,
      "loss": 1.6563,
      "step": 301500
    },
    {
      "epoch": 23.625254582484725,
      "grad_norm": 5.196381568908691,
      "learning_rate": 3.03122878479294e-05,
      "loss": 1.6268,
      "step": 301600
    },
    {
      "epoch": 23.633087889707035,
      "grad_norm": 6.403707981109619,
      "learning_rate": 3.0305760091910808e-05,
      "loss": 1.7684,
      "step": 301700
    },
    {
      "epoch": 23.640921196929344,
      "grad_norm": 5.461384296417236,
      "learning_rate": 3.0299232335892213e-05,
      "loss": 1.722,
      "step": 301800
    },
    {
      "epoch": 23.648754504151654,
      "grad_norm": 5.76884126663208,
      "learning_rate": 3.0292704579873626e-05,
      "loss": 1.8045,
      "step": 301900
    },
    {
      "epoch": 23.656587811373964,
      "grad_norm": 7.115994453430176,
      "learning_rate": 3.0286176823855035e-05,
      "loss": 1.7603,
      "step": 302000
    },
    {
      "epoch": 23.664421118596273,
      "grad_norm": 5.801485061645508,
      "learning_rate": 3.027964906783644e-05,
      "loss": 1.7532,
      "step": 302100
    },
    {
      "epoch": 23.67225442581858,
      "grad_norm": 7.294934272766113,
      "learning_rate": 3.0273121311817853e-05,
      "loss": 1.8247,
      "step": 302200
    },
    {
      "epoch": 23.68008773304089,
      "grad_norm": 6.227355003356934,
      "learning_rate": 3.0266593555799262e-05,
      "loss": 1.7382,
      "step": 302300
    },
    {
      "epoch": 23.6879210402632,
      "grad_norm": 6.592912197113037,
      "learning_rate": 3.0260065799780668e-05,
      "loss": 1.7593,
      "step": 302400
    },
    {
      "epoch": 23.695754347485508,
      "grad_norm": 6.37900972366333,
      "learning_rate": 3.0253538043762074e-05,
      "loss": 1.7748,
      "step": 302500
    },
    {
      "epoch": 23.703587654707817,
      "grad_norm": 6.7605204582214355,
      "learning_rate": 3.0247010287743487e-05,
      "loss": 1.6833,
      "step": 302600
    },
    {
      "epoch": 23.711420961930127,
      "grad_norm": 6.388756275177002,
      "learning_rate": 3.0240482531724896e-05,
      "loss": 1.728,
      "step": 302700
    },
    {
      "epoch": 23.719254269152437,
      "grad_norm": 5.147825241088867,
      "learning_rate": 3.02339547757063e-05,
      "loss": 1.7307,
      "step": 302800
    },
    {
      "epoch": 23.727087576374746,
      "grad_norm": 4.894703388214111,
      "learning_rate": 3.0227427019687714e-05,
      "loss": 1.7196,
      "step": 302900
    },
    {
      "epoch": 23.734920883597056,
      "grad_norm": 6.099783420562744,
      "learning_rate": 3.0220899263669123e-05,
      "loss": 1.7167,
      "step": 303000
    },
    {
      "epoch": 23.742754190819365,
      "grad_norm": 6.097110271453857,
      "learning_rate": 3.021437150765053e-05,
      "loss": 1.7705,
      "step": 303100
    },
    {
      "epoch": 23.750587498041675,
      "grad_norm": 3.4946279525756836,
      "learning_rate": 3.020784375163194e-05,
      "loss": 1.7054,
      "step": 303200
    },
    {
      "epoch": 23.75842080526398,
      "grad_norm": 5.5258965492248535,
      "learning_rate": 3.020131599561335e-05,
      "loss": 1.695,
      "step": 303300
    },
    {
      "epoch": 23.76625411248629,
      "grad_norm": 7.568657398223877,
      "learning_rate": 3.0194788239594756e-05,
      "loss": 1.7045,
      "step": 303400
    },
    {
      "epoch": 23.7740874197086,
      "grad_norm": 6.137415409088135,
      "learning_rate": 3.018826048357617e-05,
      "loss": 1.652,
      "step": 303500
    },
    {
      "epoch": 23.78192072693091,
      "grad_norm": 9.189851760864258,
      "learning_rate": 3.0181732727557578e-05,
      "loss": 1.6478,
      "step": 303600
    },
    {
      "epoch": 23.78975403415322,
      "grad_norm": 6.434330940246582,
      "learning_rate": 3.0175204971538984e-05,
      "loss": 1.7461,
      "step": 303700
    },
    {
      "epoch": 23.79758734137553,
      "grad_norm": 7.060622692108154,
      "learning_rate": 3.0168677215520396e-05,
      "loss": 1.712,
      "step": 303800
    },
    {
      "epoch": 23.80542064859784,
      "grad_norm": 6.82851505279541,
      "learning_rate": 3.0162149459501806e-05,
      "loss": 1.6708,
      "step": 303900
    },
    {
      "epoch": 23.813253955820148,
      "grad_norm": 6.8315629959106445,
      "learning_rate": 3.015562170348321e-05,
      "loss": 1.629,
      "step": 304000
    },
    {
      "epoch": 23.821087263042457,
      "grad_norm": 6.161035060882568,
      "learning_rate": 3.0149093947464617e-05,
      "loss": 1.7093,
      "step": 304100
    },
    {
      "epoch": 23.828920570264767,
      "grad_norm": 5.856283187866211,
      "learning_rate": 3.014256619144603e-05,
      "loss": 1.6672,
      "step": 304200
    },
    {
      "epoch": 23.836753877487077,
      "grad_norm": 5.643558502197266,
      "learning_rate": 3.013603843542744e-05,
      "loss": 1.7499,
      "step": 304300
    },
    {
      "epoch": 23.844587184709383,
      "grad_norm": 6.594325542449951,
      "learning_rate": 3.0129510679408845e-05,
      "loss": 1.7868,
      "step": 304400
    },
    {
      "epoch": 23.852420491931692,
      "grad_norm": 5.7443084716796875,
      "learning_rate": 3.0122982923390257e-05,
      "loss": 1.6845,
      "step": 304500
    },
    {
      "epoch": 23.860253799154002,
      "grad_norm": 22.88768768310547,
      "learning_rate": 3.0116455167371666e-05,
      "loss": 1.7268,
      "step": 304600
    },
    {
      "epoch": 23.86808710637631,
      "grad_norm": 5.827956676483154,
      "learning_rate": 3.0109927411353072e-05,
      "loss": 1.6483,
      "step": 304700
    },
    {
      "epoch": 23.87592041359862,
      "grad_norm": 5.932849884033203,
      "learning_rate": 3.0103399655334485e-05,
      "loss": 1.7538,
      "step": 304800
    },
    {
      "epoch": 23.88375372082093,
      "grad_norm": 7.333895206451416,
      "learning_rate": 3.0096871899315894e-05,
      "loss": 1.6785,
      "step": 304900
    },
    {
      "epoch": 23.89158702804324,
      "grad_norm": 7.939584732055664,
      "learning_rate": 3.00903441432973e-05,
      "loss": 1.7215,
      "step": 305000
    },
    {
      "epoch": 23.89942033526555,
      "grad_norm": 5.566534042358398,
      "learning_rate": 3.0083816387278712e-05,
      "loss": 1.7882,
      "step": 305100
    },
    {
      "epoch": 23.90725364248786,
      "grad_norm": 6.816390514373779,
      "learning_rate": 3.007728863126012e-05,
      "loss": 1.6691,
      "step": 305200
    },
    {
      "epoch": 23.91508694971017,
      "grad_norm": 5.217456340789795,
      "learning_rate": 3.0070760875241527e-05,
      "loss": 1.7527,
      "step": 305300
    },
    {
      "epoch": 23.92292025693248,
      "grad_norm": 6.085311412811279,
      "learning_rate": 3.006423311922294e-05,
      "loss": 1.7545,
      "step": 305400
    },
    {
      "epoch": 23.930753564154784,
      "grad_norm": 4.665317058563232,
      "learning_rate": 3.005770536320435e-05,
      "loss": 1.7149,
      "step": 305500
    },
    {
      "epoch": 23.938586871377094,
      "grad_norm": 5.738807678222656,
      "learning_rate": 3.0051177607185754e-05,
      "loss": 1.7944,
      "step": 305600
    },
    {
      "epoch": 23.946420178599404,
      "grad_norm": 5.663893222808838,
      "learning_rate": 3.004464985116716e-05,
      "loss": 1.713,
      "step": 305700
    },
    {
      "epoch": 23.954253485821713,
      "grad_norm": 7.7169413566589355,
      "learning_rate": 3.0038122095148573e-05,
      "loss": 1.8185,
      "step": 305800
    },
    {
      "epoch": 23.962086793044023,
      "grad_norm": 5.819611549377441,
      "learning_rate": 3.0031594339129982e-05,
      "loss": 1.6628,
      "step": 305900
    },
    {
      "epoch": 23.969920100266332,
      "grad_norm": 7.936609268188477,
      "learning_rate": 3.0025066583111388e-05,
      "loss": 1.6953,
      "step": 306000
    },
    {
      "epoch": 23.977753407488642,
      "grad_norm": 6.500904083251953,
      "learning_rate": 3.00185388270928e-05,
      "loss": 1.7199,
      "step": 306100
    },
    {
      "epoch": 23.98558671471095,
      "grad_norm": 6.124362945556641,
      "learning_rate": 3.001201107107421e-05,
      "loss": 1.6722,
      "step": 306200
    },
    {
      "epoch": 23.99342002193326,
      "grad_norm": 6.002731800079346,
      "learning_rate": 3.0005483315055615e-05,
      "loss": 1.6525,
      "step": 306300
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.7952661514282227,
      "eval_runtime": 2.8768,
      "eval_samples_per_second": 233.591,
      "eval_steps_per_second": 233.591,
      "step": 306384
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.4778598546981812,
      "eval_runtime": 54.3498,
      "eval_samples_per_second": 234.886,
      "eval_steps_per_second": 234.886,
      "step": 306384
    },
    {
      "epoch": 24.00125332915557,
      "grad_norm": 5.663117408752441,
      "learning_rate": 2.9998955559037028e-05,
      "loss": 1.751,
      "step": 306400
    },
    {
      "epoch": 24.00908663637788,
      "grad_norm": 4.336132526397705,
      "learning_rate": 2.9992427803018437e-05,
      "loss": 1.7538,
      "step": 306500
    },
    {
      "epoch": 24.01691994360019,
      "grad_norm": 5.897252082824707,
      "learning_rate": 2.9985900046999843e-05,
      "loss": 1.7534,
      "step": 306600
    },
    {
      "epoch": 24.024753250822496,
      "grad_norm": 6.0540289878845215,
      "learning_rate": 2.9979372290981255e-05,
      "loss": 1.5777,
      "step": 306700
    },
    {
      "epoch": 24.032586558044805,
      "grad_norm": 6.750725269317627,
      "learning_rate": 2.9972844534962664e-05,
      "loss": 1.7459,
      "step": 306800
    },
    {
      "epoch": 24.040419865267115,
      "grad_norm": 5.931966781616211,
      "learning_rate": 2.996631677894407e-05,
      "loss": 1.6125,
      "step": 306900
    },
    {
      "epoch": 24.048253172489424,
      "grad_norm": 6.915338039398193,
      "learning_rate": 2.9959789022925483e-05,
      "loss": 1.7475,
      "step": 307000
    },
    {
      "epoch": 24.056086479711734,
      "grad_norm": 5.666867733001709,
      "learning_rate": 2.9953261266906892e-05,
      "loss": 1.7139,
      "step": 307100
    },
    {
      "epoch": 24.063919786934044,
      "grad_norm": 5.4315032958984375,
      "learning_rate": 2.9946733510888298e-05,
      "loss": 1.5987,
      "step": 307200
    },
    {
      "epoch": 24.071753094156353,
      "grad_norm": 6.228776454925537,
      "learning_rate": 2.994020575486971e-05,
      "loss": 1.7047,
      "step": 307300
    },
    {
      "epoch": 24.079586401378663,
      "grad_norm": 8.520169258117676,
      "learning_rate": 2.9933677998851116e-05,
      "loss": 1.7277,
      "step": 307400
    },
    {
      "epoch": 24.087419708600972,
      "grad_norm": 4.846692085266113,
      "learning_rate": 2.9927150242832525e-05,
      "loss": 1.7996,
      "step": 307500
    },
    {
      "epoch": 24.095253015823282,
      "grad_norm": 6.6911940574646,
      "learning_rate": 2.992062248681393e-05,
      "loss": 1.7561,
      "step": 307600
    },
    {
      "epoch": 24.10308632304559,
      "grad_norm": 7.938900947570801,
      "learning_rate": 2.9914094730795343e-05,
      "loss": 1.6574,
      "step": 307700
    },
    {
      "epoch": 24.110919630267897,
      "grad_norm": 6.633103370666504,
      "learning_rate": 2.9907566974776753e-05,
      "loss": 1.6734,
      "step": 307800
    },
    {
      "epoch": 24.118752937490207,
      "grad_norm": 5.610545635223389,
      "learning_rate": 2.9901039218758158e-05,
      "loss": 1.648,
      "step": 307900
    },
    {
      "epoch": 24.126586244712517,
      "grad_norm": 5.054140567779541,
      "learning_rate": 2.989451146273957e-05,
      "loss": 1.6129,
      "step": 308000
    },
    {
      "epoch": 24.134419551934826,
      "grad_norm": 5.6923041343688965,
      "learning_rate": 2.988798370672098e-05,
      "loss": 1.7128,
      "step": 308100
    },
    {
      "epoch": 24.142252859157136,
      "grad_norm": 5.24464750289917,
      "learning_rate": 2.9881455950702386e-05,
      "loss": 1.6493,
      "step": 308200
    },
    {
      "epoch": 24.150086166379445,
      "grad_norm": 5.610669136047363,
      "learning_rate": 2.9874928194683798e-05,
      "loss": 1.6962,
      "step": 308300
    },
    {
      "epoch": 24.157919473601755,
      "grad_norm": 5.070152282714844,
      "learning_rate": 2.9868400438665207e-05,
      "loss": 1.6492,
      "step": 308400
    },
    {
      "epoch": 24.165752780824064,
      "grad_norm": 8.811079025268555,
      "learning_rate": 2.9861872682646613e-05,
      "loss": 1.7324,
      "step": 308500
    },
    {
      "epoch": 24.173586088046374,
      "grad_norm": 5.599169731140137,
      "learning_rate": 2.9855344926628026e-05,
      "loss": 1.5994,
      "step": 308600
    },
    {
      "epoch": 24.181419395268684,
      "grad_norm": 6.090287685394287,
      "learning_rate": 2.9848817170609435e-05,
      "loss": 1.7524,
      "step": 308700
    },
    {
      "epoch": 24.189252702490993,
      "grad_norm": 7.774356365203857,
      "learning_rate": 2.984228941459084e-05,
      "loss": 1.7965,
      "step": 308800
    },
    {
      "epoch": 24.197086009713303,
      "grad_norm": 5.9102582931518555,
      "learning_rate": 2.9835761658572253e-05,
      "loss": 1.7409,
      "step": 308900
    },
    {
      "epoch": 24.20491931693561,
      "grad_norm": 5.646472454071045,
      "learning_rate": 2.982923390255366e-05,
      "loss": 1.8453,
      "step": 309000
    },
    {
      "epoch": 24.21275262415792,
      "grad_norm": 5.98084831237793,
      "learning_rate": 2.9822706146535068e-05,
      "loss": 1.6929,
      "step": 309100
    },
    {
      "epoch": 24.220585931380228,
      "grad_norm": 6.222063064575195,
      "learning_rate": 2.9816178390516474e-05,
      "loss": 1.6967,
      "step": 309200
    },
    {
      "epoch": 24.228419238602537,
      "grad_norm": 5.783519268035889,
      "learning_rate": 2.9809650634497886e-05,
      "loss": 1.7632,
      "step": 309300
    },
    {
      "epoch": 24.236252545824847,
      "grad_norm": 6.141939163208008,
      "learning_rate": 2.9803122878479296e-05,
      "loss": 1.6156,
      "step": 309400
    },
    {
      "epoch": 24.244085853047157,
      "grad_norm": 6.136861324310303,
      "learning_rate": 2.97965951224607e-05,
      "loss": 1.713,
      "step": 309500
    },
    {
      "epoch": 24.251919160269466,
      "grad_norm": 5.992790699005127,
      "learning_rate": 2.9790067366442114e-05,
      "loss": 1.6293,
      "step": 309600
    },
    {
      "epoch": 24.259752467491776,
      "grad_norm": 5.3347015380859375,
      "learning_rate": 2.9783539610423523e-05,
      "loss": 1.7371,
      "step": 309700
    },
    {
      "epoch": 24.267585774714085,
      "grad_norm": 6.9049835205078125,
      "learning_rate": 2.977701185440493e-05,
      "loss": 1.742,
      "step": 309800
    },
    {
      "epoch": 24.275419081936395,
      "grad_norm": 6.444005489349365,
      "learning_rate": 2.977048409838634e-05,
      "loss": 1.7328,
      "step": 309900
    },
    {
      "epoch": 24.283252389158704,
      "grad_norm": 5.6548004150390625,
      "learning_rate": 2.976395634236775e-05,
      "loss": 1.8098,
      "step": 310000
    },
    {
      "epoch": 24.29108569638101,
      "grad_norm": 6.606100559234619,
      "learning_rate": 2.9757428586349156e-05,
      "loss": 1.6808,
      "step": 310100
    },
    {
      "epoch": 24.29891900360332,
      "grad_norm": 6.675109386444092,
      "learning_rate": 2.975090083033057e-05,
      "loss": 1.6786,
      "step": 310200
    },
    {
      "epoch": 24.30675231082563,
      "grad_norm": 6.1411895751953125,
      "learning_rate": 2.9744373074311978e-05,
      "loss": 1.6835,
      "step": 310300
    },
    {
      "epoch": 24.31458561804794,
      "grad_norm": 6.266902923583984,
      "learning_rate": 2.9737845318293384e-05,
      "loss": 1.6809,
      "step": 310400
    },
    {
      "epoch": 24.32241892527025,
      "grad_norm": 8.566302299499512,
      "learning_rate": 2.9731317562274796e-05,
      "loss": 1.6061,
      "step": 310500
    },
    {
      "epoch": 24.33025223249256,
      "grad_norm": 4.903294563293457,
      "learning_rate": 2.9724789806256202e-05,
      "loss": 1.6956,
      "step": 310600
    },
    {
      "epoch": 24.338085539714868,
      "grad_norm": 5.480888843536377,
      "learning_rate": 2.971826205023761e-05,
      "loss": 1.6894,
      "step": 310700
    },
    {
      "epoch": 24.345918846937177,
      "grad_norm": 6.860055446624756,
      "learning_rate": 2.9711734294219017e-05,
      "loss": 1.6741,
      "step": 310800
    },
    {
      "epoch": 24.353752154159487,
      "grad_norm": 4.828103065490723,
      "learning_rate": 2.970520653820043e-05,
      "loss": 1.6292,
      "step": 310900
    },
    {
      "epoch": 24.361585461381797,
      "grad_norm": 7.965334892272949,
      "learning_rate": 2.969867878218184e-05,
      "loss": 1.7072,
      "step": 311000
    },
    {
      "epoch": 24.369418768604106,
      "grad_norm": 6.780887603759766,
      "learning_rate": 2.9692151026163245e-05,
      "loss": 1.7976,
      "step": 311100
    },
    {
      "epoch": 24.377252075826412,
      "grad_norm": 6.91868257522583,
      "learning_rate": 2.9685623270144657e-05,
      "loss": 1.6611,
      "step": 311200
    },
    {
      "epoch": 24.38508538304872,
      "grad_norm": 5.084920406341553,
      "learning_rate": 2.9679095514126066e-05,
      "loss": 1.6773,
      "step": 311300
    },
    {
      "epoch": 24.39291869027103,
      "grad_norm": 7.821724891662598,
      "learning_rate": 2.9672567758107472e-05,
      "loss": 1.634,
      "step": 311400
    },
    {
      "epoch": 24.40075199749334,
      "grad_norm": 5.878067970275879,
      "learning_rate": 2.9666040002088885e-05,
      "loss": 1.6789,
      "step": 311500
    },
    {
      "epoch": 24.40858530471565,
      "grad_norm": 5.241084098815918,
      "learning_rate": 2.9659512246070294e-05,
      "loss": 1.7072,
      "step": 311600
    },
    {
      "epoch": 24.41641861193796,
      "grad_norm": 4.59781551361084,
      "learning_rate": 2.96529844900517e-05,
      "loss": 1.7519,
      "step": 311700
    },
    {
      "epoch": 24.42425191916027,
      "grad_norm": 7.776451110839844,
      "learning_rate": 2.9646456734033112e-05,
      "loss": 1.6872,
      "step": 311800
    },
    {
      "epoch": 24.43208522638258,
      "grad_norm": 7.065437316894531,
      "learning_rate": 2.963992897801452e-05,
      "loss": 1.7396,
      "step": 311900
    },
    {
      "epoch": 24.43991853360489,
      "grad_norm": 4.4979681968688965,
      "learning_rate": 2.9633401221995927e-05,
      "loss": 1.794,
      "step": 312000
    },
    {
      "epoch": 24.4477518408272,
      "grad_norm": 6.461912631988525,
      "learning_rate": 2.962687346597734e-05,
      "loss": 1.7228,
      "step": 312100
    },
    {
      "epoch": 24.455585148049508,
      "grad_norm": 7.484115123748779,
      "learning_rate": 2.9620345709958745e-05,
      "loss": 1.6766,
      "step": 312200
    },
    {
      "epoch": 24.463418455271817,
      "grad_norm": 6.091533660888672,
      "learning_rate": 2.9613817953940154e-05,
      "loss": 1.6628,
      "step": 312300
    },
    {
      "epoch": 24.471251762494123,
      "grad_norm": 7.5034918785095215,
      "learning_rate": 2.9607290197921567e-05,
      "loss": 1.7252,
      "step": 312400
    },
    {
      "epoch": 24.479085069716433,
      "grad_norm": 6.395909309387207,
      "learning_rate": 2.9600762441902973e-05,
      "loss": 1.6783,
      "step": 312500
    },
    {
      "epoch": 24.486918376938743,
      "grad_norm": 4.856006145477295,
      "learning_rate": 2.9594234685884382e-05,
      "loss": 1.7351,
      "step": 312600
    },
    {
      "epoch": 24.494751684161052,
      "grad_norm": 5.84362268447876,
      "learning_rate": 2.9587706929865788e-05,
      "loss": 1.6786,
      "step": 312700
    },
    {
      "epoch": 24.50258499138336,
      "grad_norm": 5.578062057495117,
      "learning_rate": 2.95811791738472e-05,
      "loss": 1.7023,
      "step": 312800
    },
    {
      "epoch": 24.51041829860567,
      "grad_norm": 5.438096523284912,
      "learning_rate": 2.957465141782861e-05,
      "loss": 1.6468,
      "step": 312900
    },
    {
      "epoch": 24.51825160582798,
      "grad_norm": 9.487610816955566,
      "learning_rate": 2.9568123661810015e-05,
      "loss": 1.742,
      "step": 313000
    },
    {
      "epoch": 24.52608491305029,
      "grad_norm": 7.2783098220825195,
      "learning_rate": 2.9561595905791428e-05,
      "loss": 1.7496,
      "step": 313100
    },
    {
      "epoch": 24.5339182202726,
      "grad_norm": 5.39827299118042,
      "learning_rate": 2.9555068149772837e-05,
      "loss": 1.7182,
      "step": 313200
    },
    {
      "epoch": 24.54175152749491,
      "grad_norm": 6.642062187194824,
      "learning_rate": 2.9548540393754243e-05,
      "loss": 1.7436,
      "step": 313300
    },
    {
      "epoch": 24.54958483471722,
      "grad_norm": 5.136872291564941,
      "learning_rate": 2.9542012637735655e-05,
      "loss": 1.7069,
      "step": 313400
    },
    {
      "epoch": 24.557418141939525,
      "grad_norm": 6.607693672180176,
      "learning_rate": 2.9535484881717064e-05,
      "loss": 1.814,
      "step": 313500
    },
    {
      "epoch": 24.565251449161835,
      "grad_norm": 5.740915298461914,
      "learning_rate": 2.952895712569847e-05,
      "loss": 1.72,
      "step": 313600
    },
    {
      "epoch": 24.573084756384144,
      "grad_norm": 3.8939921855926514,
      "learning_rate": 2.9522429369679883e-05,
      "loss": 1.8287,
      "step": 313700
    },
    {
      "epoch": 24.580918063606454,
      "grad_norm": 6.682138442993164,
      "learning_rate": 2.951590161366129e-05,
      "loss": 1.6879,
      "step": 313800
    },
    {
      "epoch": 24.588751370828763,
      "grad_norm": 6.028415679931641,
      "learning_rate": 2.9509373857642698e-05,
      "loss": 1.7136,
      "step": 313900
    },
    {
      "epoch": 24.596584678051073,
      "grad_norm": 5.315757751464844,
      "learning_rate": 2.950284610162411e-05,
      "loss": 1.6508,
      "step": 314000
    },
    {
      "epoch": 24.604417985273383,
      "grad_norm": 5.644733905792236,
      "learning_rate": 2.9496318345605516e-05,
      "loss": 1.7042,
      "step": 314100
    },
    {
      "epoch": 24.612251292495692,
      "grad_norm": 6.11570930480957,
      "learning_rate": 2.9489790589586925e-05,
      "loss": 1.6614,
      "step": 314200
    },
    {
      "epoch": 24.620084599718002,
      "grad_norm": 4.359543323516846,
      "learning_rate": 2.948326283356833e-05,
      "loss": 1.7021,
      "step": 314300
    },
    {
      "epoch": 24.62791790694031,
      "grad_norm": 5.714600563049316,
      "learning_rate": 2.9476735077549743e-05,
      "loss": 1.6701,
      "step": 314400
    },
    {
      "epoch": 24.63575121416262,
      "grad_norm": 6.811051845550537,
      "learning_rate": 2.9470207321531152e-05,
      "loss": 1.7432,
      "step": 314500
    },
    {
      "epoch": 24.64358452138493,
      "grad_norm": 4.711703300476074,
      "learning_rate": 2.9463679565512558e-05,
      "loss": 1.6948,
      "step": 314600
    },
    {
      "epoch": 24.651417828607237,
      "grad_norm": 5.5658392906188965,
      "learning_rate": 2.945715180949397e-05,
      "loss": 1.6919,
      "step": 314700
    },
    {
      "epoch": 24.659251135829546,
      "grad_norm": 7.693349838256836,
      "learning_rate": 2.945062405347538e-05,
      "loss": 1.7652,
      "step": 314800
    },
    {
      "epoch": 24.667084443051856,
      "grad_norm": 5.53732967376709,
      "learning_rate": 2.9444096297456786e-05,
      "loss": 1.7058,
      "step": 314900
    },
    {
      "epoch": 24.674917750274165,
      "grad_norm": 5.6386189460754395,
      "learning_rate": 2.9437568541438198e-05,
      "loss": 1.6819,
      "step": 315000
    },
    {
      "epoch": 24.682751057496475,
      "grad_norm": 5.465884208679199,
      "learning_rate": 2.9431040785419607e-05,
      "loss": 1.7098,
      "step": 315100
    },
    {
      "epoch": 24.690584364718784,
      "grad_norm": 6.184324264526367,
      "learning_rate": 2.9424513029401013e-05,
      "loss": 1.7282,
      "step": 315200
    },
    {
      "epoch": 24.698417671941094,
      "grad_norm": 6.4682135581970215,
      "learning_rate": 2.9417985273382426e-05,
      "loss": 1.7455,
      "step": 315300
    },
    {
      "epoch": 24.706250979163404,
      "grad_norm": 6.301985740661621,
      "learning_rate": 2.941145751736383e-05,
      "loss": 1.7028,
      "step": 315400
    },
    {
      "epoch": 24.714084286385713,
      "grad_norm": 5.9345245361328125,
      "learning_rate": 2.940492976134524e-05,
      "loss": 1.6334,
      "step": 315500
    },
    {
      "epoch": 24.721917593608023,
      "grad_norm": 6.369958877563477,
      "learning_rate": 2.9398402005326653e-05,
      "loss": 1.727,
      "step": 315600
    },
    {
      "epoch": 24.729750900830332,
      "grad_norm": 8.341557502746582,
      "learning_rate": 2.939187424930806e-05,
      "loss": 1.7254,
      "step": 315700
    },
    {
      "epoch": 24.73758420805264,
      "grad_norm": 5.47755241394043,
      "learning_rate": 2.9385346493289468e-05,
      "loss": 1.822,
      "step": 315800
    },
    {
      "epoch": 24.745417515274948,
      "grad_norm": 5.690097332000732,
      "learning_rate": 2.9378818737270874e-05,
      "loss": 1.7274,
      "step": 315900
    },
    {
      "epoch": 24.753250822497257,
      "grad_norm": 6.903855800628662,
      "learning_rate": 2.9372290981252286e-05,
      "loss": 1.6672,
      "step": 316000
    },
    {
      "epoch": 24.761084129719567,
      "grad_norm": 7.155076026916504,
      "learning_rate": 2.9365763225233696e-05,
      "loss": 1.7512,
      "step": 316100
    },
    {
      "epoch": 24.768917436941877,
      "grad_norm": 4.537452697753906,
      "learning_rate": 2.93592354692151e-05,
      "loss": 1.781,
      "step": 316200
    },
    {
      "epoch": 24.776750744164186,
      "grad_norm": 5.195444583892822,
      "learning_rate": 2.9352707713196514e-05,
      "loss": 1.6458,
      "step": 316300
    },
    {
      "epoch": 24.784584051386496,
      "grad_norm": 7.367711544036865,
      "learning_rate": 2.9346179957177923e-05,
      "loss": 1.7024,
      "step": 316400
    },
    {
      "epoch": 24.792417358608805,
      "grad_norm": 4.979743003845215,
      "learning_rate": 2.933965220115933e-05,
      "loss": 1.6564,
      "step": 316500
    },
    {
      "epoch": 24.800250665831115,
      "grad_norm": 6.058644771575928,
      "learning_rate": 2.933312444514074e-05,
      "loss": 1.7488,
      "step": 316600
    },
    {
      "epoch": 24.808083973053424,
      "grad_norm": 5.58523416519165,
      "learning_rate": 2.932659668912215e-05,
      "loss": 1.6859,
      "step": 316700
    },
    {
      "epoch": 24.815917280275734,
      "grad_norm": 4.772208213806152,
      "learning_rate": 2.9320068933103556e-05,
      "loss": 1.7678,
      "step": 316800
    },
    {
      "epoch": 24.82375058749804,
      "grad_norm": 4.999089241027832,
      "learning_rate": 2.931354117708497e-05,
      "loss": 1.7829,
      "step": 316900
    },
    {
      "epoch": 24.83158389472035,
      "grad_norm": 4.467641830444336,
      "learning_rate": 2.9307013421066375e-05,
      "loss": 1.6382,
      "step": 317000
    },
    {
      "epoch": 24.83941720194266,
      "grad_norm": 6.009289264678955,
      "learning_rate": 2.9300485665047784e-05,
      "loss": 1.7821,
      "step": 317100
    },
    {
      "epoch": 24.84725050916497,
      "grad_norm": 3.8703153133392334,
      "learning_rate": 2.9293957909029196e-05,
      "loss": 1.7219,
      "step": 317200
    },
    {
      "epoch": 24.85508381638728,
      "grad_norm": 7.537326812744141,
      "learning_rate": 2.9287430153010602e-05,
      "loss": 1.6799,
      "step": 317300
    },
    {
      "epoch": 24.862917123609588,
      "grad_norm": 7.274473190307617,
      "learning_rate": 2.928090239699201e-05,
      "loss": 1.7451,
      "step": 317400
    },
    {
      "epoch": 24.870750430831897,
      "grad_norm": 7.561649799346924,
      "learning_rate": 2.9274374640973417e-05,
      "loss": 1.763,
      "step": 317500
    },
    {
      "epoch": 24.878583738054207,
      "grad_norm": 4.783104419708252,
      "learning_rate": 2.926784688495483e-05,
      "loss": 1.7433,
      "step": 317600
    },
    {
      "epoch": 24.886417045276517,
      "grad_norm": 5.861774921417236,
      "learning_rate": 2.926131912893624e-05,
      "loss": 1.7464,
      "step": 317700
    },
    {
      "epoch": 24.894250352498826,
      "grad_norm": 7.885166645050049,
      "learning_rate": 2.9254791372917644e-05,
      "loss": 1.7236,
      "step": 317800
    },
    {
      "epoch": 24.902083659721136,
      "grad_norm": 5.537078380584717,
      "learning_rate": 2.9248263616899057e-05,
      "loss": 1.659,
      "step": 317900
    },
    {
      "epoch": 24.90991696694344,
      "grad_norm": 6.419899940490723,
      "learning_rate": 2.9241735860880466e-05,
      "loss": 1.7317,
      "step": 318000
    },
    {
      "epoch": 24.91775027416575,
      "grad_norm": 4.643237113952637,
      "learning_rate": 2.9235208104861872e-05,
      "loss": 1.7919,
      "step": 318100
    },
    {
      "epoch": 24.92558358138806,
      "grad_norm": 5.547338962554932,
      "learning_rate": 2.9228680348843284e-05,
      "loss": 1.6583,
      "step": 318200
    },
    {
      "epoch": 24.93341688861037,
      "grad_norm": 5.665976047515869,
      "learning_rate": 2.9222152592824694e-05,
      "loss": 1.6695,
      "step": 318300
    },
    {
      "epoch": 24.94125019583268,
      "grad_norm": 6.550771236419678,
      "learning_rate": 2.92156248368061e-05,
      "loss": 1.7213,
      "step": 318400
    },
    {
      "epoch": 24.94908350305499,
      "grad_norm": 8.77747631072998,
      "learning_rate": 2.9209097080787512e-05,
      "loss": 1.7613,
      "step": 318500
    },
    {
      "epoch": 24.9569168102773,
      "grad_norm": 6.299250602722168,
      "learning_rate": 2.9202569324768918e-05,
      "loss": 1.6846,
      "step": 318600
    },
    {
      "epoch": 24.96475011749961,
      "grad_norm": 5.722545623779297,
      "learning_rate": 2.9196041568750327e-05,
      "loss": 1.7002,
      "step": 318700
    },
    {
      "epoch": 24.97258342472192,
      "grad_norm": 9.564659118652344,
      "learning_rate": 2.918951381273174e-05,
      "loss": 1.7055,
      "step": 318800
    },
    {
      "epoch": 24.980416731944228,
      "grad_norm": 5.702820301055908,
      "learning_rate": 2.9182986056713145e-05,
      "loss": 1.7201,
      "step": 318900
    },
    {
      "epoch": 24.988250039166537,
      "grad_norm": 6.509477138519287,
      "learning_rate": 2.9176458300694554e-05,
      "loss": 1.6216,
      "step": 319000
    },
    {
      "epoch": 24.996083346388847,
      "grad_norm": 6.620894908905029,
      "learning_rate": 2.9169930544675967e-05,
      "loss": 1.7664,
      "step": 319100
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.784332513809204,
      "eval_runtime": 2.9055,
      "eval_samples_per_second": 231.284,
      "eval_steps_per_second": 231.284,
      "step": 319150
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.470786213874817,
      "eval_runtime": 56.1255,
      "eval_samples_per_second": 227.455,
      "eval_steps_per_second": 227.455,
      "step": 319150
    },
    {
      "epoch": 25.003916653611153,
      "grad_norm": 6.7764081954956055,
      "learning_rate": 2.9163402788657373e-05,
      "loss": 1.7146,
      "step": 319200
    },
    {
      "epoch": 25.011749960833463,
      "grad_norm": 5.05922269821167,
      "learning_rate": 2.9156875032638782e-05,
      "loss": 1.696,
      "step": 319300
    },
    {
      "epoch": 25.019583268055772,
      "grad_norm": 6.149895668029785,
      "learning_rate": 2.9150347276620188e-05,
      "loss": 1.7599,
      "step": 319400
    },
    {
      "epoch": 25.02741657527808,
      "grad_norm": 5.513327121734619,
      "learning_rate": 2.91438195206016e-05,
      "loss": 1.7152,
      "step": 319500
    },
    {
      "epoch": 25.03524988250039,
      "grad_norm": 6.9868574142456055,
      "learning_rate": 2.913729176458301e-05,
      "loss": 1.6814,
      "step": 319600
    },
    {
      "epoch": 25.0430831897227,
      "grad_norm": 5.696534633636475,
      "learning_rate": 2.9130764008564415e-05,
      "loss": 1.7008,
      "step": 319700
    },
    {
      "epoch": 25.05091649694501,
      "grad_norm": 6.0074005126953125,
      "learning_rate": 2.9124236252545828e-05,
      "loss": 1.6365,
      "step": 319800
    },
    {
      "epoch": 25.05874980416732,
      "grad_norm": 8.869709014892578,
      "learning_rate": 2.9117708496527237e-05,
      "loss": 1.714,
      "step": 319900
    },
    {
      "epoch": 25.06658311138963,
      "grad_norm": 5.411596775054932,
      "learning_rate": 2.9111180740508643e-05,
      "loss": 1.7331,
      "step": 320000
    },
    {
      "epoch": 25.07441641861194,
      "grad_norm": 4.905209064483643,
      "learning_rate": 2.9104652984490055e-05,
      "loss": 1.7051,
      "step": 320100
    },
    {
      "epoch": 25.08224972583425,
      "grad_norm": 5.525511264801025,
      "learning_rate": 2.909812522847146e-05,
      "loss": 1.6174,
      "step": 320200
    },
    {
      "epoch": 25.090083033056555,
      "grad_norm": 5.756738662719727,
      "learning_rate": 2.909159747245287e-05,
      "loss": 1.6784,
      "step": 320300
    },
    {
      "epoch": 25.097916340278864,
      "grad_norm": 5.2093706130981445,
      "learning_rate": 2.9085069716434283e-05,
      "loss": 1.6131,
      "step": 320400
    },
    {
      "epoch": 25.105749647501174,
      "grad_norm": 6.039037704467773,
      "learning_rate": 2.9078541960415688e-05,
      "loss": 1.7399,
      "step": 320500
    },
    {
      "epoch": 25.113582954723483,
      "grad_norm": 6.9187726974487305,
      "learning_rate": 2.9072014204397097e-05,
      "loss": 1.6595,
      "step": 320600
    },
    {
      "epoch": 25.121416261945793,
      "grad_norm": 4.95695161819458,
      "learning_rate": 2.906548644837851e-05,
      "loss": 1.7613,
      "step": 320700
    },
    {
      "epoch": 25.129249569168103,
      "grad_norm": 6.3689866065979,
      "learning_rate": 2.9058958692359916e-05,
      "loss": 1.7716,
      "step": 320800
    },
    {
      "epoch": 25.137082876390412,
      "grad_norm": 5.8457417488098145,
      "learning_rate": 2.9052430936341325e-05,
      "loss": 1.6649,
      "step": 320900
    },
    {
      "epoch": 25.14491618361272,
      "grad_norm": 5.8709893226623535,
      "learning_rate": 2.904590318032273e-05,
      "loss": 1.7429,
      "step": 321000
    },
    {
      "epoch": 25.15274949083503,
      "grad_norm": 4.329802989959717,
      "learning_rate": 2.9039375424304143e-05,
      "loss": 1.7129,
      "step": 321100
    },
    {
      "epoch": 25.16058279805734,
      "grad_norm": 5.228540420532227,
      "learning_rate": 2.9032847668285552e-05,
      "loss": 1.7442,
      "step": 321200
    },
    {
      "epoch": 25.16841610527965,
      "grad_norm": 5.864494323730469,
      "learning_rate": 2.9026319912266958e-05,
      "loss": 1.7825,
      "step": 321300
    },
    {
      "epoch": 25.17624941250196,
      "grad_norm": 5.695182800292969,
      "learning_rate": 2.901979215624837e-05,
      "loss": 1.6192,
      "step": 321400
    },
    {
      "epoch": 25.184082719724266,
      "grad_norm": 5.695032119750977,
      "learning_rate": 2.901326440022978e-05,
      "loss": 1.6392,
      "step": 321500
    },
    {
      "epoch": 25.191916026946576,
      "grad_norm": 4.712505340576172,
      "learning_rate": 2.9006736644211186e-05,
      "loss": 1.6807,
      "step": 321600
    },
    {
      "epoch": 25.199749334168885,
      "grad_norm": 5.894165515899658,
      "learning_rate": 2.9000208888192598e-05,
      "loss": 1.7419,
      "step": 321700
    },
    {
      "epoch": 25.207582641391195,
      "grad_norm": 6.988150596618652,
      "learning_rate": 2.8993681132174004e-05,
      "loss": 1.7693,
      "step": 321800
    },
    {
      "epoch": 25.215415948613504,
      "grad_norm": 8.477352142333984,
      "learning_rate": 2.8987153376155413e-05,
      "loss": 1.7934,
      "step": 321900
    },
    {
      "epoch": 25.223249255835814,
      "grad_norm": 6.091339588165283,
      "learning_rate": 2.8980625620136826e-05,
      "loss": 1.6768,
      "step": 322000
    },
    {
      "epoch": 25.231082563058123,
      "grad_norm": 5.0557098388671875,
      "learning_rate": 2.897409786411823e-05,
      "loss": 1.7099,
      "step": 322100
    },
    {
      "epoch": 25.238915870280433,
      "grad_norm": 6.35676908493042,
      "learning_rate": 2.896757010809964e-05,
      "loss": 1.8336,
      "step": 322200
    },
    {
      "epoch": 25.246749177502743,
      "grad_norm": 5.948770523071289,
      "learning_rate": 2.8961042352081053e-05,
      "loss": 1.6932,
      "step": 322300
    },
    {
      "epoch": 25.254582484725052,
      "grad_norm": 7.388363838195801,
      "learning_rate": 2.895451459606246e-05,
      "loss": 1.6161,
      "step": 322400
    },
    {
      "epoch": 25.26241579194736,
      "grad_norm": 6.8787031173706055,
      "learning_rate": 2.8947986840043868e-05,
      "loss": 1.6718,
      "step": 322500
    },
    {
      "epoch": 25.270249099169668,
      "grad_norm": 6.988375663757324,
      "learning_rate": 2.8941459084025274e-05,
      "loss": 1.7753,
      "step": 322600
    },
    {
      "epoch": 25.278082406391977,
      "grad_norm": 7.471538543701172,
      "learning_rate": 2.8934931328006686e-05,
      "loss": 1.6778,
      "step": 322700
    },
    {
      "epoch": 25.285915713614287,
      "grad_norm": 7.619792938232422,
      "learning_rate": 2.8928403571988096e-05,
      "loss": 1.6872,
      "step": 322800
    },
    {
      "epoch": 25.293749020836596,
      "grad_norm": 5.180993556976318,
      "learning_rate": 2.89218758159695e-05,
      "loss": 1.759,
      "step": 322900
    },
    {
      "epoch": 25.301582328058906,
      "grad_norm": 8.083141326904297,
      "learning_rate": 2.8915348059950914e-05,
      "loss": 1.6829,
      "step": 323000
    },
    {
      "epoch": 25.309415635281216,
      "grad_norm": 5.284648418426514,
      "learning_rate": 2.8908820303932323e-05,
      "loss": 1.7208,
      "step": 323100
    },
    {
      "epoch": 25.317248942503525,
      "grad_norm": 7.359057426452637,
      "learning_rate": 2.890229254791373e-05,
      "loss": 1.7234,
      "step": 323200
    },
    {
      "epoch": 25.325082249725835,
      "grad_norm": 5.737076759338379,
      "learning_rate": 2.889576479189514e-05,
      "loss": 1.6634,
      "step": 323300
    },
    {
      "epoch": 25.332915556948144,
      "grad_norm": 8.262517929077148,
      "learning_rate": 2.8889237035876547e-05,
      "loss": 1.6427,
      "step": 323400
    },
    {
      "epoch": 25.340748864170454,
      "grad_norm": 5.678173542022705,
      "learning_rate": 2.8882709279857956e-05,
      "loss": 1.7223,
      "step": 323500
    },
    {
      "epoch": 25.348582171392763,
      "grad_norm": 5.449994087219238,
      "learning_rate": 2.887618152383937e-05,
      "loss": 1.687,
      "step": 323600
    },
    {
      "epoch": 25.35641547861507,
      "grad_norm": 6.623538970947266,
      "learning_rate": 2.8869653767820775e-05,
      "loss": 1.629,
      "step": 323700
    },
    {
      "epoch": 25.36424878583738,
      "grad_norm": 6.913210391998291,
      "learning_rate": 2.8863126011802184e-05,
      "loss": 1.7318,
      "step": 323800
    },
    {
      "epoch": 25.37208209305969,
      "grad_norm": 7.063660144805908,
      "learning_rate": 2.8856598255783596e-05,
      "loss": 1.6926,
      "step": 323900
    },
    {
      "epoch": 25.379915400281998,
      "grad_norm": 7.810660362243652,
      "learning_rate": 2.8850070499765002e-05,
      "loss": 1.7004,
      "step": 324000
    },
    {
      "epoch": 25.387748707504308,
      "grad_norm": 6.848188877105713,
      "learning_rate": 2.884354274374641e-05,
      "loss": 1.7787,
      "step": 324100
    },
    {
      "epoch": 25.395582014726617,
      "grad_norm": 5.446656227111816,
      "learning_rate": 2.8837014987727824e-05,
      "loss": 1.6835,
      "step": 324200
    },
    {
      "epoch": 25.403415321948927,
      "grad_norm": 7.3188042640686035,
      "learning_rate": 2.883048723170923e-05,
      "loss": 1.6842,
      "step": 324300
    },
    {
      "epoch": 25.411248629171237,
      "grad_norm": 6.385046005249023,
      "learning_rate": 2.882395947569064e-05,
      "loss": 1.6584,
      "step": 324400
    },
    {
      "epoch": 25.419081936393546,
      "grad_norm": 6.589714527130127,
      "learning_rate": 2.8817431719672044e-05,
      "loss": 1.6557,
      "step": 324500
    },
    {
      "epoch": 25.426915243615856,
      "grad_norm": 8.153048515319824,
      "learning_rate": 2.8810903963653457e-05,
      "loss": 1.6494,
      "step": 324600
    },
    {
      "epoch": 25.434748550838165,
      "grad_norm": 7.396248817443848,
      "learning_rate": 2.8804376207634866e-05,
      "loss": 1.6367,
      "step": 324700
    },
    {
      "epoch": 25.442581858060475,
      "grad_norm": 5.426959037780762,
      "learning_rate": 2.8797848451616272e-05,
      "loss": 1.7699,
      "step": 324800
    },
    {
      "epoch": 25.45041516528278,
      "grad_norm": 6.913524150848389,
      "learning_rate": 2.8791320695597684e-05,
      "loss": 1.7004,
      "step": 324900
    },
    {
      "epoch": 25.45824847250509,
      "grad_norm": 4.791196346282959,
      "learning_rate": 2.878479293957909e-05,
      "loss": 1.6437,
      "step": 325000
    },
    {
      "epoch": 25.4660817797274,
      "grad_norm": 7.2397541999816895,
      "learning_rate": 2.87782651835605e-05,
      "loss": 1.7438,
      "step": 325100
    },
    {
      "epoch": 25.47391508694971,
      "grad_norm": 7.111997604370117,
      "learning_rate": 2.8771737427541912e-05,
      "loss": 1.6526,
      "step": 325200
    },
    {
      "epoch": 25.48174839417202,
      "grad_norm": 7.257874011993408,
      "learning_rate": 2.8765209671523318e-05,
      "loss": 1.7395,
      "step": 325300
    },
    {
      "epoch": 25.48958170139433,
      "grad_norm": 6.09026575088501,
      "learning_rate": 2.8758681915504727e-05,
      "loss": 1.7333,
      "step": 325400
    },
    {
      "epoch": 25.49741500861664,
      "grad_norm": 5.269992351531982,
      "learning_rate": 2.875215415948614e-05,
      "loss": 1.6596,
      "step": 325500
    },
    {
      "epoch": 25.505248315838948,
      "grad_norm": 7.232702255249023,
      "learning_rate": 2.8745626403467545e-05,
      "loss": 1.7647,
      "step": 325600
    },
    {
      "epoch": 25.513081623061257,
      "grad_norm": 5.320744037628174,
      "learning_rate": 2.8739098647448954e-05,
      "loss": 1.6909,
      "step": 325700
    },
    {
      "epoch": 25.520914930283567,
      "grad_norm": 5.8396077156066895,
      "learning_rate": 2.8732570891430367e-05,
      "loss": 1.7904,
      "step": 325800
    },
    {
      "epoch": 25.528748237505877,
      "grad_norm": 4.377099990844727,
      "learning_rate": 2.8726043135411773e-05,
      "loss": 1.8072,
      "step": 325900
    },
    {
      "epoch": 25.536581544728183,
      "grad_norm": 6.141603946685791,
      "learning_rate": 2.8719515379393182e-05,
      "loss": 1.6647,
      "step": 326000
    },
    {
      "epoch": 25.544414851950492,
      "grad_norm": 6.19868278503418,
      "learning_rate": 2.8712987623374588e-05,
      "loss": 1.6339,
      "step": 326100
    },
    {
      "epoch": 25.5522481591728,
      "grad_norm": 7.880746364593506,
      "learning_rate": 2.8706459867356e-05,
      "loss": 1.7652,
      "step": 326200
    },
    {
      "epoch": 25.56008146639511,
      "grad_norm": 10.510912895202637,
      "learning_rate": 2.869993211133741e-05,
      "loss": 1.7192,
      "step": 326300
    },
    {
      "epoch": 25.56791477361742,
      "grad_norm": 5.850913047790527,
      "learning_rate": 2.8693404355318815e-05,
      "loss": 1.7251,
      "step": 326400
    },
    {
      "epoch": 25.57574808083973,
      "grad_norm": 7.597745895385742,
      "learning_rate": 2.8686876599300228e-05,
      "loss": 1.6934,
      "step": 326500
    },
    {
      "epoch": 25.58358138806204,
      "grad_norm": 7.498904228210449,
      "learning_rate": 2.8680348843281633e-05,
      "loss": 1.6985,
      "step": 326600
    },
    {
      "epoch": 25.59141469528435,
      "grad_norm": 5.572516441345215,
      "learning_rate": 2.8673821087263042e-05,
      "loss": 1.6499,
      "step": 326700
    },
    {
      "epoch": 25.59924800250666,
      "grad_norm": 6.396934509277344,
      "learning_rate": 2.8667293331244455e-05,
      "loss": 1.655,
      "step": 326800
    },
    {
      "epoch": 25.60708130972897,
      "grad_norm": 4.282068252563477,
      "learning_rate": 2.866076557522586e-05,
      "loss": 1.6966,
      "step": 326900
    },
    {
      "epoch": 25.61491461695128,
      "grad_norm": 7.5861101150512695,
      "learning_rate": 2.865423781920727e-05,
      "loss": 1.6342,
      "step": 327000
    },
    {
      "epoch": 25.622747924173588,
      "grad_norm": 6.681838035583496,
      "learning_rate": 2.8647710063188682e-05,
      "loss": 1.672,
      "step": 327100
    },
    {
      "epoch": 25.630581231395894,
      "grad_norm": 5.968194961547852,
      "learning_rate": 2.8641182307170088e-05,
      "loss": 1.6919,
      "step": 327200
    },
    {
      "epoch": 25.638414538618203,
      "grad_norm": 5.34242582321167,
      "learning_rate": 2.8634654551151497e-05,
      "loss": 1.6687,
      "step": 327300
    },
    {
      "epoch": 25.646247845840513,
      "grad_norm": 6.766563415527344,
      "learning_rate": 2.862812679513291e-05,
      "loss": 1.7262,
      "step": 327400
    },
    {
      "epoch": 25.654081153062823,
      "grad_norm": 5.360467910766602,
      "learning_rate": 2.8621599039114316e-05,
      "loss": 1.8182,
      "step": 327500
    },
    {
      "epoch": 25.661914460285132,
      "grad_norm": 6.04625940322876,
      "learning_rate": 2.8615071283095725e-05,
      "loss": 1.7024,
      "step": 327600
    },
    {
      "epoch": 25.66974776750744,
      "grad_norm": 6.180078983306885,
      "learning_rate": 2.860854352707713e-05,
      "loss": 1.7257,
      "step": 327700
    },
    {
      "epoch": 25.67758107472975,
      "grad_norm": 6.672821044921875,
      "learning_rate": 2.8602015771058543e-05,
      "loss": 1.6494,
      "step": 327800
    },
    {
      "epoch": 25.68541438195206,
      "grad_norm": 6.966070652008057,
      "learning_rate": 2.8595488015039952e-05,
      "loss": 1.7029,
      "step": 327900
    },
    {
      "epoch": 25.69324768917437,
      "grad_norm": 5.871072769165039,
      "learning_rate": 2.8588960259021358e-05,
      "loss": 1.749,
      "step": 328000
    },
    {
      "epoch": 25.70108099639668,
      "grad_norm": 6.759286880493164,
      "learning_rate": 2.858243250300277e-05,
      "loss": 1.7272,
      "step": 328100
    },
    {
      "epoch": 25.70891430361899,
      "grad_norm": 7.558340549468994,
      "learning_rate": 2.8575904746984176e-05,
      "loss": 1.6357,
      "step": 328200
    },
    {
      "epoch": 25.716747610841296,
      "grad_norm": 7.532166957855225,
      "learning_rate": 2.8569376990965586e-05,
      "loss": 1.6678,
      "step": 328300
    },
    {
      "epoch": 25.724580918063605,
      "grad_norm": 5.833254337310791,
      "learning_rate": 2.8562849234946998e-05,
      "loss": 1.6634,
      "step": 328400
    },
    {
      "epoch": 25.732414225285915,
      "grad_norm": 9.346339225769043,
      "learning_rate": 2.8556321478928404e-05,
      "loss": 1.6851,
      "step": 328500
    },
    {
      "epoch": 25.740247532508224,
      "grad_norm": 6.047585964202881,
      "learning_rate": 2.8549793722909813e-05,
      "loss": 1.7379,
      "step": 328600
    },
    {
      "epoch": 25.748080839730534,
      "grad_norm": 5.984462738037109,
      "learning_rate": 2.8543265966891226e-05,
      "loss": 1.7802,
      "step": 328700
    },
    {
      "epoch": 25.755914146952843,
      "grad_norm": 6.1883697509765625,
      "learning_rate": 2.853673821087263e-05,
      "loss": 1.653,
      "step": 328800
    },
    {
      "epoch": 25.763747454175153,
      "grad_norm": 7.54331636428833,
      "learning_rate": 2.853021045485404e-05,
      "loss": 1.689,
      "step": 328900
    },
    {
      "epoch": 25.771580761397463,
      "grad_norm": 9.316981315612793,
      "learning_rate": 2.8523682698835453e-05,
      "loss": 1.6811,
      "step": 329000
    },
    {
      "epoch": 25.779414068619772,
      "grad_norm": 6.076673984527588,
      "learning_rate": 2.851715494281686e-05,
      "loss": 1.7368,
      "step": 329100
    },
    {
      "epoch": 25.78724737584208,
      "grad_norm": 5.871100902557373,
      "learning_rate": 2.8510627186798268e-05,
      "loss": 1.7257,
      "step": 329200
    },
    {
      "epoch": 25.79508068306439,
      "grad_norm": 7.076476097106934,
      "learning_rate": 2.8504099430779674e-05,
      "loss": 1.7534,
      "step": 329300
    },
    {
      "epoch": 25.802913990286697,
      "grad_norm": 5.835281848907471,
      "learning_rate": 2.8497571674761086e-05,
      "loss": 1.7272,
      "step": 329400
    },
    {
      "epoch": 25.810747297509007,
      "grad_norm": 12.338935852050781,
      "learning_rate": 2.8491043918742495e-05,
      "loss": 1.8083,
      "step": 329500
    },
    {
      "epoch": 25.818580604731316,
      "grad_norm": 10.234487533569336,
      "learning_rate": 2.84845161627239e-05,
      "loss": 1.6845,
      "step": 329600
    },
    {
      "epoch": 25.826413911953626,
      "grad_norm": 6.735688209533691,
      "learning_rate": 2.8477988406705314e-05,
      "loss": 1.7901,
      "step": 329700
    },
    {
      "epoch": 25.834247219175936,
      "grad_norm": 6.9281086921691895,
      "learning_rate": 2.847146065068672e-05,
      "loss": 1.6048,
      "step": 329800
    },
    {
      "epoch": 25.842080526398245,
      "grad_norm": 6.511399269104004,
      "learning_rate": 2.846493289466813e-05,
      "loss": 1.67,
      "step": 329900
    },
    {
      "epoch": 25.849913833620555,
      "grad_norm": 3.945951223373413,
      "learning_rate": 2.845840513864954e-05,
      "loss": 1.6687,
      "step": 330000
    },
    {
      "epoch": 25.857747140842864,
      "grad_norm": 5.6228837966918945,
      "learning_rate": 2.8451877382630947e-05,
      "loss": 1.6378,
      "step": 330100
    },
    {
      "epoch": 25.865580448065174,
      "grad_norm": 6.134608745574951,
      "learning_rate": 2.8445349626612356e-05,
      "loss": 1.7029,
      "step": 330200
    },
    {
      "epoch": 25.873413755287483,
      "grad_norm": 5.955774307250977,
      "learning_rate": 2.843882187059377e-05,
      "loss": 1.6505,
      "step": 330300
    },
    {
      "epoch": 25.881247062509793,
      "grad_norm": 6.497199058532715,
      "learning_rate": 2.8432294114575174e-05,
      "loss": 1.6861,
      "step": 330400
    },
    {
      "epoch": 25.889080369732103,
      "grad_norm": 6.658634185791016,
      "learning_rate": 2.8425766358556584e-05,
      "loss": 1.6667,
      "step": 330500
    },
    {
      "epoch": 25.89691367695441,
      "grad_norm": 6.2889323234558105,
      "learning_rate": 2.8419238602537996e-05,
      "loss": 1.6993,
      "step": 330600
    },
    {
      "epoch": 25.904746984176718,
      "grad_norm": 4.40130090713501,
      "learning_rate": 2.8412710846519402e-05,
      "loss": 1.7936,
      "step": 330700
    },
    {
      "epoch": 25.912580291399028,
      "grad_norm": 9.528058052062988,
      "learning_rate": 2.840618309050081e-05,
      "loss": 1.6858,
      "step": 330800
    },
    {
      "epoch": 25.920413598621337,
      "grad_norm": 6.5876240730285645,
      "learning_rate": 2.8399655334482224e-05,
      "loss": 1.6912,
      "step": 330900
    },
    {
      "epoch": 25.928246905843647,
      "grad_norm": 7.2178802490234375,
      "learning_rate": 2.839312757846363e-05,
      "loss": 1.7664,
      "step": 331000
    },
    {
      "epoch": 25.936080213065956,
      "grad_norm": 5.41940975189209,
      "learning_rate": 2.838659982244504e-05,
      "loss": 1.7154,
      "step": 331100
    },
    {
      "epoch": 25.943913520288266,
      "grad_norm": 8.79753589630127,
      "learning_rate": 2.8380072066426444e-05,
      "loss": 1.7148,
      "step": 331200
    },
    {
      "epoch": 25.951746827510576,
      "grad_norm": 7.540628433227539,
      "learning_rate": 2.8373544310407857e-05,
      "loss": 1.7424,
      "step": 331300
    },
    {
      "epoch": 25.959580134732885,
      "grad_norm": 4.93049430847168,
      "learning_rate": 2.8367016554389263e-05,
      "loss": 1.6846,
      "step": 331400
    },
    {
      "epoch": 25.967413441955195,
      "grad_norm": 6.995084285736084,
      "learning_rate": 2.8360488798370672e-05,
      "loss": 1.6694,
      "step": 331500
    },
    {
      "epoch": 25.975246749177504,
      "grad_norm": 5.22654914855957,
      "learning_rate": 2.8353961042352084e-05,
      "loss": 1.801,
      "step": 331600
    },
    {
      "epoch": 25.98308005639981,
      "grad_norm": 6.774546146392822,
      "learning_rate": 2.834743328633349e-05,
      "loss": 1.7923,
      "step": 331700
    },
    {
      "epoch": 25.99091336362212,
      "grad_norm": 5.03398323059082,
      "learning_rate": 2.83409055303149e-05,
      "loss": 1.7448,
      "step": 331800
    },
    {
      "epoch": 25.99874667084443,
      "grad_norm": 5.97454833984375,
      "learning_rate": 2.8334377774296312e-05,
      "loss": 1.6559,
      "step": 331900
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.7873204946517944,
      "eval_runtime": 2.8671,
      "eval_samples_per_second": 234.382,
      "eval_steps_per_second": 234.382,
      "step": 331916
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.464337944984436,
      "eval_runtime": 55.6301,
      "eval_samples_per_second": 229.48,
      "eval_steps_per_second": 229.48,
      "step": 331916
    },
    {
      "epoch": 26.00657997806674,
      "grad_norm": 5.551685333251953,
      "learning_rate": 2.8327850018277718e-05,
      "loss": 1.6701,
      "step": 332000
    },
    {
      "epoch": 26.01441328528905,
      "grad_norm": 6.070644378662109,
      "learning_rate": 2.8321322262259127e-05,
      "loss": 1.7066,
      "step": 332100
    },
    {
      "epoch": 26.022246592511358,
      "grad_norm": 6.071122646331787,
      "learning_rate": 2.831479450624054e-05,
      "loss": 1.7681,
      "step": 332200
    },
    {
      "epoch": 26.030079899733668,
      "grad_norm": 6.2623701095581055,
      "learning_rate": 2.8308266750221945e-05,
      "loss": 1.6854,
      "step": 332300
    },
    {
      "epoch": 26.037913206955977,
      "grad_norm": 7.253583908081055,
      "learning_rate": 2.8301738994203354e-05,
      "loss": 1.7216,
      "step": 332400
    },
    {
      "epoch": 26.045746514178287,
      "grad_norm": 5.233240604400635,
      "learning_rate": 2.8295211238184767e-05,
      "loss": 1.6733,
      "step": 332500
    },
    {
      "epoch": 26.053579821400596,
      "grad_norm": 5.677178859710693,
      "learning_rate": 2.8288683482166173e-05,
      "loss": 1.6372,
      "step": 332600
    },
    {
      "epoch": 26.061413128622906,
      "grad_norm": 9.541971206665039,
      "learning_rate": 2.828215572614758e-05,
      "loss": 1.729,
      "step": 332700
    },
    {
      "epoch": 26.069246435845216,
      "grad_norm": 8.71751880645752,
      "learning_rate": 2.8275627970128987e-05,
      "loss": 1.6598,
      "step": 332800
    },
    {
      "epoch": 26.07707974306752,
      "grad_norm": 5.427886009216309,
      "learning_rate": 2.82691002141104e-05,
      "loss": 1.5883,
      "step": 332900
    },
    {
      "epoch": 26.08491305028983,
      "grad_norm": 5.170170783996582,
      "learning_rate": 2.8262572458091806e-05,
      "loss": 1.6561,
      "step": 333000
    },
    {
      "epoch": 26.09274635751214,
      "grad_norm": 4.998543739318848,
      "learning_rate": 2.8256044702073215e-05,
      "loss": 1.6494,
      "step": 333100
    },
    {
      "epoch": 26.10057966473445,
      "grad_norm": 7.50977087020874,
      "learning_rate": 2.8249516946054627e-05,
      "loss": 1.7008,
      "step": 333200
    },
    {
      "epoch": 26.10841297195676,
      "grad_norm": 6.08302640914917,
      "learning_rate": 2.8242989190036033e-05,
      "loss": 1.6816,
      "step": 333300
    },
    {
      "epoch": 26.11624627917907,
      "grad_norm": 5.21246337890625,
      "learning_rate": 2.8236461434017442e-05,
      "loss": 1.7016,
      "step": 333400
    },
    {
      "epoch": 26.12407958640138,
      "grad_norm": 5.850960731506348,
      "learning_rate": 2.8229933677998855e-05,
      "loss": 1.7996,
      "step": 333500
    },
    {
      "epoch": 26.13191289362369,
      "grad_norm": 5.859346866607666,
      "learning_rate": 2.822340592198026e-05,
      "loss": 1.6862,
      "step": 333600
    },
    {
      "epoch": 26.139746200845998,
      "grad_norm": 6.040102005004883,
      "learning_rate": 2.821687816596167e-05,
      "loss": 1.6729,
      "step": 333700
    },
    {
      "epoch": 26.147579508068308,
      "grad_norm": 6.308354377746582,
      "learning_rate": 2.8210350409943082e-05,
      "loss": 1.6739,
      "step": 333800
    },
    {
      "epoch": 26.155412815290617,
      "grad_norm": 4.445935249328613,
      "learning_rate": 2.8203822653924488e-05,
      "loss": 1.7229,
      "step": 333900
    },
    {
      "epoch": 26.163246122512923,
      "grad_norm": 5.297983169555664,
      "learning_rate": 2.8197294897905897e-05,
      "loss": 1.6952,
      "step": 334000
    },
    {
      "epoch": 26.171079429735233,
      "grad_norm": 7.30648136138916,
      "learning_rate": 2.819076714188731e-05,
      "loss": 1.7195,
      "step": 334100
    },
    {
      "epoch": 26.178912736957543,
      "grad_norm": 5.035122394561768,
      "learning_rate": 2.8184239385868716e-05,
      "loss": 1.6659,
      "step": 334200
    },
    {
      "epoch": 26.186746044179852,
      "grad_norm": 6.953337669372559,
      "learning_rate": 2.8177711629850125e-05,
      "loss": 1.6929,
      "step": 334300
    },
    {
      "epoch": 26.19457935140216,
      "grad_norm": 7.297478675842285,
      "learning_rate": 2.817118387383153e-05,
      "loss": 1.7079,
      "step": 334400
    },
    {
      "epoch": 26.20241265862447,
      "grad_norm": 9.068733215332031,
      "learning_rate": 2.8164656117812943e-05,
      "loss": 1.6829,
      "step": 334500
    },
    {
      "epoch": 26.21024596584678,
      "grad_norm": 7.707092761993408,
      "learning_rate": 2.815812836179435e-05,
      "loss": 1.7591,
      "step": 334600
    },
    {
      "epoch": 26.21807927306909,
      "grad_norm": 7.264258861541748,
      "learning_rate": 2.8151600605775758e-05,
      "loss": 1.6368,
      "step": 334700
    },
    {
      "epoch": 26.2259125802914,
      "grad_norm": 6.2529401779174805,
      "learning_rate": 2.814507284975717e-05,
      "loss": 1.7105,
      "step": 334800
    },
    {
      "epoch": 26.23374588751371,
      "grad_norm": 9.246849060058594,
      "learning_rate": 2.8138545093738576e-05,
      "loss": 1.6668,
      "step": 334900
    },
    {
      "epoch": 26.24157919473602,
      "grad_norm": 6.4509382247924805,
      "learning_rate": 2.8132017337719985e-05,
      "loss": 1.5918,
      "step": 335000
    },
    {
      "epoch": 26.249412501958325,
      "grad_norm": 7.2009453773498535,
      "learning_rate": 2.8125489581701398e-05,
      "loss": 1.7564,
      "step": 335100
    },
    {
      "epoch": 26.257245809180635,
      "grad_norm": 7.875260829925537,
      "learning_rate": 2.8118961825682804e-05,
      "loss": 1.651,
      "step": 335200
    },
    {
      "epoch": 26.265079116402944,
      "grad_norm": 5.976859092712402,
      "learning_rate": 2.8112434069664213e-05,
      "loss": 1.7999,
      "step": 335300
    },
    {
      "epoch": 26.272912423625254,
      "grad_norm": 7.892162322998047,
      "learning_rate": 2.8105906313645626e-05,
      "loss": 1.6553,
      "step": 335400
    },
    {
      "epoch": 26.280745730847563,
      "grad_norm": 5.317221641540527,
      "learning_rate": 2.809937855762703e-05,
      "loss": 1.7356,
      "step": 335500
    },
    {
      "epoch": 26.288579038069873,
      "grad_norm": 5.953667163848877,
      "learning_rate": 2.809285080160844e-05,
      "loss": 1.708,
      "step": 335600
    },
    {
      "epoch": 26.296412345292183,
      "grad_norm": 4.967347621917725,
      "learning_rate": 2.8086323045589853e-05,
      "loss": 1.6633,
      "step": 335700
    },
    {
      "epoch": 26.304245652514492,
      "grad_norm": 5.4342474937438965,
      "learning_rate": 2.807979528957126e-05,
      "loss": 1.7589,
      "step": 335800
    },
    {
      "epoch": 26.3120789597368,
      "grad_norm": 6.636502265930176,
      "learning_rate": 2.8073267533552668e-05,
      "loss": 1.7006,
      "step": 335900
    },
    {
      "epoch": 26.31991226695911,
      "grad_norm": 6.126121997833252,
      "learning_rate": 2.8066739777534074e-05,
      "loss": 1.6643,
      "step": 336000
    },
    {
      "epoch": 26.32774557418142,
      "grad_norm": 4.860569477081299,
      "learning_rate": 2.8060212021515486e-05,
      "loss": 1.7433,
      "step": 336100
    },
    {
      "epoch": 26.33557888140373,
      "grad_norm": 8.741573333740234,
      "learning_rate": 2.8053684265496892e-05,
      "loss": 1.7669,
      "step": 336200
    },
    {
      "epoch": 26.343412188626036,
      "grad_norm": 6.492733478546143,
      "learning_rate": 2.80471565094783e-05,
      "loss": 1.7006,
      "step": 336300
    },
    {
      "epoch": 26.351245495848346,
      "grad_norm": 5.95454216003418,
      "learning_rate": 2.8040628753459714e-05,
      "loss": 1.7927,
      "step": 336400
    },
    {
      "epoch": 26.359078803070656,
      "grad_norm": 6.551870346069336,
      "learning_rate": 2.803410099744112e-05,
      "loss": 1.7536,
      "step": 336500
    },
    {
      "epoch": 26.366912110292965,
      "grad_norm": 5.929382801055908,
      "learning_rate": 2.802757324142253e-05,
      "loss": 1.753,
      "step": 336600
    },
    {
      "epoch": 26.374745417515275,
      "grad_norm": 5.879663944244385,
      "learning_rate": 2.802104548540394e-05,
      "loss": 1.7118,
      "step": 336700
    },
    {
      "epoch": 26.382578724737584,
      "grad_norm": 4.7188591957092285,
      "learning_rate": 2.8014517729385347e-05,
      "loss": 1.7208,
      "step": 336800
    },
    {
      "epoch": 26.390412031959894,
      "grad_norm": 6.5447306632995605,
      "learning_rate": 2.8007989973366756e-05,
      "loss": 1.815,
      "step": 336900
    },
    {
      "epoch": 26.398245339182203,
      "grad_norm": 8.064010620117188,
      "learning_rate": 2.800146221734817e-05,
      "loss": 1.6856,
      "step": 337000
    },
    {
      "epoch": 26.406078646404513,
      "grad_norm": 7.3090314865112305,
      "learning_rate": 2.7994934461329574e-05,
      "loss": 1.6378,
      "step": 337100
    },
    {
      "epoch": 26.413911953626823,
      "grad_norm": 6.610480308532715,
      "learning_rate": 2.7988406705310984e-05,
      "loss": 1.7309,
      "step": 337200
    },
    {
      "epoch": 26.421745260849132,
      "grad_norm": 5.535432815551758,
      "learning_rate": 2.7981878949292396e-05,
      "loss": 1.7044,
      "step": 337300
    },
    {
      "epoch": 26.429578568071438,
      "grad_norm": 6.805323600769043,
      "learning_rate": 2.7975351193273802e-05,
      "loss": 1.7367,
      "step": 337400
    },
    {
      "epoch": 26.437411875293748,
      "grad_norm": 5.533208847045898,
      "learning_rate": 2.796882343725521e-05,
      "loss": 1.6657,
      "step": 337500
    },
    {
      "epoch": 26.445245182516057,
      "grad_norm": 7.996016025543213,
      "learning_rate": 2.7962295681236624e-05,
      "loss": 1.7029,
      "step": 337600
    },
    {
      "epoch": 26.453078489738367,
      "grad_norm": 6.563424587249756,
      "learning_rate": 2.795576792521803e-05,
      "loss": 1.7468,
      "step": 337700
    },
    {
      "epoch": 26.460911796960676,
      "grad_norm": 8.16661548614502,
      "learning_rate": 2.7949240169199435e-05,
      "loss": 1.687,
      "step": 337800
    },
    {
      "epoch": 26.468745104182986,
      "grad_norm": 5.4685468673706055,
      "learning_rate": 2.7942712413180844e-05,
      "loss": 1.6558,
      "step": 337900
    },
    {
      "epoch": 26.476578411405296,
      "grad_norm": 6.717013359069824,
      "learning_rate": 2.7936184657162257e-05,
      "loss": 1.63,
      "step": 338000
    },
    {
      "epoch": 26.484411718627605,
      "grad_norm": 7.827563762664795,
      "learning_rate": 2.7929656901143663e-05,
      "loss": 1.7674,
      "step": 338100
    },
    {
      "epoch": 26.492245025849915,
      "grad_norm": 5.576962947845459,
      "learning_rate": 2.7923129145125072e-05,
      "loss": 1.7166,
      "step": 338200
    },
    {
      "epoch": 26.500078333072224,
      "grad_norm": 7.469271659851074,
      "learning_rate": 2.7916601389106484e-05,
      "loss": 1.7105,
      "step": 338300
    },
    {
      "epoch": 26.507911640294534,
      "grad_norm": 6.491005897521973,
      "learning_rate": 2.791007363308789e-05,
      "loss": 1.6282,
      "step": 338400
    },
    {
      "epoch": 26.515744947516843,
      "grad_norm": 5.916766166687012,
      "learning_rate": 2.79035458770693e-05,
      "loss": 1.6347,
      "step": 338500
    },
    {
      "epoch": 26.52357825473915,
      "grad_norm": 6.553427219390869,
      "learning_rate": 2.7897018121050712e-05,
      "loss": 1.7048,
      "step": 338600
    },
    {
      "epoch": 26.53141156196146,
      "grad_norm": 10.080180168151855,
      "learning_rate": 2.7890490365032118e-05,
      "loss": 1.6127,
      "step": 338700
    },
    {
      "epoch": 26.53924486918377,
      "grad_norm": 8.762929916381836,
      "learning_rate": 2.7883962609013527e-05,
      "loss": 1.6777,
      "step": 338800
    },
    {
      "epoch": 26.547078176406078,
      "grad_norm": 5.814288139343262,
      "learning_rate": 2.787743485299494e-05,
      "loss": 1.6429,
      "step": 338900
    },
    {
      "epoch": 26.554911483628388,
      "grad_norm": 6.587350845336914,
      "learning_rate": 2.7870907096976345e-05,
      "loss": 1.6841,
      "step": 339000
    },
    {
      "epoch": 26.562744790850697,
      "grad_norm": 6.440980911254883,
      "learning_rate": 2.7864379340957754e-05,
      "loss": 1.7026,
      "step": 339100
    },
    {
      "epoch": 26.570578098073007,
      "grad_norm": 5.695816993713379,
      "learning_rate": 2.7857851584939167e-05,
      "loss": 1.7249,
      "step": 339200
    },
    {
      "epoch": 26.578411405295316,
      "grad_norm": 5.998441219329834,
      "learning_rate": 2.7851323828920572e-05,
      "loss": 1.7273,
      "step": 339300
    },
    {
      "epoch": 26.586244712517626,
      "grad_norm": 6.259095668792725,
      "learning_rate": 2.7844796072901978e-05,
      "loss": 1.6559,
      "step": 339400
    },
    {
      "epoch": 26.594078019739936,
      "grad_norm": 6.230983257293701,
      "learning_rate": 2.7838268316883387e-05,
      "loss": 1.6981,
      "step": 339500
    },
    {
      "epoch": 26.601911326962245,
      "grad_norm": 5.576531410217285,
      "learning_rate": 2.78317405608648e-05,
      "loss": 1.7677,
      "step": 339600
    },
    {
      "epoch": 26.60974463418455,
      "grad_norm": 5.226507663726807,
      "learning_rate": 2.7825212804846206e-05,
      "loss": 1.7381,
      "step": 339700
    },
    {
      "epoch": 26.61757794140686,
      "grad_norm": 6.671957492828369,
      "learning_rate": 2.7818685048827615e-05,
      "loss": 1.7092,
      "step": 339800
    },
    {
      "epoch": 26.62541124862917,
      "grad_norm": 7.507387638092041,
      "learning_rate": 2.7812157292809027e-05,
      "loss": 1.768,
      "step": 339900
    },
    {
      "epoch": 26.63324455585148,
      "grad_norm": 7.432920455932617,
      "learning_rate": 2.7805629536790433e-05,
      "loss": 1.6628,
      "step": 340000
    },
    {
      "epoch": 26.64107786307379,
      "grad_norm": 6.284633159637451,
      "learning_rate": 2.7799101780771842e-05,
      "loss": 1.642,
      "step": 340100
    },
    {
      "epoch": 26.6489111702961,
      "grad_norm": 4.90413236618042,
      "learning_rate": 2.7792574024753255e-05,
      "loss": 1.7006,
      "step": 340200
    },
    {
      "epoch": 26.65674447751841,
      "grad_norm": 6.482367515563965,
      "learning_rate": 2.778604626873466e-05,
      "loss": 1.7362,
      "step": 340300
    },
    {
      "epoch": 26.664577784740718,
      "grad_norm": 5.366489410400391,
      "learning_rate": 2.777951851271607e-05,
      "loss": 1.7122,
      "step": 340400
    },
    {
      "epoch": 26.672411091963028,
      "grad_norm": 6.295133113861084,
      "learning_rate": 2.7772990756697482e-05,
      "loss": 1.6865,
      "step": 340500
    },
    {
      "epoch": 26.680244399185337,
      "grad_norm": 5.34132194519043,
      "learning_rate": 2.7766463000678888e-05,
      "loss": 1.7223,
      "step": 340600
    },
    {
      "epoch": 26.688077706407647,
      "grad_norm": 3.5718908309936523,
      "learning_rate": 2.7759935244660297e-05,
      "loss": 1.7064,
      "step": 340700
    },
    {
      "epoch": 26.695911013629953,
      "grad_norm": 6.03391695022583,
      "learning_rate": 2.775340748864171e-05,
      "loss": 1.6583,
      "step": 340800
    },
    {
      "epoch": 26.703744320852262,
      "grad_norm": 5.69860315322876,
      "learning_rate": 2.7746879732623116e-05,
      "loss": 1.6479,
      "step": 340900
    },
    {
      "epoch": 26.711577628074572,
      "grad_norm": 8.327921867370605,
      "learning_rate": 2.774035197660452e-05,
      "loss": 1.7088,
      "step": 341000
    },
    {
      "epoch": 26.71941093529688,
      "grad_norm": 7.1801066398620605,
      "learning_rate": 2.773382422058593e-05,
      "loss": 1.6456,
      "step": 341100
    },
    {
      "epoch": 26.72724424251919,
      "grad_norm": 4.910275459289551,
      "learning_rate": 2.7727296464567343e-05,
      "loss": 1.7632,
      "step": 341200
    },
    {
      "epoch": 26.7350775497415,
      "grad_norm": 8.615582466125488,
      "learning_rate": 2.772076870854875e-05,
      "loss": 1.6935,
      "step": 341300
    },
    {
      "epoch": 26.74291085696381,
      "grad_norm": 5.298538684844971,
      "learning_rate": 2.7714240952530158e-05,
      "loss": 1.6632,
      "step": 341400
    },
    {
      "epoch": 26.75074416418612,
      "grad_norm": 5.070774555206299,
      "learning_rate": 2.770771319651157e-05,
      "loss": 1.7316,
      "step": 341500
    },
    {
      "epoch": 26.75857747140843,
      "grad_norm": 5.401425838470459,
      "learning_rate": 2.7701185440492976e-05,
      "loss": 1.6851,
      "step": 341600
    },
    {
      "epoch": 26.76641077863074,
      "grad_norm": 6.571678638458252,
      "learning_rate": 2.7694657684474385e-05,
      "loss": 1.6075,
      "step": 341700
    },
    {
      "epoch": 26.77424408585305,
      "grad_norm": 5.622014045715332,
      "learning_rate": 2.7688129928455798e-05,
      "loss": 1.6849,
      "step": 341800
    },
    {
      "epoch": 26.782077393075355,
      "grad_norm": 5.739011287689209,
      "learning_rate": 2.7681602172437204e-05,
      "loss": 1.721,
      "step": 341900
    },
    {
      "epoch": 26.789910700297664,
      "grad_norm": 6.144694805145264,
      "learning_rate": 2.7675074416418613e-05,
      "loss": 1.775,
      "step": 342000
    },
    {
      "epoch": 26.797744007519974,
      "grad_norm": 6.420528411865234,
      "learning_rate": 2.7668546660400025e-05,
      "loss": 1.7021,
      "step": 342100
    },
    {
      "epoch": 26.805577314742283,
      "grad_norm": 5.65159273147583,
      "learning_rate": 2.766201890438143e-05,
      "loss": 1.7486,
      "step": 342200
    },
    {
      "epoch": 26.813410621964593,
      "grad_norm": 7.152499198913574,
      "learning_rate": 2.765549114836284e-05,
      "loss": 1.7014,
      "step": 342300
    },
    {
      "epoch": 26.821243929186902,
      "grad_norm": 5.482458114624023,
      "learning_rate": 2.7648963392344253e-05,
      "loss": 1.6881,
      "step": 342400
    },
    {
      "epoch": 26.829077236409212,
      "grad_norm": 8.000720977783203,
      "learning_rate": 2.764243563632566e-05,
      "loss": 1.7042,
      "step": 342500
    },
    {
      "epoch": 26.83691054363152,
      "grad_norm": 5.078714847564697,
      "learning_rate": 2.7635907880307064e-05,
      "loss": 1.6666,
      "step": 342600
    },
    {
      "epoch": 26.84474385085383,
      "grad_norm": 4.279018402099609,
      "learning_rate": 2.7629380124288477e-05,
      "loss": 1.6743,
      "step": 342700
    },
    {
      "epoch": 26.85257715807614,
      "grad_norm": 6.941052436828613,
      "learning_rate": 2.7622852368269886e-05,
      "loss": 1.6635,
      "step": 342800
    },
    {
      "epoch": 26.86041046529845,
      "grad_norm": 6.452537536621094,
      "learning_rate": 2.7616324612251292e-05,
      "loss": 1.7139,
      "step": 342900
    },
    {
      "epoch": 26.86824377252076,
      "grad_norm": 6.435547351837158,
      "learning_rate": 2.76097968562327e-05,
      "loss": 1.7575,
      "step": 343000
    },
    {
      "epoch": 26.876077079743066,
      "grad_norm": 7.372215747833252,
      "learning_rate": 2.7603269100214114e-05,
      "loss": 1.6743,
      "step": 343100
    },
    {
      "epoch": 26.883910386965375,
      "grad_norm": 4.316598892211914,
      "learning_rate": 2.759674134419552e-05,
      "loss": 1.6355,
      "step": 343200
    },
    {
      "epoch": 26.891743694187685,
      "grad_norm": 6.747611999511719,
      "learning_rate": 2.759021358817693e-05,
      "loss": 1.7386,
      "step": 343300
    },
    {
      "epoch": 26.899577001409995,
      "grad_norm": 5.276968002319336,
      "learning_rate": 2.758368583215834e-05,
      "loss": 1.7254,
      "step": 343400
    },
    {
      "epoch": 26.907410308632304,
      "grad_norm": 5.860980033874512,
      "learning_rate": 2.7577158076139747e-05,
      "loss": 1.6542,
      "step": 343500
    },
    {
      "epoch": 26.915243615854614,
      "grad_norm": 5.502077102661133,
      "learning_rate": 2.7570630320121156e-05,
      "loss": 1.7346,
      "step": 343600
    },
    {
      "epoch": 26.923076923076923,
      "grad_norm": 6.343630790710449,
      "learning_rate": 2.756410256410257e-05,
      "loss": 1.6965,
      "step": 343700
    },
    {
      "epoch": 26.930910230299233,
      "grad_norm": 5.785621166229248,
      "learning_rate": 2.7557574808083974e-05,
      "loss": 1.6571,
      "step": 343800
    },
    {
      "epoch": 26.938743537521542,
      "grad_norm": 5.230284690856934,
      "learning_rate": 2.7551047052065383e-05,
      "loss": 1.7774,
      "step": 343900
    },
    {
      "epoch": 26.946576844743852,
      "grad_norm": 5.842126369476318,
      "learning_rate": 2.7544519296046796e-05,
      "loss": 1.7994,
      "step": 344000
    },
    {
      "epoch": 26.95441015196616,
      "grad_norm": 5.523085594177246,
      "learning_rate": 2.7537991540028202e-05,
      "loss": 1.7345,
      "step": 344100
    },
    {
      "epoch": 26.962243459188468,
      "grad_norm": 8.775959014892578,
      "learning_rate": 2.7531463784009608e-05,
      "loss": 1.6752,
      "step": 344200
    },
    {
      "epoch": 26.970076766410777,
      "grad_norm": 4.933228015899658,
      "learning_rate": 2.752493602799102e-05,
      "loss": 1.7357,
      "step": 344300
    },
    {
      "epoch": 26.977910073633087,
      "grad_norm": 6.391910552978516,
      "learning_rate": 2.751840827197243e-05,
      "loss": 1.7079,
      "step": 344400
    },
    {
      "epoch": 26.985743380855396,
      "grad_norm": 6.770281791687012,
      "learning_rate": 2.7511880515953835e-05,
      "loss": 1.6956,
      "step": 344500
    },
    {
      "epoch": 26.993576688077706,
      "grad_norm": 7.04901647567749,
      "learning_rate": 2.7505352759935244e-05,
      "loss": 1.7543,
      "step": 344600
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.7850737571716309,
      "eval_runtime": 2.9196,
      "eval_samples_per_second": 230.167,
      "eval_steps_per_second": 230.167,
      "step": 344682
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.4577950239181519,
      "eval_runtime": 39.0068,
      "eval_samples_per_second": 327.276,
      "eval_steps_per_second": 327.276,
      "step": 344682
    },
    {
      "epoch": 27.001409995300016,
      "grad_norm": 7.361001968383789,
      "learning_rate": 2.7498825003916657e-05,
      "loss": 1.6665,
      "step": 344700
    },
    {
      "epoch": 27.009243302522325,
      "grad_norm": 9.241270065307617,
      "learning_rate": 2.7492297247898062e-05,
      "loss": 1.7233,
      "step": 344800
    },
    {
      "epoch": 27.017076609744635,
      "grad_norm": 5.4423675537109375,
      "learning_rate": 2.748576949187947e-05,
      "loss": 1.6556,
      "step": 344900
    },
    {
      "epoch": 27.024909916966944,
      "grad_norm": 6.775557994842529,
      "learning_rate": 2.7479241735860884e-05,
      "loss": 1.6245,
      "step": 345000
    },
    {
      "epoch": 27.032743224189254,
      "grad_norm": 5.30014181137085,
      "learning_rate": 2.747271397984229e-05,
      "loss": 1.6813,
      "step": 345100
    },
    {
      "epoch": 27.040576531411563,
      "grad_norm": 5.596435070037842,
      "learning_rate": 2.74661862238237e-05,
      "loss": 1.7274,
      "step": 345200
    },
    {
      "epoch": 27.048409838633873,
      "grad_norm": 5.978935241699219,
      "learning_rate": 2.745965846780511e-05,
      "loss": 1.6805,
      "step": 345300
    },
    {
      "epoch": 27.05624314585618,
      "grad_norm": 5.165824890136719,
      "learning_rate": 2.7453130711786517e-05,
      "loss": 1.7188,
      "step": 345400
    },
    {
      "epoch": 27.06407645307849,
      "grad_norm": 7.399711608886719,
      "learning_rate": 2.7446602955767927e-05,
      "loss": 1.7146,
      "step": 345500
    },
    {
      "epoch": 27.071909760300798,
      "grad_norm": 6.245728969573975,
      "learning_rate": 2.744007519974934e-05,
      "loss": 1.6875,
      "step": 345600
    },
    {
      "epoch": 27.079743067523108,
      "grad_norm": 9.064480781555176,
      "learning_rate": 2.7433547443730745e-05,
      "loss": 1.7466,
      "step": 345700
    },
    {
      "epoch": 27.087576374745417,
      "grad_norm": 5.220175266265869,
      "learning_rate": 2.742701968771215e-05,
      "loss": 1.7035,
      "step": 345800
    },
    {
      "epoch": 27.095409681967727,
      "grad_norm": 7.112119674682617,
      "learning_rate": 2.7420491931693563e-05,
      "loss": 1.6981,
      "step": 345900
    },
    {
      "epoch": 27.103242989190036,
      "grad_norm": 5.1825385093688965,
      "learning_rate": 2.7413964175674972e-05,
      "loss": 1.7168,
      "step": 346000
    },
    {
      "epoch": 27.111076296412346,
      "grad_norm": 6.4443359375,
      "learning_rate": 2.7407436419656378e-05,
      "loss": 1.6985,
      "step": 346100
    },
    {
      "epoch": 27.118909603634656,
      "grad_norm": 6.227043628692627,
      "learning_rate": 2.7400908663637787e-05,
      "loss": 1.6343,
      "step": 346200
    },
    {
      "epoch": 27.126742910856965,
      "grad_norm": 7.283267974853516,
      "learning_rate": 2.73943809076192e-05,
      "loss": 1.7039,
      "step": 346300
    },
    {
      "epoch": 27.134576218079275,
      "grad_norm": 5.660894870758057,
      "learning_rate": 2.7387853151600606e-05,
      "loss": 1.6228,
      "step": 346400
    },
    {
      "epoch": 27.14240952530158,
      "grad_norm": 5.387594223022461,
      "learning_rate": 2.7381325395582015e-05,
      "loss": 1.6507,
      "step": 346500
    },
    {
      "epoch": 27.15024283252389,
      "grad_norm": 5.834493637084961,
      "learning_rate": 2.7374797639563427e-05,
      "loss": 1.6455,
      "step": 346600
    },
    {
      "epoch": 27.1580761397462,
      "grad_norm": 7.060842514038086,
      "learning_rate": 2.7368269883544833e-05,
      "loss": 1.6769,
      "step": 346700
    },
    {
      "epoch": 27.16590944696851,
      "grad_norm": 7.2811360359191895,
      "learning_rate": 2.7361742127526242e-05,
      "loss": 1.738,
      "step": 346800
    },
    {
      "epoch": 27.17374275419082,
      "grad_norm": 6.129593372344971,
      "learning_rate": 2.7355214371507655e-05,
      "loss": 1.7027,
      "step": 346900
    },
    {
      "epoch": 27.18157606141313,
      "grad_norm": 6.315053939819336,
      "learning_rate": 2.734868661548906e-05,
      "loss": 1.7054,
      "step": 347000
    },
    {
      "epoch": 27.189409368635438,
      "grad_norm": 6.015635967254639,
      "learning_rate": 2.734215885947047e-05,
      "loss": 1.7134,
      "step": 347100
    },
    {
      "epoch": 27.197242675857748,
      "grad_norm": 6.1477837562561035,
      "learning_rate": 2.7335631103451882e-05,
      "loss": 1.7096,
      "step": 347200
    },
    {
      "epoch": 27.205075983080057,
      "grad_norm": 5.432371616363525,
      "learning_rate": 2.7329103347433288e-05,
      "loss": 1.6819,
      "step": 347300
    },
    {
      "epoch": 27.212909290302367,
      "grad_norm": 1.9240418672561646,
      "learning_rate": 2.7322575591414694e-05,
      "loss": 1.7114,
      "step": 347400
    },
    {
      "epoch": 27.220742597524676,
      "grad_norm": 7.069964408874512,
      "learning_rate": 2.7316047835396106e-05,
      "loss": 1.6261,
      "step": 347500
    },
    {
      "epoch": 27.228575904746982,
      "grad_norm": 5.059299945831299,
      "learning_rate": 2.7309520079377515e-05,
      "loss": 1.6157,
      "step": 347600
    },
    {
      "epoch": 27.236409211969292,
      "grad_norm": 7.220086574554443,
      "learning_rate": 2.730299232335892e-05,
      "loss": 1.7184,
      "step": 347700
    },
    {
      "epoch": 27.2442425191916,
      "grad_norm": 6.796145439147949,
      "learning_rate": 2.729646456734033e-05,
      "loss": 1.7109,
      "step": 347800
    },
    {
      "epoch": 27.25207582641391,
      "grad_norm": 6.131519317626953,
      "learning_rate": 2.7289936811321743e-05,
      "loss": 1.6702,
      "step": 347900
    },
    {
      "epoch": 27.25990913363622,
      "grad_norm": 6.368988037109375,
      "learning_rate": 2.728340905530315e-05,
      "loss": 1.7653,
      "step": 348000
    },
    {
      "epoch": 27.26774244085853,
      "grad_norm": 6.491076469421387,
      "learning_rate": 2.7276881299284558e-05,
      "loss": 1.7589,
      "step": 348100
    },
    {
      "epoch": 27.27557574808084,
      "grad_norm": 4.50307035446167,
      "learning_rate": 2.727035354326597e-05,
      "loss": 1.7245,
      "step": 348200
    },
    {
      "epoch": 27.28340905530315,
      "grad_norm": 5.473911285400391,
      "learning_rate": 2.7263825787247376e-05,
      "loss": 1.7619,
      "step": 348300
    },
    {
      "epoch": 27.29124236252546,
      "grad_norm": 7.859044551849365,
      "learning_rate": 2.7257298031228785e-05,
      "loss": 1.7093,
      "step": 348400
    },
    {
      "epoch": 27.29907566974777,
      "grad_norm": 5.524664402008057,
      "learning_rate": 2.7250770275210198e-05,
      "loss": 1.72,
      "step": 348500
    },
    {
      "epoch": 27.306908976970078,
      "grad_norm": 4.947566509246826,
      "learning_rate": 2.7244242519191604e-05,
      "loss": 1.6291,
      "step": 348600
    },
    {
      "epoch": 27.314742284192388,
      "grad_norm": 5.929074287414551,
      "learning_rate": 2.7237714763173013e-05,
      "loss": 1.7275,
      "step": 348700
    },
    {
      "epoch": 27.322575591414694,
      "grad_norm": 5.31874942779541,
      "learning_rate": 2.7231187007154425e-05,
      "loss": 1.6978,
      "step": 348800
    },
    {
      "epoch": 27.330408898637003,
      "grad_norm": 5.882228374481201,
      "learning_rate": 2.722465925113583e-05,
      "loss": 1.6144,
      "step": 348900
    },
    {
      "epoch": 27.338242205859313,
      "grad_norm": 4.154293060302734,
      "learning_rate": 2.7218131495117237e-05,
      "loss": 1.6666,
      "step": 349000
    },
    {
      "epoch": 27.346075513081622,
      "grad_norm": 4.595165252685547,
      "learning_rate": 2.721160373909865e-05,
      "loss": 1.651,
      "step": 349100
    },
    {
      "epoch": 27.353908820303932,
      "grad_norm": 3.297358512878418,
      "learning_rate": 2.720507598308006e-05,
      "loss": 1.6568,
      "step": 349200
    },
    {
      "epoch": 27.36174212752624,
      "grad_norm": 8.588919639587402,
      "learning_rate": 2.7198548227061464e-05,
      "loss": 1.6601,
      "step": 349300
    },
    {
      "epoch": 27.36957543474855,
      "grad_norm": 5.670596122741699,
      "learning_rate": 2.7192020471042877e-05,
      "loss": 1.6645,
      "step": 349400
    },
    {
      "epoch": 27.37740874197086,
      "grad_norm": 6.25799560546875,
      "learning_rate": 2.7185492715024286e-05,
      "loss": 1.7617,
      "step": 349500
    },
    {
      "epoch": 27.38524204919317,
      "grad_norm": 4.897523880004883,
      "learning_rate": 2.7178964959005692e-05,
      "loss": 1.6937,
      "step": 349600
    },
    {
      "epoch": 27.39307535641548,
      "grad_norm": 6.954018592834473,
      "learning_rate": 2.71724372029871e-05,
      "loss": 1.6538,
      "step": 349700
    },
    {
      "epoch": 27.40090866363779,
      "grad_norm": 7.576491832733154,
      "learning_rate": 2.7165909446968514e-05,
      "loss": 1.666,
      "step": 349800
    },
    {
      "epoch": 27.408741970860095,
      "grad_norm": 9.550752639770508,
      "learning_rate": 2.715938169094992e-05,
      "loss": 1.6475,
      "step": 349900
    },
    {
      "epoch": 27.416575278082405,
      "grad_norm": 5.943192005157471,
      "learning_rate": 2.715285393493133e-05,
      "loss": 1.6397,
      "step": 350000
    },
    {
      "epoch": 27.424408585304715,
      "grad_norm": 4.603248596191406,
      "learning_rate": 2.714632617891274e-05,
      "loss": 1.7186,
      "step": 350100
    },
    {
      "epoch": 27.432241892527024,
      "grad_norm": 7.474924564361572,
      "learning_rate": 2.7139798422894147e-05,
      "loss": 1.6552,
      "step": 350200
    },
    {
      "epoch": 27.440075199749334,
      "grad_norm": 5.903706073760986,
      "learning_rate": 2.7133270666875556e-05,
      "loss": 1.6873,
      "step": 350300
    },
    {
      "epoch": 27.447908506971643,
      "grad_norm": 5.550760746002197,
      "learning_rate": 2.712674291085697e-05,
      "loss": 1.5694,
      "step": 350400
    },
    {
      "epoch": 27.455741814193953,
      "grad_norm": 6.413599014282227,
      "learning_rate": 2.7120215154838374e-05,
      "loss": 1.6746,
      "step": 350500
    },
    {
      "epoch": 27.463575121416262,
      "grad_norm": 5.578631401062012,
      "learning_rate": 2.711368739881978e-05,
      "loss": 1.7561,
      "step": 350600
    },
    {
      "epoch": 27.471408428638572,
      "grad_norm": 5.752120018005371,
      "learning_rate": 2.7107159642801193e-05,
      "loss": 1.6529,
      "step": 350700
    },
    {
      "epoch": 27.47924173586088,
      "grad_norm": 4.983463764190674,
      "learning_rate": 2.7100631886782602e-05,
      "loss": 1.675,
      "step": 350800
    },
    {
      "epoch": 27.48707504308319,
      "grad_norm": 7.142340183258057,
      "learning_rate": 2.7094104130764007e-05,
      "loss": 1.7368,
      "step": 350900
    },
    {
      "epoch": 27.4949083503055,
      "grad_norm": 5.353498458862305,
      "learning_rate": 2.708757637474542e-05,
      "loss": 1.6708,
      "step": 351000
    },
    {
      "epoch": 27.502741657527807,
      "grad_norm": 7.277495861053467,
      "learning_rate": 2.708104861872683e-05,
      "loss": 1.8019,
      "step": 351100
    },
    {
      "epoch": 27.510574964750116,
      "grad_norm": 7.190350532531738,
      "learning_rate": 2.7074520862708235e-05,
      "loss": 1.7755,
      "step": 351200
    },
    {
      "epoch": 27.518408271972426,
      "grad_norm": 5.388846397399902,
      "learning_rate": 2.7067993106689644e-05,
      "loss": 1.6897,
      "step": 351300
    },
    {
      "epoch": 27.526241579194735,
      "grad_norm": 7.5855865478515625,
      "learning_rate": 2.7061465350671057e-05,
      "loss": 1.7222,
      "step": 351400
    },
    {
      "epoch": 27.534074886417045,
      "grad_norm": 5.505348205566406,
      "learning_rate": 2.7054937594652462e-05,
      "loss": 1.5811,
      "step": 351500
    },
    {
      "epoch": 27.541908193639355,
      "grad_norm": 6.060662269592285,
      "learning_rate": 2.704840983863387e-05,
      "loss": 1.6373,
      "step": 351600
    },
    {
      "epoch": 27.549741500861664,
      "grad_norm": 6.090273857116699,
      "learning_rate": 2.7041882082615284e-05,
      "loss": 1.6318,
      "step": 351700
    },
    {
      "epoch": 27.557574808083974,
      "grad_norm": 6.324852466583252,
      "learning_rate": 2.703535432659669e-05,
      "loss": 1.6745,
      "step": 351800
    },
    {
      "epoch": 27.565408115306283,
      "grad_norm": 4.754481792449951,
      "learning_rate": 2.70288265705781e-05,
      "loss": 1.597,
      "step": 351900
    },
    {
      "epoch": 27.573241422528593,
      "grad_norm": 7.652126789093018,
      "learning_rate": 2.702229881455951e-05,
      "loss": 1.6593,
      "step": 352000
    },
    {
      "epoch": 27.581074729750902,
      "grad_norm": 6.019192695617676,
      "learning_rate": 2.7015771058540917e-05,
      "loss": 1.7176,
      "step": 352100
    },
    {
      "epoch": 27.58890803697321,
      "grad_norm": 6.620492935180664,
      "learning_rate": 2.7009243302522323e-05,
      "loss": 1.654,
      "step": 352200
    },
    {
      "epoch": 27.596741344195518,
      "grad_norm": 4.68381404876709,
      "learning_rate": 2.7002715546503736e-05,
      "loss": 1.707,
      "step": 352300
    },
    {
      "epoch": 27.604574651417828,
      "grad_norm": 5.599862098693848,
      "learning_rate": 2.6996187790485145e-05,
      "loss": 1.6751,
      "step": 352400
    },
    {
      "epoch": 27.612407958640137,
      "grad_norm": 5.979346752166748,
      "learning_rate": 2.698966003446655e-05,
      "loss": 1.6944,
      "step": 352500
    },
    {
      "epoch": 27.620241265862447,
      "grad_norm": 5.64046049118042,
      "learning_rate": 2.6983132278447963e-05,
      "loss": 1.7433,
      "step": 352600
    },
    {
      "epoch": 27.628074573084756,
      "grad_norm": 3.130995750427246,
      "learning_rate": 2.6976604522429372e-05,
      "loss": 1.6769,
      "step": 352700
    },
    {
      "epoch": 27.635907880307066,
      "grad_norm": 7.851424217224121,
      "learning_rate": 2.6970076766410778e-05,
      "loss": 1.7668,
      "step": 352800
    },
    {
      "epoch": 27.643741187529375,
      "grad_norm": 5.496628284454346,
      "learning_rate": 2.6963549010392187e-05,
      "loss": 1.6258,
      "step": 352900
    },
    {
      "epoch": 27.651574494751685,
      "grad_norm": 5.599788665771484,
      "learning_rate": 2.69570212543736e-05,
      "loss": 1.6891,
      "step": 353000
    },
    {
      "epoch": 27.659407801973995,
      "grad_norm": 9.868889808654785,
      "learning_rate": 2.6950493498355006e-05,
      "loss": 1.7628,
      "step": 353100
    },
    {
      "epoch": 27.667241109196304,
      "grad_norm": 8.136153221130371,
      "learning_rate": 2.6943965742336415e-05,
      "loss": 1.7774,
      "step": 353200
    },
    {
      "epoch": 27.67507441641861,
      "grad_norm": 6.756158351898193,
      "learning_rate": 2.6937437986317827e-05,
      "loss": 1.7024,
      "step": 353300
    },
    {
      "epoch": 27.68290772364092,
      "grad_norm": 8.388501167297363,
      "learning_rate": 2.6930910230299233e-05,
      "loss": 1.7302,
      "step": 353400
    },
    {
      "epoch": 27.69074103086323,
      "grad_norm": 8.349839210510254,
      "learning_rate": 2.6924382474280642e-05,
      "loss": 1.7674,
      "step": 353500
    },
    {
      "epoch": 27.69857433808554,
      "grad_norm": 5.595146179199219,
      "learning_rate": 2.6917854718262055e-05,
      "loss": 1.6272,
      "step": 353600
    },
    {
      "epoch": 27.70640764530785,
      "grad_norm": 6.422841548919678,
      "learning_rate": 2.691132696224346e-05,
      "loss": 1.6095,
      "step": 353700
    },
    {
      "epoch": 27.714240952530158,
      "grad_norm": 7.178310394287109,
      "learning_rate": 2.6904799206224866e-05,
      "loss": 1.8141,
      "step": 353800
    },
    {
      "epoch": 27.722074259752468,
      "grad_norm": 7.554553985595703,
      "learning_rate": 2.689827145020628e-05,
      "loss": 1.7153,
      "step": 353900
    },
    {
      "epoch": 27.729907566974777,
      "grad_norm": 6.968037128448486,
      "learning_rate": 2.6891743694187688e-05,
      "loss": 1.6782,
      "step": 354000
    },
    {
      "epoch": 27.737740874197087,
      "grad_norm": 7.6766357421875,
      "learning_rate": 2.6885215938169094e-05,
      "loss": 1.7267,
      "step": 354100
    },
    {
      "epoch": 27.745574181419396,
      "grad_norm": 6.5279974937438965,
      "learning_rate": 2.6878688182150506e-05,
      "loss": 1.6967,
      "step": 354200
    },
    {
      "epoch": 27.753407488641706,
      "grad_norm": 6.492184162139893,
      "learning_rate": 2.6872160426131915e-05,
      "loss": 1.7822,
      "step": 354300
    },
    {
      "epoch": 27.761240795864015,
      "grad_norm": 6.134205341339111,
      "learning_rate": 2.686563267011332e-05,
      "loss": 1.7196,
      "step": 354400
    },
    {
      "epoch": 27.76907410308632,
      "grad_norm": 5.7650980949401855,
      "learning_rate": 2.6859104914094734e-05,
      "loss": 1.7642,
      "step": 354500
    },
    {
      "epoch": 27.77690741030863,
      "grad_norm": 6.574864864349365,
      "learning_rate": 2.6852577158076143e-05,
      "loss": 1.5492,
      "step": 354600
    },
    {
      "epoch": 27.78474071753094,
      "grad_norm": 7.7437334060668945,
      "learning_rate": 2.684604940205755e-05,
      "loss": 1.7709,
      "step": 354700
    },
    {
      "epoch": 27.79257402475325,
      "grad_norm": 4.6825361251831055,
      "learning_rate": 2.6839521646038958e-05,
      "loss": 1.6999,
      "step": 354800
    },
    {
      "epoch": 27.80040733197556,
      "grad_norm": 3.826000928878784,
      "learning_rate": 2.683299389002037e-05,
      "loss": 1.6909,
      "step": 354900
    },
    {
      "epoch": 27.80824063919787,
      "grad_norm": 5.915297985076904,
      "learning_rate": 2.6826466134001776e-05,
      "loss": 1.6963,
      "step": 355000
    },
    {
      "epoch": 27.81607394642018,
      "grad_norm": 7.004995822906494,
      "learning_rate": 2.6819938377983185e-05,
      "loss": 1.765,
      "step": 355100
    },
    {
      "epoch": 27.82390725364249,
      "grad_norm": 7.400693893432617,
      "learning_rate": 2.6813410621964598e-05,
      "loss": 1.6803,
      "step": 355200
    },
    {
      "epoch": 27.831740560864798,
      "grad_norm": 4.960346698760986,
      "learning_rate": 2.6806882865946004e-05,
      "loss": 1.7025,
      "step": 355300
    },
    {
      "epoch": 27.839573868087108,
      "grad_norm": 5.189248561859131,
      "learning_rate": 2.680035510992741e-05,
      "loss": 1.7584,
      "step": 355400
    },
    {
      "epoch": 27.847407175309417,
      "grad_norm": 6.89113712310791,
      "learning_rate": 2.6793827353908822e-05,
      "loss": 1.7162,
      "step": 355500
    },
    {
      "epoch": 27.855240482531723,
      "grad_norm": 7.789115905761719,
      "learning_rate": 2.678729959789023e-05,
      "loss": 1.6785,
      "step": 355600
    },
    {
      "epoch": 27.863073789754033,
      "grad_norm": 8.229473114013672,
      "learning_rate": 2.6780771841871637e-05,
      "loss": 1.5801,
      "step": 355700
    },
    {
      "epoch": 27.870907096976342,
      "grad_norm": 5.738151550292969,
      "learning_rate": 2.677424408585305e-05,
      "loss": 1.6861,
      "step": 355800
    },
    {
      "epoch": 27.878740404198652,
      "grad_norm": 9.34225845336914,
      "learning_rate": 2.676771632983446e-05,
      "loss": 1.7444,
      "step": 355900
    },
    {
      "epoch": 27.88657371142096,
      "grad_norm": 5.507238864898682,
      "learning_rate": 2.6761188573815864e-05,
      "loss": 1.7109,
      "step": 356000
    },
    {
      "epoch": 27.89440701864327,
      "grad_norm": 6.002559661865234,
      "learning_rate": 2.6754660817797277e-05,
      "loss": 1.7546,
      "step": 356100
    },
    {
      "epoch": 27.90224032586558,
      "grad_norm": 5.479971885681152,
      "learning_rate": 2.6748133061778686e-05,
      "loss": 1.7414,
      "step": 356200
    },
    {
      "epoch": 27.91007363308789,
      "grad_norm": 6.206109523773193,
      "learning_rate": 2.6741605305760092e-05,
      "loss": 1.7858,
      "step": 356300
    },
    {
      "epoch": 27.9179069403102,
      "grad_norm": 4.725085258483887,
      "learning_rate": 2.67350775497415e-05,
      "loss": 1.7425,
      "step": 356400
    },
    {
      "epoch": 27.92574024753251,
      "grad_norm": 6.585155963897705,
      "learning_rate": 2.6728549793722913e-05,
      "loss": 1.7018,
      "step": 356500
    },
    {
      "epoch": 27.93357355475482,
      "grad_norm": 7.776817798614502,
      "learning_rate": 2.672202203770432e-05,
      "loss": 1.6805,
      "step": 356600
    },
    {
      "epoch": 27.94140686197713,
      "grad_norm": 6.248134613037109,
      "learning_rate": 2.671549428168573e-05,
      "loss": 1.6054,
      "step": 356700
    },
    {
      "epoch": 27.949240169199435,
      "grad_norm": 7.924685955047607,
      "learning_rate": 2.670896652566714e-05,
      "loss": 1.6306,
      "step": 356800
    },
    {
      "epoch": 27.957073476421744,
      "grad_norm": 7.396246433258057,
      "learning_rate": 2.6702438769648547e-05,
      "loss": 1.7464,
      "step": 356900
    },
    {
      "epoch": 27.964906783644054,
      "grad_norm": 7.980695724487305,
      "learning_rate": 2.6695911013629952e-05,
      "loss": 1.7796,
      "step": 357000
    },
    {
      "epoch": 27.972740090866363,
      "grad_norm": 7.577665328979492,
      "learning_rate": 2.6689383257611365e-05,
      "loss": 1.7782,
      "step": 357100
    },
    {
      "epoch": 27.980573398088673,
      "grad_norm": 7.575852394104004,
      "learning_rate": 2.6682855501592774e-05,
      "loss": 1.724,
      "step": 357200
    },
    {
      "epoch": 27.988406705310982,
      "grad_norm": 6.109042644500732,
      "learning_rate": 2.667632774557418e-05,
      "loss": 1.6959,
      "step": 357300
    },
    {
      "epoch": 27.996240012533292,
      "grad_norm": 6.764620780944824,
      "learning_rate": 2.6669799989555592e-05,
      "loss": 1.5847,
      "step": 357400
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.7844946384429932,
      "eval_runtime": 1.5516,
      "eval_samples_per_second": 433.088,
      "eval_steps_per_second": 433.088,
      "step": 357448
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.451066017150879,
      "eval_runtime": 35.5303,
      "eval_samples_per_second": 359.299,
      "eval_steps_per_second": 359.299,
      "step": 357448
    },
    {
      "epoch": 28.0040733197556,
      "grad_norm": 6.340132713317871,
      "learning_rate": 2.6663272233537e-05,
      "loss": 1.7114,
      "step": 357500
    },
    {
      "epoch": 28.01190662697791,
      "grad_norm": 6.689503192901611,
      "learning_rate": 2.6656744477518407e-05,
      "loss": 1.6913,
      "step": 357600
    },
    {
      "epoch": 28.01973993420022,
      "grad_norm": 6.573731899261475,
      "learning_rate": 2.665021672149982e-05,
      "loss": 1.6005,
      "step": 357700
    },
    {
      "epoch": 28.02757324142253,
      "grad_norm": 6.790256500244141,
      "learning_rate": 2.664368896548123e-05,
      "loss": 1.6631,
      "step": 357800
    },
    {
      "epoch": 28.035406548644836,
      "grad_norm": 7.224578857421875,
      "learning_rate": 2.6637161209462635e-05,
      "loss": 1.6407,
      "step": 357900
    },
    {
      "epoch": 28.043239855867146,
      "grad_norm": 7.136960029602051,
      "learning_rate": 2.6630633453444044e-05,
      "loss": 1.6823,
      "step": 358000
    },
    {
      "epoch": 28.051073163089455,
      "grad_norm": 7.653718948364258,
      "learning_rate": 2.6624105697425457e-05,
      "loss": 1.6812,
      "step": 358100
    },
    {
      "epoch": 28.058906470311765,
      "grad_norm": 7.785960674285889,
      "learning_rate": 2.6617577941406862e-05,
      "loss": 1.7137,
      "step": 358200
    },
    {
      "epoch": 28.066739777534075,
      "grad_norm": 5.835428237915039,
      "learning_rate": 2.661105018538827e-05,
      "loss": 1.7338,
      "step": 358300
    },
    {
      "epoch": 28.074573084756384,
      "grad_norm": 6.130595684051514,
      "learning_rate": 2.6604522429369684e-05,
      "loss": 1.7011,
      "step": 358400
    },
    {
      "epoch": 28.082406391978694,
      "grad_norm": 2.1048011779785156,
      "learning_rate": 2.659799467335109e-05,
      "loss": 1.6449,
      "step": 358500
    },
    {
      "epoch": 28.090239699201003,
      "grad_norm": 6.772695541381836,
      "learning_rate": 2.6591466917332496e-05,
      "loss": 1.6702,
      "step": 358600
    },
    {
      "epoch": 28.098073006423313,
      "grad_norm": 6.394213676452637,
      "learning_rate": 2.6584939161313908e-05,
      "loss": 1.6232,
      "step": 358700
    },
    {
      "epoch": 28.105906313645622,
      "grad_norm": 8.043148040771484,
      "learning_rate": 2.6578411405295317e-05,
      "loss": 1.6558,
      "step": 358800
    },
    {
      "epoch": 28.113739620867932,
      "grad_norm": 5.314398765563965,
      "learning_rate": 2.6571883649276723e-05,
      "loss": 1.6877,
      "step": 358900
    },
    {
      "epoch": 28.121572928090238,
      "grad_norm": 7.29562520980835,
      "learning_rate": 2.6565355893258136e-05,
      "loss": 1.6694,
      "step": 359000
    },
    {
      "epoch": 28.129406235312548,
      "grad_norm": 6.790257930755615,
      "learning_rate": 2.6558828137239545e-05,
      "loss": 1.6608,
      "step": 359100
    },
    {
      "epoch": 28.137239542534857,
      "grad_norm": 6.03131628036499,
      "learning_rate": 2.655230038122095e-05,
      "loss": 1.6169,
      "step": 359200
    },
    {
      "epoch": 28.145072849757167,
      "grad_norm": 5.119325160980225,
      "learning_rate": 2.6545772625202363e-05,
      "loss": 1.6822,
      "step": 359300
    },
    {
      "epoch": 28.152906156979476,
      "grad_norm": 7.679657459259033,
      "learning_rate": 2.6539244869183772e-05,
      "loss": 1.6096,
      "step": 359400
    },
    {
      "epoch": 28.160739464201786,
      "grad_norm": 15.218598365783691,
      "learning_rate": 2.6532717113165178e-05,
      "loss": 1.7214,
      "step": 359500
    },
    {
      "epoch": 28.168572771424095,
      "grad_norm": 5.2415666580200195,
      "learning_rate": 2.6526189357146587e-05,
      "loss": 1.6587,
      "step": 359600
    },
    {
      "epoch": 28.176406078646405,
      "grad_norm": 8.433937072753906,
      "learning_rate": 2.6519661601128e-05,
      "loss": 1.6972,
      "step": 359700
    },
    {
      "epoch": 28.184239385868715,
      "grad_norm": 5.971495151519775,
      "learning_rate": 2.6513133845109405e-05,
      "loss": 1.7361,
      "step": 359800
    },
    {
      "epoch": 28.192072693091024,
      "grad_norm": 6.440729141235352,
      "learning_rate": 2.6506606089090815e-05,
      "loss": 1.601,
      "step": 359900
    },
    {
      "epoch": 28.199906000313334,
      "grad_norm": 5.11760950088501,
      "learning_rate": 2.6500078333072227e-05,
      "loss": 1.6956,
      "step": 360000
    },
    {
      "epoch": 28.207739307535643,
      "grad_norm": 7.522558689117432,
      "learning_rate": 2.6493550577053633e-05,
      "loss": 1.6583,
      "step": 360100
    },
    {
      "epoch": 28.21557261475795,
      "grad_norm": 7.1174774169921875,
      "learning_rate": 2.648702282103504e-05,
      "loss": 1.7236,
      "step": 360200
    },
    {
      "epoch": 28.22340592198026,
      "grad_norm": 3.6037709712982178,
      "learning_rate": 2.648049506501645e-05,
      "loss": 1.6953,
      "step": 360300
    },
    {
      "epoch": 28.23123922920257,
      "grad_norm": 6.268794536590576,
      "learning_rate": 2.647396730899786e-05,
      "loss": 1.6062,
      "step": 360400
    },
    {
      "epoch": 28.239072536424878,
      "grad_norm": 4.237228870391846,
      "learning_rate": 2.6467439552979266e-05,
      "loss": 1.662,
      "step": 360500
    },
    {
      "epoch": 28.246905843647188,
      "grad_norm": 5.765003204345703,
      "learning_rate": 2.646091179696068e-05,
      "loss": 1.6452,
      "step": 360600
    },
    {
      "epoch": 28.254739150869497,
      "grad_norm": 5.989500045776367,
      "learning_rate": 2.6454384040942088e-05,
      "loss": 1.6482,
      "step": 360700
    },
    {
      "epoch": 28.262572458091807,
      "grad_norm": 4.9808349609375,
      "learning_rate": 2.6447856284923494e-05,
      "loss": 1.6199,
      "step": 360800
    },
    {
      "epoch": 28.270405765314116,
      "grad_norm": 8.066926002502441,
      "learning_rate": 2.6441328528904906e-05,
      "loss": 1.686,
      "step": 360900
    },
    {
      "epoch": 28.278239072536426,
      "grad_norm": 4.758934020996094,
      "learning_rate": 2.6434800772886315e-05,
      "loss": 1.7865,
      "step": 361000
    },
    {
      "epoch": 28.286072379758735,
      "grad_norm": 3.994478464126587,
      "learning_rate": 2.642827301686772e-05,
      "loss": 1.6827,
      "step": 361100
    },
    {
      "epoch": 28.293905686981045,
      "grad_norm": 6.24155330657959,
      "learning_rate": 2.6421745260849134e-05,
      "loss": 1.7379,
      "step": 361200
    },
    {
      "epoch": 28.30173899420335,
      "grad_norm": 5.399667739868164,
      "learning_rate": 2.6415217504830543e-05,
      "loss": 1.6831,
      "step": 361300
    },
    {
      "epoch": 28.30957230142566,
      "grad_norm": 6.378545761108398,
      "learning_rate": 2.640868974881195e-05,
      "loss": 1.619,
      "step": 361400
    },
    {
      "epoch": 28.31740560864797,
      "grad_norm": 7.852341651916504,
      "learning_rate": 2.6402161992793358e-05,
      "loss": 1.6715,
      "step": 361500
    },
    {
      "epoch": 28.32523891587028,
      "grad_norm": 9.12209701538086,
      "learning_rate": 2.639563423677477e-05,
      "loss": 1.6369,
      "step": 361600
    },
    {
      "epoch": 28.33307222309259,
      "grad_norm": 5.631840705871582,
      "learning_rate": 2.6389106480756176e-05,
      "loss": 1.7681,
      "step": 361700
    },
    {
      "epoch": 28.3409055303149,
      "grad_norm": 4.361958026885986,
      "learning_rate": 2.6382578724737582e-05,
      "loss": 1.6851,
      "step": 361800
    },
    {
      "epoch": 28.34873883753721,
      "grad_norm": 5.952447414398193,
      "learning_rate": 2.6376050968718994e-05,
      "loss": 1.6468,
      "step": 361900
    },
    {
      "epoch": 28.356572144759518,
      "grad_norm": 6.241386890411377,
      "learning_rate": 2.6369523212700404e-05,
      "loss": 1.728,
      "step": 362000
    },
    {
      "epoch": 28.364405451981828,
      "grad_norm": 4.2293267250061035,
      "learning_rate": 2.636299545668181e-05,
      "loss": 1.6214,
      "step": 362100
    },
    {
      "epoch": 28.372238759204137,
      "grad_norm": 6.570171356201172,
      "learning_rate": 2.6356467700663222e-05,
      "loss": 1.5905,
      "step": 362200
    },
    {
      "epoch": 28.380072066426447,
      "grad_norm": 5.359292507171631,
      "learning_rate": 2.634993994464463e-05,
      "loss": 1.6889,
      "step": 362300
    },
    {
      "epoch": 28.387905373648753,
      "grad_norm": 6.901095867156982,
      "learning_rate": 2.6343412188626037e-05,
      "loss": 1.787,
      "step": 362400
    },
    {
      "epoch": 28.395738680871062,
      "grad_norm": 6.199187755584717,
      "learning_rate": 2.633688443260745e-05,
      "loss": 1.6682,
      "step": 362500
    },
    {
      "epoch": 28.403571988093372,
      "grad_norm": 4.965441703796387,
      "learning_rate": 2.633035667658886e-05,
      "loss": 1.6896,
      "step": 362600
    },
    {
      "epoch": 28.41140529531568,
      "grad_norm": 7.154070854187012,
      "learning_rate": 2.6323828920570264e-05,
      "loss": 1.785,
      "step": 362700
    },
    {
      "epoch": 28.41923860253799,
      "grad_norm": 6.025720596313477,
      "learning_rate": 2.6317301164551677e-05,
      "loss": 1.6219,
      "step": 362800
    },
    {
      "epoch": 28.4270719097603,
      "grad_norm": 6.606451988220215,
      "learning_rate": 2.6310773408533086e-05,
      "loss": 1.599,
      "step": 362900
    },
    {
      "epoch": 28.43490521698261,
      "grad_norm": 6.538637638092041,
      "learning_rate": 2.6304245652514492e-05,
      "loss": 1.6033,
      "step": 363000
    },
    {
      "epoch": 28.44273852420492,
      "grad_norm": 5.151206016540527,
      "learning_rate": 2.62977178964959e-05,
      "loss": 1.6597,
      "step": 363100
    },
    {
      "epoch": 28.45057183142723,
      "grad_norm": 7.1900434494018555,
      "learning_rate": 2.6291190140477313e-05,
      "loss": 1.6792,
      "step": 363200
    },
    {
      "epoch": 28.45840513864954,
      "grad_norm": 6.61590051651001,
      "learning_rate": 2.628466238445872e-05,
      "loss": 1.63,
      "step": 363300
    },
    {
      "epoch": 28.46623844587185,
      "grad_norm": 6.392599105834961,
      "learning_rate": 2.6278134628440125e-05,
      "loss": 1.76,
      "step": 363400
    },
    {
      "epoch": 28.474071753094158,
      "grad_norm": 5.511166095733643,
      "learning_rate": 2.6271606872421537e-05,
      "loss": 1.7467,
      "step": 363500
    },
    {
      "epoch": 28.481905060316464,
      "grad_norm": 4.403999328613281,
      "learning_rate": 2.6265079116402947e-05,
      "loss": 1.5266,
      "step": 363600
    },
    {
      "epoch": 28.489738367538774,
      "grad_norm": 8.18177604675293,
      "learning_rate": 2.6258551360384352e-05,
      "loss": 1.6542,
      "step": 363700
    },
    {
      "epoch": 28.497571674761083,
      "grad_norm": 8.543501853942871,
      "learning_rate": 2.6252023604365765e-05,
      "loss": 1.7156,
      "step": 363800
    },
    {
      "epoch": 28.505404981983393,
      "grad_norm": 6.595260143280029,
      "learning_rate": 2.6245495848347174e-05,
      "loss": 1.7349,
      "step": 363900
    },
    {
      "epoch": 28.513238289205702,
      "grad_norm": 6.48949670791626,
      "learning_rate": 2.623896809232858e-05,
      "loss": 1.7746,
      "step": 364000
    },
    {
      "epoch": 28.521071596428012,
      "grad_norm": 7.264676570892334,
      "learning_rate": 2.6232440336309992e-05,
      "loss": 1.6385,
      "step": 364100
    },
    {
      "epoch": 28.52890490365032,
      "grad_norm": 6.114163398742676,
      "learning_rate": 2.62259125802914e-05,
      "loss": 1.7645,
      "step": 364200
    },
    {
      "epoch": 28.53673821087263,
      "grad_norm": 5.0596160888671875,
      "learning_rate": 2.6219384824272807e-05,
      "loss": 1.656,
      "step": 364300
    },
    {
      "epoch": 28.54457151809494,
      "grad_norm": 6.263687610626221,
      "learning_rate": 2.621285706825422e-05,
      "loss": 1.6905,
      "step": 364400
    },
    {
      "epoch": 28.55240482531725,
      "grad_norm": 8.014200210571289,
      "learning_rate": 2.620632931223563e-05,
      "loss": 1.6601,
      "step": 364500
    },
    {
      "epoch": 28.56023813253956,
      "grad_norm": 7.101950645446777,
      "learning_rate": 2.6199801556217035e-05,
      "loss": 1.6992,
      "step": 364600
    },
    {
      "epoch": 28.568071439761866,
      "grad_norm": 6.452006816864014,
      "learning_rate": 2.6193273800198444e-05,
      "loss": 1.7355,
      "step": 364700
    },
    {
      "epoch": 28.575904746984175,
      "grad_norm": 6.491293907165527,
      "learning_rate": 2.6186746044179857e-05,
      "loss": 1.634,
      "step": 364800
    },
    {
      "epoch": 28.583738054206485,
      "grad_norm": 6.630031585693359,
      "learning_rate": 2.6180218288161262e-05,
      "loss": 1.6811,
      "step": 364900
    },
    {
      "epoch": 28.591571361428795,
      "grad_norm": 6.044717311859131,
      "learning_rate": 2.6173690532142668e-05,
      "loss": 1.7681,
      "step": 365000
    },
    {
      "epoch": 28.599404668651104,
      "grad_norm": 5.518641471862793,
      "learning_rate": 2.616716277612408e-05,
      "loss": 1.7089,
      "step": 365100
    },
    {
      "epoch": 28.607237975873414,
      "grad_norm": 6.03860330581665,
      "learning_rate": 2.616063502010549e-05,
      "loss": 1.6644,
      "step": 365200
    },
    {
      "epoch": 28.615071283095723,
      "grad_norm": 4.7925944328308105,
      "learning_rate": 2.6154107264086896e-05,
      "loss": 1.8054,
      "step": 365300
    },
    {
      "epoch": 28.622904590318033,
      "grad_norm": 4.048715114593506,
      "learning_rate": 2.6147579508068308e-05,
      "loss": 1.7355,
      "step": 365400
    },
    {
      "epoch": 28.630737897540342,
      "grad_norm": 7.758759498596191,
      "learning_rate": 2.6141051752049717e-05,
      "loss": 1.6444,
      "step": 365500
    },
    {
      "epoch": 28.638571204762652,
      "grad_norm": 6.023367404937744,
      "learning_rate": 2.6134523996031123e-05,
      "loss": 1.5743,
      "step": 365600
    },
    {
      "epoch": 28.64640451198496,
      "grad_norm": 5.282700061798096,
      "learning_rate": 2.6127996240012536e-05,
      "loss": 1.7578,
      "step": 365700
    },
    {
      "epoch": 28.654237819207268,
      "grad_norm": 4.842654228210449,
      "learning_rate": 2.6121468483993945e-05,
      "loss": 1.6752,
      "step": 365800
    },
    {
      "epoch": 28.662071126429577,
      "grad_norm": 8.032753944396973,
      "learning_rate": 2.611494072797535e-05,
      "loss": 1.7808,
      "step": 365900
    },
    {
      "epoch": 28.669904433651887,
      "grad_norm": 7.990169048309326,
      "learning_rate": 2.6108412971956763e-05,
      "loss": 1.7112,
      "step": 366000
    },
    {
      "epoch": 28.677737740874196,
      "grad_norm": 6.7204413414001465,
      "learning_rate": 2.6101885215938172e-05,
      "loss": 1.7192,
      "step": 366100
    },
    {
      "epoch": 28.685571048096506,
      "grad_norm": 6.077042102813721,
      "learning_rate": 2.6095357459919578e-05,
      "loss": 1.6763,
      "step": 366200
    },
    {
      "epoch": 28.693404355318815,
      "grad_norm": 6.775947570800781,
      "learning_rate": 2.608882970390099e-05,
      "loss": 1.7405,
      "step": 366300
    },
    {
      "epoch": 28.701237662541125,
      "grad_norm": 6.788825035095215,
      "learning_rate": 2.60823019478824e-05,
      "loss": 1.6594,
      "step": 366400
    },
    {
      "epoch": 28.709070969763435,
      "grad_norm": 4.745222568511963,
      "learning_rate": 2.6075774191863805e-05,
      "loss": 1.6527,
      "step": 366500
    },
    {
      "epoch": 28.716904276985744,
      "grad_norm": 6.011394500732422,
      "learning_rate": 2.606924643584521e-05,
      "loss": 1.6345,
      "step": 366600
    },
    {
      "epoch": 28.724737584208054,
      "grad_norm": 6.189807415008545,
      "learning_rate": 2.6062718679826624e-05,
      "loss": 1.6984,
      "step": 366700
    },
    {
      "epoch": 28.732570891430363,
      "grad_norm": 8.20859146118164,
      "learning_rate": 2.6056190923808033e-05,
      "loss": 1.6648,
      "step": 366800
    },
    {
      "epoch": 28.740404198652673,
      "grad_norm": 6.818535327911377,
      "learning_rate": 2.604966316778944e-05,
      "loss": 1.7671,
      "step": 366900
    },
    {
      "epoch": 28.74823750587498,
      "grad_norm": 6.029496669769287,
      "learning_rate": 2.604313541177085e-05,
      "loss": 1.6318,
      "step": 367000
    },
    {
      "epoch": 28.75607081309729,
      "grad_norm": 7.862279891967773,
      "learning_rate": 2.603660765575226e-05,
      "loss": 1.6977,
      "step": 367100
    },
    {
      "epoch": 28.763904120319598,
      "grad_norm": 7.733575344085693,
      "learning_rate": 2.6030079899733666e-05,
      "loss": 1.6694,
      "step": 367200
    },
    {
      "epoch": 28.771737427541908,
      "grad_norm": 5.956611633300781,
      "learning_rate": 2.602355214371508e-05,
      "loss": 1.6657,
      "step": 367300
    },
    {
      "epoch": 28.779570734764217,
      "grad_norm": 4.664202690124512,
      "learning_rate": 2.6017024387696488e-05,
      "loss": 1.7805,
      "step": 367400
    },
    {
      "epoch": 28.787404041986527,
      "grad_norm": 7.1260762214660645,
      "learning_rate": 2.6010496631677894e-05,
      "loss": 1.7265,
      "step": 367500
    },
    {
      "epoch": 28.795237349208836,
      "grad_norm": 5.164019584655762,
      "learning_rate": 2.6003968875659306e-05,
      "loss": 1.6955,
      "step": 367600
    },
    {
      "epoch": 28.803070656431146,
      "grad_norm": 7.837399959564209,
      "learning_rate": 2.5997441119640715e-05,
      "loss": 1.7239,
      "step": 367700
    },
    {
      "epoch": 28.810903963653455,
      "grad_norm": 4.829874515533447,
      "learning_rate": 2.599091336362212e-05,
      "loss": 1.7994,
      "step": 367800
    },
    {
      "epoch": 28.818737270875765,
      "grad_norm": 8.601873397827148,
      "learning_rate": 2.5984385607603534e-05,
      "loss": 1.7007,
      "step": 367900
    },
    {
      "epoch": 28.826570578098075,
      "grad_norm": 6.622461318969727,
      "learning_rate": 2.5977857851584943e-05,
      "loss": 1.7651,
      "step": 368000
    },
    {
      "epoch": 28.83440388532038,
      "grad_norm": 6.009946823120117,
      "learning_rate": 2.597133009556635e-05,
      "loss": 1.7304,
      "step": 368100
    },
    {
      "epoch": 28.84223719254269,
      "grad_norm": 7.45835018157959,
      "learning_rate": 2.5964802339547754e-05,
      "loss": 1.7396,
      "step": 368200
    },
    {
      "epoch": 28.850070499765,
      "grad_norm": 5.071157932281494,
      "learning_rate": 2.5958274583529167e-05,
      "loss": 1.6733,
      "step": 368300
    },
    {
      "epoch": 28.85790380698731,
      "grad_norm": 9.417525291442871,
      "learning_rate": 2.5951746827510576e-05,
      "loss": 1.6986,
      "step": 368400
    },
    {
      "epoch": 28.86573711420962,
      "grad_norm": 6.541939735412598,
      "learning_rate": 2.5945219071491982e-05,
      "loss": 1.6618,
      "step": 368500
    },
    {
      "epoch": 28.87357042143193,
      "grad_norm": 7.263866424560547,
      "learning_rate": 2.5938691315473394e-05,
      "loss": 1.7899,
      "step": 368600
    },
    {
      "epoch": 28.881403728654238,
      "grad_norm": 5.173153877258301,
      "learning_rate": 2.5932163559454803e-05,
      "loss": 1.6421,
      "step": 368700
    },
    {
      "epoch": 28.889237035876548,
      "grad_norm": 6.014456272125244,
      "learning_rate": 2.592563580343621e-05,
      "loss": 1.7693,
      "step": 368800
    },
    {
      "epoch": 28.897070343098857,
      "grad_norm": 8.844122886657715,
      "learning_rate": 2.5919108047417622e-05,
      "loss": 1.7137,
      "step": 368900
    },
    {
      "epoch": 28.904903650321167,
      "grad_norm": 6.6385416984558105,
      "learning_rate": 2.591258029139903e-05,
      "loss": 1.7306,
      "step": 369000
    },
    {
      "epoch": 28.912736957543476,
      "grad_norm": 6.693345069885254,
      "learning_rate": 2.5906052535380437e-05,
      "loss": 1.7624,
      "step": 369100
    },
    {
      "epoch": 28.920570264765786,
      "grad_norm": 5.667866230010986,
      "learning_rate": 2.589952477936185e-05,
      "loss": 1.7332,
      "step": 369200
    },
    {
      "epoch": 28.928403571988092,
      "grad_norm": 5.971314907073975,
      "learning_rate": 2.589299702334326e-05,
      "loss": 1.7613,
      "step": 369300
    },
    {
      "epoch": 28.9362368792104,
      "grad_norm": 7.155736923217773,
      "learning_rate": 2.5886469267324664e-05,
      "loss": 1.8195,
      "step": 369400
    },
    {
      "epoch": 28.94407018643271,
      "grad_norm": 6.882861614227295,
      "learning_rate": 2.5879941511306077e-05,
      "loss": 1.7118,
      "step": 369500
    },
    {
      "epoch": 28.95190349365502,
      "grad_norm": 6.077860355377197,
      "learning_rate": 2.5873413755287486e-05,
      "loss": 1.6592,
      "step": 369600
    },
    {
      "epoch": 28.95973680087733,
      "grad_norm": 6.1509904861450195,
      "learning_rate": 2.586688599926889e-05,
      "loss": 1.6784,
      "step": 369700
    },
    {
      "epoch": 28.96757010809964,
      "grad_norm": 4.789956092834473,
      "learning_rate": 2.5860358243250297e-05,
      "loss": 1.7276,
      "step": 369800
    },
    {
      "epoch": 28.97540341532195,
      "grad_norm": 4.5758771896362305,
      "learning_rate": 2.585383048723171e-05,
      "loss": 1.7526,
      "step": 369900
    },
    {
      "epoch": 28.98323672254426,
      "grad_norm": 5.585819721221924,
      "learning_rate": 2.584730273121312e-05,
      "loss": 1.6682,
      "step": 370000
    },
    {
      "epoch": 28.99107002976657,
      "grad_norm": 8.256203651428223,
      "learning_rate": 2.5840774975194525e-05,
      "loss": 1.6757,
      "step": 370100
    },
    {
      "epoch": 28.998903336988878,
      "grad_norm": 5.79316520690918,
      "learning_rate": 2.5834247219175937e-05,
      "loss": 1.6682,
      "step": 370200
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.7847868204116821,
      "eval_runtime": 2.8958,
      "eval_samples_per_second": 232.059,
      "eval_steps_per_second": 232.059,
      "step": 370214
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.4482719898223877,
      "eval_runtime": 56.5798,
      "eval_samples_per_second": 225.628,
      "eval_steps_per_second": 225.628,
      "step": 370214
    },
    {
      "epoch": 29.006736644211188,
      "grad_norm": 6.424941539764404,
      "learning_rate": 2.5827719463157347e-05,
      "loss": 1.6125,
      "step": 370300
    },
    {
      "epoch": 29.014569951433494,
      "grad_norm": 5.939695358276367,
      "learning_rate": 2.5821191707138752e-05,
      "loss": 1.6428,
      "step": 370400
    },
    {
      "epoch": 29.022403258655803,
      "grad_norm": 4.703230857849121,
      "learning_rate": 2.5814663951120165e-05,
      "loss": 1.732,
      "step": 370500
    },
    {
      "epoch": 29.030236565878113,
      "grad_norm": 6.329183578491211,
      "learning_rate": 2.5808136195101574e-05,
      "loss": 1.5931,
      "step": 370600
    },
    {
      "epoch": 29.038069873100422,
      "grad_norm": 6.13674783706665,
      "learning_rate": 2.580160843908298e-05,
      "loss": 1.627,
      "step": 370700
    },
    {
      "epoch": 29.045903180322732,
      "grad_norm": 6.2601752281188965,
      "learning_rate": 2.5795080683064392e-05,
      "loss": 1.7163,
      "step": 370800
    },
    {
      "epoch": 29.05373648754504,
      "grad_norm": 8.017866134643555,
      "learning_rate": 2.57885529270458e-05,
      "loss": 1.7953,
      "step": 370900
    },
    {
      "epoch": 29.06156979476735,
      "grad_norm": 8.442211151123047,
      "learning_rate": 2.5782025171027207e-05,
      "loss": 1.6034,
      "step": 371000
    },
    {
      "epoch": 29.06940310198966,
      "grad_norm": 5.682506561279297,
      "learning_rate": 2.577549741500862e-05,
      "loss": 1.6376,
      "step": 371100
    },
    {
      "epoch": 29.07723640921197,
      "grad_norm": 5.204995632171631,
      "learning_rate": 2.576896965899003e-05,
      "loss": 1.6251,
      "step": 371200
    },
    {
      "epoch": 29.08506971643428,
      "grad_norm": 7.664292812347412,
      "learning_rate": 2.5762441902971435e-05,
      "loss": 1.7043,
      "step": 371300
    },
    {
      "epoch": 29.09290302365659,
      "grad_norm": 4.740330219268799,
      "learning_rate": 2.575591414695284e-05,
      "loss": 1.6744,
      "step": 371400
    },
    {
      "epoch": 29.100736330878895,
      "grad_norm": 5.8942179679870605,
      "learning_rate": 2.5749386390934253e-05,
      "loss": 1.7531,
      "step": 371500
    },
    {
      "epoch": 29.108569638101205,
      "grad_norm": 8.115228652954102,
      "learning_rate": 2.5742858634915662e-05,
      "loss": 1.6434,
      "step": 371600
    },
    {
      "epoch": 29.116402945323514,
      "grad_norm": 5.83556604385376,
      "learning_rate": 2.5736330878897068e-05,
      "loss": 1.7238,
      "step": 371700
    },
    {
      "epoch": 29.124236252545824,
      "grad_norm": 5.552720546722412,
      "learning_rate": 2.572980312287848e-05,
      "loss": 1.6776,
      "step": 371800
    },
    {
      "epoch": 29.132069559768134,
      "grad_norm": 6.253515243530273,
      "learning_rate": 2.572327536685989e-05,
      "loss": 1.7407,
      "step": 371900
    },
    {
      "epoch": 29.139902866990443,
      "grad_norm": 5.842400550842285,
      "learning_rate": 2.5716747610841295e-05,
      "loss": 1.69,
      "step": 372000
    },
    {
      "epoch": 29.147736174212753,
      "grad_norm": 6.668534755706787,
      "learning_rate": 2.5710219854822708e-05,
      "loss": 1.6692,
      "step": 372100
    },
    {
      "epoch": 29.155569481435062,
      "grad_norm": 7.027695655822754,
      "learning_rate": 2.5703692098804117e-05,
      "loss": 1.714,
      "step": 372200
    },
    {
      "epoch": 29.163402788657372,
      "grad_norm": 6.246438026428223,
      "learning_rate": 2.5697164342785523e-05,
      "loss": 1.667,
      "step": 372300
    },
    {
      "epoch": 29.17123609587968,
      "grad_norm": 5.378107070922852,
      "learning_rate": 2.5690636586766935e-05,
      "loss": 1.7608,
      "step": 372400
    },
    {
      "epoch": 29.17906940310199,
      "grad_norm": 6.615233421325684,
      "learning_rate": 2.5684108830748345e-05,
      "loss": 1.65,
      "step": 372500
    },
    {
      "epoch": 29.1869027103243,
      "grad_norm": 7.357483863830566,
      "learning_rate": 2.567758107472975e-05,
      "loss": 1.6825,
      "step": 372600
    },
    {
      "epoch": 29.194736017546607,
      "grad_norm": 4.330660820007324,
      "learning_rate": 2.5671053318711163e-05,
      "loss": 1.6962,
      "step": 372700
    },
    {
      "epoch": 29.202569324768916,
      "grad_norm": 4.615071773529053,
      "learning_rate": 2.5664525562692572e-05,
      "loss": 1.6557,
      "step": 372800
    },
    {
      "epoch": 29.210402631991226,
      "grad_norm": 5.240508556365967,
      "learning_rate": 2.5657997806673978e-05,
      "loss": 1.7224,
      "step": 372900
    },
    {
      "epoch": 29.218235939213535,
      "grad_norm": 8.776010513305664,
      "learning_rate": 2.565147005065539e-05,
      "loss": 1.734,
      "step": 373000
    },
    {
      "epoch": 29.226069246435845,
      "grad_norm": 9.105987548828125,
      "learning_rate": 2.5644942294636796e-05,
      "loss": 1.6521,
      "step": 373100
    },
    {
      "epoch": 29.233902553658154,
      "grad_norm": 6.797991752624512,
      "learning_rate": 2.5638414538618205e-05,
      "loss": 1.6983,
      "step": 373200
    },
    {
      "epoch": 29.241735860880464,
      "grad_norm": 6.900949001312256,
      "learning_rate": 2.563188678259961e-05,
      "loss": 1.6532,
      "step": 373300
    },
    {
      "epoch": 29.249569168102774,
      "grad_norm": 6.722201347351074,
      "learning_rate": 2.5625359026581024e-05,
      "loss": 1.6198,
      "step": 373400
    },
    {
      "epoch": 29.257402475325083,
      "grad_norm": 8.273555755615234,
      "learning_rate": 2.5618831270562433e-05,
      "loss": 1.7755,
      "step": 373500
    },
    {
      "epoch": 29.265235782547393,
      "grad_norm": 6.633912563323975,
      "learning_rate": 2.561230351454384e-05,
      "loss": 1.644,
      "step": 373600
    },
    {
      "epoch": 29.273069089769702,
      "grad_norm": 6.847485065460205,
      "learning_rate": 2.560577575852525e-05,
      "loss": 1.7013,
      "step": 373700
    },
    {
      "epoch": 29.28090239699201,
      "grad_norm": 5.973060607910156,
      "learning_rate": 2.559924800250666e-05,
      "loss": 1.6335,
      "step": 373800
    },
    {
      "epoch": 29.288735704214318,
      "grad_norm": 6.3334574699401855,
      "learning_rate": 2.5592720246488066e-05,
      "loss": 1.6286,
      "step": 373900
    },
    {
      "epoch": 29.296569011436628,
      "grad_norm": 8.574767112731934,
      "learning_rate": 2.558619249046948e-05,
      "loss": 1.6452,
      "step": 374000
    },
    {
      "epoch": 29.304402318658937,
      "grad_norm": 5.8645710945129395,
      "learning_rate": 2.5579664734450888e-05,
      "loss": 1.6214,
      "step": 374100
    },
    {
      "epoch": 29.312235625881247,
      "grad_norm": 6.6235575675964355,
      "learning_rate": 2.5573136978432294e-05,
      "loss": 1.7557,
      "step": 374200
    },
    {
      "epoch": 29.320068933103556,
      "grad_norm": 7.148390769958496,
      "learning_rate": 2.5566609222413706e-05,
      "loss": 1.6645,
      "step": 374300
    },
    {
      "epoch": 29.327902240325866,
      "grad_norm": 5.448415756225586,
      "learning_rate": 2.5560081466395115e-05,
      "loss": 1.7084,
      "step": 374400
    },
    {
      "epoch": 29.335735547548175,
      "grad_norm": 5.708821773529053,
      "learning_rate": 2.555355371037652e-05,
      "loss": 1.712,
      "step": 374500
    },
    {
      "epoch": 29.343568854770485,
      "grad_norm": 6.976916313171387,
      "learning_rate": 2.5547025954357934e-05,
      "loss": 1.6068,
      "step": 374600
    },
    {
      "epoch": 29.351402161992795,
      "grad_norm": 7.5331010818481445,
      "learning_rate": 2.554049819833934e-05,
      "loss": 1.613,
      "step": 374700
    },
    {
      "epoch": 29.359235469215104,
      "grad_norm": 6.481563091278076,
      "learning_rate": 2.553397044232075e-05,
      "loss": 1.6699,
      "step": 374800
    },
    {
      "epoch": 29.367068776437414,
      "grad_norm": 8.778680801391602,
      "learning_rate": 2.5527442686302154e-05,
      "loss": 1.6994,
      "step": 374900
    },
    {
      "epoch": 29.37490208365972,
      "grad_norm": 6.045337677001953,
      "learning_rate": 2.5520914930283567e-05,
      "loss": 1.6274,
      "step": 375000
    },
    {
      "epoch": 29.38273539088203,
      "grad_norm": 8.311396598815918,
      "learning_rate": 2.5514387174264976e-05,
      "loss": 1.7833,
      "step": 375100
    },
    {
      "epoch": 29.39056869810434,
      "grad_norm": 5.2041192054748535,
      "learning_rate": 2.5507859418246382e-05,
      "loss": 1.7143,
      "step": 375200
    },
    {
      "epoch": 29.39840200532665,
      "grad_norm": 6.31348991394043,
      "learning_rate": 2.5501331662227794e-05,
      "loss": 1.7595,
      "step": 375300
    },
    {
      "epoch": 29.406235312548958,
      "grad_norm": 6.0774335861206055,
      "learning_rate": 2.5494803906209203e-05,
      "loss": 1.7579,
      "step": 375400
    },
    {
      "epoch": 29.414068619771268,
      "grad_norm": 6.022188186645508,
      "learning_rate": 2.548827615019061e-05,
      "loss": 1.7253,
      "step": 375500
    },
    {
      "epoch": 29.421901926993577,
      "grad_norm": 7.579689025878906,
      "learning_rate": 2.5481748394172022e-05,
      "loss": 1.6535,
      "step": 375600
    },
    {
      "epoch": 29.429735234215887,
      "grad_norm": 6.713321685791016,
      "learning_rate": 2.547522063815343e-05,
      "loss": 1.7014,
      "step": 375700
    },
    {
      "epoch": 29.437568541438196,
      "grad_norm": 6.399442672729492,
      "learning_rate": 2.5468692882134837e-05,
      "loss": 1.6348,
      "step": 375800
    },
    {
      "epoch": 29.445401848660506,
      "grad_norm": 5.40228271484375,
      "learning_rate": 2.546216512611625e-05,
      "loss": 1.6888,
      "step": 375900
    },
    {
      "epoch": 29.453235155882815,
      "grad_norm": 6.685108184814453,
      "learning_rate": 2.545563737009766e-05,
      "loss": 1.7513,
      "step": 376000
    },
    {
      "epoch": 29.46106846310512,
      "grad_norm": 5.272825717926025,
      "learning_rate": 2.5449109614079064e-05,
      "loss": 1.6478,
      "step": 376100
    },
    {
      "epoch": 29.46890177032743,
      "grad_norm": 7.608333587646484,
      "learning_rate": 2.5442581858060477e-05,
      "loss": 1.6227,
      "step": 376200
    },
    {
      "epoch": 29.47673507754974,
      "grad_norm": 4.922985553741455,
      "learning_rate": 2.5436054102041882e-05,
      "loss": 1.7248,
      "step": 376300
    },
    {
      "epoch": 29.48456838477205,
      "grad_norm": 7.047316074371338,
      "learning_rate": 2.542952634602329e-05,
      "loss": 1.6415,
      "step": 376400
    },
    {
      "epoch": 29.49240169199436,
      "grad_norm": 6.145280361175537,
      "learning_rate": 2.5422998590004697e-05,
      "loss": 1.6309,
      "step": 376500
    },
    {
      "epoch": 29.50023499921667,
      "grad_norm": 6.111328125,
      "learning_rate": 2.541647083398611e-05,
      "loss": 1.6644,
      "step": 376600
    },
    {
      "epoch": 29.50806830643898,
      "grad_norm": 6.230031967163086,
      "learning_rate": 2.540994307796752e-05,
      "loss": 1.7759,
      "step": 376700
    },
    {
      "epoch": 29.51590161366129,
      "grad_norm": 6.978379726409912,
      "learning_rate": 2.5403415321948925e-05,
      "loss": 1.644,
      "step": 376800
    },
    {
      "epoch": 29.523734920883598,
      "grad_norm": 7.256933212280273,
      "learning_rate": 2.5396887565930337e-05,
      "loss": 1.6831,
      "step": 376900
    },
    {
      "epoch": 29.531568228105908,
      "grad_norm": 7.5397467613220215,
      "learning_rate": 2.5390359809911747e-05,
      "loss": 1.6775,
      "step": 377000
    },
    {
      "epoch": 29.539401535328217,
      "grad_norm": 6.777020454406738,
      "learning_rate": 2.5383832053893152e-05,
      "loss": 1.7264,
      "step": 377100
    },
    {
      "epoch": 29.547234842550523,
      "grad_norm": 7.0619378089904785,
      "learning_rate": 2.5377304297874565e-05,
      "loss": 1.7479,
      "step": 377200
    },
    {
      "epoch": 29.555068149772833,
      "grad_norm": 4.3421101570129395,
      "learning_rate": 2.5370776541855974e-05,
      "loss": 1.6764,
      "step": 377300
    },
    {
      "epoch": 29.562901456995142,
      "grad_norm": 4.910360813140869,
      "learning_rate": 2.536424878583738e-05,
      "loss": 1.6866,
      "step": 377400
    },
    {
      "epoch": 29.570734764217452,
      "grad_norm": 6.894341468811035,
      "learning_rate": 2.5357721029818792e-05,
      "loss": 1.7099,
      "step": 377500
    },
    {
      "epoch": 29.57856807143976,
      "grad_norm": 5.327122688293457,
      "learning_rate": 2.53511932738002e-05,
      "loss": 1.6441,
      "step": 377600
    },
    {
      "epoch": 29.58640137866207,
      "grad_norm": 5.7489237785339355,
      "learning_rate": 2.5344665517781607e-05,
      "loss": 1.6637,
      "step": 377700
    },
    {
      "epoch": 29.59423468588438,
      "grad_norm": 3.9582533836364746,
      "learning_rate": 2.533813776176302e-05,
      "loss": 1.6956,
      "step": 377800
    },
    {
      "epoch": 29.60206799310669,
      "grad_norm": 6.775559425354004,
      "learning_rate": 2.5331610005744426e-05,
      "loss": 1.6631,
      "step": 377900
    },
    {
      "epoch": 29.609901300329,
      "grad_norm": 7.173839092254639,
      "learning_rate": 2.5325082249725835e-05,
      "loss": 1.7224,
      "step": 378000
    },
    {
      "epoch": 29.61773460755131,
      "grad_norm": 5.798106670379639,
      "learning_rate": 2.531855449370724e-05,
      "loss": 1.7791,
      "step": 378100
    },
    {
      "epoch": 29.62556791477362,
      "grad_norm": 6.400971412658691,
      "learning_rate": 2.5312026737688653e-05,
      "loss": 1.7148,
      "step": 378200
    },
    {
      "epoch": 29.633401221995925,
      "grad_norm": 4.8558149337768555,
      "learning_rate": 2.5305498981670062e-05,
      "loss": 1.7488,
      "step": 378300
    },
    {
      "epoch": 29.641234529218234,
      "grad_norm": 7.3969645500183105,
      "learning_rate": 2.5298971225651468e-05,
      "loss": 1.6335,
      "step": 378400
    },
    {
      "epoch": 29.649067836440544,
      "grad_norm": 4.482174396514893,
      "learning_rate": 2.529244346963288e-05,
      "loss": 1.7347,
      "step": 378500
    },
    {
      "epoch": 29.656901143662854,
      "grad_norm": 5.671955108642578,
      "learning_rate": 2.528591571361429e-05,
      "loss": 1.6742,
      "step": 378600
    },
    {
      "epoch": 29.664734450885163,
      "grad_norm": 6.196183204650879,
      "learning_rate": 2.5279387957595695e-05,
      "loss": 1.6976,
      "step": 378700
    },
    {
      "epoch": 29.672567758107473,
      "grad_norm": 5.83505392074585,
      "learning_rate": 2.5272860201577108e-05,
      "loss": 1.6661,
      "step": 378800
    },
    {
      "epoch": 29.680401065329782,
      "grad_norm": 7.650895595550537,
      "learning_rate": 2.5266332445558517e-05,
      "loss": 1.7041,
      "step": 378900
    },
    {
      "epoch": 29.688234372552092,
      "grad_norm": 6.613894939422607,
      "learning_rate": 2.5259804689539923e-05,
      "loss": 1.7059,
      "step": 379000
    },
    {
      "epoch": 29.6960676797744,
      "grad_norm": 6.316540241241455,
      "learning_rate": 2.5253276933521335e-05,
      "loss": 1.6931,
      "step": 379100
    },
    {
      "epoch": 29.70390098699671,
      "grad_norm": 7.118444919586182,
      "learning_rate": 2.5246749177502745e-05,
      "loss": 1.5971,
      "step": 379200
    },
    {
      "epoch": 29.71173429421902,
      "grad_norm": 7.678362846374512,
      "learning_rate": 2.524022142148415e-05,
      "loss": 1.6128,
      "step": 379300
    },
    {
      "epoch": 29.71956760144133,
      "grad_norm": 5.556310176849365,
      "learning_rate": 2.5233693665465563e-05,
      "loss": 1.7579,
      "step": 379400
    },
    {
      "epoch": 29.727400908663636,
      "grad_norm": 5.833585262298584,
      "learning_rate": 2.522716590944697e-05,
      "loss": 1.6743,
      "step": 379500
    },
    {
      "epoch": 29.735234215885946,
      "grad_norm": 6.69159460067749,
      "learning_rate": 2.5220638153428378e-05,
      "loss": 1.6848,
      "step": 379600
    },
    {
      "epoch": 29.743067523108255,
      "grad_norm": 7.107185363769531,
      "learning_rate": 2.521411039740979e-05,
      "loss": 1.5363,
      "step": 379700
    },
    {
      "epoch": 29.750900830330565,
      "grad_norm": 6.002321243286133,
      "learning_rate": 2.5207582641391196e-05,
      "loss": 1.6939,
      "step": 379800
    },
    {
      "epoch": 29.758734137552874,
      "grad_norm": 7.671699047088623,
      "learning_rate": 2.5201054885372605e-05,
      "loss": 1.6598,
      "step": 379900
    },
    {
      "epoch": 29.766567444775184,
      "grad_norm": 6.154739856719971,
      "learning_rate": 2.519452712935401e-05,
      "loss": 1.6635,
      "step": 380000
    },
    {
      "epoch": 29.774400751997494,
      "grad_norm": 4.8510637283325195,
      "learning_rate": 2.5187999373335424e-05,
      "loss": 1.6641,
      "step": 380100
    },
    {
      "epoch": 29.782234059219803,
      "grad_norm": 5.4115753173828125,
      "learning_rate": 2.5181471617316833e-05,
      "loss": 1.686,
      "step": 380200
    },
    {
      "epoch": 29.790067366442113,
      "grad_norm": 5.333044052124023,
      "learning_rate": 2.517494386129824e-05,
      "loss": 1.705,
      "step": 380300
    },
    {
      "epoch": 29.797900673664422,
      "grad_norm": 5.110471248626709,
      "learning_rate": 2.516841610527965e-05,
      "loss": 1.6522,
      "step": 380400
    },
    {
      "epoch": 29.805733980886732,
      "grad_norm": 6.568271160125732,
      "learning_rate": 2.516188834926106e-05,
      "loss": 1.6212,
      "step": 380500
    },
    {
      "epoch": 29.81356728810904,
      "grad_norm": 6.78879976272583,
      "learning_rate": 2.5155360593242466e-05,
      "loss": 1.7226,
      "step": 380600
    },
    {
      "epoch": 29.821400595331347,
      "grad_norm": 4.829015254974365,
      "learning_rate": 2.514883283722388e-05,
      "loss": 1.7214,
      "step": 380700
    },
    {
      "epoch": 29.829233902553657,
      "grad_norm": 4.698103427886963,
      "learning_rate": 2.5142305081205288e-05,
      "loss": 1.7523,
      "step": 380800
    },
    {
      "epoch": 29.837067209775967,
      "grad_norm": 6.538991928100586,
      "learning_rate": 2.5135777325186693e-05,
      "loss": 1.7498,
      "step": 380900
    },
    {
      "epoch": 29.844900516998276,
      "grad_norm": 7.857959747314453,
      "learning_rate": 2.5129249569168106e-05,
      "loss": 1.7543,
      "step": 381000
    },
    {
      "epoch": 29.852733824220586,
      "grad_norm": 6.436624050140381,
      "learning_rate": 2.5122721813149512e-05,
      "loss": 1.6854,
      "step": 381100
    },
    {
      "epoch": 29.860567131442895,
      "grad_norm": 6.929923057556152,
      "learning_rate": 2.511619405713092e-05,
      "loss": 1.7429,
      "step": 381200
    },
    {
      "epoch": 29.868400438665205,
      "grad_norm": 7.945666790008545,
      "learning_rate": 2.5109666301112333e-05,
      "loss": 1.6385,
      "step": 381300
    },
    {
      "epoch": 29.876233745887514,
      "grad_norm": 6.429987907409668,
      "learning_rate": 2.510313854509374e-05,
      "loss": 1.7716,
      "step": 381400
    },
    {
      "epoch": 29.884067053109824,
      "grad_norm": 5.633700847625732,
      "learning_rate": 2.509661078907515e-05,
      "loss": 1.5949,
      "step": 381500
    },
    {
      "epoch": 29.891900360332134,
      "grad_norm": 5.942412376403809,
      "learning_rate": 2.5090083033056554e-05,
      "loss": 1.6677,
      "step": 381600
    },
    {
      "epoch": 29.899733667554443,
      "grad_norm": 5.825905799865723,
      "learning_rate": 2.5083555277037967e-05,
      "loss": 1.6975,
      "step": 381700
    },
    {
      "epoch": 29.90756697477675,
      "grad_norm": 5.815174102783203,
      "learning_rate": 2.5077027521019376e-05,
      "loss": 1.7155,
      "step": 381800
    },
    {
      "epoch": 29.91540028199906,
      "grad_norm": 4.981179714202881,
      "learning_rate": 2.507049976500078e-05,
      "loss": 1.689,
      "step": 381900
    },
    {
      "epoch": 29.92323358922137,
      "grad_norm": 5.876561641693115,
      "learning_rate": 2.5063972008982194e-05,
      "loss": 1.6736,
      "step": 382000
    },
    {
      "epoch": 29.931066896443678,
      "grad_norm": 6.425660133361816,
      "learning_rate": 2.5057444252963603e-05,
      "loss": 1.7001,
      "step": 382100
    },
    {
      "epoch": 29.938900203665987,
      "grad_norm": 8.611370086669922,
      "learning_rate": 2.505091649694501e-05,
      "loss": 1.6658,
      "step": 382200
    },
    {
      "epoch": 29.946733510888297,
      "grad_norm": 4.548445701599121,
      "learning_rate": 2.504438874092642e-05,
      "loss": 1.6876,
      "step": 382300
    },
    {
      "epoch": 29.954566818110607,
      "grad_norm": 5.1708550453186035,
      "learning_rate": 2.503786098490783e-05,
      "loss": 1.6355,
      "step": 382400
    },
    {
      "epoch": 29.962400125332916,
      "grad_norm": 5.243041515350342,
      "learning_rate": 2.5031333228889237e-05,
      "loss": 1.7997,
      "step": 382500
    },
    {
      "epoch": 29.970233432555226,
      "grad_norm": 8.49841594696045,
      "learning_rate": 2.502480547287065e-05,
      "loss": 1.7446,
      "step": 382600
    },
    {
      "epoch": 29.978066739777535,
      "grad_norm": 4.8212971687316895,
      "learning_rate": 2.5018277716852055e-05,
      "loss": 1.7547,
      "step": 382700
    },
    {
      "epoch": 29.985900046999845,
      "grad_norm": 5.605349540710449,
      "learning_rate": 2.5011749960833464e-05,
      "loss": 1.752,
      "step": 382800
    },
    {
      "epoch": 29.99373335422215,
      "grad_norm": 5.7011871337890625,
      "learning_rate": 2.5005222204814877e-05,
      "loss": 1.7071,
      "step": 382900
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.7800580263137817,
      "eval_runtime": 1.505,
      "eval_samples_per_second": 446.52,
      "eval_steps_per_second": 446.52,
      "step": 382980
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.4450546503067017,
      "eval_runtime": 36.1695,
      "eval_samples_per_second": 352.949,
      "eval_steps_per_second": 352.949,
      "step": 382980
    },
    {
      "epoch": 30.00156666144446,
      "grad_norm": 6.542777061462402,
      "learning_rate": 2.4998694448796282e-05,
      "loss": 1.7791,
      "step": 383000
    },
    {
      "epoch": 30.00939996866677,
      "grad_norm": 4.781476974487305,
      "learning_rate": 2.499216669277769e-05,
      "loss": 1.6143,
      "step": 383100
    },
    {
      "epoch": 30.01723327588908,
      "grad_norm": 7.092007637023926,
      "learning_rate": 2.49856389367591e-05,
      "loss": 1.6355,
      "step": 383200
    },
    {
      "epoch": 30.02506658311139,
      "grad_norm": 8.088309288024902,
      "learning_rate": 2.497911118074051e-05,
      "loss": 1.7301,
      "step": 383300
    },
    {
      "epoch": 30.0328998903337,
      "grad_norm": 7.786987781524658,
      "learning_rate": 2.497258342472192e-05,
      "loss": 1.708,
      "step": 383400
    },
    {
      "epoch": 30.04073319755601,
      "grad_norm": 7.757772922515869,
      "learning_rate": 2.4966055668703328e-05,
      "loss": 1.6203,
      "step": 383500
    },
    {
      "epoch": 30.048566504778318,
      "grad_norm": 5.100827217102051,
      "learning_rate": 2.4959527912684734e-05,
      "loss": 1.6038,
      "step": 383600
    },
    {
      "epoch": 30.056399812000627,
      "grad_norm": 6.630222320556641,
      "learning_rate": 2.4953000156666146e-05,
      "loss": 1.6692,
      "step": 383700
    },
    {
      "epoch": 30.064233119222937,
      "grad_norm": 5.415098190307617,
      "learning_rate": 2.4946472400647556e-05,
      "loss": 1.7365,
      "step": 383800
    },
    {
      "epoch": 30.072066426445247,
      "grad_norm": 6.645745277404785,
      "learning_rate": 2.493994464462896e-05,
      "loss": 1.7681,
      "step": 383900
    },
    {
      "epoch": 30.079899733667556,
      "grad_norm": 6.24583625793457,
      "learning_rate": 2.4933416888610374e-05,
      "loss": 1.6687,
      "step": 384000
    },
    {
      "epoch": 30.087733040889862,
      "grad_norm": 6.112700939178467,
      "learning_rate": 2.4926889132591783e-05,
      "loss": 1.6724,
      "step": 384100
    },
    {
      "epoch": 30.095566348112172,
      "grad_norm": 5.943938732147217,
      "learning_rate": 2.492036137657319e-05,
      "loss": 1.7436,
      "step": 384200
    },
    {
      "epoch": 30.10339965533448,
      "grad_norm": 5.890546798706055,
      "learning_rate": 2.4913833620554598e-05,
      "loss": 1.6598,
      "step": 384300
    },
    {
      "epoch": 30.11123296255679,
      "grad_norm": 5.650106906890869,
      "learning_rate": 2.490730586453601e-05,
      "loss": 1.5364,
      "step": 384400
    },
    {
      "epoch": 30.1190662697791,
      "grad_norm": 5.877457618713379,
      "learning_rate": 2.4900778108517416e-05,
      "loss": 1.6855,
      "step": 384500
    },
    {
      "epoch": 30.12689957700141,
      "grad_norm": 3.145355224609375,
      "learning_rate": 2.4894250352498825e-05,
      "loss": 1.6535,
      "step": 384600
    },
    {
      "epoch": 30.13473288422372,
      "grad_norm": 5.646071910858154,
      "learning_rate": 2.4887722596480235e-05,
      "loss": 1.673,
      "step": 384700
    },
    {
      "epoch": 30.14256619144603,
      "grad_norm": 5.0385212898254395,
      "learning_rate": 2.4881194840461644e-05,
      "loss": 1.6606,
      "step": 384800
    },
    {
      "epoch": 30.15039949866834,
      "grad_norm": 7.31695556640625,
      "learning_rate": 2.4874667084443053e-05,
      "loss": 1.7016,
      "step": 384900
    },
    {
      "epoch": 30.15823280589065,
      "grad_norm": 5.891234397888184,
      "learning_rate": 2.4868139328424462e-05,
      "loss": 1.7175,
      "step": 385000
    },
    {
      "epoch": 30.166066113112958,
      "grad_norm": 4.507506370544434,
      "learning_rate": 2.486161157240587e-05,
      "loss": 1.7149,
      "step": 385100
    },
    {
      "epoch": 30.173899420335264,
      "grad_norm": 4.762602806091309,
      "learning_rate": 2.485508381638728e-05,
      "loss": 1.7334,
      "step": 385200
    },
    {
      "epoch": 30.181732727557574,
      "grad_norm": 6.410942554473877,
      "learning_rate": 2.484855606036869e-05,
      "loss": 1.6968,
      "step": 385300
    },
    {
      "epoch": 30.189566034779883,
      "grad_norm": 5.929881572723389,
      "learning_rate": 2.48420283043501e-05,
      "loss": 1.6896,
      "step": 385400
    },
    {
      "epoch": 30.197399342002193,
      "grad_norm": 8.069868087768555,
      "learning_rate": 2.4835500548331504e-05,
      "loss": 1.759,
      "step": 385500
    },
    {
      "epoch": 30.205232649224502,
      "grad_norm": 5.332417011260986,
      "learning_rate": 2.4828972792312917e-05,
      "loss": 1.704,
      "step": 385600
    },
    {
      "epoch": 30.213065956446812,
      "grad_norm": 6.607368469238281,
      "learning_rate": 2.4822445036294326e-05,
      "loss": 1.7344,
      "step": 385700
    },
    {
      "epoch": 30.22089926366912,
      "grad_norm": 6.6358962059021,
      "learning_rate": 2.4815917280275732e-05,
      "loss": 1.7424,
      "step": 385800
    },
    {
      "epoch": 30.22873257089143,
      "grad_norm": 6.290672779083252,
      "learning_rate": 2.480938952425714e-05,
      "loss": 1.7441,
      "step": 385900
    },
    {
      "epoch": 30.23656587811374,
      "grad_norm": 5.264052391052246,
      "learning_rate": 2.4802861768238554e-05,
      "loss": 1.6623,
      "step": 386000
    },
    {
      "epoch": 30.24439918533605,
      "grad_norm": 7.369937419891357,
      "learning_rate": 2.479633401221996e-05,
      "loss": 1.6219,
      "step": 386100
    },
    {
      "epoch": 30.25223249255836,
      "grad_norm": 5.449853420257568,
      "learning_rate": 2.478980625620137e-05,
      "loss": 1.5758,
      "step": 386200
    },
    {
      "epoch": 30.260065799780666,
      "grad_norm": 5.540972709655762,
      "learning_rate": 2.4783278500182778e-05,
      "loss": 1.6073,
      "step": 386300
    },
    {
      "epoch": 30.267899107002975,
      "grad_norm": 6.308067798614502,
      "learning_rate": 2.4776750744164187e-05,
      "loss": 1.6478,
      "step": 386400
    },
    {
      "epoch": 30.275732414225285,
      "grad_norm": 5.381731033325195,
      "learning_rate": 2.4770222988145596e-05,
      "loss": 1.6741,
      "step": 386500
    },
    {
      "epoch": 30.283565721447594,
      "grad_norm": 6.37380838394165,
      "learning_rate": 2.4763695232127005e-05,
      "loss": 1.679,
      "step": 386600
    },
    {
      "epoch": 30.291399028669904,
      "grad_norm": 9.455320358276367,
      "learning_rate": 2.4757167476108414e-05,
      "loss": 1.6527,
      "step": 386700
    },
    {
      "epoch": 30.299232335892214,
      "grad_norm": 5.932353973388672,
      "learning_rate": 2.4750639720089824e-05,
      "loss": 1.7356,
      "step": 386800
    },
    {
      "epoch": 30.307065643114523,
      "grad_norm": 5.810090065002441,
      "learning_rate": 2.4744111964071233e-05,
      "loss": 1.7384,
      "step": 386900
    },
    {
      "epoch": 30.314898950336833,
      "grad_norm": 4.035346508026123,
      "learning_rate": 2.4737584208052642e-05,
      "loss": 1.7665,
      "step": 387000
    },
    {
      "epoch": 30.322732257559142,
      "grad_norm": 7.210624694824219,
      "learning_rate": 2.4731056452034048e-05,
      "loss": 1.6334,
      "step": 387100
    },
    {
      "epoch": 30.330565564781452,
      "grad_norm": 6.8443732261657715,
      "learning_rate": 2.472452869601546e-05,
      "loss": 1.7793,
      "step": 387200
    },
    {
      "epoch": 30.33839887200376,
      "grad_norm": 6.799034595489502,
      "learning_rate": 2.471800093999687e-05,
      "loss": 1.6825,
      "step": 387300
    },
    {
      "epoch": 30.34623217922607,
      "grad_norm": 7.541965961456299,
      "learning_rate": 2.4711473183978275e-05,
      "loss": 1.8079,
      "step": 387400
    },
    {
      "epoch": 30.354065486448377,
      "grad_norm": 5.505828380584717,
      "learning_rate": 2.4704945427959684e-05,
      "loss": 1.7183,
      "step": 387500
    },
    {
      "epoch": 30.361898793670687,
      "grad_norm": 5.716728687286377,
      "learning_rate": 2.4698417671941097e-05,
      "loss": 1.7569,
      "step": 387600
    },
    {
      "epoch": 30.369732100892996,
      "grad_norm": 5.402038097381592,
      "learning_rate": 2.4691889915922503e-05,
      "loss": 1.6852,
      "step": 387700
    },
    {
      "epoch": 30.377565408115306,
      "grad_norm": 6.586706161499023,
      "learning_rate": 2.4685362159903912e-05,
      "loss": 1.5795,
      "step": 387800
    },
    {
      "epoch": 30.385398715337615,
      "grad_norm": 6.385010242462158,
      "learning_rate": 2.467883440388532e-05,
      "loss": 1.7542,
      "step": 387900
    },
    {
      "epoch": 30.393232022559925,
      "grad_norm": 4.858890056610107,
      "learning_rate": 2.467230664786673e-05,
      "loss": 1.7503,
      "step": 388000
    },
    {
      "epoch": 30.401065329782234,
      "grad_norm": 4.979510307312012,
      "learning_rate": 2.466577889184814e-05,
      "loss": 1.6527,
      "step": 388100
    },
    {
      "epoch": 30.408898637004544,
      "grad_norm": 9.510805130004883,
      "learning_rate": 2.465925113582955e-05,
      "loss": 1.6439,
      "step": 388200
    },
    {
      "epoch": 30.416731944226854,
      "grad_norm": 5.739162445068359,
      "learning_rate": 2.4652723379810957e-05,
      "loss": 1.6653,
      "step": 388300
    },
    {
      "epoch": 30.424565251449163,
      "grad_norm": 6.023935317993164,
      "learning_rate": 2.4646195623792367e-05,
      "loss": 1.6735,
      "step": 388400
    },
    {
      "epoch": 30.432398558671473,
      "grad_norm": 7.06978702545166,
      "learning_rate": 2.4639667867773776e-05,
      "loss": 1.6638,
      "step": 388500
    },
    {
      "epoch": 30.44023186589378,
      "grad_norm": 5.545316219329834,
      "learning_rate": 2.4633140111755185e-05,
      "loss": 1.7079,
      "step": 388600
    },
    {
      "epoch": 30.44806517311609,
      "grad_norm": 6.073758602142334,
      "learning_rate": 2.462661235573659e-05,
      "loss": 1.6475,
      "step": 388700
    },
    {
      "epoch": 30.455898480338398,
      "grad_norm": 6.450587272644043,
      "learning_rate": 2.4620084599718003e-05,
      "loss": 1.7159,
      "step": 388800
    },
    {
      "epoch": 30.463731787560707,
      "grad_norm": 4.358129978179932,
      "learning_rate": 2.4613556843699412e-05,
      "loss": 1.6839,
      "step": 388900
    },
    {
      "epoch": 30.471565094783017,
      "grad_norm": 6.666938304901123,
      "learning_rate": 2.4607029087680818e-05,
      "loss": 1.7037,
      "step": 389000
    },
    {
      "epoch": 30.479398402005327,
      "grad_norm": 6.982847213745117,
      "learning_rate": 2.4600501331662227e-05,
      "loss": 1.6871,
      "step": 389100
    },
    {
      "epoch": 30.487231709227636,
      "grad_norm": 5.814908981323242,
      "learning_rate": 2.459397357564364e-05,
      "loss": 1.5801,
      "step": 389200
    },
    {
      "epoch": 30.495065016449946,
      "grad_norm": 7.142263889312744,
      "learning_rate": 2.4587445819625046e-05,
      "loss": 1.6237,
      "step": 389300
    },
    {
      "epoch": 30.502898323672255,
      "grad_norm": 6.89280366897583,
      "learning_rate": 2.4580918063606455e-05,
      "loss": 1.8088,
      "step": 389400
    },
    {
      "epoch": 30.510731630894565,
      "grad_norm": 6.00979471206665,
      "learning_rate": 2.4574390307587864e-05,
      "loss": 1.7193,
      "step": 389500
    },
    {
      "epoch": 30.518564938116874,
      "grad_norm": 6.01514196395874,
      "learning_rate": 2.4567862551569273e-05,
      "loss": 1.7059,
      "step": 389600
    },
    {
      "epoch": 30.52639824533918,
      "grad_norm": 5.6237568855285645,
      "learning_rate": 2.4561334795550682e-05,
      "loss": 1.7602,
      "step": 389700
    },
    {
      "epoch": 30.53423155256149,
      "grad_norm": 6.253465175628662,
      "learning_rate": 2.455480703953209e-05,
      "loss": 1.6497,
      "step": 389800
    },
    {
      "epoch": 30.5420648597838,
      "grad_norm": 6.944881439208984,
      "learning_rate": 2.45482792835135e-05,
      "loss": 1.6958,
      "step": 389900
    },
    {
      "epoch": 30.54989816700611,
      "grad_norm": 6.102910995483398,
      "learning_rate": 2.454175152749491e-05,
      "loss": 1.7378,
      "step": 390000
    },
    {
      "epoch": 30.55773147422842,
      "grad_norm": 7.467772483825684,
      "learning_rate": 2.453522377147632e-05,
      "loss": 1.6317,
      "step": 390100
    },
    {
      "epoch": 30.56556478145073,
      "grad_norm": 8.140321731567383,
      "learning_rate": 2.4528696015457728e-05,
      "loss": 1.7357,
      "step": 390200
    },
    {
      "epoch": 30.573398088673038,
      "grad_norm": 4.843563556671143,
      "learning_rate": 2.4522168259439137e-05,
      "loss": 1.6762,
      "step": 390300
    },
    {
      "epoch": 30.581231395895347,
      "grad_norm": 6.547203540802002,
      "learning_rate": 2.4515640503420546e-05,
      "loss": 1.7091,
      "step": 390400
    },
    {
      "epoch": 30.589064703117657,
      "grad_norm": 5.7136311531066895,
      "learning_rate": 2.4509112747401956e-05,
      "loss": 1.6665,
      "step": 390500
    },
    {
      "epoch": 30.596898010339967,
      "grad_norm": 7.273599147796631,
      "learning_rate": 2.450258499138336e-05,
      "loss": 1.7339,
      "step": 390600
    },
    {
      "epoch": 30.604731317562276,
      "grad_norm": 5.2110185623168945,
      "learning_rate": 2.449605723536477e-05,
      "loss": 1.7482,
      "step": 390700
    },
    {
      "epoch": 30.612564624784586,
      "grad_norm": 6.360124588012695,
      "learning_rate": 2.4489529479346183e-05,
      "loss": 1.6815,
      "step": 390800
    },
    {
      "epoch": 30.62039793200689,
      "grad_norm": 5.764978408813477,
      "learning_rate": 2.448300172332759e-05,
      "loss": 1.6215,
      "step": 390900
    },
    {
      "epoch": 30.6282312392292,
      "grad_norm": 5.150896072387695,
      "learning_rate": 2.4476473967308998e-05,
      "loss": 1.5928,
      "step": 391000
    },
    {
      "epoch": 30.63606454645151,
      "grad_norm": 7.158154487609863,
      "learning_rate": 2.446994621129041e-05,
      "loss": 1.583,
      "step": 391100
    },
    {
      "epoch": 30.64389785367382,
      "grad_norm": 4.932797908782959,
      "learning_rate": 2.4463418455271816e-05,
      "loss": 1.6826,
      "step": 391200
    },
    {
      "epoch": 30.65173116089613,
      "grad_norm": 6.6125688552856445,
      "learning_rate": 2.4456890699253225e-05,
      "loss": 1.595,
      "step": 391300
    },
    {
      "epoch": 30.65956446811844,
      "grad_norm": 5.489135265350342,
      "learning_rate": 2.4450362943234635e-05,
      "loss": 1.6629,
      "step": 391400
    },
    {
      "epoch": 30.66739777534075,
      "grad_norm": 7.193421840667725,
      "learning_rate": 2.4443835187216044e-05,
      "loss": 1.6879,
      "step": 391500
    },
    {
      "epoch": 30.67523108256306,
      "grad_norm": 6.649265766143799,
      "learning_rate": 2.4437307431197453e-05,
      "loss": 1.6128,
      "step": 391600
    },
    {
      "epoch": 30.68306438978537,
      "grad_norm": 6.247601509094238,
      "learning_rate": 2.4430779675178862e-05,
      "loss": 1.6917,
      "step": 391700
    },
    {
      "epoch": 30.690897697007678,
      "grad_norm": 6.591115951538086,
      "learning_rate": 2.442425191916027e-05,
      "loss": 1.7118,
      "step": 391800
    },
    {
      "epoch": 30.698731004229987,
      "grad_norm": 6.574705123901367,
      "learning_rate": 2.441772416314168e-05,
      "loss": 1.7263,
      "step": 391900
    },
    {
      "epoch": 30.706564311452293,
      "grad_norm": 5.274892330169678,
      "learning_rate": 2.441119640712309e-05,
      "loss": 1.7395,
      "step": 392000
    },
    {
      "epoch": 30.714397618674603,
      "grad_norm": 7.360898971557617,
      "learning_rate": 2.44046686511045e-05,
      "loss": 1.6751,
      "step": 392100
    },
    {
      "epoch": 30.722230925896913,
      "grad_norm": 7.144443511962891,
      "learning_rate": 2.4398140895085904e-05,
      "loss": 1.695,
      "step": 392200
    },
    {
      "epoch": 30.730064233119222,
      "grad_norm": 6.963554382324219,
      "learning_rate": 2.4391613139067314e-05,
      "loss": 1.682,
      "step": 392300
    },
    {
      "epoch": 30.73789754034153,
      "grad_norm": 6.375462055206299,
      "learning_rate": 2.4385085383048726e-05,
      "loss": 1.6468,
      "step": 392400
    },
    {
      "epoch": 30.74573084756384,
      "grad_norm": 7.464288711547852,
      "learning_rate": 2.4378557627030132e-05,
      "loss": 1.7923,
      "step": 392500
    },
    {
      "epoch": 30.75356415478615,
      "grad_norm": 7.044816970825195,
      "learning_rate": 2.437202987101154e-05,
      "loss": 1.7212,
      "step": 392600
    },
    {
      "epoch": 30.76139746200846,
      "grad_norm": 4.940764427185059,
      "learning_rate": 2.4365502114992954e-05,
      "loss": 1.648,
      "step": 392700
    },
    {
      "epoch": 30.76923076923077,
      "grad_norm": 4.809828758239746,
      "learning_rate": 2.435897435897436e-05,
      "loss": 1.6367,
      "step": 392800
    },
    {
      "epoch": 30.77706407645308,
      "grad_norm": 6.112159252166748,
      "learning_rate": 2.435244660295577e-05,
      "loss": 1.6828,
      "step": 392900
    },
    {
      "epoch": 30.78489738367539,
      "grad_norm": 6.289001941680908,
      "learning_rate": 2.4345918846937178e-05,
      "loss": 1.6247,
      "step": 393000
    },
    {
      "epoch": 30.7927306908977,
      "grad_norm": 5.9821248054504395,
      "learning_rate": 2.4339391090918587e-05,
      "loss": 1.7073,
      "step": 393100
    },
    {
      "epoch": 30.800563998120005,
      "grad_norm": 6.420628547668457,
      "learning_rate": 2.4332863334899996e-05,
      "loss": 1.6645,
      "step": 393200
    },
    {
      "epoch": 30.808397305342314,
      "grad_norm": 5.688937187194824,
      "learning_rate": 2.4326335578881405e-05,
      "loss": 1.6285,
      "step": 393300
    },
    {
      "epoch": 30.816230612564624,
      "grad_norm": 6.724416255950928,
      "learning_rate": 2.4319807822862814e-05,
      "loss": 1.6769,
      "step": 393400
    },
    {
      "epoch": 30.824063919786933,
      "grad_norm": 7.2685041427612305,
      "learning_rate": 2.4313280066844223e-05,
      "loss": 1.6297,
      "step": 393500
    },
    {
      "epoch": 30.831897227009243,
      "grad_norm": 8.717528343200684,
      "learning_rate": 2.4306752310825633e-05,
      "loss": 1.6335,
      "step": 393600
    },
    {
      "epoch": 30.839730534231553,
      "grad_norm": 6.734769344329834,
      "learning_rate": 2.4300224554807042e-05,
      "loss": 1.7037,
      "step": 393700
    },
    {
      "epoch": 30.847563841453862,
      "grad_norm": 7.016237735748291,
      "learning_rate": 2.4293696798788448e-05,
      "loss": 1.6642,
      "step": 393800
    },
    {
      "epoch": 30.855397148676172,
      "grad_norm": 5.827826976776123,
      "learning_rate": 2.4287169042769857e-05,
      "loss": 1.6581,
      "step": 393900
    },
    {
      "epoch": 30.86323045589848,
      "grad_norm": 6.65272855758667,
      "learning_rate": 2.428064128675127e-05,
      "loss": 1.7641,
      "step": 394000
    },
    {
      "epoch": 30.87106376312079,
      "grad_norm": 8.770486831665039,
      "learning_rate": 2.4274113530732675e-05,
      "loss": 1.7149,
      "step": 394100
    },
    {
      "epoch": 30.8788970703431,
      "grad_norm": 3.1367738246917725,
      "learning_rate": 2.4267585774714084e-05,
      "loss": 1.6576,
      "step": 394200
    },
    {
      "epoch": 30.886730377565407,
      "grad_norm": 9.394455909729004,
      "learning_rate": 2.4261058018695497e-05,
      "loss": 1.6486,
      "step": 394300
    },
    {
      "epoch": 30.894563684787716,
      "grad_norm": 5.745446681976318,
      "learning_rate": 2.4254530262676902e-05,
      "loss": 1.734,
      "step": 394400
    },
    {
      "epoch": 30.902396992010026,
      "grad_norm": 8.721807479858398,
      "learning_rate": 2.424800250665831e-05,
      "loss": 1.6465,
      "step": 394500
    },
    {
      "epoch": 30.910230299232335,
      "grad_norm": 6.745692729949951,
      "learning_rate": 2.424147475063972e-05,
      "loss": 1.6793,
      "step": 394600
    },
    {
      "epoch": 30.918063606454645,
      "grad_norm": 6.881898880004883,
      "learning_rate": 2.423494699462113e-05,
      "loss": 1.6544,
      "step": 394700
    },
    {
      "epoch": 30.925896913676954,
      "grad_norm": 5.484185695648193,
      "learning_rate": 2.422841923860254e-05,
      "loss": 1.6491,
      "step": 394800
    },
    {
      "epoch": 30.933730220899264,
      "grad_norm": 7.644157886505127,
      "learning_rate": 2.4221891482583948e-05,
      "loss": 1.7108,
      "step": 394900
    },
    {
      "epoch": 30.941563528121574,
      "grad_norm": 7.3543701171875,
      "learning_rate": 2.4215363726565357e-05,
      "loss": 1.7822,
      "step": 395000
    },
    {
      "epoch": 30.949396835343883,
      "grad_norm": 5.176658630371094,
      "learning_rate": 2.4208835970546767e-05,
      "loss": 1.7018,
      "step": 395100
    },
    {
      "epoch": 30.957230142566193,
      "grad_norm": 5.296631336212158,
      "learning_rate": 2.4202308214528176e-05,
      "loss": 1.7474,
      "step": 395200
    },
    {
      "epoch": 30.965063449788502,
      "grad_norm": 5.4399213790893555,
      "learning_rate": 2.4195780458509585e-05,
      "loss": 1.7089,
      "step": 395300
    },
    {
      "epoch": 30.97289675701081,
      "grad_norm": 7.472738265991211,
      "learning_rate": 2.418925270249099e-05,
      "loss": 1.688,
      "step": 395400
    },
    {
      "epoch": 30.980730064233118,
      "grad_norm": 5.6350836753845215,
      "learning_rate": 2.41827249464724e-05,
      "loss": 1.6431,
      "step": 395500
    },
    {
      "epoch": 30.988563371455427,
      "grad_norm": 7.69849157333374,
      "learning_rate": 2.4176197190453812e-05,
      "loss": 1.6237,
      "step": 395600
    },
    {
      "epoch": 30.996396678677737,
      "grad_norm": 5.73150634765625,
      "learning_rate": 2.4169669434435218e-05,
      "loss": 1.7181,
      "step": 395700
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.77871835231781,
      "eval_runtime": 1.5311,
      "eval_samples_per_second": 438.913,
      "eval_steps_per_second": 438.913,
      "step": 395746
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.441796898841858,
      "eval_runtime": 29.4718,
      "eval_samples_per_second": 433.16,
      "eval_steps_per_second": 433.16,
      "step": 395746
    },
    {
      "epoch": 31.004229985900047,
      "grad_norm": 5.543959140777588,
      "learning_rate": 2.4163141678416627e-05,
      "loss": 1.7734,
      "step": 395800
    },
    {
      "epoch": 31.012063293122356,
      "grad_norm": 6.677624225616455,
      "learning_rate": 2.415661392239804e-05,
      "loss": 1.6501,
      "step": 395900
    },
    {
      "epoch": 31.019896600344666,
      "grad_norm": 6.094818592071533,
      "learning_rate": 2.4150086166379446e-05,
      "loss": 1.7384,
      "step": 396000
    },
    {
      "epoch": 31.027729907566975,
      "grad_norm": 6.371100425720215,
      "learning_rate": 2.4143558410360855e-05,
      "loss": 1.6643,
      "step": 396100
    },
    {
      "epoch": 31.035563214789285,
      "grad_norm": 6.612988471984863,
      "learning_rate": 2.4137030654342267e-05,
      "loss": 1.7046,
      "step": 396200
    },
    {
      "epoch": 31.043396522011594,
      "grad_norm": 5.432276725769043,
      "learning_rate": 2.4130502898323673e-05,
      "loss": 1.7549,
      "step": 396300
    },
    {
      "epoch": 31.051229829233904,
      "grad_norm": 7.745401859283447,
      "learning_rate": 2.4123975142305082e-05,
      "loss": 1.6859,
      "step": 396400
    },
    {
      "epoch": 31.059063136456214,
      "grad_norm": 5.737145900726318,
      "learning_rate": 2.411744738628649e-05,
      "loss": 1.632,
      "step": 396500
    },
    {
      "epoch": 31.06689644367852,
      "grad_norm": 6.851413726806641,
      "learning_rate": 2.41109196302679e-05,
      "loss": 1.7665,
      "step": 396600
    },
    {
      "epoch": 31.07472975090083,
      "grad_norm": 5.077342510223389,
      "learning_rate": 2.410439187424931e-05,
      "loss": 1.629,
      "step": 396700
    },
    {
      "epoch": 31.08256305812314,
      "grad_norm": 7.789459228515625,
      "learning_rate": 2.409786411823072e-05,
      "loss": 1.688,
      "step": 396800
    },
    {
      "epoch": 31.09039636534545,
      "grad_norm": 5.688416004180908,
      "learning_rate": 2.4091336362212128e-05,
      "loss": 1.6633,
      "step": 396900
    },
    {
      "epoch": 31.098229672567758,
      "grad_norm": 5.9488749504089355,
      "learning_rate": 2.4084808606193537e-05,
      "loss": 1.6246,
      "step": 397000
    },
    {
      "epoch": 31.106062979790067,
      "grad_norm": 4.79551362991333,
      "learning_rate": 2.4078280850174943e-05,
      "loss": 1.6477,
      "step": 397100
    },
    {
      "epoch": 31.113896287012377,
      "grad_norm": 7.305731296539307,
      "learning_rate": 2.4071753094156355e-05,
      "loss": 1.7113,
      "step": 397200
    },
    {
      "epoch": 31.121729594234687,
      "grad_norm": 6.691035270690918,
      "learning_rate": 2.406522533813776e-05,
      "loss": 1.5735,
      "step": 397300
    },
    {
      "epoch": 31.129562901456996,
      "grad_norm": 5.802509784698486,
      "learning_rate": 2.405869758211917e-05,
      "loss": 1.7102,
      "step": 397400
    },
    {
      "epoch": 31.137396208679306,
      "grad_norm": 5.5314154624938965,
      "learning_rate": 2.4052169826100583e-05,
      "loss": 1.6178,
      "step": 397500
    },
    {
      "epoch": 31.145229515901615,
      "grad_norm": 6.068631172180176,
      "learning_rate": 2.404564207008199e-05,
      "loss": 1.6749,
      "step": 397600
    },
    {
      "epoch": 31.15306282312392,
      "grad_norm": 5.426786422729492,
      "learning_rate": 2.4039114314063398e-05,
      "loss": 1.6823,
      "step": 397700
    },
    {
      "epoch": 31.16089613034623,
      "grad_norm": 5.360808849334717,
      "learning_rate": 2.403258655804481e-05,
      "loss": 1.6941,
      "step": 397800
    },
    {
      "epoch": 31.16872943756854,
      "grad_norm": 5.968844890594482,
      "learning_rate": 2.4026058802026216e-05,
      "loss": 1.696,
      "step": 397900
    },
    {
      "epoch": 31.17656274479085,
      "grad_norm": 5.02562141418457,
      "learning_rate": 2.4019531046007625e-05,
      "loss": 1.6655,
      "step": 398000
    },
    {
      "epoch": 31.18439605201316,
      "grad_norm": 6.605874538421631,
      "learning_rate": 2.4013003289989034e-05,
      "loss": 1.6021,
      "step": 398100
    },
    {
      "epoch": 31.19222935923547,
      "grad_norm": 5.221163749694824,
      "learning_rate": 2.4006475533970444e-05,
      "loss": 1.7562,
      "step": 398200
    },
    {
      "epoch": 31.20006266645778,
      "grad_norm": 4.533100605010986,
      "learning_rate": 2.3999947777951853e-05,
      "loss": 1.708,
      "step": 398300
    },
    {
      "epoch": 31.20789597368009,
      "grad_norm": 5.496299743652344,
      "learning_rate": 2.3993420021933262e-05,
      "loss": 1.6775,
      "step": 398400
    },
    {
      "epoch": 31.215729280902398,
      "grad_norm": 7.958763122558594,
      "learning_rate": 2.398689226591467e-05,
      "loss": 1.7447,
      "step": 398500
    },
    {
      "epoch": 31.223562588124707,
      "grad_norm": 6.00861930847168,
      "learning_rate": 2.398036450989608e-05,
      "loss": 1.7098,
      "step": 398600
    },
    {
      "epoch": 31.231395895347017,
      "grad_norm": 5.515324592590332,
      "learning_rate": 2.3973836753877486e-05,
      "loss": 1.671,
      "step": 398700
    },
    {
      "epoch": 31.239229202569323,
      "grad_norm": 5.800499439239502,
      "learning_rate": 2.39673089978589e-05,
      "loss": 1.6862,
      "step": 398800
    },
    {
      "epoch": 31.247062509791633,
      "grad_norm": 6.3977532386779785,
      "learning_rate": 2.3960781241840304e-05,
      "loss": 1.6661,
      "step": 398900
    },
    {
      "epoch": 31.254895817013942,
      "grad_norm": 6.9137701988220215,
      "learning_rate": 2.3954253485821714e-05,
      "loss": 1.6423,
      "step": 399000
    },
    {
      "epoch": 31.26272912423625,
      "grad_norm": 6.957578659057617,
      "learning_rate": 2.3947725729803126e-05,
      "loss": 1.7061,
      "step": 399100
    },
    {
      "epoch": 31.27056243145856,
      "grad_norm": 7.5101423263549805,
      "learning_rate": 2.3941197973784532e-05,
      "loss": 1.69,
      "step": 399200
    },
    {
      "epoch": 31.27839573868087,
      "grad_norm": 5.053806304931641,
      "learning_rate": 2.393467021776594e-05,
      "loss": 1.7308,
      "step": 399300
    },
    {
      "epoch": 31.28622904590318,
      "grad_norm": 5.004740238189697,
      "learning_rate": 2.3928142461747354e-05,
      "loss": 1.7076,
      "step": 399400
    },
    {
      "epoch": 31.29406235312549,
      "grad_norm": 4.822628974914551,
      "learning_rate": 2.392161470572876e-05,
      "loss": 1.6653,
      "step": 399500
    },
    {
      "epoch": 31.3018956603478,
      "grad_norm": 6.344300746917725,
      "learning_rate": 2.391508694971017e-05,
      "loss": 1.7226,
      "step": 399600
    },
    {
      "epoch": 31.30972896757011,
      "grad_norm": 6.797227382659912,
      "learning_rate": 2.3908559193691578e-05,
      "loss": 1.6876,
      "step": 399700
    },
    {
      "epoch": 31.31756227479242,
      "grad_norm": 3.8510873317718506,
      "learning_rate": 2.3902031437672987e-05,
      "loss": 1.7557,
      "step": 399800
    },
    {
      "epoch": 31.32539558201473,
      "grad_norm": 5.927575588226318,
      "learning_rate": 2.3895503681654396e-05,
      "loss": 1.6716,
      "step": 399900
    },
    {
      "epoch": 31.333228889237034,
      "grad_norm": 7.318292617797852,
      "learning_rate": 2.3888975925635805e-05,
      "loss": 1.7075,
      "step": 400000
    },
    {
      "epoch": 31.341062196459344,
      "grad_norm": 6.071827411651611,
      "learning_rate": 2.3882448169617214e-05,
      "loss": 1.6031,
      "step": 400100
    },
    {
      "epoch": 31.348895503681653,
      "grad_norm": 7.124364376068115,
      "learning_rate": 2.3875920413598623e-05,
      "loss": 1.6556,
      "step": 400200
    },
    {
      "epoch": 31.356728810903963,
      "grad_norm": 6.86112117767334,
      "learning_rate": 2.386939265758003e-05,
      "loss": 1.6514,
      "step": 400300
    },
    {
      "epoch": 31.364562118126273,
      "grad_norm": 7.564629077911377,
      "learning_rate": 2.3862864901561442e-05,
      "loss": 1.6021,
      "step": 400400
    },
    {
      "epoch": 31.372395425348582,
      "grad_norm": 4.622961521148682,
      "learning_rate": 2.3856337145542847e-05,
      "loss": 1.6373,
      "step": 400500
    },
    {
      "epoch": 31.38022873257089,
      "grad_norm": 5.62821102142334,
      "learning_rate": 2.3849809389524257e-05,
      "loss": 1.6667,
      "step": 400600
    },
    {
      "epoch": 31.3880620397932,
      "grad_norm": 6.307492733001709,
      "learning_rate": 2.384328163350567e-05,
      "loss": 1.6724,
      "step": 400700
    },
    {
      "epoch": 31.39589534701551,
      "grad_norm": 5.459537982940674,
      "learning_rate": 2.3836753877487075e-05,
      "loss": 1.7692,
      "step": 400800
    },
    {
      "epoch": 31.40372865423782,
      "grad_norm": 6.891292095184326,
      "learning_rate": 2.3830226121468484e-05,
      "loss": 1.7125,
      "step": 400900
    },
    {
      "epoch": 31.41156196146013,
      "grad_norm": 4.7185821533203125,
      "learning_rate": 2.3823698365449897e-05,
      "loss": 1.6723,
      "step": 401000
    },
    {
      "epoch": 31.419395268682436,
      "grad_norm": 6.769164085388184,
      "learning_rate": 2.3817170609431302e-05,
      "loss": 1.6731,
      "step": 401100
    },
    {
      "epoch": 31.427228575904746,
      "grad_norm": 7.556330680847168,
      "learning_rate": 2.381064285341271e-05,
      "loss": 1.6331,
      "step": 401200
    },
    {
      "epoch": 31.435061883127055,
      "grad_norm": 6.578916072845459,
      "learning_rate": 2.380411509739412e-05,
      "loss": 1.6314,
      "step": 401300
    },
    {
      "epoch": 31.442895190349365,
      "grad_norm": 5.207754135131836,
      "learning_rate": 2.379758734137553e-05,
      "loss": 1.6119,
      "step": 401400
    },
    {
      "epoch": 31.450728497571674,
      "grad_norm": 5.4813432693481445,
      "learning_rate": 2.379105958535694e-05,
      "loss": 1.7379,
      "step": 401500
    },
    {
      "epoch": 31.458561804793984,
      "grad_norm": 4.914362907409668,
      "learning_rate": 2.3784531829338348e-05,
      "loss": 1.658,
      "step": 401600
    },
    {
      "epoch": 31.466395112016293,
      "grad_norm": 5.673322677612305,
      "learning_rate": 2.3778004073319757e-05,
      "loss": 1.7302,
      "step": 401700
    },
    {
      "epoch": 31.474228419238603,
      "grad_norm": 5.65484619140625,
      "learning_rate": 2.3771476317301167e-05,
      "loss": 1.6402,
      "step": 401800
    },
    {
      "epoch": 31.482061726460913,
      "grad_norm": 6.784540176391602,
      "learning_rate": 2.3764948561282572e-05,
      "loss": 1.6227,
      "step": 401900
    },
    {
      "epoch": 31.489895033683222,
      "grad_norm": 5.4979119300842285,
      "learning_rate": 2.3758420805263985e-05,
      "loss": 1.6766,
      "step": 402000
    },
    {
      "epoch": 31.49772834090553,
      "grad_norm": 7.03111457824707,
      "learning_rate": 2.375189304924539e-05,
      "loss": 1.7474,
      "step": 402100
    },
    {
      "epoch": 31.505561648127838,
      "grad_norm": 6.299478530883789,
      "learning_rate": 2.37453652932268e-05,
      "loss": 1.6971,
      "step": 402200
    },
    {
      "epoch": 31.513394955350147,
      "grad_norm": 6.85056734085083,
      "learning_rate": 2.3738837537208212e-05,
      "loss": 1.6394,
      "step": 402300
    },
    {
      "epoch": 31.521228262572457,
      "grad_norm": 6.013059139251709,
      "learning_rate": 2.3732309781189618e-05,
      "loss": 1.6993,
      "step": 402400
    },
    {
      "epoch": 31.529061569794766,
      "grad_norm": 7.499669075012207,
      "learning_rate": 2.3725782025171027e-05,
      "loss": 1.6376,
      "step": 402500
    },
    {
      "epoch": 31.536894877017076,
      "grad_norm": 6.751385688781738,
      "learning_rate": 2.371925426915244e-05,
      "loss": 1.6977,
      "step": 402600
    },
    {
      "epoch": 31.544728184239386,
      "grad_norm": 6.996962070465088,
      "learning_rate": 2.3712726513133846e-05,
      "loss": 1.6715,
      "step": 402700
    },
    {
      "epoch": 31.552561491461695,
      "grad_norm": 6.468231201171875,
      "learning_rate": 2.3706198757115255e-05,
      "loss": 1.7644,
      "step": 402800
    },
    {
      "epoch": 31.560394798684005,
      "grad_norm": 5.085803031921387,
      "learning_rate": 2.3699671001096664e-05,
      "loss": 1.6688,
      "step": 402900
    },
    {
      "epoch": 31.568228105906314,
      "grad_norm": 5.573971271514893,
      "learning_rate": 2.3693143245078073e-05,
      "loss": 1.6326,
      "step": 403000
    },
    {
      "epoch": 31.576061413128624,
      "grad_norm": 5.740731716156006,
      "learning_rate": 2.3686615489059482e-05,
      "loss": 1.7381,
      "step": 403100
    },
    {
      "epoch": 31.583894720350933,
      "grad_norm": 6.939858913421631,
      "learning_rate": 2.368008773304089e-05,
      "loss": 1.6922,
      "step": 403200
    },
    {
      "epoch": 31.591728027573243,
      "grad_norm": 5.547467231750488,
      "learning_rate": 2.36735599770223e-05,
      "loss": 1.7153,
      "step": 403300
    },
    {
      "epoch": 31.59956133479555,
      "grad_norm": 4.413971424102783,
      "learning_rate": 2.366703222100371e-05,
      "loss": 1.6804,
      "step": 403400
    },
    {
      "epoch": 31.60739464201786,
      "grad_norm": 7.0668134689331055,
      "learning_rate": 2.3660504464985115e-05,
      "loss": 1.6819,
      "step": 403500
    },
    {
      "epoch": 31.615227949240168,
      "grad_norm": 6.507326602935791,
      "learning_rate": 2.3653976708966528e-05,
      "loss": 1.7054,
      "step": 403600
    },
    {
      "epoch": 31.623061256462478,
      "grad_norm": 7.866150856018066,
      "learning_rate": 2.3647448952947937e-05,
      "loss": 1.6885,
      "step": 403700
    },
    {
      "epoch": 31.630894563684787,
      "grad_norm": 6.545636177062988,
      "learning_rate": 2.3640921196929343e-05,
      "loss": 1.6748,
      "step": 403800
    },
    {
      "epoch": 31.638727870907097,
      "grad_norm": 5.245673656463623,
      "learning_rate": 2.3634393440910755e-05,
      "loss": 1.7782,
      "step": 403900
    },
    {
      "epoch": 31.646561178129406,
      "grad_norm": 15.746825218200684,
      "learning_rate": 2.362786568489216e-05,
      "loss": 1.7333,
      "step": 404000
    },
    {
      "epoch": 31.654394485351716,
      "grad_norm": 5.201424598693848,
      "learning_rate": 2.362133792887357e-05,
      "loss": 1.6491,
      "step": 404100
    },
    {
      "epoch": 31.662227792574026,
      "grad_norm": 6.28651237487793,
      "learning_rate": 2.3614810172854983e-05,
      "loss": 1.6977,
      "step": 404200
    },
    {
      "epoch": 31.670061099796335,
      "grad_norm": 4.84366512298584,
      "learning_rate": 2.360828241683639e-05,
      "loss": 1.7609,
      "step": 404300
    },
    {
      "epoch": 31.677894407018645,
      "grad_norm": 6.25759744644165,
      "learning_rate": 2.3601754660817798e-05,
      "loss": 1.6578,
      "step": 404400
    },
    {
      "epoch": 31.685727714240954,
      "grad_norm": 7.2428154945373535,
      "learning_rate": 2.3595226904799207e-05,
      "loss": 1.6633,
      "step": 404500
    },
    {
      "epoch": 31.69356102146326,
      "grad_norm": 5.953053951263428,
      "learning_rate": 2.3588699148780616e-05,
      "loss": 1.6094,
      "step": 404600
    },
    {
      "epoch": 31.70139432868557,
      "grad_norm": 5.517004013061523,
      "learning_rate": 2.3582171392762025e-05,
      "loss": 1.6665,
      "step": 404700
    },
    {
      "epoch": 31.70922763590788,
      "grad_norm": 6.017984867095947,
      "learning_rate": 2.3575643636743434e-05,
      "loss": 1.6751,
      "step": 404800
    },
    {
      "epoch": 31.71706094313019,
      "grad_norm": 7.167672157287598,
      "learning_rate": 2.3569115880724844e-05,
      "loss": 1.7212,
      "step": 404900
    },
    {
      "epoch": 31.7248942503525,
      "grad_norm": 5.159060478210449,
      "learning_rate": 2.3562588124706253e-05,
      "loss": 1.6285,
      "step": 405000
    },
    {
      "epoch": 31.73272755757481,
      "grad_norm": 4.823263168334961,
      "learning_rate": 2.355606036868766e-05,
      "loss": 1.6099,
      "step": 405100
    },
    {
      "epoch": 31.740560864797118,
      "grad_norm": 6.041316509246826,
      "learning_rate": 2.354953261266907e-05,
      "loss": 1.6876,
      "step": 405200
    },
    {
      "epoch": 31.748394172019427,
      "grad_norm": 6.225603103637695,
      "learning_rate": 2.354300485665048e-05,
      "loss": 1.6903,
      "step": 405300
    },
    {
      "epoch": 31.756227479241737,
      "grad_norm": 6.829249858856201,
      "learning_rate": 2.3536477100631886e-05,
      "loss": 1.6097,
      "step": 405400
    },
    {
      "epoch": 31.764060786464047,
      "grad_norm": 6.682859897613525,
      "learning_rate": 2.35299493446133e-05,
      "loss": 1.6882,
      "step": 405500
    },
    {
      "epoch": 31.771894093686356,
      "grad_norm": 5.89401388168335,
      "learning_rate": 2.3523421588594704e-05,
      "loss": 1.7442,
      "step": 405600
    },
    {
      "epoch": 31.779727400908662,
      "grad_norm": 7.301473617553711,
      "learning_rate": 2.3516893832576113e-05,
      "loss": 1.6733,
      "step": 405700
    },
    {
      "epoch": 31.78756070813097,
      "grad_norm": 7.422860145568848,
      "learning_rate": 2.3510366076557526e-05,
      "loss": 1.6035,
      "step": 405800
    },
    {
      "epoch": 31.79539401535328,
      "grad_norm": 5.355954647064209,
      "learning_rate": 2.3503838320538932e-05,
      "loss": 1.6742,
      "step": 405900
    },
    {
      "epoch": 31.80322732257559,
      "grad_norm": 6.230803489685059,
      "learning_rate": 2.349731056452034e-05,
      "loss": 1.6664,
      "step": 406000
    },
    {
      "epoch": 31.8110606297979,
      "grad_norm": 5.4332098960876465,
      "learning_rate": 2.349078280850175e-05,
      "loss": 1.6903,
      "step": 406100
    },
    {
      "epoch": 31.81889393702021,
      "grad_norm": 5.928790092468262,
      "learning_rate": 2.348425505248316e-05,
      "loss": 1.7192,
      "step": 406200
    },
    {
      "epoch": 31.82672724424252,
      "grad_norm": 7.124424457550049,
      "learning_rate": 2.347772729646457e-05,
      "loss": 1.7865,
      "step": 406300
    },
    {
      "epoch": 31.83456055146483,
      "grad_norm": 5.814246654510498,
      "learning_rate": 2.3471199540445978e-05,
      "loss": 1.7083,
      "step": 406400
    },
    {
      "epoch": 31.84239385868714,
      "grad_norm": 5.750524520874023,
      "learning_rate": 2.3464671784427387e-05,
      "loss": 1.6681,
      "step": 406500
    },
    {
      "epoch": 31.85022716590945,
      "grad_norm": 6.2926025390625,
      "learning_rate": 2.3458144028408796e-05,
      "loss": 1.5927,
      "step": 406600
    },
    {
      "epoch": 31.858060473131758,
      "grad_norm": 6.7649760246276855,
      "learning_rate": 2.34516162723902e-05,
      "loss": 1.6761,
      "step": 406700
    },
    {
      "epoch": 31.865893780354064,
      "grad_norm": 5.689759731292725,
      "learning_rate": 2.3445088516371614e-05,
      "loss": 1.6555,
      "step": 406800
    },
    {
      "epoch": 31.873727087576373,
      "grad_norm": 8.1788969039917,
      "learning_rate": 2.3438560760353023e-05,
      "loss": 1.7247,
      "step": 406900
    },
    {
      "epoch": 31.881560394798683,
      "grad_norm": 5.630218982696533,
      "learning_rate": 2.343203300433443e-05,
      "loss": 1.6746,
      "step": 407000
    },
    {
      "epoch": 31.889393702020993,
      "grad_norm": 4.951847076416016,
      "learning_rate": 2.342550524831584e-05,
      "loss": 1.6757,
      "step": 407100
    },
    {
      "epoch": 31.897227009243302,
      "grad_norm": 9.121658325195312,
      "learning_rate": 2.3418977492297247e-05,
      "loss": 1.7216,
      "step": 407200
    },
    {
      "epoch": 31.90506031646561,
      "grad_norm": 5.6236467361450195,
      "learning_rate": 2.3412449736278657e-05,
      "loss": 1.7639,
      "step": 407300
    },
    {
      "epoch": 31.91289362368792,
      "grad_norm": 9.085261344909668,
      "learning_rate": 2.340592198026007e-05,
      "loss": 1.6815,
      "step": 407400
    },
    {
      "epoch": 31.92072693091023,
      "grad_norm": 7.310342311859131,
      "learning_rate": 2.3399394224241475e-05,
      "loss": 1.6689,
      "step": 407500
    },
    {
      "epoch": 31.92856023813254,
      "grad_norm": 5.520034313201904,
      "learning_rate": 2.3392866468222884e-05,
      "loss": 1.6879,
      "step": 407600
    },
    {
      "epoch": 31.93639354535485,
      "grad_norm": 5.923092365264893,
      "learning_rate": 2.3386338712204293e-05,
      "loss": 1.6994,
      "step": 407700
    },
    {
      "epoch": 31.94422685257716,
      "grad_norm": 8.392939567565918,
      "learning_rate": 2.3379810956185702e-05,
      "loss": 1.6426,
      "step": 407800
    },
    {
      "epoch": 31.952060159799466,
      "grad_norm": 7.819351673126221,
      "learning_rate": 2.337328320016711e-05,
      "loss": 1.7182,
      "step": 407900
    },
    {
      "epoch": 31.959893467021775,
      "grad_norm": 6.065601348876953,
      "learning_rate": 2.336675544414852e-05,
      "loss": 1.6925,
      "step": 408000
    },
    {
      "epoch": 31.967726774244085,
      "grad_norm": 6.681980133056641,
      "learning_rate": 2.336022768812993e-05,
      "loss": 1.6746,
      "step": 408100
    },
    {
      "epoch": 31.975560081466394,
      "grad_norm": 9.065221786499023,
      "learning_rate": 2.335369993211134e-05,
      "loss": 1.6689,
      "step": 408200
    },
    {
      "epoch": 31.983393388688704,
      "grad_norm": 10.807093620300293,
      "learning_rate": 2.3347172176092745e-05,
      "loss": 1.72,
      "step": 408300
    },
    {
      "epoch": 31.991226695911013,
      "grad_norm": 6.307589054107666,
      "learning_rate": 2.3340644420074157e-05,
      "loss": 1.6925,
      "step": 408400
    },
    {
      "epoch": 31.999060003133323,
      "grad_norm": 6.452294826507568,
      "learning_rate": 2.3334116664055566e-05,
      "loss": 1.6255,
      "step": 408500
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.7906261682510376,
      "eval_runtime": 1.5335,
      "eval_samples_per_second": 438.201,
      "eval_steps_per_second": 438.201,
      "step": 408512
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.440178632736206,
      "eval_runtime": 29.1548,
      "eval_samples_per_second": 437.869,
      "eval_steps_per_second": 437.869,
      "step": 408512
    },
    {
      "epoch": 32.00689331035563,
      "grad_norm": 8.471186637878418,
      "learning_rate": 2.3327588908036972e-05,
      "loss": 1.6495,
      "step": 408600
    },
    {
      "epoch": 32.01472661757794,
      "grad_norm": 7.814523696899414,
      "learning_rate": 2.3321061152018385e-05,
      "loss": 1.716,
      "step": 408700
    },
    {
      "epoch": 32.02255992480025,
      "grad_norm": 4.965019702911377,
      "learning_rate": 2.3314533395999794e-05,
      "loss": 1.6971,
      "step": 408800
    },
    {
      "epoch": 32.03039323202256,
      "grad_norm": 8.032613754272461,
      "learning_rate": 2.33080056399812e-05,
      "loss": 1.6396,
      "step": 408900
    },
    {
      "epoch": 32.03822653924487,
      "grad_norm": 8.330352783203125,
      "learning_rate": 2.3301477883962612e-05,
      "loss": 1.6896,
      "step": 409000
    },
    {
      "epoch": 32.04605984646718,
      "grad_norm": 4.839177131652832,
      "learning_rate": 2.3294950127944018e-05,
      "loss": 1.6871,
      "step": 409100
    },
    {
      "epoch": 32.053893153689486,
      "grad_norm": 6.582057476043701,
      "learning_rate": 2.3288422371925427e-05,
      "loss": 1.674,
      "step": 409200
    },
    {
      "epoch": 32.0617264609118,
      "grad_norm": 6.939159870147705,
      "learning_rate": 2.3281894615906836e-05,
      "loss": 1.6749,
      "step": 409300
    },
    {
      "epoch": 32.069559768134106,
      "grad_norm": 5.754915714263916,
      "learning_rate": 2.3275366859888245e-05,
      "loss": 1.6463,
      "step": 409400
    },
    {
      "epoch": 32.07739307535642,
      "grad_norm": 6.3304829597473145,
      "learning_rate": 2.3268839103869655e-05,
      "loss": 1.665,
      "step": 409500
    },
    {
      "epoch": 32.085226382578725,
      "grad_norm": 4.554378032684326,
      "learning_rate": 2.3262311347851064e-05,
      "loss": 1.5962,
      "step": 409600
    },
    {
      "epoch": 32.09305968980103,
      "grad_norm": 6.646822929382324,
      "learning_rate": 2.3255783591832473e-05,
      "loss": 1.7204,
      "step": 409700
    },
    {
      "epoch": 32.100892997023344,
      "grad_norm": 7.511344909667969,
      "learning_rate": 2.3249255835813882e-05,
      "loss": 1.7472,
      "step": 409800
    },
    {
      "epoch": 32.10872630424565,
      "grad_norm": 5.681105136871338,
      "learning_rate": 2.3242728079795288e-05,
      "loss": 1.6688,
      "step": 409900
    },
    {
      "epoch": 32.11655961146796,
      "grad_norm": 9.02283763885498,
      "learning_rate": 2.32362003237767e-05,
      "loss": 1.6513,
      "step": 410000
    },
    {
      "epoch": 32.12439291869027,
      "grad_norm": 6.5051703453063965,
      "learning_rate": 2.322967256775811e-05,
      "loss": 1.6569,
      "step": 410100
    },
    {
      "epoch": 32.13222622591258,
      "grad_norm": 8.202857971191406,
      "learning_rate": 2.3223144811739515e-05,
      "loss": 1.6527,
      "step": 410200
    },
    {
      "epoch": 32.14005953313489,
      "grad_norm": 6.4792866706848145,
      "learning_rate": 2.3216617055720928e-05,
      "loss": 1.6297,
      "step": 410300
    },
    {
      "epoch": 32.1478928403572,
      "grad_norm": 5.27342414855957,
      "learning_rate": 2.3210089299702337e-05,
      "loss": 1.7325,
      "step": 410400
    },
    {
      "epoch": 32.15572614757951,
      "grad_norm": 6.053101539611816,
      "learning_rate": 2.3203561543683743e-05,
      "loss": 1.6023,
      "step": 410500
    },
    {
      "epoch": 32.16355945480182,
      "grad_norm": 6.467463493347168,
      "learning_rate": 2.3197033787665155e-05,
      "loss": 1.7324,
      "step": 410600
    },
    {
      "epoch": 32.17139276202413,
      "grad_norm": 6.789968013763428,
      "learning_rate": 2.319050603164656e-05,
      "loss": 1.5686,
      "step": 410700
    },
    {
      "epoch": 32.17922606924643,
      "grad_norm": 6.6221208572387695,
      "learning_rate": 2.318397827562797e-05,
      "loss": 1.6507,
      "step": 410800
    },
    {
      "epoch": 32.187059376468746,
      "grad_norm": 4.595924377441406,
      "learning_rate": 2.317745051960938e-05,
      "loss": 1.6676,
      "step": 410900
    },
    {
      "epoch": 32.19489268369105,
      "grad_norm": 6.100231647491455,
      "learning_rate": 2.317092276359079e-05,
      "loss": 1.6938,
      "step": 411000
    },
    {
      "epoch": 32.202725990913365,
      "grad_norm": 6.0133376121521,
      "learning_rate": 2.3164395007572198e-05,
      "loss": 1.7022,
      "step": 411100
    },
    {
      "epoch": 32.21055929813567,
      "grad_norm": 5.299611568450928,
      "learning_rate": 2.3157867251553607e-05,
      "loss": 1.5911,
      "step": 411200
    },
    {
      "epoch": 32.218392605357984,
      "grad_norm": 6.670680522918701,
      "learning_rate": 2.3151339495535016e-05,
      "loss": 1.6195,
      "step": 411300
    },
    {
      "epoch": 32.22622591258029,
      "grad_norm": 5.434689521789551,
      "learning_rate": 2.3144811739516425e-05,
      "loss": 1.7318,
      "step": 411400
    },
    {
      "epoch": 32.2340592198026,
      "grad_norm": 6.130394458770752,
      "learning_rate": 2.3138283983497834e-05,
      "loss": 1.6442,
      "step": 411500
    },
    {
      "epoch": 32.24189252702491,
      "grad_norm": 7.647844314575195,
      "learning_rate": 2.3131756227479244e-05,
      "loss": 1.6251,
      "step": 411600
    },
    {
      "epoch": 32.24972583424722,
      "grad_norm": 6.719253063201904,
      "learning_rate": 2.3125228471460653e-05,
      "loss": 1.5925,
      "step": 411700
    },
    {
      "epoch": 32.25755914146953,
      "grad_norm": 5.4402289390563965,
      "learning_rate": 2.311870071544206e-05,
      "loss": 1.7643,
      "step": 411800
    },
    {
      "epoch": 32.265392448691834,
      "grad_norm": 6.377037048339844,
      "learning_rate": 2.311217295942347e-05,
      "loss": 1.6127,
      "step": 411900
    },
    {
      "epoch": 32.27322575591415,
      "grad_norm": 5.648370742797852,
      "learning_rate": 2.310564520340488e-05,
      "loss": 1.6431,
      "step": 412000
    },
    {
      "epoch": 32.28105906313645,
      "grad_norm": 5.168208122253418,
      "learning_rate": 2.3099117447386286e-05,
      "loss": 1.7756,
      "step": 412100
    },
    {
      "epoch": 32.28889237035877,
      "grad_norm": 5.7372236251831055,
      "learning_rate": 2.30925896913677e-05,
      "loss": 1.7232,
      "step": 412200
    },
    {
      "epoch": 32.29672567758107,
      "grad_norm": 6.690184593200684,
      "learning_rate": 2.3086061935349104e-05,
      "loss": 1.7262,
      "step": 412300
    },
    {
      "epoch": 32.304558984803386,
      "grad_norm": 4.6712260246276855,
      "learning_rate": 2.3079534179330513e-05,
      "loss": 1.6482,
      "step": 412400
    },
    {
      "epoch": 32.31239229202569,
      "grad_norm": 4.766361236572266,
      "learning_rate": 2.3073006423311923e-05,
      "loss": 1.6642,
      "step": 412500
    },
    {
      "epoch": 32.320225599248005,
      "grad_norm": 4.901473045349121,
      "learning_rate": 2.306647866729333e-05,
      "loss": 1.7401,
      "step": 412600
    },
    {
      "epoch": 32.32805890647031,
      "grad_norm": 5.247072219848633,
      "learning_rate": 2.305995091127474e-05,
      "loss": 1.5838,
      "step": 412700
    },
    {
      "epoch": 32.335892213692624,
      "grad_norm": 4.912524700164795,
      "learning_rate": 2.305342315525615e-05,
      "loss": 1.6623,
      "step": 412800
    },
    {
      "epoch": 32.34372552091493,
      "grad_norm": 7.954224586486816,
      "learning_rate": 2.304689539923756e-05,
      "loss": 1.6634,
      "step": 412900
    },
    {
      "epoch": 32.35155882813724,
      "grad_norm": 6.31932258605957,
      "learning_rate": 2.304036764321897e-05,
      "loss": 1.6594,
      "step": 413000
    },
    {
      "epoch": 32.35939213535955,
      "grad_norm": 5.674829483032227,
      "learning_rate": 2.3033839887200377e-05,
      "loss": 1.75,
      "step": 413100
    },
    {
      "epoch": 32.367225442581855,
      "grad_norm": 6.904797077178955,
      "learning_rate": 2.3027312131181787e-05,
      "loss": 1.6639,
      "step": 413200
    },
    {
      "epoch": 32.37505874980417,
      "grad_norm": 6.034582138061523,
      "learning_rate": 2.3020784375163196e-05,
      "loss": 1.6958,
      "step": 413300
    },
    {
      "epoch": 32.382892057026474,
      "grad_norm": 6.014035224914551,
      "learning_rate": 2.30142566191446e-05,
      "loss": 1.5722,
      "step": 413400
    },
    {
      "epoch": 32.39072536424879,
      "grad_norm": 5.171239852905273,
      "learning_rate": 2.3007728863126014e-05,
      "loss": 1.7083,
      "step": 413500
    },
    {
      "epoch": 32.39855867147109,
      "grad_norm": 5.874298572540283,
      "learning_rate": 2.3001201107107423e-05,
      "loss": 1.6152,
      "step": 413600
    },
    {
      "epoch": 32.40639197869341,
      "grad_norm": 4.94738245010376,
      "learning_rate": 2.299467335108883e-05,
      "loss": 1.7245,
      "step": 413700
    },
    {
      "epoch": 32.41422528591571,
      "grad_norm": 5.510321140289307,
      "learning_rate": 2.298814559507024e-05,
      "loss": 1.6301,
      "step": 413800
    },
    {
      "epoch": 32.422058593138026,
      "grad_norm": 7.444720268249512,
      "learning_rate": 2.2981617839051647e-05,
      "loss": 1.7444,
      "step": 413900
    },
    {
      "epoch": 32.42989190036033,
      "grad_norm": 8.140161514282227,
      "learning_rate": 2.2975090083033056e-05,
      "loss": 1.6392,
      "step": 414000
    },
    {
      "epoch": 32.437725207582645,
      "grad_norm": 7.474374771118164,
      "learning_rate": 2.2968562327014466e-05,
      "loss": 1.6735,
      "step": 414100
    },
    {
      "epoch": 32.44555851480495,
      "grad_norm": 9.496650695800781,
      "learning_rate": 2.2962034570995875e-05,
      "loss": 1.7895,
      "step": 414200
    },
    {
      "epoch": 32.45339182202726,
      "grad_norm": 4.431787490844727,
      "learning_rate": 2.2955506814977284e-05,
      "loss": 1.7794,
      "step": 414300
    },
    {
      "epoch": 32.46122512924957,
      "grad_norm": 5.067509174346924,
      "learning_rate": 2.2948979058958693e-05,
      "loss": 1.7564,
      "step": 414400
    },
    {
      "epoch": 32.469058436471876,
      "grad_norm": 5.519662857055664,
      "learning_rate": 2.2942451302940102e-05,
      "loss": 1.6846,
      "step": 414500
    },
    {
      "epoch": 32.47689174369419,
      "grad_norm": 6.038987159729004,
      "learning_rate": 2.293592354692151e-05,
      "loss": 1.7401,
      "step": 414600
    },
    {
      "epoch": 32.484725050916495,
      "grad_norm": 5.667124271392822,
      "learning_rate": 2.292939579090292e-05,
      "loss": 1.7094,
      "step": 414700
    },
    {
      "epoch": 32.49255835813881,
      "grad_norm": 5.981652736663818,
      "learning_rate": 2.292286803488433e-05,
      "loss": 1.735,
      "step": 414800
    },
    {
      "epoch": 32.500391665361114,
      "grad_norm": 5.717319011688232,
      "learning_rate": 2.291634027886574e-05,
      "loss": 1.7308,
      "step": 414900
    },
    {
      "epoch": 32.50822497258343,
      "grad_norm": 5.734714508056641,
      "learning_rate": 2.2909812522847145e-05,
      "loss": 1.6409,
      "step": 415000
    },
    {
      "epoch": 32.51605827980573,
      "grad_norm": 9.564680099487305,
      "learning_rate": 2.2903284766828557e-05,
      "loss": 1.6388,
      "step": 415100
    },
    {
      "epoch": 32.52389158702805,
      "grad_norm": 7.277428150177002,
      "learning_rate": 2.2896757010809966e-05,
      "loss": 1.6557,
      "step": 415200
    },
    {
      "epoch": 32.53172489425035,
      "grad_norm": 3.8477025032043457,
      "learning_rate": 2.2890229254791372e-05,
      "loss": 1.6693,
      "step": 415300
    },
    {
      "epoch": 32.53955820147266,
      "grad_norm": 6.021574020385742,
      "learning_rate": 2.2883701498772785e-05,
      "loss": 1.6988,
      "step": 415400
    },
    {
      "epoch": 32.54739150869497,
      "grad_norm": 5.50032901763916,
      "learning_rate": 2.2877173742754194e-05,
      "loss": 1.6648,
      "step": 415500
    },
    {
      "epoch": 32.55522481591728,
      "grad_norm": 6.92563533782959,
      "learning_rate": 2.28706459867356e-05,
      "loss": 1.7427,
      "step": 415600
    },
    {
      "epoch": 32.56305812313959,
      "grad_norm": 5.990606784820557,
      "learning_rate": 2.286411823071701e-05,
      "loss": 1.7866,
      "step": 415700
    },
    {
      "epoch": 32.5708914303619,
      "grad_norm": 6.011637210845947,
      "learning_rate": 2.2857590474698418e-05,
      "loss": 1.7048,
      "step": 415800
    },
    {
      "epoch": 32.57872473758421,
      "grad_norm": 6.129598140716553,
      "learning_rate": 2.2851062718679827e-05,
      "loss": 1.669,
      "step": 415900
    },
    {
      "epoch": 32.586558044806516,
      "grad_norm": 5.807168483734131,
      "learning_rate": 2.2844534962661236e-05,
      "loss": 1.6982,
      "step": 416000
    },
    {
      "epoch": 32.59439135202883,
      "grad_norm": 6.369377136230469,
      "learning_rate": 2.2838007206642645e-05,
      "loss": 1.7462,
      "step": 416100
    },
    {
      "epoch": 32.602224659251135,
      "grad_norm": 5.194389820098877,
      "learning_rate": 2.2831479450624055e-05,
      "loss": 1.6887,
      "step": 416200
    },
    {
      "epoch": 32.61005796647345,
      "grad_norm": 6.105053424835205,
      "learning_rate": 2.2824951694605464e-05,
      "loss": 1.6794,
      "step": 416300
    },
    {
      "epoch": 32.617891273695754,
      "grad_norm": 7.827905654907227,
      "learning_rate": 2.2818423938586873e-05,
      "loss": 1.6634,
      "step": 416400
    },
    {
      "epoch": 32.62572458091806,
      "grad_norm": 6.369583606719971,
      "learning_rate": 2.2811896182568282e-05,
      "loss": 1.6882,
      "step": 416500
    },
    {
      "epoch": 32.63355788814037,
      "grad_norm": 7.208706855773926,
      "learning_rate": 2.2805368426549688e-05,
      "loss": 1.6701,
      "step": 416600
    },
    {
      "epoch": 32.64139119536268,
      "grad_norm": 7.009931564331055,
      "learning_rate": 2.27988406705311e-05,
      "loss": 1.7003,
      "step": 416700
    },
    {
      "epoch": 32.64922450258499,
      "grad_norm": 6.491602420806885,
      "learning_rate": 2.279231291451251e-05,
      "loss": 1.6411,
      "step": 416800
    },
    {
      "epoch": 32.6570578098073,
      "grad_norm": 6.456654071807861,
      "learning_rate": 2.2785785158493915e-05,
      "loss": 1.6633,
      "step": 416900
    },
    {
      "epoch": 32.66489111702961,
      "grad_norm": 7.441940784454346,
      "learning_rate": 2.2779257402475328e-05,
      "loss": 1.7073,
      "step": 417000
    },
    {
      "epoch": 32.67272442425192,
      "grad_norm": 7.465702533721924,
      "learning_rate": 2.2772729646456737e-05,
      "loss": 1.6646,
      "step": 417100
    },
    {
      "epoch": 32.68055773147423,
      "grad_norm": 7.2095232009887695,
      "learning_rate": 2.2766201890438143e-05,
      "loss": 1.6847,
      "step": 417200
    },
    {
      "epoch": 32.68839103869654,
      "grad_norm": 4.934532642364502,
      "learning_rate": 2.2759674134419552e-05,
      "loss": 1.7492,
      "step": 417300
    },
    {
      "epoch": 32.69622434591885,
      "grad_norm": 7.370438098907471,
      "learning_rate": 2.275314637840096e-05,
      "loss": 1.7047,
      "step": 417400
    },
    {
      "epoch": 32.704057653141156,
      "grad_norm": 5.481296539306641,
      "learning_rate": 2.274661862238237e-05,
      "loss": 1.6677,
      "step": 417500
    },
    {
      "epoch": 32.71189096036346,
      "grad_norm": 6.443589210510254,
      "learning_rate": 2.274009086636378e-05,
      "loss": 1.6993,
      "step": 417600
    },
    {
      "epoch": 32.719724267585775,
      "grad_norm": 7.213998794555664,
      "learning_rate": 2.273356311034519e-05,
      "loss": 1.6468,
      "step": 417700
    },
    {
      "epoch": 32.72755757480808,
      "grad_norm": 5.716279983520508,
      "learning_rate": 2.2727035354326598e-05,
      "loss": 1.6565,
      "step": 417800
    },
    {
      "epoch": 32.735390882030394,
      "grad_norm": 2.4587552547454834,
      "learning_rate": 2.2720507598308007e-05,
      "loss": 1.6874,
      "step": 417900
    },
    {
      "epoch": 32.7432241892527,
      "grad_norm": 7.31844425201416,
      "learning_rate": 2.2713979842289416e-05,
      "loss": 1.6633,
      "step": 418000
    },
    {
      "epoch": 32.75105749647501,
      "grad_norm": 6.785045623779297,
      "learning_rate": 2.2707452086270825e-05,
      "loss": 1.7222,
      "step": 418100
    },
    {
      "epoch": 32.75889080369732,
      "grad_norm": 8.970070838928223,
      "learning_rate": 2.270092433025223e-05,
      "loss": 1.6312,
      "step": 418200
    },
    {
      "epoch": 32.76672411091963,
      "grad_norm": 7.186064720153809,
      "learning_rate": 2.2694396574233643e-05,
      "loss": 1.646,
      "step": 418300
    },
    {
      "epoch": 32.77455741814194,
      "grad_norm": 6.916512966156006,
      "learning_rate": 2.2687868818215053e-05,
      "loss": 1.6962,
      "step": 418400
    },
    {
      "epoch": 32.78239072536425,
      "grad_norm": 3.7207000255584717,
      "learning_rate": 2.268134106219646e-05,
      "loss": 1.5611,
      "step": 418500
    },
    {
      "epoch": 32.79022403258656,
      "grad_norm": 5.822124004364014,
      "learning_rate": 2.267481330617787e-05,
      "loss": 1.7089,
      "step": 418600
    },
    {
      "epoch": 32.798057339808864,
      "grad_norm": 7.404306888580322,
      "learning_rate": 2.266828555015928e-05,
      "loss": 1.7817,
      "step": 418700
    },
    {
      "epoch": 32.80589064703118,
      "grad_norm": 7.021477222442627,
      "learning_rate": 2.2661757794140686e-05,
      "loss": 1.6131,
      "step": 418800
    },
    {
      "epoch": 32.81372395425348,
      "grad_norm": 6.1797871589660645,
      "learning_rate": 2.2655230038122095e-05,
      "loss": 1.7056,
      "step": 418900
    },
    {
      "epoch": 32.821557261475796,
      "grad_norm": 5.993925094604492,
      "learning_rate": 2.2648702282103504e-05,
      "loss": 1.615,
      "step": 419000
    },
    {
      "epoch": 32.8293905686981,
      "grad_norm": 6.161030292510986,
      "learning_rate": 2.2642174526084913e-05,
      "loss": 1.6849,
      "step": 419100
    },
    {
      "epoch": 32.837223875920415,
      "grad_norm": 5.297654628753662,
      "learning_rate": 2.2635646770066322e-05,
      "loss": 1.6416,
      "step": 419200
    },
    {
      "epoch": 32.84505718314272,
      "grad_norm": 5.377970218658447,
      "learning_rate": 2.262911901404773e-05,
      "loss": 1.5607,
      "step": 419300
    },
    {
      "epoch": 32.852890490365034,
      "grad_norm": 6.34100866317749,
      "learning_rate": 2.262259125802914e-05,
      "loss": 1.7318,
      "step": 419400
    },
    {
      "epoch": 32.86072379758734,
      "grad_norm": 5.468799591064453,
      "learning_rate": 2.261606350201055e-05,
      "loss": 1.638,
      "step": 419500
    },
    {
      "epoch": 32.86855710480965,
      "grad_norm": 6.831272602081299,
      "learning_rate": 2.260953574599196e-05,
      "loss": 1.6763,
      "step": 419600
    },
    {
      "epoch": 32.87639041203196,
      "grad_norm": 6.5160956382751465,
      "learning_rate": 2.2603007989973368e-05,
      "loss": 1.6512,
      "step": 419700
    },
    {
      "epoch": 32.88422371925427,
      "grad_norm": 6.964200496673584,
      "learning_rate": 2.2596480233954774e-05,
      "loss": 1.6336,
      "step": 419800
    },
    {
      "epoch": 32.89205702647658,
      "grad_norm": 6.807992458343506,
      "learning_rate": 2.2589952477936187e-05,
      "loss": 1.712,
      "step": 419900
    },
    {
      "epoch": 32.899890333698885,
      "grad_norm": 7.287318706512451,
      "learning_rate": 2.2583424721917596e-05,
      "loss": 1.6197,
      "step": 420000
    },
    {
      "epoch": 32.9077236409212,
      "grad_norm": 9.48681354522705,
      "learning_rate": 2.2576896965899e-05,
      "loss": 1.6547,
      "step": 420100
    },
    {
      "epoch": 32.915556948143504,
      "grad_norm": 6.54929256439209,
      "learning_rate": 2.2570369209880414e-05,
      "loss": 1.6247,
      "step": 420200
    },
    {
      "epoch": 32.92339025536582,
      "grad_norm": 6.886945724487305,
      "learning_rate": 2.2563841453861823e-05,
      "loss": 1.6622,
      "step": 420300
    },
    {
      "epoch": 32.93122356258812,
      "grad_norm": 7.465999603271484,
      "learning_rate": 2.255731369784323e-05,
      "loss": 1.6695,
      "step": 420400
    },
    {
      "epoch": 32.939056869810436,
      "grad_norm": 5.217909336090088,
      "learning_rate": 2.2550785941824638e-05,
      "loss": 1.7348,
      "step": 420500
    },
    {
      "epoch": 32.94689017703274,
      "grad_norm": 7.5548529624938965,
      "learning_rate": 2.254425818580605e-05,
      "loss": 1.6646,
      "step": 420600
    },
    {
      "epoch": 32.954723484255055,
      "grad_norm": 6.8963823318481445,
      "learning_rate": 2.2537730429787456e-05,
      "loss": 1.6898,
      "step": 420700
    },
    {
      "epoch": 32.96255679147736,
      "grad_norm": 5.923842430114746,
      "learning_rate": 2.2531202673768866e-05,
      "loss": 1.6947,
      "step": 420800
    },
    {
      "epoch": 32.970390098699674,
      "grad_norm": 5.350850582122803,
      "learning_rate": 2.2524674917750275e-05,
      "loss": 1.7048,
      "step": 420900
    },
    {
      "epoch": 32.97822340592198,
      "grad_norm": 5.82844877243042,
      "learning_rate": 2.2518147161731684e-05,
      "loss": 1.7001,
      "step": 421000
    },
    {
      "epoch": 32.986056713144286,
      "grad_norm": 5.93824577331543,
      "learning_rate": 2.2511619405713093e-05,
      "loss": 1.695,
      "step": 421100
    },
    {
      "epoch": 32.9938900203666,
      "grad_norm": 5.790129661560059,
      "learning_rate": 2.2505091649694502e-05,
      "loss": 1.6629,
      "step": 421200
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.7833547592163086,
      "eval_runtime": 1.5429,
      "eval_samples_per_second": 435.544,
      "eval_steps_per_second": 435.544,
      "step": 421278
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.4315407276153564,
      "eval_runtime": 29.1412,
      "eval_samples_per_second": 438.073,
      "eval_steps_per_second": 438.073,
      "step": 421278
    },
    {
      "epoch": 33.001723327588905,
      "grad_norm": 7.529293537139893,
      "learning_rate": 2.249856389367591e-05,
      "loss": 1.6466,
      "step": 421300
    },
    {
      "epoch": 33.00955663481122,
      "grad_norm": 4.270078182220459,
      "learning_rate": 2.249203613765732e-05,
      "loss": 1.5129,
      "step": 421400
    },
    {
      "epoch": 33.017389942033525,
      "grad_norm": 3.5561184883117676,
      "learning_rate": 2.248550838163873e-05,
      "loss": 1.5899,
      "step": 421500
    },
    {
      "epoch": 33.02522324925584,
      "grad_norm": 5.8455963134765625,
      "learning_rate": 2.247898062562014e-05,
      "loss": 1.6419,
      "step": 421600
    },
    {
      "epoch": 33.033056556478144,
      "grad_norm": 6.79557466506958,
      "learning_rate": 2.2472452869601545e-05,
      "loss": 1.5845,
      "step": 421700
    },
    {
      "epoch": 33.04088986370046,
      "grad_norm": 5.728506565093994,
      "learning_rate": 2.2465925113582957e-05,
      "loss": 1.6543,
      "step": 421800
    },
    {
      "epoch": 33.04872317092276,
      "grad_norm": 7.330992221832275,
      "learning_rate": 2.2459397357564366e-05,
      "loss": 1.5919,
      "step": 421900
    },
    {
      "epoch": 33.056556478145076,
      "grad_norm": 7.457455158233643,
      "learning_rate": 2.2452869601545772e-05,
      "loss": 1.6695,
      "step": 422000
    },
    {
      "epoch": 33.06438978536738,
      "grad_norm": 8.62674331665039,
      "learning_rate": 2.244634184552718e-05,
      "loss": 1.7171,
      "step": 422100
    },
    {
      "epoch": 33.07222309258969,
      "grad_norm": 8.711686134338379,
      "learning_rate": 2.2439814089508594e-05,
      "loss": 1.7365,
      "step": 422200
    },
    {
      "epoch": 33.080056399812,
      "grad_norm": 5.39000129699707,
      "learning_rate": 2.243328633349e-05,
      "loss": 1.7713,
      "step": 422300
    },
    {
      "epoch": 33.08788970703431,
      "grad_norm": 6.434212684631348,
      "learning_rate": 2.242675857747141e-05,
      "loss": 1.661,
      "step": 422400
    },
    {
      "epoch": 33.09572301425662,
      "grad_norm": 4.39650821685791,
      "learning_rate": 2.2420230821452818e-05,
      "loss": 1.6788,
      "step": 422500
    },
    {
      "epoch": 33.103556321478926,
      "grad_norm": 7.1925811767578125,
      "learning_rate": 2.2413703065434227e-05,
      "loss": 1.7881,
      "step": 422600
    },
    {
      "epoch": 33.11138962870124,
      "grad_norm": 7.601382255554199,
      "learning_rate": 2.2407175309415636e-05,
      "loss": 1.6585,
      "step": 422700
    },
    {
      "epoch": 33.119222935923545,
      "grad_norm": 5.754086017608643,
      "learning_rate": 2.2400647553397045e-05,
      "loss": 1.6287,
      "step": 422800
    },
    {
      "epoch": 33.12705624314586,
      "grad_norm": 7.393820285797119,
      "learning_rate": 2.2394119797378454e-05,
      "loss": 1.6264,
      "step": 422900
    },
    {
      "epoch": 33.134889550368165,
      "grad_norm": 8.375444412231445,
      "learning_rate": 2.2387592041359864e-05,
      "loss": 1.6051,
      "step": 423000
    },
    {
      "epoch": 33.14272285759048,
      "grad_norm": 7.085587501525879,
      "learning_rate": 2.2381064285341273e-05,
      "loss": 1.677,
      "step": 423100
    },
    {
      "epoch": 33.150556164812784,
      "grad_norm": 5.922543048858643,
      "learning_rate": 2.2374536529322682e-05,
      "loss": 1.7044,
      "step": 423200
    },
    {
      "epoch": 33.15838947203509,
      "grad_norm": 7.153533935546875,
      "learning_rate": 2.2368008773304088e-05,
      "loss": 1.7051,
      "step": 423300
    },
    {
      "epoch": 33.1662227792574,
      "grad_norm": 6.924927711486816,
      "learning_rate": 2.23614810172855e-05,
      "loss": 1.6568,
      "step": 423400
    },
    {
      "epoch": 33.17405608647971,
      "grad_norm": 6.438848495483398,
      "learning_rate": 2.235495326126691e-05,
      "loss": 1.6993,
      "step": 423500
    },
    {
      "epoch": 33.18188939370202,
      "grad_norm": 5.999514102935791,
      "learning_rate": 2.2348425505248315e-05,
      "loss": 1.6915,
      "step": 423600
    },
    {
      "epoch": 33.18972270092433,
      "grad_norm": 4.814288139343262,
      "learning_rate": 2.2341897749229724e-05,
      "loss": 1.6522,
      "step": 423700
    },
    {
      "epoch": 33.19755600814664,
      "grad_norm": 7.642020225524902,
      "learning_rate": 2.2335369993211137e-05,
      "loss": 1.7452,
      "step": 423800
    },
    {
      "epoch": 33.20538931536895,
      "grad_norm": 6.571920394897461,
      "learning_rate": 2.2328842237192543e-05,
      "loss": 1.6449,
      "step": 423900
    },
    {
      "epoch": 33.21322262259126,
      "grad_norm": 7.066227436065674,
      "learning_rate": 2.2322314481173952e-05,
      "loss": 1.582,
      "step": 424000
    },
    {
      "epoch": 33.221055929813566,
      "grad_norm": 5.760585784912109,
      "learning_rate": 2.231578672515536e-05,
      "loss": 1.7068,
      "step": 424100
    },
    {
      "epoch": 33.22888923703588,
      "grad_norm": 5.733551502227783,
      "learning_rate": 2.230925896913677e-05,
      "loss": 1.6514,
      "step": 424200
    },
    {
      "epoch": 33.236722544258186,
      "grad_norm": 5.707858562469482,
      "learning_rate": 2.230273121311818e-05,
      "loss": 1.6661,
      "step": 424300
    },
    {
      "epoch": 33.24455585148049,
      "grad_norm": 6.22572660446167,
      "learning_rate": 2.229620345709959e-05,
      "loss": 1.772,
      "step": 424400
    },
    {
      "epoch": 33.252389158702805,
      "grad_norm": 9.378189086914062,
      "learning_rate": 2.2289675701080998e-05,
      "loss": 1.7027,
      "step": 424500
    },
    {
      "epoch": 33.26022246592511,
      "grad_norm": 5.62184476852417,
      "learning_rate": 2.2283147945062407e-05,
      "loss": 1.6069,
      "step": 424600
    },
    {
      "epoch": 33.268055773147424,
      "grad_norm": 5.9488725662231445,
      "learning_rate": 2.2276620189043816e-05,
      "loss": 1.6812,
      "step": 424700
    },
    {
      "epoch": 33.27588908036973,
      "grad_norm": 8.451469421386719,
      "learning_rate": 2.2270092433025225e-05,
      "loss": 1.7245,
      "step": 424800
    },
    {
      "epoch": 33.28372238759204,
      "grad_norm": 5.912177562713623,
      "learning_rate": 2.226356467700663e-05,
      "loss": 1.637,
      "step": 424900
    },
    {
      "epoch": 33.29155569481435,
      "grad_norm": 9.313135147094727,
      "learning_rate": 2.2257036920988043e-05,
      "loss": 1.6551,
      "step": 425000
    },
    {
      "epoch": 33.29938900203666,
      "grad_norm": 7.248538494110107,
      "learning_rate": 2.2250509164969453e-05,
      "loss": 1.7457,
      "step": 425100
    },
    {
      "epoch": 33.30722230925897,
      "grad_norm": 6.526729583740234,
      "learning_rate": 2.2243981408950858e-05,
      "loss": 1.739,
      "step": 425200
    },
    {
      "epoch": 33.31505561648128,
      "grad_norm": 5.477330684661865,
      "learning_rate": 2.2237453652932267e-05,
      "loss": 1.7281,
      "step": 425300
    },
    {
      "epoch": 33.32288892370359,
      "grad_norm": 5.647579669952393,
      "learning_rate": 2.223092589691368e-05,
      "loss": 1.7135,
      "step": 425400
    },
    {
      "epoch": 33.3307222309259,
      "grad_norm": 5.177770614624023,
      "learning_rate": 2.2224398140895086e-05,
      "loss": 1.7402,
      "step": 425500
    },
    {
      "epoch": 33.338555538148206,
      "grad_norm": 6.28756856918335,
      "learning_rate": 2.2217870384876495e-05,
      "loss": 1.7176,
      "step": 425600
    },
    {
      "epoch": 33.34638884537051,
      "grad_norm": 6.565103530883789,
      "learning_rate": 2.2211342628857904e-05,
      "loss": 1.61,
      "step": 425700
    },
    {
      "epoch": 33.354222152592826,
      "grad_norm": 7.649090766906738,
      "learning_rate": 2.2204814872839313e-05,
      "loss": 1.6996,
      "step": 425800
    },
    {
      "epoch": 33.36205545981513,
      "grad_norm": 5.0597357749938965,
      "learning_rate": 2.2198287116820722e-05,
      "loss": 1.6801,
      "step": 425900
    },
    {
      "epoch": 33.369888767037445,
      "grad_norm": 4.737555503845215,
      "learning_rate": 2.219175936080213e-05,
      "loss": 1.5742,
      "step": 426000
    },
    {
      "epoch": 33.37772207425975,
      "grad_norm": 3.656864881515503,
      "learning_rate": 2.218523160478354e-05,
      "loss": 1.6111,
      "step": 426100
    },
    {
      "epoch": 33.385555381482064,
      "grad_norm": 6.195775032043457,
      "learning_rate": 2.217870384876495e-05,
      "loss": 1.703,
      "step": 426200
    },
    {
      "epoch": 33.39338868870437,
      "grad_norm": 6.919210433959961,
      "learning_rate": 2.217217609274636e-05,
      "loss": 1.6898,
      "step": 426300
    },
    {
      "epoch": 33.40122199592668,
      "grad_norm": 5.33837890625,
      "learning_rate": 2.2165648336727768e-05,
      "loss": 1.6129,
      "step": 426400
    },
    {
      "epoch": 33.40905530314899,
      "grad_norm": 7.8415703773498535,
      "learning_rate": 2.2159120580709177e-05,
      "loss": 1.5893,
      "step": 426500
    },
    {
      "epoch": 33.4168886103713,
      "grad_norm": 6.093045234680176,
      "learning_rate": 2.2152592824690586e-05,
      "loss": 1.6497,
      "step": 426600
    },
    {
      "epoch": 33.42472191759361,
      "grad_norm": 5.740977764129639,
      "learning_rate": 2.2146065068671996e-05,
      "loss": 1.6533,
      "step": 426700
    },
    {
      "epoch": 33.432555224815914,
      "grad_norm": 6.024376392364502,
      "learning_rate": 2.21395373126534e-05,
      "loss": 1.6821,
      "step": 426800
    },
    {
      "epoch": 33.44038853203823,
      "grad_norm": 4.956670761108398,
      "learning_rate": 2.213300955663481e-05,
      "loss": 1.7189,
      "step": 426900
    },
    {
      "epoch": 33.44822183926053,
      "grad_norm": 6.3260979652404785,
      "learning_rate": 2.2126481800616223e-05,
      "loss": 1.6246,
      "step": 427000
    },
    {
      "epoch": 33.456055146482846,
      "grad_norm": 6.45249605178833,
      "learning_rate": 2.211995404459763e-05,
      "loss": 1.6807,
      "step": 427100
    },
    {
      "epoch": 33.46388845370515,
      "grad_norm": 5.8886637687683105,
      "learning_rate": 2.2113426288579038e-05,
      "loss": 1.6423,
      "step": 427200
    },
    {
      "epoch": 33.471721760927466,
      "grad_norm": 8.112308502197266,
      "learning_rate": 2.210689853256045e-05,
      "loss": 1.6881,
      "step": 427300
    },
    {
      "epoch": 33.47955506814977,
      "grad_norm": 10.492344856262207,
      "learning_rate": 2.2100370776541856e-05,
      "loss": 1.6793,
      "step": 427400
    },
    {
      "epoch": 33.487388375372085,
      "grad_norm": 5.695095539093018,
      "learning_rate": 2.2093843020523266e-05,
      "loss": 1.7113,
      "step": 427500
    },
    {
      "epoch": 33.49522168259439,
      "grad_norm": 6.307187557220459,
      "learning_rate": 2.2087315264504675e-05,
      "loss": 1.5711,
      "step": 427600
    },
    {
      "epoch": 33.503054989816704,
      "grad_norm": 6.376465320587158,
      "learning_rate": 2.2080787508486084e-05,
      "loss": 1.6703,
      "step": 427700
    },
    {
      "epoch": 33.51088829703901,
      "grad_norm": 7.423404693603516,
      "learning_rate": 2.2074259752467493e-05,
      "loss": 1.6475,
      "step": 427800
    },
    {
      "epoch": 33.518721604261316,
      "grad_norm": 4.9466633796691895,
      "learning_rate": 2.2067731996448902e-05,
      "loss": 1.6844,
      "step": 427900
    },
    {
      "epoch": 33.52655491148363,
      "grad_norm": 6.817868232727051,
      "learning_rate": 2.206120424043031e-05,
      "loss": 1.7152,
      "step": 428000
    },
    {
      "epoch": 33.534388218705935,
      "grad_norm": 6.128878116607666,
      "learning_rate": 2.205467648441172e-05,
      "loss": 1.6519,
      "step": 428100
    },
    {
      "epoch": 33.54222152592825,
      "grad_norm": 6.4707465171813965,
      "learning_rate": 2.204814872839313e-05,
      "loss": 1.6792,
      "step": 428200
    },
    {
      "epoch": 33.550054833150554,
      "grad_norm": 6.5943074226379395,
      "learning_rate": 2.204162097237454e-05,
      "loss": 1.7171,
      "step": 428300
    },
    {
      "epoch": 33.55788814037287,
      "grad_norm": 4.355861186981201,
      "learning_rate": 2.2035093216355945e-05,
      "loss": 1.6743,
      "step": 428400
    },
    {
      "epoch": 33.56572144759517,
      "grad_norm": 6.60885763168335,
      "learning_rate": 2.2028565460337354e-05,
      "loss": 1.7128,
      "step": 428500
    },
    {
      "epoch": 33.573554754817486,
      "grad_norm": 6.556209564208984,
      "learning_rate": 2.2022037704318766e-05,
      "loss": 1.6809,
      "step": 428600
    },
    {
      "epoch": 33.58138806203979,
      "grad_norm": 8.125504493713379,
      "learning_rate": 2.2015509948300172e-05,
      "loss": 1.6869,
      "step": 428700
    },
    {
      "epoch": 33.589221369262106,
      "grad_norm": 3.805485963821411,
      "learning_rate": 2.200898219228158e-05,
      "loss": 1.6329,
      "step": 428800
    },
    {
      "epoch": 33.59705467648441,
      "grad_norm": 4.946738243103027,
      "learning_rate": 2.2002454436262994e-05,
      "loss": 1.6832,
      "step": 428900
    },
    {
      "epoch": 33.60488798370672,
      "grad_norm": 6.543231010437012,
      "learning_rate": 2.19959266802444e-05,
      "loss": 1.7183,
      "step": 429000
    },
    {
      "epoch": 33.61272129092903,
      "grad_norm": 8.058924674987793,
      "learning_rate": 2.198939892422581e-05,
      "loss": 1.642,
      "step": 429100
    },
    {
      "epoch": 33.62055459815134,
      "grad_norm": 7.741182327270508,
      "learning_rate": 2.1982871168207218e-05,
      "loss": 1.6958,
      "step": 429200
    },
    {
      "epoch": 33.62838790537365,
      "grad_norm": 6.957149028778076,
      "learning_rate": 2.1976343412188627e-05,
      "loss": 1.6592,
      "step": 429300
    },
    {
      "epoch": 33.636221212595956,
      "grad_norm": 8.6131010055542,
      "learning_rate": 2.1969815656170036e-05,
      "loss": 1.6237,
      "step": 429400
    },
    {
      "epoch": 33.64405451981827,
      "grad_norm": 6.850696563720703,
      "learning_rate": 2.1963287900151445e-05,
      "loss": 1.7543,
      "step": 429500
    },
    {
      "epoch": 33.651887827040575,
      "grad_norm": 6.075791835784912,
      "learning_rate": 2.1956760144132854e-05,
      "loss": 1.6548,
      "step": 429600
    },
    {
      "epoch": 33.65972113426289,
      "grad_norm": 6.322214603424072,
      "learning_rate": 2.1950232388114264e-05,
      "loss": 1.6708,
      "step": 429700
    },
    {
      "epoch": 33.667554441485194,
      "grad_norm": 8.055259704589844,
      "learning_rate": 2.1943704632095673e-05,
      "loss": 1.6632,
      "step": 429800
    },
    {
      "epoch": 33.67538774870751,
      "grad_norm": 7.571239948272705,
      "learning_rate": 2.1937176876077082e-05,
      "loss": 1.6637,
      "step": 429900
    },
    {
      "epoch": 33.68322105592981,
      "grad_norm": 5.535062313079834,
      "learning_rate": 2.1930649120058488e-05,
      "loss": 1.7192,
      "step": 430000
    },
    {
      "epoch": 33.69105436315212,
      "grad_norm": 7.095712184906006,
      "learning_rate": 2.1924121364039897e-05,
      "loss": 1.6167,
      "step": 430100
    },
    {
      "epoch": 33.69888767037443,
      "grad_norm": 6.675171375274658,
      "learning_rate": 2.191759360802131e-05,
      "loss": 1.6139,
      "step": 430200
    },
    {
      "epoch": 33.70672097759674,
      "grad_norm": 5.728940010070801,
      "learning_rate": 2.1911065852002715e-05,
      "loss": 1.6803,
      "step": 430300
    },
    {
      "epoch": 33.71455428481905,
      "grad_norm": 6.453934669494629,
      "learning_rate": 2.1904538095984124e-05,
      "loss": 1.6718,
      "step": 430400
    },
    {
      "epoch": 33.72238759204136,
      "grad_norm": 6.282187461853027,
      "learning_rate": 2.1898010339965537e-05,
      "loss": 1.6047,
      "step": 430500
    },
    {
      "epoch": 33.73022089926367,
      "grad_norm": 6.3454084396362305,
      "learning_rate": 2.1891482583946943e-05,
      "loss": 1.6837,
      "step": 430600
    },
    {
      "epoch": 33.73805420648598,
      "grad_norm": 6.426464557647705,
      "learning_rate": 2.1884954827928352e-05,
      "loss": 1.721,
      "step": 430700
    },
    {
      "epoch": 33.74588751370829,
      "grad_norm": 7.645761489868164,
      "learning_rate": 2.187842707190976e-05,
      "loss": 1.5863,
      "step": 430800
    },
    {
      "epoch": 33.753720820930596,
      "grad_norm": 6.677402496337891,
      "learning_rate": 2.187189931589117e-05,
      "loss": 1.6812,
      "step": 430900
    },
    {
      "epoch": 33.76155412815291,
      "grad_norm": 8.164277076721191,
      "learning_rate": 2.186537155987258e-05,
      "loss": 1.7619,
      "step": 431000
    },
    {
      "epoch": 33.769387435375215,
      "grad_norm": 5.812943458557129,
      "learning_rate": 2.185884380385399e-05,
      "loss": 1.6594,
      "step": 431100
    },
    {
      "epoch": 33.77722074259753,
      "grad_norm": 5.9227800369262695,
      "learning_rate": 2.1852316047835398e-05,
      "loss": 1.6944,
      "step": 431200
    },
    {
      "epoch": 33.785054049819834,
      "grad_norm": 3.543344259262085,
      "learning_rate": 2.1845788291816807e-05,
      "loss": 1.6626,
      "step": 431300
    },
    {
      "epoch": 33.79288735704214,
      "grad_norm": 6.42153263092041,
      "learning_rate": 2.1839260535798216e-05,
      "loss": 1.7424,
      "step": 431400
    },
    {
      "epoch": 33.80072066426445,
      "grad_norm": 5.238714218139648,
      "learning_rate": 2.1832732779779625e-05,
      "loss": 1.736,
      "step": 431500
    },
    {
      "epoch": 33.80855397148676,
      "grad_norm": 6.778879165649414,
      "learning_rate": 2.182620502376103e-05,
      "loss": 1.6248,
      "step": 431600
    },
    {
      "epoch": 33.81638727870907,
      "grad_norm": 6.927711009979248,
      "learning_rate": 2.181967726774244e-05,
      "loss": 1.7114,
      "step": 431700
    },
    {
      "epoch": 33.82422058593138,
      "grad_norm": 7.963430404663086,
      "learning_rate": 2.1813149511723852e-05,
      "loss": 1.8021,
      "step": 431800
    },
    {
      "epoch": 33.83205389315369,
      "grad_norm": 6.228050231933594,
      "learning_rate": 2.1806621755705258e-05,
      "loss": 1.6836,
      "step": 431900
    },
    {
      "epoch": 33.839887200376,
      "grad_norm": 6.848330974578857,
      "learning_rate": 2.1800093999686667e-05,
      "loss": 1.6332,
      "step": 432000
    },
    {
      "epoch": 33.84772050759831,
      "grad_norm": 7.803352355957031,
      "learning_rate": 2.179356624366808e-05,
      "loss": 1.7032,
      "step": 432100
    },
    {
      "epoch": 33.85555381482062,
      "grad_norm": 5.7597737312316895,
      "learning_rate": 2.1787038487649486e-05,
      "loss": 1.614,
      "step": 432200
    },
    {
      "epoch": 33.86338712204293,
      "grad_norm": 9.075312614440918,
      "learning_rate": 2.1780510731630895e-05,
      "loss": 1.6218,
      "step": 432300
    },
    {
      "epoch": 33.871220429265236,
      "grad_norm": 6.267805099487305,
      "learning_rate": 2.1773982975612307e-05,
      "loss": 1.655,
      "step": 432400
    },
    {
      "epoch": 33.87905373648754,
      "grad_norm": 6.871285915374756,
      "learning_rate": 2.1767455219593713e-05,
      "loss": 1.7952,
      "step": 432500
    },
    {
      "epoch": 33.886887043709855,
      "grad_norm": 7.5182294845581055,
      "learning_rate": 2.1760927463575122e-05,
      "loss": 1.6769,
      "step": 432600
    },
    {
      "epoch": 33.89472035093216,
      "grad_norm": 6.465356349945068,
      "learning_rate": 2.175439970755653e-05,
      "loss": 1.6966,
      "step": 432700
    },
    {
      "epoch": 33.902553658154474,
      "grad_norm": 5.17374849319458,
      "learning_rate": 2.174787195153794e-05,
      "loss": 1.6877,
      "step": 432800
    },
    {
      "epoch": 33.91038696537678,
      "grad_norm": 5.991213321685791,
      "learning_rate": 2.174134419551935e-05,
      "loss": 1.7226,
      "step": 432900
    },
    {
      "epoch": 33.91822027259909,
      "grad_norm": 6.366955757141113,
      "learning_rate": 2.173481643950076e-05,
      "loss": 1.644,
      "step": 433000
    },
    {
      "epoch": 33.9260535798214,
      "grad_norm": 6.887526988983154,
      "learning_rate": 2.1728288683482168e-05,
      "loss": 1.6879,
      "step": 433100
    },
    {
      "epoch": 33.93388688704371,
      "grad_norm": 6.580845832824707,
      "learning_rate": 2.1721760927463577e-05,
      "loss": 1.7397,
      "step": 433200
    },
    {
      "epoch": 33.94172019426602,
      "grad_norm": 4.791543006896973,
      "learning_rate": 2.1715233171444983e-05,
      "loss": 1.7187,
      "step": 433300
    },
    {
      "epoch": 33.94955350148833,
      "grad_norm": 7.937172889709473,
      "learning_rate": 2.1708705415426396e-05,
      "loss": 1.6891,
      "step": 433400
    },
    {
      "epoch": 33.95738680871064,
      "grad_norm": 6.118809223175049,
      "learning_rate": 2.17021776594078e-05,
      "loss": 1.7952,
      "step": 433500
    },
    {
      "epoch": 33.965220115932944,
      "grad_norm": 7.288529396057129,
      "learning_rate": 2.169564990338921e-05,
      "loss": 1.6585,
      "step": 433600
    },
    {
      "epoch": 33.97305342315526,
      "grad_norm": 6.023906230926514,
      "learning_rate": 2.1689122147370623e-05,
      "loss": 1.6714,
      "step": 433700
    },
    {
      "epoch": 33.98088673037756,
      "grad_norm": 7.042069911956787,
      "learning_rate": 2.168259439135203e-05,
      "loss": 1.6699,
      "step": 433800
    },
    {
      "epoch": 33.988720037599876,
      "grad_norm": 5.696055889129639,
      "learning_rate": 2.1676066635333438e-05,
      "loss": 1.7611,
      "step": 433900
    },
    {
      "epoch": 33.99655334482218,
      "grad_norm": 5.705899715423584,
      "learning_rate": 2.166953887931485e-05,
      "loss": 1.6065,
      "step": 434000
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.77446711063385,
      "eval_runtime": 1.5212,
      "eval_samples_per_second": 441.757,
      "eval_steps_per_second": 441.757,
      "step": 434044
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.4263103008270264,
      "eval_runtime": 28.8578,
      "eval_samples_per_second": 442.376,
      "eval_steps_per_second": 442.376,
      "step": 434044
    },
    {
      "epoch": 34.004386652044495,
      "grad_norm": 6.589372634887695,
      "learning_rate": 2.1663011123296256e-05,
      "loss": 1.62,
      "step": 434100
    },
    {
      "epoch": 34.0122199592668,
      "grad_norm": 8.368361473083496,
      "learning_rate": 2.1656483367277665e-05,
      "loss": 1.6696,
      "step": 434200
    },
    {
      "epoch": 34.020053266489114,
      "grad_norm": 4.893921375274658,
      "learning_rate": 2.1649955611259075e-05,
      "loss": 1.6254,
      "step": 434300
    },
    {
      "epoch": 34.02788657371142,
      "grad_norm": 4.918912410736084,
      "learning_rate": 2.1643427855240484e-05,
      "loss": 1.6972,
      "step": 434400
    },
    {
      "epoch": 34.03571988093373,
      "grad_norm": 5.910942554473877,
      "learning_rate": 2.1636900099221893e-05,
      "loss": 1.6322,
      "step": 434500
    },
    {
      "epoch": 34.04355318815604,
      "grad_norm": 7.491869926452637,
      "learning_rate": 2.1630372343203302e-05,
      "loss": 1.6796,
      "step": 434600
    },
    {
      "epoch": 34.051386495378345,
      "grad_norm": 6.3203277587890625,
      "learning_rate": 2.162384458718471e-05,
      "loss": 1.673,
      "step": 434700
    },
    {
      "epoch": 34.05921980260066,
      "grad_norm": 6.407268524169922,
      "learning_rate": 2.161731683116612e-05,
      "loss": 1.6695,
      "step": 434800
    },
    {
      "epoch": 34.067053109822965,
      "grad_norm": 5.990756988525391,
      "learning_rate": 2.1610789075147526e-05,
      "loss": 1.7055,
      "step": 434900
    },
    {
      "epoch": 34.07488641704528,
      "grad_norm": 4.932746410369873,
      "learning_rate": 2.160426131912894e-05,
      "loss": 1.6368,
      "step": 435000
    },
    {
      "epoch": 34.082719724267584,
      "grad_norm": 6.3143157958984375,
      "learning_rate": 2.1597733563110344e-05,
      "loss": 1.6239,
      "step": 435100
    },
    {
      "epoch": 34.0905530314899,
      "grad_norm": 7.262462139129639,
      "learning_rate": 2.1591205807091754e-05,
      "loss": 1.6401,
      "step": 435200
    },
    {
      "epoch": 34.0983863387122,
      "grad_norm": 6.992152214050293,
      "learning_rate": 2.1584678051073166e-05,
      "loss": 1.5876,
      "step": 435300
    },
    {
      "epoch": 34.106219645934516,
      "grad_norm": 5.968423843383789,
      "learning_rate": 2.1578150295054572e-05,
      "loss": 1.6638,
      "step": 435400
    },
    {
      "epoch": 34.11405295315682,
      "grad_norm": 8.40834903717041,
      "learning_rate": 2.157162253903598e-05,
      "loss": 1.6721,
      "step": 435500
    },
    {
      "epoch": 34.121886260379135,
      "grad_norm": 5.390454292297363,
      "learning_rate": 2.1565094783017394e-05,
      "loss": 1.7344,
      "step": 435600
    },
    {
      "epoch": 34.12971956760144,
      "grad_norm": 5.714684009552002,
      "learning_rate": 2.15585670269988e-05,
      "loss": 1.7247,
      "step": 435700
    },
    {
      "epoch": 34.13755287482375,
      "grad_norm": 6.151775360107422,
      "learning_rate": 2.155203927098021e-05,
      "loss": 1.6919,
      "step": 435800
    },
    {
      "epoch": 34.14538618204606,
      "grad_norm": 5.033383369445801,
      "learning_rate": 2.1545511514961618e-05,
      "loss": 1.6935,
      "step": 435900
    },
    {
      "epoch": 34.153219489268366,
      "grad_norm": 5.630482196807861,
      "learning_rate": 2.1538983758943027e-05,
      "loss": 1.6588,
      "step": 436000
    },
    {
      "epoch": 34.16105279649068,
      "grad_norm": 5.636383533477783,
      "learning_rate": 2.1532456002924436e-05,
      "loss": 1.6607,
      "step": 436100
    },
    {
      "epoch": 34.168886103712985,
      "grad_norm": 6.993265628814697,
      "learning_rate": 2.1525928246905845e-05,
      "loss": 1.714,
      "step": 436200
    },
    {
      "epoch": 34.1767194109353,
      "grad_norm": 5.973097801208496,
      "learning_rate": 2.1519400490887254e-05,
      "loss": 1.6375,
      "step": 436300
    },
    {
      "epoch": 34.184552718157605,
      "grad_norm": 5.093976020812988,
      "learning_rate": 2.1512872734868664e-05,
      "loss": 1.6605,
      "step": 436400
    },
    {
      "epoch": 34.19238602537992,
      "grad_norm": 7.359581470489502,
      "learning_rate": 2.150634497885007e-05,
      "loss": 1.6979,
      "step": 436500
    },
    {
      "epoch": 34.200219332602224,
      "grad_norm": 7.64363956451416,
      "learning_rate": 2.1499817222831482e-05,
      "loss": 1.5632,
      "step": 436600
    },
    {
      "epoch": 34.20805263982454,
      "grad_norm": 7.216948509216309,
      "learning_rate": 2.1493289466812888e-05,
      "loss": 1.6285,
      "step": 436700
    },
    {
      "epoch": 34.21588594704684,
      "grad_norm": 6.362281322479248,
      "learning_rate": 2.1486761710794297e-05,
      "loss": 1.621,
      "step": 436800
    },
    {
      "epoch": 34.223719254269156,
      "grad_norm": 4.792776584625244,
      "learning_rate": 2.148023395477571e-05,
      "loss": 1.6702,
      "step": 436900
    },
    {
      "epoch": 34.23155256149146,
      "grad_norm": 7.3492865562438965,
      "learning_rate": 2.1473706198757115e-05,
      "loss": 1.6283,
      "step": 437000
    },
    {
      "epoch": 34.23938586871377,
      "grad_norm": 6.098209381103516,
      "learning_rate": 2.1467178442738524e-05,
      "loss": 1.6897,
      "step": 437100
    },
    {
      "epoch": 34.24721917593608,
      "grad_norm": 6.238315105438232,
      "learning_rate": 2.1460650686719937e-05,
      "loss": 1.6483,
      "step": 437200
    },
    {
      "epoch": 34.25505248315839,
      "grad_norm": 4.346864223480225,
      "learning_rate": 2.1454122930701343e-05,
      "loss": 1.6618,
      "step": 437300
    },
    {
      "epoch": 34.2628857903807,
      "grad_norm": 6.623025417327881,
      "learning_rate": 2.144759517468275e-05,
      "loss": 1.6687,
      "step": 437400
    },
    {
      "epoch": 34.270719097603006,
      "grad_norm": 7.831139087677002,
      "learning_rate": 2.144106741866416e-05,
      "loss": 1.6651,
      "step": 437500
    },
    {
      "epoch": 34.27855240482532,
      "grad_norm": 8.435400009155273,
      "learning_rate": 2.143453966264557e-05,
      "loss": 1.654,
      "step": 437600
    },
    {
      "epoch": 34.286385712047625,
      "grad_norm": 7.7957844734191895,
      "learning_rate": 2.142801190662698e-05,
      "loss": 1.6272,
      "step": 437700
    },
    {
      "epoch": 34.29421901926994,
      "grad_norm": 9.528878211975098,
      "learning_rate": 2.1421484150608388e-05,
      "loss": 1.7114,
      "step": 437800
    },
    {
      "epoch": 34.302052326492245,
      "grad_norm": 6.645228862762451,
      "learning_rate": 2.1414956394589797e-05,
      "loss": 1.6864,
      "step": 437900
    },
    {
      "epoch": 34.30988563371456,
      "grad_norm": 3.8500001430511475,
      "learning_rate": 2.1408428638571207e-05,
      "loss": 1.6863,
      "step": 438000
    },
    {
      "epoch": 34.317718940936864,
      "grad_norm": 7.4223222732543945,
      "learning_rate": 2.1401900882552612e-05,
      "loss": 1.684,
      "step": 438100
    },
    {
      "epoch": 34.32555224815917,
      "grad_norm": 4.98826789855957,
      "learning_rate": 2.1395373126534025e-05,
      "loss": 1.7501,
      "step": 438200
    },
    {
      "epoch": 34.33338555538148,
      "grad_norm": 5.828303813934326,
      "learning_rate": 2.1388845370515434e-05,
      "loss": 1.6082,
      "step": 438300
    },
    {
      "epoch": 34.34121886260379,
      "grad_norm": 6.899249076843262,
      "learning_rate": 2.138231761449684e-05,
      "loss": 1.6621,
      "step": 438400
    },
    {
      "epoch": 34.3490521698261,
      "grad_norm": 5.315393924713135,
      "learning_rate": 2.1375789858478252e-05,
      "loss": 1.6527,
      "step": 438500
    },
    {
      "epoch": 34.35688547704841,
      "grad_norm": 6.547765731811523,
      "learning_rate": 2.1369262102459658e-05,
      "loss": 1.7176,
      "step": 438600
    },
    {
      "epoch": 34.36471878427072,
      "grad_norm": 7.277853965759277,
      "learning_rate": 2.1362734346441067e-05,
      "loss": 1.7315,
      "step": 438700
    },
    {
      "epoch": 34.37255209149303,
      "grad_norm": 6.285191059112549,
      "learning_rate": 2.135620659042248e-05,
      "loss": 1.705,
      "step": 438800
    },
    {
      "epoch": 34.38038539871534,
      "grad_norm": 6.1404595375061035,
      "learning_rate": 2.1349678834403886e-05,
      "loss": 1.7075,
      "step": 438900
    },
    {
      "epoch": 34.388218705937646,
      "grad_norm": 8.303650856018066,
      "learning_rate": 2.1343151078385295e-05,
      "loss": 1.6419,
      "step": 439000
    },
    {
      "epoch": 34.39605201315996,
      "grad_norm": 8.628412246704102,
      "learning_rate": 2.1336623322366704e-05,
      "loss": 1.6586,
      "step": 439100
    },
    {
      "epoch": 34.403885320382265,
      "grad_norm": 5.900317668914795,
      "learning_rate": 2.1330095566348113e-05,
      "loss": 1.6421,
      "step": 439200
    },
    {
      "epoch": 34.41171862760457,
      "grad_norm": 7.08776330947876,
      "learning_rate": 2.1323567810329522e-05,
      "loss": 1.7531,
      "step": 439300
    },
    {
      "epoch": 34.419551934826885,
      "grad_norm": 6.939919471740723,
      "learning_rate": 2.131704005431093e-05,
      "loss": 1.7218,
      "step": 439400
    },
    {
      "epoch": 34.42738524204919,
      "grad_norm": 5.364986896514893,
      "learning_rate": 2.131051229829234e-05,
      "loss": 1.664,
      "step": 439500
    },
    {
      "epoch": 34.435218549271504,
      "grad_norm": 7.261593341827393,
      "learning_rate": 2.130398454227375e-05,
      "loss": 1.5247,
      "step": 439600
    },
    {
      "epoch": 34.44305185649381,
      "grad_norm": 7.994295597076416,
      "learning_rate": 2.1297456786255156e-05,
      "loss": 1.7665,
      "step": 439700
    },
    {
      "epoch": 34.45088516371612,
      "grad_norm": 7.031672954559326,
      "learning_rate": 2.1290929030236568e-05,
      "loss": 1.697,
      "step": 439800
    },
    {
      "epoch": 34.45871847093843,
      "grad_norm": 7.276601791381836,
      "learning_rate": 2.1284401274217977e-05,
      "loss": 1.6133,
      "step": 439900
    },
    {
      "epoch": 34.46655177816074,
      "grad_norm": 6.896847724914551,
      "learning_rate": 2.1277873518199383e-05,
      "loss": 1.6825,
      "step": 440000
    },
    {
      "epoch": 34.47438508538305,
      "grad_norm": 5.747932434082031,
      "learning_rate": 2.1271345762180796e-05,
      "loss": 1.6555,
      "step": 440100
    },
    {
      "epoch": 34.48221839260536,
      "grad_norm": 7.005621910095215,
      "learning_rate": 2.12648180061622e-05,
      "loss": 1.7064,
      "step": 440200
    },
    {
      "epoch": 34.49005169982767,
      "grad_norm": 5.486302375793457,
      "learning_rate": 2.125829025014361e-05,
      "loss": 1.6363,
      "step": 440300
    },
    {
      "epoch": 34.49788500704997,
      "grad_norm": 5.700759410858154,
      "learning_rate": 2.1251762494125023e-05,
      "loss": 1.6237,
      "step": 440400
    },
    {
      "epoch": 34.505718314272286,
      "grad_norm": 5.826670169830322,
      "learning_rate": 2.124523473810643e-05,
      "loss": 1.6071,
      "step": 440500
    },
    {
      "epoch": 34.51355162149459,
      "grad_norm": 7.086772918701172,
      "learning_rate": 2.1238706982087838e-05,
      "loss": 1.702,
      "step": 440600
    },
    {
      "epoch": 34.521384928716905,
      "grad_norm": 5.860368728637695,
      "learning_rate": 2.1232179226069247e-05,
      "loss": 1.7908,
      "step": 440700
    },
    {
      "epoch": 34.52921823593921,
      "grad_norm": 6.025461196899414,
      "learning_rate": 2.1225651470050656e-05,
      "loss": 1.6784,
      "step": 440800
    },
    {
      "epoch": 34.537051543161525,
      "grad_norm": 6.276752471923828,
      "learning_rate": 2.1219123714032065e-05,
      "loss": 1.6047,
      "step": 440900
    },
    {
      "epoch": 34.54488485038383,
      "grad_norm": 5.949711799621582,
      "learning_rate": 2.1212595958013475e-05,
      "loss": 1.6361,
      "step": 441000
    },
    {
      "epoch": 34.552718157606144,
      "grad_norm": 5.153453350067139,
      "learning_rate": 2.1206068201994884e-05,
      "loss": 1.6932,
      "step": 441100
    },
    {
      "epoch": 34.56055146482845,
      "grad_norm": 7.67908239364624,
      "learning_rate": 2.1199540445976293e-05,
      "loss": 1.6405,
      "step": 441200
    },
    {
      "epoch": 34.56838477205076,
      "grad_norm": 7.986481666564941,
      "learning_rate": 2.11930126899577e-05,
      "loss": 1.6386,
      "step": 441300
    },
    {
      "epoch": 34.57621807927307,
      "grad_norm": 4.745147705078125,
      "learning_rate": 2.118648493393911e-05,
      "loss": 1.6323,
      "step": 441400
    },
    {
      "epoch": 34.584051386495375,
      "grad_norm": 5.12487268447876,
      "learning_rate": 2.117995717792052e-05,
      "loss": 1.6098,
      "step": 441500
    },
    {
      "epoch": 34.59188469371769,
      "grad_norm": 5.982395648956299,
      "learning_rate": 2.1173429421901926e-05,
      "loss": 1.6371,
      "step": 441600
    },
    {
      "epoch": 34.599718000939994,
      "grad_norm": 5.229960918426514,
      "learning_rate": 2.116690166588334e-05,
      "loss": 1.645,
      "step": 441700
    },
    {
      "epoch": 34.60755130816231,
      "grad_norm": 6.841376304626465,
      "learning_rate": 2.1160373909864744e-05,
      "loss": 1.6666,
      "step": 441800
    },
    {
      "epoch": 34.61538461538461,
      "grad_norm": 7.826835632324219,
      "learning_rate": 2.1153846153846154e-05,
      "loss": 1.6259,
      "step": 441900
    },
    {
      "epoch": 34.623217922606926,
      "grad_norm": 4.517951965332031,
      "learning_rate": 2.1147318397827566e-05,
      "loss": 1.6821,
      "step": 442000
    },
    {
      "epoch": 34.63105122982923,
      "grad_norm": 7.292026042938232,
      "learning_rate": 2.1140790641808972e-05,
      "loss": 1.6792,
      "step": 442100
    },
    {
      "epoch": 34.638884537051545,
      "grad_norm": 5.8273701667785645,
      "learning_rate": 2.113426288579038e-05,
      "loss": 1.6581,
      "step": 442200
    },
    {
      "epoch": 34.64671784427385,
      "grad_norm": 6.877757549285889,
      "learning_rate": 2.112773512977179e-05,
      "loss": 1.6736,
      "step": 442300
    },
    {
      "epoch": 34.654551151496165,
      "grad_norm": 6.2561116218566895,
      "learning_rate": 2.11212073737532e-05,
      "loss": 1.677,
      "step": 442400
    },
    {
      "epoch": 34.66238445871847,
      "grad_norm": 5.853832244873047,
      "learning_rate": 2.111467961773461e-05,
      "loss": 1.7015,
      "step": 442500
    },
    {
      "epoch": 34.67021776594078,
      "grad_norm": 5.874725341796875,
      "learning_rate": 2.1108151861716018e-05,
      "loss": 1.732,
      "step": 442600
    },
    {
      "epoch": 34.67805107316309,
      "grad_norm": 8.172525405883789,
      "learning_rate": 2.1101624105697427e-05,
      "loss": 1.6776,
      "step": 442700
    },
    {
      "epoch": 34.685884380385396,
      "grad_norm": 5.5733113288879395,
      "learning_rate": 2.1095096349678836e-05,
      "loss": 1.6087,
      "step": 442800
    },
    {
      "epoch": 34.69371768760771,
      "grad_norm": 6.1440300941467285,
      "learning_rate": 2.1088568593660242e-05,
      "loss": 1.5678,
      "step": 442900
    },
    {
      "epoch": 34.701550994830015,
      "grad_norm": 6.485428333282471,
      "learning_rate": 2.1082040837641654e-05,
      "loss": 1.6109,
      "step": 443000
    },
    {
      "epoch": 34.70938430205233,
      "grad_norm": 6.841461658477783,
      "learning_rate": 2.1075513081623063e-05,
      "loss": 1.7221,
      "step": 443100
    },
    {
      "epoch": 34.717217609274634,
      "grad_norm": 4.186244487762451,
      "learning_rate": 2.106898532560447e-05,
      "loss": 1.6779,
      "step": 443200
    },
    {
      "epoch": 34.72505091649695,
      "grad_norm": 5.57611608505249,
      "learning_rate": 2.1062457569585882e-05,
      "loss": 1.7677,
      "step": 443300
    },
    {
      "epoch": 34.73288422371925,
      "grad_norm": 8.094864845275879,
      "learning_rate": 2.1055929813567288e-05,
      "loss": 1.7301,
      "step": 443400
    },
    {
      "epoch": 34.740717530941566,
      "grad_norm": 7.035470962524414,
      "learning_rate": 2.1049402057548697e-05,
      "loss": 1.6114,
      "step": 443500
    },
    {
      "epoch": 34.74855083816387,
      "grad_norm": 6.334265232086182,
      "learning_rate": 2.104287430153011e-05,
      "loss": 1.7803,
      "step": 443600
    },
    {
      "epoch": 34.756384145386185,
      "grad_norm": 5.442199230194092,
      "learning_rate": 2.1036346545511515e-05,
      "loss": 1.667,
      "step": 443700
    },
    {
      "epoch": 34.76421745260849,
      "grad_norm": 4.683063507080078,
      "learning_rate": 2.1029818789492924e-05,
      "loss": 1.7429,
      "step": 443800
    },
    {
      "epoch": 34.7720507598308,
      "grad_norm": 5.777497291564941,
      "learning_rate": 2.1023291033474333e-05,
      "loss": 1.5231,
      "step": 443900
    },
    {
      "epoch": 34.77988406705311,
      "grad_norm": 7.454638957977295,
      "learning_rate": 2.1016763277455742e-05,
      "loss": 1.7008,
      "step": 444000
    },
    {
      "epoch": 34.78771737427542,
      "grad_norm": 4.80476188659668,
      "learning_rate": 2.101023552143715e-05,
      "loss": 1.6883,
      "step": 444100
    },
    {
      "epoch": 34.79555068149773,
      "grad_norm": 7.623110771179199,
      "learning_rate": 2.100370776541856e-05,
      "loss": 1.7299,
      "step": 444200
    },
    {
      "epoch": 34.803383988720036,
      "grad_norm": 5.979969501495361,
      "learning_rate": 2.099718000939997e-05,
      "loss": 1.6476,
      "step": 444300
    },
    {
      "epoch": 34.81121729594235,
      "grad_norm": 6.602956295013428,
      "learning_rate": 2.099065225338138e-05,
      "loss": 1.5913,
      "step": 444400
    },
    {
      "epoch": 34.819050603164655,
      "grad_norm": 5.137205600738525,
      "learning_rate": 2.0984124497362785e-05,
      "loss": 1.6267,
      "step": 444500
    },
    {
      "epoch": 34.82688391038697,
      "grad_norm": 5.440038681030273,
      "learning_rate": 2.0977596741344197e-05,
      "loss": 1.6442,
      "step": 444600
    },
    {
      "epoch": 34.834717217609274,
      "grad_norm": 6.997143745422363,
      "learning_rate": 2.0971068985325607e-05,
      "loss": 1.7065,
      "step": 444700
    },
    {
      "epoch": 34.84255052483159,
      "grad_norm": 4.353954315185547,
      "learning_rate": 2.0964541229307012e-05,
      "loss": 1.7175,
      "step": 444800
    },
    {
      "epoch": 34.85038383205389,
      "grad_norm": 4.363712787628174,
      "learning_rate": 2.0958013473288425e-05,
      "loss": 1.7063,
      "step": 444900
    },
    {
      "epoch": 34.8582171392762,
      "grad_norm": 9.376005172729492,
      "learning_rate": 2.0951485717269834e-05,
      "loss": 1.7241,
      "step": 445000
    },
    {
      "epoch": 34.86605044649851,
      "grad_norm": 7.756702423095703,
      "learning_rate": 2.094495796125124e-05,
      "loss": 1.5999,
      "step": 445100
    },
    {
      "epoch": 34.87388375372082,
      "grad_norm": 5.859556674957275,
      "learning_rate": 2.0938430205232652e-05,
      "loss": 1.7512,
      "step": 445200
    },
    {
      "epoch": 34.88171706094313,
      "grad_norm": 5.674627304077148,
      "learning_rate": 2.0931902449214058e-05,
      "loss": 1.629,
      "step": 445300
    },
    {
      "epoch": 34.88955036816544,
      "grad_norm": 5.514002799987793,
      "learning_rate": 2.0925374693195467e-05,
      "loss": 1.6948,
      "step": 445400
    },
    {
      "epoch": 34.89738367538775,
      "grad_norm": 5.7811970710754395,
      "learning_rate": 2.0918846937176876e-05,
      "loss": 1.7265,
      "step": 445500
    },
    {
      "epoch": 34.90521698261006,
      "grad_norm": 6.054728984832764,
      "learning_rate": 2.0912319181158286e-05,
      "loss": 1.77,
      "step": 445600
    },
    {
      "epoch": 34.91305028983237,
      "grad_norm": 4.52036714553833,
      "learning_rate": 2.0905791425139695e-05,
      "loss": 1.702,
      "step": 445700
    },
    {
      "epoch": 34.920883597054676,
      "grad_norm": 7.196091175079346,
      "learning_rate": 2.0899263669121104e-05,
      "loss": 1.7676,
      "step": 445800
    },
    {
      "epoch": 34.92871690427699,
      "grad_norm": 8.457015991210938,
      "learning_rate": 2.0892735913102513e-05,
      "loss": 1.7475,
      "step": 445900
    },
    {
      "epoch": 34.936550211499295,
      "grad_norm": 6.4482340812683105,
      "learning_rate": 2.0886208157083922e-05,
      "loss": 1.7071,
      "step": 446000
    },
    {
      "epoch": 34.9443835187216,
      "grad_norm": 4.071132659912109,
      "learning_rate": 2.0879680401065328e-05,
      "loss": 1.6854,
      "step": 446100
    },
    {
      "epoch": 34.952216825943914,
      "grad_norm": 7.283242702484131,
      "learning_rate": 2.087315264504674e-05,
      "loss": 1.7404,
      "step": 446200
    },
    {
      "epoch": 34.96005013316622,
      "grad_norm": 4.616259574890137,
      "learning_rate": 2.086662488902815e-05,
      "loss": 1.6318,
      "step": 446300
    },
    {
      "epoch": 34.96788344038853,
      "grad_norm": 6.050899028778076,
      "learning_rate": 2.0860097133009555e-05,
      "loss": 1.673,
      "step": 446400
    },
    {
      "epoch": 34.97571674761084,
      "grad_norm": 4.923831939697266,
      "learning_rate": 2.0853569376990968e-05,
      "loss": 1.6676,
      "step": 446500
    },
    {
      "epoch": 34.98355005483315,
      "grad_norm": 7.3720574378967285,
      "learning_rate": 2.0847041620972377e-05,
      "loss": 1.5966,
      "step": 446600
    },
    {
      "epoch": 34.99138336205546,
      "grad_norm": 8.749725341796875,
      "learning_rate": 2.0840513864953783e-05,
      "loss": 1.7346,
      "step": 446700
    },
    {
      "epoch": 34.99921666927777,
      "grad_norm": 5.6995673179626465,
      "learning_rate": 2.0833986108935195e-05,
      "loss": 1.637,
      "step": 446800
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.773601770401001,
      "eval_runtime": 1.508,
      "eval_samples_per_second": 445.61,
      "eval_steps_per_second": 445.61,
      "step": 446810
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.4207395315170288,
      "eval_runtime": 29.3217,
      "eval_samples_per_second": 435.377,
      "eval_steps_per_second": 435.377,
      "step": 446810
    },
    {
      "epoch": 35.00704997650008,
      "grad_norm": 8.899532318115234,
      "learning_rate": 2.08274583529166e-05,
      "loss": 1.7322,
      "step": 446900
    },
    {
      "epoch": 35.01488328372239,
      "grad_norm": 5.525047779083252,
      "learning_rate": 2.082093059689801e-05,
      "loss": 1.5622,
      "step": 447000
    },
    {
      "epoch": 35.0227165909447,
      "grad_norm": 3.465273380279541,
      "learning_rate": 2.081440284087942e-05,
      "loss": 1.6368,
      "step": 447100
    },
    {
      "epoch": 35.030549898167,
      "grad_norm": 4.6410908699035645,
      "learning_rate": 2.080787508486083e-05,
      "loss": 1.7345,
      "step": 447200
    },
    {
      "epoch": 35.038383205389316,
      "grad_norm": 5.515801429748535,
      "learning_rate": 2.0801347328842238e-05,
      "loss": 1.6539,
      "step": 447300
    },
    {
      "epoch": 35.04621651261162,
      "grad_norm": 6.779230117797852,
      "learning_rate": 2.0794819572823647e-05,
      "loss": 1.6023,
      "step": 447400
    },
    {
      "epoch": 35.054049819833935,
      "grad_norm": 5.815415382385254,
      "learning_rate": 2.0788291816805056e-05,
      "loss": 1.6973,
      "step": 447500
    },
    {
      "epoch": 35.06188312705624,
      "grad_norm": 6.364845275878906,
      "learning_rate": 2.0781764060786465e-05,
      "loss": 1.6903,
      "step": 447600
    },
    {
      "epoch": 35.069716434278554,
      "grad_norm": 5.339504718780518,
      "learning_rate": 2.077523630476787e-05,
      "loss": 1.7022,
      "step": 447700
    },
    {
      "epoch": 35.07754974150086,
      "grad_norm": 5.77523946762085,
      "learning_rate": 2.0768708548749284e-05,
      "loss": 1.6928,
      "step": 447800
    },
    {
      "epoch": 35.08538304872317,
      "grad_norm": 5.708443641662598,
      "learning_rate": 2.0762180792730693e-05,
      "loss": 1.6821,
      "step": 447900
    },
    {
      "epoch": 35.09321635594548,
      "grad_norm": 6.437938690185547,
      "learning_rate": 2.07556530367121e-05,
      "loss": 1.6368,
      "step": 448000
    },
    {
      "epoch": 35.10104966316779,
      "grad_norm": 5.829113960266113,
      "learning_rate": 2.074912528069351e-05,
      "loss": 1.7199,
      "step": 448100
    },
    {
      "epoch": 35.1088829703901,
      "grad_norm": 7.431174278259277,
      "learning_rate": 2.074259752467492e-05,
      "loss": 1.6438,
      "step": 448200
    },
    {
      "epoch": 35.116716277612404,
      "grad_norm": 5.706779956817627,
      "learning_rate": 2.0736069768656326e-05,
      "loss": 1.6923,
      "step": 448300
    },
    {
      "epoch": 35.12454958483472,
      "grad_norm": 4.675773620605469,
      "learning_rate": 2.072954201263774e-05,
      "loss": 1.6892,
      "step": 448400
    },
    {
      "epoch": 35.132382892057024,
      "grad_norm": 7.969119071960449,
      "learning_rate": 2.0723014256619144e-05,
      "loss": 1.673,
      "step": 448500
    },
    {
      "epoch": 35.14021619927934,
      "grad_norm": 5.332494258880615,
      "learning_rate": 2.0716486500600553e-05,
      "loss": 1.6734,
      "step": 448600
    },
    {
      "epoch": 35.14804950650164,
      "grad_norm": 5.947103977203369,
      "learning_rate": 2.0709958744581963e-05,
      "loss": 1.6005,
      "step": 448700
    },
    {
      "epoch": 35.155882813723956,
      "grad_norm": 7.804078102111816,
      "learning_rate": 2.0703430988563372e-05,
      "loss": 1.7084,
      "step": 448800
    },
    {
      "epoch": 35.16371612094626,
      "grad_norm": 5.404847145080566,
      "learning_rate": 2.069690323254478e-05,
      "loss": 1.6873,
      "step": 448900
    },
    {
      "epoch": 35.171549428168575,
      "grad_norm": 6.453312873840332,
      "learning_rate": 2.069037547652619e-05,
      "loss": 1.7056,
      "step": 449000
    },
    {
      "epoch": 35.17938273539088,
      "grad_norm": 7.58221960067749,
      "learning_rate": 2.06838477205076e-05,
      "loss": 1.7338,
      "step": 449100
    },
    {
      "epoch": 35.187216042613194,
      "grad_norm": 6.137740135192871,
      "learning_rate": 2.067731996448901e-05,
      "loss": 1.6439,
      "step": 449200
    },
    {
      "epoch": 35.1950493498355,
      "grad_norm": 5.7720770835876465,
      "learning_rate": 2.0670792208470414e-05,
      "loss": 1.7637,
      "step": 449300
    },
    {
      "epoch": 35.20288265705781,
      "grad_norm": 4.877588272094727,
      "learning_rate": 2.0664264452451827e-05,
      "loss": 1.6465,
      "step": 449400
    },
    {
      "epoch": 35.21071596428012,
      "grad_norm": 4.9060516357421875,
      "learning_rate": 2.0657736696433236e-05,
      "loss": 1.5095,
      "step": 449500
    },
    {
      "epoch": 35.218549271502425,
      "grad_norm": 7.590464115142822,
      "learning_rate": 2.065120894041464e-05,
      "loss": 1.5943,
      "step": 449600
    },
    {
      "epoch": 35.22638257872474,
      "grad_norm": 5.95566463470459,
      "learning_rate": 2.0644681184396054e-05,
      "loss": 1.6638,
      "step": 449700
    },
    {
      "epoch": 35.234215885947044,
      "grad_norm": 7.892106056213379,
      "learning_rate": 2.0638153428377463e-05,
      "loss": 1.6436,
      "step": 449800
    },
    {
      "epoch": 35.24204919316936,
      "grad_norm": 5.799794673919678,
      "learning_rate": 2.063162567235887e-05,
      "loss": 1.6948,
      "step": 449900
    },
    {
      "epoch": 35.249882500391664,
      "grad_norm": 5.319974899291992,
      "learning_rate": 2.062509791634028e-05,
      "loss": 1.7116,
      "step": 450000
    },
    {
      "epoch": 35.25771580761398,
      "grad_norm": 7.430633068084717,
      "learning_rate": 2.0618570160321687e-05,
      "loss": 1.6022,
      "step": 450100
    },
    {
      "epoch": 35.26554911483628,
      "grad_norm": 5.918078422546387,
      "learning_rate": 2.0612042404303097e-05,
      "loss": 1.6853,
      "step": 450200
    },
    {
      "epoch": 35.273382422058596,
      "grad_norm": 5.383022308349609,
      "learning_rate": 2.0605514648284506e-05,
      "loss": 1.7057,
      "step": 450300
    },
    {
      "epoch": 35.2812157292809,
      "grad_norm": 6.177216053009033,
      "learning_rate": 2.0598986892265915e-05,
      "loss": 1.5984,
      "step": 450400
    },
    {
      "epoch": 35.289049036503215,
      "grad_norm": 7.499117374420166,
      "learning_rate": 2.0592459136247324e-05,
      "loss": 1.7202,
      "step": 450500
    },
    {
      "epoch": 35.29688234372552,
      "grad_norm": 7.219175338745117,
      "learning_rate": 2.0585931380228733e-05,
      "loss": 1.6834,
      "step": 450600
    },
    {
      "epoch": 35.30471565094783,
      "grad_norm": 6.5000200271606445,
      "learning_rate": 2.0579403624210142e-05,
      "loss": 1.6489,
      "step": 450700
    },
    {
      "epoch": 35.31254895817014,
      "grad_norm": 7.149421691894531,
      "learning_rate": 2.057287586819155e-05,
      "loss": 1.6879,
      "step": 450800
    },
    {
      "epoch": 35.320382265392446,
      "grad_norm": 8.705491065979004,
      "learning_rate": 2.056634811217296e-05,
      "loss": 1.7227,
      "step": 450900
    },
    {
      "epoch": 35.32821557261476,
      "grad_norm": 7.512808322906494,
      "learning_rate": 2.055982035615437e-05,
      "loss": 1.6378,
      "step": 451000
    },
    {
      "epoch": 35.336048879837065,
      "grad_norm": 5.081315040588379,
      "learning_rate": 2.055329260013578e-05,
      "loss": 1.7454,
      "step": 451100
    },
    {
      "epoch": 35.34388218705938,
      "grad_norm": 6.731387138366699,
      "learning_rate": 2.0546764844117185e-05,
      "loss": 1.6539,
      "step": 451200
    },
    {
      "epoch": 35.351715494281684,
      "grad_norm": 6.423977851867676,
      "learning_rate": 2.0540237088098597e-05,
      "loss": 1.6541,
      "step": 451300
    },
    {
      "epoch": 35.359548801504,
      "grad_norm": 5.143545150756836,
      "learning_rate": 2.0533709332080006e-05,
      "loss": 1.6012,
      "step": 451400
    },
    {
      "epoch": 35.367382108726304,
      "grad_norm": 5.868366241455078,
      "learning_rate": 2.0527181576061412e-05,
      "loss": 1.6689,
      "step": 451500
    },
    {
      "epoch": 35.37521541594862,
      "grad_norm": 5.760565280914307,
      "learning_rate": 2.0520653820042825e-05,
      "loss": 1.5986,
      "step": 451600
    },
    {
      "epoch": 35.38304872317092,
      "grad_norm": 6.699697494506836,
      "learning_rate": 2.0514126064024234e-05,
      "loss": 1.6158,
      "step": 451700
    },
    {
      "epoch": 35.39088203039323,
      "grad_norm": 5.252264022827148,
      "learning_rate": 2.050759830800564e-05,
      "loss": 1.6214,
      "step": 451800
    },
    {
      "epoch": 35.39871533761554,
      "grad_norm": 5.577713489532471,
      "learning_rate": 2.050107055198705e-05,
      "loss": 1.737,
      "step": 451900
    },
    {
      "epoch": 35.40654864483785,
      "grad_norm": 5.500284671783447,
      "learning_rate": 2.0494542795968458e-05,
      "loss": 1.6105,
      "step": 452000
    },
    {
      "epoch": 35.41438195206016,
      "grad_norm": 6.1619873046875,
      "learning_rate": 2.0488015039949867e-05,
      "loss": 1.5571,
      "step": 452100
    },
    {
      "epoch": 35.42221525928247,
      "grad_norm": 4.491711616516113,
      "learning_rate": 2.0481487283931276e-05,
      "loss": 1.6534,
      "step": 452200
    },
    {
      "epoch": 35.43004856650478,
      "grad_norm": 7.035039901733398,
      "learning_rate": 2.0474959527912686e-05,
      "loss": 1.6348,
      "step": 452300
    },
    {
      "epoch": 35.437881873727086,
      "grad_norm": 4.909665107727051,
      "learning_rate": 2.0468431771894095e-05,
      "loss": 1.6406,
      "step": 452400
    },
    {
      "epoch": 35.4457151809494,
      "grad_norm": 6.899715900421143,
      "learning_rate": 2.0461904015875504e-05,
      "loss": 1.711,
      "step": 452500
    },
    {
      "epoch": 35.453548488171705,
      "grad_norm": 5.70128059387207,
      "learning_rate": 2.0455376259856913e-05,
      "loss": 1.7608,
      "step": 452600
    },
    {
      "epoch": 35.46138179539402,
      "grad_norm": 6.467832565307617,
      "learning_rate": 2.0448848503838322e-05,
      "loss": 1.612,
      "step": 452700
    },
    {
      "epoch": 35.469215102616324,
      "grad_norm": 6.409539222717285,
      "learning_rate": 2.0442320747819728e-05,
      "loss": 1.5868,
      "step": 452800
    },
    {
      "epoch": 35.47704840983863,
      "grad_norm": 5.902675628662109,
      "learning_rate": 2.043579299180114e-05,
      "loss": 1.6945,
      "step": 452900
    },
    {
      "epoch": 35.484881717060944,
      "grad_norm": 6.624504089355469,
      "learning_rate": 2.042926523578255e-05,
      "loss": 1.7157,
      "step": 453000
    },
    {
      "epoch": 35.49271502428325,
      "grad_norm": 8.236576080322266,
      "learning_rate": 2.0422737479763955e-05,
      "loss": 1.6891,
      "step": 453100
    },
    {
      "epoch": 35.50054833150556,
      "grad_norm": 6.438527584075928,
      "learning_rate": 2.0416209723745368e-05,
      "loss": 1.7083,
      "step": 453200
    },
    {
      "epoch": 35.50838163872787,
      "grad_norm": 5.957846164703369,
      "learning_rate": 2.0409681967726777e-05,
      "loss": 1.6078,
      "step": 453300
    },
    {
      "epoch": 35.51621494595018,
      "grad_norm": 6.218883037567139,
      "learning_rate": 2.0403154211708183e-05,
      "loss": 1.7169,
      "step": 453400
    },
    {
      "epoch": 35.52404825317249,
      "grad_norm": 5.057875156402588,
      "learning_rate": 2.0396626455689592e-05,
      "loss": 1.6574,
      "step": 453500
    },
    {
      "epoch": 35.5318815603948,
      "grad_norm": 9.67941951751709,
      "learning_rate": 2.0390098699671e-05,
      "loss": 1.6578,
      "step": 453600
    },
    {
      "epoch": 35.53971486761711,
      "grad_norm": 5.728203773498535,
      "learning_rate": 2.038357094365241e-05,
      "loss": 1.7016,
      "step": 453700
    },
    {
      "epoch": 35.54754817483942,
      "grad_norm": 8.331694602966309,
      "learning_rate": 2.037704318763382e-05,
      "loss": 1.5914,
      "step": 453800
    },
    {
      "epoch": 35.555381482061726,
      "grad_norm": 6.0507402420043945,
      "learning_rate": 2.037051543161523e-05,
      "loss": 1.6929,
      "step": 453900
    },
    {
      "epoch": 35.56321478928403,
      "grad_norm": 6.35313606262207,
      "learning_rate": 2.0363987675596638e-05,
      "loss": 1.6371,
      "step": 454000
    },
    {
      "epoch": 35.571048096506345,
      "grad_norm": 6.476990222930908,
      "learning_rate": 2.0357459919578047e-05,
      "loss": 1.6724,
      "step": 454100
    },
    {
      "epoch": 35.57888140372865,
      "grad_norm": 6.495195388793945,
      "learning_rate": 2.0350932163559456e-05,
      "loss": 1.6486,
      "step": 454200
    },
    {
      "epoch": 35.586714710950965,
      "grad_norm": 6.684386253356934,
      "learning_rate": 2.0344404407540865e-05,
      "loss": 1.6286,
      "step": 454300
    },
    {
      "epoch": 35.59454801817327,
      "grad_norm": 8.959887504577637,
      "learning_rate": 2.033787665152227e-05,
      "loss": 1.6948,
      "step": 454400
    },
    {
      "epoch": 35.602381325395584,
      "grad_norm": 5.883159637451172,
      "learning_rate": 2.0331348895503684e-05,
      "loss": 1.5634,
      "step": 454500
    },
    {
      "epoch": 35.61021463261789,
      "grad_norm": 6.844120979309082,
      "learning_rate": 2.0324821139485093e-05,
      "loss": 1.5824,
      "step": 454600
    },
    {
      "epoch": 35.6180479398402,
      "grad_norm": 6.763250827789307,
      "learning_rate": 2.03182933834665e-05,
      "loss": 1.5903,
      "step": 454700
    },
    {
      "epoch": 35.62588124706251,
      "grad_norm": 9.203876495361328,
      "learning_rate": 2.031176562744791e-05,
      "loss": 1.5955,
      "step": 454800
    },
    {
      "epoch": 35.63371455428482,
      "grad_norm": 7.260658264160156,
      "learning_rate": 2.030523787142932e-05,
      "loss": 1.7728,
      "step": 454900
    },
    {
      "epoch": 35.64154786150713,
      "grad_norm": 4.658194065093994,
      "learning_rate": 2.0298710115410726e-05,
      "loss": 1.6817,
      "step": 455000
    },
    {
      "epoch": 35.649381168729434,
      "grad_norm": 6.5655412673950195,
      "learning_rate": 2.0292182359392135e-05,
      "loss": 1.6371,
      "step": 455100
    },
    {
      "epoch": 35.65721447595175,
      "grad_norm": 6.333637237548828,
      "learning_rate": 2.0285654603373544e-05,
      "loss": 1.7396,
      "step": 455200
    },
    {
      "epoch": 35.66504778317405,
      "grad_norm": 8.359379768371582,
      "learning_rate": 2.0279126847354953e-05,
      "loss": 1.5951,
      "step": 455300
    },
    {
      "epoch": 35.672881090396366,
      "grad_norm": 6.663665294647217,
      "learning_rate": 2.0272599091336363e-05,
      "loss": 1.7744,
      "step": 455400
    },
    {
      "epoch": 35.68071439761867,
      "grad_norm": 5.702882766723633,
      "learning_rate": 2.0266071335317772e-05,
      "loss": 1.6444,
      "step": 455500
    },
    {
      "epoch": 35.688547704840985,
      "grad_norm": 6.965274333953857,
      "learning_rate": 2.025954357929918e-05,
      "loss": 1.6877,
      "step": 455600
    },
    {
      "epoch": 35.69638101206329,
      "grad_norm": 5.8984575271606445,
      "learning_rate": 2.025301582328059e-05,
      "loss": 1.6681,
      "step": 455700
    },
    {
      "epoch": 35.704214319285605,
      "grad_norm": 9.358139038085938,
      "learning_rate": 2.0246488067262e-05,
      "loss": 1.7191,
      "step": 455800
    },
    {
      "epoch": 35.71204762650791,
      "grad_norm": 7.643101215362549,
      "learning_rate": 2.023996031124341e-05,
      "loss": 1.7481,
      "step": 455900
    },
    {
      "epoch": 35.719880933730224,
      "grad_norm": 5.669153690338135,
      "learning_rate": 2.0233432555224814e-05,
      "loss": 1.6343,
      "step": 456000
    },
    {
      "epoch": 35.72771424095253,
      "grad_norm": 6.247568607330322,
      "learning_rate": 2.0226904799206227e-05,
      "loss": 1.7351,
      "step": 456100
    },
    {
      "epoch": 35.73554754817484,
      "grad_norm": 7.219926357269287,
      "learning_rate": 2.0220377043187636e-05,
      "loss": 1.7434,
      "step": 456200
    },
    {
      "epoch": 35.74338085539715,
      "grad_norm": 11.117695808410645,
      "learning_rate": 2.021384928716904e-05,
      "loss": 1.6914,
      "step": 456300
    },
    {
      "epoch": 35.751214162619455,
      "grad_norm": 6.112598419189453,
      "learning_rate": 2.0207321531150454e-05,
      "loss": 1.6206,
      "step": 456400
    },
    {
      "epoch": 35.75904746984177,
      "grad_norm": 5.750875949859619,
      "learning_rate": 2.0200793775131863e-05,
      "loss": 1.6821,
      "step": 456500
    },
    {
      "epoch": 35.766880777064074,
      "grad_norm": 6.320060729980469,
      "learning_rate": 2.019426601911327e-05,
      "loss": 1.73,
      "step": 456600
    },
    {
      "epoch": 35.77471408428639,
      "grad_norm": 6.982560157775879,
      "learning_rate": 2.0187738263094678e-05,
      "loss": 1.6088,
      "step": 456700
    },
    {
      "epoch": 35.78254739150869,
      "grad_norm": 6.16977596282959,
      "learning_rate": 2.018121050707609e-05,
      "loss": 1.6757,
      "step": 456800
    },
    {
      "epoch": 35.790380698731006,
      "grad_norm": 6.436767101287842,
      "learning_rate": 2.0174682751057497e-05,
      "loss": 1.6527,
      "step": 456900
    },
    {
      "epoch": 35.79821400595331,
      "grad_norm": 4.796050071716309,
      "learning_rate": 2.0168154995038906e-05,
      "loss": 1.6266,
      "step": 457000
    },
    {
      "epoch": 35.806047313175625,
      "grad_norm": 3.3078360557556152,
      "learning_rate": 2.0161627239020315e-05,
      "loss": 1.5986,
      "step": 457100
    },
    {
      "epoch": 35.81388062039793,
      "grad_norm": 6.454399585723877,
      "learning_rate": 2.0155099483001724e-05,
      "loss": 1.6933,
      "step": 457200
    },
    {
      "epoch": 35.821713927620245,
      "grad_norm": 7.5832672119140625,
      "learning_rate": 2.0148571726983133e-05,
      "loss": 1.5896,
      "step": 457300
    },
    {
      "epoch": 35.82954723484255,
      "grad_norm": 5.940907001495361,
      "learning_rate": 2.0142043970964542e-05,
      "loss": 1.6879,
      "step": 457400
    },
    {
      "epoch": 35.83738054206486,
      "grad_norm": 8.72518539428711,
      "learning_rate": 2.013551621494595e-05,
      "loss": 1.7124,
      "step": 457500
    },
    {
      "epoch": 35.84521384928717,
      "grad_norm": 5.203157424926758,
      "learning_rate": 2.012898845892736e-05,
      "loss": 1.6158,
      "step": 457600
    },
    {
      "epoch": 35.853047156509476,
      "grad_norm": 7.4506940841674805,
      "learning_rate": 2.012246070290877e-05,
      "loss": 1.68,
      "step": 457700
    },
    {
      "epoch": 35.86088046373179,
      "grad_norm": 5.898285865783691,
      "learning_rate": 2.011593294689018e-05,
      "loss": 1.7431,
      "step": 457800
    },
    {
      "epoch": 35.868713770954095,
      "grad_norm": 4.923792839050293,
      "learning_rate": 2.0109405190871585e-05,
      "loss": 1.7556,
      "step": 457900
    },
    {
      "epoch": 35.87654707817641,
      "grad_norm": 8.560501098632812,
      "learning_rate": 2.0102877434852997e-05,
      "loss": 1.6544,
      "step": 458000
    },
    {
      "epoch": 35.884380385398714,
      "grad_norm": 5.152389049530029,
      "learning_rate": 2.0096349678834406e-05,
      "loss": 1.7342,
      "step": 458100
    },
    {
      "epoch": 35.89221369262103,
      "grad_norm": 12.982802391052246,
      "learning_rate": 2.0089821922815812e-05,
      "loss": 1.691,
      "step": 458200
    },
    {
      "epoch": 35.90004699984333,
      "grad_norm": 2.7246432304382324,
      "learning_rate": 2.008329416679722e-05,
      "loss": 1.6781,
      "step": 458300
    },
    {
      "epoch": 35.907880307065646,
      "grad_norm": 7.108757019042969,
      "learning_rate": 2.0076766410778634e-05,
      "loss": 1.7469,
      "step": 458400
    },
    {
      "epoch": 35.91571361428795,
      "grad_norm": 5.075719356536865,
      "learning_rate": 2.007023865476004e-05,
      "loss": 1.6062,
      "step": 458500
    },
    {
      "epoch": 35.92354692151026,
      "grad_norm": 6.346677780151367,
      "learning_rate": 2.006371089874145e-05,
      "loss": 1.7152,
      "step": 458600
    },
    {
      "epoch": 35.93138022873257,
      "grad_norm": 9.403286933898926,
      "learning_rate": 2.0057183142722858e-05,
      "loss": 1.6911,
      "step": 458700
    },
    {
      "epoch": 35.93921353595488,
      "grad_norm": 6.775664806365967,
      "learning_rate": 2.0050655386704267e-05,
      "loss": 1.6918,
      "step": 458800
    },
    {
      "epoch": 35.94704684317719,
      "grad_norm": 4.587355613708496,
      "learning_rate": 2.0044127630685676e-05,
      "loss": 1.6169,
      "step": 458900
    },
    {
      "epoch": 35.9548801503995,
      "grad_norm": 5.742031574249268,
      "learning_rate": 2.0037599874667085e-05,
      "loss": 1.6722,
      "step": 459000
    },
    {
      "epoch": 35.96271345762181,
      "grad_norm": 4.8115315437316895,
      "learning_rate": 2.0031072118648495e-05,
      "loss": 1.6585,
      "step": 459100
    },
    {
      "epoch": 35.970546764844116,
      "grad_norm": 7.98599100112915,
      "learning_rate": 2.0024544362629904e-05,
      "loss": 1.6359,
      "step": 459200
    },
    {
      "epoch": 35.97838007206643,
      "grad_norm": 6.924598693847656,
      "learning_rate": 2.0018016606611313e-05,
      "loss": 1.7185,
      "step": 459300
    },
    {
      "epoch": 35.986213379288735,
      "grad_norm": 5.445963382720947,
      "learning_rate": 2.0011488850592722e-05,
      "loss": 1.6173,
      "step": 459400
    },
    {
      "epoch": 35.99404668651105,
      "grad_norm": 6.90252161026001,
      "learning_rate": 2.0004961094574128e-05,
      "loss": 1.702,
      "step": 459500
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.7763932943344116,
      "eval_runtime": 1.584,
      "eval_samples_per_second": 424.239,
      "eval_steps_per_second": 424.239,
      "step": 459576
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.4188657999038696,
      "eval_runtime": 29.4259,
      "eval_samples_per_second": 433.836,
      "eval_steps_per_second": 433.836,
      "step": 459576
    },
    {
      "epoch": 36.001879993733354,
      "grad_norm": 6.416742324829102,
      "learning_rate": 1.999843333855554e-05,
      "loss": 1.6099,
      "step": 459600
    },
    {
      "epoch": 36.00971330095566,
      "grad_norm": 2.7663354873657227,
      "learning_rate": 1.999190558253695e-05,
      "loss": 1.6571,
      "step": 459700
    },
    {
      "epoch": 36.01754660817797,
      "grad_norm": 6.688314914703369,
      "learning_rate": 1.9985377826518355e-05,
      "loss": 1.5886,
      "step": 459800
    },
    {
      "epoch": 36.02537991540028,
      "grad_norm": 5.975431442260742,
      "learning_rate": 1.9978850070499764e-05,
      "loss": 1.6231,
      "step": 459900
    },
    {
      "epoch": 36.03321322262259,
      "grad_norm": 6.400528907775879,
      "learning_rate": 1.9972322314481177e-05,
      "loss": 1.7052,
      "step": 460000
    },
    {
      "epoch": 36.0410465298449,
      "grad_norm": 6.945518493652344,
      "learning_rate": 1.9965794558462583e-05,
      "loss": 1.6544,
      "step": 460100
    },
    {
      "epoch": 36.04887983706721,
      "grad_norm": 8.456489562988281,
      "learning_rate": 1.9959266802443992e-05,
      "loss": 1.5985,
      "step": 460200
    },
    {
      "epoch": 36.05671314428952,
      "grad_norm": 6.891598224639893,
      "learning_rate": 1.99527390464254e-05,
      "loss": 1.6471,
      "step": 460300
    },
    {
      "epoch": 36.06454645151183,
      "grad_norm": 6.383113861083984,
      "learning_rate": 1.994621129040681e-05,
      "loss": 1.7142,
      "step": 460400
    },
    {
      "epoch": 36.07237975873414,
      "grad_norm": 7.396158218383789,
      "learning_rate": 1.993968353438822e-05,
      "loss": 1.597,
      "step": 460500
    },
    {
      "epoch": 36.08021306595645,
      "grad_norm": 6.019947052001953,
      "learning_rate": 1.993315577836963e-05,
      "loss": 1.6438,
      "step": 460600
    },
    {
      "epoch": 36.088046373178756,
      "grad_norm": 4.910099983215332,
      "learning_rate": 1.9926628022351038e-05,
      "loss": 1.6871,
      "step": 460700
    },
    {
      "epoch": 36.09587968040107,
      "grad_norm": 7.709684371948242,
      "learning_rate": 1.9920100266332447e-05,
      "loss": 1.5872,
      "step": 460800
    },
    {
      "epoch": 36.103712987623375,
      "grad_norm": 8.569326400756836,
      "learning_rate": 1.9913572510313856e-05,
      "loss": 1.6842,
      "step": 460900
    },
    {
      "epoch": 36.11154629484568,
      "grad_norm": 5.77240514755249,
      "learning_rate": 1.9907044754295265e-05,
      "loss": 1.6819,
      "step": 461000
    },
    {
      "epoch": 36.119379602067994,
      "grad_norm": 4.8621392250061035,
      "learning_rate": 1.990051699827667e-05,
      "loss": 1.664,
      "step": 461100
    },
    {
      "epoch": 36.1272129092903,
      "grad_norm": 4.865151882171631,
      "learning_rate": 1.9893989242258083e-05,
      "loss": 1.6655,
      "step": 461200
    },
    {
      "epoch": 36.13504621651261,
      "grad_norm": 6.814988136291504,
      "learning_rate": 1.9887461486239493e-05,
      "loss": 1.633,
      "step": 461300
    },
    {
      "epoch": 36.14287952373492,
      "grad_norm": 5.807636737823486,
      "learning_rate": 1.98809337302209e-05,
      "loss": 1.6187,
      "step": 461400
    },
    {
      "epoch": 36.15071283095723,
      "grad_norm": 7.021735668182373,
      "learning_rate": 1.9874405974202308e-05,
      "loss": 1.6637,
      "step": 461500
    },
    {
      "epoch": 36.15854613817954,
      "grad_norm": 6.8238725662231445,
      "learning_rate": 1.986787821818372e-05,
      "loss": 1.6783,
      "step": 461600
    },
    {
      "epoch": 36.16637944540185,
      "grad_norm": 5.406258583068848,
      "learning_rate": 1.9861350462165126e-05,
      "loss": 1.742,
      "step": 461700
    },
    {
      "epoch": 36.17421275262416,
      "grad_norm": 6.563796043395996,
      "learning_rate": 1.9854822706146535e-05,
      "loss": 1.6758,
      "step": 461800
    },
    {
      "epoch": 36.18204605984647,
      "grad_norm": 6.791496753692627,
      "learning_rate": 1.9848294950127944e-05,
      "loss": 1.6861,
      "step": 461900
    },
    {
      "epoch": 36.18987936706878,
      "grad_norm": 7.831600189208984,
      "learning_rate": 1.9841767194109353e-05,
      "loss": 1.7285,
      "step": 462000
    },
    {
      "epoch": 36.19771267429108,
      "grad_norm": 5.711757183074951,
      "learning_rate": 1.9835239438090763e-05,
      "loss": 1.6083,
      "step": 462100
    },
    {
      "epoch": 36.205545981513396,
      "grad_norm": 6.139946460723877,
      "learning_rate": 1.982871168207217e-05,
      "loss": 1.6298,
      "step": 462200
    },
    {
      "epoch": 36.2133792887357,
      "grad_norm": 8.637234687805176,
      "learning_rate": 1.982218392605358e-05,
      "loss": 1.651,
      "step": 462300
    },
    {
      "epoch": 36.221212595958015,
      "grad_norm": 6.087234020233154,
      "learning_rate": 1.981565617003499e-05,
      "loss": 1.6314,
      "step": 462400
    },
    {
      "epoch": 36.22904590318032,
      "grad_norm": 6.654080390930176,
      "learning_rate": 1.98091284140164e-05,
      "loss": 1.6406,
      "step": 462500
    },
    {
      "epoch": 36.236879210402634,
      "grad_norm": 5.32564640045166,
      "learning_rate": 1.9802600657997808e-05,
      "loss": 1.6848,
      "step": 462600
    },
    {
      "epoch": 36.24471251762494,
      "grad_norm": 4.8234992027282715,
      "learning_rate": 1.9796072901979217e-05,
      "loss": 1.6054,
      "step": 462700
    },
    {
      "epoch": 36.25254582484725,
      "grad_norm": 5.822021007537842,
      "learning_rate": 1.9789545145960627e-05,
      "loss": 1.6653,
      "step": 462800
    },
    {
      "epoch": 36.26037913206956,
      "grad_norm": 5.847538948059082,
      "learning_rate": 1.9783017389942036e-05,
      "loss": 1.6594,
      "step": 462900
    },
    {
      "epoch": 36.26821243929187,
      "grad_norm": 6.231684684753418,
      "learning_rate": 1.977648963392344e-05,
      "loss": 1.7081,
      "step": 463000
    },
    {
      "epoch": 36.27604574651418,
      "grad_norm": 6.614078521728516,
      "learning_rate": 1.976996187790485e-05,
      "loss": 1.7041,
      "step": 463100
    },
    {
      "epoch": 36.283879053736484,
      "grad_norm": 5.340005397796631,
      "learning_rate": 1.9763434121886263e-05,
      "loss": 1.6204,
      "step": 463200
    },
    {
      "epoch": 36.2917123609588,
      "grad_norm": 4.9041314125061035,
      "learning_rate": 1.975690636586767e-05,
      "loss": 1.7211,
      "step": 463300
    },
    {
      "epoch": 36.2995456681811,
      "grad_norm": 6.945085525512695,
      "learning_rate": 1.9750378609849078e-05,
      "loss": 1.6567,
      "step": 463400
    },
    {
      "epoch": 36.30737897540342,
      "grad_norm": 5.382814407348633,
      "learning_rate": 1.974385085383049e-05,
      "loss": 1.7457,
      "step": 463500
    },
    {
      "epoch": 36.31521228262572,
      "grad_norm": 7.091043949127197,
      "learning_rate": 1.9737323097811896e-05,
      "loss": 1.6262,
      "step": 463600
    },
    {
      "epoch": 36.323045589848036,
      "grad_norm": 6.919525146484375,
      "learning_rate": 1.9730795341793306e-05,
      "loss": 1.6892,
      "step": 463700
    },
    {
      "epoch": 36.33087889707034,
      "grad_norm": 6.798994064331055,
      "learning_rate": 1.9724267585774715e-05,
      "loss": 1.6252,
      "step": 463800
    },
    {
      "epoch": 36.338712204292655,
      "grad_norm": 7.84628438949585,
      "learning_rate": 1.9717739829756124e-05,
      "loss": 1.6874,
      "step": 463900
    },
    {
      "epoch": 36.34654551151496,
      "grad_norm": 6.6514434814453125,
      "learning_rate": 1.9711212073737533e-05,
      "loss": 1.6997,
      "step": 464000
    },
    {
      "epoch": 36.354378818737274,
      "grad_norm": 6.6315717697143555,
      "learning_rate": 1.9704684317718942e-05,
      "loss": 1.7685,
      "step": 464100
    },
    {
      "epoch": 36.36221212595958,
      "grad_norm": 5.612180709838867,
      "learning_rate": 1.969815656170035e-05,
      "loss": 1.7581,
      "step": 464200
    },
    {
      "epoch": 36.370045433181886,
      "grad_norm": 5.659090042114258,
      "learning_rate": 1.969162880568176e-05,
      "loss": 1.6844,
      "step": 464300
    },
    {
      "epoch": 36.3778787404042,
      "grad_norm": 6.459591388702393,
      "learning_rate": 1.968510104966317e-05,
      "loss": 1.6973,
      "step": 464400
    },
    {
      "epoch": 36.385712047626505,
      "grad_norm": 7.518200874328613,
      "learning_rate": 1.967857329364458e-05,
      "loss": 1.6938,
      "step": 464500
    },
    {
      "epoch": 36.39354535484882,
      "grad_norm": 5.887615203857422,
      "learning_rate": 1.9672045537625985e-05,
      "loss": 1.6683,
      "step": 464600
    },
    {
      "epoch": 36.401378662071124,
      "grad_norm": 6.792207717895508,
      "learning_rate": 1.9665517781607394e-05,
      "loss": 1.7131,
      "step": 464700
    },
    {
      "epoch": 36.40921196929344,
      "grad_norm": 4.7157673835754395,
      "learning_rate": 1.9658990025588806e-05,
      "loss": 1.6515,
      "step": 464800
    },
    {
      "epoch": 36.41704527651574,
      "grad_norm": 6.868871688842773,
      "learning_rate": 1.9652462269570212e-05,
      "loss": 1.6815,
      "step": 464900
    },
    {
      "epoch": 36.42487858373806,
      "grad_norm": 6.1699652671813965,
      "learning_rate": 1.964593451355162e-05,
      "loss": 1.6531,
      "step": 465000
    },
    {
      "epoch": 36.43271189096036,
      "grad_norm": 7.491590976715088,
      "learning_rate": 1.9639406757533034e-05,
      "loss": 1.6274,
      "step": 465100
    },
    {
      "epoch": 36.440545198182676,
      "grad_norm": 5.972707748413086,
      "learning_rate": 1.963287900151444e-05,
      "loss": 1.6127,
      "step": 465200
    },
    {
      "epoch": 36.44837850540498,
      "grad_norm": 5.962981700897217,
      "learning_rate": 1.962635124549585e-05,
      "loss": 1.6455,
      "step": 465300
    },
    {
      "epoch": 36.45621181262729,
      "grad_norm": 5.166921615600586,
      "learning_rate": 1.9619823489477258e-05,
      "loss": 1.6709,
      "step": 465400
    },
    {
      "epoch": 36.4640451198496,
      "grad_norm": 9.040482521057129,
      "learning_rate": 1.9613295733458667e-05,
      "loss": 1.6196,
      "step": 465500
    },
    {
      "epoch": 36.47187842707191,
      "grad_norm": 6.701688289642334,
      "learning_rate": 1.9606767977440076e-05,
      "loss": 1.7249,
      "step": 465600
    },
    {
      "epoch": 36.47971173429422,
      "grad_norm": 7.331379413604736,
      "learning_rate": 1.9600240221421485e-05,
      "loss": 1.5974,
      "step": 465700
    },
    {
      "epoch": 36.487545041516526,
      "grad_norm": 7.079747200012207,
      "learning_rate": 1.9593712465402895e-05,
      "loss": 1.6816,
      "step": 465800
    },
    {
      "epoch": 36.49537834873884,
      "grad_norm": 4.873974800109863,
      "learning_rate": 1.9587184709384304e-05,
      "loss": 1.7627,
      "step": 465900
    },
    {
      "epoch": 36.503211655961145,
      "grad_norm": 6.565652847290039,
      "learning_rate": 1.9580656953365713e-05,
      "loss": 1.6957,
      "step": 466000
    },
    {
      "epoch": 36.51104496318346,
      "grad_norm": 5.712224006652832,
      "learning_rate": 1.9574129197347122e-05,
      "loss": 1.6379,
      "step": 466100
    },
    {
      "epoch": 36.518878270405764,
      "grad_norm": 6.40791130065918,
      "learning_rate": 1.9567601441328528e-05,
      "loss": 1.6154,
      "step": 466200
    },
    {
      "epoch": 36.52671157762808,
      "grad_norm": 6.9255757331848145,
      "learning_rate": 1.9561073685309937e-05,
      "loss": 1.7441,
      "step": 466300
    },
    {
      "epoch": 36.53454488485038,
      "grad_norm": 8.032848358154297,
      "learning_rate": 1.955454592929135e-05,
      "loss": 1.6831,
      "step": 466400
    },
    {
      "epoch": 36.54237819207269,
      "grad_norm": 6.2270402908325195,
      "learning_rate": 1.9548018173272755e-05,
      "loss": 1.6699,
      "step": 466500
    },
    {
      "epoch": 36.550211499295,
      "grad_norm": 8.459860801696777,
      "learning_rate": 1.9541490417254164e-05,
      "loss": 1.6037,
      "step": 466600
    },
    {
      "epoch": 36.55804480651731,
      "grad_norm": 5.896195411682129,
      "learning_rate": 1.9534962661235577e-05,
      "loss": 1.6041,
      "step": 466700
    },
    {
      "epoch": 36.56587811373962,
      "grad_norm": 6.978347301483154,
      "learning_rate": 1.9528434905216983e-05,
      "loss": 1.6597,
      "step": 466800
    },
    {
      "epoch": 36.57371142096193,
      "grad_norm": 5.038865089416504,
      "learning_rate": 1.9521907149198392e-05,
      "loss": 1.5513,
      "step": 466900
    },
    {
      "epoch": 36.58154472818424,
      "grad_norm": 7.240941047668457,
      "learning_rate": 1.95153793931798e-05,
      "loss": 1.6088,
      "step": 467000
    },
    {
      "epoch": 36.58937803540655,
      "grad_norm": 10.842485427856445,
      "learning_rate": 1.950885163716121e-05,
      "loss": 1.6097,
      "step": 467100
    },
    {
      "epoch": 36.59721134262886,
      "grad_norm": 5.620626926422119,
      "learning_rate": 1.950232388114262e-05,
      "loss": 1.687,
      "step": 467200
    },
    {
      "epoch": 36.605044649851166,
      "grad_norm": 5.960925579071045,
      "learning_rate": 1.949579612512403e-05,
      "loss": 1.6728,
      "step": 467300
    },
    {
      "epoch": 36.61287795707348,
      "grad_norm": 7.01161527633667,
      "learning_rate": 1.9489268369105438e-05,
      "loss": 1.6746,
      "step": 467400
    },
    {
      "epoch": 36.620711264295785,
      "grad_norm": 6.582667350769043,
      "learning_rate": 1.9482740613086847e-05,
      "loss": 1.6223,
      "step": 467500
    },
    {
      "epoch": 36.6285445715181,
      "grad_norm": 6.2885613441467285,
      "learning_rate": 1.9476212857068256e-05,
      "loss": 1.6903,
      "step": 467600
    },
    {
      "epoch": 36.636377878740404,
      "grad_norm": 5.777473449707031,
      "learning_rate": 1.9469685101049665e-05,
      "loss": 1.754,
      "step": 467700
    },
    {
      "epoch": 36.64421118596271,
      "grad_norm": 8.098363876342773,
      "learning_rate": 1.946315734503107e-05,
      "loss": 1.6068,
      "step": 467800
    },
    {
      "epoch": 36.652044493185024,
      "grad_norm": 7.353821277618408,
      "learning_rate": 1.945662958901248e-05,
      "loss": 1.7226,
      "step": 467900
    },
    {
      "epoch": 36.65987780040733,
      "grad_norm": 5.566543102264404,
      "learning_rate": 1.9450101832993893e-05,
      "loss": 1.6782,
      "step": 468000
    },
    {
      "epoch": 36.66771110762964,
      "grad_norm": 7.682069778442383,
      "learning_rate": 1.94435740769753e-05,
      "loss": 1.6185,
      "step": 468100
    },
    {
      "epoch": 36.67554441485195,
      "grad_norm": 6.780455589294434,
      "learning_rate": 1.9437046320956708e-05,
      "loss": 1.7201,
      "step": 468200
    },
    {
      "epoch": 36.68337772207426,
      "grad_norm": 7.641737937927246,
      "learning_rate": 1.943051856493812e-05,
      "loss": 1.7262,
      "step": 468300
    },
    {
      "epoch": 36.69121102929657,
      "grad_norm": 6.175944805145264,
      "learning_rate": 1.9423990808919526e-05,
      "loss": 1.5959,
      "step": 468400
    },
    {
      "epoch": 36.69904433651888,
      "grad_norm": 6.894794940948486,
      "learning_rate": 1.9417463052900935e-05,
      "loss": 1.7201,
      "step": 468500
    },
    {
      "epoch": 36.70687764374119,
      "grad_norm": 6.157379627227783,
      "learning_rate": 1.9410935296882348e-05,
      "loss": 1.6293,
      "step": 468600
    },
    {
      "epoch": 36.7147109509635,
      "grad_norm": 5.881774425506592,
      "learning_rate": 1.9404407540863753e-05,
      "loss": 1.7254,
      "step": 468700
    },
    {
      "epoch": 36.722544258185806,
      "grad_norm": 6.163973808288574,
      "learning_rate": 1.9397879784845162e-05,
      "loss": 1.6586,
      "step": 468800
    },
    {
      "epoch": 36.73037756540811,
      "grad_norm": 4.0455403327941895,
      "learning_rate": 1.939135202882657e-05,
      "loss": 1.7901,
      "step": 468900
    },
    {
      "epoch": 36.738210872630425,
      "grad_norm": 5.421700954437256,
      "learning_rate": 1.938482427280798e-05,
      "loss": 1.6827,
      "step": 469000
    },
    {
      "epoch": 36.74604417985273,
      "grad_norm": 6.4774346351623535,
      "learning_rate": 1.937829651678939e-05,
      "loss": 1.7217,
      "step": 469100
    },
    {
      "epoch": 36.753877487075044,
      "grad_norm": 4.99378776550293,
      "learning_rate": 1.93717687607708e-05,
      "loss": 1.6368,
      "step": 469200
    },
    {
      "epoch": 36.76171079429735,
      "grad_norm": 5.375178813934326,
      "learning_rate": 1.9365241004752208e-05,
      "loss": 1.5711,
      "step": 469300
    },
    {
      "epoch": 36.769544101519664,
      "grad_norm": 5.8974738121032715,
      "learning_rate": 1.9358713248733617e-05,
      "loss": 1.6704,
      "step": 469400
    },
    {
      "epoch": 36.77737740874197,
      "grad_norm": 2.561100721359253,
      "learning_rate": 1.9352185492715023e-05,
      "loss": 1.7128,
      "step": 469500
    },
    {
      "epoch": 36.78521071596428,
      "grad_norm": 6.47822380065918,
      "learning_rate": 1.9345657736696436e-05,
      "loss": 1.65,
      "step": 469600
    },
    {
      "epoch": 36.79304402318659,
      "grad_norm": 5.975465774536133,
      "learning_rate": 1.933912998067784e-05,
      "loss": 1.6339,
      "step": 469700
    },
    {
      "epoch": 36.8008773304089,
      "grad_norm": 7.072047710418701,
      "learning_rate": 1.933260222465925e-05,
      "loss": 1.6749,
      "step": 469800
    },
    {
      "epoch": 36.80871063763121,
      "grad_norm": 5.961910247802734,
      "learning_rate": 1.9326074468640663e-05,
      "loss": 1.6189,
      "step": 469900
    },
    {
      "epoch": 36.816543944853514,
      "grad_norm": 5.566514015197754,
      "learning_rate": 1.931954671262207e-05,
      "loss": 1.628,
      "step": 470000
    },
    {
      "epoch": 36.82437725207583,
      "grad_norm": 7.756912708282471,
      "learning_rate": 1.9313018956603478e-05,
      "loss": 1.6466,
      "step": 470100
    },
    {
      "epoch": 36.83221055929813,
      "grad_norm": 5.347565650939941,
      "learning_rate": 1.930649120058489e-05,
      "loss": 1.7321,
      "step": 470200
    },
    {
      "epoch": 36.840043866520446,
      "grad_norm": 6.473099231719971,
      "learning_rate": 1.9299963444566296e-05,
      "loss": 1.7572,
      "step": 470300
    },
    {
      "epoch": 36.84787717374275,
      "grad_norm": 5.771466255187988,
      "learning_rate": 1.9293435688547706e-05,
      "loss": 1.7292,
      "step": 470400
    },
    {
      "epoch": 36.855710480965065,
      "grad_norm": 6.1760687828063965,
      "learning_rate": 1.9286907932529115e-05,
      "loss": 1.6566,
      "step": 470500
    },
    {
      "epoch": 36.86354378818737,
      "grad_norm": 5.657931327819824,
      "learning_rate": 1.9280380176510524e-05,
      "loss": 1.6271,
      "step": 470600
    },
    {
      "epoch": 36.871377095409684,
      "grad_norm": 6.863827228546143,
      "learning_rate": 1.9273852420491933e-05,
      "loss": 1.6104,
      "step": 470700
    },
    {
      "epoch": 36.87921040263199,
      "grad_norm": 6.944338321685791,
      "learning_rate": 1.9267324664473342e-05,
      "loss": 1.6922,
      "step": 470800
    },
    {
      "epoch": 36.887043709854304,
      "grad_norm": 6.635646820068359,
      "learning_rate": 1.926079690845475e-05,
      "loss": 1.6203,
      "step": 470900
    },
    {
      "epoch": 36.89487701707661,
      "grad_norm": 5.223423957824707,
      "learning_rate": 1.925426915243616e-05,
      "loss": 1.6901,
      "step": 471000
    },
    {
      "epoch": 36.902710324298916,
      "grad_norm": 2.142052412033081,
      "learning_rate": 1.9247741396417566e-05,
      "loss": 1.5995,
      "step": 471100
    },
    {
      "epoch": 36.91054363152123,
      "grad_norm": 5.931860446929932,
      "learning_rate": 1.924121364039898e-05,
      "loss": 1.6584,
      "step": 471200
    },
    {
      "epoch": 36.918376938743535,
      "grad_norm": 5.751895427703857,
      "learning_rate": 1.9234685884380385e-05,
      "loss": 1.6453,
      "step": 471300
    },
    {
      "epoch": 36.92621024596585,
      "grad_norm": 4.680863857269287,
      "learning_rate": 1.9228158128361794e-05,
      "loss": 1.6833,
      "step": 471400
    },
    {
      "epoch": 36.934043553188154,
      "grad_norm": 6.8450751304626465,
      "learning_rate": 1.9221630372343206e-05,
      "loss": 1.7105,
      "step": 471500
    },
    {
      "epoch": 36.94187686041047,
      "grad_norm": 6.367254734039307,
      "learning_rate": 1.9215102616324612e-05,
      "loss": 1.6627,
      "step": 471600
    },
    {
      "epoch": 36.94971016763277,
      "grad_norm": 6.856729030609131,
      "learning_rate": 1.920857486030602e-05,
      "loss": 1.6109,
      "step": 471700
    },
    {
      "epoch": 36.957543474855086,
      "grad_norm": 5.708682537078857,
      "learning_rate": 1.9202047104287434e-05,
      "loss": 1.6113,
      "step": 471800
    },
    {
      "epoch": 36.96537678207739,
      "grad_norm": 8.135946273803711,
      "learning_rate": 1.919551934826884e-05,
      "loss": 1.6542,
      "step": 471900
    },
    {
      "epoch": 36.973210089299705,
      "grad_norm": 6.0401177406311035,
      "learning_rate": 1.918899159225025e-05,
      "loss": 1.6152,
      "step": 472000
    },
    {
      "epoch": 36.98104339652201,
      "grad_norm": 6.347146511077881,
      "learning_rate": 1.9182463836231658e-05,
      "loss": 1.6406,
      "step": 472100
    },
    {
      "epoch": 36.988876703744324,
      "grad_norm": 5.567721843719482,
      "learning_rate": 1.9175936080213067e-05,
      "loss": 1.6963,
      "step": 472200
    },
    {
      "epoch": 36.99671001096663,
      "grad_norm": 6.798395156860352,
      "learning_rate": 1.9169408324194476e-05,
      "loss": 1.6065,
      "step": 472300
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.777458906173706,
      "eval_runtime": 1.5285,
      "eval_samples_per_second": 439.645,
      "eval_steps_per_second": 439.645,
      "step": 472342
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.4163366556167603,
      "eval_runtime": 29.3286,
      "eval_samples_per_second": 435.275,
      "eval_steps_per_second": 435.275,
      "step": 472342
    },
    {
      "epoch": 37.00454331818894,
      "grad_norm": 6.0052103996276855,
      "learning_rate": 1.9162880568175885e-05,
      "loss": 1.6622,
      "step": 472400
    },
    {
      "epoch": 37.01237662541125,
      "grad_norm": 6.043507099151611,
      "learning_rate": 1.9156352812157294e-05,
      "loss": 1.7001,
      "step": 472500
    },
    {
      "epoch": 37.020209932633556,
      "grad_norm": 6.183141231536865,
      "learning_rate": 1.9149825056138704e-05,
      "loss": 1.7491,
      "step": 472600
    },
    {
      "epoch": 37.02804323985587,
      "grad_norm": 6.209666728973389,
      "learning_rate": 1.914329730012011e-05,
      "loss": 1.6696,
      "step": 472700
    },
    {
      "epoch": 37.035876547078175,
      "grad_norm": 5.677446365356445,
      "learning_rate": 1.9136769544101522e-05,
      "loss": 1.5661,
      "step": 472800
    },
    {
      "epoch": 37.04370985430049,
      "grad_norm": 6.935235977172852,
      "learning_rate": 1.9130241788082928e-05,
      "loss": 1.6429,
      "step": 472900
    },
    {
      "epoch": 37.051543161522794,
      "grad_norm": 6.825396537780762,
      "learning_rate": 1.9123714032064337e-05,
      "loss": 1.6727,
      "step": 473000
    },
    {
      "epoch": 37.05937646874511,
      "grad_norm": 7.218846797943115,
      "learning_rate": 1.911718627604575e-05,
      "loss": 1.6954,
      "step": 473100
    },
    {
      "epoch": 37.06720977596741,
      "grad_norm": 7.50275182723999,
      "learning_rate": 1.9110658520027155e-05,
      "loss": 1.625,
      "step": 473200
    },
    {
      "epoch": 37.075043083189726,
      "grad_norm": 6.565479278564453,
      "learning_rate": 1.9104130764008564e-05,
      "loss": 1.6167,
      "step": 473300
    },
    {
      "epoch": 37.08287639041203,
      "grad_norm": 5.915707111358643,
      "learning_rate": 1.9097603007989977e-05,
      "loss": 1.6998,
      "step": 473400
    },
    {
      "epoch": 37.09070969763434,
      "grad_norm": 5.238318920135498,
      "learning_rate": 1.9091075251971383e-05,
      "loss": 1.6309,
      "step": 473500
    },
    {
      "epoch": 37.09854300485665,
      "grad_norm": 6.557913780212402,
      "learning_rate": 1.9084547495952792e-05,
      "loss": 1.6233,
      "step": 473600
    },
    {
      "epoch": 37.10637631207896,
      "grad_norm": 5.82119607925415,
      "learning_rate": 1.90780197399342e-05,
      "loss": 1.5785,
      "step": 473700
    },
    {
      "epoch": 37.11420961930127,
      "grad_norm": 4.918447494506836,
      "learning_rate": 1.907149198391561e-05,
      "loss": 1.6447,
      "step": 473800
    },
    {
      "epoch": 37.12204292652358,
      "grad_norm": 6.6815667152404785,
      "learning_rate": 1.906496422789702e-05,
      "loss": 1.7217,
      "step": 473900
    },
    {
      "epoch": 37.12987623374589,
      "grad_norm": 6.91478967666626,
      "learning_rate": 1.905843647187843e-05,
      "loss": 1.6376,
      "step": 474000
    },
    {
      "epoch": 37.137709540968196,
      "grad_norm": 6.148957252502441,
      "learning_rate": 1.9051908715859838e-05,
      "loss": 1.7665,
      "step": 474100
    },
    {
      "epoch": 37.14554284819051,
      "grad_norm": 6.927971363067627,
      "learning_rate": 1.9045380959841247e-05,
      "loss": 1.608,
      "step": 474200
    },
    {
      "epoch": 37.153376155412815,
      "grad_norm": 5.467336177825928,
      "learning_rate": 1.9038853203822652e-05,
      "loss": 1.6424,
      "step": 474300
    },
    {
      "epoch": 37.16120946263513,
      "grad_norm": 6.305957317352295,
      "learning_rate": 1.9032325447804065e-05,
      "loss": 1.6616,
      "step": 474400
    },
    {
      "epoch": 37.169042769857434,
      "grad_norm": 5.725352764129639,
      "learning_rate": 1.9025797691785474e-05,
      "loss": 1.5841,
      "step": 474500
    },
    {
      "epoch": 37.17687607707974,
      "grad_norm": 8.03720474243164,
      "learning_rate": 1.901926993576688e-05,
      "loss": 1.618,
      "step": 474600
    },
    {
      "epoch": 37.18470938430205,
      "grad_norm": 5.95594596862793,
      "learning_rate": 1.9012742179748293e-05,
      "loss": 1.648,
      "step": 474700
    },
    {
      "epoch": 37.19254269152436,
      "grad_norm": 7.414519786834717,
      "learning_rate": 1.9006214423729698e-05,
      "loss": 1.6491,
      "step": 474800
    },
    {
      "epoch": 37.20037599874667,
      "grad_norm": 6.569479465484619,
      "learning_rate": 1.8999686667711107e-05,
      "loss": 1.6244,
      "step": 474900
    },
    {
      "epoch": 37.20820930596898,
      "grad_norm": 8.129137992858887,
      "learning_rate": 1.899315891169252e-05,
      "loss": 1.6914,
      "step": 475000
    },
    {
      "epoch": 37.21604261319129,
      "grad_norm": 7.243891716003418,
      "learning_rate": 1.8986631155673926e-05,
      "loss": 1.5773,
      "step": 475100
    },
    {
      "epoch": 37.2238759204136,
      "grad_norm": 6.132123947143555,
      "learning_rate": 1.8980103399655335e-05,
      "loss": 1.7006,
      "step": 475200
    },
    {
      "epoch": 37.23170922763591,
      "grad_norm": 3.824321985244751,
      "learning_rate": 1.8973575643636744e-05,
      "loss": 1.7255,
      "step": 475300
    },
    {
      "epoch": 37.23954253485822,
      "grad_norm": 6.21067476272583,
      "learning_rate": 1.8967047887618153e-05,
      "loss": 1.6119,
      "step": 475400
    },
    {
      "epoch": 37.24737584208053,
      "grad_norm": 5.548550605773926,
      "learning_rate": 1.8960520131599562e-05,
      "loss": 1.6383,
      "step": 475500
    },
    {
      "epoch": 37.255209149302836,
      "grad_norm": 5.574978828430176,
      "learning_rate": 1.895399237558097e-05,
      "loss": 1.6272,
      "step": 475600
    },
    {
      "epoch": 37.26304245652514,
      "grad_norm": 8.268813133239746,
      "learning_rate": 1.894746461956238e-05,
      "loss": 1.7336,
      "step": 475700
    },
    {
      "epoch": 37.270875763747455,
      "grad_norm": 4.65593147277832,
      "learning_rate": 1.894093686354379e-05,
      "loss": 1.5955,
      "step": 475800
    },
    {
      "epoch": 37.27870907096976,
      "grad_norm": 6.864517688751221,
      "learning_rate": 1.8934409107525196e-05,
      "loss": 1.7006,
      "step": 475900
    },
    {
      "epoch": 37.286542378192074,
      "grad_norm": 6.628692626953125,
      "learning_rate": 1.8927881351506608e-05,
      "loss": 1.6177,
      "step": 476000
    },
    {
      "epoch": 37.29437568541438,
      "grad_norm": 5.639157772064209,
      "learning_rate": 1.8921353595488017e-05,
      "loss": 1.6838,
      "step": 476100
    },
    {
      "epoch": 37.30220899263669,
      "grad_norm": 3.996312379837036,
      "learning_rate": 1.8914825839469423e-05,
      "loss": 1.6481,
      "step": 476200
    },
    {
      "epoch": 37.310042299859,
      "grad_norm": 5.549581050872803,
      "learning_rate": 1.8908298083450836e-05,
      "loss": 1.5862,
      "step": 476300
    },
    {
      "epoch": 37.31787560708131,
      "grad_norm": 6.7580976486206055,
      "learning_rate": 1.890177032743224e-05,
      "loss": 1.6391,
      "step": 476400
    },
    {
      "epoch": 37.32570891430362,
      "grad_norm": 8.022398948669434,
      "learning_rate": 1.889524257141365e-05,
      "loss": 1.6339,
      "step": 476500
    },
    {
      "epoch": 37.33354222152593,
      "grad_norm": 7.067420482635498,
      "learning_rate": 1.8888714815395063e-05,
      "loss": 1.737,
      "step": 476600
    },
    {
      "epoch": 37.34137552874824,
      "grad_norm": 4.677265167236328,
      "learning_rate": 1.888218705937647e-05,
      "loss": 1.7427,
      "step": 476700
    },
    {
      "epoch": 37.34920883597054,
      "grad_norm": 9.494867324829102,
      "learning_rate": 1.8875659303357878e-05,
      "loss": 1.5783,
      "step": 476800
    },
    {
      "epoch": 37.35704214319286,
      "grad_norm": 6.605324745178223,
      "learning_rate": 1.8869131547339287e-05,
      "loss": 1.7697,
      "step": 476900
    },
    {
      "epoch": 37.36487545041516,
      "grad_norm": 7.535428524017334,
      "learning_rate": 1.8862603791320696e-05,
      "loss": 1.6487,
      "step": 477000
    },
    {
      "epoch": 37.372708757637476,
      "grad_norm": 4.715029239654541,
      "learning_rate": 1.8856076035302105e-05,
      "loss": 1.6454,
      "step": 477100
    },
    {
      "epoch": 37.38054206485978,
      "grad_norm": 8.500076293945312,
      "learning_rate": 1.8849548279283515e-05,
      "loss": 1.6603,
      "step": 477200
    },
    {
      "epoch": 37.388375372082095,
      "grad_norm": 8.896571159362793,
      "learning_rate": 1.8843020523264924e-05,
      "loss": 1.6499,
      "step": 477300
    },
    {
      "epoch": 37.3962086793044,
      "grad_norm": 5.070554256439209,
      "learning_rate": 1.8836492767246333e-05,
      "loss": 1.6776,
      "step": 477400
    },
    {
      "epoch": 37.404041986526714,
      "grad_norm": 5.948328971862793,
      "learning_rate": 1.882996501122774e-05,
      "loss": 1.6642,
      "step": 477500
    },
    {
      "epoch": 37.41187529374902,
      "grad_norm": 8.53267765045166,
      "learning_rate": 1.882343725520915e-05,
      "loss": 1.5943,
      "step": 477600
    },
    {
      "epoch": 37.41970860097133,
      "grad_norm": 5.2980756759643555,
      "learning_rate": 1.881690949919056e-05,
      "loss": 1.6539,
      "step": 477700
    },
    {
      "epoch": 37.42754190819364,
      "grad_norm": 6.577241897583008,
      "learning_rate": 1.8810381743171966e-05,
      "loss": 1.6509,
      "step": 477800
    },
    {
      "epoch": 37.435375215415945,
      "grad_norm": 6.547137260437012,
      "learning_rate": 1.880385398715338e-05,
      "loss": 1.6678,
      "step": 477900
    },
    {
      "epoch": 37.44320852263826,
      "grad_norm": 7.016325950622559,
      "learning_rate": 1.8797326231134785e-05,
      "loss": 1.6253,
      "step": 478000
    },
    {
      "epoch": 37.451041829860564,
      "grad_norm": 7.034413814544678,
      "learning_rate": 1.8790798475116194e-05,
      "loss": 1.6078,
      "step": 478100
    },
    {
      "epoch": 37.45887513708288,
      "grad_norm": 5.722536087036133,
      "learning_rate": 1.8784270719097606e-05,
      "loss": 1.6226,
      "step": 478200
    },
    {
      "epoch": 37.46670844430518,
      "grad_norm": 10.810517311096191,
      "learning_rate": 1.8777742963079012e-05,
      "loss": 1.6824,
      "step": 478300
    },
    {
      "epoch": 37.4745417515275,
      "grad_norm": 5.405993461608887,
      "learning_rate": 1.877121520706042e-05,
      "loss": 1.6479,
      "step": 478400
    },
    {
      "epoch": 37.4823750587498,
      "grad_norm": 6.313968181610107,
      "learning_rate": 1.876468745104183e-05,
      "loss": 1.5854,
      "step": 478500
    },
    {
      "epoch": 37.490208365972116,
      "grad_norm": 4.6804728507995605,
      "learning_rate": 1.875815969502324e-05,
      "loss": 1.6883,
      "step": 478600
    },
    {
      "epoch": 37.49804167319442,
      "grad_norm": 5.734461307525635,
      "learning_rate": 1.875163193900465e-05,
      "loss": 1.6239,
      "step": 478700
    },
    {
      "epoch": 37.505874980416735,
      "grad_norm": 6.4493231773376465,
      "learning_rate": 1.8745104182986058e-05,
      "loss": 1.6761,
      "step": 478800
    },
    {
      "epoch": 37.51370828763904,
      "grad_norm": 4.36554479598999,
      "learning_rate": 1.8738576426967467e-05,
      "loss": 1.6946,
      "step": 478900
    },
    {
      "epoch": 37.52154159486135,
      "grad_norm": 6.608180046081543,
      "learning_rate": 1.8732048670948876e-05,
      "loss": 1.6734,
      "step": 479000
    },
    {
      "epoch": 37.52937490208366,
      "grad_norm": 8.603970527648926,
      "learning_rate": 1.8725520914930282e-05,
      "loss": 1.7115,
      "step": 479100
    },
    {
      "epoch": 37.537208209305966,
      "grad_norm": 6.817020893096924,
      "learning_rate": 1.8718993158911694e-05,
      "loss": 1.7499,
      "step": 479200
    },
    {
      "epoch": 37.54504151652828,
      "grad_norm": 5.258434295654297,
      "learning_rate": 1.8712465402893104e-05,
      "loss": 1.6138,
      "step": 479300
    },
    {
      "epoch": 37.552874823750585,
      "grad_norm": 6.4748711585998535,
      "learning_rate": 1.870593764687451e-05,
      "loss": 1.621,
      "step": 479400
    },
    {
      "epoch": 37.5607081309729,
      "grad_norm": 5.899398326873779,
      "learning_rate": 1.8699409890855922e-05,
      "loss": 1.6378,
      "step": 479500
    },
    {
      "epoch": 37.568541438195204,
      "grad_norm": 4.822193145751953,
      "learning_rate": 1.8692882134837328e-05,
      "loss": 1.6686,
      "step": 479600
    },
    {
      "epoch": 37.57637474541752,
      "grad_norm": 6.310924530029297,
      "learning_rate": 1.8686354378818737e-05,
      "loss": 1.5861,
      "step": 479700
    },
    {
      "epoch": 37.58420805263982,
      "grad_norm": 5.736908435821533,
      "learning_rate": 1.867982662280015e-05,
      "loss": 1.6865,
      "step": 479800
    },
    {
      "epoch": 37.59204135986214,
      "grad_norm": 4.94522762298584,
      "learning_rate": 1.8673298866781555e-05,
      "loss": 1.5861,
      "step": 479900
    },
    {
      "epoch": 37.59987466708444,
      "grad_norm": 11.2507963180542,
      "learning_rate": 1.8666771110762964e-05,
      "loss": 1.6682,
      "step": 480000
    },
    {
      "epoch": 37.607707974306756,
      "grad_norm": 6.305850028991699,
      "learning_rate": 1.8660243354744373e-05,
      "loss": 1.7186,
      "step": 480100
    },
    {
      "epoch": 37.61554128152906,
      "grad_norm": 6.432507038116455,
      "learning_rate": 1.8653715598725783e-05,
      "loss": 1.6399,
      "step": 480200
    },
    {
      "epoch": 37.62337458875137,
      "grad_norm": 5.662169456481934,
      "learning_rate": 1.8647187842707192e-05,
      "loss": 1.6526,
      "step": 480300
    },
    {
      "epoch": 37.63120789597368,
      "grad_norm": 6.806694507598877,
      "learning_rate": 1.86406600866886e-05,
      "loss": 1.6075,
      "step": 480400
    },
    {
      "epoch": 37.63904120319599,
      "grad_norm": 5.557224273681641,
      "learning_rate": 1.863413233067001e-05,
      "loss": 1.5539,
      "step": 480500
    },
    {
      "epoch": 37.6468745104183,
      "grad_norm": 6.787841320037842,
      "learning_rate": 1.862760457465142e-05,
      "loss": 1.7016,
      "step": 480600
    },
    {
      "epoch": 37.654707817640606,
      "grad_norm": 8.176290512084961,
      "learning_rate": 1.8621076818632825e-05,
      "loss": 1.6599,
      "step": 480700
    },
    {
      "epoch": 37.66254112486292,
      "grad_norm": 7.7969069480896,
      "learning_rate": 1.8614549062614238e-05,
      "loss": 1.6574,
      "step": 480800
    },
    {
      "epoch": 37.670374432085225,
      "grad_norm": 6.856266975402832,
      "learning_rate": 1.8608021306595647e-05,
      "loss": 1.6614,
      "step": 480900
    },
    {
      "epoch": 37.67820773930754,
      "grad_norm": 6.4532976150512695,
      "learning_rate": 1.8601493550577052e-05,
      "loss": 1.6205,
      "step": 481000
    },
    {
      "epoch": 37.686041046529844,
      "grad_norm": 7.208635330200195,
      "learning_rate": 1.8594965794558465e-05,
      "loss": 1.6603,
      "step": 481100
    },
    {
      "epoch": 37.69387435375216,
      "grad_norm": 7.100578308105469,
      "learning_rate": 1.8588438038539874e-05,
      "loss": 1.6907,
      "step": 481200
    },
    {
      "epoch": 37.70170766097446,
      "grad_norm": 5.2136430740356445,
      "learning_rate": 1.858191028252128e-05,
      "loss": 1.8056,
      "step": 481300
    },
    {
      "epoch": 37.70954096819677,
      "grad_norm": 6.282312870025635,
      "learning_rate": 1.8575382526502692e-05,
      "loss": 1.65,
      "step": 481400
    },
    {
      "epoch": 37.71737427541908,
      "grad_norm": 5.726964950561523,
      "learning_rate": 1.8568854770484098e-05,
      "loss": 1.6969,
      "step": 481500
    },
    {
      "epoch": 37.72520758264139,
      "grad_norm": 7.005117416381836,
      "learning_rate": 1.8562327014465507e-05,
      "loss": 1.7327,
      "step": 481600
    },
    {
      "epoch": 37.7330408898637,
      "grad_norm": 6.08441686630249,
      "learning_rate": 1.8555799258446917e-05,
      "loss": 1.7371,
      "step": 481700
    },
    {
      "epoch": 37.74087419708601,
      "grad_norm": 7.0398478507995605,
      "learning_rate": 1.8549271502428326e-05,
      "loss": 1.7397,
      "step": 481800
    },
    {
      "epoch": 37.74870750430832,
      "grad_norm": 5.648531913757324,
      "learning_rate": 1.8542743746409735e-05,
      "loss": 1.6252,
      "step": 481900
    },
    {
      "epoch": 37.75654081153063,
      "grad_norm": 6.336424827575684,
      "learning_rate": 1.8536215990391144e-05,
      "loss": 1.6238,
      "step": 482000
    },
    {
      "epoch": 37.76437411875294,
      "grad_norm": 5.934199810028076,
      "learning_rate": 1.8529688234372553e-05,
      "loss": 1.6789,
      "step": 482100
    },
    {
      "epoch": 37.772207425975246,
      "grad_norm": 4.564912796020508,
      "learning_rate": 1.8523160478353962e-05,
      "loss": 1.7092,
      "step": 482200
    },
    {
      "epoch": 37.78004073319756,
      "grad_norm": 6.6171770095825195,
      "learning_rate": 1.8516632722335368e-05,
      "loss": 1.7553,
      "step": 482300
    },
    {
      "epoch": 37.787874040419865,
      "grad_norm": 5.010427474975586,
      "learning_rate": 1.851010496631678e-05,
      "loss": 1.5797,
      "step": 482400
    },
    {
      "epoch": 37.79570734764217,
      "grad_norm": 5.3996381759643555,
      "learning_rate": 1.850357721029819e-05,
      "loss": 1.6802,
      "step": 482500
    },
    {
      "epoch": 37.803540654864484,
      "grad_norm": 5.238704681396484,
      "learning_rate": 1.8497049454279596e-05,
      "loss": 1.668,
      "step": 482600
    },
    {
      "epoch": 37.81137396208679,
      "grad_norm": 5.5035481452941895,
      "learning_rate": 1.8490521698261008e-05,
      "loss": 1.648,
      "step": 482700
    },
    {
      "epoch": 37.8192072693091,
      "grad_norm": 6.316007614135742,
      "learning_rate": 1.8483993942242417e-05,
      "loss": 1.6097,
      "step": 482800
    },
    {
      "epoch": 37.82704057653141,
      "grad_norm": 4.9421586990356445,
      "learning_rate": 1.8477466186223823e-05,
      "loss": 1.6806,
      "step": 482900
    },
    {
      "epoch": 37.83487388375372,
      "grad_norm": 6.945919036865234,
      "learning_rate": 1.8470938430205236e-05,
      "loss": 1.7168,
      "step": 483000
    },
    {
      "epoch": 37.84270719097603,
      "grad_norm": 5.089539527893066,
      "learning_rate": 1.846441067418664e-05,
      "loss": 1.7112,
      "step": 483100
    },
    {
      "epoch": 37.85054049819834,
      "grad_norm": 6.60103178024292,
      "learning_rate": 1.845788291816805e-05,
      "loss": 1.6645,
      "step": 483200
    },
    {
      "epoch": 37.85837380542065,
      "grad_norm": 7.552379131317139,
      "learning_rate": 1.845135516214946e-05,
      "loss": 1.6058,
      "step": 483300
    },
    {
      "epoch": 37.86620711264296,
      "grad_norm": 6.899907112121582,
      "learning_rate": 1.844482740613087e-05,
      "loss": 1.7382,
      "step": 483400
    },
    {
      "epoch": 37.87404041986527,
      "grad_norm": 6.894085884094238,
      "learning_rate": 1.8438299650112278e-05,
      "loss": 1.631,
      "step": 483500
    },
    {
      "epoch": 37.88187372708757,
      "grad_norm": 6.31610631942749,
      "learning_rate": 1.8431771894093687e-05,
      "loss": 1.6974,
      "step": 483600
    },
    {
      "epoch": 37.889707034309886,
      "grad_norm": 7.138937950134277,
      "learning_rate": 1.8425244138075096e-05,
      "loss": 1.6877,
      "step": 483700
    },
    {
      "epoch": 37.89754034153219,
      "grad_norm": 7.1443071365356445,
      "learning_rate": 1.8418716382056505e-05,
      "loss": 1.6898,
      "step": 483800
    },
    {
      "epoch": 37.905373648754505,
      "grad_norm": 5.469813823699951,
      "learning_rate": 1.841218862603791e-05,
      "loss": 1.726,
      "step": 483900
    },
    {
      "epoch": 37.91320695597681,
      "grad_norm": 6.527857303619385,
      "learning_rate": 1.8405660870019324e-05,
      "loss": 1.6131,
      "step": 484000
    },
    {
      "epoch": 37.921040263199124,
      "grad_norm": 7.3963212966918945,
      "learning_rate": 1.8399133114000733e-05,
      "loss": 1.7208,
      "step": 484100
    },
    {
      "epoch": 37.92887357042143,
      "grad_norm": 5.656981468200684,
      "learning_rate": 1.839260535798214e-05,
      "loss": 1.6015,
      "step": 484200
    },
    {
      "epoch": 37.93670687764374,
      "grad_norm": 7.893441677093506,
      "learning_rate": 1.838607760196355e-05,
      "loss": 1.7433,
      "step": 484300
    },
    {
      "epoch": 37.94454018486605,
      "grad_norm": 6.51806116104126,
      "learning_rate": 1.837954984594496e-05,
      "loss": 1.5947,
      "step": 484400
    },
    {
      "epoch": 37.95237349208836,
      "grad_norm": 5.919670104980469,
      "learning_rate": 1.8373022089926366e-05,
      "loss": 1.708,
      "step": 484500
    },
    {
      "epoch": 37.96020679931067,
      "grad_norm": 6.978476047515869,
      "learning_rate": 1.836649433390778e-05,
      "loss": 1.6542,
      "step": 484600
    },
    {
      "epoch": 37.96804010653298,
      "grad_norm": 7.051127910614014,
      "learning_rate": 1.8359966577889184e-05,
      "loss": 1.6567,
      "step": 484700
    },
    {
      "epoch": 37.97587341375529,
      "grad_norm": 8.113486289978027,
      "learning_rate": 1.8353438821870594e-05,
      "loss": 1.6519,
      "step": 484800
    },
    {
      "epoch": 37.983706720977594,
      "grad_norm": 5.761632442474365,
      "learning_rate": 1.8346911065852003e-05,
      "loss": 1.6998,
      "step": 484900
    },
    {
      "epoch": 37.99154002819991,
      "grad_norm": 6.321515083312988,
      "learning_rate": 1.8340383309833412e-05,
      "loss": 1.6959,
      "step": 485000
    },
    {
      "epoch": 37.99937333542221,
      "grad_norm": 7.0345072746276855,
      "learning_rate": 1.833385555381482e-05,
      "loss": 1.6611,
      "step": 485100
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.7708768844604492,
      "eval_runtime": 1.5503,
      "eval_samples_per_second": 433.469,
      "eval_steps_per_second": 433.469,
      "step": 485108
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.4111610651016235,
      "eval_runtime": 29.1697,
      "eval_samples_per_second": 437.646,
      "eval_steps_per_second": 437.646,
      "step": 485108
    },
    {
      "epoch": 38.007206642644526,
      "grad_norm": 5.268243789672852,
      "learning_rate": 1.832732779779623e-05,
      "loss": 1.6537,
      "step": 485200
    },
    {
      "epoch": 38.01503994986683,
      "grad_norm": 5.651756763458252,
      "learning_rate": 1.832080004177764e-05,
      "loss": 1.6068,
      "step": 485300
    },
    {
      "epoch": 38.022873257089145,
      "grad_norm": 6.756056785583496,
      "learning_rate": 1.831427228575905e-05,
      "loss": 1.6389,
      "step": 485400
    },
    {
      "epoch": 38.03070656431145,
      "grad_norm": 6.241972923278809,
      "learning_rate": 1.8307744529740454e-05,
      "loss": 1.7458,
      "step": 485500
    },
    {
      "epoch": 38.038539871533764,
      "grad_norm": 9.670695304870605,
      "learning_rate": 1.8301216773721867e-05,
      "loss": 1.5584,
      "step": 485600
    },
    {
      "epoch": 38.04637317875607,
      "grad_norm": 6.5448760986328125,
      "learning_rate": 1.8294689017703276e-05,
      "loss": 1.5759,
      "step": 485700
    },
    {
      "epoch": 38.05420648597838,
      "grad_norm": 6.064481258392334,
      "learning_rate": 1.8288161261684682e-05,
      "loss": 1.6409,
      "step": 485800
    },
    {
      "epoch": 38.06203979320069,
      "grad_norm": 4.863558292388916,
      "learning_rate": 1.8281633505666094e-05,
      "loss": 1.7013,
      "step": 485900
    },
    {
      "epoch": 38.069873100422996,
      "grad_norm": 6.633358955383301,
      "learning_rate": 1.8275105749647503e-05,
      "loss": 1.6342,
      "step": 486000
    },
    {
      "epoch": 38.07770640764531,
      "grad_norm": 7.649672031402588,
      "learning_rate": 1.826857799362891e-05,
      "loss": 1.6855,
      "step": 486100
    },
    {
      "epoch": 38.085539714867615,
      "grad_norm": 6.593759059906006,
      "learning_rate": 1.8262050237610322e-05,
      "loss": 1.7008,
      "step": 486200
    },
    {
      "epoch": 38.09337302208993,
      "grad_norm": 6.485004901885986,
      "learning_rate": 1.825552248159173e-05,
      "loss": 1.5938,
      "step": 486300
    },
    {
      "epoch": 38.101206329312234,
      "grad_norm": 6.4176740646362305,
      "learning_rate": 1.8248994725573137e-05,
      "loss": 1.6415,
      "step": 486400
    },
    {
      "epoch": 38.10903963653455,
      "grad_norm": 4.340022087097168,
      "learning_rate": 1.8242466969554546e-05,
      "loss": 1.566,
      "step": 486500
    },
    {
      "epoch": 38.11687294375685,
      "grad_norm": 5.899941444396973,
      "learning_rate": 1.8235939213535955e-05,
      "loss": 1.6954,
      "step": 486600
    },
    {
      "epoch": 38.124706250979166,
      "grad_norm": 2.7089931964874268,
      "learning_rate": 1.8229411457517364e-05,
      "loss": 1.5528,
      "step": 486700
    },
    {
      "epoch": 38.13253955820147,
      "grad_norm": 6.283118724822998,
      "learning_rate": 1.8222883701498773e-05,
      "loss": 1.7123,
      "step": 486800
    },
    {
      "epoch": 38.140372865423785,
      "grad_norm": 5.204877853393555,
      "learning_rate": 1.8216355945480183e-05,
      "loss": 1.6117,
      "step": 486900
    },
    {
      "epoch": 38.14820617264609,
      "grad_norm": 5.837136745452881,
      "learning_rate": 1.820982818946159e-05,
      "loss": 1.6126,
      "step": 487000
    },
    {
      "epoch": 38.1560394798684,
      "grad_norm": 5.931814670562744,
      "learning_rate": 1.8203300433443e-05,
      "loss": 1.6644,
      "step": 487100
    },
    {
      "epoch": 38.16387278709071,
      "grad_norm": 5.757288932800293,
      "learning_rate": 1.819677267742441e-05,
      "loss": 1.6344,
      "step": 487200
    },
    {
      "epoch": 38.171706094313016,
      "grad_norm": 6.8994364738464355,
      "learning_rate": 1.819024492140582e-05,
      "loss": 1.6908,
      "step": 487300
    },
    {
      "epoch": 38.17953940153533,
      "grad_norm": 4.782016754150391,
      "learning_rate": 1.8183717165387225e-05,
      "loss": 1.7157,
      "step": 487400
    },
    {
      "epoch": 38.187372708757636,
      "grad_norm": 6.9754638671875,
      "learning_rate": 1.8177189409368637e-05,
      "loss": 1.6825,
      "step": 487500
    },
    {
      "epoch": 38.19520601597995,
      "grad_norm": 5.858761787414551,
      "learning_rate": 1.8170661653350047e-05,
      "loss": 1.6309,
      "step": 487600
    },
    {
      "epoch": 38.203039323202255,
      "grad_norm": 6.413374900817871,
      "learning_rate": 1.8164133897331452e-05,
      "loss": 1.6397,
      "step": 487700
    },
    {
      "epoch": 38.21087263042457,
      "grad_norm": 5.62343168258667,
      "learning_rate": 1.8157606141312865e-05,
      "loss": 1.6035,
      "step": 487800
    },
    {
      "epoch": 38.218705937646874,
      "grad_norm": 5.963904857635498,
      "learning_rate": 1.8151078385294274e-05,
      "loss": 1.601,
      "step": 487900
    },
    {
      "epoch": 38.22653924486919,
      "grad_norm": 5.077375888824463,
      "learning_rate": 1.814455062927568e-05,
      "loss": 1.6959,
      "step": 488000
    },
    {
      "epoch": 38.23437255209149,
      "grad_norm": 5.277821063995361,
      "learning_rate": 1.813802287325709e-05,
      "loss": 1.6828,
      "step": 488100
    },
    {
      "epoch": 38.2422058593138,
      "grad_norm": 7.209403038024902,
      "learning_rate": 1.8131495117238498e-05,
      "loss": 1.6989,
      "step": 488200
    },
    {
      "epoch": 38.25003916653611,
      "grad_norm": 5.132119655609131,
      "learning_rate": 1.8124967361219907e-05,
      "loss": 1.755,
      "step": 488300
    },
    {
      "epoch": 38.25787247375842,
      "grad_norm": 8.845019340515137,
      "learning_rate": 1.8118439605201316e-05,
      "loss": 1.6727,
      "step": 488400
    },
    {
      "epoch": 38.26570578098073,
      "grad_norm": 5.004575729370117,
      "learning_rate": 1.8111911849182726e-05,
      "loss": 1.6043,
      "step": 488500
    },
    {
      "epoch": 38.27353908820304,
      "grad_norm": 4.365059852600098,
      "learning_rate": 1.8105384093164135e-05,
      "loss": 1.6015,
      "step": 488600
    },
    {
      "epoch": 38.28137239542535,
      "grad_norm": 7.17712926864624,
      "learning_rate": 1.8098856337145544e-05,
      "loss": 1.669,
      "step": 488700
    },
    {
      "epoch": 38.289205702647656,
      "grad_norm": 7.463867664337158,
      "learning_rate": 1.8092328581126953e-05,
      "loss": 1.6396,
      "step": 488800
    },
    {
      "epoch": 38.29703900986997,
      "grad_norm": 8.9493408203125,
      "learning_rate": 1.8085800825108362e-05,
      "loss": 1.6615,
      "step": 488900
    },
    {
      "epoch": 38.304872317092276,
      "grad_norm": 7.137838363647461,
      "learning_rate": 1.8079273069089768e-05,
      "loss": 1.7394,
      "step": 489000
    },
    {
      "epoch": 38.31270562431459,
      "grad_norm": 6.664431571960449,
      "learning_rate": 1.807274531307118e-05,
      "loss": 1.663,
      "step": 489100
    },
    {
      "epoch": 38.320538931536895,
      "grad_norm": 8.05195426940918,
      "learning_rate": 1.806621755705259e-05,
      "loss": 1.6552,
      "step": 489200
    },
    {
      "epoch": 38.3283722387592,
      "grad_norm": 6.935858249664307,
      "learning_rate": 1.8059689801033995e-05,
      "loss": 1.7207,
      "step": 489300
    },
    {
      "epoch": 38.336205545981514,
      "grad_norm": 4.9771246910095215,
      "learning_rate": 1.8053162045015408e-05,
      "loss": 1.6662,
      "step": 489400
    },
    {
      "epoch": 38.34403885320382,
      "grad_norm": 8.348076820373535,
      "learning_rate": 1.8046634288996817e-05,
      "loss": 1.706,
      "step": 489500
    },
    {
      "epoch": 38.35187216042613,
      "grad_norm": 5.880156993865967,
      "learning_rate": 1.8040106532978223e-05,
      "loss": 1.6384,
      "step": 489600
    },
    {
      "epoch": 38.35970546764844,
      "grad_norm": 6.996504783630371,
      "learning_rate": 1.8033578776959632e-05,
      "loss": 1.6973,
      "step": 489700
    },
    {
      "epoch": 38.36753877487075,
      "grad_norm": 9.536767959594727,
      "learning_rate": 1.802705102094104e-05,
      "loss": 1.6597,
      "step": 489800
    },
    {
      "epoch": 38.37537208209306,
      "grad_norm": 6.169300556182861,
      "learning_rate": 1.802052326492245e-05,
      "loss": 1.7131,
      "step": 489900
    },
    {
      "epoch": 38.38320538931537,
      "grad_norm": 5.724698543548584,
      "learning_rate": 1.801399550890386e-05,
      "loss": 1.713,
      "step": 490000
    },
    {
      "epoch": 38.39103869653768,
      "grad_norm": 7.756222248077393,
      "learning_rate": 1.800746775288527e-05,
      "loss": 1.6881,
      "step": 490100
    },
    {
      "epoch": 38.39887200375999,
      "grad_norm": 7.430004596710205,
      "learning_rate": 1.8000939996866678e-05,
      "loss": 1.6831,
      "step": 490200
    },
    {
      "epoch": 38.4067053109823,
      "grad_norm": 5.344332218170166,
      "learning_rate": 1.7994412240848087e-05,
      "loss": 1.64,
      "step": 490300
    },
    {
      "epoch": 38.4145386182046,
      "grad_norm": 6.58643913269043,
      "learning_rate": 1.7987884484829496e-05,
      "loss": 1.6763,
      "step": 490400
    },
    {
      "epoch": 38.422371925426916,
      "grad_norm": 5.833206653594971,
      "learning_rate": 1.7981356728810905e-05,
      "loss": 1.6307,
      "step": 490500
    },
    {
      "epoch": 38.43020523264922,
      "grad_norm": 6.4927754402160645,
      "learning_rate": 1.797482897279231e-05,
      "loss": 1.7154,
      "step": 490600
    },
    {
      "epoch": 38.438038539871535,
      "grad_norm": 7.8031392097473145,
      "learning_rate": 1.7968301216773724e-05,
      "loss": 1.6026,
      "step": 490700
    },
    {
      "epoch": 38.44587184709384,
      "grad_norm": 10.226820945739746,
      "learning_rate": 1.7961773460755133e-05,
      "loss": 1.5998,
      "step": 490800
    },
    {
      "epoch": 38.453705154316154,
      "grad_norm": 6.517136096954346,
      "learning_rate": 1.795524570473654e-05,
      "loss": 1.6304,
      "step": 490900
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 6.417905330657959,
      "learning_rate": 1.794871794871795e-05,
      "loss": 1.6421,
      "step": 491000
    },
    {
      "epoch": 38.46937176876077,
      "grad_norm": 7.554717063903809,
      "learning_rate": 1.794219019269936e-05,
      "loss": 1.6554,
      "step": 491100
    },
    {
      "epoch": 38.47720507598308,
      "grad_norm": 9.505655288696289,
      "learning_rate": 1.7935662436680766e-05,
      "loss": 1.6591,
      "step": 491200
    },
    {
      "epoch": 38.48503838320539,
      "grad_norm": 5.700621604919434,
      "learning_rate": 1.7929134680662175e-05,
      "loss": 1.6166,
      "step": 491300
    },
    {
      "epoch": 38.4928716904277,
      "grad_norm": 5.720452785491943,
      "learning_rate": 1.7922606924643584e-05,
      "loss": 1.7037,
      "step": 491400
    },
    {
      "epoch": 38.50070499765001,
      "grad_norm": 5.543811798095703,
      "learning_rate": 1.7916079168624994e-05,
      "loss": 1.6111,
      "step": 491500
    },
    {
      "epoch": 38.50853830487232,
      "grad_norm": 6.316507816314697,
      "learning_rate": 1.7909551412606403e-05,
      "loss": 1.5737,
      "step": 491600
    },
    {
      "epoch": 38.51637161209462,
      "grad_norm": 5.2269392013549805,
      "learning_rate": 1.7903023656587812e-05,
      "loss": 1.6568,
      "step": 491700
    },
    {
      "epoch": 38.52420491931694,
      "grad_norm": 5.315141201019287,
      "learning_rate": 1.789649590056922e-05,
      "loss": 1.6158,
      "step": 491800
    },
    {
      "epoch": 38.53203822653924,
      "grad_norm": 6.16383695602417,
      "learning_rate": 1.788996814455063e-05,
      "loss": 1.6454,
      "step": 491900
    },
    {
      "epoch": 38.539871533761556,
      "grad_norm": 6.858824729919434,
      "learning_rate": 1.788344038853204e-05,
      "loss": 1.6471,
      "step": 492000
    },
    {
      "epoch": 38.54770484098386,
      "grad_norm": 9.383258819580078,
      "learning_rate": 1.787691263251345e-05,
      "loss": 1.6498,
      "step": 492100
    },
    {
      "epoch": 38.555538148206175,
      "grad_norm": 6.423909664154053,
      "learning_rate": 1.7870384876494854e-05,
      "loss": 1.5665,
      "step": 492200
    },
    {
      "epoch": 38.56337145542848,
      "grad_norm": 4.9812912940979,
      "learning_rate": 1.7863857120476267e-05,
      "loss": 1.677,
      "step": 492300
    },
    {
      "epoch": 38.571204762650794,
      "grad_norm": 8.184298515319824,
      "learning_rate": 1.7857329364457676e-05,
      "loss": 1.6123,
      "step": 492400
    },
    {
      "epoch": 38.5790380698731,
      "grad_norm": 5.149948596954346,
      "learning_rate": 1.7850801608439082e-05,
      "loss": 1.6125,
      "step": 492500
    },
    {
      "epoch": 38.58687137709541,
      "grad_norm": 5.930699348449707,
      "learning_rate": 1.7844273852420494e-05,
      "loss": 1.6826,
      "step": 492600
    },
    {
      "epoch": 38.59470468431772,
      "grad_norm": 6.815000534057617,
      "learning_rate": 1.7837746096401903e-05,
      "loss": 1.6846,
      "step": 492700
    },
    {
      "epoch": 38.602537991540025,
      "grad_norm": 7.152773380279541,
      "learning_rate": 1.783121834038331e-05,
      "loss": 1.6281,
      "step": 492800
    },
    {
      "epoch": 38.61037129876234,
      "grad_norm": 6.841921806335449,
      "learning_rate": 1.782469058436472e-05,
      "loss": 1.6754,
      "step": 492900
    },
    {
      "epoch": 38.618204605984644,
      "grad_norm": 5.923386096954346,
      "learning_rate": 1.781816282834613e-05,
      "loss": 1.7351,
      "step": 493000
    },
    {
      "epoch": 38.62603791320696,
      "grad_norm": 7.701366424560547,
      "learning_rate": 1.7811635072327537e-05,
      "loss": 1.6204,
      "step": 493100
    },
    {
      "epoch": 38.63387122042926,
      "grad_norm": 5.254095554351807,
      "learning_rate": 1.7805107316308946e-05,
      "loss": 1.6032,
      "step": 493200
    },
    {
      "epoch": 38.64170452765158,
      "grad_norm": 6.408204555511475,
      "learning_rate": 1.7798579560290355e-05,
      "loss": 1.5936,
      "step": 493300
    },
    {
      "epoch": 38.64953783487388,
      "grad_norm": 9.185043334960938,
      "learning_rate": 1.7792051804271764e-05,
      "loss": 1.7377,
      "step": 493400
    },
    {
      "epoch": 38.657371142096196,
      "grad_norm": 5.451897144317627,
      "learning_rate": 1.7785524048253173e-05,
      "loss": 1.6901,
      "step": 493500
    },
    {
      "epoch": 38.6652044493185,
      "grad_norm": 7.18890380859375,
      "learning_rate": 1.7778996292234582e-05,
      "loss": 1.6934,
      "step": 493600
    },
    {
      "epoch": 38.673037756540815,
      "grad_norm": 5.199539661407471,
      "learning_rate": 1.777246853621599e-05,
      "loss": 1.5985,
      "step": 493700
    },
    {
      "epoch": 38.68087106376312,
      "grad_norm": 6.203615188598633,
      "learning_rate": 1.77659407801974e-05,
      "loss": 1.7004,
      "step": 493800
    },
    {
      "epoch": 38.68870437098543,
      "grad_norm": 5.709694862365723,
      "learning_rate": 1.775941302417881e-05,
      "loss": 1.6521,
      "step": 493900
    },
    {
      "epoch": 38.69653767820774,
      "grad_norm": 5.78544282913208,
      "learning_rate": 1.775288526816022e-05,
      "loss": 1.6232,
      "step": 494000
    },
    {
      "epoch": 38.704370985430046,
      "grad_norm": 6.371719837188721,
      "learning_rate": 1.7746357512141625e-05,
      "loss": 1.6255,
      "step": 494100
    },
    {
      "epoch": 38.71220429265236,
      "grad_norm": 5.796802997589111,
      "learning_rate": 1.7739829756123037e-05,
      "loss": 1.7276,
      "step": 494200
    },
    {
      "epoch": 38.720037599874665,
      "grad_norm": 5.90762996673584,
      "learning_rate": 1.7733302000104447e-05,
      "loss": 1.7854,
      "step": 494300
    },
    {
      "epoch": 38.72787090709698,
      "grad_norm": 18.1339111328125,
      "learning_rate": 1.7726774244085852e-05,
      "loss": 1.7181,
      "step": 494400
    },
    {
      "epoch": 38.735704214319284,
      "grad_norm": 5.829667568206787,
      "learning_rate": 1.772024648806726e-05,
      "loss": 1.5632,
      "step": 494500
    },
    {
      "epoch": 38.7435375215416,
      "grad_norm": 6.6684465408325195,
      "learning_rate": 1.7713718732048674e-05,
      "loss": 1.6239,
      "step": 494600
    },
    {
      "epoch": 38.7513708287639,
      "grad_norm": 6.779818534851074,
      "learning_rate": 1.770719097603008e-05,
      "loss": 1.6117,
      "step": 494700
    },
    {
      "epoch": 38.75920413598622,
      "grad_norm": 7.818873405456543,
      "learning_rate": 1.770066322001149e-05,
      "loss": 1.6795,
      "step": 494800
    },
    {
      "epoch": 38.76703744320852,
      "grad_norm": 6.832021713256836,
      "learning_rate": 1.7694135463992898e-05,
      "loss": 1.6497,
      "step": 494900
    },
    {
      "epoch": 38.77487075043083,
      "grad_norm": 7.202234268188477,
      "learning_rate": 1.7687607707974307e-05,
      "loss": 1.6053,
      "step": 495000
    },
    {
      "epoch": 38.78270405765314,
      "grad_norm": 5.312087059020996,
      "learning_rate": 1.7681079951955716e-05,
      "loss": 1.6506,
      "step": 495100
    },
    {
      "epoch": 38.79053736487545,
      "grad_norm": 4.432389736175537,
      "learning_rate": 1.7674552195937126e-05,
      "loss": 1.7103,
      "step": 495200
    },
    {
      "epoch": 38.79837067209776,
      "grad_norm": 6.4591498374938965,
      "learning_rate": 1.7668024439918535e-05,
      "loss": 1.6095,
      "step": 495300
    },
    {
      "epoch": 38.80620397932007,
      "grad_norm": 6.148783206939697,
      "learning_rate": 1.7661496683899944e-05,
      "loss": 1.7,
      "step": 495400
    },
    {
      "epoch": 38.81403728654238,
      "grad_norm": 7.52623987197876,
      "learning_rate": 1.7654968927881353e-05,
      "loss": 1.6905,
      "step": 495500
    },
    {
      "epoch": 38.821870593764686,
      "grad_norm": 7.993773937225342,
      "learning_rate": 1.7648441171862762e-05,
      "loss": 1.7755,
      "step": 495600
    },
    {
      "epoch": 38.829703900987,
      "grad_norm": 7.239151954650879,
      "learning_rate": 1.7641913415844168e-05,
      "loss": 1.6798,
      "step": 495700
    },
    {
      "epoch": 38.837537208209305,
      "grad_norm": 9.156907081604004,
      "learning_rate": 1.763538565982558e-05,
      "loss": 1.7393,
      "step": 495800
    },
    {
      "epoch": 38.84537051543162,
      "grad_norm": 5.110862731933594,
      "learning_rate": 1.762885790380699e-05,
      "loss": 1.6093,
      "step": 495900
    },
    {
      "epoch": 38.853203822653924,
      "grad_norm": 8.560774803161621,
      "learning_rate": 1.7622330147788395e-05,
      "loss": 1.683,
      "step": 496000
    },
    {
      "epoch": 38.86103712987623,
      "grad_norm": 6.0482306480407715,
      "learning_rate": 1.7615802391769805e-05,
      "loss": 1.723,
      "step": 496100
    },
    {
      "epoch": 38.86887043709854,
      "grad_norm": 7.932888984680176,
      "learning_rate": 1.7609274635751217e-05,
      "loss": 1.6591,
      "step": 496200
    },
    {
      "epoch": 38.87670374432085,
      "grad_norm": 5.3884429931640625,
      "learning_rate": 1.7602746879732623e-05,
      "loss": 1.6206,
      "step": 496300
    },
    {
      "epoch": 38.88453705154316,
      "grad_norm": 7.070710182189941,
      "learning_rate": 1.7596219123714032e-05,
      "loss": 1.5972,
      "step": 496400
    },
    {
      "epoch": 38.89237035876547,
      "grad_norm": 6.802396297454834,
      "learning_rate": 1.758969136769544e-05,
      "loss": 1.6458,
      "step": 496500
    },
    {
      "epoch": 38.90020366598778,
      "grad_norm": 6.122471809387207,
      "learning_rate": 1.758316361167685e-05,
      "loss": 1.7044,
      "step": 496600
    },
    {
      "epoch": 38.90803697321009,
      "grad_norm": 7.183401584625244,
      "learning_rate": 1.757663585565826e-05,
      "loss": 1.6703,
      "step": 496700
    },
    {
      "epoch": 38.9158702804324,
      "grad_norm": 6.9106011390686035,
      "learning_rate": 1.757010809963967e-05,
      "loss": 1.615,
      "step": 496800
    },
    {
      "epoch": 38.92370358765471,
      "grad_norm": 5.792872428894043,
      "learning_rate": 1.7563580343621078e-05,
      "loss": 1.6619,
      "step": 496900
    },
    {
      "epoch": 38.93153689487702,
      "grad_norm": 6.573162078857422,
      "learning_rate": 1.7557052587602487e-05,
      "loss": 1.7193,
      "step": 497000
    },
    {
      "epoch": 38.939370202099326,
      "grad_norm": 6.411771774291992,
      "learning_rate": 1.7550524831583896e-05,
      "loss": 1.7219,
      "step": 497100
    },
    {
      "epoch": 38.94720350932164,
      "grad_norm": 7.906123638153076,
      "learning_rate": 1.7543997075565305e-05,
      "loss": 1.541,
      "step": 497200
    },
    {
      "epoch": 38.955036816543945,
      "grad_norm": 7.0426483154296875,
      "learning_rate": 1.753746931954671e-05,
      "loss": 1.6886,
      "step": 497300
    },
    {
      "epoch": 38.96287012376625,
      "grad_norm": 6.755789279937744,
      "learning_rate": 1.7530941563528124e-05,
      "loss": 1.6592,
      "step": 497400
    },
    {
      "epoch": 38.970703430988564,
      "grad_norm": 7.529294013977051,
      "learning_rate": 1.7524413807509533e-05,
      "loss": 1.7292,
      "step": 497500
    },
    {
      "epoch": 38.97853673821087,
      "grad_norm": 6.697014808654785,
      "learning_rate": 1.751788605149094e-05,
      "loss": 1.7515,
      "step": 497600
    },
    {
      "epoch": 38.98637004543318,
      "grad_norm": 6.219267845153809,
      "learning_rate": 1.7511358295472348e-05,
      "loss": 1.6756,
      "step": 497700
    },
    {
      "epoch": 38.99420335265549,
      "grad_norm": 6.6673784255981445,
      "learning_rate": 1.750483053945376e-05,
      "loss": 1.6826,
      "step": 497800
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.7771141529083252,
      "eval_runtime": 1.6087,
      "eval_samples_per_second": 417.731,
      "eval_steps_per_second": 417.731,
      "step": 497874
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.4137476682662964,
      "eval_runtime": 29.6695,
      "eval_samples_per_second": 430.273,
      "eval_steps_per_second": 430.273,
      "step": 497874
    },
    {
      "epoch": 39.0020366598778,
      "grad_norm": 7.103667736053467,
      "learning_rate": 1.7498302783435166e-05,
      "loss": 1.6687,
      "step": 497900
    },
    {
      "epoch": 39.00986996710011,
      "grad_norm": 5.9225263595581055,
      "learning_rate": 1.7491775027416575e-05,
      "loss": 1.6939,
      "step": 498000
    },
    {
      "epoch": 39.01770327432242,
      "grad_norm": 6.366184234619141,
      "learning_rate": 1.7485247271397984e-05,
      "loss": 1.6684,
      "step": 498100
    },
    {
      "epoch": 39.02553658154473,
      "grad_norm": 5.6492204666137695,
      "learning_rate": 1.7478719515379393e-05,
      "loss": 1.6273,
      "step": 498200
    },
    {
      "epoch": 39.03336988876704,
      "grad_norm": 6.999748229980469,
      "learning_rate": 1.7472191759360803e-05,
      "loss": 1.7715,
      "step": 498300
    },
    {
      "epoch": 39.04120319598935,
      "grad_norm": 5.530455589294434,
      "learning_rate": 1.7465664003342212e-05,
      "loss": 1.6561,
      "step": 498400
    },
    {
      "epoch": 39.04903650321165,
      "grad_norm": 6.409930229187012,
      "learning_rate": 1.745913624732362e-05,
      "loss": 1.6808,
      "step": 498500
    },
    {
      "epoch": 39.056869810433966,
      "grad_norm": 6.654006004333496,
      "learning_rate": 1.745260849130503e-05,
      "loss": 1.6022,
      "step": 498600
    },
    {
      "epoch": 39.06470311765627,
      "grad_norm": 4.852878570556641,
      "learning_rate": 1.744608073528644e-05,
      "loss": 1.6836,
      "step": 498700
    },
    {
      "epoch": 39.072536424878585,
      "grad_norm": 7.40531063079834,
      "learning_rate": 1.743955297926785e-05,
      "loss": 1.749,
      "step": 498800
    },
    {
      "epoch": 39.08036973210089,
      "grad_norm": 5.960167407989502,
      "learning_rate": 1.7433025223249258e-05,
      "loss": 1.5796,
      "step": 498900
    },
    {
      "epoch": 39.088203039323204,
      "grad_norm": 5.611711502075195,
      "learning_rate": 1.7426497467230667e-05,
      "loss": 1.6419,
      "step": 499000
    },
    {
      "epoch": 39.09603634654551,
      "grad_norm": 7.36002779006958,
      "learning_rate": 1.7419969711212076e-05,
      "loss": 1.6306,
      "step": 499100
    },
    {
      "epoch": 39.10386965376782,
      "grad_norm": 6.369647026062012,
      "learning_rate": 1.741344195519348e-05,
      "loss": 1.6107,
      "step": 499200
    },
    {
      "epoch": 39.11170296099013,
      "grad_norm": 5.498560905456543,
      "learning_rate": 1.740691419917489e-05,
      "loss": 1.5277,
      "step": 499300
    },
    {
      "epoch": 39.11953626821244,
      "grad_norm": 5.776296138763428,
      "learning_rate": 1.7400386443156303e-05,
      "loss": 1.5841,
      "step": 499400
    },
    {
      "epoch": 39.12736957543475,
      "grad_norm": 6.215823173522949,
      "learning_rate": 1.739385868713771e-05,
      "loss": 1.6717,
      "step": 499500
    },
    {
      "epoch": 39.135202882657055,
      "grad_norm": 9.163893699645996,
      "learning_rate": 1.7387330931119118e-05,
      "loss": 1.5865,
      "step": 499600
    },
    {
      "epoch": 39.14303618987937,
      "grad_norm": 5.587677478790283,
      "learning_rate": 1.738080317510053e-05,
      "loss": 1.6023,
      "step": 499700
    },
    {
      "epoch": 39.150869497101674,
      "grad_norm": 7.487746238708496,
      "learning_rate": 1.7374275419081937e-05,
      "loss": 1.5627,
      "step": 499800
    },
    {
      "epoch": 39.15870280432399,
      "grad_norm": 5.79608678817749,
      "learning_rate": 1.7367747663063346e-05,
      "loss": 1.6862,
      "step": 499900
    },
    {
      "epoch": 39.16653611154629,
      "grad_norm": 6.627346038818359,
      "learning_rate": 1.7361219907044755e-05,
      "loss": 1.6695,
      "step": 500000
    },
    {
      "epoch": 39.174369418768606,
      "grad_norm": 7.968845844268799,
      "learning_rate": 1.7354692151026164e-05,
      "loss": 1.5943,
      "step": 500100
    },
    {
      "epoch": 39.18220272599091,
      "grad_norm": 6.828822612762451,
      "learning_rate": 1.7348164395007573e-05,
      "loss": 1.6373,
      "step": 500200
    },
    {
      "epoch": 39.190036033213225,
      "grad_norm": 5.297598838806152,
      "learning_rate": 1.7341636638988982e-05,
      "loss": 1.6488,
      "step": 500300
    },
    {
      "epoch": 39.19786934043553,
      "grad_norm": 8.25067138671875,
      "learning_rate": 1.733510888297039e-05,
      "loss": 1.6345,
      "step": 500400
    },
    {
      "epoch": 39.205702647657844,
      "grad_norm": 5.680692672729492,
      "learning_rate": 1.73285811269518e-05,
      "loss": 1.7366,
      "step": 500500
    },
    {
      "epoch": 39.21353595488015,
      "grad_norm": 5.169276237487793,
      "learning_rate": 1.732205337093321e-05,
      "loss": 1.6051,
      "step": 500600
    },
    {
      "epoch": 39.221369262102456,
      "grad_norm": 7.3282790184021,
      "learning_rate": 1.731552561491462e-05,
      "loss": 1.6964,
      "step": 500700
    },
    {
      "epoch": 39.22920256932477,
      "grad_norm": 4.982437610626221,
      "learning_rate": 1.7308997858896025e-05,
      "loss": 1.676,
      "step": 500800
    },
    {
      "epoch": 39.237035876547075,
      "grad_norm": 5.664684295654297,
      "learning_rate": 1.7302470102877434e-05,
      "loss": 1.6055,
      "step": 500900
    },
    {
      "epoch": 39.24486918376939,
      "grad_norm": 5.725796222686768,
      "learning_rate": 1.7295942346858846e-05,
      "loss": 1.5499,
      "step": 501000
    },
    {
      "epoch": 39.252702490991695,
      "grad_norm": 6.960221290588379,
      "learning_rate": 1.7289414590840252e-05,
      "loss": 1.6603,
      "step": 501100
    },
    {
      "epoch": 39.26053579821401,
      "grad_norm": 8.185473442077637,
      "learning_rate": 1.728288683482166e-05,
      "loss": 1.5861,
      "step": 501200
    },
    {
      "epoch": 39.268369105436314,
      "grad_norm": 6.833752632141113,
      "learning_rate": 1.7276359078803074e-05,
      "loss": 1.6951,
      "step": 501300
    },
    {
      "epoch": 39.27620241265863,
      "grad_norm": 5.294275760650635,
      "learning_rate": 1.726983132278448e-05,
      "loss": 1.7237,
      "step": 501400
    },
    {
      "epoch": 39.28403571988093,
      "grad_norm": 7.516124725341797,
      "learning_rate": 1.726330356676589e-05,
      "loss": 1.7351,
      "step": 501500
    },
    {
      "epoch": 39.291869027103246,
      "grad_norm": 5.605423450469971,
      "learning_rate": 1.7256775810747298e-05,
      "loss": 1.7651,
      "step": 501600
    },
    {
      "epoch": 39.29970233432555,
      "grad_norm": 5.012169361114502,
      "learning_rate": 1.7250248054728707e-05,
      "loss": 1.6143,
      "step": 501700
    },
    {
      "epoch": 39.30753564154786,
      "grad_norm": 5.644984722137451,
      "learning_rate": 1.7243720298710116e-05,
      "loss": 1.6068,
      "step": 501800
    },
    {
      "epoch": 39.31536894877017,
      "grad_norm": 7.1790771484375,
      "learning_rate": 1.7237192542691525e-05,
      "loss": 1.6263,
      "step": 501900
    },
    {
      "epoch": 39.32320225599248,
      "grad_norm": 7.3274993896484375,
      "learning_rate": 1.7230664786672935e-05,
      "loss": 1.5871,
      "step": 502000
    },
    {
      "epoch": 39.33103556321479,
      "grad_norm": 5.6965413093566895,
      "learning_rate": 1.7224137030654344e-05,
      "loss": 1.5632,
      "step": 502100
    },
    {
      "epoch": 39.338868870437096,
      "grad_norm": 8.916413307189941,
      "learning_rate": 1.7217609274635753e-05,
      "loss": 1.7227,
      "step": 502200
    },
    {
      "epoch": 39.34670217765941,
      "grad_norm": 5.631313323974609,
      "learning_rate": 1.7211081518617162e-05,
      "loss": 1.7002,
      "step": 502300
    },
    {
      "epoch": 39.354535484881715,
      "grad_norm": 6.7750701904296875,
      "learning_rate": 1.7204553762598568e-05,
      "loss": 1.6736,
      "step": 502400
    },
    {
      "epoch": 39.36236879210403,
      "grad_norm": 4.855024814605713,
      "learning_rate": 1.7198026006579977e-05,
      "loss": 1.6934,
      "step": 502500
    },
    {
      "epoch": 39.370202099326335,
      "grad_norm": 5.352849006652832,
      "learning_rate": 1.719149825056139e-05,
      "loss": 1.6222,
      "step": 502600
    },
    {
      "epoch": 39.37803540654865,
      "grad_norm": 7.412282466888428,
      "learning_rate": 1.7184970494542795e-05,
      "loss": 1.5619,
      "step": 502700
    },
    {
      "epoch": 39.385868713770954,
      "grad_norm": 7.335672378540039,
      "learning_rate": 1.7178442738524205e-05,
      "loss": 1.6449,
      "step": 502800
    },
    {
      "epoch": 39.39370202099326,
      "grad_norm": 5.747612953186035,
      "learning_rate": 1.7171914982505617e-05,
      "loss": 1.8393,
      "step": 502900
    },
    {
      "epoch": 39.40153532821557,
      "grad_norm": 5.750638008117676,
      "learning_rate": 1.7165387226487023e-05,
      "loss": 1.6861,
      "step": 503000
    },
    {
      "epoch": 39.40936863543788,
      "grad_norm": 5.035622596740723,
      "learning_rate": 1.7158859470468432e-05,
      "loss": 1.6476,
      "step": 503100
    },
    {
      "epoch": 39.41720194266019,
      "grad_norm": 7.2677459716796875,
      "learning_rate": 1.715233171444984e-05,
      "loss": 1.6129,
      "step": 503200
    },
    {
      "epoch": 39.4250352498825,
      "grad_norm": 5.637241363525391,
      "learning_rate": 1.714580395843125e-05,
      "loss": 1.587,
      "step": 503300
    },
    {
      "epoch": 39.43286855710481,
      "grad_norm": 6.990375518798828,
      "learning_rate": 1.713927620241266e-05,
      "loss": 1.5737,
      "step": 503400
    },
    {
      "epoch": 39.44070186432712,
      "grad_norm": 6.230291366577148,
      "learning_rate": 1.713274844639407e-05,
      "loss": 1.7402,
      "step": 503500
    },
    {
      "epoch": 39.44853517154943,
      "grad_norm": 8.131509780883789,
      "learning_rate": 1.7126220690375478e-05,
      "loss": 1.6492,
      "step": 503600
    },
    {
      "epoch": 39.456368478771736,
      "grad_norm": 6.488815784454346,
      "learning_rate": 1.7119692934356887e-05,
      "loss": 1.6547,
      "step": 503700
    },
    {
      "epoch": 39.46420178599405,
      "grad_norm": 6.428523540496826,
      "learning_rate": 1.7113165178338296e-05,
      "loss": 1.6403,
      "step": 503800
    },
    {
      "epoch": 39.472035093216356,
      "grad_norm": 6.838775157928467,
      "learning_rate": 1.7106637422319705e-05,
      "loss": 1.5998,
      "step": 503900
    },
    {
      "epoch": 39.47986840043867,
      "grad_norm": 7.404804706573486,
      "learning_rate": 1.710010966630111e-05,
      "loss": 1.6493,
      "step": 504000
    },
    {
      "epoch": 39.487701707660975,
      "grad_norm": 6.064059257507324,
      "learning_rate": 1.709358191028252e-05,
      "loss": 1.6638,
      "step": 504100
    },
    {
      "epoch": 39.49553501488328,
      "grad_norm": 5.189219951629639,
      "learning_rate": 1.7087054154263933e-05,
      "loss": 1.6583,
      "step": 504200
    },
    {
      "epoch": 39.503368322105594,
      "grad_norm": 7.904295444488525,
      "learning_rate": 1.708052639824534e-05,
      "loss": 1.6996,
      "step": 504300
    },
    {
      "epoch": 39.5112016293279,
      "grad_norm": 5.821497917175293,
      "learning_rate": 1.7073998642226748e-05,
      "loss": 1.6851,
      "step": 504400
    },
    {
      "epoch": 39.51903493655021,
      "grad_norm": 6.172458648681641,
      "learning_rate": 1.706747088620816e-05,
      "loss": 1.6766,
      "step": 504500
    },
    {
      "epoch": 39.52686824377252,
      "grad_norm": 6.924849033355713,
      "learning_rate": 1.7060943130189566e-05,
      "loss": 1.6771,
      "step": 504600
    },
    {
      "epoch": 39.53470155099483,
      "grad_norm": 2.8797366619110107,
      "learning_rate": 1.7054415374170975e-05,
      "loss": 1.5729,
      "step": 504700
    },
    {
      "epoch": 39.54253485821714,
      "grad_norm": 7.3580708503723145,
      "learning_rate": 1.7047887618152388e-05,
      "loss": 1.6805,
      "step": 504800
    },
    {
      "epoch": 39.55036816543945,
      "grad_norm": 4.326708793640137,
      "learning_rate": 1.7041359862133793e-05,
      "loss": 1.6581,
      "step": 504900
    },
    {
      "epoch": 39.55820147266176,
      "grad_norm": 5.406234264373779,
      "learning_rate": 1.7034832106115203e-05,
      "loss": 1.6768,
      "step": 505000
    },
    {
      "epoch": 39.56603477988407,
      "grad_norm": 8.953048706054688,
      "learning_rate": 1.7028304350096612e-05,
      "loss": 1.7345,
      "step": 505100
    },
    {
      "epoch": 39.573868087106376,
      "grad_norm": 5.960973262786865,
      "learning_rate": 1.702177659407802e-05,
      "loss": 1.617,
      "step": 505200
    },
    {
      "epoch": 39.58170139432868,
      "grad_norm": 7.233418941497803,
      "learning_rate": 1.701524883805943e-05,
      "loss": 1.626,
      "step": 505300
    },
    {
      "epoch": 39.589534701550996,
      "grad_norm": 6.636528968811035,
      "learning_rate": 1.700872108204084e-05,
      "loss": 1.698,
      "step": 505400
    },
    {
      "epoch": 39.5973680087733,
      "grad_norm": 5.922030925750732,
      "learning_rate": 1.700219332602225e-05,
      "loss": 1.6073,
      "step": 505500
    },
    {
      "epoch": 39.605201315995615,
      "grad_norm": 8.441893577575684,
      "learning_rate": 1.6995665570003657e-05,
      "loss": 1.6824,
      "step": 505600
    },
    {
      "epoch": 39.61303462321792,
      "grad_norm": 4.8974690437316895,
      "learning_rate": 1.6989137813985063e-05,
      "loss": 1.6902,
      "step": 505700
    },
    {
      "epoch": 39.620867930440234,
      "grad_norm": 5.040355682373047,
      "learning_rate": 1.6982610057966476e-05,
      "loss": 1.6684,
      "step": 505800
    },
    {
      "epoch": 39.62870123766254,
      "grad_norm": 7.778487682342529,
      "learning_rate": 1.697608230194788e-05,
      "loss": 1.6551,
      "step": 505900
    },
    {
      "epoch": 39.63653454488485,
      "grad_norm": 6.827167987823486,
      "learning_rate": 1.696955454592929e-05,
      "loss": 1.729,
      "step": 506000
    },
    {
      "epoch": 39.64436785210716,
      "grad_norm": 5.776289939880371,
      "learning_rate": 1.6963026789910703e-05,
      "loss": 1.7341,
      "step": 506100
    },
    {
      "epoch": 39.65220115932947,
      "grad_norm": 4.879217147827148,
      "learning_rate": 1.695649903389211e-05,
      "loss": 1.69,
      "step": 506200
    },
    {
      "epoch": 39.66003446655178,
      "grad_norm": 5.896151065826416,
      "learning_rate": 1.6949971277873518e-05,
      "loss": 1.643,
      "step": 506300
    },
    {
      "epoch": 39.667867773774084,
      "grad_norm": 6.928366661071777,
      "learning_rate": 1.694344352185493e-05,
      "loss": 1.6721,
      "step": 506400
    },
    {
      "epoch": 39.6757010809964,
      "grad_norm": 7.659358978271484,
      "learning_rate": 1.6936915765836337e-05,
      "loss": 1.5989,
      "step": 506500
    },
    {
      "epoch": 39.6835343882187,
      "grad_norm": 6.514948844909668,
      "learning_rate": 1.6930388009817746e-05,
      "loss": 1.6374,
      "step": 506600
    },
    {
      "epoch": 39.691367695441016,
      "grad_norm": 6.2633233070373535,
      "learning_rate": 1.6923860253799155e-05,
      "loss": 1.6195,
      "step": 506700
    },
    {
      "epoch": 39.69920100266332,
      "grad_norm": 6.768889904022217,
      "learning_rate": 1.6917332497780564e-05,
      "loss": 1.6346,
      "step": 506800
    },
    {
      "epoch": 39.707034309885636,
      "grad_norm": 5.954580307006836,
      "learning_rate": 1.6910804741761973e-05,
      "loss": 1.535,
      "step": 506900
    },
    {
      "epoch": 39.71486761710794,
      "grad_norm": 8.092055320739746,
      "learning_rate": 1.6904276985743382e-05,
      "loss": 1.5883,
      "step": 507000
    },
    {
      "epoch": 39.722700924330255,
      "grad_norm": 7.505741119384766,
      "learning_rate": 1.689774922972479e-05,
      "loss": 1.7065,
      "step": 507100
    },
    {
      "epoch": 39.73053423155256,
      "grad_norm": 5.676733016967773,
      "learning_rate": 1.68912214737062e-05,
      "loss": 1.7052,
      "step": 507200
    },
    {
      "epoch": 39.738367538774874,
      "grad_norm": 7.937126159667969,
      "learning_rate": 1.6884693717687606e-05,
      "loss": 1.6481,
      "step": 507300
    },
    {
      "epoch": 39.74620084599718,
      "grad_norm": 7.329621315002441,
      "learning_rate": 1.687816596166902e-05,
      "loss": 1.6885,
      "step": 507400
    },
    {
      "epoch": 39.754034153219486,
      "grad_norm": 7.101975917816162,
      "learning_rate": 1.6871638205650425e-05,
      "loss": 1.7334,
      "step": 507500
    },
    {
      "epoch": 39.7618674604418,
      "grad_norm": 7.099099159240723,
      "learning_rate": 1.6865110449631834e-05,
      "loss": 1.6486,
      "step": 507600
    },
    {
      "epoch": 39.769700767664105,
      "grad_norm": 5.703364372253418,
      "learning_rate": 1.6858582693613246e-05,
      "loss": 1.6214,
      "step": 507700
    },
    {
      "epoch": 39.77753407488642,
      "grad_norm": 4.259610652923584,
      "learning_rate": 1.6852054937594652e-05,
      "loss": 1.6132,
      "step": 507800
    },
    {
      "epoch": 39.785367382108724,
      "grad_norm": 2.8734731674194336,
      "learning_rate": 1.684552718157606e-05,
      "loss": 1.6522,
      "step": 507900
    },
    {
      "epoch": 39.79320068933104,
      "grad_norm": 8.334986686706543,
      "learning_rate": 1.6838999425557474e-05,
      "loss": 1.7103,
      "step": 508000
    },
    {
      "epoch": 39.80103399655334,
      "grad_norm": 6.049718379974365,
      "learning_rate": 1.683247166953888e-05,
      "loss": 1.665,
      "step": 508100
    },
    {
      "epoch": 39.808867303775656,
      "grad_norm": 4.345896244049072,
      "learning_rate": 1.682594391352029e-05,
      "loss": 1.6171,
      "step": 508200
    },
    {
      "epoch": 39.81670061099796,
      "grad_norm": 7.58408784866333,
      "learning_rate": 1.6819416157501698e-05,
      "loss": 1.6449,
      "step": 508300
    },
    {
      "epoch": 39.824533918220276,
      "grad_norm": 6.017792224884033,
      "learning_rate": 1.6812888401483107e-05,
      "loss": 1.6616,
      "step": 508400
    },
    {
      "epoch": 39.83236722544258,
      "grad_norm": 7.528408527374268,
      "learning_rate": 1.6806360645464516e-05,
      "loss": 1.7129,
      "step": 508500
    },
    {
      "epoch": 39.840200532664895,
      "grad_norm": 4.269364356994629,
      "learning_rate": 1.6799832889445925e-05,
      "loss": 1.6594,
      "step": 508600
    },
    {
      "epoch": 39.8480338398872,
      "grad_norm": 5.915319442749023,
      "learning_rate": 1.6793305133427335e-05,
      "loss": 1.6282,
      "step": 508700
    },
    {
      "epoch": 39.85586714710951,
      "grad_norm": 7.151802062988281,
      "learning_rate": 1.6786777377408744e-05,
      "loss": 1.616,
      "step": 508800
    },
    {
      "epoch": 39.86370045433182,
      "grad_norm": 8.879291534423828,
      "learning_rate": 1.678024962139015e-05,
      "loss": 1.7386,
      "step": 508900
    },
    {
      "epoch": 39.871533761554126,
      "grad_norm": 7.663946151733398,
      "learning_rate": 1.6773721865371562e-05,
      "loss": 1.6586,
      "step": 509000
    },
    {
      "epoch": 39.87936706877644,
      "grad_norm": 7.719170093536377,
      "learning_rate": 1.6767194109352968e-05,
      "loss": 1.6334,
      "step": 509100
    },
    {
      "epoch": 39.887200375998745,
      "grad_norm": 6.969802379608154,
      "learning_rate": 1.6760666353334377e-05,
      "loss": 1.8283,
      "step": 509200
    },
    {
      "epoch": 39.89503368322106,
      "grad_norm": 8.820304870605469,
      "learning_rate": 1.675413859731579e-05,
      "loss": 1.7101,
      "step": 509300
    },
    {
      "epoch": 39.902866990443364,
      "grad_norm": 6.676407337188721,
      "learning_rate": 1.6747610841297195e-05,
      "loss": 1.6171,
      "step": 509400
    },
    {
      "epoch": 39.91070029766568,
      "grad_norm": 9.506097793579102,
      "learning_rate": 1.6741083085278604e-05,
      "loss": 1.6978,
      "step": 509500
    },
    {
      "epoch": 39.91853360488798,
      "grad_norm": 5.77532958984375,
      "learning_rate": 1.6734555329260017e-05,
      "loss": 1.6149,
      "step": 509600
    },
    {
      "epoch": 39.926366912110296,
      "grad_norm": 5.9340314865112305,
      "learning_rate": 1.6728027573241423e-05,
      "loss": 1.6851,
      "step": 509700
    },
    {
      "epoch": 39.9342002193326,
      "grad_norm": 6.6510329246521,
      "learning_rate": 1.6721499817222832e-05,
      "loss": 1.7039,
      "step": 509800
    },
    {
      "epoch": 39.94203352655491,
      "grad_norm": 5.252978801727295,
      "learning_rate": 1.671497206120424e-05,
      "loss": 1.7997,
      "step": 509900
    },
    {
      "epoch": 39.94986683377722,
      "grad_norm": 7.757778644561768,
      "learning_rate": 1.670844430518565e-05,
      "loss": 1.6218,
      "step": 510000
    },
    {
      "epoch": 39.95770014099953,
      "grad_norm": 8.368474006652832,
      "learning_rate": 1.670191654916706e-05,
      "loss": 1.569,
      "step": 510100
    },
    {
      "epoch": 39.96553344822184,
      "grad_norm": 9.525720596313477,
      "learning_rate": 1.669538879314847e-05,
      "loss": 1.6389,
      "step": 510200
    },
    {
      "epoch": 39.97336675544415,
      "grad_norm": 5.210793495178223,
      "learning_rate": 1.6688861037129878e-05,
      "loss": 1.746,
      "step": 510300
    },
    {
      "epoch": 39.98120006266646,
      "grad_norm": 6.88789701461792,
      "learning_rate": 1.6682333281111287e-05,
      "loss": 1.7088,
      "step": 510400
    },
    {
      "epoch": 39.989033369888766,
      "grad_norm": 7.693245887756348,
      "learning_rate": 1.6675805525092693e-05,
      "loss": 1.5429,
      "step": 510500
    },
    {
      "epoch": 39.99686667711108,
      "grad_norm": 9.70819091796875,
      "learning_rate": 1.6669277769074105e-05,
      "loss": 1.6248,
      "step": 510600
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.7759428024291992,
      "eval_runtime": 1.522,
      "eval_samples_per_second": 441.52,
      "eval_steps_per_second": 441.52,
      "step": 510640
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.4071505069732666,
      "eval_runtime": 29.4059,
      "eval_samples_per_second": 434.13,
      "eval_steps_per_second": 434.13,
      "step": 510640
    },
    {
      "epoch": 40.004699984333385,
      "grad_norm": 4.587733268737793,
      "learning_rate": 1.6662750013055514e-05,
      "loss": 1.6226,
      "step": 510700
    },
    {
      "epoch": 40.0125332915557,
      "grad_norm": 5.188752174377441,
      "learning_rate": 1.665622225703692e-05,
      "loss": 1.6424,
      "step": 510800
    },
    {
      "epoch": 40.020366598778004,
      "grad_norm": 7.699771881103516,
      "learning_rate": 1.6649694501018333e-05,
      "loss": 1.6616,
      "step": 510900
    },
    {
      "epoch": 40.02819990600031,
      "grad_norm": 6.932044506072998,
      "learning_rate": 1.664316674499974e-05,
      "loss": 1.633,
      "step": 511000
    },
    {
      "epoch": 40.03603321322262,
      "grad_norm": 5.936316013336182,
      "learning_rate": 1.6636638988981148e-05,
      "loss": 1.6016,
      "step": 511100
    },
    {
      "epoch": 40.04386652044493,
      "grad_norm": 3.260096549987793,
      "learning_rate": 1.663011123296256e-05,
      "loss": 1.5809,
      "step": 511200
    },
    {
      "epoch": 40.05169982766724,
      "grad_norm": 7.924971103668213,
      "learning_rate": 1.6623583476943966e-05,
      "loss": 1.7207,
      "step": 511300
    },
    {
      "epoch": 40.05953313488955,
      "grad_norm": 8.041500091552734,
      "learning_rate": 1.6617055720925375e-05,
      "loss": 1.6694,
      "step": 511400
    },
    {
      "epoch": 40.06736644211186,
      "grad_norm": 5.169628143310547,
      "learning_rate": 1.6610527964906784e-05,
      "loss": 1.6727,
      "step": 511500
    },
    {
      "epoch": 40.07519974933417,
      "grad_norm": 6.457724571228027,
      "learning_rate": 1.6604000208888193e-05,
      "loss": 1.5591,
      "step": 511600
    },
    {
      "epoch": 40.08303305655648,
      "grad_norm": 6.728246688842773,
      "learning_rate": 1.6597472452869602e-05,
      "loss": 1.6008,
      "step": 511700
    },
    {
      "epoch": 40.09086636377879,
      "grad_norm": 9.766742706298828,
      "learning_rate": 1.659094469685101e-05,
      "loss": 1.6711,
      "step": 511800
    },
    {
      "epoch": 40.0986996710011,
      "grad_norm": 7.344738006591797,
      "learning_rate": 1.658441694083242e-05,
      "loss": 1.6911,
      "step": 511900
    },
    {
      "epoch": 40.106532978223406,
      "grad_norm": 5.957961082458496,
      "learning_rate": 1.657788918481383e-05,
      "loss": 1.7269,
      "step": 512000
    },
    {
      "epoch": 40.11436628544571,
      "grad_norm": 5.362010478973389,
      "learning_rate": 1.6571361428795236e-05,
      "loss": 1.6465,
      "step": 512100
    },
    {
      "epoch": 40.122199592668025,
      "grad_norm": 7.527046203613281,
      "learning_rate": 1.6564833672776648e-05,
      "loss": 1.5342,
      "step": 512200
    },
    {
      "epoch": 40.13003289989033,
      "grad_norm": 6.430174350738525,
      "learning_rate": 1.6558305916758057e-05,
      "loss": 1.6344,
      "step": 512300
    },
    {
      "epoch": 40.137866207112644,
      "grad_norm": 6.003640174865723,
      "learning_rate": 1.6551778160739463e-05,
      "loss": 1.658,
      "step": 512400
    },
    {
      "epoch": 40.14569951433495,
      "grad_norm": 6.977646827697754,
      "learning_rate": 1.6545250404720876e-05,
      "loss": 1.6806,
      "step": 512500
    },
    {
      "epoch": 40.15353282155726,
      "grad_norm": 6.646536350250244,
      "learning_rate": 1.653872264870228e-05,
      "loss": 1.5628,
      "step": 512600
    },
    {
      "epoch": 40.16136612877957,
      "grad_norm": 8.8724946975708,
      "learning_rate": 1.653219489268369e-05,
      "loss": 1.6582,
      "step": 512700
    },
    {
      "epoch": 40.16919943600188,
      "grad_norm": 8.029648780822754,
      "learning_rate": 1.6525667136665103e-05,
      "loss": 1.6584,
      "step": 512800
    },
    {
      "epoch": 40.17703274322419,
      "grad_norm": 6.888341426849365,
      "learning_rate": 1.651913938064651e-05,
      "loss": 1.6031,
      "step": 512900
    },
    {
      "epoch": 40.1848660504465,
      "grad_norm": 9.972458839416504,
      "learning_rate": 1.6512611624627918e-05,
      "loss": 1.608,
      "step": 513000
    },
    {
      "epoch": 40.19269935766881,
      "grad_norm": 7.186378479003906,
      "learning_rate": 1.6506083868609327e-05,
      "loss": 1.7051,
      "step": 513100
    },
    {
      "epoch": 40.200532664891114,
      "grad_norm": 6.158236026763916,
      "learning_rate": 1.6499556112590736e-05,
      "loss": 1.5879,
      "step": 513200
    },
    {
      "epoch": 40.20836597211343,
      "grad_norm": 8.0310697555542,
      "learning_rate": 1.6493028356572146e-05,
      "loss": 1.5815,
      "step": 513300
    },
    {
      "epoch": 40.21619927933573,
      "grad_norm": 6.071181774139404,
      "learning_rate": 1.6486500600553555e-05,
      "loss": 1.6511,
      "step": 513400
    },
    {
      "epoch": 40.224032586558046,
      "grad_norm": 5.73724365234375,
      "learning_rate": 1.6479972844534964e-05,
      "loss": 1.7014,
      "step": 513500
    },
    {
      "epoch": 40.23186589378035,
      "grad_norm": 6.7763471603393555,
      "learning_rate": 1.6473445088516373e-05,
      "loss": 1.6933,
      "step": 513600
    },
    {
      "epoch": 40.239699201002665,
      "grad_norm": 6.051180362701416,
      "learning_rate": 1.646691733249778e-05,
      "loss": 1.6845,
      "step": 513700
    },
    {
      "epoch": 40.24753250822497,
      "grad_norm": 6.5350751876831055,
      "learning_rate": 1.646038957647919e-05,
      "loss": 1.631,
      "step": 513800
    },
    {
      "epoch": 40.255365815447284,
      "grad_norm": 6.482514381408691,
      "learning_rate": 1.64538618204606e-05,
      "loss": 1.6149,
      "step": 513900
    },
    {
      "epoch": 40.26319912266959,
      "grad_norm": 5.924928665161133,
      "learning_rate": 1.6447334064442006e-05,
      "loss": 1.6121,
      "step": 514000
    },
    {
      "epoch": 40.2710324298919,
      "grad_norm": 8.338021278381348,
      "learning_rate": 1.644080630842342e-05,
      "loss": 1.6616,
      "step": 514100
    },
    {
      "epoch": 40.27886573711421,
      "grad_norm": 5.65580415725708,
      "learning_rate": 1.6434278552404825e-05,
      "loss": 1.66,
      "step": 514200
    },
    {
      "epoch": 40.286699044336515,
      "grad_norm": 6.22198486328125,
      "learning_rate": 1.6427750796386234e-05,
      "loss": 1.6156,
      "step": 514300
    },
    {
      "epoch": 40.29453235155883,
      "grad_norm": 6.970071792602539,
      "learning_rate": 1.6421223040367646e-05,
      "loss": 1.7381,
      "step": 514400
    },
    {
      "epoch": 40.302365658781135,
      "grad_norm": 6.4056572914123535,
      "learning_rate": 1.6414695284349052e-05,
      "loss": 1.6753,
      "step": 514500
    },
    {
      "epoch": 40.31019896600345,
      "grad_norm": 4.793847560882568,
      "learning_rate": 1.640816752833046e-05,
      "loss": 1.7486,
      "step": 514600
    },
    {
      "epoch": 40.318032273225754,
      "grad_norm": 9.858471870422363,
      "learning_rate": 1.640163977231187e-05,
      "loss": 1.6247,
      "step": 514700
    },
    {
      "epoch": 40.32586558044807,
      "grad_norm": 7.876747131347656,
      "learning_rate": 1.639511201629328e-05,
      "loss": 1.5764,
      "step": 514800
    },
    {
      "epoch": 40.33369888767037,
      "grad_norm": 5.733185291290283,
      "learning_rate": 1.638858426027469e-05,
      "loss": 1.6775,
      "step": 514900
    },
    {
      "epoch": 40.341532194892686,
      "grad_norm": 6.869467735290527,
      "learning_rate": 1.6382056504256098e-05,
      "loss": 1.5729,
      "step": 515000
    },
    {
      "epoch": 40.34936550211499,
      "grad_norm": 7.102451801300049,
      "learning_rate": 1.6375528748237507e-05,
      "loss": 1.71,
      "step": 515100
    },
    {
      "epoch": 40.357198809337305,
      "grad_norm": 6.084489822387695,
      "learning_rate": 1.6369000992218916e-05,
      "loss": 1.6841,
      "step": 515200
    },
    {
      "epoch": 40.36503211655961,
      "grad_norm": 7.337813854217529,
      "learning_rate": 1.6362473236200322e-05,
      "loss": 1.6611,
      "step": 515300
    },
    {
      "epoch": 40.372865423781924,
      "grad_norm": 6.677798271179199,
      "learning_rate": 1.6355945480181735e-05,
      "loss": 1.6859,
      "step": 515400
    },
    {
      "epoch": 40.38069873100423,
      "grad_norm": 5.962549209594727,
      "learning_rate": 1.6349417724163144e-05,
      "loss": 1.6079,
      "step": 515500
    },
    {
      "epoch": 40.388532038226536,
      "grad_norm": 4.275101661682129,
      "learning_rate": 1.634288996814455e-05,
      "loss": 1.715,
      "step": 515600
    },
    {
      "epoch": 40.39636534544885,
      "grad_norm": 7.215025424957275,
      "learning_rate": 1.6336362212125962e-05,
      "loss": 1.665,
      "step": 515700
    },
    {
      "epoch": 40.404198652671155,
      "grad_norm": 6.493987083435059,
      "learning_rate": 1.6329834456107368e-05,
      "loss": 1.618,
      "step": 515800
    },
    {
      "epoch": 40.41203195989347,
      "grad_norm": 5.7045674324035645,
      "learning_rate": 1.6323306700088777e-05,
      "loss": 1.6786,
      "step": 515900
    },
    {
      "epoch": 40.419865267115775,
      "grad_norm": 5.131170272827148,
      "learning_rate": 1.631677894407019e-05,
      "loss": 1.7104,
      "step": 516000
    },
    {
      "epoch": 40.42769857433809,
      "grad_norm": 6.103832721710205,
      "learning_rate": 1.6310251188051595e-05,
      "loss": 1.5888,
      "step": 516100
    },
    {
      "epoch": 40.435531881560394,
      "grad_norm": 6.095661640167236,
      "learning_rate": 1.6303723432033004e-05,
      "loss": 1.5869,
      "step": 516200
    },
    {
      "epoch": 40.44336518878271,
      "grad_norm": 8.581130981445312,
      "learning_rate": 1.6297195676014414e-05,
      "loss": 1.6158,
      "step": 516300
    },
    {
      "epoch": 40.45119849600501,
      "grad_norm": 6.459486484527588,
      "learning_rate": 1.6290667919995823e-05,
      "loss": 1.7328,
      "step": 516400
    },
    {
      "epoch": 40.459031803227326,
      "grad_norm": 5.572443008422852,
      "learning_rate": 1.6284140163977232e-05,
      "loss": 1.6611,
      "step": 516500
    },
    {
      "epoch": 40.46686511044963,
      "grad_norm": 7.108273983001709,
      "learning_rate": 1.627761240795864e-05,
      "loss": 1.6007,
      "step": 516600
    },
    {
      "epoch": 40.47469841767194,
      "grad_norm": 5.774118900299072,
      "learning_rate": 1.627108465194005e-05,
      "loss": 1.6944,
      "step": 516700
    },
    {
      "epoch": 40.48253172489425,
      "grad_norm": 4.960936069488525,
      "learning_rate": 1.626455689592146e-05,
      "loss": 1.6382,
      "step": 516800
    },
    {
      "epoch": 40.49036503211656,
      "grad_norm": 5.714853286743164,
      "learning_rate": 1.6258029139902865e-05,
      "loss": 1.6981,
      "step": 516900
    },
    {
      "epoch": 40.49819833933887,
      "grad_norm": 7.362206935882568,
      "learning_rate": 1.6251501383884278e-05,
      "loss": 1.595,
      "step": 517000
    },
    {
      "epoch": 40.506031646561176,
      "grad_norm": 4.860485553741455,
      "learning_rate": 1.6244973627865687e-05,
      "loss": 1.6048,
      "step": 517100
    },
    {
      "epoch": 40.51386495378349,
      "grad_norm": 5.399504661560059,
      "learning_rate": 1.6238445871847093e-05,
      "loss": 1.6263,
      "step": 517200
    },
    {
      "epoch": 40.521698261005795,
      "grad_norm": 6.083198547363281,
      "learning_rate": 1.6231918115828505e-05,
      "loss": 1.6517,
      "step": 517300
    },
    {
      "epoch": 40.52953156822811,
      "grad_norm": 6.455031871795654,
      "learning_rate": 1.6225390359809914e-05,
      "loss": 1.7204,
      "step": 517400
    },
    {
      "epoch": 40.537364875450415,
      "grad_norm": 6.158466815948486,
      "learning_rate": 1.621886260379132e-05,
      "loss": 1.7792,
      "step": 517500
    },
    {
      "epoch": 40.54519818267273,
      "grad_norm": 8.110943794250488,
      "learning_rate": 1.6212334847772733e-05,
      "loss": 1.6716,
      "step": 517600
    },
    {
      "epoch": 40.553031489895034,
      "grad_norm": 8.062260627746582,
      "learning_rate": 1.620580709175414e-05,
      "loss": 1.5808,
      "step": 517700
    },
    {
      "epoch": 40.56086479711734,
      "grad_norm": 6.650671482086182,
      "learning_rate": 1.6199279335735547e-05,
      "loss": 1.6345,
      "step": 517800
    },
    {
      "epoch": 40.56869810433965,
      "grad_norm": 6.081559658050537,
      "learning_rate": 1.6192751579716957e-05,
      "loss": 1.5987,
      "step": 517900
    },
    {
      "epoch": 40.57653141156196,
      "grad_norm": 8.991271018981934,
      "learning_rate": 1.6186223823698366e-05,
      "loss": 1.6729,
      "step": 518000
    },
    {
      "epoch": 40.58436471878427,
      "grad_norm": 7.495429039001465,
      "learning_rate": 1.6179696067679775e-05,
      "loss": 1.5706,
      "step": 518100
    },
    {
      "epoch": 40.59219802600658,
      "grad_norm": 5.081238746643066,
      "learning_rate": 1.6173168311661184e-05,
      "loss": 1.6732,
      "step": 518200
    },
    {
      "epoch": 40.60003133322889,
      "grad_norm": 7.059936046600342,
      "learning_rate": 1.6166640555642593e-05,
      "loss": 1.6388,
      "step": 518300
    },
    {
      "epoch": 40.6078646404512,
      "grad_norm": 6.76003885269165,
      "learning_rate": 1.6160112799624002e-05,
      "loss": 1.678,
      "step": 518400
    },
    {
      "epoch": 40.61569794767351,
      "grad_norm": 5.611542224884033,
      "learning_rate": 1.6153585043605408e-05,
      "loss": 1.7158,
      "step": 518500
    },
    {
      "epoch": 40.623531254895816,
      "grad_norm": 4.999081611633301,
      "learning_rate": 1.614705728758682e-05,
      "loss": 1.6346,
      "step": 518600
    },
    {
      "epoch": 40.63136456211813,
      "grad_norm": 6.896697521209717,
      "learning_rate": 1.614052953156823e-05,
      "loss": 1.673,
      "step": 518700
    },
    {
      "epoch": 40.639197869340435,
      "grad_norm": 5.5463690757751465,
      "learning_rate": 1.6134001775549636e-05,
      "loss": 1.6322,
      "step": 518800
    },
    {
      "epoch": 40.64703117656274,
      "grad_norm": 6.856935977935791,
      "learning_rate": 1.6127474019531048e-05,
      "loss": 1.6167,
      "step": 518900
    },
    {
      "epoch": 40.654864483785055,
      "grad_norm": 4.735774993896484,
      "learning_rate": 1.6120946263512457e-05,
      "loss": 1.7014,
      "step": 519000
    },
    {
      "epoch": 40.66269779100736,
      "grad_norm": 7.869349002838135,
      "learning_rate": 1.6114418507493863e-05,
      "loss": 1.61,
      "step": 519100
    },
    {
      "epoch": 40.670531098229674,
      "grad_norm": 7.0868000984191895,
      "learning_rate": 1.6107890751475276e-05,
      "loss": 1.5565,
      "step": 519200
    },
    {
      "epoch": 40.67836440545198,
      "grad_norm": 4.6511149406433105,
      "learning_rate": 1.610136299545668e-05,
      "loss": 1.6886,
      "step": 519300
    },
    {
      "epoch": 40.68619771267429,
      "grad_norm": 6.767848491668701,
      "learning_rate": 1.609483523943809e-05,
      "loss": 1.6582,
      "step": 519400
    },
    {
      "epoch": 40.6940310198966,
      "grad_norm": 4.761241436004639,
      "learning_rate": 1.60883074834195e-05,
      "loss": 1.6376,
      "step": 519500
    },
    {
      "epoch": 40.70186432711891,
      "grad_norm": 7.609195232391357,
      "learning_rate": 1.608177972740091e-05,
      "loss": 1.6891,
      "step": 519600
    },
    {
      "epoch": 40.70969763434122,
      "grad_norm": 6.931679725646973,
      "learning_rate": 1.6075251971382318e-05,
      "loss": 1.6114,
      "step": 519700
    },
    {
      "epoch": 40.71753094156353,
      "grad_norm": 5.9153032302856445,
      "learning_rate": 1.6068724215363727e-05,
      "loss": 1.6467,
      "step": 519800
    },
    {
      "epoch": 40.72536424878584,
      "grad_norm": 6.39709997177124,
      "learning_rate": 1.6062196459345136e-05,
      "loss": 1.7645,
      "step": 519900
    },
    {
      "epoch": 40.73319755600814,
      "grad_norm": 6.579895973205566,
      "learning_rate": 1.6055668703326546e-05,
      "loss": 1.7247,
      "step": 520000
    },
    {
      "epoch": 40.741030863230456,
      "grad_norm": 6.719729423522949,
      "learning_rate": 1.604914094730795e-05,
      "loss": 1.5338,
      "step": 520100
    },
    {
      "epoch": 40.74886417045276,
      "grad_norm": 6.528964042663574,
      "learning_rate": 1.6042613191289364e-05,
      "loss": 1.6495,
      "step": 520200
    },
    {
      "epoch": 40.756697477675075,
      "grad_norm": 7.054366111755371,
      "learning_rate": 1.6036085435270773e-05,
      "loss": 1.5795,
      "step": 520300
    },
    {
      "epoch": 40.76453078489738,
      "grad_norm": 6.001204490661621,
      "learning_rate": 1.602955767925218e-05,
      "loss": 1.661,
      "step": 520400
    },
    {
      "epoch": 40.772364092119695,
      "grad_norm": 5.990231037139893,
      "learning_rate": 1.602302992323359e-05,
      "loss": 1.6235,
      "step": 520500
    },
    {
      "epoch": 40.780197399342,
      "grad_norm": 6.154508590698242,
      "learning_rate": 1.6016502167215e-05,
      "loss": 1.589,
      "step": 520600
    },
    {
      "epoch": 40.788030706564314,
      "grad_norm": 7.546328067779541,
      "learning_rate": 1.6009974411196406e-05,
      "loss": 1.699,
      "step": 520700
    },
    {
      "epoch": 40.79586401378662,
      "grad_norm": 5.932715892791748,
      "learning_rate": 1.600344665517782e-05,
      "loss": 1.6455,
      "step": 520800
    },
    {
      "epoch": 40.80369732100893,
      "grad_norm": 6.104176044464111,
      "learning_rate": 1.5996918899159225e-05,
      "loss": 1.6025,
      "step": 520900
    },
    {
      "epoch": 40.81153062823124,
      "grad_norm": 5.628568649291992,
      "learning_rate": 1.5990391143140634e-05,
      "loss": 1.6694,
      "step": 521000
    },
    {
      "epoch": 40.81936393545355,
      "grad_norm": 6.3208231925964355,
      "learning_rate": 1.5983863387122043e-05,
      "loss": 1.6286,
      "step": 521100
    },
    {
      "epoch": 40.82719724267586,
      "grad_norm": 5.019432544708252,
      "learning_rate": 1.5977335631103452e-05,
      "loss": 1.6697,
      "step": 521200
    },
    {
      "epoch": 40.835030549898164,
      "grad_norm": 6.765524864196777,
      "learning_rate": 1.597080787508486e-05,
      "loss": 1.6014,
      "step": 521300
    },
    {
      "epoch": 40.84286385712048,
      "grad_norm": 5.701605319976807,
      "learning_rate": 1.596428011906627e-05,
      "loss": 1.7388,
      "step": 521400
    },
    {
      "epoch": 40.85069716434278,
      "grad_norm": 7.18975305557251,
      "learning_rate": 1.595775236304768e-05,
      "loss": 1.7418,
      "step": 521500
    },
    {
      "epoch": 40.858530471565096,
      "grad_norm": 7.100734710693359,
      "learning_rate": 1.595122460702909e-05,
      "loss": 1.707,
      "step": 521600
    },
    {
      "epoch": 40.8663637787874,
      "grad_norm": 6.021498680114746,
      "learning_rate": 1.5944696851010494e-05,
      "loss": 1.5914,
      "step": 521700
    },
    {
      "epoch": 40.874197086009715,
      "grad_norm": 8.418222427368164,
      "learning_rate": 1.5938169094991907e-05,
      "loss": 1.6169,
      "step": 521800
    },
    {
      "epoch": 40.88203039323202,
      "grad_norm": 7.942999362945557,
      "learning_rate": 1.5931641338973316e-05,
      "loss": 1.769,
      "step": 521900
    },
    {
      "epoch": 40.889863700454335,
      "grad_norm": 7.822787761688232,
      "learning_rate": 1.5925113582954722e-05,
      "loss": 1.6829,
      "step": 522000
    },
    {
      "epoch": 40.89769700767664,
      "grad_norm": 7.497725009918213,
      "learning_rate": 1.5918585826936134e-05,
      "loss": 1.6773,
      "step": 522100
    },
    {
      "epoch": 40.905530314898954,
      "grad_norm": 7.215211391448975,
      "learning_rate": 1.5912058070917544e-05,
      "loss": 1.7319,
      "step": 522200
    },
    {
      "epoch": 40.91336362212126,
      "grad_norm": 6.988643646240234,
      "learning_rate": 1.590553031489895e-05,
      "loss": 1.6296,
      "step": 522300
    },
    {
      "epoch": 40.921196929343566,
      "grad_norm": 4.707725524902344,
      "learning_rate": 1.5899002558880362e-05,
      "loss": 1.7539,
      "step": 522400
    },
    {
      "epoch": 40.92903023656588,
      "grad_norm": 4.651571750640869,
      "learning_rate": 1.589247480286177e-05,
      "loss": 1.6426,
      "step": 522500
    },
    {
      "epoch": 40.936863543788185,
      "grad_norm": 7.2495808601379395,
      "learning_rate": 1.5885947046843177e-05,
      "loss": 1.6498,
      "step": 522600
    },
    {
      "epoch": 40.9446968510105,
      "grad_norm": 7.573060989379883,
      "learning_rate": 1.5879419290824586e-05,
      "loss": 1.5076,
      "step": 522700
    },
    {
      "epoch": 40.952530158232804,
      "grad_norm": 7.160334587097168,
      "learning_rate": 1.5872891534805995e-05,
      "loss": 1.6834,
      "step": 522800
    },
    {
      "epoch": 40.96036346545512,
      "grad_norm": 7.067497730255127,
      "learning_rate": 1.5866363778787404e-05,
      "loss": 1.6721,
      "step": 522900
    },
    {
      "epoch": 40.96819677267742,
      "grad_norm": 8.254907608032227,
      "learning_rate": 1.5859836022768813e-05,
      "loss": 1.6812,
      "step": 523000
    },
    {
      "epoch": 40.976030079899736,
      "grad_norm": 5.743383884429932,
      "learning_rate": 1.5853308266750223e-05,
      "loss": 1.6944,
      "step": 523100
    },
    {
      "epoch": 40.98386338712204,
      "grad_norm": 4.698981761932373,
      "learning_rate": 1.5846780510731632e-05,
      "loss": 1.6915,
      "step": 523200
    },
    {
      "epoch": 40.991696694344355,
      "grad_norm": 9.31033992767334,
      "learning_rate": 1.584025275471304e-05,
      "loss": 1.6572,
      "step": 523300
    },
    {
      "epoch": 40.99953000156666,
      "grad_norm": 6.243048191070557,
      "learning_rate": 1.583372499869445e-05,
      "loss": 1.7018,
      "step": 523400
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.7754814624786377,
      "eval_runtime": 1.5018,
      "eval_samples_per_second": 447.476,
      "eval_steps_per_second": 447.476,
      "step": 523406
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.4045904874801636,
      "eval_runtime": 28.9704,
      "eval_samples_per_second": 440.657,
      "eval_steps_per_second": 440.657,
      "step": 523406
    },
    {
      "epoch": 41.00736330878897,
      "grad_norm": 5.288258075714111,
      "learning_rate": 1.582719724267586e-05,
      "loss": 1.5861,
      "step": 523500
    },
    {
      "epoch": 41.01519661601128,
      "grad_norm": 6.373002529144287,
      "learning_rate": 1.5820669486657265e-05,
      "loss": 1.6001,
      "step": 523600
    },
    {
      "epoch": 41.02302992323359,
      "grad_norm": 6.585043907165527,
      "learning_rate": 1.5814141730638678e-05,
      "loss": 1.5618,
      "step": 523700
    },
    {
      "epoch": 41.0308632304559,
      "grad_norm": 7.313203811645508,
      "learning_rate": 1.5807613974620087e-05,
      "loss": 1.5994,
      "step": 523800
    },
    {
      "epoch": 41.038696537678206,
      "grad_norm": 5.853020668029785,
      "learning_rate": 1.5801086218601492e-05,
      "loss": 1.7622,
      "step": 523900
    },
    {
      "epoch": 41.04652984490052,
      "grad_norm": 7.115742206573486,
      "learning_rate": 1.5794558462582905e-05,
      "loss": 1.73,
      "step": 524000
    },
    {
      "epoch": 41.054363152122825,
      "grad_norm": 7.615276336669922,
      "learning_rate": 1.5788030706564314e-05,
      "loss": 1.6358,
      "step": 524100
    },
    {
      "epoch": 41.06219645934514,
      "grad_norm": 7.419320583343506,
      "learning_rate": 1.578150295054572e-05,
      "loss": 1.5737,
      "step": 524200
    },
    {
      "epoch": 41.070029766567444,
      "grad_norm": 7.609023571014404,
      "learning_rate": 1.577497519452713e-05,
      "loss": 1.5745,
      "step": 524300
    },
    {
      "epoch": 41.07786307378976,
      "grad_norm": 6.177606582641602,
      "learning_rate": 1.5768447438508538e-05,
      "loss": 1.6222,
      "step": 524400
    },
    {
      "epoch": 41.08569638101206,
      "grad_norm": 8.854586601257324,
      "learning_rate": 1.5761919682489947e-05,
      "loss": 1.6313,
      "step": 524500
    },
    {
      "epoch": 41.09352968823437,
      "grad_norm": 7.0653076171875,
      "learning_rate": 1.5755391926471357e-05,
      "loss": 1.5845,
      "step": 524600
    },
    {
      "epoch": 41.10136299545668,
      "grad_norm": 8.776119232177734,
      "learning_rate": 1.5748864170452766e-05,
      "loss": 1.6749,
      "step": 524700
    },
    {
      "epoch": 41.10919630267899,
      "grad_norm": 7.039214611053467,
      "learning_rate": 1.5742336414434175e-05,
      "loss": 1.5669,
      "step": 524800
    },
    {
      "epoch": 41.1170296099013,
      "grad_norm": 7.140871524810791,
      "learning_rate": 1.5735808658415584e-05,
      "loss": 1.6172,
      "step": 524900
    },
    {
      "epoch": 41.12486291712361,
      "grad_norm": 6.680428504943848,
      "learning_rate": 1.5729280902396993e-05,
      "loss": 1.7182,
      "step": 525000
    },
    {
      "epoch": 41.13269622434592,
      "grad_norm": 7.189818859100342,
      "learning_rate": 1.5722753146378402e-05,
      "loss": 1.6194,
      "step": 525100
    },
    {
      "epoch": 41.14052953156823,
      "grad_norm": 6.858285903930664,
      "learning_rate": 1.5716225390359808e-05,
      "loss": 1.6463,
      "step": 525200
    },
    {
      "epoch": 41.14836283879054,
      "grad_norm": 7.544467926025391,
      "learning_rate": 1.570969763434122e-05,
      "loss": 1.5817,
      "step": 525300
    },
    {
      "epoch": 41.156196146012846,
      "grad_norm": 6.390841484069824,
      "learning_rate": 1.570316987832263e-05,
      "loss": 1.6679,
      "step": 525400
    },
    {
      "epoch": 41.16402945323516,
      "grad_norm": 14.859992027282715,
      "learning_rate": 1.5696642122304036e-05,
      "loss": 1.6542,
      "step": 525500
    },
    {
      "epoch": 41.171862760457465,
      "grad_norm": 6.962042808532715,
      "learning_rate": 1.5690114366285448e-05,
      "loss": 1.584,
      "step": 525600
    },
    {
      "epoch": 41.17969606767977,
      "grad_norm": 8.456938743591309,
      "learning_rate": 1.5683586610266857e-05,
      "loss": 1.6233,
      "step": 525700
    },
    {
      "epoch": 41.187529374902084,
      "grad_norm": 8.12782096862793,
      "learning_rate": 1.5677058854248263e-05,
      "loss": 1.6357,
      "step": 525800
    },
    {
      "epoch": 41.19536268212439,
      "grad_norm": 4.671410083770752,
      "learning_rate": 1.5670531098229672e-05,
      "loss": 1.5175,
      "step": 525900
    },
    {
      "epoch": 41.2031959893467,
      "grad_norm": 6.043245792388916,
      "learning_rate": 1.566400334221108e-05,
      "loss": 1.7461,
      "step": 526000
    },
    {
      "epoch": 41.21102929656901,
      "grad_norm": 6.850502014160156,
      "learning_rate": 1.565747558619249e-05,
      "loss": 1.6755,
      "step": 526100
    },
    {
      "epoch": 41.21886260379132,
      "grad_norm": 7.315610885620117,
      "learning_rate": 1.56509478301739e-05,
      "loss": 1.5731,
      "step": 526200
    },
    {
      "epoch": 41.22669591101363,
      "grad_norm": 4.604071140289307,
      "learning_rate": 1.564442007415531e-05,
      "loss": 1.6607,
      "step": 526300
    },
    {
      "epoch": 41.23452921823594,
      "grad_norm": 6.808980941772461,
      "learning_rate": 1.5637892318136718e-05,
      "loss": 1.735,
      "step": 526400
    },
    {
      "epoch": 41.24236252545825,
      "grad_norm": 7.033324718475342,
      "learning_rate": 1.5631364562118127e-05,
      "loss": 1.6487,
      "step": 526500
    },
    {
      "epoch": 41.25019583268056,
      "grad_norm": 6.223521709442139,
      "learning_rate": 1.5624836806099536e-05,
      "loss": 1.6302,
      "step": 526600
    },
    {
      "epoch": 41.25802913990287,
      "grad_norm": 5.812294960021973,
      "learning_rate": 1.5618309050080945e-05,
      "loss": 1.6245,
      "step": 526700
    },
    {
      "epoch": 41.26586244712517,
      "grad_norm": 6.373169422149658,
      "learning_rate": 1.561178129406235e-05,
      "loss": 1.6952,
      "step": 526800
    },
    {
      "epoch": 41.273695754347486,
      "grad_norm": 10.310800552368164,
      "learning_rate": 1.5605253538043764e-05,
      "loss": 1.6247,
      "step": 526900
    },
    {
      "epoch": 41.28152906156979,
      "grad_norm": 7.428092956542969,
      "learning_rate": 1.5598725782025173e-05,
      "loss": 1.6541,
      "step": 527000
    },
    {
      "epoch": 41.289362368792105,
      "grad_norm": 7.236042022705078,
      "learning_rate": 1.559219802600658e-05,
      "loss": 1.5958,
      "step": 527100
    },
    {
      "epoch": 41.29719567601441,
      "grad_norm": 6.333719253540039,
      "learning_rate": 1.558567026998799e-05,
      "loss": 1.6158,
      "step": 527200
    },
    {
      "epoch": 41.305028983236724,
      "grad_norm": 5.617288589477539,
      "learning_rate": 1.55791425139694e-05,
      "loss": 1.6197,
      "step": 527300
    },
    {
      "epoch": 41.31286229045903,
      "grad_norm": 8.319207191467285,
      "learning_rate": 1.5572614757950806e-05,
      "loss": 1.6606,
      "step": 527400
    },
    {
      "epoch": 41.32069559768134,
      "grad_norm": 5.664085865020752,
      "learning_rate": 1.5566087001932215e-05,
      "loss": 1.7472,
      "step": 527500
    },
    {
      "epoch": 41.32852890490365,
      "grad_norm": 6.40560245513916,
      "learning_rate": 1.5559559245913624e-05,
      "loss": 1.6539,
      "step": 527600
    },
    {
      "epoch": 41.33636221212596,
      "grad_norm": 6.197160243988037,
      "learning_rate": 1.5553031489895034e-05,
      "loss": 1.6503,
      "step": 527700
    },
    {
      "epoch": 41.34419551934827,
      "grad_norm": 4.177526950836182,
      "learning_rate": 1.5546503733876443e-05,
      "loss": 1.6678,
      "step": 527800
    },
    {
      "epoch": 41.35202882657058,
      "grad_norm": 8.890961647033691,
      "learning_rate": 1.5539975977857852e-05,
      "loss": 1.6523,
      "step": 527900
    },
    {
      "epoch": 41.35986213379289,
      "grad_norm": 7.144186019897461,
      "learning_rate": 1.553344822183926e-05,
      "loss": 1.6904,
      "step": 528000
    },
    {
      "epoch": 41.367695441015194,
      "grad_norm": 7.139956951141357,
      "learning_rate": 1.552692046582067e-05,
      "loss": 1.6622,
      "step": 528100
    },
    {
      "epoch": 41.37552874823751,
      "grad_norm": 6.315507888793945,
      "learning_rate": 1.552039270980208e-05,
      "loss": 1.6206,
      "step": 528200
    },
    {
      "epoch": 41.38336205545981,
      "grad_norm": 6.125392913818359,
      "learning_rate": 1.551386495378349e-05,
      "loss": 1.7314,
      "step": 528300
    },
    {
      "epoch": 41.391195362682126,
      "grad_norm": 5.093698978424072,
      "learning_rate": 1.5507337197764898e-05,
      "loss": 1.6711,
      "step": 528400
    },
    {
      "epoch": 41.39902866990443,
      "grad_norm": 5.646484851837158,
      "learning_rate": 1.5500809441746307e-05,
      "loss": 1.7129,
      "step": 528500
    },
    {
      "epoch": 41.406861977126745,
      "grad_norm": 4.882418632507324,
      "learning_rate": 1.5494281685727716e-05,
      "loss": 1.5981,
      "step": 528600
    },
    {
      "epoch": 41.41469528434905,
      "grad_norm": 4.218845844268799,
      "learning_rate": 1.5487753929709122e-05,
      "loss": 1.5961,
      "step": 528700
    },
    {
      "epoch": 41.422528591571364,
      "grad_norm": 5.806874752044678,
      "learning_rate": 1.5481226173690534e-05,
      "loss": 1.5595,
      "step": 528800
    },
    {
      "epoch": 41.43036189879367,
      "grad_norm": 7.272171974182129,
      "learning_rate": 1.5474698417671944e-05,
      "loss": 1.6839,
      "step": 528900
    },
    {
      "epoch": 41.43819520601598,
      "grad_norm": 7.694274425506592,
      "learning_rate": 1.546817066165335e-05,
      "loss": 1.716,
      "step": 529000
    },
    {
      "epoch": 41.44602851323829,
      "grad_norm": 7.08233642578125,
      "learning_rate": 1.546164290563476e-05,
      "loss": 1.728,
      "step": 529100
    },
    {
      "epoch": 41.453861820460595,
      "grad_norm": 5.684261798858643,
      "learning_rate": 1.545511514961617e-05,
      "loss": 1.614,
      "step": 529200
    },
    {
      "epoch": 41.46169512768291,
      "grad_norm": 5.507787227630615,
      "learning_rate": 1.5448587393597577e-05,
      "loss": 1.6469,
      "step": 529300
    },
    {
      "epoch": 41.469528434905214,
      "grad_norm": 6.790987491607666,
      "learning_rate": 1.5442059637578986e-05,
      "loss": 1.6961,
      "step": 529400
    },
    {
      "epoch": 41.47736174212753,
      "grad_norm": 7.3947625160217285,
      "learning_rate": 1.5435531881560395e-05,
      "loss": 1.6282,
      "step": 529500
    },
    {
      "epoch": 41.485195049349834,
      "grad_norm": 8.069494247436523,
      "learning_rate": 1.5429004125541804e-05,
      "loss": 1.6463,
      "step": 529600
    },
    {
      "epoch": 41.49302835657215,
      "grad_norm": 5.909555912017822,
      "learning_rate": 1.5422476369523213e-05,
      "loss": 1.686,
      "step": 529700
    },
    {
      "epoch": 41.50086166379445,
      "grad_norm": 7.847078800201416,
      "learning_rate": 1.5415948613504623e-05,
      "loss": 1.5937,
      "step": 529800
    },
    {
      "epoch": 41.508694971016766,
      "grad_norm": 7.830620288848877,
      "learning_rate": 1.5409420857486032e-05,
      "loss": 1.6739,
      "step": 529900
    },
    {
      "epoch": 41.51652827823907,
      "grad_norm": 5.491872787475586,
      "learning_rate": 1.540289310146744e-05,
      "loss": 1.6729,
      "step": 530000
    },
    {
      "epoch": 41.524361585461385,
      "grad_norm": 5.016047954559326,
      "learning_rate": 1.539636534544885e-05,
      "loss": 1.7098,
      "step": 530100
    },
    {
      "epoch": 41.53219489268369,
      "grad_norm": 7.0263471603393555,
      "learning_rate": 1.538983758943026e-05,
      "loss": 1.7017,
      "step": 530200
    },
    {
      "epoch": 41.540028199906,
      "grad_norm": 5.966753959655762,
      "learning_rate": 1.5383309833411665e-05,
      "loss": 1.6819,
      "step": 530300
    },
    {
      "epoch": 41.54786150712831,
      "grad_norm": 7.674476146697998,
      "learning_rate": 1.5376782077393077e-05,
      "loss": 1.6671,
      "step": 530400
    },
    {
      "epoch": 41.555694814350616,
      "grad_norm": 6.575902462005615,
      "learning_rate": 1.5370254321374487e-05,
      "loss": 1.6317,
      "step": 530500
    },
    {
      "epoch": 41.56352812157293,
      "grad_norm": 5.3610639572143555,
      "learning_rate": 1.5363726565355892e-05,
      "loss": 1.6555,
      "step": 530600
    },
    {
      "epoch": 41.571361428795235,
      "grad_norm": 7.305164813995361,
      "learning_rate": 1.53571988093373e-05,
      "loss": 1.6836,
      "step": 530700
    },
    {
      "epoch": 41.57919473601755,
      "grad_norm": 5.676869869232178,
      "learning_rate": 1.5350671053318714e-05,
      "loss": 1.6542,
      "step": 530800
    },
    {
      "epoch": 41.587028043239854,
      "grad_norm": 5.680378437042236,
      "learning_rate": 1.534414329730012e-05,
      "loss": 1.6339,
      "step": 530900
    },
    {
      "epoch": 41.59486135046217,
      "grad_norm": 5.933274269104004,
      "learning_rate": 1.533761554128153e-05,
      "loss": 1.6832,
      "step": 531000
    },
    {
      "epoch": 41.602694657684474,
      "grad_norm": 8.102911949157715,
      "learning_rate": 1.5331087785262938e-05,
      "loss": 1.5752,
      "step": 531100
    },
    {
      "epoch": 41.61052796490679,
      "grad_norm": 5.547482013702393,
      "learning_rate": 1.5324560029244347e-05,
      "loss": 1.6152,
      "step": 531200
    },
    {
      "epoch": 41.61836127212909,
      "grad_norm": 4.606173038482666,
      "learning_rate": 1.5318032273225757e-05,
      "loss": 1.6802,
      "step": 531300
    },
    {
      "epoch": 41.6261945793514,
      "grad_norm": 5.726096153259277,
      "learning_rate": 1.5311504517207166e-05,
      "loss": 1.6921,
      "step": 531400
    },
    {
      "epoch": 41.63402788657371,
      "grad_norm": 7.599339008331299,
      "learning_rate": 1.5304976761188575e-05,
      "loss": 1.6034,
      "step": 531500
    },
    {
      "epoch": 41.64186119379602,
      "grad_norm": 6.857303142547607,
      "learning_rate": 1.5298449005169984e-05,
      "loss": 1.6758,
      "step": 531600
    },
    {
      "epoch": 41.64969450101833,
      "grad_norm": 6.98626708984375,
      "learning_rate": 1.5291921249151393e-05,
      "loss": 1.5793,
      "step": 531700
    },
    {
      "epoch": 41.65752780824064,
      "grad_norm": 7.672294616699219,
      "learning_rate": 1.5285393493132802e-05,
      "loss": 1.7209,
      "step": 531800
    },
    {
      "epoch": 41.66536111546295,
      "grad_norm": 6.5168986320495605,
      "learning_rate": 1.5278865737114208e-05,
      "loss": 1.7019,
      "step": 531900
    },
    {
      "epoch": 41.673194422685256,
      "grad_norm": 5.109098434448242,
      "learning_rate": 1.527233798109562e-05,
      "loss": 1.6184,
      "step": 532000
    },
    {
      "epoch": 41.68102772990757,
      "grad_norm": 7.308300971984863,
      "learning_rate": 1.526581022507703e-05,
      "loss": 1.653,
      "step": 532100
    },
    {
      "epoch": 41.688861037129875,
      "grad_norm": 6.709860324859619,
      "learning_rate": 1.5259282469058436e-05,
      "loss": 1.6103,
      "step": 532200
    },
    {
      "epoch": 41.69669434435219,
      "grad_norm": 6.811485767364502,
      "learning_rate": 1.5252754713039846e-05,
      "loss": 1.6829,
      "step": 532300
    },
    {
      "epoch": 41.704527651574494,
      "grad_norm": 7.174330234527588,
      "learning_rate": 1.5246226957021256e-05,
      "loss": 1.6635,
      "step": 532400
    },
    {
      "epoch": 41.71236095879681,
      "grad_norm": 6.5913004875183105,
      "learning_rate": 1.5239699201002663e-05,
      "loss": 1.692,
      "step": 532500
    },
    {
      "epoch": 41.720194266019114,
      "grad_norm": 5.048369884490967,
      "learning_rate": 1.5233171444984074e-05,
      "loss": 1.7163,
      "step": 532600
    },
    {
      "epoch": 41.72802757324142,
      "grad_norm": 6.621036052703857,
      "learning_rate": 1.5226643688965481e-05,
      "loss": 1.6685,
      "step": 532700
    },
    {
      "epoch": 41.73586088046373,
      "grad_norm": 7.026121139526367,
      "learning_rate": 1.522011593294689e-05,
      "loss": 1.5587,
      "step": 532800
    },
    {
      "epoch": 41.74369418768604,
      "grad_norm": 8.223564147949219,
      "learning_rate": 1.5213588176928301e-05,
      "loss": 1.5979,
      "step": 532900
    },
    {
      "epoch": 41.75152749490835,
      "grad_norm": 4.64029598236084,
      "learning_rate": 1.5207060420909707e-05,
      "loss": 1.6686,
      "step": 533000
    },
    {
      "epoch": 41.75936080213066,
      "grad_norm": 4.5052618980407715,
      "learning_rate": 1.5200532664891118e-05,
      "loss": 1.6227,
      "step": 533100
    },
    {
      "epoch": 41.76719410935297,
      "grad_norm": 4.529797554016113,
      "learning_rate": 1.5194004908872527e-05,
      "loss": 1.6233,
      "step": 533200
    },
    {
      "epoch": 41.77502741657528,
      "grad_norm": 6.200943470001221,
      "learning_rate": 1.5187477152853935e-05,
      "loss": 1.6423,
      "step": 533300
    },
    {
      "epoch": 41.78286072379759,
      "grad_norm": 6.39898681640625,
      "learning_rate": 1.5180949396835345e-05,
      "loss": 1.6339,
      "step": 533400
    },
    {
      "epoch": 41.790694031019896,
      "grad_norm": 6.9665422439575195,
      "learning_rate": 1.5174421640816753e-05,
      "loss": 1.6502,
      "step": 533500
    },
    {
      "epoch": 41.79852733824221,
      "grad_norm": 6.189455986022949,
      "learning_rate": 1.5167893884798162e-05,
      "loss": 1.7268,
      "step": 533600
    },
    {
      "epoch": 41.806360645464515,
      "grad_norm": 6.432456970214844,
      "learning_rate": 1.5161366128779573e-05,
      "loss": 1.7059,
      "step": 533700
    },
    {
      "epoch": 41.81419395268682,
      "grad_norm": 10.235828399658203,
      "learning_rate": 1.5154838372760979e-05,
      "loss": 1.7165,
      "step": 533800
    },
    {
      "epoch": 41.822027259909135,
      "grad_norm": 5.4769978523254395,
      "learning_rate": 1.514831061674239e-05,
      "loss": 1.6632,
      "step": 533900
    },
    {
      "epoch": 41.82986056713144,
      "grad_norm": 7.255345344543457,
      "learning_rate": 1.5141782860723799e-05,
      "loss": 1.6402,
      "step": 534000
    },
    {
      "epoch": 41.837693874353754,
      "grad_norm": 5.010313510894775,
      "learning_rate": 1.5135255104705206e-05,
      "loss": 1.6678,
      "step": 534100
    },
    {
      "epoch": 41.84552718157606,
      "grad_norm": 8.821901321411133,
      "learning_rate": 1.5128727348686617e-05,
      "loss": 1.5805,
      "step": 534200
    },
    {
      "epoch": 41.85336048879837,
      "grad_norm": 5.425909042358398,
      "learning_rate": 1.5122199592668026e-05,
      "loss": 1.6334,
      "step": 534300
    },
    {
      "epoch": 41.86119379602068,
      "grad_norm": 5.931000232696533,
      "learning_rate": 1.5115671836649434e-05,
      "loss": 1.6538,
      "step": 534400
    },
    {
      "epoch": 41.86902710324299,
      "grad_norm": 6.5960845947265625,
      "learning_rate": 1.5109144080630844e-05,
      "loss": 1.637,
      "step": 534500
    },
    {
      "epoch": 41.8768604104653,
      "grad_norm": 6.977649211883545,
      "learning_rate": 1.510261632461225e-05,
      "loss": 1.6807,
      "step": 534600
    },
    {
      "epoch": 41.88469371768761,
      "grad_norm": 7.2912092208862305,
      "learning_rate": 1.5096088568593661e-05,
      "loss": 1.6334,
      "step": 534700
    },
    {
      "epoch": 41.89252702490992,
      "grad_norm": 5.657418251037598,
      "learning_rate": 1.508956081257507e-05,
      "loss": 1.6681,
      "step": 534800
    },
    {
      "epoch": 41.90036033213222,
      "grad_norm": 7.336667537689209,
      "learning_rate": 1.5083033056556478e-05,
      "loss": 1.7204,
      "step": 534900
    },
    {
      "epoch": 41.908193639354536,
      "grad_norm": 7.286803722381592,
      "learning_rate": 1.5076505300537889e-05,
      "loss": 1.697,
      "step": 535000
    },
    {
      "epoch": 41.91602694657684,
      "grad_norm": 5.764598846435547,
      "learning_rate": 1.5069977544519298e-05,
      "loss": 1.7859,
      "step": 535100
    },
    {
      "epoch": 41.923860253799155,
      "grad_norm": 5.761805534362793,
      "learning_rate": 1.5063449788500705e-05,
      "loss": 1.6425,
      "step": 535200
    },
    {
      "epoch": 41.93169356102146,
      "grad_norm": 6.297846794128418,
      "learning_rate": 1.5056922032482116e-05,
      "loss": 1.6537,
      "step": 535300
    },
    {
      "epoch": 41.939526868243775,
      "grad_norm": 6.380642890930176,
      "learning_rate": 1.5050394276463522e-05,
      "loss": 1.6385,
      "step": 535400
    },
    {
      "epoch": 41.94736017546608,
      "grad_norm": 6.079801082611084,
      "learning_rate": 1.5043866520444933e-05,
      "loss": 1.6835,
      "step": 535500
    },
    {
      "epoch": 41.955193482688394,
      "grad_norm": 6.270619869232178,
      "learning_rate": 1.5037338764426342e-05,
      "loss": 1.6395,
      "step": 535600
    },
    {
      "epoch": 41.9630267899107,
      "grad_norm": 6.331121921539307,
      "learning_rate": 1.503081100840775e-05,
      "loss": 1.6186,
      "step": 535700
    },
    {
      "epoch": 41.97086009713301,
      "grad_norm": 7.322159767150879,
      "learning_rate": 1.502428325238916e-05,
      "loss": 1.5854,
      "step": 535800
    },
    {
      "epoch": 41.97869340435532,
      "grad_norm": 7.420041561126709,
      "learning_rate": 1.501775549637057e-05,
      "loss": 1.6502,
      "step": 535900
    },
    {
      "epoch": 41.986526711577625,
      "grad_norm": 8.191802978515625,
      "learning_rate": 1.5011227740351977e-05,
      "loss": 1.6315,
      "step": 536000
    },
    {
      "epoch": 41.99436001879994,
      "grad_norm": 5.546645164489746,
      "learning_rate": 1.5004699984333388e-05,
      "loss": 1.6545,
      "step": 536100
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.7764012813568115,
      "eval_runtime": 1.5457,
      "eval_samples_per_second": 434.75,
      "eval_steps_per_second": 434.75,
      "step": 536172
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.400647759437561,
      "eval_runtime": 28.7788,
      "eval_samples_per_second": 443.59,
      "eval_steps_per_second": 443.59,
      "step": 536172
    },
    {
      "epoch": 42.002193326022244,
      "grad_norm": 6.5015692710876465,
      "learning_rate": 1.4998172228314793e-05,
      "loss": 1.6083,
      "step": 536200
    },
    {
      "epoch": 42.01002663324456,
      "grad_norm": 3.8701932430267334,
      "learning_rate": 1.4991644472296204e-05,
      "loss": 1.6598,
      "step": 536300
    },
    {
      "epoch": 42.01785994046686,
      "grad_norm": 7.092289447784424,
      "learning_rate": 1.4985116716277613e-05,
      "loss": 1.6588,
      "step": 536400
    },
    {
      "epoch": 42.025693247689176,
      "grad_norm": 5.291107177734375,
      "learning_rate": 1.497858896025902e-05,
      "loss": 1.6204,
      "step": 536500
    },
    {
      "epoch": 42.03352655491148,
      "grad_norm": 6.217799663543701,
      "learning_rate": 1.4972061204240432e-05,
      "loss": 1.6455,
      "step": 536600
    },
    {
      "epoch": 42.041359862133795,
      "grad_norm": 5.9302520751953125,
      "learning_rate": 1.496553344822184e-05,
      "loss": 1.6617,
      "step": 536700
    },
    {
      "epoch": 42.0491931693561,
      "grad_norm": 6.891948223114014,
      "learning_rate": 1.4959005692203248e-05,
      "loss": 1.6406,
      "step": 536800
    },
    {
      "epoch": 42.057026476578415,
      "grad_norm": 7.473586082458496,
      "learning_rate": 1.4952477936184659e-05,
      "loss": 1.6279,
      "step": 536900
    },
    {
      "epoch": 42.06485978380072,
      "grad_norm": 5.888340473175049,
      "learning_rate": 1.4945950180166065e-05,
      "loss": 1.6773,
      "step": 537000
    },
    {
      "epoch": 42.07269309102303,
      "grad_norm": 6.484455585479736,
      "learning_rate": 1.4939422424147476e-05,
      "loss": 1.6312,
      "step": 537100
    },
    {
      "epoch": 42.08052639824534,
      "grad_norm": 5.369904041290283,
      "learning_rate": 1.4932894668128885e-05,
      "loss": 1.6767,
      "step": 537200
    },
    {
      "epoch": 42.088359705467646,
      "grad_norm": 8.385430335998535,
      "learning_rate": 1.4926366912110292e-05,
      "loss": 1.6525,
      "step": 537300
    },
    {
      "epoch": 42.09619301268996,
      "grad_norm": 4.3282318115234375,
      "learning_rate": 1.4919839156091703e-05,
      "loss": 1.6879,
      "step": 537400
    },
    {
      "epoch": 42.104026319912265,
      "grad_norm": 7.635977745056152,
      "learning_rate": 1.4913311400073112e-05,
      "loss": 1.7437,
      "step": 537500
    },
    {
      "epoch": 42.11185962713458,
      "grad_norm": 5.472428798675537,
      "learning_rate": 1.490678364405452e-05,
      "loss": 1.6615,
      "step": 537600
    },
    {
      "epoch": 42.119692934356884,
      "grad_norm": 6.0465874671936035,
      "learning_rate": 1.490025588803593e-05,
      "loss": 1.6146,
      "step": 537700
    },
    {
      "epoch": 42.1275262415792,
      "grad_norm": 5.323846817016602,
      "learning_rate": 1.4893728132017336e-05,
      "loss": 1.5174,
      "step": 537800
    },
    {
      "epoch": 42.1353595488015,
      "grad_norm": 7.443435192108154,
      "learning_rate": 1.4887200375998747e-05,
      "loss": 1.6673,
      "step": 537900
    },
    {
      "epoch": 42.143192856023816,
      "grad_norm": 5.8714680671691895,
      "learning_rate": 1.4880672619980156e-05,
      "loss": 1.5366,
      "step": 538000
    },
    {
      "epoch": 42.15102616324612,
      "grad_norm": 6.7782368659973145,
      "learning_rate": 1.4874144863961564e-05,
      "loss": 1.5999,
      "step": 538100
    },
    {
      "epoch": 42.15885947046843,
      "grad_norm": 8.8309326171875,
      "learning_rate": 1.4867617107942975e-05,
      "loss": 1.6844,
      "step": 538200
    },
    {
      "epoch": 42.16669277769074,
      "grad_norm": 5.5627241134643555,
      "learning_rate": 1.4861089351924384e-05,
      "loss": 1.6945,
      "step": 538300
    },
    {
      "epoch": 42.17452608491305,
      "grad_norm": 5.536362648010254,
      "learning_rate": 1.4854561595905791e-05,
      "loss": 1.6267,
      "step": 538400
    },
    {
      "epoch": 42.18235939213536,
      "grad_norm": 6.861382484436035,
      "learning_rate": 1.4848033839887202e-05,
      "loss": 1.7083,
      "step": 538500
    },
    {
      "epoch": 42.19019269935767,
      "grad_norm": 6.225449562072754,
      "learning_rate": 1.4841506083868608e-05,
      "loss": 1.6423,
      "step": 538600
    },
    {
      "epoch": 42.19802600657998,
      "grad_norm": 4.6985297203063965,
      "learning_rate": 1.4834978327850019e-05,
      "loss": 1.6212,
      "step": 538700
    },
    {
      "epoch": 42.205859313802286,
      "grad_norm": 8.595359802246094,
      "learning_rate": 1.4828450571831428e-05,
      "loss": 1.6282,
      "step": 538800
    },
    {
      "epoch": 42.2136926210246,
      "grad_norm": 6.5129852294921875,
      "learning_rate": 1.4821922815812835e-05,
      "loss": 1.7139,
      "step": 538900
    },
    {
      "epoch": 42.221525928246905,
      "grad_norm": 5.557201862335205,
      "learning_rate": 1.4815395059794246e-05,
      "loss": 1.6958,
      "step": 539000
    },
    {
      "epoch": 42.22935923546922,
      "grad_norm": 6.199084758758545,
      "learning_rate": 1.4808867303775655e-05,
      "loss": 1.7356,
      "step": 539100
    },
    {
      "epoch": 42.237192542691524,
      "grad_norm": 6.768163681030273,
      "learning_rate": 1.4802339547757063e-05,
      "loss": 1.5967,
      "step": 539200
    },
    {
      "epoch": 42.24502584991384,
      "grad_norm": 6.135941028594971,
      "learning_rate": 1.4795811791738474e-05,
      "loss": 1.6787,
      "step": 539300
    },
    {
      "epoch": 42.25285915713614,
      "grad_norm": 5.900644779205322,
      "learning_rate": 1.478928403571988e-05,
      "loss": 1.5851,
      "step": 539400
    },
    {
      "epoch": 42.26069246435845,
      "grad_norm": 5.2933549880981445,
      "learning_rate": 1.478275627970129e-05,
      "loss": 1.7042,
      "step": 539500
    },
    {
      "epoch": 42.26852577158076,
      "grad_norm": 5.196659088134766,
      "learning_rate": 1.47762285236827e-05,
      "loss": 1.6067,
      "step": 539600
    },
    {
      "epoch": 42.27635907880307,
      "grad_norm": 6.821725368499756,
      "learning_rate": 1.4769700767664107e-05,
      "loss": 1.5874,
      "step": 539700
    },
    {
      "epoch": 42.28419238602538,
      "grad_norm": 9.522319793701172,
      "learning_rate": 1.4763173011645518e-05,
      "loss": 1.6963,
      "step": 539800
    },
    {
      "epoch": 42.29202569324769,
      "grad_norm": 7.798415184020996,
      "learning_rate": 1.4756645255626927e-05,
      "loss": 1.6064,
      "step": 539900
    },
    {
      "epoch": 42.29985900047,
      "grad_norm": 5.128242492675781,
      "learning_rate": 1.4750117499608334e-05,
      "loss": 1.5548,
      "step": 540000
    },
    {
      "epoch": 42.30769230769231,
      "grad_norm": 7.074860095977783,
      "learning_rate": 1.4743589743589745e-05,
      "loss": 1.7409,
      "step": 540100
    },
    {
      "epoch": 42.31552561491462,
      "grad_norm": 7.314104080200195,
      "learning_rate": 1.4737061987571151e-05,
      "loss": 1.6144,
      "step": 540200
    },
    {
      "epoch": 42.323358922136926,
      "grad_norm": 7.3682475090026855,
      "learning_rate": 1.4730534231552562e-05,
      "loss": 1.5075,
      "step": 540300
    },
    {
      "epoch": 42.33119222935924,
      "grad_norm": 3.696537733078003,
      "learning_rate": 1.4724006475533971e-05,
      "loss": 1.5842,
      "step": 540400
    },
    {
      "epoch": 42.339025536581545,
      "grad_norm": 8.325136184692383,
      "learning_rate": 1.4717478719515379e-05,
      "loss": 1.755,
      "step": 540500
    },
    {
      "epoch": 42.34685884380385,
      "grad_norm": 7.834619998931885,
      "learning_rate": 1.471095096349679e-05,
      "loss": 1.6469,
      "step": 540600
    },
    {
      "epoch": 42.354692151026164,
      "grad_norm": 5.565808296203613,
      "learning_rate": 1.4704423207478199e-05,
      "loss": 1.6403,
      "step": 540700
    },
    {
      "epoch": 42.36252545824847,
      "grad_norm": 6.315858364105225,
      "learning_rate": 1.4697895451459606e-05,
      "loss": 1.6244,
      "step": 540800
    },
    {
      "epoch": 42.37035876547078,
      "grad_norm": 6.848046779632568,
      "learning_rate": 1.4691367695441017e-05,
      "loss": 1.6207,
      "step": 540900
    },
    {
      "epoch": 42.37819207269309,
      "grad_norm": 5.3589582443237305,
      "learning_rate": 1.4684839939422426e-05,
      "loss": 1.6204,
      "step": 541000
    },
    {
      "epoch": 42.3860253799154,
      "grad_norm": 6.079220771789551,
      "learning_rate": 1.4678312183403834e-05,
      "loss": 1.5907,
      "step": 541100
    },
    {
      "epoch": 42.39385868713771,
      "grad_norm": 5.6803154945373535,
      "learning_rate": 1.4671784427385243e-05,
      "loss": 1.7734,
      "step": 541200
    },
    {
      "epoch": 42.40169199436002,
      "grad_norm": 7.642117500305176,
      "learning_rate": 1.466525667136665e-05,
      "loss": 1.6809,
      "step": 541300
    },
    {
      "epoch": 42.40952530158233,
      "grad_norm": 4.629001140594482,
      "learning_rate": 1.4658728915348061e-05,
      "loss": 1.5966,
      "step": 541400
    },
    {
      "epoch": 42.41735860880464,
      "grad_norm": 6.521772384643555,
      "learning_rate": 1.465220115932947e-05,
      "loss": 1.5544,
      "step": 541500
    },
    {
      "epoch": 42.42519191602695,
      "grad_norm": 7.074463367462158,
      "learning_rate": 1.4645673403310878e-05,
      "loss": 1.7468,
      "step": 541600
    },
    {
      "epoch": 42.43302522324925,
      "grad_norm": 6.99595832824707,
      "learning_rate": 1.4639145647292288e-05,
      "loss": 1.6778,
      "step": 541700
    },
    {
      "epoch": 42.440858530471566,
      "grad_norm": 9.148112297058105,
      "learning_rate": 1.4632617891273698e-05,
      "loss": 1.6911,
      "step": 541800
    },
    {
      "epoch": 42.44869183769387,
      "grad_norm": 6.287863254547119,
      "learning_rate": 1.4626090135255105e-05,
      "loss": 1.6732,
      "step": 541900
    },
    {
      "epoch": 42.456525144916185,
      "grad_norm": 5.82510232925415,
      "learning_rate": 1.4619562379236514e-05,
      "loss": 1.5988,
      "step": 542000
    },
    {
      "epoch": 42.46435845213849,
      "grad_norm": 5.877793312072754,
      "learning_rate": 1.4613034623217922e-05,
      "loss": 1.6124,
      "step": 542100
    },
    {
      "epoch": 42.472191759360804,
      "grad_norm": 7.679119110107422,
      "learning_rate": 1.4606506867199333e-05,
      "loss": 1.6358,
      "step": 542200
    },
    {
      "epoch": 42.48002506658311,
      "grad_norm": 8.009096145629883,
      "learning_rate": 1.4599979111180742e-05,
      "loss": 1.7132,
      "step": 542300
    },
    {
      "epoch": 42.48785837380542,
      "grad_norm": 5.40360164642334,
      "learning_rate": 1.459345135516215e-05,
      "loss": 1.6031,
      "step": 542400
    },
    {
      "epoch": 42.49569168102773,
      "grad_norm": 6.359516620635986,
      "learning_rate": 1.458692359914356e-05,
      "loss": 1.6119,
      "step": 542500
    },
    {
      "epoch": 42.50352498825004,
      "grad_norm": 10.105914115905762,
      "learning_rate": 1.458039584312497e-05,
      "loss": 1.6719,
      "step": 542600
    },
    {
      "epoch": 42.51135829547235,
      "grad_norm": 5.908161640167236,
      "learning_rate": 1.4573868087106377e-05,
      "loss": 1.5423,
      "step": 542700
    },
    {
      "epoch": 42.519191602694654,
      "grad_norm": 6.45525598526001,
      "learning_rate": 1.4567340331087786e-05,
      "loss": 1.5852,
      "step": 542800
    },
    {
      "epoch": 42.52702490991697,
      "grad_norm": 6.555942535400391,
      "learning_rate": 1.4560812575069193e-05,
      "loss": 1.5453,
      "step": 542900
    },
    {
      "epoch": 42.53485821713927,
      "grad_norm": 4.477598667144775,
      "learning_rate": 1.4554284819050604e-05,
      "loss": 1.538,
      "step": 543000
    },
    {
      "epoch": 42.54269152436159,
      "grad_norm": 6.4003472328186035,
      "learning_rate": 1.4547757063032013e-05,
      "loss": 1.6597,
      "step": 543100
    },
    {
      "epoch": 42.55052483158389,
      "grad_norm": 5.278556823730469,
      "learning_rate": 1.454122930701342e-05,
      "loss": 1.5966,
      "step": 543200
    },
    {
      "epoch": 42.558358138806206,
      "grad_norm": 4.457127094268799,
      "learning_rate": 1.4534701550994832e-05,
      "loss": 1.6325,
      "step": 543300
    },
    {
      "epoch": 42.56619144602851,
      "grad_norm": 6.593593120574951,
      "learning_rate": 1.452817379497624e-05,
      "loss": 1.6703,
      "step": 543400
    },
    {
      "epoch": 42.574024753250825,
      "grad_norm": 5.075165748596191,
      "learning_rate": 1.4521646038957648e-05,
      "loss": 1.635,
      "step": 543500
    },
    {
      "epoch": 42.58185806047313,
      "grad_norm": 7.00200891494751,
      "learning_rate": 1.4515118282939057e-05,
      "loss": 1.5464,
      "step": 543600
    },
    {
      "epoch": 42.589691367695444,
      "grad_norm": 7.490413188934326,
      "learning_rate": 1.4508590526920465e-05,
      "loss": 1.6717,
      "step": 543700
    },
    {
      "epoch": 42.59752467491775,
      "grad_norm": 7.146188735961914,
      "learning_rate": 1.4502062770901876e-05,
      "loss": 1.8194,
      "step": 543800
    },
    {
      "epoch": 42.605357982140056,
      "grad_norm": 7.236843585968018,
      "learning_rate": 1.4495535014883285e-05,
      "loss": 1.6785,
      "step": 543900
    },
    {
      "epoch": 42.61319128936237,
      "grad_norm": 5.477669715881348,
      "learning_rate": 1.4489007258864692e-05,
      "loss": 1.5928,
      "step": 544000
    },
    {
      "epoch": 42.621024596584675,
      "grad_norm": 6.1541619300842285,
      "learning_rate": 1.4482479502846103e-05,
      "loss": 1.5689,
      "step": 544100
    },
    {
      "epoch": 42.62885790380699,
      "grad_norm": 8.030254364013672,
      "learning_rate": 1.4475951746827512e-05,
      "loss": 1.6911,
      "step": 544200
    },
    {
      "epoch": 42.636691211029294,
      "grad_norm": 8.267600059509277,
      "learning_rate": 1.446942399080892e-05,
      "loss": 1.5643,
      "step": 544300
    },
    {
      "epoch": 42.64452451825161,
      "grad_norm": 7.455018520355225,
      "learning_rate": 1.4462896234790329e-05,
      "loss": 1.738,
      "step": 544400
    },
    {
      "epoch": 42.65235782547391,
      "grad_norm": 10.342214584350586,
      "learning_rate": 1.4456368478771736e-05,
      "loss": 1.6177,
      "step": 544500
    },
    {
      "epoch": 42.66019113269623,
      "grad_norm": 5.952691555023193,
      "learning_rate": 1.4449840722753147e-05,
      "loss": 1.7283,
      "step": 544600
    },
    {
      "epoch": 42.66802443991853,
      "grad_norm": 5.859338283538818,
      "learning_rate": 1.4443312966734556e-05,
      "loss": 1.6396,
      "step": 544700
    },
    {
      "epoch": 42.675857747140846,
      "grad_norm": 6.073835849761963,
      "learning_rate": 1.4436785210715964e-05,
      "loss": 1.5476,
      "step": 544800
    },
    {
      "epoch": 42.68369105436315,
      "grad_norm": 7.443109035491943,
      "learning_rate": 1.4430257454697375e-05,
      "loss": 1.6989,
      "step": 544900
    },
    {
      "epoch": 42.691524361585465,
      "grad_norm": 6.511122226715088,
      "learning_rate": 1.4423729698678784e-05,
      "loss": 1.6915,
      "step": 545000
    },
    {
      "epoch": 42.69935766880777,
      "grad_norm": 6.460758209228516,
      "learning_rate": 1.4417201942660191e-05,
      "loss": 1.6198,
      "step": 545100
    },
    {
      "epoch": 42.70719097603008,
      "grad_norm": 6.049631118774414,
      "learning_rate": 1.44106741866416e-05,
      "loss": 1.611,
      "step": 545200
    },
    {
      "epoch": 42.71502428325239,
      "grad_norm": 9.73098373413086,
      "learning_rate": 1.4404146430623008e-05,
      "loss": 1.6696,
      "step": 545300
    },
    {
      "epoch": 42.722857590474696,
      "grad_norm": 5.714992046356201,
      "learning_rate": 1.4397618674604419e-05,
      "loss": 1.752,
      "step": 545400
    },
    {
      "epoch": 42.73069089769701,
      "grad_norm": 5.200297832489014,
      "learning_rate": 1.4391090918585828e-05,
      "loss": 1.6575,
      "step": 545500
    },
    {
      "epoch": 42.738524204919315,
      "grad_norm": 6.359674453735352,
      "learning_rate": 1.4384563162567235e-05,
      "loss": 1.5722,
      "step": 545600
    },
    {
      "epoch": 42.74635751214163,
      "grad_norm": 6.346898078918457,
      "learning_rate": 1.4378035406548646e-05,
      "loss": 1.6311,
      "step": 545700
    },
    {
      "epoch": 42.754190819363934,
      "grad_norm": 6.690434455871582,
      "learning_rate": 1.4371507650530055e-05,
      "loss": 1.6066,
      "step": 545800
    },
    {
      "epoch": 42.76202412658625,
      "grad_norm": 7.949891090393066,
      "learning_rate": 1.4364979894511463e-05,
      "loss": 1.677,
      "step": 545900
    },
    {
      "epoch": 42.76985743380855,
      "grad_norm": 9.618274688720703,
      "learning_rate": 1.4358452138492872e-05,
      "loss": 1.7287,
      "step": 546000
    },
    {
      "epoch": 42.77769074103087,
      "grad_norm": 6.590265274047852,
      "learning_rate": 1.435192438247428e-05,
      "loss": 1.6628,
      "step": 546100
    },
    {
      "epoch": 42.78552404825317,
      "grad_norm": 7.380544662475586,
      "learning_rate": 1.434539662645569e-05,
      "loss": 1.6525,
      "step": 546200
    },
    {
      "epoch": 42.79335735547548,
      "grad_norm": 7.041524410247803,
      "learning_rate": 1.43388688704371e-05,
      "loss": 1.7002,
      "step": 546300
    },
    {
      "epoch": 42.80119066269779,
      "grad_norm": 5.124875068664551,
      "learning_rate": 1.4332341114418507e-05,
      "loss": 1.586,
      "step": 546400
    },
    {
      "epoch": 42.8090239699201,
      "grad_norm": 8.500016212463379,
      "learning_rate": 1.4325813358399918e-05,
      "loss": 1.7223,
      "step": 546500
    },
    {
      "epoch": 42.81685727714241,
      "grad_norm": 6.8124494552612305,
      "learning_rate": 1.4319285602381327e-05,
      "loss": 1.7111,
      "step": 546600
    },
    {
      "epoch": 42.82469058436472,
      "grad_norm": 6.46771764755249,
      "learning_rate": 1.4312757846362734e-05,
      "loss": 1.6252,
      "step": 546700
    },
    {
      "epoch": 42.83252389158703,
      "grad_norm": 6.417212963104248,
      "learning_rate": 1.4306230090344144e-05,
      "loss": 1.7081,
      "step": 546800
    },
    {
      "epoch": 42.840357198809336,
      "grad_norm": 5.9207682609558105,
      "learning_rate": 1.4299702334325554e-05,
      "loss": 1.6041,
      "step": 546900
    },
    {
      "epoch": 42.84819050603165,
      "grad_norm": 4.97233772277832,
      "learning_rate": 1.4293174578306962e-05,
      "loss": 1.7046,
      "step": 547000
    },
    {
      "epoch": 42.856023813253955,
      "grad_norm": 5.57540225982666,
      "learning_rate": 1.4286646822288371e-05,
      "loss": 1.6461,
      "step": 547100
    },
    {
      "epoch": 42.86385712047627,
      "grad_norm": 6.959863185882568,
      "learning_rate": 1.4280119066269779e-05,
      "loss": 1.5641,
      "step": 547200
    },
    {
      "epoch": 42.871690427698574,
      "grad_norm": 5.91579532623291,
      "learning_rate": 1.427359131025119e-05,
      "loss": 1.7236,
      "step": 547300
    },
    {
      "epoch": 42.87952373492088,
      "grad_norm": 8.474043846130371,
      "learning_rate": 1.4267063554232599e-05,
      "loss": 1.6944,
      "step": 547400
    },
    {
      "epoch": 42.887357042143194,
      "grad_norm": 5.680886745452881,
      "learning_rate": 1.4260535798214006e-05,
      "loss": 1.6593,
      "step": 547500
    },
    {
      "epoch": 42.8951903493655,
      "grad_norm": 6.927678108215332,
      "learning_rate": 1.4254008042195415e-05,
      "loss": 1.6593,
      "step": 547600
    },
    {
      "epoch": 42.90302365658781,
      "grad_norm": 7.360616683959961,
      "learning_rate": 1.4247480286176826e-05,
      "loss": 1.6344,
      "step": 547700
    },
    {
      "epoch": 42.91085696381012,
      "grad_norm": 6.13829231262207,
      "learning_rate": 1.4240952530158233e-05,
      "loss": 1.7093,
      "step": 547800
    },
    {
      "epoch": 42.91869027103243,
      "grad_norm": 7.6829915046691895,
      "learning_rate": 1.4234424774139643e-05,
      "loss": 1.8299,
      "step": 547900
    },
    {
      "epoch": 42.92652357825474,
      "grad_norm": 6.839879512786865,
      "learning_rate": 1.422789701812105e-05,
      "loss": 1.6678,
      "step": 548000
    },
    {
      "epoch": 42.93435688547705,
      "grad_norm": 7.6694183349609375,
      "learning_rate": 1.4221369262102461e-05,
      "loss": 1.6071,
      "step": 548100
    },
    {
      "epoch": 42.94219019269936,
      "grad_norm": 8.649506568908691,
      "learning_rate": 1.421484150608387e-05,
      "loss": 1.6901,
      "step": 548200
    },
    {
      "epoch": 42.95002349992167,
      "grad_norm": 7.0340070724487305,
      "learning_rate": 1.4208313750065278e-05,
      "loss": 1.6288,
      "step": 548300
    },
    {
      "epoch": 42.957856807143976,
      "grad_norm": 8.282894134521484,
      "learning_rate": 1.4201785994046687e-05,
      "loss": 1.6254,
      "step": 548400
    },
    {
      "epoch": 42.96569011436628,
      "grad_norm": 6.074650287628174,
      "learning_rate": 1.4195258238028098e-05,
      "loss": 1.6133,
      "step": 548500
    },
    {
      "epoch": 42.973523421588595,
      "grad_norm": 6.020152568817139,
      "learning_rate": 1.4188730482009505e-05,
      "loss": 1.5915,
      "step": 548600
    },
    {
      "epoch": 42.9813567288109,
      "grad_norm": 6.250034809112549,
      "learning_rate": 1.4182202725990914e-05,
      "loss": 1.5777,
      "step": 548700
    },
    {
      "epoch": 42.989190036033214,
      "grad_norm": 4.955681324005127,
      "learning_rate": 1.4175674969972322e-05,
      "loss": 1.7025,
      "step": 548800
    },
    {
      "epoch": 42.99702334325552,
      "grad_norm": 6.20505428314209,
      "learning_rate": 1.4169147213953732e-05,
      "loss": 1.7171,
      "step": 548900
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.7729156017303467,
      "eval_runtime": 1.583,
      "eval_samples_per_second": 424.51,
      "eval_steps_per_second": 424.51,
      "step": 548938
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.3986107110977173,
      "eval_runtime": 29.3933,
      "eval_samples_per_second": 434.317,
      "eval_steps_per_second": 434.317,
      "step": 548938
    },
    {
      "epoch": 43.004856650477834,
      "grad_norm": 4.1286420822143555,
      "learning_rate": 1.4162619457935142e-05,
      "loss": 1.6805,
      "step": 549000
    },
    {
      "epoch": 43.01268995770014,
      "grad_norm": 6.132596969604492,
      "learning_rate": 1.4156091701916549e-05,
      "loss": 1.6781,
      "step": 549100
    },
    {
      "epoch": 43.02052326492245,
      "grad_norm": 7.859146595001221,
      "learning_rate": 1.4149563945897958e-05,
      "loss": 1.6913,
      "step": 549200
    },
    {
      "epoch": 43.02835657214476,
      "grad_norm": 4.463287353515625,
      "learning_rate": 1.4143036189879369e-05,
      "loss": 1.5724,
      "step": 549300
    },
    {
      "epoch": 43.03618987936707,
      "grad_norm": 4.962738037109375,
      "learning_rate": 1.4136508433860777e-05,
      "loss": 1.5853,
      "step": 549400
    },
    {
      "epoch": 43.04402318658938,
      "grad_norm": 4.859294891357422,
      "learning_rate": 1.4129980677842186e-05,
      "loss": 1.6681,
      "step": 549500
    },
    {
      "epoch": 43.051856493811684,
      "grad_norm": 7.155933856964111,
      "learning_rate": 1.4123452921823593e-05,
      "loss": 1.5374,
      "step": 549600
    },
    {
      "epoch": 43.059689801034,
      "grad_norm": 7.3850297927856445,
      "learning_rate": 1.4116925165805004e-05,
      "loss": 1.6021,
      "step": 549700
    },
    {
      "epoch": 43.0675231082563,
      "grad_norm": 4.959650039672852,
      "learning_rate": 1.4110397409786413e-05,
      "loss": 1.5934,
      "step": 549800
    },
    {
      "epoch": 43.075356415478616,
      "grad_norm": 6.147825241088867,
      "learning_rate": 1.410386965376782e-05,
      "loss": 1.6222,
      "step": 549900
    },
    {
      "epoch": 43.08318972270092,
      "grad_norm": 6.165607452392578,
      "learning_rate": 1.409734189774923e-05,
      "loss": 1.7208,
      "step": 550000
    },
    {
      "epoch": 43.091023029923235,
      "grad_norm": 6.72637939453125,
      "learning_rate": 1.409081414173064e-05,
      "loss": 1.655,
      "step": 550100
    },
    {
      "epoch": 43.09885633714554,
      "grad_norm": 6.210076332092285,
      "learning_rate": 1.4084286385712048e-05,
      "loss": 1.5679,
      "step": 550200
    },
    {
      "epoch": 43.106689644367854,
      "grad_norm": 5.554610252380371,
      "learning_rate": 1.4077758629693457e-05,
      "loss": 1.6739,
      "step": 550300
    },
    {
      "epoch": 43.11452295159016,
      "grad_norm": 5.485045433044434,
      "learning_rate": 1.4071230873674865e-05,
      "loss": 1.5902,
      "step": 550400
    },
    {
      "epoch": 43.122356258812474,
      "grad_norm": 4.764103412628174,
      "learning_rate": 1.4064703117656276e-05,
      "loss": 1.6854,
      "step": 550500
    },
    {
      "epoch": 43.13018956603478,
      "grad_norm": 8.291638374328613,
      "learning_rate": 1.4058175361637685e-05,
      "loss": 1.6745,
      "step": 550600
    },
    {
      "epoch": 43.138022873257086,
      "grad_norm": 8.084808349609375,
      "learning_rate": 1.4051647605619092e-05,
      "loss": 1.685,
      "step": 550700
    },
    {
      "epoch": 43.1458561804794,
      "grad_norm": 7.546998500823975,
      "learning_rate": 1.4045119849600501e-05,
      "loss": 1.7071,
      "step": 550800
    },
    {
      "epoch": 43.153689487701705,
      "grad_norm": 7.032942295074463,
      "learning_rate": 1.4038592093581912e-05,
      "loss": 1.5969,
      "step": 550900
    },
    {
      "epoch": 43.16152279492402,
      "grad_norm": 5.465444564819336,
      "learning_rate": 1.403206433756332e-05,
      "loss": 1.6668,
      "step": 551000
    },
    {
      "epoch": 43.169356102146324,
      "grad_norm": 6.206048011779785,
      "learning_rate": 1.4025536581544729e-05,
      "loss": 1.596,
      "step": 551100
    },
    {
      "epoch": 43.17718940936864,
      "grad_norm": 6.037168025970459,
      "learning_rate": 1.4019008825526136e-05,
      "loss": 1.6244,
      "step": 551200
    },
    {
      "epoch": 43.18502271659094,
      "grad_norm": 5.666195392608643,
      "learning_rate": 1.4012481069507547e-05,
      "loss": 1.6407,
      "step": 551300
    },
    {
      "epoch": 43.192856023813256,
      "grad_norm": 6.701074123382568,
      "learning_rate": 1.4005953313488956e-05,
      "loss": 1.6282,
      "step": 551400
    },
    {
      "epoch": 43.20068933103556,
      "grad_norm": 7.7139410972595215,
      "learning_rate": 1.3999425557470364e-05,
      "loss": 1.6766,
      "step": 551500
    },
    {
      "epoch": 43.208522638257875,
      "grad_norm": 6.715250015258789,
      "learning_rate": 1.3992897801451773e-05,
      "loss": 1.6852,
      "step": 551600
    },
    {
      "epoch": 43.21635594548018,
      "grad_norm": 6.64898681640625,
      "learning_rate": 1.3986370045433184e-05,
      "loss": 1.5995,
      "step": 551700
    },
    {
      "epoch": 43.224189252702494,
      "grad_norm": 6.984548568725586,
      "learning_rate": 1.3979842289414591e-05,
      "loss": 1.6782,
      "step": 551800
    },
    {
      "epoch": 43.2320225599248,
      "grad_norm": 4.9777750968933105,
      "learning_rate": 1.3973314533396e-05,
      "loss": 1.6699,
      "step": 551900
    },
    {
      "epoch": 43.23985586714711,
      "grad_norm": 7.307971477508545,
      "learning_rate": 1.3966786777377408e-05,
      "loss": 1.6189,
      "step": 552000
    },
    {
      "epoch": 43.24768917436942,
      "grad_norm": 6.440977096557617,
      "learning_rate": 1.3960259021358819e-05,
      "loss": 1.6262,
      "step": 552100
    },
    {
      "epoch": 43.255522481591726,
      "grad_norm": 6.184187889099121,
      "learning_rate": 1.3953731265340228e-05,
      "loss": 1.5387,
      "step": 552200
    },
    {
      "epoch": 43.26335578881404,
      "grad_norm": 5.752367973327637,
      "learning_rate": 1.3947203509321635e-05,
      "loss": 1.6213,
      "step": 552300
    },
    {
      "epoch": 43.271189096036345,
      "grad_norm": 4.928745269775391,
      "learning_rate": 1.3940675753303044e-05,
      "loss": 1.5891,
      "step": 552400
    },
    {
      "epoch": 43.27902240325866,
      "grad_norm": 5.511805534362793,
      "learning_rate": 1.3934147997284455e-05,
      "loss": 1.5971,
      "step": 552500
    },
    {
      "epoch": 43.286855710480964,
      "grad_norm": 6.032497882843018,
      "learning_rate": 1.3927620241265863e-05,
      "loss": 1.6097,
      "step": 552600
    },
    {
      "epoch": 43.29468901770328,
      "grad_norm": 6.113102912902832,
      "learning_rate": 1.3921092485247272e-05,
      "loss": 1.6318,
      "step": 552700
    },
    {
      "epoch": 43.30252232492558,
      "grad_norm": 5.710521697998047,
      "learning_rate": 1.3914564729228683e-05,
      "loss": 1.6464,
      "step": 552800
    },
    {
      "epoch": 43.310355632147896,
      "grad_norm": 6.594756126403809,
      "learning_rate": 1.390803697321009e-05,
      "loss": 1.5863,
      "step": 552900
    },
    {
      "epoch": 43.3181889393702,
      "grad_norm": 11.427727699279785,
      "learning_rate": 1.39015092171915e-05,
      "loss": 1.7103,
      "step": 553000
    },
    {
      "epoch": 43.32602224659251,
      "grad_norm": 7.163048267364502,
      "learning_rate": 1.3894981461172907e-05,
      "loss": 1.5899,
      "step": 553100
    },
    {
      "epoch": 43.33385555381482,
      "grad_norm": 4.722680568695068,
      "learning_rate": 1.3888453705154316e-05,
      "loss": 1.7016,
      "step": 553200
    },
    {
      "epoch": 43.34168886103713,
      "grad_norm": 5.040211200714111,
      "learning_rate": 1.3881925949135727e-05,
      "loss": 1.6874,
      "step": 553300
    },
    {
      "epoch": 43.34952216825944,
      "grad_norm": 5.8470048904418945,
      "learning_rate": 1.3875398193117134e-05,
      "loss": 1.644,
      "step": 553400
    },
    {
      "epoch": 43.35735547548175,
      "grad_norm": 7.601113796234131,
      "learning_rate": 1.3868870437098544e-05,
      "loss": 1.6402,
      "step": 553500
    },
    {
      "epoch": 43.36518878270406,
      "grad_norm": 6.044227123260498,
      "learning_rate": 1.3862342681079954e-05,
      "loss": 1.6348,
      "step": 553600
    },
    {
      "epoch": 43.373022089926366,
      "grad_norm": 7.000268936157227,
      "learning_rate": 1.3855814925061362e-05,
      "loss": 1.7726,
      "step": 553700
    },
    {
      "epoch": 43.38085539714868,
      "grad_norm": 6.349696636199951,
      "learning_rate": 1.3849287169042771e-05,
      "loss": 1.7534,
      "step": 553800
    },
    {
      "epoch": 43.388688704370985,
      "grad_norm": 5.539776802062988,
      "learning_rate": 1.3842759413024178e-05,
      "loss": 1.5634,
      "step": 553900
    },
    {
      "epoch": 43.3965220115933,
      "grad_norm": 6.053438663482666,
      "learning_rate": 1.3836231657005588e-05,
      "loss": 1.6299,
      "step": 554000
    },
    {
      "epoch": 43.404355318815604,
      "grad_norm": 5.176630973815918,
      "learning_rate": 1.3829703900986998e-05,
      "loss": 1.6492,
      "step": 554100
    },
    {
      "epoch": 43.41218862603791,
      "grad_norm": 5.556693077087402,
      "learning_rate": 1.3823176144968406e-05,
      "loss": 1.5506,
      "step": 554200
    },
    {
      "epoch": 43.42002193326022,
      "grad_norm": 5.338263034820557,
      "learning_rate": 1.3816648388949815e-05,
      "loss": 1.5906,
      "step": 554300
    },
    {
      "epoch": 43.42785524048253,
      "grad_norm": 6.597352027893066,
      "learning_rate": 1.3810120632931226e-05,
      "loss": 1.6842,
      "step": 554400
    },
    {
      "epoch": 43.43568854770484,
      "grad_norm": 9.187067031860352,
      "learning_rate": 1.3803592876912633e-05,
      "loss": 1.7376,
      "step": 554500
    },
    {
      "epoch": 43.44352185492715,
      "grad_norm": 6.695196628570557,
      "learning_rate": 1.3797065120894043e-05,
      "loss": 1.6313,
      "step": 554600
    },
    {
      "epoch": 43.45135516214946,
      "grad_norm": 6.577307224273682,
      "learning_rate": 1.379053736487545e-05,
      "loss": 1.7442,
      "step": 554700
    },
    {
      "epoch": 43.45918846937177,
      "grad_norm": 7.052733898162842,
      "learning_rate": 1.378400960885686e-05,
      "loss": 1.6567,
      "step": 554800
    },
    {
      "epoch": 43.46702177659408,
      "grad_norm": 7.393036842346191,
      "learning_rate": 1.377748185283827e-05,
      "loss": 1.5524,
      "step": 554900
    },
    {
      "epoch": 43.47485508381639,
      "grad_norm": 6.655355930328369,
      "learning_rate": 1.3770954096819677e-05,
      "loss": 1.6875,
      "step": 555000
    },
    {
      "epoch": 43.4826883910387,
      "grad_norm": 8.74329662322998,
      "learning_rate": 1.3764426340801087e-05,
      "loss": 1.6392,
      "step": 555100
    },
    {
      "epoch": 43.490521698261006,
      "grad_norm": 6.720222473144531,
      "learning_rate": 1.3757898584782497e-05,
      "loss": 1.7042,
      "step": 555200
    },
    {
      "epoch": 43.49835500548331,
      "grad_norm": 6.526832103729248,
      "learning_rate": 1.3751370828763905e-05,
      "loss": 1.6567,
      "step": 555300
    },
    {
      "epoch": 43.506188312705625,
      "grad_norm": 6.782805442810059,
      "learning_rate": 1.3744843072745314e-05,
      "loss": 1.6441,
      "step": 555400
    },
    {
      "epoch": 43.51402161992793,
      "grad_norm": 7.147798538208008,
      "learning_rate": 1.3738315316726722e-05,
      "loss": 1.6282,
      "step": 555500
    },
    {
      "epoch": 43.521854927150244,
      "grad_norm": 7.352962970733643,
      "learning_rate": 1.373178756070813e-05,
      "loss": 1.7273,
      "step": 555600
    },
    {
      "epoch": 43.52968823437255,
      "grad_norm": 4.038611888885498,
      "learning_rate": 1.3725259804689542e-05,
      "loss": 1.7151,
      "step": 555700
    },
    {
      "epoch": 43.53752154159486,
      "grad_norm": 8.271448135375977,
      "learning_rate": 1.3718732048670949e-05,
      "loss": 1.7434,
      "step": 555800
    },
    {
      "epoch": 43.54535484881717,
      "grad_norm": 6.356660842895508,
      "learning_rate": 1.3712204292652358e-05,
      "loss": 1.5127,
      "step": 555900
    },
    {
      "epoch": 43.55318815603948,
      "grad_norm": 7.442627906799316,
      "learning_rate": 1.3705676536633769e-05,
      "loss": 1.5952,
      "step": 556000
    },
    {
      "epoch": 43.56102146326179,
      "grad_norm": 5.982226848602295,
      "learning_rate": 1.3699148780615176e-05,
      "loss": 1.5786,
      "step": 556100
    },
    {
      "epoch": 43.5688547704841,
      "grad_norm": 6.6106414794921875,
      "learning_rate": 1.3692621024596586e-05,
      "loss": 1.7372,
      "step": 556200
    },
    {
      "epoch": 43.57668807770641,
      "grad_norm": 7.165831089019775,
      "learning_rate": 1.3686093268577993e-05,
      "loss": 1.6614,
      "step": 556300
    },
    {
      "epoch": 43.58452138492871,
      "grad_norm": 7.13639497756958,
      "learning_rate": 1.3679565512559402e-05,
      "loss": 1.6155,
      "step": 556400
    },
    {
      "epoch": 43.59235469215103,
      "grad_norm": 6.172730922698975,
      "learning_rate": 1.3673037756540813e-05,
      "loss": 1.6363,
      "step": 556500
    },
    {
      "epoch": 43.60018799937333,
      "grad_norm": 5.634725093841553,
      "learning_rate": 1.366651000052222e-05,
      "loss": 1.6427,
      "step": 556600
    },
    {
      "epoch": 43.608021306595646,
      "grad_norm": 6.210888385772705,
      "learning_rate": 1.365998224450363e-05,
      "loss": 1.7041,
      "step": 556700
    },
    {
      "epoch": 43.61585461381795,
      "grad_norm": 5.88292121887207,
      "learning_rate": 1.365345448848504e-05,
      "loss": 1.5776,
      "step": 556800
    },
    {
      "epoch": 43.623687921040265,
      "grad_norm": 6.060051918029785,
      "learning_rate": 1.3646926732466448e-05,
      "loss": 1.6275,
      "step": 556900
    },
    {
      "epoch": 43.63152122826257,
      "grad_norm": 7.325675010681152,
      "learning_rate": 1.3640398976447857e-05,
      "loss": 1.699,
      "step": 557000
    },
    {
      "epoch": 43.639354535484884,
      "grad_norm": 5.949163436889648,
      "learning_rate": 1.3633871220429265e-05,
      "loss": 1.619,
      "step": 557100
    },
    {
      "epoch": 43.64718784270719,
      "grad_norm": 6.152415752410889,
      "learning_rate": 1.3627343464410674e-05,
      "loss": 1.6821,
      "step": 557200
    },
    {
      "epoch": 43.6550211499295,
      "grad_norm": 7.283843517303467,
      "learning_rate": 1.3620815708392085e-05,
      "loss": 1.6518,
      "step": 557300
    },
    {
      "epoch": 43.66285445715181,
      "grad_norm": 4.744487762451172,
      "learning_rate": 1.3614287952373492e-05,
      "loss": 1.6159,
      "step": 557400
    },
    {
      "epoch": 43.67068776437412,
      "grad_norm": 6.952725410461426,
      "learning_rate": 1.3607760196354901e-05,
      "loss": 1.6726,
      "step": 557500
    },
    {
      "epoch": 43.67852107159643,
      "grad_norm": 7.3688530921936035,
      "learning_rate": 1.3601232440336312e-05,
      "loss": 1.7101,
      "step": 557600
    },
    {
      "epoch": 43.686354378818734,
      "grad_norm": 4.956141948699951,
      "learning_rate": 1.359470468431772e-05,
      "loss": 1.6471,
      "step": 557700
    },
    {
      "epoch": 43.69418768604105,
      "grad_norm": 6.322798728942871,
      "learning_rate": 1.3588176928299129e-05,
      "loss": 1.6208,
      "step": 557800
    },
    {
      "epoch": 43.70202099326335,
      "grad_norm": 6.367140293121338,
      "learning_rate": 1.3581649172280536e-05,
      "loss": 1.5709,
      "step": 557900
    },
    {
      "epoch": 43.70985430048567,
      "grad_norm": 6.1845221519470215,
      "learning_rate": 1.3575121416261945e-05,
      "loss": 1.6248,
      "step": 558000
    },
    {
      "epoch": 43.71768760770797,
      "grad_norm": 7.373555660247803,
      "learning_rate": 1.3568593660243356e-05,
      "loss": 1.7493,
      "step": 558100
    },
    {
      "epoch": 43.725520914930286,
      "grad_norm": 5.3368048667907715,
      "learning_rate": 1.3562065904224764e-05,
      "loss": 1.6278,
      "step": 558200
    },
    {
      "epoch": 43.73335422215259,
      "grad_norm": 6.855421543121338,
      "learning_rate": 1.3555538148206173e-05,
      "loss": 1.5641,
      "step": 558300
    },
    {
      "epoch": 43.741187529374905,
      "grad_norm": 6.5515642166137695,
      "learning_rate": 1.3549010392187584e-05,
      "loss": 1.5704,
      "step": 558400
    },
    {
      "epoch": 43.74902083659721,
      "grad_norm": 6.2858567237854,
      "learning_rate": 1.3542482636168991e-05,
      "loss": 1.6651,
      "step": 558500
    },
    {
      "epoch": 43.756854143819524,
      "grad_norm": 5.198268890380859,
      "learning_rate": 1.35359548801504e-05,
      "loss": 1.6725,
      "step": 558600
    },
    {
      "epoch": 43.76468745104183,
      "grad_norm": 5.060476779937744,
      "learning_rate": 1.3529427124131811e-05,
      "loss": 1.6551,
      "step": 558700
    },
    {
      "epoch": 43.772520758264136,
      "grad_norm": 7.561342239379883,
      "learning_rate": 1.3522899368113217e-05,
      "loss": 1.6377,
      "step": 558800
    },
    {
      "epoch": 43.78035406548645,
      "grad_norm": 7.570928573608398,
      "learning_rate": 1.3516371612094628e-05,
      "loss": 1.613,
      "step": 558900
    },
    {
      "epoch": 43.788187372708755,
      "grad_norm": 5.282495021820068,
      "learning_rate": 1.3509843856076035e-05,
      "loss": 1.6285,
      "step": 559000
    },
    {
      "epoch": 43.79602067993107,
      "grad_norm": 7.165212631225586,
      "learning_rate": 1.3503316100057444e-05,
      "loss": 1.6823,
      "step": 559100
    },
    {
      "epoch": 43.803853987153374,
      "grad_norm": 7.886323928833008,
      "learning_rate": 1.3496788344038855e-05,
      "loss": 1.579,
      "step": 559200
    },
    {
      "epoch": 43.81168729437569,
      "grad_norm": 6.042444705963135,
      "learning_rate": 1.3490260588020263e-05,
      "loss": 1.6839,
      "step": 559300
    },
    {
      "epoch": 43.81952060159799,
      "grad_norm": 5.711961269378662,
      "learning_rate": 1.3483732832001672e-05,
      "loss": 1.6569,
      "step": 559400
    },
    {
      "epoch": 43.82735390882031,
      "grad_norm": 5.868766784667969,
      "learning_rate": 1.3477205075983083e-05,
      "loss": 1.6345,
      "step": 559500
    },
    {
      "epoch": 43.83518721604261,
      "grad_norm": 6.218331813812256,
      "learning_rate": 1.3470677319964489e-05,
      "loss": 1.6429,
      "step": 559600
    },
    {
      "epoch": 43.843020523264926,
      "grad_norm": 7.0681633949279785,
      "learning_rate": 1.34641495639459e-05,
      "loss": 1.7202,
      "step": 559700
    },
    {
      "epoch": 43.85085383048723,
      "grad_norm": 5.6561665534973145,
      "learning_rate": 1.3457621807927307e-05,
      "loss": 1.6607,
      "step": 559800
    },
    {
      "epoch": 43.85868713770954,
      "grad_norm": 6.799452781677246,
      "learning_rate": 1.3451094051908716e-05,
      "loss": 1.7199,
      "step": 559900
    },
    {
      "epoch": 43.86652044493185,
      "grad_norm": 5.096371650695801,
      "learning_rate": 1.3444566295890127e-05,
      "loss": 1.6533,
      "step": 560000
    },
    {
      "epoch": 43.87435375215416,
      "grad_norm": 8.975003242492676,
      "learning_rate": 1.3438038539871534e-05,
      "loss": 1.6552,
      "step": 560100
    },
    {
      "epoch": 43.88218705937647,
      "grad_norm": 6.441169261932373,
      "learning_rate": 1.3431510783852943e-05,
      "loss": 1.6063,
      "step": 560200
    },
    {
      "epoch": 43.890020366598776,
      "grad_norm": 5.466777324676514,
      "learning_rate": 1.3424983027834354e-05,
      "loss": 1.7106,
      "step": 560300
    },
    {
      "epoch": 43.89785367382109,
      "grad_norm": 5.878749847412109,
      "learning_rate": 1.341845527181576e-05,
      "loss": 1.7134,
      "step": 560400
    },
    {
      "epoch": 43.905686981043395,
      "grad_norm": 7.269510269165039,
      "learning_rate": 1.3411927515797171e-05,
      "loss": 1.5921,
      "step": 560500
    },
    {
      "epoch": 43.91352028826571,
      "grad_norm": 7.470027923583984,
      "learning_rate": 1.3405399759778578e-05,
      "loss": 1.6746,
      "step": 560600
    },
    {
      "epoch": 43.921353595488014,
      "grad_norm": 5.881045341491699,
      "learning_rate": 1.3398872003759988e-05,
      "loss": 1.5546,
      "step": 560700
    },
    {
      "epoch": 43.92918690271033,
      "grad_norm": 7.611062049865723,
      "learning_rate": 1.3392344247741398e-05,
      "loss": 1.7903,
      "step": 560800
    },
    {
      "epoch": 43.93702020993263,
      "grad_norm": 5.446566581726074,
      "learning_rate": 1.3385816491722806e-05,
      "loss": 1.7087,
      "step": 560900
    },
    {
      "epoch": 43.94485351715494,
      "grad_norm": 6.730710506439209,
      "learning_rate": 1.3379288735704215e-05,
      "loss": 1.6302,
      "step": 561000
    },
    {
      "epoch": 43.95268682437725,
      "grad_norm": 6.302209854125977,
      "learning_rate": 1.3372760979685626e-05,
      "loss": 1.5814,
      "step": 561100
    },
    {
      "epoch": 43.96052013159956,
      "grad_norm": 6.421390056610107,
      "learning_rate": 1.3366233223667032e-05,
      "loss": 1.6677,
      "step": 561200
    },
    {
      "epoch": 43.96835343882187,
      "grad_norm": 5.770300388336182,
      "learning_rate": 1.3359705467648442e-05,
      "loss": 1.618,
      "step": 561300
    },
    {
      "epoch": 43.97618674604418,
      "grad_norm": 6.371960639953613,
      "learning_rate": 1.335317771162985e-05,
      "loss": 1.6543,
      "step": 561400
    },
    {
      "epoch": 43.98402005326649,
      "grad_norm": 6.393773555755615,
      "learning_rate": 1.3346649955611259e-05,
      "loss": 1.7174,
      "step": 561500
    },
    {
      "epoch": 43.9918533604888,
      "grad_norm": 10.807486534118652,
      "learning_rate": 1.334012219959267e-05,
      "loss": 1.6241,
      "step": 561600
    },
    {
      "epoch": 43.99968666771111,
      "grad_norm": 6.9861836433410645,
      "learning_rate": 1.3333594443574077e-05,
      "loss": 1.6884,
      "step": 561700
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.7737517356872559,
      "eval_runtime": 1.5525,
      "eval_samples_per_second": 432.851,
      "eval_steps_per_second": 432.851,
      "step": 561704
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.3966450691223145,
      "eval_runtime": 29.3899,
      "eval_samples_per_second": 434.367,
      "eval_steps_per_second": 434.367,
      "step": 561704
    },
    {
      "epoch": 44.007519974933416,
      "grad_norm": 7.234401226043701,
      "learning_rate": 1.3327066687555487e-05,
      "loss": 1.6289,
      "step": 561800
    },
    {
      "epoch": 44.01535328215573,
      "grad_norm": 5.233829975128174,
      "learning_rate": 1.3320538931536897e-05,
      "loss": 1.6685,
      "step": 561900
    },
    {
      "epoch": 44.023186589378035,
      "grad_norm": 6.887497901916504,
      "learning_rate": 1.3314011175518303e-05,
      "loss": 1.5838,
      "step": 562000
    },
    {
      "epoch": 44.03101989660034,
      "grad_norm": 9.803681373596191,
      "learning_rate": 1.3307483419499714e-05,
      "loss": 1.6511,
      "step": 562100
    },
    {
      "epoch": 44.038853203822654,
      "grad_norm": 7.499837875366211,
      "learning_rate": 1.3300955663481121e-05,
      "loss": 1.589,
      "step": 562200
    },
    {
      "epoch": 44.04668651104496,
      "grad_norm": 6.071097373962402,
      "learning_rate": 1.329442790746253e-05,
      "loss": 1.6338,
      "step": 562300
    },
    {
      "epoch": 44.05451981826727,
      "grad_norm": 6.01518440246582,
      "learning_rate": 1.3287900151443941e-05,
      "loss": 1.7093,
      "step": 562400
    },
    {
      "epoch": 44.06235312548958,
      "grad_norm": 6.166517734527588,
      "learning_rate": 1.3281372395425349e-05,
      "loss": 1.5872,
      "step": 562500
    },
    {
      "epoch": 44.07018643271189,
      "grad_norm": 6.78246545791626,
      "learning_rate": 1.3274844639406758e-05,
      "loss": 1.7051,
      "step": 562600
    },
    {
      "epoch": 44.0780197399342,
      "grad_norm": 5.948806285858154,
      "learning_rate": 1.3268316883388169e-05,
      "loss": 1.6234,
      "step": 562700
    },
    {
      "epoch": 44.08585304715651,
      "grad_norm": 7.694138526916504,
      "learning_rate": 1.3261789127369575e-05,
      "loss": 1.5721,
      "step": 562800
    },
    {
      "epoch": 44.09368635437882,
      "grad_norm": 5.540808200836182,
      "learning_rate": 1.3255261371350986e-05,
      "loss": 1.5831,
      "step": 562900
    },
    {
      "epoch": 44.10151966160113,
      "grad_norm": 5.299178600311279,
      "learning_rate": 1.3248733615332393e-05,
      "loss": 1.6778,
      "step": 563000
    },
    {
      "epoch": 44.10935296882344,
      "grad_norm": 4.5063700675964355,
      "learning_rate": 1.3242205859313802e-05,
      "loss": 1.5638,
      "step": 563100
    },
    {
      "epoch": 44.11718627604575,
      "grad_norm": 7.016480922698975,
      "learning_rate": 1.3235678103295213e-05,
      "loss": 1.6789,
      "step": 563200
    },
    {
      "epoch": 44.125019583268056,
      "grad_norm": 5.71115255355835,
      "learning_rate": 1.322915034727662e-05,
      "loss": 1.6649,
      "step": 563300
    },
    {
      "epoch": 44.13285289049036,
      "grad_norm": 4.1783528327941895,
      "learning_rate": 1.322262259125803e-05,
      "loss": 1.6054,
      "step": 563400
    },
    {
      "epoch": 44.140686197712675,
      "grad_norm": 8.429932594299316,
      "learning_rate": 1.321609483523944e-05,
      "loss": 1.7582,
      "step": 563500
    },
    {
      "epoch": 44.14851950493498,
      "grad_norm": 10.720866203308105,
      "learning_rate": 1.3209567079220846e-05,
      "loss": 1.5891,
      "step": 563600
    },
    {
      "epoch": 44.156352812157294,
      "grad_norm": 7.5076985359191895,
      "learning_rate": 1.3203039323202257e-05,
      "loss": 1.691,
      "step": 563700
    },
    {
      "epoch": 44.1641861193796,
      "grad_norm": 5.332090854644775,
      "learning_rate": 1.3196511567183665e-05,
      "loss": 1.5845,
      "step": 563800
    },
    {
      "epoch": 44.17201942660191,
      "grad_norm": 7.560540199279785,
      "learning_rate": 1.3189983811165074e-05,
      "loss": 1.6641,
      "step": 563900
    },
    {
      "epoch": 44.17985273382422,
      "grad_norm": 8.138752937316895,
      "learning_rate": 1.3183456055146485e-05,
      "loss": 1.6004,
      "step": 564000
    },
    {
      "epoch": 44.18768604104653,
      "grad_norm": 5.401037216186523,
      "learning_rate": 1.3176928299127892e-05,
      "loss": 1.5646,
      "step": 564100
    },
    {
      "epoch": 44.19551934826884,
      "grad_norm": 6.037656784057617,
      "learning_rate": 1.3170400543109301e-05,
      "loss": 1.7158,
      "step": 564200
    },
    {
      "epoch": 44.20335265549115,
      "grad_norm": 6.101730823516846,
      "learning_rate": 1.3163872787090712e-05,
      "loss": 1.6116,
      "step": 564300
    },
    {
      "epoch": 44.21118596271346,
      "grad_norm": 5.405995845794678,
      "learning_rate": 1.3157345031072118e-05,
      "loss": 1.6378,
      "step": 564400
    },
    {
      "epoch": 44.219019269935764,
      "grad_norm": 6.471457004547119,
      "learning_rate": 1.3150817275053529e-05,
      "loss": 1.5306,
      "step": 564500
    },
    {
      "epoch": 44.22685257715808,
      "grad_norm": 5.679694652557373,
      "learning_rate": 1.3144289519034938e-05,
      "loss": 1.6416,
      "step": 564600
    },
    {
      "epoch": 44.23468588438038,
      "grad_norm": 6.290844917297363,
      "learning_rate": 1.3137761763016345e-05,
      "loss": 1.6336,
      "step": 564700
    },
    {
      "epoch": 44.242519191602696,
      "grad_norm": 6.68294095993042,
      "learning_rate": 1.3131234006997756e-05,
      "loss": 1.7314,
      "step": 564800
    },
    {
      "epoch": 44.250352498825,
      "grad_norm": 5.9314422607421875,
      "learning_rate": 1.3124706250979164e-05,
      "loss": 1.6444,
      "step": 564900
    },
    {
      "epoch": 44.258185806047315,
      "grad_norm": 5.632862091064453,
      "learning_rate": 1.3118178494960573e-05,
      "loss": 1.5969,
      "step": 565000
    },
    {
      "epoch": 44.26601911326962,
      "grad_norm": 11.373150825500488,
      "learning_rate": 1.3111650738941984e-05,
      "loss": 1.6245,
      "step": 565100
    },
    {
      "epoch": 44.273852420491934,
      "grad_norm": 8.696601867675781,
      "learning_rate": 1.310512298292339e-05,
      "loss": 1.6728,
      "step": 565200
    },
    {
      "epoch": 44.28168572771424,
      "grad_norm": 6.3039631843566895,
      "learning_rate": 1.30985952269048e-05,
      "loss": 1.5562,
      "step": 565300
    },
    {
      "epoch": 44.28951903493655,
      "grad_norm": 6.01578950881958,
      "learning_rate": 1.309206747088621e-05,
      "loss": 1.649,
      "step": 565400
    },
    {
      "epoch": 44.29735234215886,
      "grad_norm": 7.826879024505615,
      "learning_rate": 1.3085539714867617e-05,
      "loss": 1.6662,
      "step": 565500
    },
    {
      "epoch": 44.305185649381166,
      "grad_norm": 6.689350605010986,
      "learning_rate": 1.3079011958849028e-05,
      "loss": 1.5413,
      "step": 565600
    },
    {
      "epoch": 44.31301895660348,
      "grad_norm": 7.163825511932373,
      "learning_rate": 1.3072484202830435e-05,
      "loss": 1.6309,
      "step": 565700
    },
    {
      "epoch": 44.320852263825785,
      "grad_norm": 5.645066261291504,
      "learning_rate": 1.3065956446811844e-05,
      "loss": 1.6519,
      "step": 565800
    },
    {
      "epoch": 44.3286855710481,
      "grad_norm": 8.173077583312988,
      "learning_rate": 1.3059428690793255e-05,
      "loss": 1.5827,
      "step": 565900
    },
    {
      "epoch": 44.336518878270404,
      "grad_norm": 5.795448303222656,
      "learning_rate": 1.3052900934774661e-05,
      "loss": 1.6229,
      "step": 566000
    },
    {
      "epoch": 44.34435218549272,
      "grad_norm": 7.254110336303711,
      "learning_rate": 1.3046373178756072e-05,
      "loss": 1.6134,
      "step": 566100
    },
    {
      "epoch": 44.35218549271502,
      "grad_norm": 7.263813495635986,
      "learning_rate": 1.3039845422737481e-05,
      "loss": 1.6767,
      "step": 566200
    },
    {
      "epoch": 44.360018799937336,
      "grad_norm": 6.990516662597656,
      "learning_rate": 1.3033317666718888e-05,
      "loss": 1.7074,
      "step": 566300
    },
    {
      "epoch": 44.36785210715964,
      "grad_norm": 6.849332809448242,
      "learning_rate": 1.30267899107003e-05,
      "loss": 1.6015,
      "step": 566400
    },
    {
      "epoch": 44.375685414381955,
      "grad_norm": 5.780101299285889,
      "learning_rate": 1.3020262154681707e-05,
      "loss": 1.7051,
      "step": 566500
    },
    {
      "epoch": 44.38351872160426,
      "grad_norm": 4.934586048126221,
      "learning_rate": 1.3013734398663116e-05,
      "loss": 1.7076,
      "step": 566600
    },
    {
      "epoch": 44.39135202882657,
      "grad_norm": 5.329050064086914,
      "learning_rate": 1.3007206642644527e-05,
      "loss": 1.6489,
      "step": 566700
    },
    {
      "epoch": 44.39918533604888,
      "grad_norm": 6.003519535064697,
      "learning_rate": 1.3000678886625933e-05,
      "loss": 1.7239,
      "step": 566800
    },
    {
      "epoch": 44.407018643271186,
      "grad_norm": 7.368763446807861,
      "learning_rate": 1.2994151130607343e-05,
      "loss": 1.6828,
      "step": 566900
    },
    {
      "epoch": 44.4148519504935,
      "grad_norm": 5.910543918609619,
      "learning_rate": 1.2987623374588753e-05,
      "loss": 1.639,
      "step": 567000
    },
    {
      "epoch": 44.422685257715806,
      "grad_norm": 5.849328517913818,
      "learning_rate": 1.298109561857016e-05,
      "loss": 1.621,
      "step": 567100
    },
    {
      "epoch": 44.43051856493812,
      "grad_norm": 7.010167121887207,
      "learning_rate": 1.297456786255157e-05,
      "loss": 1.7062,
      "step": 567200
    },
    {
      "epoch": 44.438351872160425,
      "grad_norm": 7.278550624847412,
      "learning_rate": 1.2968040106532978e-05,
      "loss": 1.5545,
      "step": 567300
    },
    {
      "epoch": 44.44618517938274,
      "grad_norm": 7.65494441986084,
      "learning_rate": 1.2961512350514387e-05,
      "loss": 1.7196,
      "step": 567400
    },
    {
      "epoch": 44.454018486605044,
      "grad_norm": 7.732874870300293,
      "learning_rate": 1.2954984594495798e-05,
      "loss": 1.6107,
      "step": 567500
    },
    {
      "epoch": 44.46185179382736,
      "grad_norm": 6.843708515167236,
      "learning_rate": 1.2948456838477204e-05,
      "loss": 1.7024,
      "step": 567600
    },
    {
      "epoch": 44.46968510104966,
      "grad_norm": 5.873419761657715,
      "learning_rate": 1.2941929082458615e-05,
      "loss": 1.5927,
      "step": 567700
    },
    {
      "epoch": 44.47751840827197,
      "grad_norm": 6.274692058563232,
      "learning_rate": 1.2935401326440024e-05,
      "loss": 1.6336,
      "step": 567800
    },
    {
      "epoch": 44.48535171549428,
      "grad_norm": 6.5211873054504395,
      "learning_rate": 1.2928873570421432e-05,
      "loss": 1.6059,
      "step": 567900
    },
    {
      "epoch": 44.49318502271659,
      "grad_norm": 7.832123279571533,
      "learning_rate": 1.2922345814402842e-05,
      "loss": 1.8088,
      "step": 568000
    },
    {
      "epoch": 44.5010183299389,
      "grad_norm": 6.022308349609375,
      "learning_rate": 1.291581805838425e-05,
      "loss": 1.6692,
      "step": 568100
    },
    {
      "epoch": 44.50885163716121,
      "grad_norm": 7.750123023986816,
      "learning_rate": 1.2909290302365659e-05,
      "loss": 1.6159,
      "step": 568200
    },
    {
      "epoch": 44.51668494438352,
      "grad_norm": 7.969788551330566,
      "learning_rate": 1.290276254634707e-05,
      "loss": 1.6737,
      "step": 568300
    },
    {
      "epoch": 44.524518251605826,
      "grad_norm": 6.694847583770752,
      "learning_rate": 1.2896234790328476e-05,
      "loss": 1.7078,
      "step": 568400
    },
    {
      "epoch": 44.53235155882814,
      "grad_norm": 7.9868035316467285,
      "learning_rate": 1.2889707034309886e-05,
      "loss": 1.6141,
      "step": 568500
    },
    {
      "epoch": 44.540184866050446,
      "grad_norm": 5.538177013397217,
      "learning_rate": 1.2883179278291296e-05,
      "loss": 1.6496,
      "step": 568600
    },
    {
      "epoch": 44.54801817327276,
      "grad_norm": 7.811926364898682,
      "learning_rate": 1.2876651522272703e-05,
      "loss": 1.6306,
      "step": 568700
    },
    {
      "epoch": 44.555851480495065,
      "grad_norm": 5.69514799118042,
      "learning_rate": 1.2870123766254114e-05,
      "loss": 1.5541,
      "step": 568800
    },
    {
      "epoch": 44.56368478771738,
      "grad_norm": 8.81080436706543,
      "learning_rate": 1.2863596010235521e-05,
      "loss": 1.6315,
      "step": 568900
    },
    {
      "epoch": 44.571518094939684,
      "grad_norm": 5.398140907287598,
      "learning_rate": 1.285706825421693e-05,
      "loss": 1.6184,
      "step": 569000
    },
    {
      "epoch": 44.57935140216199,
      "grad_norm": 7.834624767303467,
      "learning_rate": 1.2850540498198341e-05,
      "loss": 1.7083,
      "step": 569100
    },
    {
      "epoch": 44.5871847093843,
      "grad_norm": 6.88976526260376,
      "learning_rate": 1.2844012742179747e-05,
      "loss": 1.7229,
      "step": 569200
    },
    {
      "epoch": 44.59501801660661,
      "grad_norm": 5.597855091094971,
      "learning_rate": 1.2837484986161158e-05,
      "loss": 1.5986,
      "step": 569300
    },
    {
      "epoch": 44.60285132382892,
      "grad_norm": 7.8715620040893555,
      "learning_rate": 1.2830957230142567e-05,
      "loss": 1.5981,
      "step": 569400
    },
    {
      "epoch": 44.61068463105123,
      "grad_norm": 7.584070682525635,
      "learning_rate": 1.2824429474123975e-05,
      "loss": 1.5935,
      "step": 569500
    },
    {
      "epoch": 44.61851793827354,
      "grad_norm": 6.446340084075928,
      "learning_rate": 1.2817901718105386e-05,
      "loss": 1.6607,
      "step": 569600
    },
    {
      "epoch": 44.62635124549585,
      "grad_norm": 9.7868013381958,
      "learning_rate": 1.2811373962086793e-05,
      "loss": 1.6456,
      "step": 569700
    },
    {
      "epoch": 44.63418455271816,
      "grad_norm": 7.257204532623291,
      "learning_rate": 1.2804846206068202e-05,
      "loss": 1.5906,
      "step": 569800
    },
    {
      "epoch": 44.64201785994047,
      "grad_norm": 6.774889945983887,
      "learning_rate": 1.2798318450049613e-05,
      "loss": 1.5428,
      "step": 569900
    },
    {
      "epoch": 44.64985116716278,
      "grad_norm": 6.885758876800537,
      "learning_rate": 1.2791790694031019e-05,
      "loss": 1.6523,
      "step": 570000
    },
    {
      "epoch": 44.657684474385086,
      "grad_norm": 5.485638618469238,
      "learning_rate": 1.278526293801243e-05,
      "loss": 1.5511,
      "step": 570100
    },
    {
      "epoch": 44.66551778160739,
      "grad_norm": 6.755278587341309,
      "learning_rate": 1.2778735181993839e-05,
      "loss": 1.6533,
      "step": 570200
    },
    {
      "epoch": 44.673351088829705,
      "grad_norm": 6.056342601776123,
      "learning_rate": 1.2772207425975246e-05,
      "loss": 1.6144,
      "step": 570300
    },
    {
      "epoch": 44.68118439605201,
      "grad_norm": 5.017569065093994,
      "learning_rate": 1.2765679669956657e-05,
      "loss": 1.6436,
      "step": 570400
    },
    {
      "epoch": 44.689017703274324,
      "grad_norm": 7.285404682159424,
      "learning_rate": 1.2759151913938066e-05,
      "loss": 1.6907,
      "step": 570500
    },
    {
      "epoch": 44.69685101049663,
      "grad_norm": 8.572400093078613,
      "learning_rate": 1.2752624157919474e-05,
      "loss": 1.5628,
      "step": 570600
    },
    {
      "epoch": 44.70468431771894,
      "grad_norm": 8.647025108337402,
      "learning_rate": 1.2746096401900885e-05,
      "loss": 1.6958,
      "step": 570700
    },
    {
      "epoch": 44.71251762494125,
      "grad_norm": 5.9989094734191895,
      "learning_rate": 1.273956864588229e-05,
      "loss": 1.729,
      "step": 570800
    },
    {
      "epoch": 44.72035093216356,
      "grad_norm": 8.350475311279297,
      "learning_rate": 1.2733040889863701e-05,
      "loss": 1.6739,
      "step": 570900
    },
    {
      "epoch": 44.72818423938587,
      "grad_norm": 6.61470365524292,
      "learning_rate": 1.272651313384511e-05,
      "loss": 1.6914,
      "step": 571000
    },
    {
      "epoch": 44.73601754660818,
      "grad_norm": 15.140291213989258,
      "learning_rate": 1.2719985377826518e-05,
      "loss": 1.6407,
      "step": 571100
    },
    {
      "epoch": 44.74385085383049,
      "grad_norm": 6.115360736846924,
      "learning_rate": 1.2713457621807929e-05,
      "loss": 1.5206,
      "step": 571200
    },
    {
      "epoch": 44.75168416105279,
      "grad_norm": 6.312800407409668,
      "learning_rate": 1.2706929865789338e-05,
      "loss": 1.7007,
      "step": 571300
    },
    {
      "epoch": 44.75951746827511,
      "grad_norm": 6.800872325897217,
      "learning_rate": 1.2700402109770745e-05,
      "loss": 1.738,
      "step": 571400
    },
    {
      "epoch": 44.76735077549741,
      "grad_norm": 7.105500221252441,
      "learning_rate": 1.2693874353752156e-05,
      "loss": 1.6636,
      "step": 571500
    },
    {
      "epoch": 44.775184082719726,
      "grad_norm": 8.730271339416504,
      "learning_rate": 1.2687346597733562e-05,
      "loss": 1.6628,
      "step": 571600
    },
    {
      "epoch": 44.78301738994203,
      "grad_norm": 7.125766754150391,
      "learning_rate": 1.2680818841714973e-05,
      "loss": 1.6627,
      "step": 571700
    },
    {
      "epoch": 44.790850697164345,
      "grad_norm": 5.581849098205566,
      "learning_rate": 1.2674291085696382e-05,
      "loss": 1.6619,
      "step": 571800
    },
    {
      "epoch": 44.79868400438665,
      "grad_norm": 6.892821788787842,
      "learning_rate": 1.266776332967779e-05,
      "loss": 1.6255,
      "step": 571900
    },
    {
      "epoch": 44.806517311608964,
      "grad_norm": 5.7622971534729,
      "learning_rate": 1.26612355736592e-05,
      "loss": 1.5962,
      "step": 572000
    },
    {
      "epoch": 44.81435061883127,
      "grad_norm": 6.544081211090088,
      "learning_rate": 1.265470781764061e-05,
      "loss": 1.7175,
      "step": 572100
    },
    {
      "epoch": 44.82218392605358,
      "grad_norm": 5.586607933044434,
      "learning_rate": 1.2648180061622017e-05,
      "loss": 1.6773,
      "step": 572200
    },
    {
      "epoch": 44.83001723327589,
      "grad_norm": 5.671753406524658,
      "learning_rate": 1.2641652305603428e-05,
      "loss": 1.6573,
      "step": 572300
    },
    {
      "epoch": 44.837850540498195,
      "grad_norm": 6.242079734802246,
      "learning_rate": 1.2635124549584833e-05,
      "loss": 1.6932,
      "step": 572400
    },
    {
      "epoch": 44.84568384772051,
      "grad_norm": 4.778037071228027,
      "learning_rate": 1.2628596793566244e-05,
      "loss": 1.5878,
      "step": 572500
    },
    {
      "epoch": 44.853517154942814,
      "grad_norm": 6.813902854919434,
      "learning_rate": 1.2622069037547653e-05,
      "loss": 1.711,
      "step": 572600
    },
    {
      "epoch": 44.86135046216513,
      "grad_norm": 4.553537845611572,
      "learning_rate": 1.2615541281529061e-05,
      "loss": 1.7117,
      "step": 572700
    },
    {
      "epoch": 44.86918376938743,
      "grad_norm": 5.684170722961426,
      "learning_rate": 1.2609013525510472e-05,
      "loss": 1.67,
      "step": 572800
    },
    {
      "epoch": 44.87701707660975,
      "grad_norm": 8.581130981445312,
      "learning_rate": 1.2602485769491881e-05,
      "loss": 1.6479,
      "step": 572900
    },
    {
      "epoch": 44.88485038383205,
      "grad_norm": 6.010395526885986,
      "learning_rate": 1.2595958013473288e-05,
      "loss": 1.6114,
      "step": 573000
    },
    {
      "epoch": 44.892683691054366,
      "grad_norm": 6.857607841491699,
      "learning_rate": 1.25894302574547e-05,
      "loss": 1.6122,
      "step": 573100
    },
    {
      "epoch": 44.90051699827667,
      "grad_norm": 4.042070388793945,
      "learning_rate": 1.2582902501436105e-05,
      "loss": 1.7511,
      "step": 573200
    },
    {
      "epoch": 44.908350305498985,
      "grad_norm": 5.092945575714111,
      "learning_rate": 1.2576374745417516e-05,
      "loss": 1.5049,
      "step": 573300
    },
    {
      "epoch": 44.91618361272129,
      "grad_norm": 7.706809997558594,
      "learning_rate": 1.2569846989398925e-05,
      "loss": 1.6207,
      "step": 573400
    },
    {
      "epoch": 44.9240169199436,
      "grad_norm": 9.549168586730957,
      "learning_rate": 1.2563319233380332e-05,
      "loss": 1.6872,
      "step": 573500
    },
    {
      "epoch": 44.93185022716591,
      "grad_norm": 7.924493312835693,
      "learning_rate": 1.2556791477361743e-05,
      "loss": 1.5496,
      "step": 573600
    },
    {
      "epoch": 44.939683534388216,
      "grad_norm": 5.1905927658081055,
      "learning_rate": 1.2550263721343152e-05,
      "loss": 1.6909,
      "step": 573700
    },
    {
      "epoch": 44.94751684161053,
      "grad_norm": 6.606729984283447,
      "learning_rate": 1.254373596532456e-05,
      "loss": 1.6554,
      "step": 573800
    },
    {
      "epoch": 44.955350148832835,
      "grad_norm": 6.543650150299072,
      "learning_rate": 1.253720820930597e-05,
      "loss": 1.6228,
      "step": 573900
    },
    {
      "epoch": 44.96318345605515,
      "grad_norm": 6.5458269119262695,
      "learning_rate": 1.2530680453287377e-05,
      "loss": 1.5433,
      "step": 574000
    },
    {
      "epoch": 44.971016763277454,
      "grad_norm": 6.543735504150391,
      "learning_rate": 1.2524152697268787e-05,
      "loss": 1.5922,
      "step": 574100
    },
    {
      "epoch": 44.97885007049977,
      "grad_norm": 5.140466690063477,
      "learning_rate": 1.2517624941250197e-05,
      "loss": 1.7676,
      "step": 574200
    },
    {
      "epoch": 44.98668337772207,
      "grad_norm": 6.408929824829102,
      "learning_rate": 1.2511097185231604e-05,
      "loss": 1.62,
      "step": 574300
    },
    {
      "epoch": 44.99451668494439,
      "grad_norm": 5.66832971572876,
      "learning_rate": 1.2504569429213015e-05,
      "loss": 1.5942,
      "step": 574400
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.7751566171646118,
      "eval_runtime": 1.483,
      "eval_samples_per_second": 453.131,
      "eval_steps_per_second": 453.131,
      "step": 574470
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.3956787586212158,
      "eval_runtime": 29.3033,
      "eval_samples_per_second": 435.65,
      "eval_steps_per_second": 435.65,
      "step": 574470
    },
    {
      "epoch": 45.00234999216669,
      "grad_norm": 5.915899753570557,
      "learning_rate": 1.2498041673194422e-05,
      "loss": 1.7078,
      "step": 574500
    },
    {
      "epoch": 45.010183299389,
      "grad_norm": 8.823041915893555,
      "learning_rate": 1.2491513917175831e-05,
      "loss": 1.6895,
      "step": 574600
    },
    {
      "epoch": 45.01801660661131,
      "grad_norm": 8.08475112915039,
      "learning_rate": 1.2484986161157242e-05,
      "loss": 1.6171,
      "step": 574700
    },
    {
      "epoch": 45.02584991383362,
      "grad_norm": 6.5700602531433105,
      "learning_rate": 1.247845840513865e-05,
      "loss": 1.6497,
      "step": 574800
    },
    {
      "epoch": 45.03368322105593,
      "grad_norm": 6.330269813537598,
      "learning_rate": 1.2471930649120059e-05,
      "loss": 1.4916,
      "step": 574900
    },
    {
      "epoch": 45.04151652827824,
      "grad_norm": 6.175714492797852,
      "learning_rate": 1.2465402893101468e-05,
      "loss": 1.5709,
      "step": 575000
    },
    {
      "epoch": 45.04934983550055,
      "grad_norm": 5.7919230461120605,
      "learning_rate": 1.2458875137082877e-05,
      "loss": 1.7253,
      "step": 575100
    },
    {
      "epoch": 45.057183142722856,
      "grad_norm": 7.30367374420166,
      "learning_rate": 1.2452347381064286e-05,
      "loss": 1.5489,
      "step": 575200
    },
    {
      "epoch": 45.06501644994517,
      "grad_norm": 7.910895347595215,
      "learning_rate": 1.2445819625045694e-05,
      "loss": 1.683,
      "step": 575300
    },
    {
      "epoch": 45.072849757167475,
      "grad_norm": 7.143433094024658,
      "learning_rate": 1.2439291869027103e-05,
      "loss": 1.6052,
      "step": 575400
    },
    {
      "epoch": 45.08068306438979,
      "grad_norm": 6.759220123291016,
      "learning_rate": 1.2432764113008514e-05,
      "loss": 1.669,
      "step": 575500
    },
    {
      "epoch": 45.088516371612094,
      "grad_norm": 5.456079959869385,
      "learning_rate": 1.2426236356989921e-05,
      "loss": 1.6349,
      "step": 575600
    },
    {
      "epoch": 45.09634967883441,
      "grad_norm": 6.676394939422607,
      "learning_rate": 1.241970860097133e-05,
      "loss": 1.6673,
      "step": 575700
    },
    {
      "epoch": 45.10418298605671,
      "grad_norm": 5.477818489074707,
      "learning_rate": 1.241318084495274e-05,
      "loss": 1.6523,
      "step": 575800
    },
    {
      "epoch": 45.11201629327902,
      "grad_norm": 8.092390060424805,
      "learning_rate": 1.2406653088934149e-05,
      "loss": 1.5933,
      "step": 575900
    },
    {
      "epoch": 45.11984960050133,
      "grad_norm": 6.229920864105225,
      "learning_rate": 1.2400125332915558e-05,
      "loss": 1.66,
      "step": 576000
    },
    {
      "epoch": 45.12768290772364,
      "grad_norm": 5.313188552856445,
      "learning_rate": 1.2393597576896965e-05,
      "loss": 1.6846,
      "step": 576100
    },
    {
      "epoch": 45.13551621494595,
      "grad_norm": 7.766329288482666,
      "learning_rate": 1.2387069820878375e-05,
      "loss": 1.6267,
      "step": 576200
    },
    {
      "epoch": 45.14334952216826,
      "grad_norm": 6.5999298095703125,
      "learning_rate": 1.2380542064859785e-05,
      "loss": 1.5898,
      "step": 576300
    },
    {
      "epoch": 45.15118282939057,
      "grad_norm": 6.125629425048828,
      "learning_rate": 1.2374014308841193e-05,
      "loss": 1.5884,
      "step": 576400
    },
    {
      "epoch": 45.15901613661288,
      "grad_norm": 6.574364185333252,
      "learning_rate": 1.2367486552822602e-05,
      "loss": 1.5602,
      "step": 576500
    },
    {
      "epoch": 45.16684944383519,
      "grad_norm": 4.979483604431152,
      "learning_rate": 1.2360958796804011e-05,
      "loss": 1.626,
      "step": 576600
    },
    {
      "epoch": 45.174682751057496,
      "grad_norm": 7.210423946380615,
      "learning_rate": 1.235443104078542e-05,
      "loss": 1.5648,
      "step": 576700
    },
    {
      "epoch": 45.18251605827981,
      "grad_norm": 6.763993263244629,
      "learning_rate": 1.234790328476683e-05,
      "loss": 1.7191,
      "step": 576800
    },
    {
      "epoch": 45.190349365502115,
      "grad_norm": 7.580489158630371,
      "learning_rate": 1.2341375528748237e-05,
      "loss": 1.5977,
      "step": 576900
    },
    {
      "epoch": 45.19818267272442,
      "grad_norm": 6.4286603927612305,
      "learning_rate": 1.2334847772729648e-05,
      "loss": 1.6162,
      "step": 577000
    },
    {
      "epoch": 45.206015979946734,
      "grad_norm": 5.197363376617432,
      "learning_rate": 1.2328320016711057e-05,
      "loss": 1.591,
      "step": 577100
    },
    {
      "epoch": 45.21384928716904,
      "grad_norm": 2.9747564792633057,
      "learning_rate": 1.2321792260692464e-05,
      "loss": 1.6315,
      "step": 577200
    },
    {
      "epoch": 45.22168259439135,
      "grad_norm": 6.6678080558776855,
      "learning_rate": 1.2315264504673874e-05,
      "loss": 1.7181,
      "step": 577300
    },
    {
      "epoch": 45.22951590161366,
      "grad_norm": 7.85490083694458,
      "learning_rate": 1.2308736748655283e-05,
      "loss": 1.5105,
      "step": 577400
    },
    {
      "epoch": 45.23734920883597,
      "grad_norm": 7.336121082305908,
      "learning_rate": 1.2302208992636692e-05,
      "loss": 1.6918,
      "step": 577500
    },
    {
      "epoch": 45.24518251605828,
      "grad_norm": 8.663137435913086,
      "learning_rate": 1.2295681236618101e-05,
      "loss": 1.6761,
      "step": 577600
    },
    {
      "epoch": 45.25301582328059,
      "grad_norm": 6.863602638244629,
      "learning_rate": 1.2289153480599509e-05,
      "loss": 1.6549,
      "step": 577700
    },
    {
      "epoch": 45.2608491305029,
      "grad_norm": 4.472343444824219,
      "learning_rate": 1.228262572458092e-05,
      "loss": 1.6553,
      "step": 577800
    },
    {
      "epoch": 45.26868243772521,
      "grad_norm": 7.186108112335205,
      "learning_rate": 1.2276097968562329e-05,
      "loss": 1.6096,
      "step": 577900
    },
    {
      "epoch": 45.27651574494752,
      "grad_norm": 6.376300811767578,
      "learning_rate": 1.2269570212543736e-05,
      "loss": 1.5676,
      "step": 578000
    },
    {
      "epoch": 45.28434905216982,
      "grad_norm": 4.744771480560303,
      "learning_rate": 1.2263042456525145e-05,
      "loss": 1.594,
      "step": 578100
    },
    {
      "epoch": 45.292182359392136,
      "grad_norm": 6.434579372406006,
      "learning_rate": 1.2256514700506554e-05,
      "loss": 1.6626,
      "step": 578200
    },
    {
      "epoch": 45.30001566661444,
      "grad_norm": 6.548386096954346,
      "learning_rate": 1.2249986944487964e-05,
      "loss": 1.59,
      "step": 578300
    },
    {
      "epoch": 45.307848973836755,
      "grad_norm": 5.7340407371521,
      "learning_rate": 1.2243459188469373e-05,
      "loss": 1.6571,
      "step": 578400
    },
    {
      "epoch": 45.31568228105906,
      "grad_norm": 6.758190631866455,
      "learning_rate": 1.223693143245078e-05,
      "loss": 1.6985,
      "step": 578500
    },
    {
      "epoch": 45.323515588281374,
      "grad_norm": 7.78448486328125,
      "learning_rate": 1.2230403676432191e-05,
      "loss": 1.6723,
      "step": 578600
    },
    {
      "epoch": 45.33134889550368,
      "grad_norm": 7.474784851074219,
      "learning_rate": 1.22238759204136e-05,
      "loss": 1.7162,
      "step": 578700
    },
    {
      "epoch": 45.33918220272599,
      "grad_norm": 6.2745537757873535,
      "learning_rate": 1.2217348164395008e-05,
      "loss": 1.6763,
      "step": 578800
    },
    {
      "epoch": 45.3470155099483,
      "grad_norm": 5.801009178161621,
      "learning_rate": 1.2210820408376417e-05,
      "loss": 1.5939,
      "step": 578900
    },
    {
      "epoch": 45.35484881717061,
      "grad_norm": 4.881373405456543,
      "learning_rate": 1.2204292652357826e-05,
      "loss": 1.5739,
      "step": 579000
    },
    {
      "epoch": 45.36268212439292,
      "grad_norm": 5.783195495605469,
      "learning_rate": 1.2197764896339235e-05,
      "loss": 1.6357,
      "step": 579100
    },
    {
      "epoch": 45.370515431615225,
      "grad_norm": 8.145097732543945,
      "learning_rate": 1.2191237140320644e-05,
      "loss": 1.6434,
      "step": 579200
    },
    {
      "epoch": 45.37834873883754,
      "grad_norm": 6.324071407318115,
      "learning_rate": 1.2184709384302052e-05,
      "loss": 1.6339,
      "step": 579300
    },
    {
      "epoch": 45.386182046059844,
      "grad_norm": 4.265348434448242,
      "learning_rate": 1.2178181628283463e-05,
      "loss": 1.7201,
      "step": 579400
    },
    {
      "epoch": 45.39401535328216,
      "grad_norm": 9.245243072509766,
      "learning_rate": 1.2171653872264872e-05,
      "loss": 1.7116,
      "step": 579500
    },
    {
      "epoch": 45.40184866050446,
      "grad_norm": 6.732615947723389,
      "learning_rate": 1.2165126116246279e-05,
      "loss": 1.5918,
      "step": 579600
    },
    {
      "epoch": 45.409681967726776,
      "grad_norm": 9.247039794921875,
      "learning_rate": 1.2158598360227688e-05,
      "loss": 1.6702,
      "step": 579700
    },
    {
      "epoch": 45.41751527494908,
      "grad_norm": 7.011544227600098,
      "learning_rate": 1.2152070604209097e-05,
      "loss": 1.6686,
      "step": 579800
    },
    {
      "epoch": 45.425348582171395,
      "grad_norm": 6.451968193054199,
      "learning_rate": 1.2145542848190507e-05,
      "loss": 1.7047,
      "step": 579900
    },
    {
      "epoch": 45.4331818893937,
      "grad_norm": 8.301407814025879,
      "learning_rate": 1.2139015092171916e-05,
      "loss": 1.694,
      "step": 580000
    },
    {
      "epoch": 45.441015196616014,
      "grad_norm": 6.70506477355957,
      "learning_rate": 1.2132487336153323e-05,
      "loss": 1.651,
      "step": 580100
    },
    {
      "epoch": 45.44884850383832,
      "grad_norm": 5.547287940979004,
      "learning_rate": 1.2125959580134734e-05,
      "loss": 1.6217,
      "step": 580200
    },
    {
      "epoch": 45.456681811060626,
      "grad_norm": 7.674478530883789,
      "learning_rate": 1.2119431824116143e-05,
      "loss": 1.5987,
      "step": 580300
    },
    {
      "epoch": 45.46451511828294,
      "grad_norm": 6.221518516540527,
      "learning_rate": 1.211290406809755e-05,
      "loss": 1.6223,
      "step": 580400
    },
    {
      "epoch": 45.472348425505245,
      "grad_norm": 6.725008964538574,
      "learning_rate": 1.210637631207896e-05,
      "loss": 1.7216,
      "step": 580500
    },
    {
      "epoch": 45.48018173272756,
      "grad_norm": 8.279857635498047,
      "learning_rate": 1.2099848556060369e-05,
      "loss": 1.5797,
      "step": 580600
    },
    {
      "epoch": 45.488015039949865,
      "grad_norm": 6.985400199890137,
      "learning_rate": 1.2093320800041778e-05,
      "loss": 1.6273,
      "step": 580700
    },
    {
      "epoch": 45.49584834717218,
      "grad_norm": 4.872811317443848,
      "learning_rate": 1.2086793044023187e-05,
      "loss": 1.647,
      "step": 580800
    },
    {
      "epoch": 45.503681654394484,
      "grad_norm": 7.545378684997559,
      "learning_rate": 1.2080265288004595e-05,
      "loss": 1.5576,
      "step": 580900
    },
    {
      "epoch": 45.5115149616168,
      "grad_norm": 5.7287211418151855,
      "learning_rate": 1.2073737531986006e-05,
      "loss": 1.6434,
      "step": 581000
    },
    {
      "epoch": 45.5193482688391,
      "grad_norm": 7.402082920074463,
      "learning_rate": 1.2067209775967415e-05,
      "loss": 1.6424,
      "step": 581100
    },
    {
      "epoch": 45.527181576061416,
      "grad_norm": 7.948408603668213,
      "learning_rate": 1.2060682019948822e-05,
      "loss": 1.6173,
      "step": 581200
    },
    {
      "epoch": 45.53501488328372,
      "grad_norm": 3.5480260848999023,
      "learning_rate": 1.2054154263930231e-05,
      "loss": 1.583,
      "step": 581300
    },
    {
      "epoch": 45.542848190506035,
      "grad_norm": 7.2992753982543945,
      "learning_rate": 1.204762650791164e-05,
      "loss": 1.671,
      "step": 581400
    },
    {
      "epoch": 45.55068149772834,
      "grad_norm": 7.725306034088135,
      "learning_rate": 1.204109875189305e-05,
      "loss": 1.7003,
      "step": 581500
    },
    {
      "epoch": 45.55851480495065,
      "grad_norm": 6.912730693817139,
      "learning_rate": 1.2034570995874459e-05,
      "loss": 1.6226,
      "step": 581600
    },
    {
      "epoch": 45.56634811217296,
      "grad_norm": 8.843817710876465,
      "learning_rate": 1.2028043239855866e-05,
      "loss": 1.6856,
      "step": 581700
    },
    {
      "epoch": 45.574181419395266,
      "grad_norm": 6.14866828918457,
      "learning_rate": 1.2021515483837277e-05,
      "loss": 1.617,
      "step": 581800
    },
    {
      "epoch": 45.58201472661758,
      "grad_norm": 6.257324695587158,
      "learning_rate": 1.2014987727818686e-05,
      "loss": 1.6076,
      "step": 581900
    },
    {
      "epoch": 45.589848033839885,
      "grad_norm": 6.8241071701049805,
      "learning_rate": 1.2008459971800094e-05,
      "loss": 1.7149,
      "step": 582000
    },
    {
      "epoch": 45.5976813410622,
      "grad_norm": 5.843328952789307,
      "learning_rate": 1.2001932215781503e-05,
      "loss": 1.5915,
      "step": 582100
    },
    {
      "epoch": 45.605514648284505,
      "grad_norm": 5.852339744567871,
      "learning_rate": 1.1995404459762912e-05,
      "loss": 1.7295,
      "step": 582200
    },
    {
      "epoch": 45.61334795550682,
      "grad_norm": 6.599064350128174,
      "learning_rate": 1.1988876703744321e-05,
      "loss": 1.6255,
      "step": 582300
    },
    {
      "epoch": 45.621181262729124,
      "grad_norm": 6.7295613288879395,
      "learning_rate": 1.198234894772573e-05,
      "loss": 1.6946,
      "step": 582400
    },
    {
      "epoch": 45.62901456995144,
      "grad_norm": 4.242435455322266,
      "learning_rate": 1.197582119170714e-05,
      "loss": 1.5308,
      "step": 582500
    },
    {
      "epoch": 45.63684787717374,
      "grad_norm": 6.664979934692383,
      "learning_rate": 1.1969293435688549e-05,
      "loss": 1.6486,
      "step": 582600
    },
    {
      "epoch": 45.64468118439605,
      "grad_norm": 5.6429901123046875,
      "learning_rate": 1.1962765679669958e-05,
      "loss": 1.6211,
      "step": 582700
    },
    {
      "epoch": 45.65251449161836,
      "grad_norm": 7.480396747589111,
      "learning_rate": 1.1956237923651365e-05,
      "loss": 1.6559,
      "step": 582800
    },
    {
      "epoch": 45.66034779884067,
      "grad_norm": 10.375795364379883,
      "learning_rate": 1.1949710167632776e-05,
      "loss": 1.685,
      "step": 582900
    },
    {
      "epoch": 45.66818110606298,
      "grad_norm": 5.945115089416504,
      "learning_rate": 1.1943182411614184e-05,
      "loss": 1.636,
      "step": 583000
    },
    {
      "epoch": 45.67601441328529,
      "grad_norm": 7.889832973480225,
      "learning_rate": 1.1936654655595593e-05,
      "loss": 1.6365,
      "step": 583100
    },
    {
      "epoch": 45.6838477205076,
      "grad_norm": 5.638306140899658,
      "learning_rate": 1.1930126899577002e-05,
      "loss": 1.7177,
      "step": 583200
    },
    {
      "epoch": 45.691681027729906,
      "grad_norm": 5.539122104644775,
      "learning_rate": 1.1923599143558411e-05,
      "loss": 1.6101,
      "step": 583300
    },
    {
      "epoch": 45.69951433495222,
      "grad_norm": 6.317498207092285,
      "learning_rate": 1.191707138753982e-05,
      "loss": 1.694,
      "step": 583400
    },
    {
      "epoch": 45.707347642174526,
      "grad_norm": 7.357955455780029,
      "learning_rate": 1.191054363152123e-05,
      "loss": 1.6218,
      "step": 583500
    },
    {
      "epoch": 45.71518094939684,
      "grad_norm": 5.764813423156738,
      "learning_rate": 1.1904015875502637e-05,
      "loss": 1.587,
      "step": 583600
    },
    {
      "epoch": 45.723014256619145,
      "grad_norm": 6.210453033447266,
      "learning_rate": 1.1897488119484048e-05,
      "loss": 1.6783,
      "step": 583700
    },
    {
      "epoch": 45.73084756384145,
      "grad_norm": 4.80612850189209,
      "learning_rate": 1.1890960363465455e-05,
      "loss": 1.6416,
      "step": 583800
    },
    {
      "epoch": 45.738680871063764,
      "grad_norm": 6.907227516174316,
      "learning_rate": 1.1884432607446864e-05,
      "loss": 1.5426,
      "step": 583900
    },
    {
      "epoch": 45.74651417828607,
      "grad_norm": 7.295297622680664,
      "learning_rate": 1.1877904851428274e-05,
      "loss": 1.5833,
      "step": 584000
    },
    {
      "epoch": 45.75434748550838,
      "grad_norm": 8.3840913772583,
      "learning_rate": 1.1871377095409683e-05,
      "loss": 1.8032,
      "step": 584100
    },
    {
      "epoch": 45.76218079273069,
      "grad_norm": 5.11515998840332,
      "learning_rate": 1.1864849339391092e-05,
      "loss": 1.6715,
      "step": 584200
    },
    {
      "epoch": 45.770014099953,
      "grad_norm": 5.194273471832275,
      "learning_rate": 1.1858321583372501e-05,
      "loss": 1.6404,
      "step": 584300
    },
    {
      "epoch": 45.77784740717531,
      "grad_norm": 4.9832763671875,
      "learning_rate": 1.1851793827353908e-05,
      "loss": 1.5643,
      "step": 584400
    },
    {
      "epoch": 45.78568071439762,
      "grad_norm": 8.13733196258545,
      "learning_rate": 1.184526607133532e-05,
      "loss": 1.7141,
      "step": 584500
    },
    {
      "epoch": 45.79351402161993,
      "grad_norm": 6.06252908706665,
      "learning_rate": 1.1838738315316727e-05,
      "loss": 1.6178,
      "step": 584600
    },
    {
      "epoch": 45.80134732884224,
      "grad_norm": 6.946268558502197,
      "learning_rate": 1.1832210559298136e-05,
      "loss": 1.5441,
      "step": 584700
    },
    {
      "epoch": 45.809180636064546,
      "grad_norm": 5.995053768157959,
      "learning_rate": 1.1825682803279545e-05,
      "loss": 1.6559,
      "step": 584800
    },
    {
      "epoch": 45.81701394328685,
      "grad_norm": 7.1884307861328125,
      "learning_rate": 1.1819155047260954e-05,
      "loss": 1.6324,
      "step": 584900
    },
    {
      "epoch": 45.824847250509166,
      "grad_norm": 6.576467037200928,
      "learning_rate": 1.1812627291242363e-05,
      "loss": 1.7147,
      "step": 585000
    },
    {
      "epoch": 45.83268055773147,
      "grad_norm": 6.111149787902832,
      "learning_rate": 1.1806099535223773e-05,
      "loss": 1.6563,
      "step": 585100
    },
    {
      "epoch": 45.840513864953785,
      "grad_norm": 5.90895414352417,
      "learning_rate": 1.179957177920518e-05,
      "loss": 1.5787,
      "step": 585200
    },
    {
      "epoch": 45.84834717217609,
      "grad_norm": 6.228346347808838,
      "learning_rate": 1.1793044023186591e-05,
      "loss": 1.621,
      "step": 585300
    },
    {
      "epoch": 45.856180479398404,
      "grad_norm": 7.418881893157959,
      "learning_rate": 1.1786516267167998e-05,
      "loss": 1.6635,
      "step": 585400
    },
    {
      "epoch": 45.86401378662071,
      "grad_norm": 8.388477325439453,
      "learning_rate": 1.1779988511149408e-05,
      "loss": 1.6871,
      "step": 585500
    },
    {
      "epoch": 45.87184709384302,
      "grad_norm": 6.051763534545898,
      "learning_rate": 1.1773460755130817e-05,
      "loss": 1.6853,
      "step": 585600
    },
    {
      "epoch": 45.87968040106533,
      "grad_norm": 7.740657806396484,
      "learning_rate": 1.1766932999112226e-05,
      "loss": 1.6619,
      "step": 585700
    },
    {
      "epoch": 45.88751370828764,
      "grad_norm": 5.382646083831787,
      "learning_rate": 1.1760405243093635e-05,
      "loss": 1.6492,
      "step": 585800
    },
    {
      "epoch": 45.89534701550995,
      "grad_norm": 7.682801723480225,
      "learning_rate": 1.1753877487075044e-05,
      "loss": 1.6227,
      "step": 585900
    },
    {
      "epoch": 45.903180322732254,
      "grad_norm": 5.561460494995117,
      "learning_rate": 1.1747349731056452e-05,
      "loss": 1.6568,
      "step": 586000
    },
    {
      "epoch": 45.91101362995457,
      "grad_norm": 6.9137139320373535,
      "learning_rate": 1.1740821975037862e-05,
      "loss": 1.6855,
      "step": 586100
    },
    {
      "epoch": 45.91884693717687,
      "grad_norm": 8.236699104309082,
      "learning_rate": 1.173429421901927e-05,
      "loss": 1.5303,
      "step": 586200
    },
    {
      "epoch": 45.926680244399186,
      "grad_norm": 5.8935933113098145,
      "learning_rate": 1.1727766463000679e-05,
      "loss": 1.5832,
      "step": 586300
    },
    {
      "epoch": 45.93451355162149,
      "grad_norm": 8.081236839294434,
      "learning_rate": 1.1721238706982088e-05,
      "loss": 1.7085,
      "step": 586400
    },
    {
      "epoch": 45.942346858843806,
      "grad_norm": 7.761313438415527,
      "learning_rate": 1.1714710950963497e-05,
      "loss": 1.58,
      "step": 586500
    },
    {
      "epoch": 45.95018016606611,
      "grad_norm": 7.189233303070068,
      "learning_rate": 1.1708183194944907e-05,
      "loss": 1.6129,
      "step": 586600
    },
    {
      "epoch": 45.958013473288425,
      "grad_norm": 6.693062782287598,
      "learning_rate": 1.1701655438926316e-05,
      "loss": 1.6238,
      "step": 586700
    },
    {
      "epoch": 45.96584678051073,
      "grad_norm": 6.339937686920166,
      "learning_rate": 1.1695127682907723e-05,
      "loss": 1.7249,
      "step": 586800
    },
    {
      "epoch": 45.973680087733044,
      "grad_norm": 5.15433931350708,
      "learning_rate": 1.1688599926889134e-05,
      "loss": 1.6078,
      "step": 586900
    },
    {
      "epoch": 45.98151339495535,
      "grad_norm": 5.361433029174805,
      "learning_rate": 1.1682072170870541e-05,
      "loss": 1.693,
      "step": 587000
    },
    {
      "epoch": 45.989346702177656,
      "grad_norm": 6.955986022949219,
      "learning_rate": 1.167554441485195e-05,
      "loss": 1.6591,
      "step": 587100
    },
    {
      "epoch": 45.99718000939997,
      "grad_norm": 6.828004360198975,
      "learning_rate": 1.166901665883336e-05,
      "loss": 1.7277,
      "step": 587200
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.766904592514038,
      "eval_runtime": 1.514,
      "eval_samples_per_second": 443.848,
      "eval_steps_per_second": 443.848,
      "step": 587236
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.3903363943099976,
      "eval_runtime": 29.1115,
      "eval_samples_per_second": 438.521,
      "eval_steps_per_second": 438.521,
      "step": 587236
    },
    {
      "epoch": 46.005013316622275,
      "grad_norm": 6.22675895690918,
      "learning_rate": 1.1662488902814769e-05,
      "loss": 1.6848,
      "step": 587300
    },
    {
      "epoch": 46.01284662384459,
      "grad_norm": 9.106687545776367,
      "learning_rate": 1.1655961146796178e-05,
      "loss": 1.6665,
      "step": 587400
    },
    {
      "epoch": 46.020679931066894,
      "grad_norm": 7.391462326049805,
      "learning_rate": 1.1649433390777587e-05,
      "loss": 1.6983,
      "step": 587500
    },
    {
      "epoch": 46.02851323828921,
      "grad_norm": 7.1748762130737305,
      "learning_rate": 1.1642905634758995e-05,
      "loss": 1.6161,
      "step": 587600
    },
    {
      "epoch": 46.03634654551151,
      "grad_norm": 7.537607669830322,
      "learning_rate": 1.1636377878740406e-05,
      "loss": 1.4334,
      "step": 587700
    },
    {
      "epoch": 46.044179852733826,
      "grad_norm": 5.987433433532715,
      "learning_rate": 1.1629850122721813e-05,
      "loss": 1.67,
      "step": 587800
    },
    {
      "epoch": 46.05201315995613,
      "grad_norm": 8.015931129455566,
      "learning_rate": 1.1623322366703222e-05,
      "loss": 1.6323,
      "step": 587900
    },
    {
      "epoch": 46.059846467178446,
      "grad_norm": 6.167571544647217,
      "learning_rate": 1.1616794610684631e-05,
      "loss": 1.5921,
      "step": 588000
    },
    {
      "epoch": 46.06767977440075,
      "grad_norm": 8.071181297302246,
      "learning_rate": 1.161026685466604e-05,
      "loss": 1.6356,
      "step": 588100
    },
    {
      "epoch": 46.075513081623065,
      "grad_norm": 6.4043869972229,
      "learning_rate": 1.160373909864745e-05,
      "loss": 1.5921,
      "step": 588200
    },
    {
      "epoch": 46.08334638884537,
      "grad_norm": 4.249767303466797,
      "learning_rate": 1.1597211342628859e-05,
      "loss": 1.5544,
      "step": 588300
    },
    {
      "epoch": 46.09117969606768,
      "grad_norm": 7.865460395812988,
      "learning_rate": 1.1590683586610268e-05,
      "loss": 1.6301,
      "step": 588400
    },
    {
      "epoch": 46.09901300328999,
      "grad_norm": 7.277997970581055,
      "learning_rate": 1.1584155830591677e-05,
      "loss": 1.6168,
      "step": 588500
    },
    {
      "epoch": 46.106846310512296,
      "grad_norm": 7.404197692871094,
      "learning_rate": 1.1577628074573085e-05,
      "loss": 1.6509,
      "step": 588600
    },
    {
      "epoch": 46.11467961773461,
      "grad_norm": 5.696571350097656,
      "learning_rate": 1.1571100318554494e-05,
      "loss": 1.5814,
      "step": 588700
    },
    {
      "epoch": 46.122512924956915,
      "grad_norm": 6.380342483520508,
      "learning_rate": 1.1564572562535905e-05,
      "loss": 1.6231,
      "step": 588800
    },
    {
      "epoch": 46.13034623217923,
      "grad_norm": 6.1989288330078125,
      "learning_rate": 1.1558044806517312e-05,
      "loss": 1.6113,
      "step": 588900
    },
    {
      "epoch": 46.138179539401534,
      "grad_norm": 6.994887828826904,
      "learning_rate": 1.1551517050498721e-05,
      "loss": 1.6194,
      "step": 589000
    },
    {
      "epoch": 46.14601284662385,
      "grad_norm": 7.4249162673950195,
      "learning_rate": 1.154498929448013e-05,
      "loss": 1.6035,
      "step": 589100
    },
    {
      "epoch": 46.15384615384615,
      "grad_norm": 6.010107040405273,
      "learning_rate": 1.153846153846154e-05,
      "loss": 1.5834,
      "step": 589200
    },
    {
      "epoch": 46.161679461068466,
      "grad_norm": 6.754095554351807,
      "learning_rate": 1.1531933782442949e-05,
      "loss": 1.6684,
      "step": 589300
    },
    {
      "epoch": 46.16951276829077,
      "grad_norm": 6.232850074768066,
      "learning_rate": 1.1525406026424356e-05,
      "loss": 1.5474,
      "step": 589400
    },
    {
      "epoch": 46.17734607551308,
      "grad_norm": 6.550522804260254,
      "learning_rate": 1.1518878270405765e-05,
      "loss": 1.5092,
      "step": 589500
    },
    {
      "epoch": 46.18517938273539,
      "grad_norm": 5.097327709197998,
      "learning_rate": 1.1512350514387176e-05,
      "loss": 1.5458,
      "step": 589600
    },
    {
      "epoch": 46.1930126899577,
      "grad_norm": 5.089413642883301,
      "learning_rate": 1.1505822758368584e-05,
      "loss": 1.6674,
      "step": 589700
    },
    {
      "epoch": 46.20084599718001,
      "grad_norm": 6.515706539154053,
      "learning_rate": 1.1499295002349993e-05,
      "loss": 1.6086,
      "step": 589800
    },
    {
      "epoch": 46.20867930440232,
      "grad_norm": 5.869128227233887,
      "learning_rate": 1.1492767246331402e-05,
      "loss": 1.688,
      "step": 589900
    },
    {
      "epoch": 46.21651261162463,
      "grad_norm": 7.276639938354492,
      "learning_rate": 1.1486239490312811e-05,
      "loss": 1.7754,
      "step": 590000
    },
    {
      "epoch": 46.224345918846936,
      "grad_norm": 6.332765579223633,
      "learning_rate": 1.147971173429422e-05,
      "loss": 1.5571,
      "step": 590100
    },
    {
      "epoch": 46.23217922606925,
      "grad_norm": 5.561609745025635,
      "learning_rate": 1.1473183978275628e-05,
      "loss": 1.6157,
      "step": 590200
    },
    {
      "epoch": 46.240012533291555,
      "grad_norm": 8.600464820861816,
      "learning_rate": 1.1466656222257037e-05,
      "loss": 1.7082,
      "step": 590300
    },
    {
      "epoch": 46.24784584051387,
      "grad_norm": 6.700912952423096,
      "learning_rate": 1.1460128466238448e-05,
      "loss": 1.6959,
      "step": 590400
    },
    {
      "epoch": 46.255679147736174,
      "grad_norm": 7.049300193786621,
      "learning_rate": 1.1453600710219855e-05,
      "loss": 1.5664,
      "step": 590500
    },
    {
      "epoch": 46.26351245495848,
      "grad_norm": 6.386099815368652,
      "learning_rate": 1.1447072954201264e-05,
      "loss": 1.6927,
      "step": 590600
    },
    {
      "epoch": 46.27134576218079,
      "grad_norm": 5.791698932647705,
      "learning_rate": 1.1440545198182673e-05,
      "loss": 1.5883,
      "step": 590700
    },
    {
      "epoch": 46.2791790694031,
      "grad_norm": 7.121848106384277,
      "learning_rate": 1.1434017442164083e-05,
      "loss": 1.6917,
      "step": 590800
    },
    {
      "epoch": 46.28701237662541,
      "grad_norm": 6.493826866149902,
      "learning_rate": 1.1427489686145492e-05,
      "loss": 1.6543,
      "step": 590900
    },
    {
      "epoch": 46.29484568384772,
      "grad_norm": 6.759713649749756,
      "learning_rate": 1.14209619301269e-05,
      "loss": 1.6309,
      "step": 591000
    },
    {
      "epoch": 46.30267899107003,
      "grad_norm": 8.005474090576172,
      "learning_rate": 1.1414434174108308e-05,
      "loss": 1.6393,
      "step": 591100
    },
    {
      "epoch": 46.31051229829234,
      "grad_norm": 6.768611431121826,
      "learning_rate": 1.140790641808972e-05,
      "loss": 1.5965,
      "step": 591200
    },
    {
      "epoch": 46.31834560551465,
      "grad_norm": 5.2560272216796875,
      "learning_rate": 1.1401378662071127e-05,
      "loss": 1.6773,
      "step": 591300
    },
    {
      "epoch": 46.32617891273696,
      "grad_norm": 5.97832727432251,
      "learning_rate": 1.1394850906052536e-05,
      "loss": 1.704,
      "step": 591400
    },
    {
      "epoch": 46.33401221995927,
      "grad_norm": 5.121367454528809,
      "learning_rate": 1.1388323150033945e-05,
      "loss": 1.5747,
      "step": 591500
    },
    {
      "epoch": 46.341845527181576,
      "grad_norm": 7.682938575744629,
      "learning_rate": 1.1381795394015354e-05,
      "loss": 1.6964,
      "step": 591600
    },
    {
      "epoch": 46.34967883440388,
      "grad_norm": 4.981710433959961,
      "learning_rate": 1.1375267637996763e-05,
      "loss": 1.6927,
      "step": 591700
    },
    {
      "epoch": 46.357512141626195,
      "grad_norm": 6.2081708908081055,
      "learning_rate": 1.136873988197817e-05,
      "loss": 1.628,
      "step": 591800
    },
    {
      "epoch": 46.3653454488485,
      "grad_norm": 8.780156135559082,
      "learning_rate": 1.136221212595958e-05,
      "loss": 1.6265,
      "step": 591900
    },
    {
      "epoch": 46.373178756070814,
      "grad_norm": 6.419532775878906,
      "learning_rate": 1.135568436994099e-05,
      "loss": 1.685,
      "step": 592000
    },
    {
      "epoch": 46.38101206329312,
      "grad_norm": 5.3406829833984375,
      "learning_rate": 1.1349156613922398e-05,
      "loss": 1.7299,
      "step": 592100
    },
    {
      "epoch": 46.38884537051543,
      "grad_norm": 8.635050773620605,
      "learning_rate": 1.1342628857903807e-05,
      "loss": 1.626,
      "step": 592200
    },
    {
      "epoch": 46.39667867773774,
      "grad_norm": 7.16201114654541,
      "learning_rate": 1.1336101101885217e-05,
      "loss": 1.671,
      "step": 592300
    },
    {
      "epoch": 46.40451198496005,
      "grad_norm": 6.413225173950195,
      "learning_rate": 1.1329573345866626e-05,
      "loss": 1.6842,
      "step": 592400
    },
    {
      "epoch": 46.41234529218236,
      "grad_norm": 9.217154502868652,
      "learning_rate": 1.1323045589848035e-05,
      "loss": 1.5883,
      "step": 592500
    },
    {
      "epoch": 46.42017859940467,
      "grad_norm": 9.19399356842041,
      "learning_rate": 1.1316517833829442e-05,
      "loss": 1.6838,
      "step": 592600
    },
    {
      "epoch": 46.42801190662698,
      "grad_norm": 5.7862019538879395,
      "learning_rate": 1.1309990077810852e-05,
      "loss": 1.5816,
      "step": 592700
    },
    {
      "epoch": 46.435845213849284,
      "grad_norm": 4.882627010345459,
      "learning_rate": 1.1303462321792262e-05,
      "loss": 1.6508,
      "step": 592800
    },
    {
      "epoch": 46.4436785210716,
      "grad_norm": 7.024655818939209,
      "learning_rate": 1.129693456577367e-05,
      "loss": 1.6917,
      "step": 592900
    },
    {
      "epoch": 46.4515118282939,
      "grad_norm": 6.3717875480651855,
      "learning_rate": 1.1290406809755079e-05,
      "loss": 1.5755,
      "step": 593000
    },
    {
      "epoch": 46.459345135516216,
      "grad_norm": 7.015635013580322,
      "learning_rate": 1.1283879053736488e-05,
      "loss": 1.641,
      "step": 593100
    },
    {
      "epoch": 46.46717844273852,
      "grad_norm": 8.333209037780762,
      "learning_rate": 1.1277351297717897e-05,
      "loss": 1.6049,
      "step": 593200
    },
    {
      "epoch": 46.475011749960835,
      "grad_norm": 7.0588603019714355,
      "learning_rate": 1.1270823541699306e-05,
      "loss": 1.6122,
      "step": 593300
    },
    {
      "epoch": 46.48284505718314,
      "grad_norm": 4.918203353881836,
      "learning_rate": 1.1264295785680714e-05,
      "loss": 1.5509,
      "step": 593400
    },
    {
      "epoch": 46.490678364405454,
      "grad_norm": 6.238585472106934,
      "learning_rate": 1.1257768029662123e-05,
      "loss": 1.6547,
      "step": 593500
    },
    {
      "epoch": 46.49851167162776,
      "grad_norm": 6.1707444190979,
      "learning_rate": 1.1251240273643534e-05,
      "loss": 1.7146,
      "step": 593600
    },
    {
      "epoch": 46.50634497885007,
      "grad_norm": 6.555055618286133,
      "learning_rate": 1.1244712517624941e-05,
      "loss": 1.5688,
      "step": 593700
    },
    {
      "epoch": 46.51417828607238,
      "grad_norm": 5.387073516845703,
      "learning_rate": 1.123818476160635e-05,
      "loss": 1.6446,
      "step": 593800
    },
    {
      "epoch": 46.52201159329469,
      "grad_norm": 8.594447135925293,
      "learning_rate": 1.123165700558776e-05,
      "loss": 1.6375,
      "step": 593900
    },
    {
      "epoch": 46.529844900517,
      "grad_norm": 6.442408084869385,
      "learning_rate": 1.1225129249569169e-05,
      "loss": 1.6373,
      "step": 594000
    },
    {
      "epoch": 46.537678207739305,
      "grad_norm": 7.841728687286377,
      "learning_rate": 1.1218601493550578e-05,
      "loss": 1.6624,
      "step": 594100
    },
    {
      "epoch": 46.54551151496162,
      "grad_norm": 7.865117073059082,
      "learning_rate": 1.1212073737531986e-05,
      "loss": 1.6223,
      "step": 594200
    },
    {
      "epoch": 46.553344822183924,
      "grad_norm": 5.75944185256958,
      "learning_rate": 1.1205545981513396e-05,
      "loss": 1.6252,
      "step": 594300
    },
    {
      "epoch": 46.56117812940624,
      "grad_norm": 8.168899536132812,
      "learning_rate": 1.1199018225494806e-05,
      "loss": 1.6543,
      "step": 594400
    },
    {
      "epoch": 46.56901143662854,
      "grad_norm": 6.5763397216796875,
      "learning_rate": 1.1192490469476213e-05,
      "loss": 1.5843,
      "step": 594500
    },
    {
      "epoch": 46.576844743850856,
      "grad_norm": 6.0916428565979,
      "learning_rate": 1.1185962713457622e-05,
      "loss": 1.614,
      "step": 594600
    },
    {
      "epoch": 46.58467805107316,
      "grad_norm": 4.633046627044678,
      "learning_rate": 1.1179434957439031e-05,
      "loss": 1.5745,
      "step": 594700
    },
    {
      "epoch": 46.592511358295475,
      "grad_norm": 10.23237419128418,
      "learning_rate": 1.117290720142044e-05,
      "loss": 1.6817,
      "step": 594800
    },
    {
      "epoch": 46.60034466551778,
      "grad_norm": 6.083680152893066,
      "learning_rate": 1.116637944540185e-05,
      "loss": 1.5822,
      "step": 594900
    },
    {
      "epoch": 46.608177972740094,
      "grad_norm": 6.3861083984375,
      "learning_rate": 1.1159851689383257e-05,
      "loss": 1.6557,
      "step": 595000
    },
    {
      "epoch": 46.6160112799624,
      "grad_norm": 7.321568489074707,
      "learning_rate": 1.1153323933364668e-05,
      "loss": 1.6639,
      "step": 595100
    },
    {
      "epoch": 46.623844587184706,
      "grad_norm": 6.624613285064697,
      "learning_rate": 1.1146796177346077e-05,
      "loss": 1.6737,
      "step": 595200
    },
    {
      "epoch": 46.63167789440702,
      "grad_norm": 6.457390785217285,
      "learning_rate": 1.1140268421327485e-05,
      "loss": 1.5675,
      "step": 595300
    },
    {
      "epoch": 46.639511201629325,
      "grad_norm": 4.898693084716797,
      "learning_rate": 1.1133740665308894e-05,
      "loss": 1.6629,
      "step": 595400
    },
    {
      "epoch": 46.64734450885164,
      "grad_norm": 6.580896377563477,
      "learning_rate": 1.1127212909290303e-05,
      "loss": 1.6929,
      "step": 595500
    },
    {
      "epoch": 46.655177816073945,
      "grad_norm": 5.3905181884765625,
      "learning_rate": 1.1120685153271712e-05,
      "loss": 1.5816,
      "step": 595600
    },
    {
      "epoch": 46.66301112329626,
      "grad_norm": 8.641242027282715,
      "learning_rate": 1.1114157397253121e-05,
      "loss": 1.7572,
      "step": 595700
    },
    {
      "epoch": 46.670844430518564,
      "grad_norm": 6.739548683166504,
      "learning_rate": 1.1107629641234529e-05,
      "loss": 1.6304,
      "step": 595800
    },
    {
      "epoch": 46.67867773774088,
      "grad_norm": 6.51738166809082,
      "learning_rate": 1.110110188521594e-05,
      "loss": 1.64,
      "step": 595900
    },
    {
      "epoch": 46.68651104496318,
      "grad_norm": 6.321924686431885,
      "learning_rate": 1.1094574129197349e-05,
      "loss": 1.5899,
      "step": 596000
    },
    {
      "epoch": 46.694344352185496,
      "grad_norm": 7.298687934875488,
      "learning_rate": 1.1088046373178756e-05,
      "loss": 1.6153,
      "step": 596100
    },
    {
      "epoch": 46.7021776594078,
      "grad_norm": 5.433426380157471,
      "learning_rate": 1.1081518617160165e-05,
      "loss": 1.5808,
      "step": 596200
    },
    {
      "epoch": 46.71001096663011,
      "grad_norm": 6.633382797241211,
      "learning_rate": 1.1074990861141574e-05,
      "loss": 1.6146,
      "step": 596300
    },
    {
      "epoch": 46.71784427385242,
      "grad_norm": 5.22654914855957,
      "learning_rate": 1.1068463105122984e-05,
      "loss": 1.559,
      "step": 596400
    },
    {
      "epoch": 46.72567758107473,
      "grad_norm": 5.918889045715332,
      "learning_rate": 1.1061935349104393e-05,
      "loss": 1.675,
      "step": 596500
    },
    {
      "epoch": 46.73351088829704,
      "grad_norm": 9.461064338684082,
      "learning_rate": 1.10554075930858e-05,
      "loss": 1.6747,
      "step": 596600
    },
    {
      "epoch": 46.741344195519346,
      "grad_norm": 4.541627883911133,
      "learning_rate": 1.1048879837067211e-05,
      "loss": 1.6743,
      "step": 596700
    },
    {
      "epoch": 46.74917750274166,
      "grad_norm": 7.629693031311035,
      "learning_rate": 1.104235208104862e-05,
      "loss": 1.6997,
      "step": 596800
    },
    {
      "epoch": 46.757010809963965,
      "grad_norm": 5.767812728881836,
      "learning_rate": 1.1035824325030028e-05,
      "loss": 1.628,
      "step": 596900
    },
    {
      "epoch": 46.76484411718628,
      "grad_norm": 10.023980140686035,
      "learning_rate": 1.1029296569011437e-05,
      "loss": 1.7008,
      "step": 597000
    },
    {
      "epoch": 46.772677424408585,
      "grad_norm": 5.08681583404541,
      "learning_rate": 1.1022768812992846e-05,
      "loss": 1.5965,
      "step": 597100
    },
    {
      "epoch": 46.7805107316309,
      "grad_norm": 7.1679582595825195,
      "learning_rate": 1.1016241056974255e-05,
      "loss": 1.6857,
      "step": 597200
    },
    {
      "epoch": 46.788344038853204,
      "grad_norm": 6.898012161254883,
      "learning_rate": 1.1009713300955664e-05,
      "loss": 1.6112,
      "step": 597300
    },
    {
      "epoch": 46.79617734607551,
      "grad_norm": 6.772216796875,
      "learning_rate": 1.1003185544937072e-05,
      "loss": 1.7551,
      "step": 597400
    },
    {
      "epoch": 46.80401065329782,
      "grad_norm": 6.425974369049072,
      "learning_rate": 1.0996657788918483e-05,
      "loss": 1.6835,
      "step": 597500
    },
    {
      "epoch": 46.81184396052013,
      "grad_norm": 5.889892101287842,
      "learning_rate": 1.0990130032899892e-05,
      "loss": 1.7126,
      "step": 597600
    },
    {
      "epoch": 46.81967726774244,
      "grad_norm": 5.387754917144775,
      "learning_rate": 1.09836022768813e-05,
      "loss": 1.6579,
      "step": 597700
    },
    {
      "epoch": 46.82751057496475,
      "grad_norm": 5.988185882568359,
      "learning_rate": 1.0977074520862708e-05,
      "loss": 1.781,
      "step": 597800
    },
    {
      "epoch": 46.83534388218706,
      "grad_norm": 6.388909339904785,
      "learning_rate": 1.0970546764844118e-05,
      "loss": 1.6397,
      "step": 597900
    },
    {
      "epoch": 46.84317718940937,
      "grad_norm": 6.313479423522949,
      "learning_rate": 1.0964019008825527e-05,
      "loss": 1.6768,
      "step": 598000
    },
    {
      "epoch": 46.85101049663168,
      "grad_norm": 6.505190849304199,
      "learning_rate": 1.0957491252806936e-05,
      "loss": 1.7256,
      "step": 598100
    },
    {
      "epoch": 46.858843803853986,
      "grad_norm": 5.001928329467773,
      "learning_rate": 1.0950963496788343e-05,
      "loss": 1.6179,
      "step": 598200
    },
    {
      "epoch": 46.8666771110763,
      "grad_norm": 11.397171974182129,
      "learning_rate": 1.0944435740769754e-05,
      "loss": 1.6854,
      "step": 598300
    },
    {
      "epoch": 46.874510418298605,
      "grad_norm": 7.032060623168945,
      "learning_rate": 1.0937907984751163e-05,
      "loss": 1.6868,
      "step": 598400
    },
    {
      "epoch": 46.88234372552091,
      "grad_norm": 8.395820617675781,
      "learning_rate": 1.093138022873257e-05,
      "loss": 1.6472,
      "step": 598500
    },
    {
      "epoch": 46.890177032743225,
      "grad_norm": 5.556825637817383,
      "learning_rate": 1.092485247271398e-05,
      "loss": 1.6477,
      "step": 598600
    },
    {
      "epoch": 46.89801033996553,
      "grad_norm": 5.747743129730225,
      "learning_rate": 1.0918324716695389e-05,
      "loss": 1.5129,
      "step": 598700
    },
    {
      "epoch": 46.905843647187844,
      "grad_norm": 6.774238109588623,
      "learning_rate": 1.0911796960676798e-05,
      "loss": 1.6081,
      "step": 598800
    },
    {
      "epoch": 46.91367695441015,
      "grad_norm": 7.917942047119141,
      "learning_rate": 1.0905269204658207e-05,
      "loss": 1.6561,
      "step": 598900
    },
    {
      "epoch": 46.92151026163246,
      "grad_norm": 12.138001441955566,
      "learning_rate": 1.0898741448639615e-05,
      "loss": 1.5988,
      "step": 599000
    },
    {
      "epoch": 46.92934356885477,
      "grad_norm": 2.7037482261657715,
      "learning_rate": 1.0892213692621026e-05,
      "loss": 1.6049,
      "step": 599100
    },
    {
      "epoch": 46.93717687607708,
      "grad_norm": 6.702340602874756,
      "learning_rate": 1.0885685936602435e-05,
      "loss": 1.6576,
      "step": 599200
    },
    {
      "epoch": 46.94501018329939,
      "grad_norm": 6.522744178771973,
      "learning_rate": 1.0879158180583842e-05,
      "loss": 1.6273,
      "step": 599300
    },
    {
      "epoch": 46.9528434905217,
      "grad_norm": 5.005252838134766,
      "learning_rate": 1.0872630424565251e-05,
      "loss": 1.7312,
      "step": 599400
    },
    {
      "epoch": 46.96067679774401,
      "grad_norm": 4.065559387207031,
      "learning_rate": 1.086610266854666e-05,
      "loss": 1.5951,
      "step": 599500
    },
    {
      "epoch": 46.96851010496631,
      "grad_norm": 6.604663372039795,
      "learning_rate": 1.085957491252807e-05,
      "loss": 1.7642,
      "step": 599600
    },
    {
      "epoch": 46.976343412188626,
      "grad_norm": 9.888587951660156,
      "learning_rate": 1.0853047156509479e-05,
      "loss": 1.6159,
      "step": 599700
    },
    {
      "epoch": 46.98417671941093,
      "grad_norm": 3.2214863300323486,
      "learning_rate": 1.0846519400490886e-05,
      "loss": 1.5207,
      "step": 599800
    },
    {
      "epoch": 46.992010026633245,
      "grad_norm": 7.1482954025268555,
      "learning_rate": 1.0839991644472297e-05,
      "loss": 1.5999,
      "step": 599900
    },
    {
      "epoch": 46.99984333385555,
      "grad_norm": 10.041339874267578,
      "learning_rate": 1.0833463888453706e-05,
      "loss": 1.757,
      "step": 600000
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.768224835395813,
      "eval_runtime": 1.5012,
      "eval_samples_per_second": 447.63,
      "eval_steps_per_second": 447.63,
      "step": 600002
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.38981032371521,
      "eval_runtime": 29.3818,
      "eval_samples_per_second": 434.487,
      "eval_steps_per_second": 434.487,
      "step": 600002
    },
    {
      "epoch": 47.007676641077865,
      "grad_norm": 8.088028907775879,
      "learning_rate": 1.0826936132435114e-05,
      "loss": 1.628,
      "step": 600100
    },
    {
      "epoch": 47.01550994830017,
      "grad_norm": 5.989517688751221,
      "learning_rate": 1.0820408376416523e-05,
      "loss": 1.6248,
      "step": 600200
    },
    {
      "epoch": 47.023343255522484,
      "grad_norm": 8.373391151428223,
      "learning_rate": 1.0813880620397932e-05,
      "loss": 1.5939,
      "step": 600300
    },
    {
      "epoch": 47.03117656274479,
      "grad_norm": 7.587339401245117,
      "learning_rate": 1.0807352864379341e-05,
      "loss": 1.6751,
      "step": 600400
    },
    {
      "epoch": 47.0390098699671,
      "grad_norm": 7.741963863372803,
      "learning_rate": 1.080082510836075e-05,
      "loss": 1.5943,
      "step": 600500
    },
    {
      "epoch": 47.04684317718941,
      "grad_norm": 7.5118303298950195,
      "learning_rate": 1.079429735234216e-05,
      "loss": 1.6837,
      "step": 600600
    },
    {
      "epoch": 47.05467648441172,
      "grad_norm": 6.869003772735596,
      "learning_rate": 1.0787769596323569e-05,
      "loss": 1.6358,
      "step": 600700
    },
    {
      "epoch": 47.06250979163403,
      "grad_norm": 4.776145935058594,
      "learning_rate": 1.0781241840304978e-05,
      "loss": 1.5943,
      "step": 600800
    },
    {
      "epoch": 47.070343098856334,
      "grad_norm": 4.520444869995117,
      "learning_rate": 1.0774714084286385e-05,
      "loss": 1.616,
      "step": 600900
    },
    {
      "epoch": 47.07817640607865,
      "grad_norm": 5.015792369842529,
      "learning_rate": 1.0768186328267796e-05,
      "loss": 1.5827,
      "step": 601000
    },
    {
      "epoch": 47.08600971330095,
      "grad_norm": 7.399521350860596,
      "learning_rate": 1.0761658572249204e-05,
      "loss": 1.5917,
      "step": 601100
    },
    {
      "epoch": 47.093843020523266,
      "grad_norm": 7.188188076019287,
      "learning_rate": 1.0755130816230613e-05,
      "loss": 1.6523,
      "step": 601200
    },
    {
      "epoch": 47.10167632774557,
      "grad_norm": 5.721461772918701,
      "learning_rate": 1.0748603060212022e-05,
      "loss": 1.6371,
      "step": 601300
    },
    {
      "epoch": 47.109509634967885,
      "grad_norm": 7.856752872467041,
      "learning_rate": 1.0742075304193431e-05,
      "loss": 1.6741,
      "step": 601400
    },
    {
      "epoch": 47.11734294219019,
      "grad_norm": 3.9224462509155273,
      "learning_rate": 1.073554754817484e-05,
      "loss": 1.6408,
      "step": 601500
    },
    {
      "epoch": 47.125176249412505,
      "grad_norm": 5.779416561126709,
      "learning_rate": 1.072901979215625e-05,
      "loss": 1.6781,
      "step": 601600
    },
    {
      "epoch": 47.13300955663481,
      "grad_norm": 6.06264591217041,
      "learning_rate": 1.0722492036137657e-05,
      "loss": 1.6161,
      "step": 601700
    },
    {
      "epoch": 47.140842863857124,
      "grad_norm": 6.420807838439941,
      "learning_rate": 1.0715964280119068e-05,
      "loss": 1.6754,
      "step": 601800
    },
    {
      "epoch": 47.14867617107943,
      "grad_norm": 7.027204990386963,
      "learning_rate": 1.0709436524100475e-05,
      "loss": 1.6368,
      "step": 601900
    },
    {
      "epoch": 47.156509478301736,
      "grad_norm": 8.541768074035645,
      "learning_rate": 1.0702908768081884e-05,
      "loss": 1.6392,
      "step": 602000
    },
    {
      "epoch": 47.16434278552405,
      "grad_norm": 8.765677452087402,
      "learning_rate": 1.0696381012063294e-05,
      "loss": 1.4906,
      "step": 602100
    },
    {
      "epoch": 47.172176092746355,
      "grad_norm": 6.993729591369629,
      "learning_rate": 1.0689853256044703e-05,
      "loss": 1.6018,
      "step": 602200
    },
    {
      "epoch": 47.18000939996867,
      "grad_norm": 7.10455846786499,
      "learning_rate": 1.0683325500026112e-05,
      "loss": 1.6467,
      "step": 602300
    },
    {
      "epoch": 47.187842707190974,
      "grad_norm": 6.533634662628174,
      "learning_rate": 1.0676797744007521e-05,
      "loss": 1.7803,
      "step": 602400
    },
    {
      "epoch": 47.19567601441329,
      "grad_norm": 4.657525062561035,
      "learning_rate": 1.0670269987988929e-05,
      "loss": 1.6274,
      "step": 602500
    },
    {
      "epoch": 47.20350932163559,
      "grad_norm": 6.182844638824463,
      "learning_rate": 1.066374223197034e-05,
      "loss": 1.6635,
      "step": 602600
    },
    {
      "epoch": 47.211342628857906,
      "grad_norm": 6.41457462310791,
      "learning_rate": 1.0657214475951747e-05,
      "loss": 1.5889,
      "step": 602700
    },
    {
      "epoch": 47.21917593608021,
      "grad_norm": 8.388641357421875,
      "learning_rate": 1.0650686719933156e-05,
      "loss": 1.59,
      "step": 602800
    },
    {
      "epoch": 47.227009243302525,
      "grad_norm": 5.4254961013793945,
      "learning_rate": 1.0644158963914565e-05,
      "loss": 1.618,
      "step": 602900
    },
    {
      "epoch": 47.23484255052483,
      "grad_norm": 7.343847274780273,
      "learning_rate": 1.0637631207895974e-05,
      "loss": 1.6376,
      "step": 603000
    },
    {
      "epoch": 47.24267585774714,
      "grad_norm": 5.54373836517334,
      "learning_rate": 1.0631103451877383e-05,
      "loss": 1.5386,
      "step": 603100
    },
    {
      "epoch": 47.25050916496945,
      "grad_norm": 6.3914313316345215,
      "learning_rate": 1.0624575695858793e-05,
      "loss": 1.7492,
      "step": 603200
    },
    {
      "epoch": 47.25834247219176,
      "grad_norm": 6.401015758514404,
      "learning_rate": 1.06180479398402e-05,
      "loss": 1.6171,
      "step": 603300
    },
    {
      "epoch": 47.26617577941407,
      "grad_norm": 7.856551647186279,
      "learning_rate": 1.0611520183821611e-05,
      "loss": 1.6184,
      "step": 603400
    },
    {
      "epoch": 47.274009086636376,
      "grad_norm": 7.722461700439453,
      "learning_rate": 1.0604992427803018e-05,
      "loss": 1.565,
      "step": 603500
    },
    {
      "epoch": 47.28184239385869,
      "grad_norm": 6.147442817687988,
      "learning_rate": 1.0598464671784428e-05,
      "loss": 1.5673,
      "step": 603600
    },
    {
      "epoch": 47.289675701080995,
      "grad_norm": 8.503679275512695,
      "learning_rate": 1.0591936915765837e-05,
      "loss": 1.5648,
      "step": 603700
    },
    {
      "epoch": 47.29750900830331,
      "grad_norm": 5.553588390350342,
      "learning_rate": 1.0585409159747246e-05,
      "loss": 1.6569,
      "step": 603800
    },
    {
      "epoch": 47.305342315525614,
      "grad_norm": 8.260546684265137,
      "learning_rate": 1.0578881403728655e-05,
      "loss": 1.6234,
      "step": 603900
    },
    {
      "epoch": 47.31317562274793,
      "grad_norm": 6.551891803741455,
      "learning_rate": 1.0572353647710064e-05,
      "loss": 1.6867,
      "step": 604000
    },
    {
      "epoch": 47.32100892997023,
      "grad_norm": 5.984936714172363,
      "learning_rate": 1.0565825891691472e-05,
      "loss": 1.6756,
      "step": 604100
    },
    {
      "epoch": 47.32884223719254,
      "grad_norm": 7.933581352233887,
      "learning_rate": 1.0559298135672883e-05,
      "loss": 1.7183,
      "step": 604200
    },
    {
      "epoch": 47.33667554441485,
      "grad_norm": 5.238315582275391,
      "learning_rate": 1.055277037965429e-05,
      "loss": 1.5694,
      "step": 604300
    },
    {
      "epoch": 47.34450885163716,
      "grad_norm": 6.609102249145508,
      "learning_rate": 1.0546242623635699e-05,
      "loss": 1.5598,
      "step": 604400
    },
    {
      "epoch": 47.35234215885947,
      "grad_norm": 6.821014881134033,
      "learning_rate": 1.0539714867617108e-05,
      "loss": 1.6859,
      "step": 604500
    },
    {
      "epoch": 47.36017546608178,
      "grad_norm": 5.418581962585449,
      "learning_rate": 1.0533187111598517e-05,
      "loss": 1.6268,
      "step": 604600
    },
    {
      "epoch": 47.36800877330409,
      "grad_norm": 7.866750717163086,
      "learning_rate": 1.0526659355579927e-05,
      "loss": 1.64,
      "step": 604700
    },
    {
      "epoch": 47.3758420805264,
      "grad_norm": 8.222426414489746,
      "learning_rate": 1.0520131599561336e-05,
      "loss": 1.5928,
      "step": 604800
    },
    {
      "epoch": 47.38367538774871,
      "grad_norm": 4.118166446685791,
      "learning_rate": 1.0513603843542743e-05,
      "loss": 1.698,
      "step": 604900
    },
    {
      "epoch": 47.391508694971016,
      "grad_norm": 6.820868015289307,
      "learning_rate": 1.0507076087524154e-05,
      "loss": 1.6595,
      "step": 605000
    },
    {
      "epoch": 47.39934200219333,
      "grad_norm": 6.648158550262451,
      "learning_rate": 1.0500548331505562e-05,
      "loss": 1.6607,
      "step": 605100
    },
    {
      "epoch": 47.407175309415635,
      "grad_norm": 8.67383861541748,
      "learning_rate": 1.049402057548697e-05,
      "loss": 1.7426,
      "step": 605200
    },
    {
      "epoch": 47.41500861663795,
      "grad_norm": 6.7156877517700195,
      "learning_rate": 1.048749281946838e-05,
      "loss": 1.6293,
      "step": 605300
    },
    {
      "epoch": 47.422841923860254,
      "grad_norm": 5.093835830688477,
      "learning_rate": 1.0480965063449789e-05,
      "loss": 1.6655,
      "step": 605400
    },
    {
      "epoch": 47.43067523108256,
      "grad_norm": 6.3628129959106445,
      "learning_rate": 1.0474437307431198e-05,
      "loss": 1.6879,
      "step": 605500
    },
    {
      "epoch": 47.43850853830487,
      "grad_norm": 9.930179595947266,
      "learning_rate": 1.0467909551412607e-05,
      "loss": 1.6002,
      "step": 605600
    },
    {
      "epoch": 47.44634184552718,
      "grad_norm": 8.141881942749023,
      "learning_rate": 1.0461381795394015e-05,
      "loss": 1.6058,
      "step": 605700
    },
    {
      "epoch": 47.45417515274949,
      "grad_norm": 5.408134937286377,
      "learning_rate": 1.0454854039375426e-05,
      "loss": 1.7208,
      "step": 605800
    },
    {
      "epoch": 47.4620084599718,
      "grad_norm": 6.264824867248535,
      "learning_rate": 1.0448326283356833e-05,
      "loss": 1.5958,
      "step": 605900
    },
    {
      "epoch": 47.46984176719411,
      "grad_norm": 6.846795082092285,
      "learning_rate": 1.0441798527338242e-05,
      "loss": 1.6401,
      "step": 606000
    },
    {
      "epoch": 47.47767507441642,
      "grad_norm": 5.980411052703857,
      "learning_rate": 1.0435270771319651e-05,
      "loss": 1.5752,
      "step": 606100
    },
    {
      "epoch": 47.48550838163873,
      "grad_norm": 7.43682861328125,
      "learning_rate": 1.042874301530106e-05,
      "loss": 1.5576,
      "step": 606200
    },
    {
      "epoch": 47.49334168886104,
      "grad_norm": 6.249880790710449,
      "learning_rate": 1.042221525928247e-05,
      "loss": 1.6873,
      "step": 606300
    },
    {
      "epoch": 47.50117499608335,
      "grad_norm": 7.416605472564697,
      "learning_rate": 1.0415687503263879e-05,
      "loss": 1.629,
      "step": 606400
    },
    {
      "epoch": 47.509008303305656,
      "grad_norm": 7.213023662567139,
      "learning_rate": 1.0409159747245288e-05,
      "loss": 1.776,
      "step": 606500
    },
    {
      "epoch": 47.51684161052796,
      "grad_norm": 4.579501628875732,
      "learning_rate": 1.0402631991226697e-05,
      "loss": 1.6361,
      "step": 606600
    },
    {
      "epoch": 47.524674917750275,
      "grad_norm": 6.065973281860352,
      "learning_rate": 1.0396104235208105e-05,
      "loss": 1.6139,
      "step": 606700
    },
    {
      "epoch": 47.53250822497258,
      "grad_norm": 5.881953239440918,
      "learning_rate": 1.0389576479189514e-05,
      "loss": 1.6306,
      "step": 606800
    },
    {
      "epoch": 47.540341532194894,
      "grad_norm": 7.802358150482178,
      "learning_rate": 1.0383048723170925e-05,
      "loss": 1.6539,
      "step": 606900
    },
    {
      "epoch": 47.5481748394172,
      "grad_norm": 6.806520462036133,
      "learning_rate": 1.0376520967152332e-05,
      "loss": 1.5847,
      "step": 607000
    },
    {
      "epoch": 47.55600814663951,
      "grad_norm": 6.213242053985596,
      "learning_rate": 1.0369993211133741e-05,
      "loss": 1.5911,
      "step": 607100
    },
    {
      "epoch": 47.56384145386182,
      "grad_norm": 6.826953411102295,
      "learning_rate": 1.036346545511515e-05,
      "loss": 1.6548,
      "step": 607200
    },
    {
      "epoch": 47.57167476108413,
      "grad_norm": 6.933564186096191,
      "learning_rate": 1.035693769909656e-05,
      "loss": 1.655,
      "step": 607300
    },
    {
      "epoch": 47.57950806830644,
      "grad_norm": 5.730192184448242,
      "learning_rate": 1.0350409943077969e-05,
      "loss": 1.5583,
      "step": 607400
    },
    {
      "epoch": 47.58734137552875,
      "grad_norm": 6.092447280883789,
      "learning_rate": 1.0343882187059376e-05,
      "loss": 1.6088,
      "step": 607500
    },
    {
      "epoch": 47.59517468275106,
      "grad_norm": 5.90043830871582,
      "learning_rate": 1.0337354431040785e-05,
      "loss": 1.6537,
      "step": 607600
    },
    {
      "epoch": 47.603007989973364,
      "grad_norm": 6.139391899108887,
      "learning_rate": 1.0330826675022196e-05,
      "loss": 1.6537,
      "step": 607700
    },
    {
      "epoch": 47.61084129719568,
      "grad_norm": 5.697770595550537,
      "learning_rate": 1.0324298919003604e-05,
      "loss": 1.6898,
      "step": 607800
    },
    {
      "epoch": 47.61867460441798,
      "grad_norm": 4.843623161315918,
      "learning_rate": 1.0317771162985013e-05,
      "loss": 1.5963,
      "step": 607900
    },
    {
      "epoch": 47.626507911640296,
      "grad_norm": 6.141672611236572,
      "learning_rate": 1.0311243406966422e-05,
      "loss": 1.6088,
      "step": 608000
    },
    {
      "epoch": 47.6343412188626,
      "grad_norm": 6.559848308563232,
      "learning_rate": 1.0304715650947831e-05,
      "loss": 1.5642,
      "step": 608100
    },
    {
      "epoch": 47.642174526084915,
      "grad_norm": 9.424032211303711,
      "learning_rate": 1.029818789492924e-05,
      "loss": 1.6178,
      "step": 608200
    },
    {
      "epoch": 47.65000783330722,
      "grad_norm": 6.678852081298828,
      "learning_rate": 1.0291660138910648e-05,
      "loss": 1.6077,
      "step": 608300
    },
    {
      "epoch": 47.657841140529534,
      "grad_norm": 9.36658763885498,
      "learning_rate": 1.0285132382892057e-05,
      "loss": 1.6371,
      "step": 608400
    },
    {
      "epoch": 47.66567444775184,
      "grad_norm": 5.900897026062012,
      "learning_rate": 1.0278604626873468e-05,
      "loss": 1.6204,
      "step": 608500
    },
    {
      "epoch": 47.67350775497415,
      "grad_norm": 10.713296890258789,
      "learning_rate": 1.0272076870854875e-05,
      "loss": 1.7138,
      "step": 608600
    },
    {
      "epoch": 47.68134106219646,
      "grad_norm": 6.0647759437561035,
      "learning_rate": 1.0265549114836284e-05,
      "loss": 1.6773,
      "step": 608700
    },
    {
      "epoch": 47.689174369418765,
      "grad_norm": 7.187102317810059,
      "learning_rate": 1.0259021358817694e-05,
      "loss": 1.6073,
      "step": 608800
    },
    {
      "epoch": 47.69700767664108,
      "grad_norm": 6.465943813323975,
      "learning_rate": 1.0252493602799103e-05,
      "loss": 1.6142,
      "step": 608900
    },
    {
      "epoch": 47.704840983863384,
      "grad_norm": 7.369331359863281,
      "learning_rate": 1.0245965846780512e-05,
      "loss": 1.6925,
      "step": 609000
    },
    {
      "epoch": 47.7126742910857,
      "grad_norm": 4.981117248535156,
      "learning_rate": 1.023943809076192e-05,
      "loss": 1.637,
      "step": 609100
    },
    {
      "epoch": 47.720507598308004,
      "grad_norm": 6.841352939605713,
      "learning_rate": 1.0232910334743328e-05,
      "loss": 1.6838,
      "step": 609200
    },
    {
      "epoch": 47.72834090553032,
      "grad_norm": 5.9805378913879395,
      "learning_rate": 1.022638257872474e-05,
      "loss": 1.5453,
      "step": 609300
    },
    {
      "epoch": 47.73617421275262,
      "grad_norm": 8.587634086608887,
      "learning_rate": 1.0219854822706147e-05,
      "loss": 1.6428,
      "step": 609400
    },
    {
      "epoch": 47.744007519974936,
      "grad_norm": 7.005102634429932,
      "learning_rate": 1.0213327066687556e-05,
      "loss": 1.6708,
      "step": 609500
    },
    {
      "epoch": 47.75184082719724,
      "grad_norm": 6.167470455169678,
      "learning_rate": 1.0206799310668965e-05,
      "loss": 1.5219,
      "step": 609600
    },
    {
      "epoch": 47.759674134419555,
      "grad_norm": 6.048064708709717,
      "learning_rate": 1.0200271554650374e-05,
      "loss": 1.6137,
      "step": 609700
    },
    {
      "epoch": 47.76750744164186,
      "grad_norm": 7.6703267097473145,
      "learning_rate": 1.0193743798631783e-05,
      "loss": 1.668,
      "step": 609800
    },
    {
      "epoch": 47.77534074886417,
      "grad_norm": 5.75773286819458,
      "learning_rate": 1.0187216042613191e-05,
      "loss": 1.6329,
      "step": 609900
    },
    {
      "epoch": 47.78317405608648,
      "grad_norm": 6.786561489105225,
      "learning_rate": 1.01806882865946e-05,
      "loss": 1.6666,
      "step": 610000
    },
    {
      "epoch": 47.791007363308786,
      "grad_norm": 8.20206356048584,
      "learning_rate": 1.0174160530576011e-05,
      "loss": 1.626,
      "step": 610100
    },
    {
      "epoch": 47.7988406705311,
      "grad_norm": 6.32942008972168,
      "learning_rate": 1.0167632774557418e-05,
      "loss": 1.7068,
      "step": 610200
    },
    {
      "epoch": 47.806673977753405,
      "grad_norm": 7.288670063018799,
      "learning_rate": 1.0161105018538828e-05,
      "loss": 1.6279,
      "step": 610300
    },
    {
      "epoch": 47.81450728497572,
      "grad_norm": 6.125120162963867,
      "learning_rate": 1.0154577262520237e-05,
      "loss": 1.6628,
      "step": 610400
    },
    {
      "epoch": 47.822340592198024,
      "grad_norm": 7.382737636566162,
      "learning_rate": 1.0148049506501646e-05,
      "loss": 1.5101,
      "step": 610500
    },
    {
      "epoch": 47.83017389942034,
      "grad_norm": 8.097515106201172,
      "learning_rate": 1.0141521750483055e-05,
      "loss": 1.6821,
      "step": 610600
    },
    {
      "epoch": 47.838007206642644,
      "grad_norm": 6.5811591148376465,
      "learning_rate": 1.0134993994464462e-05,
      "loss": 1.5745,
      "step": 610700
    },
    {
      "epoch": 47.84584051386496,
      "grad_norm": 7.178587436676025,
      "learning_rate": 1.0128466238445872e-05,
      "loss": 1.7563,
      "step": 610800
    },
    {
      "epoch": 47.85367382108726,
      "grad_norm": 8.249798774719238,
      "learning_rate": 1.0121938482427282e-05,
      "loss": 1.6994,
      "step": 610900
    },
    {
      "epoch": 47.86150712830957,
      "grad_norm": 5.594314098358154,
      "learning_rate": 1.011541072640869e-05,
      "loss": 1.657,
      "step": 611000
    },
    {
      "epoch": 47.86934043553188,
      "grad_norm": 6.925532341003418,
      "learning_rate": 1.0108882970390099e-05,
      "loss": 1.5982,
      "step": 611100
    },
    {
      "epoch": 47.87717374275419,
      "grad_norm": 4.813649654388428,
      "learning_rate": 1.0102355214371508e-05,
      "loss": 1.6058,
      "step": 611200
    },
    {
      "epoch": 47.8850070499765,
      "grad_norm": 7.473074436187744,
      "learning_rate": 1.0095827458352917e-05,
      "loss": 1.5955,
      "step": 611300
    },
    {
      "epoch": 47.89284035719881,
      "grad_norm": 6.383035182952881,
      "learning_rate": 1.0089299702334327e-05,
      "loss": 1.7164,
      "step": 611400
    },
    {
      "epoch": 47.90067366442112,
      "grad_norm": 7.784796237945557,
      "learning_rate": 1.0082771946315734e-05,
      "loss": 1.6225,
      "step": 611500
    },
    {
      "epoch": 47.908506971643426,
      "grad_norm": 5.362626552581787,
      "learning_rate": 1.0076244190297143e-05,
      "loss": 1.6604,
      "step": 611600
    },
    {
      "epoch": 47.91634027886574,
      "grad_norm": 5.417713642120361,
      "learning_rate": 1.0069716434278554e-05,
      "loss": 1.7048,
      "step": 611700
    },
    {
      "epoch": 47.924173586088045,
      "grad_norm": 5.512042999267578,
      "learning_rate": 1.0063188678259961e-05,
      "loss": 1.61,
      "step": 611800
    },
    {
      "epoch": 47.93200689331036,
      "grad_norm": 8.991780281066895,
      "learning_rate": 1.005666092224137e-05,
      "loss": 1.6034,
      "step": 611900
    },
    {
      "epoch": 47.939840200532664,
      "grad_norm": 6.423800468444824,
      "learning_rate": 1.005013316622278e-05,
      "loss": 1.6733,
      "step": 612000
    },
    {
      "epoch": 47.94767350775498,
      "grad_norm": 7.075628280639648,
      "learning_rate": 1.0043605410204189e-05,
      "loss": 1.6712,
      "step": 612100
    },
    {
      "epoch": 47.955506814977284,
      "grad_norm": 7.000735759735107,
      "learning_rate": 1.0037077654185598e-05,
      "loss": 1.712,
      "step": 612200
    },
    {
      "epoch": 47.96334012219959,
      "grad_norm": 6.874274253845215,
      "learning_rate": 1.0030549898167006e-05,
      "loss": 1.6549,
      "step": 612300
    },
    {
      "epoch": 47.9711734294219,
      "grad_norm": 7.047308921813965,
      "learning_rate": 1.0024022142148416e-05,
      "loss": 1.643,
      "step": 612400
    },
    {
      "epoch": 47.97900673664421,
      "grad_norm": 4.44744348526001,
      "learning_rate": 1.0017494386129826e-05,
      "loss": 1.6121,
      "step": 612500
    },
    {
      "epoch": 47.98684004386652,
      "grad_norm": 5.657121181488037,
      "learning_rate": 1.0010966630111233e-05,
      "loss": 1.7376,
      "step": 612600
    },
    {
      "epoch": 47.99467335108883,
      "grad_norm": 5.431872844696045,
      "learning_rate": 1.0004438874092642e-05,
      "loss": 1.6264,
      "step": 612700
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.7718349695205688,
      "eval_runtime": 1.5733,
      "eval_samples_per_second": 427.135,
      "eval_steps_per_second": 427.135,
      "step": 612768
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.3888185024261475,
      "eval_runtime": 29.1792,
      "eval_samples_per_second": 437.504,
      "eval_steps_per_second": 437.504,
      "step": 612768
    },
    {
      "epoch": 48.00250665831114,
      "grad_norm": 5.49972677230835,
      "learning_rate": 9.997911118074051e-06,
      "loss": 1.647,
      "step": 612800
    },
    {
      "epoch": 48.01033996553345,
      "grad_norm": 6.089720249176025,
      "learning_rate": 9.99138336205546e-06,
      "loss": 1.6281,
      "step": 612900
    },
    {
      "epoch": 48.01817327275576,
      "grad_norm": 5.353787422180176,
      "learning_rate": 9.98485560603687e-06,
      "loss": 1.575,
      "step": 613000
    },
    {
      "epoch": 48.026006579978066,
      "grad_norm": 2.5330846309661865,
      "learning_rate": 9.978327850018277e-06,
      "loss": 1.5921,
      "step": 613100
    },
    {
      "epoch": 48.03383988720038,
      "grad_norm": 5.291196823120117,
      "learning_rate": 9.971800093999688e-06,
      "loss": 1.5639,
      "step": 613200
    },
    {
      "epoch": 48.041673194422685,
      "grad_norm": 9.162732124328613,
      "learning_rate": 9.965272337981097e-06,
      "loss": 1.7307,
      "step": 613300
    },
    {
      "epoch": 48.04950650164499,
      "grad_norm": 6.4746994972229,
      "learning_rate": 9.958744581962505e-06,
      "loss": 1.5422,
      "step": 613400
    },
    {
      "epoch": 48.057339808867304,
      "grad_norm": 7.747020721435547,
      "learning_rate": 9.952216825943914e-06,
      "loss": 1.6317,
      "step": 613500
    },
    {
      "epoch": 48.06517311608961,
      "grad_norm": 7.504902362823486,
      "learning_rate": 9.945689069925323e-06,
      "loss": 1.6538,
      "step": 613600
    },
    {
      "epoch": 48.073006423311924,
      "grad_norm": 4.6843061447143555,
      "learning_rate": 9.939161313906732e-06,
      "loss": 1.578,
      "step": 613700
    },
    {
      "epoch": 48.08083973053423,
      "grad_norm": 5.330453872680664,
      "learning_rate": 9.932633557888141e-06,
      "loss": 1.5756,
      "step": 613800
    },
    {
      "epoch": 48.08867303775654,
      "grad_norm": 8.94079303741455,
      "learning_rate": 9.926105801869549e-06,
      "loss": 1.5736,
      "step": 613900
    },
    {
      "epoch": 48.09650634497885,
      "grad_norm": 6.604156017303467,
      "learning_rate": 9.91957804585096e-06,
      "loss": 1.4917,
      "step": 614000
    },
    {
      "epoch": 48.10433965220116,
      "grad_norm": 5.609421730041504,
      "learning_rate": 9.913050289832369e-06,
      "loss": 1.607,
      "step": 614100
    },
    {
      "epoch": 48.11217295942347,
      "grad_norm": 9.727174758911133,
      "learning_rate": 9.906522533813776e-06,
      "loss": 1.6086,
      "step": 614200
    },
    {
      "epoch": 48.12000626664578,
      "grad_norm": 7.081794738769531,
      "learning_rate": 9.899994777795185e-06,
      "loss": 1.6637,
      "step": 614300
    },
    {
      "epoch": 48.12783957386809,
      "grad_norm": 5.270418167114258,
      "learning_rate": 9.893467021776594e-06,
      "loss": 1.6211,
      "step": 614400
    },
    {
      "epoch": 48.13567288109039,
      "grad_norm": 5.539740085601807,
      "learning_rate": 9.886939265758004e-06,
      "loss": 1.662,
      "step": 614500
    },
    {
      "epoch": 48.143506188312706,
      "grad_norm": 6.412746906280518,
      "learning_rate": 9.880411509739413e-06,
      "loss": 1.7404,
      "step": 614600
    },
    {
      "epoch": 48.15133949553501,
      "grad_norm": 6.036921977996826,
      "learning_rate": 9.87388375372082e-06,
      "loss": 1.6712,
      "step": 614700
    },
    {
      "epoch": 48.159172802757325,
      "grad_norm": 7.869067192077637,
      "learning_rate": 9.867355997702231e-06,
      "loss": 1.628,
      "step": 614800
    },
    {
      "epoch": 48.16700610997963,
      "grad_norm": 7.121051788330078,
      "learning_rate": 9.86082824168364e-06,
      "loss": 1.7021,
      "step": 614900
    },
    {
      "epoch": 48.174839417201945,
      "grad_norm": 5.395070552825928,
      "learning_rate": 9.854300485665048e-06,
      "loss": 1.6847,
      "step": 615000
    },
    {
      "epoch": 48.18267272442425,
      "grad_norm": 7.820547580718994,
      "learning_rate": 9.847772729646457e-06,
      "loss": 1.6799,
      "step": 615100
    },
    {
      "epoch": 48.190506031646564,
      "grad_norm": 5.288370609283447,
      "learning_rate": 9.841244973627866e-06,
      "loss": 1.6837,
      "step": 615200
    },
    {
      "epoch": 48.19833933886887,
      "grad_norm": 7.253615379333496,
      "learning_rate": 9.834717217609275e-06,
      "loss": 1.626,
      "step": 615300
    },
    {
      "epoch": 48.20617264609118,
      "grad_norm": 6.127472400665283,
      "learning_rate": 9.828189461590684e-06,
      "loss": 1.5755,
      "step": 615400
    },
    {
      "epoch": 48.21400595331349,
      "grad_norm": 5.583757400512695,
      "learning_rate": 9.821661705572092e-06,
      "loss": 1.6561,
      "step": 615500
    },
    {
      "epoch": 48.221839260535795,
      "grad_norm": 5.83851957321167,
      "learning_rate": 9.815133949553503e-06,
      "loss": 1.6339,
      "step": 615600
    },
    {
      "epoch": 48.22967256775811,
      "grad_norm": 8.886966705322266,
      "learning_rate": 9.808606193534912e-06,
      "loss": 1.6701,
      "step": 615700
    },
    {
      "epoch": 48.237505874980414,
      "grad_norm": 5.234237194061279,
      "learning_rate": 9.80207843751632e-06,
      "loss": 1.6442,
      "step": 615800
    },
    {
      "epoch": 48.24533918220273,
      "grad_norm": 6.110256195068359,
      "learning_rate": 9.795550681497728e-06,
      "loss": 1.6462,
      "step": 615900
    },
    {
      "epoch": 48.25317248942503,
      "grad_norm": 5.332891941070557,
      "learning_rate": 9.789022925479138e-06,
      "loss": 1.6474,
      "step": 616000
    },
    {
      "epoch": 48.261005796647346,
      "grad_norm": 7.291900634765625,
      "learning_rate": 9.782495169460547e-06,
      "loss": 1.5707,
      "step": 616100
    },
    {
      "epoch": 48.26883910386965,
      "grad_norm": 6.259517669677734,
      "learning_rate": 9.775967413441956e-06,
      "loss": 1.6497,
      "step": 616200
    },
    {
      "epoch": 48.276672411091965,
      "grad_norm": 8.13926887512207,
      "learning_rate": 9.769439657423363e-06,
      "loss": 1.5412,
      "step": 616300
    },
    {
      "epoch": 48.28450571831427,
      "grad_norm": 7.79757833480835,
      "learning_rate": 9.762911901404774e-06,
      "loss": 1.5981,
      "step": 616400
    },
    {
      "epoch": 48.292339025536585,
      "grad_norm": 7.2635393142700195,
      "learning_rate": 9.756384145386183e-06,
      "loss": 1.6625,
      "step": 616500
    },
    {
      "epoch": 48.30017233275889,
      "grad_norm": 6.27377986907959,
      "learning_rate": 9.74985638936759e-06,
      "loss": 1.5906,
      "step": 616600
    },
    {
      "epoch": 48.3080056399812,
      "grad_norm": 7.684952259063721,
      "learning_rate": 9.743328633349e-06,
      "loss": 1.606,
      "step": 616700
    },
    {
      "epoch": 48.31583894720351,
      "grad_norm": 6.547037124633789,
      "learning_rate": 9.736800877330409e-06,
      "loss": 1.6465,
      "step": 616800
    },
    {
      "epoch": 48.323672254425816,
      "grad_norm": 6.531670093536377,
      "learning_rate": 9.730273121311818e-06,
      "loss": 1.749,
      "step": 616900
    },
    {
      "epoch": 48.33150556164813,
      "grad_norm": 6.5428924560546875,
      "learning_rate": 9.723745365293227e-06,
      "loss": 1.6981,
      "step": 617000
    },
    {
      "epoch": 48.339338868870435,
      "grad_norm": 7.467337608337402,
      "learning_rate": 9.717217609274635e-06,
      "loss": 1.6755,
      "step": 617100
    },
    {
      "epoch": 48.34717217609275,
      "grad_norm": 6.019836902618408,
      "learning_rate": 9.710689853256046e-06,
      "loss": 1.6885,
      "step": 617200
    },
    {
      "epoch": 48.355005483315054,
      "grad_norm": 6.334976673126221,
      "learning_rate": 9.704162097237455e-06,
      "loss": 1.5262,
      "step": 617300
    },
    {
      "epoch": 48.36283879053737,
      "grad_norm": 7.854428768157959,
      "learning_rate": 9.697634341218862e-06,
      "loss": 1.5834,
      "step": 617400
    },
    {
      "epoch": 48.37067209775967,
      "grad_norm": 6.845627784729004,
      "learning_rate": 9.691106585200272e-06,
      "loss": 1.6576,
      "step": 617500
    },
    {
      "epoch": 48.378505404981986,
      "grad_norm": 7.221433162689209,
      "learning_rate": 9.68457882918168e-06,
      "loss": 1.6186,
      "step": 617600
    },
    {
      "epoch": 48.38633871220429,
      "grad_norm": 8.001338958740234,
      "learning_rate": 9.67805107316309e-06,
      "loss": 1.6608,
      "step": 617700
    },
    {
      "epoch": 48.394172019426605,
      "grad_norm": 7.39186954498291,
      "learning_rate": 9.671523317144499e-06,
      "loss": 1.5102,
      "step": 617800
    },
    {
      "epoch": 48.40200532664891,
      "grad_norm": 4.998042583465576,
      "learning_rate": 9.664995561125906e-06,
      "loss": 1.5523,
      "step": 617900
    },
    {
      "epoch": 48.40983863387122,
      "grad_norm": 4.612893104553223,
      "learning_rate": 9.658467805107317e-06,
      "loss": 1.6454,
      "step": 618000
    },
    {
      "epoch": 48.41767194109353,
      "grad_norm": 5.084959030151367,
      "learning_rate": 9.651940049088726e-06,
      "loss": 1.6766,
      "step": 618100
    },
    {
      "epoch": 48.42550524831584,
      "grad_norm": 8.641193389892578,
      "learning_rate": 9.645412293070134e-06,
      "loss": 1.7433,
      "step": 618200
    },
    {
      "epoch": 48.43333855553815,
      "grad_norm": 7.010673999786377,
      "learning_rate": 9.638884537051545e-06,
      "loss": 1.682,
      "step": 618300
    },
    {
      "epoch": 48.441171862760456,
      "grad_norm": 5.752955436706543,
      "learning_rate": 9.632356781032952e-06,
      "loss": 1.5851,
      "step": 618400
    },
    {
      "epoch": 48.44900516998277,
      "grad_norm": 7.147384166717529,
      "learning_rate": 9.625829025014361e-06,
      "loss": 1.5145,
      "step": 618500
    },
    {
      "epoch": 48.456838477205075,
      "grad_norm": 8.098332405090332,
      "learning_rate": 9.61930126899577e-06,
      "loss": 1.5239,
      "step": 618600
    },
    {
      "epoch": 48.46467178442739,
      "grad_norm": 4.502237796783447,
      "learning_rate": 9.61277351297718e-06,
      "loss": 1.6267,
      "step": 618700
    },
    {
      "epoch": 48.472505091649694,
      "grad_norm": 5.486980438232422,
      "learning_rate": 9.606245756958589e-06,
      "loss": 1.6154,
      "step": 618800
    },
    {
      "epoch": 48.48033839887201,
      "grad_norm": 7.4065375328063965,
      "learning_rate": 9.599718000939998e-06,
      "loss": 1.5845,
      "step": 618900
    },
    {
      "epoch": 48.48817170609431,
      "grad_norm": 5.971523284912109,
      "learning_rate": 9.593190244921405e-06,
      "loss": 1.6956,
      "step": 619000
    },
    {
      "epoch": 48.49600501331662,
      "grad_norm": 5.635471820831299,
      "learning_rate": 9.586662488902816e-06,
      "loss": 1.6169,
      "step": 619100
    },
    {
      "epoch": 48.50383832053893,
      "grad_norm": 6.892655849456787,
      "learning_rate": 9.580134732884224e-06,
      "loss": 1.6543,
      "step": 619200
    },
    {
      "epoch": 48.51167162776124,
      "grad_norm": 5.2000203132629395,
      "learning_rate": 9.573606976865633e-06,
      "loss": 1.6039,
      "step": 619300
    },
    {
      "epoch": 48.51950493498355,
      "grad_norm": 7.257051467895508,
      "learning_rate": 9.567079220847042e-06,
      "loss": 1.6334,
      "step": 619400
    },
    {
      "epoch": 48.52733824220586,
      "grad_norm": 5.435245037078857,
      "learning_rate": 9.560551464828451e-06,
      "loss": 1.6768,
      "step": 619500
    },
    {
      "epoch": 48.53517154942817,
      "grad_norm": 5.845385551452637,
      "learning_rate": 9.55402370880986e-06,
      "loss": 1.6091,
      "step": 619600
    },
    {
      "epoch": 48.54300485665048,
      "grad_norm": 7.837954044342041,
      "learning_rate": 9.54749595279127e-06,
      "loss": 1.6171,
      "step": 619700
    },
    {
      "epoch": 48.55083816387279,
      "grad_norm": 7.82269287109375,
      "learning_rate": 9.540968196772677e-06,
      "loss": 1.7027,
      "step": 619800
    },
    {
      "epoch": 48.558671471095096,
      "grad_norm": 5.069441795349121,
      "learning_rate": 9.534440440754088e-06,
      "loss": 1.6605,
      "step": 619900
    },
    {
      "epoch": 48.56650477831741,
      "grad_norm": 7.736977577209473,
      "learning_rate": 9.527912684735495e-06,
      "loss": 1.6326,
      "step": 620000
    },
    {
      "epoch": 48.574338085539715,
      "grad_norm": 6.543336391448975,
      "learning_rate": 9.521384928716905e-06,
      "loss": 1.6937,
      "step": 620100
    },
    {
      "epoch": 48.58217139276202,
      "grad_norm": 5.925340175628662,
      "learning_rate": 9.514857172698314e-06,
      "loss": 1.6354,
      "step": 620200
    },
    {
      "epoch": 48.590004699984334,
      "grad_norm": 5.814194679260254,
      "learning_rate": 9.508329416679723e-06,
      "loss": 1.5958,
      "step": 620300
    },
    {
      "epoch": 48.59783800720664,
      "grad_norm": 6.627863883972168,
      "learning_rate": 9.501801660661132e-06,
      "loss": 1.5892,
      "step": 620400
    },
    {
      "epoch": 48.60567131442895,
      "grad_norm": 9.008304595947266,
      "learning_rate": 9.495273904642541e-06,
      "loss": 1.6432,
      "step": 620500
    },
    {
      "epoch": 48.61350462165126,
      "grad_norm": 6.31097412109375,
      "learning_rate": 9.488746148623949e-06,
      "loss": 1.7682,
      "step": 620600
    },
    {
      "epoch": 48.62133792887357,
      "grad_norm": 4.365619659423828,
      "learning_rate": 9.48221839260536e-06,
      "loss": 1.5834,
      "step": 620700
    },
    {
      "epoch": 48.62917123609588,
      "grad_norm": 6.403348922729492,
      "learning_rate": 9.475690636586767e-06,
      "loss": 1.6135,
      "step": 620800
    },
    {
      "epoch": 48.63700454331819,
      "grad_norm": 5.550195693969727,
      "learning_rate": 9.469162880568176e-06,
      "loss": 1.6203,
      "step": 620900
    },
    {
      "epoch": 48.6448378505405,
      "grad_norm": 7.332484722137451,
      "learning_rate": 9.462635124549585e-06,
      "loss": 1.7401,
      "step": 621000
    },
    {
      "epoch": 48.65267115776281,
      "grad_norm": 6.112302303314209,
      "learning_rate": 9.456107368530994e-06,
      "loss": 1.6253,
      "step": 621100
    },
    {
      "epoch": 48.66050446498512,
      "grad_norm": 6.846658229827881,
      "learning_rate": 9.449579612512404e-06,
      "loss": 1.6267,
      "step": 621200
    },
    {
      "epoch": 48.66833777220742,
      "grad_norm": 7.444587230682373,
      "learning_rate": 9.443051856493813e-06,
      "loss": 1.617,
      "step": 621300
    },
    {
      "epoch": 48.676171079429736,
      "grad_norm": 6.666455268859863,
      "learning_rate": 9.43652410047522e-06,
      "loss": 1.6577,
      "step": 621400
    },
    {
      "epoch": 48.68400438665204,
      "grad_norm": 7.3824052810668945,
      "learning_rate": 9.429996344456631e-06,
      "loss": 1.6242,
      "step": 621500
    },
    {
      "epoch": 48.691837693874355,
      "grad_norm": 6.555785179138184,
      "learning_rate": 9.423468588438038e-06,
      "loss": 1.6019,
      "step": 621600
    },
    {
      "epoch": 48.69967100109666,
      "grad_norm": 5.102005958557129,
      "learning_rate": 9.416940832419448e-06,
      "loss": 1.6073,
      "step": 621700
    },
    {
      "epoch": 48.707504308318974,
      "grad_norm": 6.759714603424072,
      "learning_rate": 9.410413076400857e-06,
      "loss": 1.6385,
      "step": 621800
    },
    {
      "epoch": 48.71533761554128,
      "grad_norm": 5.519532680511475,
      "learning_rate": 9.403885320382266e-06,
      "loss": 1.5774,
      "step": 621900
    },
    {
      "epoch": 48.72317092276359,
      "grad_norm": 5.1353349685668945,
      "learning_rate": 9.397357564363675e-06,
      "loss": 1.5991,
      "step": 622000
    },
    {
      "epoch": 48.7310042299859,
      "grad_norm": 6.308913230895996,
      "learning_rate": 9.390829808345084e-06,
      "loss": 1.6753,
      "step": 622100
    },
    {
      "epoch": 48.73883753720821,
      "grad_norm": 4.830815315246582,
      "learning_rate": 9.384302052326492e-06,
      "loss": 1.6524,
      "step": 622200
    },
    {
      "epoch": 48.74667084443052,
      "grad_norm": 6.265620708465576,
      "learning_rate": 9.377774296307903e-06,
      "loss": 1.6163,
      "step": 622300
    },
    {
      "epoch": 48.754504151652824,
      "grad_norm": 5.939389705657959,
      "learning_rate": 9.37124654028931e-06,
      "loss": 1.6324,
      "step": 622400
    },
    {
      "epoch": 48.76233745887514,
      "grad_norm": 6.4192352294921875,
      "learning_rate": 9.36471878427072e-06,
      "loss": 1.6459,
      "step": 622500
    },
    {
      "epoch": 48.77017076609744,
      "grad_norm": 5.8824381828308105,
      "learning_rate": 9.358191028252128e-06,
      "loss": 1.6001,
      "step": 622600
    },
    {
      "epoch": 48.77800407331976,
      "grad_norm": 7.150647163391113,
      "learning_rate": 9.351663272233538e-06,
      "loss": 1.7078,
      "step": 622700
    },
    {
      "epoch": 48.78583738054206,
      "grad_norm": 7.408761978149414,
      "learning_rate": 9.345135516214947e-06,
      "loss": 1.7059,
      "step": 622800
    },
    {
      "epoch": 48.793670687764376,
      "grad_norm": 4.660337924957275,
      "learning_rate": 9.338607760196356e-06,
      "loss": 1.6061,
      "step": 622900
    },
    {
      "epoch": 48.80150399498668,
      "grad_norm": 6.5843825340271,
      "learning_rate": 9.332080004177763e-06,
      "loss": 1.5957,
      "step": 623000
    },
    {
      "epoch": 48.809337302208995,
      "grad_norm": 6.2206573486328125,
      "learning_rate": 9.325552248159174e-06,
      "loss": 1.6551,
      "step": 623100
    },
    {
      "epoch": 48.8171706094313,
      "grad_norm": 5.442625045776367,
      "learning_rate": 9.319024492140582e-06,
      "loss": 1.6966,
      "step": 623200
    },
    {
      "epoch": 48.825003916653614,
      "grad_norm": 6.7597832679748535,
      "learning_rate": 9.31249673612199e-06,
      "loss": 1.617,
      "step": 623300
    },
    {
      "epoch": 48.83283722387592,
      "grad_norm": 6.22365665435791,
      "learning_rate": 9.3059689801034e-06,
      "loss": 1.6604,
      "step": 623400
    },
    {
      "epoch": 48.840670531098226,
      "grad_norm": 8.077031135559082,
      "learning_rate": 9.299441224084809e-06,
      "loss": 1.699,
      "step": 623500
    },
    {
      "epoch": 48.84850383832054,
      "grad_norm": 6.948170185089111,
      "learning_rate": 9.292913468066218e-06,
      "loss": 1.6508,
      "step": 623600
    },
    {
      "epoch": 48.856337145542845,
      "grad_norm": 6.490887641906738,
      "learning_rate": 9.286385712047627e-06,
      "loss": 1.7476,
      "step": 623700
    },
    {
      "epoch": 48.86417045276516,
      "grad_norm": 6.929418087005615,
      "learning_rate": 9.279857956029035e-06,
      "loss": 1.5431,
      "step": 623800
    },
    {
      "epoch": 48.872003759987464,
      "grad_norm": 6.663372039794922,
      "learning_rate": 9.273330200010446e-06,
      "loss": 1.6102,
      "step": 623900
    },
    {
      "epoch": 48.87983706720978,
      "grad_norm": 6.197811603546143,
      "learning_rate": 9.266802443991853e-06,
      "loss": 1.6341,
      "step": 624000
    },
    {
      "epoch": 48.88767037443208,
      "grad_norm": 7.481166362762451,
      "learning_rate": 9.260274687973262e-06,
      "loss": 1.5894,
      "step": 624100
    },
    {
      "epoch": 48.8955036816544,
      "grad_norm": 6.572237014770508,
      "learning_rate": 9.253746931954671e-06,
      "loss": 1.7002,
      "step": 624200
    },
    {
      "epoch": 48.9033369888767,
      "grad_norm": 5.352905750274658,
      "learning_rate": 9.24721917593608e-06,
      "loss": 1.6583,
      "step": 624300
    },
    {
      "epoch": 48.911170296099016,
      "grad_norm": 4.886260986328125,
      "learning_rate": 9.24069141991749e-06,
      "loss": 1.6484,
      "step": 624400
    },
    {
      "epoch": 48.91900360332132,
      "grad_norm": 6.398614883422852,
      "learning_rate": 9.234163663898899e-06,
      "loss": 1.6766,
      "step": 624500
    },
    {
      "epoch": 48.926836910543635,
      "grad_norm": 6.5167059898376465,
      "learning_rate": 9.227635907880308e-06,
      "loss": 1.6888,
      "step": 624600
    },
    {
      "epoch": 48.93467021776594,
      "grad_norm": 6.983889102935791,
      "learning_rate": 9.221108151861717e-06,
      "loss": 1.6064,
      "step": 624700
    },
    {
      "epoch": 48.94250352498825,
      "grad_norm": 6.604519844055176,
      "learning_rate": 9.214580395843125e-06,
      "loss": 1.6419,
      "step": 624800
    },
    {
      "epoch": 48.95033683221056,
      "grad_norm": 7.543938159942627,
      "learning_rate": 9.208052639824534e-06,
      "loss": 1.6549,
      "step": 624900
    },
    {
      "epoch": 48.958170139432866,
      "grad_norm": 6.8816375732421875,
      "learning_rate": 9.201524883805945e-06,
      "loss": 1.5581,
      "step": 625000
    },
    {
      "epoch": 48.96600344665518,
      "grad_norm": 6.963878631591797,
      "learning_rate": 9.194997127787352e-06,
      "loss": 1.5751,
      "step": 625100
    },
    {
      "epoch": 48.973836753877485,
      "grad_norm": 6.4629974365234375,
      "learning_rate": 9.188469371768761e-06,
      "loss": 1.6131,
      "step": 625200
    },
    {
      "epoch": 48.9816700610998,
      "grad_norm": 8.442010879516602,
      "learning_rate": 9.18194161575017e-06,
      "loss": 1.5955,
      "step": 625300
    },
    {
      "epoch": 48.989503368322104,
      "grad_norm": 8.849599838256836,
      "learning_rate": 9.17541385973158e-06,
      "loss": 1.6734,
      "step": 625400
    },
    {
      "epoch": 48.99733667554442,
      "grad_norm": 6.087907791137695,
      "learning_rate": 9.168886103712989e-06,
      "loss": 1.6007,
      "step": 625500
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.7737735509872437,
      "eval_runtime": 1.5379,
      "eval_samples_per_second": 436.963,
      "eval_steps_per_second": 436.963,
      "step": 625534
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.3853058815002441,
      "eval_runtime": 29.2911,
      "eval_samples_per_second": 435.832,
      "eval_steps_per_second": 435.832,
      "step": 625534
    },
    {
      "epoch": 49.00516998276672,
      "grad_norm": 4.900094985961914,
      "learning_rate": 9.162358347694396e-06,
      "loss": 1.6096,
      "step": 625600
    },
    {
      "epoch": 49.01300328998904,
      "grad_norm": 6.2922563552856445,
      "learning_rate": 9.155830591675805e-06,
      "loss": 1.5842,
      "step": 625700
    },
    {
      "epoch": 49.02083659721134,
      "grad_norm": 6.973369121551514,
      "learning_rate": 9.149302835657216e-06,
      "loss": 1.6387,
      "step": 625800
    },
    {
      "epoch": 49.02866990443365,
      "grad_norm": 5.709038734436035,
      "learning_rate": 9.142775079638624e-06,
      "loss": 1.5429,
      "step": 625900
    },
    {
      "epoch": 49.03650321165596,
      "grad_norm": 6.855927467346191,
      "learning_rate": 9.136247323620033e-06,
      "loss": 1.6355,
      "step": 626000
    },
    {
      "epoch": 49.04433651887827,
      "grad_norm": 6.45388126373291,
      "learning_rate": 9.129719567601442e-06,
      "loss": 1.6331,
      "step": 626100
    },
    {
      "epoch": 49.05216982610058,
      "grad_norm": 6.499771595001221,
      "learning_rate": 9.123191811582851e-06,
      "loss": 1.5517,
      "step": 626200
    },
    {
      "epoch": 49.06000313332289,
      "grad_norm": 5.081151485443115,
      "learning_rate": 9.11666405556426e-06,
      "loss": 1.5414,
      "step": 626300
    },
    {
      "epoch": 49.0678364405452,
      "grad_norm": 9.656091690063477,
      "learning_rate": 9.110136299545668e-06,
      "loss": 1.6848,
      "step": 626400
    },
    {
      "epoch": 49.075669747767506,
      "grad_norm": 11.444191932678223,
      "learning_rate": 9.103608543527077e-06,
      "loss": 1.6173,
      "step": 626500
    },
    {
      "epoch": 49.08350305498982,
      "grad_norm": 7.194325923919678,
      "learning_rate": 9.097080787508488e-06,
      "loss": 1.6278,
      "step": 626600
    },
    {
      "epoch": 49.091336362212125,
      "grad_norm": 7.303382396697998,
      "learning_rate": 9.090553031489895e-06,
      "loss": 1.6026,
      "step": 626700
    },
    {
      "epoch": 49.09916966943444,
      "grad_norm": 5.54720401763916,
      "learning_rate": 9.084025275471304e-06,
      "loss": 1.6401,
      "step": 626800
    },
    {
      "epoch": 49.107002976656744,
      "grad_norm": 7.426924228668213,
      "learning_rate": 9.077497519452714e-06,
      "loss": 1.6603,
      "step": 626900
    },
    {
      "epoch": 49.11483628387905,
      "grad_norm": 9.751379013061523,
      "learning_rate": 9.070969763434123e-06,
      "loss": 1.609,
      "step": 627000
    },
    {
      "epoch": 49.12266959110136,
      "grad_norm": 5.736337184906006,
      "learning_rate": 9.064442007415532e-06,
      "loss": 1.6836,
      "step": 627100
    },
    {
      "epoch": 49.13050289832367,
      "grad_norm": 8.709132194519043,
      "learning_rate": 9.05791425139694e-06,
      "loss": 1.6101,
      "step": 627200
    },
    {
      "epoch": 49.13833620554598,
      "grad_norm": 6.174644470214844,
      "learning_rate": 9.051386495378349e-06,
      "loss": 1.6446,
      "step": 627300
    },
    {
      "epoch": 49.14616951276829,
      "grad_norm": 7.937230587005615,
      "learning_rate": 9.04485873935976e-06,
      "loss": 1.6816,
      "step": 627400
    },
    {
      "epoch": 49.1540028199906,
      "grad_norm": 6.912075519561768,
      "learning_rate": 9.038330983341167e-06,
      "loss": 1.5911,
      "step": 627500
    },
    {
      "epoch": 49.16183612721291,
      "grad_norm": 7.831430912017822,
      "learning_rate": 9.031803227322576e-06,
      "loss": 1.6691,
      "step": 627600
    },
    {
      "epoch": 49.16966943443522,
      "grad_norm": 5.587452411651611,
      "learning_rate": 9.025275471303985e-06,
      "loss": 1.7169,
      "step": 627700
    },
    {
      "epoch": 49.17750274165753,
      "grad_norm": 7.0926127433776855,
      "learning_rate": 9.018747715285394e-06,
      "loss": 1.5611,
      "step": 627800
    },
    {
      "epoch": 49.18533604887984,
      "grad_norm": 4.506350517272949,
      "learning_rate": 9.012219959266803e-06,
      "loss": 1.5208,
      "step": 627900
    },
    {
      "epoch": 49.193169356102146,
      "grad_norm": 5.472132205963135,
      "learning_rate": 9.005692203248211e-06,
      "loss": 1.5787,
      "step": 628000
    },
    {
      "epoch": 49.20100266332445,
      "grad_norm": 6.401663303375244,
      "learning_rate": 8.99916444722962e-06,
      "loss": 1.6034,
      "step": 628100
    },
    {
      "epoch": 49.208835970546765,
      "grad_norm": 7.3007683753967285,
      "learning_rate": 8.992636691211031e-06,
      "loss": 1.6691,
      "step": 628200
    },
    {
      "epoch": 49.21666927776907,
      "grad_norm": 7.477040767669678,
      "learning_rate": 8.986108935192438e-06,
      "loss": 1.6576,
      "step": 628300
    },
    {
      "epoch": 49.224502584991384,
      "grad_norm": 6.183590888977051,
      "learning_rate": 8.979581179173848e-06,
      "loss": 1.6072,
      "step": 628400
    },
    {
      "epoch": 49.23233589221369,
      "grad_norm": 9.993565559387207,
      "learning_rate": 8.973053423155257e-06,
      "loss": 1.6098,
      "step": 628500
    },
    {
      "epoch": 49.240169199436004,
      "grad_norm": 6.028507232666016,
      "learning_rate": 8.966525667136666e-06,
      "loss": 1.6966,
      "step": 628600
    },
    {
      "epoch": 49.24800250665831,
      "grad_norm": 7.851330280303955,
      "learning_rate": 8.959997911118075e-06,
      "loss": 1.6037,
      "step": 628700
    },
    {
      "epoch": 49.25583581388062,
      "grad_norm": 7.505891799926758,
      "learning_rate": 8.953470155099483e-06,
      "loss": 1.5871,
      "step": 628800
    },
    {
      "epoch": 49.26366912110293,
      "grad_norm": 9.174016952514648,
      "learning_rate": 8.946942399080892e-06,
      "loss": 1.6659,
      "step": 628900
    },
    {
      "epoch": 49.27150242832524,
      "grad_norm": 6.675504207611084,
      "learning_rate": 8.940414643062303e-06,
      "loss": 1.6711,
      "step": 629000
    },
    {
      "epoch": 49.27933573554755,
      "grad_norm": 7.159400939941406,
      "learning_rate": 8.93388688704371e-06,
      "loss": 1.7105,
      "step": 629100
    },
    {
      "epoch": 49.28716904276986,
      "grad_norm": 7.101495742797852,
      "learning_rate": 8.927359131025119e-06,
      "loss": 1.6566,
      "step": 629200
    },
    {
      "epoch": 49.29500234999217,
      "grad_norm": 6.390124320983887,
      "learning_rate": 8.920831375006528e-06,
      "loss": 1.7231,
      "step": 629300
    },
    {
      "epoch": 49.30283565721447,
      "grad_norm": 7.1296515464782715,
      "learning_rate": 8.914303618987937e-06,
      "loss": 1.523,
      "step": 629400
    },
    {
      "epoch": 49.310668964436786,
      "grad_norm": 5.892308712005615,
      "learning_rate": 8.907775862969347e-06,
      "loss": 1.5774,
      "step": 629500
    },
    {
      "epoch": 49.31850227165909,
      "grad_norm": 5.511650562286377,
      "learning_rate": 8.901248106950754e-06,
      "loss": 1.7161,
      "step": 629600
    },
    {
      "epoch": 49.326335578881405,
      "grad_norm": 8.372197151184082,
      "learning_rate": 8.894720350932163e-06,
      "loss": 1.6737,
      "step": 629700
    },
    {
      "epoch": 49.33416888610371,
      "grad_norm": 6.261559963226318,
      "learning_rate": 8.888192594913574e-06,
      "loss": 1.6471,
      "step": 629800
    },
    {
      "epoch": 49.342002193326024,
      "grad_norm": 6.005577087402344,
      "learning_rate": 8.881664838894982e-06,
      "loss": 1.6384,
      "step": 629900
    },
    {
      "epoch": 49.34983550054833,
      "grad_norm": 6.3803558349609375,
      "learning_rate": 8.87513708287639e-06,
      "loss": 1.5656,
      "step": 630000
    },
    {
      "epoch": 49.357668807770644,
      "grad_norm": 7.496948719024658,
      "learning_rate": 8.8686093268578e-06,
      "loss": 1.6655,
      "step": 630100
    },
    {
      "epoch": 49.36550211499295,
      "grad_norm": 8.12350845336914,
      "learning_rate": 8.862081570839209e-06,
      "loss": 1.5919,
      "step": 630200
    },
    {
      "epoch": 49.37333542221526,
      "grad_norm": 5.67271089553833,
      "learning_rate": 8.855553814820618e-06,
      "loss": 1.5792,
      "step": 630300
    },
    {
      "epoch": 49.38116872943757,
      "grad_norm": 7.623502254486084,
      "learning_rate": 8.849026058802026e-06,
      "loss": 1.6329,
      "step": 630400
    },
    {
      "epoch": 49.389002036659875,
      "grad_norm": 7.388889312744141,
      "learning_rate": 8.842498302783436e-06,
      "loss": 1.6832,
      "step": 630500
    },
    {
      "epoch": 49.39683534388219,
      "grad_norm": 5.670216083526611,
      "learning_rate": 8.835970546764846e-06,
      "loss": 1.6264,
      "step": 630600
    },
    {
      "epoch": 49.404668651104494,
      "grad_norm": 7.766348838806152,
      "learning_rate": 8.829442790746253e-06,
      "loss": 1.6088,
      "step": 630700
    },
    {
      "epoch": 49.41250195832681,
      "grad_norm": 6.343236446380615,
      "learning_rate": 8.822915034727662e-06,
      "loss": 1.608,
      "step": 630800
    },
    {
      "epoch": 49.42033526554911,
      "grad_norm": 5.081535816192627,
      "learning_rate": 8.816387278709071e-06,
      "loss": 1.6387,
      "step": 630900
    },
    {
      "epoch": 49.428168572771426,
      "grad_norm": 7.4261322021484375,
      "learning_rate": 8.80985952269048e-06,
      "loss": 1.6103,
      "step": 631000
    },
    {
      "epoch": 49.43600187999373,
      "grad_norm": 5.5214972496032715,
      "learning_rate": 8.80333176667189e-06,
      "loss": 1.6241,
      "step": 631100
    },
    {
      "epoch": 49.443835187216045,
      "grad_norm": 7.239607810974121,
      "learning_rate": 8.796804010653297e-06,
      "loss": 1.6058,
      "step": 631200
    },
    {
      "epoch": 49.45166849443835,
      "grad_norm": 7.659011363983154,
      "learning_rate": 8.790276254634708e-06,
      "loss": 1.6203,
      "step": 631300
    },
    {
      "epoch": 49.459501801660664,
      "grad_norm": 5.92056131362915,
      "learning_rate": 8.783748498616117e-06,
      "loss": 1.619,
      "step": 631400
    },
    {
      "epoch": 49.46733510888297,
      "grad_norm": 6.55669641494751,
      "learning_rate": 8.777220742597525e-06,
      "loss": 1.6564,
      "step": 631500
    },
    {
      "epoch": 49.47516841610528,
      "grad_norm": 4.86703634262085,
      "learning_rate": 8.770692986578934e-06,
      "loss": 1.696,
      "step": 631600
    },
    {
      "epoch": 49.48300172332759,
      "grad_norm": 5.816183090209961,
      "learning_rate": 8.764165230560343e-06,
      "loss": 1.5806,
      "step": 631700
    },
    {
      "epoch": 49.490835030549896,
      "grad_norm": 4.756333351135254,
      "learning_rate": 8.757637474541752e-06,
      "loss": 1.7089,
      "step": 631800
    },
    {
      "epoch": 49.49866833777221,
      "grad_norm": 7.1239848136901855,
      "learning_rate": 8.751109718523161e-06,
      "loss": 1.6425,
      "step": 631900
    },
    {
      "epoch": 49.506501644994515,
      "grad_norm": 6.246140956878662,
      "learning_rate": 8.744581962504569e-06,
      "loss": 1.7276,
      "step": 632000
    },
    {
      "epoch": 49.51433495221683,
      "grad_norm": 7.787539482116699,
      "learning_rate": 8.73805420648598e-06,
      "loss": 1.6291,
      "step": 632100
    },
    {
      "epoch": 49.522168259439134,
      "grad_norm": 6.3097100257873535,
      "learning_rate": 8.731526450467389e-06,
      "loss": 1.5569,
      "step": 632200
    },
    {
      "epoch": 49.53000156666145,
      "grad_norm": 7.223623752593994,
      "learning_rate": 8.724998694448796e-06,
      "loss": 1.6443,
      "step": 632300
    },
    {
      "epoch": 49.53783487388375,
      "grad_norm": 4.041869640350342,
      "learning_rate": 8.718470938430205e-06,
      "loss": 1.5868,
      "step": 632400
    },
    {
      "epoch": 49.545668181106066,
      "grad_norm": 7.128711223602295,
      "learning_rate": 8.711943182411615e-06,
      "loss": 1.6407,
      "step": 632500
    },
    {
      "epoch": 49.55350148832837,
      "grad_norm": 7.887508869171143,
      "learning_rate": 8.705415426393024e-06,
      "loss": 1.6072,
      "step": 632600
    },
    {
      "epoch": 49.56133479555068,
      "grad_norm": 4.996068954467773,
      "learning_rate": 8.698887670374433e-06,
      "loss": 1.6402,
      "step": 632700
    },
    {
      "epoch": 49.56916810277299,
      "grad_norm": 6.050963401794434,
      "learning_rate": 8.69235991435584e-06,
      "loss": 1.7188,
      "step": 632800
    },
    {
      "epoch": 49.5770014099953,
      "grad_norm": 6.948966026306152,
      "learning_rate": 8.685832158337251e-06,
      "loss": 1.601,
      "step": 632900
    },
    {
      "epoch": 49.58483471721761,
      "grad_norm": 5.965123176574707,
      "learning_rate": 8.67930440231866e-06,
      "loss": 1.6689,
      "step": 633000
    },
    {
      "epoch": 49.59266802443992,
      "grad_norm": 8.392072677612305,
      "learning_rate": 8.672776646300068e-06,
      "loss": 1.6551,
      "step": 633100
    },
    {
      "epoch": 49.60050133166223,
      "grad_norm": 8.558671951293945,
      "learning_rate": 8.666248890281477e-06,
      "loss": 1.632,
      "step": 633200
    },
    {
      "epoch": 49.608334638884536,
      "grad_norm": 9.737373352050781,
      "learning_rate": 8.659721134262886e-06,
      "loss": 1.5725,
      "step": 633300
    },
    {
      "epoch": 49.61616794610685,
      "grad_norm": 6.764125823974609,
      "learning_rate": 8.653193378244295e-06,
      "loss": 1.6741,
      "step": 633400
    },
    {
      "epoch": 49.624001253329155,
      "grad_norm": 4.740997314453125,
      "learning_rate": 8.646665622225704e-06,
      "loss": 1.7168,
      "step": 633500
    },
    {
      "epoch": 49.63183456055147,
      "grad_norm": 7.092122554779053,
      "learning_rate": 8.640137866207112e-06,
      "loss": 1.6203,
      "step": 633600
    },
    {
      "epoch": 49.639667867773774,
      "grad_norm": 6.593909740447998,
      "learning_rate": 8.633610110188523e-06,
      "loss": 1.5872,
      "step": 633700
    },
    {
      "epoch": 49.64750117499608,
      "grad_norm": 5.572631359100342,
      "learning_rate": 8.627082354169932e-06,
      "loss": 1.6372,
      "step": 633800
    },
    {
      "epoch": 49.65533448221839,
      "grad_norm": 6.188982963562012,
      "learning_rate": 8.62055459815134e-06,
      "loss": 1.7001,
      "step": 633900
    },
    {
      "epoch": 49.6631677894407,
      "grad_norm": 7.695294380187988,
      "learning_rate": 8.614026842132748e-06,
      "loss": 1.5933,
      "step": 634000
    },
    {
      "epoch": 49.67100109666301,
      "grad_norm": 6.017019271850586,
      "learning_rate": 8.607499086114158e-06,
      "loss": 1.6168,
      "step": 634100
    },
    {
      "epoch": 49.67883440388532,
      "grad_norm": 6.621084690093994,
      "learning_rate": 8.600971330095567e-06,
      "loss": 1.5946,
      "step": 634200
    },
    {
      "epoch": 49.68666771110763,
      "grad_norm": 6.2086944580078125,
      "learning_rate": 8.594443574076976e-06,
      "loss": 1.6436,
      "step": 634300
    },
    {
      "epoch": 49.69450101832994,
      "grad_norm": 7.113087177276611,
      "learning_rate": 8.587915818058383e-06,
      "loss": 1.5948,
      "step": 634400
    },
    {
      "epoch": 49.70233432555225,
      "grad_norm": 7.2581305503845215,
      "learning_rate": 8.581388062039794e-06,
      "loss": 1.6116,
      "step": 634500
    },
    {
      "epoch": 49.71016763277456,
      "grad_norm": 7.205767631530762,
      "learning_rate": 8.574860306021203e-06,
      "loss": 1.7291,
      "step": 634600
    },
    {
      "epoch": 49.71800093999687,
      "grad_norm": 7.595571041107178,
      "learning_rate": 8.568332550002611e-06,
      "loss": 1.617,
      "step": 634700
    },
    {
      "epoch": 49.725834247219176,
      "grad_norm": 4.712128639221191,
      "learning_rate": 8.56180479398402e-06,
      "loss": 1.6955,
      "step": 634800
    },
    {
      "epoch": 49.73366755444148,
      "grad_norm": 4.636448860168457,
      "learning_rate": 8.55527703796543e-06,
      "loss": 1.6352,
      "step": 634900
    },
    {
      "epoch": 49.741500861663795,
      "grad_norm": 7.693312644958496,
      "learning_rate": 8.548749281946838e-06,
      "loss": 1.5203,
      "step": 635000
    },
    {
      "epoch": 49.7493341688861,
      "grad_norm": 5.694754600524902,
      "learning_rate": 8.542221525928248e-06,
      "loss": 1.6824,
      "step": 635100
    },
    {
      "epoch": 49.757167476108414,
      "grad_norm": 6.158349514007568,
      "learning_rate": 8.535693769909655e-06,
      "loss": 1.6437,
      "step": 635200
    },
    {
      "epoch": 49.76500078333072,
      "grad_norm": 6.138772010803223,
      "learning_rate": 8.529166013891066e-06,
      "loss": 1.6361,
      "step": 635300
    },
    {
      "epoch": 49.77283409055303,
      "grad_norm": 5.825451374053955,
      "learning_rate": 8.522638257872475e-06,
      "loss": 1.7299,
      "step": 635400
    },
    {
      "epoch": 49.78066739777534,
      "grad_norm": 6.441836833953857,
      "learning_rate": 8.516110501853882e-06,
      "loss": 1.6714,
      "step": 635500
    },
    {
      "epoch": 49.78850070499765,
      "grad_norm": 6.9988226890563965,
      "learning_rate": 8.509582745835292e-06,
      "loss": 1.6576,
      "step": 635600
    },
    {
      "epoch": 49.79633401221996,
      "grad_norm": 8.245384216308594,
      "learning_rate": 8.5030549898167e-06,
      "loss": 1.613,
      "step": 635700
    },
    {
      "epoch": 49.80416731944227,
      "grad_norm": 8.002610206604004,
      "learning_rate": 8.49652723379811e-06,
      "loss": 1.6476,
      "step": 635800
    },
    {
      "epoch": 49.81200062666458,
      "grad_norm": 8.897430419921875,
      "learning_rate": 8.489999477779519e-06,
      "loss": 1.6072,
      "step": 635900
    },
    {
      "epoch": 49.81983393388689,
      "grad_norm": 5.98967170715332,
      "learning_rate": 8.483471721760927e-06,
      "loss": 1.6447,
      "step": 636000
    },
    {
      "epoch": 49.8276672411092,
      "grad_norm": 6.011688232421875,
      "learning_rate": 8.476943965742337e-06,
      "loss": 1.6873,
      "step": 636100
    },
    {
      "epoch": 49.8355005483315,
      "grad_norm": 9.69975471496582,
      "learning_rate": 8.470416209723747e-06,
      "loss": 1.5564,
      "step": 636200
    },
    {
      "epoch": 49.843333855553816,
      "grad_norm": 5.362359523773193,
      "learning_rate": 8.463888453705154e-06,
      "loss": 1.681,
      "step": 636300
    },
    {
      "epoch": 49.85116716277612,
      "grad_norm": 5.911803245544434,
      "learning_rate": 8.457360697686565e-06,
      "loss": 1.5968,
      "step": 636400
    },
    {
      "epoch": 49.859000469998435,
      "grad_norm": 5.6132097244262695,
      "learning_rate": 8.450832941667972e-06,
      "loss": 1.6226,
      "step": 636500
    },
    {
      "epoch": 49.86683377722074,
      "grad_norm": 5.581428527832031,
      "learning_rate": 8.444305185649381e-06,
      "loss": 1.6218,
      "step": 636600
    },
    {
      "epoch": 49.874667084443054,
      "grad_norm": 6.7165985107421875,
      "learning_rate": 8.43777742963079e-06,
      "loss": 1.6612,
      "step": 636700
    },
    {
      "epoch": 49.88250039166536,
      "grad_norm": 4.060005187988281,
      "learning_rate": 8.4312496736122e-06,
      "loss": 1.6232,
      "step": 636800
    },
    {
      "epoch": 49.89033369888767,
      "grad_norm": 6.49163293838501,
      "learning_rate": 8.424721917593609e-06,
      "loss": 1.6207,
      "step": 636900
    },
    {
      "epoch": 49.89816700610998,
      "grad_norm": 7.521920680999756,
      "learning_rate": 8.418194161575018e-06,
      "loss": 1.5928,
      "step": 637000
    },
    {
      "epoch": 49.90600031333229,
      "grad_norm": 6.511326789855957,
      "learning_rate": 8.411666405556426e-06,
      "loss": 1.5866,
      "step": 637100
    },
    {
      "epoch": 49.9138336205546,
      "grad_norm": 5.370503902435303,
      "learning_rate": 8.405138649537836e-06,
      "loss": 1.5956,
      "step": 637200
    },
    {
      "epoch": 49.921666927776904,
      "grad_norm": 9.330978393554688,
      "learning_rate": 8.398610893519244e-06,
      "loss": 1.6873,
      "step": 637300
    },
    {
      "epoch": 49.92950023499922,
      "grad_norm": 6.229291915893555,
      "learning_rate": 8.392083137500653e-06,
      "loss": 1.6374,
      "step": 637400
    },
    {
      "epoch": 49.93733354222152,
      "grad_norm": 7.753658771514893,
      "learning_rate": 8.385555381482062e-06,
      "loss": 1.6416,
      "step": 637500
    },
    {
      "epoch": 49.94516684944384,
      "grad_norm": 6.267868995666504,
      "learning_rate": 8.379027625463471e-06,
      "loss": 1.6458,
      "step": 637600
    },
    {
      "epoch": 49.95300015666614,
      "grad_norm": 6.674680709838867,
      "learning_rate": 8.37249986944488e-06,
      "loss": 1.6348,
      "step": 637700
    },
    {
      "epoch": 49.960833463888456,
      "grad_norm": 7.184708595275879,
      "learning_rate": 8.36597211342629e-06,
      "loss": 1.5507,
      "step": 637800
    },
    {
      "epoch": 49.96866677111076,
      "grad_norm": 7.74765157699585,
      "learning_rate": 8.359444357407697e-06,
      "loss": 1.6927,
      "step": 637900
    },
    {
      "epoch": 49.976500078333075,
      "grad_norm": 8.316289901733398,
      "learning_rate": 8.352916601389108e-06,
      "loss": 1.6878,
      "step": 638000
    },
    {
      "epoch": 49.98433338555538,
      "grad_norm": 6.487425804138184,
      "learning_rate": 8.346388845370515e-06,
      "loss": 1.7073,
      "step": 638100
    },
    {
      "epoch": 49.992166692777694,
      "grad_norm": 9.358892440795898,
      "learning_rate": 8.339861089351925e-06,
      "loss": 1.5482,
      "step": 638200
    },
    {
      "epoch": 50.0,
      "grad_norm": 6.4891438484191895,
      "learning_rate": 8.333333333333334e-06,
      "loss": 1.6403,
      "step": 638300
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.7659356594085693,
      "eval_runtime": 1.5089,
      "eval_samples_per_second": 445.345,
      "eval_steps_per_second": 445.345,
      "step": 638300
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.3812371492385864,
      "eval_runtime": 29.078,
      "eval_samples_per_second": 439.027,
      "eval_steps_per_second": 439.027,
      "step": 638300
    },
    {
      "epoch": 50.007833307222306,
      "grad_norm": 7.469489574432373,
      "learning_rate": 8.326805577314743e-06,
      "loss": 1.6474,
      "step": 638400
    },
    {
      "epoch": 50.01566661444462,
      "grad_norm": 8.329645156860352,
      "learning_rate": 8.320277821296152e-06,
      "loss": 1.6881,
      "step": 638500
    },
    {
      "epoch": 50.023499921666925,
      "grad_norm": 7.536068916320801,
      "learning_rate": 8.313750065277561e-06,
      "loss": 1.6633,
      "step": 638600
    },
    {
      "epoch": 50.03133322888924,
      "grad_norm": 6.985550880432129,
      "learning_rate": 8.307222309258969e-06,
      "loss": 1.5327,
      "step": 638700
    },
    {
      "epoch": 50.039166536111544,
      "grad_norm": 6.424589157104492,
      "learning_rate": 8.30069455324038e-06,
      "loss": 1.6591,
      "step": 638800
    },
    {
      "epoch": 50.04699984333386,
      "grad_norm": 6.0695600509643555,
      "learning_rate": 8.294166797221787e-06,
      "loss": 1.6146,
      "step": 638900
    },
    {
      "epoch": 50.05483315055616,
      "grad_norm": 7.216991424560547,
      "learning_rate": 8.287639041203196e-06,
      "loss": 1.5293,
      "step": 639000
    },
    {
      "epoch": 50.06266645777848,
      "grad_norm": 4.829561710357666,
      "learning_rate": 8.281111285184605e-06,
      "loss": 1.5902,
      "step": 639100
    },
    {
      "epoch": 50.07049976500078,
      "grad_norm": 7.7985429763793945,
      "learning_rate": 8.274583529166014e-06,
      "loss": 1.6439,
      "step": 639200
    },
    {
      "epoch": 50.078333072223096,
      "grad_norm": 6.044562339782715,
      "learning_rate": 8.268055773147424e-06,
      "loss": 1.6067,
      "step": 639300
    },
    {
      "epoch": 50.0861663794454,
      "grad_norm": 6.570373058319092,
      "learning_rate": 8.261528017128833e-06,
      "loss": 1.6186,
      "step": 639400
    },
    {
      "epoch": 50.09399968666771,
      "grad_norm": 6.1002655029296875,
      "learning_rate": 8.25500026111024e-06,
      "loss": 1.5729,
      "step": 639500
    },
    {
      "epoch": 50.10183299389002,
      "grad_norm": 5.5008864402771,
      "learning_rate": 8.248472505091651e-06,
      "loss": 1.6028,
      "step": 639600
    },
    {
      "epoch": 50.10966630111233,
      "grad_norm": 2.9507102966308594,
      "learning_rate": 8.241944749073059e-06,
      "loss": 1.6741,
      "step": 639700
    },
    {
      "epoch": 50.11749960833464,
      "grad_norm": 7.280062675476074,
      "learning_rate": 8.235416993054468e-06,
      "loss": 1.6016,
      "step": 639800
    },
    {
      "epoch": 50.125332915556946,
      "grad_norm": 5.52662992477417,
      "learning_rate": 8.228889237035877e-06,
      "loss": 1.6887,
      "step": 639900
    },
    {
      "epoch": 50.13316622277926,
      "grad_norm": 7.05946159362793,
      "learning_rate": 8.222361481017286e-06,
      "loss": 1.6435,
      "step": 640000
    },
    {
      "epoch": 50.140999530001565,
      "grad_norm": 6.613650798797607,
      "learning_rate": 8.215833724998695e-06,
      "loss": 1.6463,
      "step": 640100
    },
    {
      "epoch": 50.14883283722388,
      "grad_norm": 7.364064693450928,
      "learning_rate": 8.209305968980104e-06,
      "loss": 1.6214,
      "step": 640200
    },
    {
      "epoch": 50.156666144446184,
      "grad_norm": 8.963723182678223,
      "learning_rate": 8.202778212961512e-06,
      "loss": 1.5919,
      "step": 640300
    },
    {
      "epoch": 50.1644994516685,
      "grad_norm": 3.8633885383605957,
      "learning_rate": 8.196250456942923e-06,
      "loss": 1.5935,
      "step": 640400
    },
    {
      "epoch": 50.1723327588908,
      "grad_norm": 4.054056644439697,
      "learning_rate": 8.18972270092433e-06,
      "loss": 1.7051,
      "step": 640500
    },
    {
      "epoch": 50.18016606611311,
      "grad_norm": 6.808849811553955,
      "learning_rate": 8.18319494490574e-06,
      "loss": 1.6631,
      "step": 640600
    },
    {
      "epoch": 50.18799937333542,
      "grad_norm": 5.746304988861084,
      "learning_rate": 8.176667188887148e-06,
      "loss": 1.6863,
      "step": 640700
    },
    {
      "epoch": 50.19583268055773,
      "grad_norm": 8.553589820861816,
      "learning_rate": 8.170139432868558e-06,
      "loss": 1.6381,
      "step": 640800
    },
    {
      "epoch": 50.20366598778004,
      "grad_norm": 12.57807731628418,
      "learning_rate": 8.163611676849967e-06,
      "loss": 1.6251,
      "step": 640900
    },
    {
      "epoch": 50.21149929500235,
      "grad_norm": 5.719452857971191,
      "learning_rate": 8.157083920831376e-06,
      "loss": 1.5672,
      "step": 641000
    },
    {
      "epoch": 50.21933260222466,
      "grad_norm": 6.022233486175537,
      "learning_rate": 8.150556164812783e-06,
      "loss": 1.5533,
      "step": 641100
    },
    {
      "epoch": 50.22716590944697,
      "grad_norm": 5.6288981437683105,
      "learning_rate": 8.144028408794194e-06,
      "loss": 1.6781,
      "step": 641200
    },
    {
      "epoch": 50.23499921666928,
      "grad_norm": 7.015006065368652,
      "learning_rate": 8.137500652775602e-06,
      "loss": 1.6316,
      "step": 641300
    },
    {
      "epoch": 50.242832523891586,
      "grad_norm": 10.287854194641113,
      "learning_rate": 8.13097289675701e-06,
      "loss": 1.7014,
      "step": 641400
    },
    {
      "epoch": 50.2506658311139,
      "grad_norm": 8.868370056152344,
      "learning_rate": 8.12444514073842e-06,
      "loss": 1.5966,
      "step": 641500
    },
    {
      "epoch": 50.258499138336205,
      "grad_norm": 7.437180519104004,
      "learning_rate": 8.117917384719829e-06,
      "loss": 1.5719,
      "step": 641600
    },
    {
      "epoch": 50.26633244555852,
      "grad_norm": 7.592194557189941,
      "learning_rate": 8.111389628701238e-06,
      "loss": 1.6234,
      "step": 641700
    },
    {
      "epoch": 50.274165752780824,
      "grad_norm": 5.267436504364014,
      "learning_rate": 8.104861872682647e-06,
      "loss": 1.7023,
      "step": 641800
    },
    {
      "epoch": 50.28199906000313,
      "grad_norm": 6.64206600189209,
      "learning_rate": 8.098334116664055e-06,
      "loss": 1.685,
      "step": 641900
    },
    {
      "epoch": 50.28983236722544,
      "grad_norm": 4.726987838745117,
      "learning_rate": 8.091806360645466e-06,
      "loss": 1.6113,
      "step": 642000
    },
    {
      "epoch": 50.29766567444775,
      "grad_norm": 5.973813533782959,
      "learning_rate": 8.085278604626873e-06,
      "loss": 1.5681,
      "step": 642100
    },
    {
      "epoch": 50.30549898167006,
      "grad_norm": 6.741396427154541,
      "learning_rate": 8.078750848608282e-06,
      "loss": 1.5908,
      "step": 642200
    },
    {
      "epoch": 50.31333228889237,
      "grad_norm": 6.809042453765869,
      "learning_rate": 8.072223092589692e-06,
      "loss": 1.6693,
      "step": 642300
    },
    {
      "epoch": 50.32116559611468,
      "grad_norm": 6.500993251800537,
      "learning_rate": 8.0656953365711e-06,
      "loss": 1.7065,
      "step": 642400
    },
    {
      "epoch": 50.32899890333699,
      "grad_norm": 7.0785980224609375,
      "learning_rate": 8.05916758055251e-06,
      "loss": 1.7087,
      "step": 642500
    },
    {
      "epoch": 50.3368322105593,
      "grad_norm": 7.090003490447998,
      "learning_rate": 8.052639824533919e-06,
      "loss": 1.6923,
      "step": 642600
    },
    {
      "epoch": 50.34466551778161,
      "grad_norm": 6.994401454925537,
      "learning_rate": 8.046112068515328e-06,
      "loss": 1.644,
      "step": 642700
    },
    {
      "epoch": 50.35249882500392,
      "grad_norm": 6.488579273223877,
      "learning_rate": 8.039584312496737e-06,
      "loss": 1.6411,
      "step": 642800
    },
    {
      "epoch": 50.360332132226226,
      "grad_norm": 10.113016128540039,
      "learning_rate": 8.033056556478145e-06,
      "loss": 1.5933,
      "step": 642900
    },
    {
      "epoch": 50.36816543944853,
      "grad_norm": 6.01806640625,
      "learning_rate": 8.026528800459554e-06,
      "loss": 1.7021,
      "step": 643000
    },
    {
      "epoch": 50.375998746670845,
      "grad_norm": 6.941442966461182,
      "learning_rate": 8.020001044440965e-06,
      "loss": 1.7073,
      "step": 643100
    },
    {
      "epoch": 50.38383205389315,
      "grad_norm": 6.729631423950195,
      "learning_rate": 8.013473288422372e-06,
      "loss": 1.648,
      "step": 643200
    },
    {
      "epoch": 50.391665361115464,
      "grad_norm": 6.950708389282227,
      "learning_rate": 8.006945532403781e-06,
      "loss": 1.6018,
      "step": 643300
    },
    {
      "epoch": 50.39949866833777,
      "grad_norm": 4.6035661697387695,
      "learning_rate": 8.00041777638519e-06,
      "loss": 1.6439,
      "step": 643400
    },
    {
      "epoch": 50.40733197556008,
      "grad_norm": 6.4369964599609375,
      "learning_rate": 7.9938900203666e-06,
      "loss": 1.5772,
      "step": 643500
    },
    {
      "epoch": 50.41516528278239,
      "grad_norm": 6.498289108276367,
      "learning_rate": 7.987362264348009e-06,
      "loss": 1.5913,
      "step": 643600
    },
    {
      "epoch": 50.4229985900047,
      "grad_norm": 7.819019794464111,
      "learning_rate": 7.980834508329416e-06,
      "loss": 1.561,
      "step": 643700
    },
    {
      "epoch": 50.43083189722701,
      "grad_norm": 10.014290809631348,
      "learning_rate": 7.974306752310825e-06,
      "loss": 1.6281,
      "step": 643800
    },
    {
      "epoch": 50.43866520444932,
      "grad_norm": 5.789119720458984,
      "learning_rate": 7.967778996292236e-06,
      "loss": 1.6006,
      "step": 643900
    },
    {
      "epoch": 50.44649851167163,
      "grad_norm": 6.123061656951904,
      "learning_rate": 7.961251240273644e-06,
      "loss": 1.6288,
      "step": 644000
    },
    {
      "epoch": 50.454331818893934,
      "grad_norm": 5.25398588180542,
      "learning_rate": 7.954723484255053e-06,
      "loss": 1.6361,
      "step": 644100
    },
    {
      "epoch": 50.46216512611625,
      "grad_norm": 7.068517208099365,
      "learning_rate": 7.948195728236462e-06,
      "loss": 1.6046,
      "step": 644200
    },
    {
      "epoch": 50.46999843333855,
      "grad_norm": 7.17649507522583,
      "learning_rate": 7.941667972217871e-06,
      "loss": 1.629,
      "step": 644300
    },
    {
      "epoch": 50.477831740560866,
      "grad_norm": 5.708465576171875,
      "learning_rate": 7.93514021619928e-06,
      "loss": 1.5567,
      "step": 644400
    },
    {
      "epoch": 50.48566504778317,
      "grad_norm": 7.173256874084473,
      "learning_rate": 7.928612460180688e-06,
      "loss": 1.6295,
      "step": 644500
    },
    {
      "epoch": 50.493498355005485,
      "grad_norm": 9.063092231750488,
      "learning_rate": 7.922084704162097e-06,
      "loss": 1.5796,
      "step": 644600
    },
    {
      "epoch": 50.50133166222779,
      "grad_norm": 3.8108842372894287,
      "learning_rate": 7.915556948143508e-06,
      "loss": 1.6314,
      "step": 644700
    },
    {
      "epoch": 50.509164969450104,
      "grad_norm": 4.825595378875732,
      "learning_rate": 7.909029192124915e-06,
      "loss": 1.708,
      "step": 644800
    },
    {
      "epoch": 50.51699827667241,
      "grad_norm": 7.874568939208984,
      "learning_rate": 7.902501436106325e-06,
      "loss": 1.6981,
      "step": 644900
    },
    {
      "epoch": 50.52483158389472,
      "grad_norm": 6.668266773223877,
      "learning_rate": 7.895973680087734e-06,
      "loss": 1.7015,
      "step": 645000
    },
    {
      "epoch": 50.53266489111703,
      "grad_norm": 6.028885364532471,
      "learning_rate": 7.889445924069143e-06,
      "loss": 1.663,
      "step": 645100
    },
    {
      "epoch": 50.540498198339336,
      "grad_norm": 7.384149551391602,
      "learning_rate": 7.882918168050552e-06,
      "loss": 1.6586,
      "step": 645200
    },
    {
      "epoch": 50.54833150556165,
      "grad_norm": 6.076173782348633,
      "learning_rate": 7.87639041203196e-06,
      "loss": 1.5949,
      "step": 645300
    },
    {
      "epoch": 50.556164812783955,
      "grad_norm": 7.196749687194824,
      "learning_rate": 7.869862656013369e-06,
      "loss": 1.5875,
      "step": 645400
    },
    {
      "epoch": 50.56399812000627,
      "grad_norm": 5.465664863586426,
      "learning_rate": 7.86333489999478e-06,
      "loss": 1.6466,
      "step": 645500
    },
    {
      "epoch": 50.571831427228574,
      "grad_norm": 10.280384063720703,
      "learning_rate": 7.856807143976187e-06,
      "loss": 1.6753,
      "step": 645600
    },
    {
      "epoch": 50.57966473445089,
      "grad_norm": 8.842557907104492,
      "learning_rate": 7.850279387957596e-06,
      "loss": 1.7064,
      "step": 645700
    },
    {
      "epoch": 50.58749804167319,
      "grad_norm": 6.7641777992248535,
      "learning_rate": 7.843751631939005e-06,
      "loss": 1.6369,
      "step": 645800
    },
    {
      "epoch": 50.595331348895506,
      "grad_norm": 8.063138008117676,
      "learning_rate": 7.837223875920414e-06,
      "loss": 1.6022,
      "step": 645900
    },
    {
      "epoch": 50.60316465611781,
      "grad_norm": 5.1093878746032715,
      "learning_rate": 7.830696119901824e-06,
      "loss": 1.5762,
      "step": 646000
    },
    {
      "epoch": 50.610997963340125,
      "grad_norm": 6.6454925537109375,
      "learning_rate": 7.824168363883231e-06,
      "loss": 1.6962,
      "step": 646100
    },
    {
      "epoch": 50.61883127056243,
      "grad_norm": 5.575862884521484,
      "learning_rate": 7.81764060786464e-06,
      "loss": 1.6447,
      "step": 646200
    },
    {
      "epoch": 50.62666457778474,
      "grad_norm": 6.283961772918701,
      "learning_rate": 7.811112851846051e-06,
      "loss": 1.6829,
      "step": 646300
    },
    {
      "epoch": 50.63449788500705,
      "grad_norm": 7.733001232147217,
      "learning_rate": 7.804585095827458e-06,
      "loss": 1.6323,
      "step": 646400
    },
    {
      "epoch": 50.642331192229356,
      "grad_norm": 7.0499653816223145,
      "learning_rate": 7.798057339808868e-06,
      "loss": 1.5932,
      "step": 646500
    },
    {
      "epoch": 50.65016449945167,
      "grad_norm": 6.357289791107178,
      "learning_rate": 7.791529583790277e-06,
      "loss": 1.5668,
      "step": 646600
    },
    {
      "epoch": 50.657997806673976,
      "grad_norm": 8.432831764221191,
      "learning_rate": 7.785001827771686e-06,
      "loss": 1.7187,
      "step": 646700
    },
    {
      "epoch": 50.66583111389629,
      "grad_norm": 5.8352460861206055,
      "learning_rate": 7.778474071753095e-06,
      "loss": 1.5814,
      "step": 646800
    },
    {
      "epoch": 50.673664421118595,
      "grad_norm": 6.544761657714844,
      "learning_rate": 7.771946315734503e-06,
      "loss": 1.6222,
      "step": 646900
    },
    {
      "epoch": 50.68149772834091,
      "grad_norm": 6.778499126434326,
      "learning_rate": 7.765418559715912e-06,
      "loss": 1.6408,
      "step": 647000
    },
    {
      "epoch": 50.689331035563214,
      "grad_norm": 6.467076301574707,
      "learning_rate": 7.758890803697323e-06,
      "loss": 1.5076,
      "step": 647100
    },
    {
      "epoch": 50.69716434278553,
      "grad_norm": 5.927815914154053,
      "learning_rate": 7.75236304767873e-06,
      "loss": 1.5652,
      "step": 647200
    },
    {
      "epoch": 50.70499765000783,
      "grad_norm": 5.6364426612854,
      "learning_rate": 7.74583529166014e-06,
      "loss": 1.6045,
      "step": 647300
    },
    {
      "epoch": 50.71283095723014,
      "grad_norm": 6.274407386779785,
      "learning_rate": 7.739307535641548e-06,
      "loss": 1.5865,
      "step": 647400
    },
    {
      "epoch": 50.72066426445245,
      "grad_norm": 6.908012390136719,
      "learning_rate": 7.732779779622957e-06,
      "loss": 1.5229,
      "step": 647500
    },
    {
      "epoch": 50.72849757167476,
      "grad_norm": 6.16569709777832,
      "learning_rate": 7.726252023604367e-06,
      "loss": 1.6061,
      "step": 647600
    },
    {
      "epoch": 50.73633087889707,
      "grad_norm": 6.861957550048828,
      "learning_rate": 7.719724267585774e-06,
      "loss": 1.6957,
      "step": 647700
    },
    {
      "epoch": 50.74416418611938,
      "grad_norm": 7.3005242347717285,
      "learning_rate": 7.713196511567183e-06,
      "loss": 1.6385,
      "step": 647800
    },
    {
      "epoch": 50.75199749334169,
      "grad_norm": 8.554475784301758,
      "learning_rate": 7.706668755548594e-06,
      "loss": 1.7023,
      "step": 647900
    },
    {
      "epoch": 50.759830800563996,
      "grad_norm": 6.348426342010498,
      "learning_rate": 7.700140999530002e-06,
      "loss": 1.6016,
      "step": 648000
    },
    {
      "epoch": 50.76766410778631,
      "grad_norm": 9.659214973449707,
      "learning_rate": 7.69361324351141e-06,
      "loss": 1.6182,
      "step": 648100
    },
    {
      "epoch": 50.775497415008616,
      "grad_norm": 7.432200908660889,
      "learning_rate": 7.68708548749282e-06,
      "loss": 1.6667,
      "step": 648200
    },
    {
      "epoch": 50.78333072223093,
      "grad_norm": 8.234516143798828,
      "learning_rate": 7.680557731474229e-06,
      "loss": 1.6571,
      "step": 648300
    },
    {
      "epoch": 50.791164029453235,
      "grad_norm": 10.670848846435547,
      "learning_rate": 7.674029975455638e-06,
      "loss": 1.5822,
      "step": 648400
    },
    {
      "epoch": 50.79899733667555,
      "grad_norm": 6.5734429359436035,
      "learning_rate": 7.667502219437046e-06,
      "loss": 1.69,
      "step": 648500
    },
    {
      "epoch": 50.806830643897854,
      "grad_norm": 6.270634174346924,
      "learning_rate": 7.660974463418457e-06,
      "loss": 1.7499,
      "step": 648600
    },
    {
      "epoch": 50.81466395112016,
      "grad_norm": 6.686412334442139,
      "learning_rate": 7.654446707399866e-06,
      "loss": 1.5551,
      "step": 648700
    },
    {
      "epoch": 50.82249725834247,
      "grad_norm": 8.212932586669922,
      "learning_rate": 7.647918951381273e-06,
      "loss": 1.6516,
      "step": 648800
    },
    {
      "epoch": 50.83033056556478,
      "grad_norm": 10.432519912719727,
      "learning_rate": 7.641391195362682e-06,
      "loss": 1.5979,
      "step": 648900
    },
    {
      "epoch": 50.83816387278709,
      "grad_norm": 7.1486382484436035,
      "learning_rate": 7.634863439344091e-06,
      "loss": 1.6532,
      "step": 649000
    },
    {
      "epoch": 50.8459971800094,
      "grad_norm": 7.385095596313477,
      "learning_rate": 7.628335683325501e-06,
      "loss": 1.5867,
      "step": 649100
    },
    {
      "epoch": 50.85383048723171,
      "grad_norm": 5.089524745941162,
      "learning_rate": 7.621807927306909e-06,
      "loss": 1.653,
      "step": 649200
    },
    {
      "epoch": 50.86166379445402,
      "grad_norm": 6.145640850067139,
      "learning_rate": 7.615280171288318e-06,
      "loss": 1.53,
      "step": 649300
    },
    {
      "epoch": 50.86949710167633,
      "grad_norm": 7.896122932434082,
      "learning_rate": 7.608752415269728e-06,
      "loss": 1.7141,
      "step": 649400
    },
    {
      "epoch": 50.87733040889864,
      "grad_norm": 9.906867980957031,
      "learning_rate": 7.602224659251136e-06,
      "loss": 1.6646,
      "step": 649500
    },
    {
      "epoch": 50.88516371612095,
      "grad_norm": 7.58303689956665,
      "learning_rate": 7.595696903232545e-06,
      "loss": 1.6662,
      "step": 649600
    },
    {
      "epoch": 50.892997023343256,
      "grad_norm": 6.065159320831299,
      "learning_rate": 7.589169147213954e-06,
      "loss": 1.6173,
      "step": 649700
    },
    {
      "epoch": 50.90083033056556,
      "grad_norm": 8.080951690673828,
      "learning_rate": 7.582641391195364e-06,
      "loss": 1.6279,
      "step": 649800
    },
    {
      "epoch": 50.908663637787875,
      "grad_norm": 5.813317775726318,
      "learning_rate": 7.576113635176772e-06,
      "loss": 1.6671,
      "step": 649900
    },
    {
      "epoch": 50.91649694501018,
      "grad_norm": 7.826713562011719,
      "learning_rate": 7.5695858791581805e-06,
      "loss": 1.6001,
      "step": 650000
    },
    {
      "epoch": 50.924330252232494,
      "grad_norm": 7.38337516784668,
      "learning_rate": 7.56305812313959e-06,
      "loss": 1.61,
      "step": 650100
    },
    {
      "epoch": 50.9321635594548,
      "grad_norm": 5.666874885559082,
      "learning_rate": 7.556530367121e-06,
      "loss": 1.6998,
      "step": 650200
    },
    {
      "epoch": 50.93999686667711,
      "grad_norm": 6.927981376647949,
      "learning_rate": 7.550002611102408e-06,
      "loss": 1.6721,
      "step": 650300
    },
    {
      "epoch": 50.94783017389942,
      "grad_norm": 7.088052272796631,
      "learning_rate": 7.543474855083816e-06,
      "loss": 1.6471,
      "step": 650400
    },
    {
      "epoch": 50.95566348112173,
      "grad_norm": 6.924667835235596,
      "learning_rate": 7.536947099065225e-06,
      "loss": 1.5583,
      "step": 650500
    },
    {
      "epoch": 50.96349678834404,
      "grad_norm": 5.391748428344727,
      "learning_rate": 7.530419343046635e-06,
      "loss": 1.5825,
      "step": 650600
    },
    {
      "epoch": 50.97133009556635,
      "grad_norm": 6.401374340057373,
      "learning_rate": 7.523891587028044e-06,
      "loss": 1.5852,
      "step": 650700
    },
    {
      "epoch": 50.97916340278866,
      "grad_norm": 6.118083953857422,
      "learning_rate": 7.517363831009452e-06,
      "loss": 1.5297,
      "step": 650800
    },
    {
      "epoch": 50.98699671001096,
      "grad_norm": 4.219139099121094,
      "learning_rate": 7.510836074990861e-06,
      "loss": 1.6659,
      "step": 650900
    },
    {
      "epoch": 50.99483001723328,
      "grad_norm": 7.755127906799316,
      "learning_rate": 7.504308318972271e-06,
      "loss": 1.6106,
      "step": 651000
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.7710331678390503,
      "eval_runtime": 1.5561,
      "eval_samples_per_second": 431.854,
      "eval_steps_per_second": 431.854,
      "step": 651066
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.3829587697982788,
      "eval_runtime": 29.6722,
      "eval_samples_per_second": 430.234,
      "eval_steps_per_second": 430.234,
      "step": 651066
    },
    {
      "epoch": 51.00266332445558,
      "grad_norm": 8.660467147827148,
      "learning_rate": 7.4977805629536795e-06,
      "loss": 1.6063,
      "step": 651100
    },
    {
      "epoch": 51.010496631677896,
      "grad_norm": 7.925471782684326,
      "learning_rate": 7.491252806935088e-06,
      "loss": 1.6554,
      "step": 651200
    },
    {
      "epoch": 51.0183299389002,
      "grad_norm": 6.880620002746582,
      "learning_rate": 7.484725050916497e-06,
      "loss": 1.6866,
      "step": 651300
    },
    {
      "epoch": 51.026163246122515,
      "grad_norm": 6.623692035675049,
      "learning_rate": 7.478197294897907e-06,
      "loss": 1.5409,
      "step": 651400
    },
    {
      "epoch": 51.03399655334482,
      "grad_norm": 4.842741966247559,
      "learning_rate": 7.471669538879315e-06,
      "loss": 1.5607,
      "step": 651500
    },
    {
      "epoch": 51.041829860567134,
      "grad_norm": 7.4054179191589355,
      "learning_rate": 7.465141782860724e-06,
      "loss": 1.6645,
      "step": 651600
    },
    {
      "epoch": 51.04966316778944,
      "grad_norm": 4.983521461486816,
      "learning_rate": 7.458614026842133e-06,
      "loss": 1.7048,
      "step": 651700
    },
    {
      "epoch": 51.05749647501175,
      "grad_norm": 4.762290000915527,
      "learning_rate": 7.452086270823543e-06,
      "loss": 1.6362,
      "step": 651800
    },
    {
      "epoch": 51.06532978223406,
      "grad_norm": 6.422937393188477,
      "learning_rate": 7.445558514804951e-06,
      "loss": 1.6332,
      "step": 651900
    },
    {
      "epoch": 51.073163089456365,
      "grad_norm": 8.821023941040039,
      "learning_rate": 7.439030758786359e-06,
      "loss": 1.6025,
      "step": 652000
    },
    {
      "epoch": 51.08099639667868,
      "grad_norm": 5.717712879180908,
      "learning_rate": 7.4325030027677685e-06,
      "loss": 1.6179,
      "step": 652100
    },
    {
      "epoch": 51.088829703900984,
      "grad_norm": 6.169598579406738,
      "learning_rate": 7.4259752467491785e-06,
      "loss": 1.6297,
      "step": 652200
    },
    {
      "epoch": 51.0966630111233,
      "grad_norm": 3.7969162464141846,
      "learning_rate": 7.419447490730587e-06,
      "loss": 1.5433,
      "step": 652300
    },
    {
      "epoch": 51.1044963183456,
      "grad_norm": 8.9924898147583,
      "learning_rate": 7.412919734711995e-06,
      "loss": 1.6807,
      "step": 652400
    },
    {
      "epoch": 51.11232962556792,
      "grad_norm": 9.221636772155762,
      "learning_rate": 7.406391978693404e-06,
      "loss": 1.6573,
      "step": 652500
    },
    {
      "epoch": 51.12016293279022,
      "grad_norm": 6.931714057922363,
      "learning_rate": 7.399864222674814e-06,
      "loss": 1.6265,
      "step": 652600
    },
    {
      "epoch": 51.127996240012536,
      "grad_norm": 5.601311683654785,
      "learning_rate": 7.393336466656223e-06,
      "loss": 1.6593,
      "step": 652700
    },
    {
      "epoch": 51.13582954723484,
      "grad_norm": 7.767752647399902,
      "learning_rate": 7.386808710637631e-06,
      "loss": 1.6454,
      "step": 652800
    },
    {
      "epoch": 51.143662854457155,
      "grad_norm": 7.518515110015869,
      "learning_rate": 7.38028095461904e-06,
      "loss": 1.6039,
      "step": 652900
    },
    {
      "epoch": 51.15149616167946,
      "grad_norm": 5.086601257324219,
      "learning_rate": 7.37375319860045e-06,
      "loss": 1.6048,
      "step": 653000
    },
    {
      "epoch": 51.15932946890177,
      "grad_norm": 5.392405033111572,
      "learning_rate": 7.367225442581858e-06,
      "loss": 1.6245,
      "step": 653100
    },
    {
      "epoch": 51.16716277612408,
      "grad_norm": 6.025717735290527,
      "learning_rate": 7.360697686563267e-06,
      "loss": 1.5866,
      "step": 653200
    },
    {
      "epoch": 51.174996083346386,
      "grad_norm": 5.7192535400390625,
      "learning_rate": 7.354169930544676e-06,
      "loss": 1.6079,
      "step": 653300
    },
    {
      "epoch": 51.1828293905687,
      "grad_norm": 5.890785217285156,
      "learning_rate": 7.347642174526086e-06,
      "loss": 1.6296,
      "step": 653400
    },
    {
      "epoch": 51.190662697791005,
      "grad_norm": 6.231732368469238,
      "learning_rate": 7.341114418507494e-06,
      "loss": 1.6135,
      "step": 653500
    },
    {
      "epoch": 51.19849600501332,
      "grad_norm": 7.606175899505615,
      "learning_rate": 7.3345866624889025e-06,
      "loss": 1.6023,
      "step": 653600
    },
    {
      "epoch": 51.206329312235624,
      "grad_norm": 6.607325553894043,
      "learning_rate": 7.328058906470312e-06,
      "loss": 1.5684,
      "step": 653700
    },
    {
      "epoch": 51.21416261945794,
      "grad_norm": 5.143223762512207,
      "learning_rate": 7.321531150451722e-06,
      "loss": 1.6153,
      "step": 653800
    },
    {
      "epoch": 51.22199592668024,
      "grad_norm": 7.967597961425781,
      "learning_rate": 7.31500339443313e-06,
      "loss": 1.6415,
      "step": 653900
    },
    {
      "epoch": 51.22982923390256,
      "grad_norm": 5.802616596221924,
      "learning_rate": 7.308475638414538e-06,
      "loss": 1.6613,
      "step": 654000
    },
    {
      "epoch": 51.23766254112486,
      "grad_norm": 3.772238254547119,
      "learning_rate": 7.3019478823959474e-06,
      "loss": 1.6209,
      "step": 654100
    },
    {
      "epoch": 51.245495848347176,
      "grad_norm": 4.778769493103027,
      "learning_rate": 7.2954201263773574e-06,
      "loss": 1.6242,
      "step": 654200
    },
    {
      "epoch": 51.25332915556948,
      "grad_norm": 6.6907429695129395,
      "learning_rate": 7.288892370358766e-06,
      "loss": 1.5063,
      "step": 654300
    },
    {
      "epoch": 51.26116246279179,
      "grad_norm": 5.258556842803955,
      "learning_rate": 7.282364614340174e-06,
      "loss": 1.5008,
      "step": 654400
    },
    {
      "epoch": 51.2689957700141,
      "grad_norm": 4.986043930053711,
      "learning_rate": 7.275836858321584e-06,
      "loss": 1.6576,
      "step": 654500
    },
    {
      "epoch": 51.27682907723641,
      "grad_norm": 8.104771614074707,
      "learning_rate": 7.269309102302993e-06,
      "loss": 1.5582,
      "step": 654600
    },
    {
      "epoch": 51.28466238445872,
      "grad_norm": 6.006894111633301,
      "learning_rate": 7.2627813462844015e-06,
      "loss": 1.6597,
      "step": 654700
    },
    {
      "epoch": 51.292495691681026,
      "grad_norm": 5.849141597747803,
      "learning_rate": 7.25625359026581e-06,
      "loss": 1.6323,
      "step": 654800
    },
    {
      "epoch": 51.30032899890334,
      "grad_norm": 5.488961696624756,
      "learning_rate": 7.24972583424722e-06,
      "loss": 1.6815,
      "step": 654900
    },
    {
      "epoch": 51.308162306125645,
      "grad_norm": 6.889103889465332,
      "learning_rate": 7.243198078228629e-06,
      "loss": 1.538,
      "step": 655000
    },
    {
      "epoch": 51.31599561334796,
      "grad_norm": 6.379385948181152,
      "learning_rate": 7.236670322210037e-06,
      "loss": 1.6324,
      "step": 655100
    },
    {
      "epoch": 51.323828920570264,
      "grad_norm": 6.024904727935791,
      "learning_rate": 7.230142566191446e-06,
      "loss": 1.6848,
      "step": 655200
    },
    {
      "epoch": 51.33166222779258,
      "grad_norm": 7.883644104003906,
      "learning_rate": 7.223614810172856e-06,
      "loss": 1.6493,
      "step": 655300
    },
    {
      "epoch": 51.33949553501488,
      "grad_norm": 6.500336647033691,
      "learning_rate": 7.217087054154265e-06,
      "loss": 1.6083,
      "step": 655400
    },
    {
      "epoch": 51.34732884223719,
      "grad_norm": 6.876899242401123,
      "learning_rate": 7.210559298135673e-06,
      "loss": 1.6887,
      "step": 655500
    },
    {
      "epoch": 51.3551621494595,
      "grad_norm": 4.600980758666992,
      "learning_rate": 7.204031542117081e-06,
      "loss": 1.5865,
      "step": 655600
    },
    {
      "epoch": 51.36299545668181,
      "grad_norm": 8.705987930297852,
      "learning_rate": 7.197503786098491e-06,
      "loss": 1.6661,
      "step": 655700
    },
    {
      "epoch": 51.37082876390412,
      "grad_norm": 4.95487117767334,
      "learning_rate": 7.1909760300799005e-06,
      "loss": 1.6948,
      "step": 655800
    },
    {
      "epoch": 51.37866207112643,
      "grad_norm": 7.731662750244141,
      "learning_rate": 7.184448274061309e-06,
      "loss": 1.5767,
      "step": 655900
    },
    {
      "epoch": 51.38649537834874,
      "grad_norm": 5.8851728439331055,
      "learning_rate": 7.177920518042717e-06,
      "loss": 1.5868,
      "step": 656000
    },
    {
      "epoch": 51.39432868557105,
      "grad_norm": 5.177901268005371,
      "learning_rate": 7.171392762024127e-06,
      "loss": 1.6372,
      "step": 656100
    },
    {
      "epoch": 51.40216199279336,
      "grad_norm": 7.660478591918945,
      "learning_rate": 7.164865006005536e-06,
      "loss": 1.7388,
      "step": 656200
    },
    {
      "epoch": 51.409995300015666,
      "grad_norm": 6.302779674530029,
      "learning_rate": 7.158337249986945e-06,
      "loss": 1.5736,
      "step": 656300
    },
    {
      "epoch": 51.41782860723798,
      "grad_norm": 6.843163967132568,
      "learning_rate": 7.151809493968353e-06,
      "loss": 1.6487,
      "step": 656400
    },
    {
      "epoch": 51.425661914460285,
      "grad_norm": 5.0688652992248535,
      "learning_rate": 7.145281737949763e-06,
      "loss": 1.735,
      "step": 656500
    },
    {
      "epoch": 51.43349522168259,
      "grad_norm": 6.17559289932251,
      "learning_rate": 7.138753981931172e-06,
      "loss": 1.6567,
      "step": 656600
    },
    {
      "epoch": 51.441328528904904,
      "grad_norm": 5.855766773223877,
      "learning_rate": 7.13222622591258e-06,
      "loss": 1.7354,
      "step": 656700
    },
    {
      "epoch": 51.44916183612721,
      "grad_norm": 8.027117729187012,
      "learning_rate": 7.125698469893989e-06,
      "loss": 1.6105,
      "step": 656800
    },
    {
      "epoch": 51.45699514334952,
      "grad_norm": 6.166914939880371,
      "learning_rate": 7.119170713875399e-06,
      "loss": 1.5615,
      "step": 656900
    },
    {
      "epoch": 51.46482845057183,
      "grad_norm": 6.250347137451172,
      "learning_rate": 7.112642957856808e-06,
      "loss": 1.6253,
      "step": 657000
    },
    {
      "epoch": 51.47266175779414,
      "grad_norm": 5.973516941070557,
      "learning_rate": 7.106115201838216e-06,
      "loss": 1.5688,
      "step": 657100
    },
    {
      "epoch": 51.48049506501645,
      "grad_norm": 6.733514785766602,
      "learning_rate": 7.0995874458196245e-06,
      "loss": 1.5792,
      "step": 657200
    },
    {
      "epoch": 51.48832837223876,
      "grad_norm": 7.176054954528809,
      "learning_rate": 7.0930596898010345e-06,
      "loss": 1.6078,
      "step": 657300
    },
    {
      "epoch": 51.49616167946107,
      "grad_norm": 6.3402323722839355,
      "learning_rate": 7.086531933782444e-06,
      "loss": 1.5476,
      "step": 657400
    },
    {
      "epoch": 51.50399498668338,
      "grad_norm": 7.896569728851318,
      "learning_rate": 7.080004177763852e-06,
      "loss": 1.6243,
      "step": 657500
    },
    {
      "epoch": 51.51182829390569,
      "grad_norm": 6.359704971313477,
      "learning_rate": 7.07347642174526e-06,
      "loss": 1.5063,
      "step": 657600
    },
    {
      "epoch": 51.51966160112799,
      "grad_norm": 5.186773300170898,
      "learning_rate": 7.06694866572667e-06,
      "loss": 1.6815,
      "step": 657700
    },
    {
      "epoch": 51.527494908350306,
      "grad_norm": 6.570705413818359,
      "learning_rate": 7.0604209097080794e-06,
      "loss": 1.6561,
      "step": 657800
    },
    {
      "epoch": 51.53532821557261,
      "grad_norm": 6.216885089874268,
      "learning_rate": 7.053893153689488e-06,
      "loss": 1.602,
      "step": 657900
    },
    {
      "epoch": 51.543161522794925,
      "grad_norm": 7.005775451660156,
      "learning_rate": 7.047365397670896e-06,
      "loss": 1.6791,
      "step": 658000
    },
    {
      "epoch": 51.55099483001723,
      "grad_norm": 6.438656806945801,
      "learning_rate": 7.040837641652306e-06,
      "loss": 1.6186,
      "step": 658100
    },
    {
      "epoch": 51.558828137239544,
      "grad_norm": 6.800273418426514,
      "learning_rate": 7.034309885633715e-06,
      "loss": 1.631,
      "step": 658200
    },
    {
      "epoch": 51.56666144446185,
      "grad_norm": 5.092687129974365,
      "learning_rate": 7.0277821296151235e-06,
      "loss": 1.7848,
      "step": 658300
    },
    {
      "epoch": 51.57449475168416,
      "grad_norm": 6.370614051818848,
      "learning_rate": 7.021254373596532e-06,
      "loss": 1.5953,
      "step": 658400
    },
    {
      "epoch": 51.58232805890647,
      "grad_norm": 7.069102764129639,
      "learning_rate": 7.014726617577942e-06,
      "loss": 1.5697,
      "step": 658500
    },
    {
      "epoch": 51.59016136612878,
      "grad_norm": 7.059193134307861,
      "learning_rate": 7.008198861559351e-06,
      "loss": 1.5731,
      "step": 658600
    },
    {
      "epoch": 51.59799467335109,
      "grad_norm": 6.894099712371826,
      "learning_rate": 7.001671105540759e-06,
      "loss": 1.6593,
      "step": 658700
    },
    {
      "epoch": 51.605827980573395,
      "grad_norm": 8.063136100769043,
      "learning_rate": 6.995143349522168e-06,
      "loss": 1.5409,
      "step": 658800
    },
    {
      "epoch": 51.61366128779571,
      "grad_norm": 6.148814678192139,
      "learning_rate": 6.988615593503578e-06,
      "loss": 1.6161,
      "step": 658900
    },
    {
      "epoch": 51.621494595018014,
      "grad_norm": 6.650554180145264,
      "learning_rate": 6.982087837484987e-06,
      "loss": 1.626,
      "step": 659000
    },
    {
      "epoch": 51.62932790224033,
      "grad_norm": 5.839566230773926,
      "learning_rate": 6.975560081466395e-06,
      "loss": 1.6669,
      "step": 659100
    },
    {
      "epoch": 51.63716120946263,
      "grad_norm": 6.494670391082764,
      "learning_rate": 6.969032325447803e-06,
      "loss": 1.6259,
      "step": 659200
    },
    {
      "epoch": 51.644994516684946,
      "grad_norm": 5.1156325340271,
      "learning_rate": 6.962504569429213e-06,
      "loss": 1.684,
      "step": 659300
    },
    {
      "epoch": 51.65282782390725,
      "grad_norm": 7.7606964111328125,
      "learning_rate": 6.9559768134106226e-06,
      "loss": 1.6622,
      "step": 659400
    },
    {
      "epoch": 51.660661131129565,
      "grad_norm": 6.394030570983887,
      "learning_rate": 6.949449057392031e-06,
      "loss": 1.6592,
      "step": 659500
    },
    {
      "epoch": 51.66849443835187,
      "grad_norm": 5.582521438598633,
      "learning_rate": 6.942921301373439e-06,
      "loss": 1.6794,
      "step": 659600
    },
    {
      "epoch": 51.676327745574184,
      "grad_norm": 6.582605361938477,
      "learning_rate": 6.936393545354849e-06,
      "loss": 1.6074,
      "step": 659700
    },
    {
      "epoch": 51.68416105279649,
      "grad_norm": 7.7824177742004395,
      "learning_rate": 6.929865789336258e-06,
      "loss": 1.6874,
      "step": 659800
    },
    {
      "epoch": 51.6919943600188,
      "grad_norm": 8.027905464172363,
      "learning_rate": 6.923338033317667e-06,
      "loss": 1.6527,
      "step": 659900
    },
    {
      "epoch": 51.69982766724111,
      "grad_norm": 5.7714385986328125,
      "learning_rate": 6.916810277299075e-06,
      "loss": 1.5789,
      "step": 660000
    },
    {
      "epoch": 51.707660974463415,
      "grad_norm": 6.489079475402832,
      "learning_rate": 6.910282521280485e-06,
      "loss": 1.6782,
      "step": 660100
    },
    {
      "epoch": 51.71549428168573,
      "grad_norm": 6.110073089599609,
      "learning_rate": 6.903754765261894e-06,
      "loss": 1.5426,
      "step": 660200
    },
    {
      "epoch": 51.723327588908035,
      "grad_norm": 5.376950740814209,
      "learning_rate": 6.897227009243302e-06,
      "loss": 1.6659,
      "step": 660300
    },
    {
      "epoch": 51.73116089613035,
      "grad_norm": 4.585630893707275,
      "learning_rate": 6.8906992532247124e-06,
      "loss": 1.5942,
      "step": 660400
    },
    {
      "epoch": 51.738994203352654,
      "grad_norm": 5.223812580108643,
      "learning_rate": 6.884171497206121e-06,
      "loss": 1.5033,
      "step": 660500
    },
    {
      "epoch": 51.74682751057497,
      "grad_norm": 7.134760856628418,
      "learning_rate": 6.87764374118753e-06,
      "loss": 1.6215,
      "step": 660600
    },
    {
      "epoch": 51.75466081779727,
      "grad_norm": 6.481131076812744,
      "learning_rate": 6.871115985168938e-06,
      "loss": 1.5634,
      "step": 660700
    },
    {
      "epoch": 51.762494125019586,
      "grad_norm": 8.010689735412598,
      "learning_rate": 6.864588229150348e-06,
      "loss": 1.7692,
      "step": 660800
    },
    {
      "epoch": 51.77032743224189,
      "grad_norm": 7.10715389251709,
      "learning_rate": 6.8580604731317565e-06,
      "loss": 1.6056,
      "step": 660900
    },
    {
      "epoch": 51.778160739464205,
      "grad_norm": 5.561596870422363,
      "learning_rate": 6.851532717113166e-06,
      "loss": 1.5699,
      "step": 661000
    },
    {
      "epoch": 51.78599404668651,
      "grad_norm": 4.259096622467041,
      "learning_rate": 6.845004961094574e-06,
      "loss": 1.6734,
      "step": 661100
    },
    {
      "epoch": 51.79382735390882,
      "grad_norm": 6.117951393127441,
      "learning_rate": 6.838477205075984e-06,
      "loss": 1.5526,
      "step": 661200
    },
    {
      "epoch": 51.80166066113113,
      "grad_norm": 6.841075897216797,
      "learning_rate": 6.831949449057392e-06,
      "loss": 1.5578,
      "step": 661300
    },
    {
      "epoch": 51.809493968353436,
      "grad_norm": 5.938170433044434,
      "learning_rate": 6.8254216930388015e-06,
      "loss": 1.5752,
      "step": 661400
    },
    {
      "epoch": 51.81732727557575,
      "grad_norm": 7.015256404876709,
      "learning_rate": 6.81889393702021e-06,
      "loss": 1.5897,
      "step": 661500
    },
    {
      "epoch": 51.825160582798055,
      "grad_norm": 8.34011173248291,
      "learning_rate": 6.81236618100162e-06,
      "loss": 1.6224,
      "step": 661600
    },
    {
      "epoch": 51.83299389002037,
      "grad_norm": 5.771883010864258,
      "learning_rate": 6.805838424983028e-06,
      "loss": 1.5734,
      "step": 661700
    },
    {
      "epoch": 51.840827197242675,
      "grad_norm": 5.882167816162109,
      "learning_rate": 6.799310668964437e-06,
      "loss": 1.6219,
      "step": 661800
    },
    {
      "epoch": 51.84866050446499,
      "grad_norm": 5.09916877746582,
      "learning_rate": 6.7927829129458455e-06,
      "loss": 1.6559,
      "step": 661900
    },
    {
      "epoch": 51.856493811687294,
      "grad_norm": 5.015844345092773,
      "learning_rate": 6.7862551569272555e-06,
      "loss": 1.6545,
      "step": 662000
    },
    {
      "epoch": 51.86432711890961,
      "grad_norm": 6.616437911987305,
      "learning_rate": 6.779727400908664e-06,
      "loss": 1.6486,
      "step": 662100
    },
    {
      "epoch": 51.87216042613191,
      "grad_norm": 7.129486083984375,
      "learning_rate": 6.773199644890073e-06,
      "loss": 1.6982,
      "step": 662200
    },
    {
      "epoch": 51.87999373335422,
      "grad_norm": 5.186465263366699,
      "learning_rate": 6.766671888871481e-06,
      "loss": 1.6063,
      "step": 662300
    },
    {
      "epoch": 51.88782704057653,
      "grad_norm": 5.475937843322754,
      "learning_rate": 6.760144132852891e-06,
      "loss": 1.5845,
      "step": 662400
    },
    {
      "epoch": 51.89566034779884,
      "grad_norm": 6.218810081481934,
      "learning_rate": 6.7536163768343e-06,
      "loss": 1.6087,
      "step": 662500
    },
    {
      "epoch": 51.90349365502115,
      "grad_norm": 9.038005828857422,
      "learning_rate": 6.747088620815709e-06,
      "loss": 1.6346,
      "step": 662600
    },
    {
      "epoch": 51.91132696224346,
      "grad_norm": 5.366968154907227,
      "learning_rate": 6.740560864797117e-06,
      "loss": 1.6686,
      "step": 662700
    },
    {
      "epoch": 51.91916026946577,
      "grad_norm": 8.75390625,
      "learning_rate": 6.734033108778527e-06,
      "loss": 1.6425,
      "step": 662800
    },
    {
      "epoch": 51.926993576688076,
      "grad_norm": 6.5095367431640625,
      "learning_rate": 6.727505352759935e-06,
      "loss": 1.7065,
      "step": 662900
    },
    {
      "epoch": 51.93482688391039,
      "grad_norm": 5.084835052490234,
      "learning_rate": 6.7209775967413446e-06,
      "loss": 1.6594,
      "step": 663000
    },
    {
      "epoch": 51.942660191132696,
      "grad_norm": 7.081570148468018,
      "learning_rate": 6.714449840722753e-06,
      "loss": 1.7028,
      "step": 663100
    },
    {
      "epoch": 51.95049349835501,
      "grad_norm": 5.589598655700684,
      "learning_rate": 6.707922084704163e-06,
      "loss": 1.6435,
      "step": 663200
    },
    {
      "epoch": 51.958326805577315,
      "grad_norm": 7.546515464782715,
      "learning_rate": 6.701394328685571e-06,
      "loss": 1.7309,
      "step": 663300
    },
    {
      "epoch": 51.96616011279962,
      "grad_norm": 7.618320941925049,
      "learning_rate": 6.69486657266698e-06,
      "loss": 1.6335,
      "step": 663400
    },
    {
      "epoch": 51.973993420021934,
      "grad_norm": 7.036945343017578,
      "learning_rate": 6.688338816648389e-06,
      "loss": 1.6144,
      "step": 663500
    },
    {
      "epoch": 51.98182672724424,
      "grad_norm": 7.392750263214111,
      "learning_rate": 6.681811060629799e-06,
      "loss": 1.6607,
      "step": 663600
    },
    {
      "epoch": 51.98966003446655,
      "grad_norm": 5.852175712585449,
      "learning_rate": 6.675283304611207e-06,
      "loss": 1.6314,
      "step": 663700
    },
    {
      "epoch": 51.99749334168886,
      "grad_norm": 7.955411911010742,
      "learning_rate": 6.668755548592616e-06,
      "loss": 1.6506,
      "step": 663800
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.7689285278320312,
      "eval_runtime": 3.0022,
      "eval_samples_per_second": 223.837,
      "eval_steps_per_second": 223.837,
      "step": 663832
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.3812425136566162,
      "eval_runtime": 55.1267,
      "eval_samples_per_second": 231.575,
      "eval_steps_per_second": 231.575,
      "step": 663832
    },
    {
      "epoch": 52.00532664891117,
      "grad_norm": 5.032570838928223,
      "learning_rate": 6.6622277925740244e-06,
      "loss": 1.5715,
      "step": 663900
    },
    {
      "epoch": 52.01315995613348,
      "grad_norm": 7.078854560852051,
      "learning_rate": 6.6557000365554344e-06,
      "loss": 1.6534,
      "step": 664000
    },
    {
      "epoch": 52.02099326335579,
      "grad_norm": 6.036436557769775,
      "learning_rate": 6.649172280536843e-06,
      "loss": 1.7021,
      "step": 664100
    },
    {
      "epoch": 52.0288265705781,
      "grad_norm": 6.858436107635498,
      "learning_rate": 6.642644524518252e-06,
      "loss": 1.581,
      "step": 664200
    },
    {
      "epoch": 52.03665987780041,
      "grad_norm": 6.82362174987793,
      "learning_rate": 6.63611676849966e-06,
      "loss": 1.6448,
      "step": 664300
    },
    {
      "epoch": 52.044493185022716,
      "grad_norm": 7.508977890014648,
      "learning_rate": 6.62958901248107e-06,
      "loss": 1.6728,
      "step": 664400
    },
    {
      "epoch": 52.05232649224502,
      "grad_norm": 5.634767532348633,
      "learning_rate": 6.6230612564624785e-06,
      "loss": 1.5999,
      "step": 664500
    },
    {
      "epoch": 52.060159799467336,
      "grad_norm": 6.229513645172119,
      "learning_rate": 6.616533500443888e-06,
      "loss": 1.6396,
      "step": 664600
    },
    {
      "epoch": 52.06799310668964,
      "grad_norm": 6.264803886413574,
      "learning_rate": 6.610005744425296e-06,
      "loss": 1.6874,
      "step": 664700
    },
    {
      "epoch": 52.075826413911955,
      "grad_norm": 4.613613128662109,
      "learning_rate": 6.603477988406706e-06,
      "loss": 1.5932,
      "step": 664800
    },
    {
      "epoch": 52.08365972113426,
      "grad_norm": 7.189504623413086,
      "learning_rate": 6.596950232388114e-06,
      "loss": 1.6841,
      "step": 664900
    },
    {
      "epoch": 52.091493028356574,
      "grad_norm": 5.530358791351318,
      "learning_rate": 6.5904224763695235e-06,
      "loss": 1.5856,
      "step": 665000
    },
    {
      "epoch": 52.09932633557888,
      "grad_norm": 5.827927112579346,
      "learning_rate": 6.583894720350932e-06,
      "loss": 1.5857,
      "step": 665100
    },
    {
      "epoch": 52.10715964280119,
      "grad_norm": 8.051898002624512,
      "learning_rate": 6.577366964332342e-06,
      "loss": 1.6541,
      "step": 665200
    },
    {
      "epoch": 52.1149929500235,
      "grad_norm": 5.077044486999512,
      "learning_rate": 6.57083920831375e-06,
      "loss": 1.6283,
      "step": 665300
    },
    {
      "epoch": 52.12282625724581,
      "grad_norm": 5.155308723449707,
      "learning_rate": 6.564311452295159e-06,
      "loss": 1.5877,
      "step": 665400
    },
    {
      "epoch": 52.13065956446812,
      "grad_norm": 7.969940185546875,
      "learning_rate": 6.5577836962765675e-06,
      "loss": 1.5499,
      "step": 665500
    },
    {
      "epoch": 52.13849287169043,
      "grad_norm": 6.816990852355957,
      "learning_rate": 6.5512559402579776e-06,
      "loss": 1.6124,
      "step": 665600
    },
    {
      "epoch": 52.14632617891274,
      "grad_norm": 9.243706703186035,
      "learning_rate": 6.544728184239386e-06,
      "loss": 1.4911,
      "step": 665700
    },
    {
      "epoch": 52.15415948613504,
      "grad_norm": 5.864870548248291,
      "learning_rate": 6.538200428220795e-06,
      "loss": 1.5903,
      "step": 665800
    },
    {
      "epoch": 52.161992793357356,
      "grad_norm": 5.23381233215332,
      "learning_rate": 6.531672672202203e-06,
      "loss": 1.5708,
      "step": 665900
    },
    {
      "epoch": 52.16982610057966,
      "grad_norm": 6.421432971954346,
      "learning_rate": 6.525144916183613e-06,
      "loss": 1.6254,
      "step": 666000
    },
    {
      "epoch": 52.177659407801976,
      "grad_norm": 6.806771755218506,
      "learning_rate": 6.518617160165022e-06,
      "loss": 1.4741,
      "step": 666100
    },
    {
      "epoch": 52.18549271502428,
      "grad_norm": 6.115469932556152,
      "learning_rate": 6.512089404146431e-06,
      "loss": 1.6293,
      "step": 666200
    },
    {
      "epoch": 52.193326022246595,
      "grad_norm": 6.2015790939331055,
      "learning_rate": 6.505561648127839e-06,
      "loss": 1.5716,
      "step": 666300
    },
    {
      "epoch": 52.2011593294689,
      "grad_norm": 7.241622447967529,
      "learning_rate": 6.499033892109249e-06,
      "loss": 1.6262,
      "step": 666400
    },
    {
      "epoch": 52.208992636691214,
      "grad_norm": 7.069253921508789,
      "learning_rate": 6.492506136090657e-06,
      "loss": 1.6441,
      "step": 666500
    },
    {
      "epoch": 52.21682594391352,
      "grad_norm": 6.729580402374268,
      "learning_rate": 6.485978380072067e-06,
      "loss": 1.6682,
      "step": 666600
    },
    {
      "epoch": 52.22465925113583,
      "grad_norm": 6.036837100982666,
      "learning_rate": 6.479450624053477e-06,
      "loss": 1.6026,
      "step": 666700
    },
    {
      "epoch": 52.23249255835814,
      "grad_norm": 5.809321880340576,
      "learning_rate": 6.472922868034885e-06,
      "loss": 1.6029,
      "step": 666800
    },
    {
      "epoch": 52.240325865580445,
      "grad_norm": 6.661978721618652,
      "learning_rate": 6.466395112016293e-06,
      "loss": 1.6389,
      "step": 666900
    },
    {
      "epoch": 52.24815917280276,
      "grad_norm": 5.4617743492126465,
      "learning_rate": 6.459867355997702e-06,
      "loss": 1.6804,
      "step": 667000
    },
    {
      "epoch": 52.255992480025064,
      "grad_norm": 7.966281890869141,
      "learning_rate": 6.453339599979112e-06,
      "loss": 1.6911,
      "step": 667100
    },
    {
      "epoch": 52.26382578724738,
      "grad_norm": 5.670497894287109,
      "learning_rate": 6.446811843960521e-06,
      "loss": 1.6018,
      "step": 667200
    },
    {
      "epoch": 52.27165909446968,
      "grad_norm": 6.029624938964844,
      "learning_rate": 6.440284087941929e-06,
      "loss": 1.6618,
      "step": 667300
    },
    {
      "epoch": 52.279492401691996,
      "grad_norm": 8.24257755279541,
      "learning_rate": 6.433756331923338e-06,
      "loss": 1.5896,
      "step": 667400
    },
    {
      "epoch": 52.2873257089143,
      "grad_norm": 6.801544189453125,
      "learning_rate": 6.427228575904748e-06,
      "loss": 1.5982,
      "step": 667500
    },
    {
      "epoch": 52.295159016136616,
      "grad_norm": 5.049130916595459,
      "learning_rate": 6.4207008198861564e-06,
      "loss": 1.5974,
      "step": 667600
    },
    {
      "epoch": 52.30299232335892,
      "grad_norm": 7.585036754608154,
      "learning_rate": 6.414173063867565e-06,
      "loss": 1.5818,
      "step": 667700
    },
    {
      "epoch": 52.310825630581235,
      "grad_norm": 15.190119743347168,
      "learning_rate": 6.407645307848974e-06,
      "loss": 1.7188,
      "step": 667800
    },
    {
      "epoch": 52.31865893780354,
      "grad_norm": 5.775905609130859,
      "learning_rate": 6.401117551830384e-06,
      "loss": 1.6041,
      "step": 667900
    },
    {
      "epoch": 52.32649224502585,
      "grad_norm": 6.35819149017334,
      "learning_rate": 6.394589795811792e-06,
      "loss": 1.6032,
      "step": 668000
    },
    {
      "epoch": 52.33432555224816,
      "grad_norm": 6.908580780029297,
      "learning_rate": 6.3880620397932005e-06,
      "loss": 1.6918,
      "step": 668100
    },
    {
      "epoch": 52.342158859470466,
      "grad_norm": 6.624320983886719,
      "learning_rate": 6.38153428377461e-06,
      "loss": 1.5872,
      "step": 668200
    },
    {
      "epoch": 52.34999216669278,
      "grad_norm": 7.693485260009766,
      "learning_rate": 6.37500652775602e-06,
      "loss": 1.718,
      "step": 668300
    },
    {
      "epoch": 52.357825473915085,
      "grad_norm": 7.00637674331665,
      "learning_rate": 6.368478771737428e-06,
      "loss": 1.6033,
      "step": 668400
    },
    {
      "epoch": 52.3656587811374,
      "grad_norm": 8.5392484664917,
      "learning_rate": 6.361951015718836e-06,
      "loss": 1.592,
      "step": 668500
    },
    {
      "epoch": 52.373492088359704,
      "grad_norm": 6.796286582946777,
      "learning_rate": 6.3554232597002455e-06,
      "loss": 1.5741,
      "step": 668600
    },
    {
      "epoch": 52.38132539558202,
      "grad_norm": 7.015561580657959,
      "learning_rate": 6.3488955036816555e-06,
      "loss": 1.6936,
      "step": 668700
    },
    {
      "epoch": 52.38915870280432,
      "grad_norm": 7.355456352233887,
      "learning_rate": 6.342367747663064e-06,
      "loss": 1.5405,
      "step": 668800
    },
    {
      "epoch": 52.396992010026636,
      "grad_norm": 7.744101524353027,
      "learning_rate": 6.335839991644472e-06,
      "loss": 1.6688,
      "step": 668900
    },
    {
      "epoch": 52.40482531724894,
      "grad_norm": 5.909696578979492,
      "learning_rate": 6.329312235625881e-06,
      "loss": 1.6403,
      "step": 669000
    },
    {
      "epoch": 52.41265862447125,
      "grad_norm": 6.910571575164795,
      "learning_rate": 6.322784479607291e-06,
      "loss": 1.5653,
      "step": 669100
    },
    {
      "epoch": 52.42049193169356,
      "grad_norm": 6.350181579589844,
      "learning_rate": 6.3162567235886996e-06,
      "loss": 1.6504,
      "step": 669200
    },
    {
      "epoch": 52.42832523891587,
      "grad_norm": 7.972777843475342,
      "learning_rate": 6.309728967570108e-06,
      "loss": 1.5657,
      "step": 669300
    },
    {
      "epoch": 52.43615854613818,
      "grad_norm": 6.382447242736816,
      "learning_rate": 6.303201211551517e-06,
      "loss": 1.6285,
      "step": 669400
    },
    {
      "epoch": 52.44399185336049,
      "grad_norm": 6.017836093902588,
      "learning_rate": 6.296673455532927e-06,
      "loss": 1.6631,
      "step": 669500
    },
    {
      "epoch": 52.4518251605828,
      "grad_norm": 6.022495269775391,
      "learning_rate": 6.290145699514335e-06,
      "loss": 1.6174,
      "step": 669600
    },
    {
      "epoch": 52.459658467805106,
      "grad_norm": 8.15694808959961,
      "learning_rate": 6.283617943495744e-06,
      "loss": 1.6155,
      "step": 669700
    },
    {
      "epoch": 52.46749177502742,
      "grad_norm": 6.188666343688965,
      "learning_rate": 6.277090187477153e-06,
      "loss": 1.582,
      "step": 669800
    },
    {
      "epoch": 52.475325082249725,
      "grad_norm": 7.53449821472168,
      "learning_rate": 6.270562431458563e-06,
      "loss": 1.597,
      "step": 669900
    },
    {
      "epoch": 52.48315838947204,
      "grad_norm": 6.160593509674072,
      "learning_rate": 6.264034675439971e-06,
      "loss": 1.5932,
      "step": 670000
    },
    {
      "epoch": 52.490991696694344,
      "grad_norm": 6.9479451179504395,
      "learning_rate": 6.2575069194213794e-06,
      "loss": 1.7405,
      "step": 670100
    },
    {
      "epoch": 52.49882500391665,
      "grad_norm": 4.964861869812012,
      "learning_rate": 6.250979163402789e-06,
      "loss": 1.631,
      "step": 670200
    },
    {
      "epoch": 52.50665831113896,
      "grad_norm": 6.564940452575684,
      "learning_rate": 6.244451407384198e-06,
      "loss": 1.6252,
      "step": 670300
    },
    {
      "epoch": 52.51449161836127,
      "grad_norm": 6.454919815063477,
      "learning_rate": 6.237923651365607e-06,
      "loss": 1.6275,
      "step": 670400
    },
    {
      "epoch": 52.52232492558358,
      "grad_norm": 4.768153190612793,
      "learning_rate": 6.231395895347015e-06,
      "loss": 1.6113,
      "step": 670500
    },
    {
      "epoch": 52.53015823280589,
      "grad_norm": 7.260681629180908,
      "learning_rate": 6.224868139328425e-06,
      "loss": 1.6304,
      "step": 670600
    },
    {
      "epoch": 52.5379915400282,
      "grad_norm": 7.855503082275391,
      "learning_rate": 6.2183403833098335e-06,
      "loss": 1.6018,
      "step": 670700
    },
    {
      "epoch": 52.54582484725051,
      "grad_norm": 6.859232425689697,
      "learning_rate": 6.211812627291243e-06,
      "loss": 1.6277,
      "step": 670800
    },
    {
      "epoch": 52.55365815447282,
      "grad_norm": 6.061002254486084,
      "learning_rate": 6.205284871272651e-06,
      "loss": 1.7211,
      "step": 670900
    },
    {
      "epoch": 52.56149146169513,
      "grad_norm": 8.736882209777832,
      "learning_rate": 6.198757115254061e-06,
      "loss": 1.6684,
      "step": 671000
    },
    {
      "epoch": 52.56932476891744,
      "grad_norm": 6.50589656829834,
      "learning_rate": 6.192229359235469e-06,
      "loss": 1.7116,
      "step": 671100
    },
    {
      "epoch": 52.577158076139746,
      "grad_norm": 7.628647327423096,
      "learning_rate": 6.1857016032168785e-06,
      "loss": 1.6089,
      "step": 671200
    },
    {
      "epoch": 52.58499138336205,
      "grad_norm": 9.602656364440918,
      "learning_rate": 6.179173847198287e-06,
      "loss": 1.7217,
      "step": 671300
    },
    {
      "epoch": 52.592824690584365,
      "grad_norm": 5.104248523712158,
      "learning_rate": 6.172646091179697e-06,
      "loss": 1.6369,
      "step": 671400
    },
    {
      "epoch": 52.60065799780667,
      "grad_norm": 6.130305290222168,
      "learning_rate": 6.166118335161105e-06,
      "loss": 1.5121,
      "step": 671500
    },
    {
      "epoch": 52.608491305028984,
      "grad_norm": 7.665198802947998,
      "learning_rate": 6.159590579142514e-06,
      "loss": 1.6153,
      "step": 671600
    },
    {
      "epoch": 52.61632461225129,
      "grad_norm": 8.33317756652832,
      "learning_rate": 6.1530628231239225e-06,
      "loss": 1.6545,
      "step": 671700
    },
    {
      "epoch": 52.6241579194736,
      "grad_norm": 7.546142578125,
      "learning_rate": 6.1465350671053326e-06,
      "loss": 1.571,
      "step": 671800
    },
    {
      "epoch": 52.63199122669591,
      "grad_norm": 10.17086124420166,
      "learning_rate": 6.140007311086741e-06,
      "loss": 1.6426,
      "step": 671900
    },
    {
      "epoch": 52.63982453391822,
      "grad_norm": 6.658374309539795,
      "learning_rate": 6.13347955506815e-06,
      "loss": 1.6692,
      "step": 672000
    },
    {
      "epoch": 52.64765784114053,
      "grad_norm": 6.918161392211914,
      "learning_rate": 6.126951799049558e-06,
      "loss": 1.6695,
      "step": 672100
    },
    {
      "epoch": 52.65549114836284,
      "grad_norm": 5.937437534332275,
      "learning_rate": 6.120424043030968e-06,
      "loss": 1.6905,
      "step": 672200
    },
    {
      "epoch": 52.66332445558515,
      "grad_norm": 5.247898578643799,
      "learning_rate": 6.113896287012377e-06,
      "loss": 1.563,
      "step": 672300
    },
    {
      "epoch": 52.67115776280746,
      "grad_norm": 7.281270980834961,
      "learning_rate": 6.107368530993786e-06,
      "loss": 1.5512,
      "step": 672400
    },
    {
      "epoch": 52.67899107002977,
      "grad_norm": 6.687432765960693,
      "learning_rate": 6.100840774975195e-06,
      "loss": 1.5693,
      "step": 672500
    },
    {
      "epoch": 52.68682437725207,
      "grad_norm": 8.24148941040039,
      "learning_rate": 6.094313018956604e-06,
      "loss": 1.6614,
      "step": 672600
    },
    {
      "epoch": 52.694657684474386,
      "grad_norm": 6.67753791809082,
      "learning_rate": 6.087785262938013e-06,
      "loss": 1.7449,
      "step": 672700
    },
    {
      "epoch": 52.70249099169669,
      "grad_norm": 5.59410285949707,
      "learning_rate": 6.0812575069194216e-06,
      "loss": 1.6171,
      "step": 672800
    },
    {
      "epoch": 52.710324298919005,
      "grad_norm": 4.121549606323242,
      "learning_rate": 6.074729750900831e-06,
      "loss": 1.616,
      "step": 672900
    },
    {
      "epoch": 52.71815760614131,
      "grad_norm": 8.32129955291748,
      "learning_rate": 6.06820199488224e-06,
      "loss": 1.5844,
      "step": 673000
    },
    {
      "epoch": 52.725990913363624,
      "grad_norm": 6.479093551635742,
      "learning_rate": 6.061674238863649e-06,
      "loss": 1.7583,
      "step": 673100
    },
    {
      "epoch": 52.73382422058593,
      "grad_norm": 4.4149041175842285,
      "learning_rate": 6.055146482845057e-06,
      "loss": 1.5994,
      "step": 673200
    },
    {
      "epoch": 52.74165752780824,
      "grad_norm": 6.051857948303223,
      "learning_rate": 6.0486187268264665e-06,
      "loss": 1.6219,
      "step": 673300
    },
    {
      "epoch": 52.74949083503055,
      "grad_norm": 3.987870454788208,
      "learning_rate": 6.042090970807876e-06,
      "loss": 1.5598,
      "step": 673400
    },
    {
      "epoch": 52.75732414225286,
      "grad_norm": 7.948941230773926,
      "learning_rate": 6.035563214789285e-06,
      "loss": 1.6606,
      "step": 673500
    },
    {
      "epoch": 52.76515744947517,
      "grad_norm": 9.490455627441406,
      "learning_rate": 6.029035458770693e-06,
      "loss": 1.6478,
      "step": 673600
    },
    {
      "epoch": 52.772990756697475,
      "grad_norm": 7.186367988586426,
      "learning_rate": 6.022507702752102e-06,
      "loss": 1.5578,
      "step": 673700
    },
    {
      "epoch": 52.78082406391979,
      "grad_norm": 3.407691478729248,
      "learning_rate": 6.0159799467335114e-06,
      "loss": 1.5895,
      "step": 673800
    },
    {
      "epoch": 52.788657371142094,
      "grad_norm": 6.0854082107543945,
      "learning_rate": 6.009452190714921e-06,
      "loss": 1.6996,
      "step": 673900
    },
    {
      "epoch": 52.79649067836441,
      "grad_norm": 6.5589823722839355,
      "learning_rate": 6.002924434696329e-06,
      "loss": 1.6946,
      "step": 674000
    },
    {
      "epoch": 52.80432398558671,
      "grad_norm": 6.918717384338379,
      "learning_rate": 5.996396678677738e-06,
      "loss": 1.5551,
      "step": 674100
    },
    {
      "epoch": 52.812157292809026,
      "grad_norm": 8.103160858154297,
      "learning_rate": 5.989868922659147e-06,
      "loss": 1.5754,
      "step": 674200
    },
    {
      "epoch": 52.81999060003133,
      "grad_norm": 7.686539649963379,
      "learning_rate": 5.983341166640556e-06,
      "loss": 1.6237,
      "step": 674300
    },
    {
      "epoch": 52.827823907253645,
      "grad_norm": 5.282656669616699,
      "learning_rate": 5.976813410621965e-06,
      "loss": 1.6276,
      "step": 674400
    },
    {
      "epoch": 52.83565721447595,
      "grad_norm": 8.139527320861816,
      "learning_rate": 5.970285654603374e-06,
      "loss": 1.6109,
      "step": 674500
    },
    {
      "epoch": 52.843490521698264,
      "grad_norm": 4.793598651885986,
      "learning_rate": 5.963757898584783e-06,
      "loss": 1.6219,
      "step": 674600
    },
    {
      "epoch": 52.85132382892057,
      "grad_norm": 7.118646144866943,
      "learning_rate": 5.957230142566192e-06,
      "loss": 1.6712,
      "step": 674700
    },
    {
      "epoch": 52.859157136142876,
      "grad_norm": 8.167584419250488,
      "learning_rate": 5.9507023865476005e-06,
      "loss": 1.6918,
      "step": 674800
    },
    {
      "epoch": 52.86699044336519,
      "grad_norm": 7.364569664001465,
      "learning_rate": 5.94417463052901e-06,
      "loss": 1.6241,
      "step": 674900
    },
    {
      "epoch": 52.874823750587495,
      "grad_norm": 7.346582889556885,
      "learning_rate": 5.937646874510419e-06,
      "loss": 1.6307,
      "step": 675000
    },
    {
      "epoch": 52.88265705780981,
      "grad_norm": 8.162089347839355,
      "learning_rate": 5.931119118491828e-06,
      "loss": 1.5759,
      "step": 675100
    },
    {
      "epoch": 52.890490365032115,
      "grad_norm": 8.252707481384277,
      "learning_rate": 5.924591362473236e-06,
      "loss": 1.6344,
      "step": 675200
    },
    {
      "epoch": 52.89832367225443,
      "grad_norm": 6.974992275238037,
      "learning_rate": 5.918063606454645e-06,
      "loss": 1.6403,
      "step": 675300
    },
    {
      "epoch": 52.906156979476734,
      "grad_norm": 6.1777777671813965,
      "learning_rate": 5.9115358504360546e-06,
      "loss": 1.6154,
      "step": 675400
    },
    {
      "epoch": 52.91399028669905,
      "grad_norm": 5.018465995788574,
      "learning_rate": 5.905008094417464e-06,
      "loss": 1.6013,
      "step": 675500
    },
    {
      "epoch": 52.92182359392135,
      "grad_norm": 6.254144191741943,
      "learning_rate": 5.898480338398872e-06,
      "loss": 1.6116,
      "step": 675600
    },
    {
      "epoch": 52.929656901143666,
      "grad_norm": 6.720829010009766,
      "learning_rate": 5.891952582380281e-06,
      "loss": 1.5935,
      "step": 675700
    },
    {
      "epoch": 52.93749020836597,
      "grad_norm": 5.481461048126221,
      "learning_rate": 5.88542482636169e-06,
      "loss": 1.6152,
      "step": 675800
    },
    {
      "epoch": 52.94532351558828,
      "grad_norm": 5.4778852462768555,
      "learning_rate": 5.8788970703430995e-06,
      "loss": 1.6437,
      "step": 675900
    },
    {
      "epoch": 52.95315682281059,
      "grad_norm": 5.977543830871582,
      "learning_rate": 5.872369314324508e-06,
      "loss": 1.6141,
      "step": 676000
    },
    {
      "epoch": 52.9609901300329,
      "grad_norm": 6.991710186004639,
      "learning_rate": 5.865841558305917e-06,
      "loss": 1.7399,
      "step": 676100
    },
    {
      "epoch": 52.96882343725521,
      "grad_norm": 6.741425514221191,
      "learning_rate": 5.859313802287326e-06,
      "loss": 1.6781,
      "step": 676200
    },
    {
      "epoch": 52.976656744477516,
      "grad_norm": 4.396354675292969,
      "learning_rate": 5.852786046268735e-06,
      "loss": 1.6123,
      "step": 676300
    },
    {
      "epoch": 52.98449005169983,
      "grad_norm": 6.709303855895996,
      "learning_rate": 5.846258290250144e-06,
      "loss": 1.681,
      "step": 676400
    },
    {
      "epoch": 52.992323358922135,
      "grad_norm": 7.258852481842041,
      "learning_rate": 5.839730534231553e-06,
      "loss": 1.5857,
      "step": 676500
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.7673448324203491,
      "eval_runtime": 2.8887,
      "eval_samples_per_second": 232.633,
      "eval_steps_per_second": 232.633,
      "step": 676598
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.3771109580993652,
      "eval_runtime": 54.9472,
      "eval_samples_per_second": 232.332,
      "eval_steps_per_second": 232.332,
      "step": 676598
    },
    {
      "epoch": 53.00015666614445,
      "grad_norm": 4.5159220695495605,
      "learning_rate": 5.833202778212962e-06,
      "loss": 1.6104,
      "step": 676600
    },
    {
      "epoch": 53.007989973366755,
      "grad_norm": 5.853626251220703,
      "learning_rate": 5.826675022194371e-06,
      "loss": 1.6431,
      "step": 676700
    },
    {
      "epoch": 53.01582328058907,
      "grad_norm": 7.384280204772949,
      "learning_rate": 5.820147266175779e-06,
      "loss": 1.7241,
      "step": 676800
    },
    {
      "epoch": 53.023656587811374,
      "grad_norm": 5.732924461364746,
      "learning_rate": 5.8136195101571885e-06,
      "loss": 1.5822,
      "step": 676900
    },
    {
      "epoch": 53.03148989503368,
      "grad_norm": 6.9223504066467285,
      "learning_rate": 5.807091754138598e-06,
      "loss": 1.5859,
      "step": 677000
    },
    {
      "epoch": 53.03932320225599,
      "grad_norm": 9.446432113647461,
      "learning_rate": 5.800563998120007e-06,
      "loss": 1.6307,
      "step": 677100
    },
    {
      "epoch": 53.0471565094783,
      "grad_norm": 5.2924628257751465,
      "learning_rate": 5.794036242101415e-06,
      "loss": 1.6756,
      "step": 677200
    },
    {
      "epoch": 53.05498981670061,
      "grad_norm": 5.717861652374268,
      "learning_rate": 5.787508486082824e-06,
      "loss": 1.6013,
      "step": 677300
    },
    {
      "epoch": 53.06282312392292,
      "grad_norm": 6.394556999206543,
      "learning_rate": 5.7809807300642335e-06,
      "loss": 1.7084,
      "step": 677400
    },
    {
      "epoch": 53.07065643114523,
      "grad_norm": 7.523144721984863,
      "learning_rate": 5.774452974045643e-06,
      "loss": 1.6369,
      "step": 677500
    },
    {
      "epoch": 53.07848973836754,
      "grad_norm": 4.11993408203125,
      "learning_rate": 5.767925218027051e-06,
      "loss": 1.6674,
      "step": 677600
    },
    {
      "epoch": 53.08632304558985,
      "grad_norm": 6.346866130828857,
      "learning_rate": 5.76139746200846e-06,
      "loss": 1.7254,
      "step": 677700
    },
    {
      "epoch": 53.094156352812156,
      "grad_norm": 6.65005350112915,
      "learning_rate": 5.754869705989869e-06,
      "loss": 1.6484,
      "step": 677800
    },
    {
      "epoch": 53.10198966003447,
      "grad_norm": 6.2661051750183105,
      "learning_rate": 5.748341949971278e-06,
      "loss": 1.6299,
      "step": 677900
    },
    {
      "epoch": 53.109822967256775,
      "grad_norm": 6.1045050621032715,
      "learning_rate": 5.741814193952687e-06,
      "loss": 1.5991,
      "step": 678000
    },
    {
      "epoch": 53.11765627447909,
      "grad_norm": 7.410317420959473,
      "learning_rate": 5.735286437934096e-06,
      "loss": 1.5891,
      "step": 678100
    },
    {
      "epoch": 53.125489581701395,
      "grad_norm": 7.363353729248047,
      "learning_rate": 5.728758681915505e-06,
      "loss": 1.6875,
      "step": 678200
    },
    {
      "epoch": 53.1333228889237,
      "grad_norm": 6.730284690856934,
      "learning_rate": 5.722230925896914e-06,
      "loss": 1.5626,
      "step": 678300
    },
    {
      "epoch": 53.141156196146014,
      "grad_norm": 7.1534576416015625,
      "learning_rate": 5.715703169878323e-06,
      "loss": 1.5924,
      "step": 678400
    },
    {
      "epoch": 53.14898950336832,
      "grad_norm": 6.096212863922119,
      "learning_rate": 5.709175413859732e-06,
      "loss": 1.6922,
      "step": 678500
    },
    {
      "epoch": 53.15682281059063,
      "grad_norm": 6.044783592224121,
      "learning_rate": 5.702647657841141e-06,
      "loss": 1.6515,
      "step": 678600
    },
    {
      "epoch": 53.16465611781294,
      "grad_norm": 7.080363750457764,
      "learning_rate": 5.69611990182255e-06,
      "loss": 1.5518,
      "step": 678700
    },
    {
      "epoch": 53.17248942503525,
      "grad_norm": 5.661238193511963,
      "learning_rate": 5.689592145803959e-06,
      "loss": 1.6388,
      "step": 678800
    },
    {
      "epoch": 53.18032273225756,
      "grad_norm": 3.8092145919799805,
      "learning_rate": 5.683064389785367e-06,
      "loss": 1.6026,
      "step": 678900
    },
    {
      "epoch": 53.18815603947987,
      "grad_norm": 6.6342291831970215,
      "learning_rate": 5.6765366337667766e-06,
      "loss": 1.6227,
      "step": 679000
    },
    {
      "epoch": 53.19598934670218,
      "grad_norm": 6.562293529510498,
      "learning_rate": 5.670008877748186e-06,
      "loss": 1.6369,
      "step": 679100
    },
    {
      "epoch": 53.20382265392449,
      "grad_norm": 5.3841400146484375,
      "learning_rate": 5.663481121729595e-06,
      "loss": 1.6001,
      "step": 679200
    },
    {
      "epoch": 53.211655961146796,
      "grad_norm": 5.322377681732178,
      "learning_rate": 5.656953365711003e-06,
      "loss": 1.632,
      "step": 679300
    },
    {
      "epoch": 53.2194892683691,
      "grad_norm": 5.345626354217529,
      "learning_rate": 5.650425609692412e-06,
      "loss": 1.4928,
      "step": 679400
    },
    {
      "epoch": 53.227322575591415,
      "grad_norm": 5.511261463165283,
      "learning_rate": 5.6438978536738215e-06,
      "loss": 1.6671,
      "step": 679500
    },
    {
      "epoch": 53.23515588281372,
      "grad_norm": 6.821222305297852,
      "learning_rate": 5.637370097655231e-06,
      "loss": 1.6323,
      "step": 679600
    },
    {
      "epoch": 53.242989190036035,
      "grad_norm": 5.641404151916504,
      "learning_rate": 5.630842341636639e-06,
      "loss": 1.5972,
      "step": 679700
    },
    {
      "epoch": 53.25082249725834,
      "grad_norm": 5.653120040893555,
      "learning_rate": 5.624314585618048e-06,
      "loss": 1.6842,
      "step": 679800
    },
    {
      "epoch": 53.258655804480654,
      "grad_norm": 5.541942119598389,
      "learning_rate": 5.617786829599457e-06,
      "loss": 1.6906,
      "step": 679900
    },
    {
      "epoch": 53.26648911170296,
      "grad_norm": 9.861230850219727,
      "learning_rate": 5.6112590735808664e-06,
      "loss": 1.7204,
      "step": 680000
    },
    {
      "epoch": 53.27432241892527,
      "grad_norm": 4.66350793838501,
      "learning_rate": 5.604731317562275e-06,
      "loss": 1.6079,
      "step": 680100
    },
    {
      "epoch": 53.28215572614758,
      "grad_norm": 6.8543877601623535,
      "learning_rate": 5.598203561543684e-06,
      "loss": 1.5795,
      "step": 680200
    },
    {
      "epoch": 53.28998903336989,
      "grad_norm": 8.149971961975098,
      "learning_rate": 5.591675805525093e-06,
      "loss": 1.6306,
      "step": 680300
    },
    {
      "epoch": 53.2978223405922,
      "grad_norm": 7.034457206726074,
      "learning_rate": 5.585148049506502e-06,
      "loss": 1.6641,
      "step": 680400
    },
    {
      "epoch": 53.305655647814504,
      "grad_norm": 6.266412258148193,
      "learning_rate": 5.5786202934879105e-06,
      "loss": 1.6655,
      "step": 680500
    },
    {
      "epoch": 53.31348895503682,
      "grad_norm": 6.167179584503174,
      "learning_rate": 5.57209253746932e-06,
      "loss": 1.6318,
      "step": 680600
    },
    {
      "epoch": 53.32132226225912,
      "grad_norm": 7.114006519317627,
      "learning_rate": 5.565564781450729e-06,
      "loss": 1.6803,
      "step": 680700
    },
    {
      "epoch": 53.329155569481436,
      "grad_norm": 8.952836990356445,
      "learning_rate": 5.559037025432138e-06,
      "loss": 1.651,
      "step": 680800
    },
    {
      "epoch": 53.33698887670374,
      "grad_norm": 6.608506202697754,
      "learning_rate": 5.552509269413546e-06,
      "loss": 1.5985,
      "step": 680900
    },
    {
      "epoch": 53.344822183926055,
      "grad_norm": 7.1064839363098145,
      "learning_rate": 5.5459815133949555e-06,
      "loss": 1.6875,
      "step": 681000
    },
    {
      "epoch": 53.35265549114836,
      "grad_norm": 7.069084644317627,
      "learning_rate": 5.539453757376365e-06,
      "loss": 1.4987,
      "step": 681100
    },
    {
      "epoch": 53.360488798370675,
      "grad_norm": 5.800370693206787,
      "learning_rate": 5.532926001357774e-06,
      "loss": 1.5867,
      "step": 681200
    },
    {
      "epoch": 53.36832210559298,
      "grad_norm": 6.756039619445801,
      "learning_rate": 5.526398245339182e-06,
      "loss": 1.6414,
      "step": 681300
    },
    {
      "epoch": 53.376155412815294,
      "grad_norm": 6.4201531410217285,
      "learning_rate": 5.519870489320591e-06,
      "loss": 1.5622,
      "step": 681400
    },
    {
      "epoch": 53.3839887200376,
      "grad_norm": 8.876322746276855,
      "learning_rate": 5.513342733302e-06,
      "loss": 1.5956,
      "step": 681500
    },
    {
      "epoch": 53.391822027259906,
      "grad_norm": 7.816333770751953,
      "learning_rate": 5.5068149772834096e-06,
      "loss": 1.6199,
      "step": 681600
    },
    {
      "epoch": 53.39965533448222,
      "grad_norm": 7.217838287353516,
      "learning_rate": 5.500287221264818e-06,
      "loss": 1.6345,
      "step": 681700
    },
    {
      "epoch": 53.407488641704525,
      "grad_norm": 7.955440521240234,
      "learning_rate": 5.493759465246227e-06,
      "loss": 1.6023,
      "step": 681800
    },
    {
      "epoch": 53.41532194892684,
      "grad_norm": 8.796285629272461,
      "learning_rate": 5.487231709227636e-06,
      "loss": 1.5973,
      "step": 681900
    },
    {
      "epoch": 53.423155256149144,
      "grad_norm": 5.567033767700195,
      "learning_rate": 5.480703953209045e-06,
      "loss": 1.6412,
      "step": 682000
    },
    {
      "epoch": 53.43098856337146,
      "grad_norm": 6.51765251159668,
      "learning_rate": 5.474176197190454e-06,
      "loss": 1.719,
      "step": 682100
    },
    {
      "epoch": 53.43882187059376,
      "grad_norm": 7.373922348022461,
      "learning_rate": 5.467648441171863e-06,
      "loss": 1.6074,
      "step": 682200
    },
    {
      "epoch": 53.446655177816076,
      "grad_norm": 4.402888774871826,
      "learning_rate": 5.461120685153272e-06,
      "loss": 1.5274,
      "step": 682300
    },
    {
      "epoch": 53.45448848503838,
      "grad_norm": 5.758532524108887,
      "learning_rate": 5.454592929134681e-06,
      "loss": 1.6,
      "step": 682400
    },
    {
      "epoch": 53.462321792260695,
      "grad_norm": 3.895678997039795,
      "learning_rate": 5.4480651731160894e-06,
      "loss": 1.4989,
      "step": 682500
    },
    {
      "epoch": 53.470155099483,
      "grad_norm": 7.0406999588012695,
      "learning_rate": 5.441537417097499e-06,
      "loss": 1.5843,
      "step": 682600
    },
    {
      "epoch": 53.47798840670531,
      "grad_norm": 7.535758018493652,
      "learning_rate": 5.435009661078908e-06,
      "loss": 1.62,
      "step": 682700
    },
    {
      "epoch": 53.48582171392762,
      "grad_norm": 5.808889865875244,
      "learning_rate": 5.428481905060317e-06,
      "loss": 1.5288,
      "step": 682800
    },
    {
      "epoch": 53.49365502114993,
      "grad_norm": 7.051493167877197,
      "learning_rate": 5.421954149041725e-06,
      "loss": 1.5906,
      "step": 682900
    },
    {
      "epoch": 53.50148832837224,
      "grad_norm": 6.415609359741211,
      "learning_rate": 5.415426393023134e-06,
      "loss": 1.67,
      "step": 683000
    },
    {
      "epoch": 53.509321635594546,
      "grad_norm": 8.683690071105957,
      "learning_rate": 5.4088986370045435e-06,
      "loss": 1.5257,
      "step": 683100
    },
    {
      "epoch": 53.51715494281686,
      "grad_norm": 5.808963775634766,
      "learning_rate": 5.402370880985953e-06,
      "loss": 1.5904,
      "step": 683200
    },
    {
      "epoch": 53.524988250039165,
      "grad_norm": 7.356868743896484,
      "learning_rate": 5.395843124967361e-06,
      "loss": 1.5345,
      "step": 683300
    },
    {
      "epoch": 53.53282155726148,
      "grad_norm": 4.857187271118164,
      "learning_rate": 5.38931536894877e-06,
      "loss": 1.6855,
      "step": 683400
    },
    {
      "epoch": 53.540654864483784,
      "grad_norm": 6.231029987335205,
      "learning_rate": 5.382787612930179e-06,
      "loss": 1.609,
      "step": 683500
    },
    {
      "epoch": 53.5484881717061,
      "grad_norm": 5.577967643737793,
      "learning_rate": 5.3762598569115885e-06,
      "loss": 1.6681,
      "step": 683600
    },
    {
      "epoch": 53.5563214789284,
      "grad_norm": 6.499988079071045,
      "learning_rate": 5.369732100892997e-06,
      "loss": 1.5502,
      "step": 683700
    },
    {
      "epoch": 53.56415478615071,
      "grad_norm": 8.147106170654297,
      "learning_rate": 5.363204344874406e-06,
      "loss": 1.6676,
      "step": 683800
    },
    {
      "epoch": 53.57198809337302,
      "grad_norm": 6.234705924987793,
      "learning_rate": 5.356676588855815e-06,
      "loss": 1.7158,
      "step": 683900
    },
    {
      "epoch": 53.57982140059533,
      "grad_norm": 5.291576385498047,
      "learning_rate": 5.350148832837224e-06,
      "loss": 1.6132,
      "step": 684000
    },
    {
      "epoch": 53.58765470781764,
      "grad_norm": 6.768683910369873,
      "learning_rate": 5.3436210768186325e-06,
      "loss": 1.5006,
      "step": 684100
    },
    {
      "epoch": 53.59548801503995,
      "grad_norm": 7.093318939208984,
      "learning_rate": 5.337093320800042e-06,
      "loss": 1.6121,
      "step": 684200
    },
    {
      "epoch": 53.60332132226226,
      "grad_norm": 6.138841152191162,
      "learning_rate": 5.330565564781451e-06,
      "loss": 1.5908,
      "step": 684300
    },
    {
      "epoch": 53.61115462948457,
      "grad_norm": 5.496970176696777,
      "learning_rate": 5.32403780876286e-06,
      "loss": 1.6378,
      "step": 684400
    },
    {
      "epoch": 53.61898793670688,
      "grad_norm": 6.365888595581055,
      "learning_rate": 5.317510052744269e-06,
      "loss": 1.6483,
      "step": 684500
    },
    {
      "epoch": 53.626821243929186,
      "grad_norm": 4.608502388000488,
      "learning_rate": 5.3109822967256775e-06,
      "loss": 1.7334,
      "step": 684600
    },
    {
      "epoch": 53.6346545511515,
      "grad_norm": 6.615464210510254,
      "learning_rate": 5.3044545407070875e-06,
      "loss": 1.7219,
      "step": 684700
    },
    {
      "epoch": 53.642487858373805,
      "grad_norm": 9.202138900756836,
      "learning_rate": 5.297926784688496e-06,
      "loss": 1.7619,
      "step": 684800
    },
    {
      "epoch": 53.65032116559612,
      "grad_norm": 7.565525531768799,
      "learning_rate": 5.291399028669905e-06,
      "loss": 1.6579,
      "step": 684900
    },
    {
      "epoch": 53.658154472818424,
      "grad_norm": 5.590305328369141,
      "learning_rate": 5.284871272651313e-06,
      "loss": 1.6421,
      "step": 685000
    },
    {
      "epoch": 53.66598778004073,
      "grad_norm": 4.484298229217529,
      "learning_rate": 5.278343516632723e-06,
      "loss": 1.6661,
      "step": 685100
    },
    {
      "epoch": 53.67382108726304,
      "grad_norm": 8.048530578613281,
      "learning_rate": 5.2718157606141316e-06,
      "loss": 1.6234,
      "step": 685200
    },
    {
      "epoch": 53.68165439448535,
      "grad_norm": 7.13932991027832,
      "learning_rate": 5.265288004595541e-06,
      "loss": 1.5731,
      "step": 685300
    },
    {
      "epoch": 53.68948770170766,
      "grad_norm": 6.32609748840332,
      "learning_rate": 5.258760248576949e-06,
      "loss": 1.642,
      "step": 685400
    },
    {
      "epoch": 53.69732100892997,
      "grad_norm": 6.798654556274414,
      "learning_rate": 5.252232492558359e-06,
      "loss": 1.5844,
      "step": 685500
    },
    {
      "epoch": 53.70515431615228,
      "grad_norm": 7.06183385848999,
      "learning_rate": 5.245704736539767e-06,
      "loss": 1.5703,
      "step": 685600
    },
    {
      "epoch": 53.71298762337459,
      "grad_norm": 6.181709289550781,
      "learning_rate": 5.2391769805211765e-06,
      "loss": 1.5268,
      "step": 685700
    },
    {
      "epoch": 53.7208209305969,
      "grad_norm": 7.193942546844482,
      "learning_rate": 5.232649224502585e-06,
      "loss": 1.6744,
      "step": 685800
    },
    {
      "epoch": 53.72865423781921,
      "grad_norm": 6.957716464996338,
      "learning_rate": 5.226121468483995e-06,
      "loss": 1.736,
      "step": 685900
    },
    {
      "epoch": 53.73648754504152,
      "grad_norm": 5.263199806213379,
      "learning_rate": 5.219593712465403e-06,
      "loss": 1.7137,
      "step": 686000
    },
    {
      "epoch": 53.744320852263826,
      "grad_norm": 6.386713981628418,
      "learning_rate": 5.213065956446812e-06,
      "loss": 1.6068,
      "step": 686100
    },
    {
      "epoch": 53.75215415948613,
      "grad_norm": 8.256792068481445,
      "learning_rate": 5.206538200428221e-06,
      "loss": 1.5678,
      "step": 686200
    },
    {
      "epoch": 53.759987466708445,
      "grad_norm": 6.536787986755371,
      "learning_rate": 5.200010444409631e-06,
      "loss": 1.5717,
      "step": 686300
    },
    {
      "epoch": 53.76782077393075,
      "grad_norm": 5.816901683807373,
      "learning_rate": 5.193482688391039e-06,
      "loss": 1.5844,
      "step": 686400
    },
    {
      "epoch": 53.775654081153064,
      "grad_norm": 5.509217739105225,
      "learning_rate": 5.186954932372448e-06,
      "loss": 1.6922,
      "step": 686500
    },
    {
      "epoch": 53.78348738837537,
      "grad_norm": 6.316931247711182,
      "learning_rate": 5.180427176353856e-06,
      "loss": 1.6289,
      "step": 686600
    },
    {
      "epoch": 53.79132069559768,
      "grad_norm": 7.673096656799316,
      "learning_rate": 5.173899420335266e-06,
      "loss": 1.705,
      "step": 686700
    },
    {
      "epoch": 53.79915400281999,
      "grad_norm": 7.092010498046875,
      "learning_rate": 5.167371664316675e-06,
      "loss": 1.504,
      "step": 686800
    },
    {
      "epoch": 53.8069873100423,
      "grad_norm": 5.507839202880859,
      "learning_rate": 5.160843908298084e-06,
      "loss": 1.6084,
      "step": 686900
    },
    {
      "epoch": 53.81482061726461,
      "grad_norm": 5.4466142654418945,
      "learning_rate": 5.154316152279492e-06,
      "loss": 1.631,
      "step": 687000
    },
    {
      "epoch": 53.82265392448692,
      "grad_norm": 6.180286884307861,
      "learning_rate": 5.147788396260902e-06,
      "loss": 1.6538,
      "step": 687100
    },
    {
      "epoch": 53.83048723170923,
      "grad_norm": 5.58500337600708,
      "learning_rate": 5.1412606402423105e-06,
      "loss": 1.5787,
      "step": 687200
    },
    {
      "epoch": 53.838320538931534,
      "grad_norm": 6.686302661895752,
      "learning_rate": 5.13473288422372e-06,
      "loss": 1.67,
      "step": 687300
    },
    {
      "epoch": 53.84615384615385,
      "grad_norm": 5.741768836975098,
      "learning_rate": 5.128205128205128e-06,
      "loss": 1.5922,
      "step": 687400
    },
    {
      "epoch": 53.85398715337615,
      "grad_norm": 7.462020397186279,
      "learning_rate": 5.121677372186538e-06,
      "loss": 1.697,
      "step": 687500
    },
    {
      "epoch": 53.861820460598466,
      "grad_norm": 9.303537368774414,
      "learning_rate": 5.115149616167946e-06,
      "loss": 1.6567,
      "step": 687600
    },
    {
      "epoch": 53.86965376782077,
      "grad_norm": 6.04030704498291,
      "learning_rate": 5.108621860149355e-06,
      "loss": 1.6144,
      "step": 687700
    },
    {
      "epoch": 53.877487075043085,
      "grad_norm": 5.688837051391602,
      "learning_rate": 5.102094104130764e-06,
      "loss": 1.5426,
      "step": 687800
    },
    {
      "epoch": 53.88532038226539,
      "grad_norm": 7.587066173553467,
      "learning_rate": 5.095566348112174e-06,
      "loss": 1.5995,
      "step": 687900
    },
    {
      "epoch": 53.893153689487704,
      "grad_norm": 5.741650104522705,
      "learning_rate": 5.089038592093582e-06,
      "loss": 1.563,
      "step": 688000
    },
    {
      "epoch": 53.90098699671001,
      "grad_norm": 9.085847854614258,
      "learning_rate": 5.082510836074991e-06,
      "loss": 1.6403,
      "step": 688100
    },
    {
      "epoch": 53.90882030393232,
      "grad_norm": 6.3785529136657715,
      "learning_rate": 5.0759830800563995e-06,
      "loss": 1.6118,
      "step": 688200
    },
    {
      "epoch": 53.91665361115463,
      "grad_norm": 4.529816150665283,
      "learning_rate": 5.0694553240378095e-06,
      "loss": 1.6111,
      "step": 688300
    },
    {
      "epoch": 53.924486918376935,
      "grad_norm": 6.417235851287842,
      "learning_rate": 5.062927568019218e-06,
      "loss": 1.5812,
      "step": 688400
    },
    {
      "epoch": 53.93232022559925,
      "grad_norm": 7.274611473083496,
      "learning_rate": 5.056399812000627e-06,
      "loss": 1.5812,
      "step": 688500
    },
    {
      "epoch": 53.940153532821554,
      "grad_norm": 4.759859085083008,
      "learning_rate": 5.049872055982035e-06,
      "loss": 1.7045,
      "step": 688600
    },
    {
      "epoch": 53.94798684004387,
      "grad_norm": 5.433521747589111,
      "learning_rate": 5.043344299963445e-06,
      "loss": 1.6097,
      "step": 688700
    },
    {
      "epoch": 53.955820147266174,
      "grad_norm": 5.89899206161499,
      "learning_rate": 5.036816543944854e-06,
      "loss": 1.5596,
      "step": 688800
    },
    {
      "epoch": 53.96365345448849,
      "grad_norm": 6.003942012786865,
      "learning_rate": 5.030288787926263e-06,
      "loss": 1.6655,
      "step": 688900
    },
    {
      "epoch": 53.97148676171079,
      "grad_norm": 9.76377010345459,
      "learning_rate": 5.023761031907671e-06,
      "loss": 1.6476,
      "step": 689000
    },
    {
      "epoch": 53.979320068933106,
      "grad_norm": 6.563314437866211,
      "learning_rate": 5.017233275889081e-06,
      "loss": 1.6498,
      "step": 689100
    },
    {
      "epoch": 53.98715337615541,
      "grad_norm": 6.53325891494751,
      "learning_rate": 5.010705519870489e-06,
      "loss": 1.5797,
      "step": 689200
    },
    {
      "epoch": 53.994986683377725,
      "grad_norm": 6.212404251098633,
      "learning_rate": 5.0041777638518985e-06,
      "loss": 1.6967,
      "step": 689300
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.7700663805007935,
      "eval_runtime": 2.9479,
      "eval_samples_per_second": 227.958,
      "eval_steps_per_second": 227.958,
      "step": 689364
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.3787504434585571,
      "eval_runtime": 56.4471,
      "eval_samples_per_second": 226.159,
      "eval_steps_per_second": 226.159,
      "step": 689364
    },
    {
      "epoch": 54.00281999060003,
      "grad_norm": 5.755970478057861,
      "learning_rate": 4.997650007833307e-06,
      "loss": 1.6603,
      "step": 689400
    },
    {
      "epoch": 54.010653297822344,
      "grad_norm": 7.234476566314697,
      "learning_rate": 4.991122251814717e-06,
      "loss": 1.6457,
      "step": 689500
    },
    {
      "epoch": 54.01848660504465,
      "grad_norm": 8.037458419799805,
      "learning_rate": 4.984594495796125e-06,
      "loss": 1.6263,
      "step": 689600
    },
    {
      "epoch": 54.026319912266956,
      "grad_norm": 5.402954578399658,
      "learning_rate": 4.978066739777534e-06,
      "loss": 1.6812,
      "step": 689700
    },
    {
      "epoch": 54.03415321948927,
      "grad_norm": 5.32925271987915,
      "learning_rate": 4.971538983758943e-06,
      "loss": 1.607,
      "step": 689800
    },
    {
      "epoch": 54.041986526711575,
      "grad_norm": 6.614448070526123,
      "learning_rate": 4.965011227740353e-06,
      "loss": 1.5833,
      "step": 689900
    },
    {
      "epoch": 54.04981983393389,
      "grad_norm": 5.468404769897461,
      "learning_rate": 4.958483471721761e-06,
      "loss": 1.6622,
      "step": 690000
    },
    {
      "epoch": 54.057653141156194,
      "grad_norm": 5.939357280731201,
      "learning_rate": 4.95195571570317e-06,
      "loss": 1.7262,
      "step": 690100
    },
    {
      "epoch": 54.06548644837851,
      "grad_norm": 6.292684078216553,
      "learning_rate": 4.945427959684578e-06,
      "loss": 1.6193,
      "step": 690200
    },
    {
      "epoch": 54.073319755600814,
      "grad_norm": 6.292162895202637,
      "learning_rate": 4.938900203665988e-06,
      "loss": 1.6548,
      "step": 690300
    },
    {
      "epoch": 54.08115306282313,
      "grad_norm": 7.8368377685546875,
      "learning_rate": 4.9323724476473975e-06,
      "loss": 1.6511,
      "step": 690400
    },
    {
      "epoch": 54.08898637004543,
      "grad_norm": 4.805307388305664,
      "learning_rate": 4.925844691628806e-06,
      "loss": 1.6323,
      "step": 690500
    },
    {
      "epoch": 54.096819677267746,
      "grad_norm": 5.867528915405273,
      "learning_rate": 4.919316935610215e-06,
      "loss": 1.5995,
      "step": 690600
    },
    {
      "epoch": 54.10465298449005,
      "grad_norm": 7.546618938446045,
      "learning_rate": 4.912789179591624e-06,
      "loss": 1.5448,
      "step": 690700
    },
    {
      "epoch": 54.11248629171236,
      "grad_norm": 7.773330211639404,
      "learning_rate": 4.906261423573033e-06,
      "loss": 1.6179,
      "step": 690800
    },
    {
      "epoch": 54.12031959893467,
      "grad_norm": 7.102998733520508,
      "learning_rate": 4.899733667554442e-06,
      "loss": 1.506,
      "step": 690900
    },
    {
      "epoch": 54.12815290615698,
      "grad_norm": 9.6293363571167,
      "learning_rate": 4.893205911535851e-06,
      "loss": 1.5766,
      "step": 691000
    },
    {
      "epoch": 54.13598621337929,
      "grad_norm": 5.783337593078613,
      "learning_rate": 4.88667815551726e-06,
      "loss": 1.6469,
      "step": 691100
    },
    {
      "epoch": 54.143819520601596,
      "grad_norm": 5.856747627258301,
      "learning_rate": 4.880150399498669e-06,
      "loss": 1.7877,
      "step": 691200
    },
    {
      "epoch": 54.15165282782391,
      "grad_norm": 5.516188621520996,
      "learning_rate": 4.873622643480077e-06,
      "loss": 1.6826,
      "step": 691300
    },
    {
      "epoch": 54.159486135046215,
      "grad_norm": 8.221657752990723,
      "learning_rate": 4.8670948874614866e-06,
      "loss": 1.5817,
      "step": 691400
    },
    {
      "epoch": 54.16731944226853,
      "grad_norm": 6.825107097625732,
      "learning_rate": 4.860567131442896e-06,
      "loss": 1.5542,
      "step": 691500
    },
    {
      "epoch": 54.175152749490834,
      "grad_norm": 6.5966901779174805,
      "learning_rate": 4.854039375424305e-06,
      "loss": 1.6809,
      "step": 691600
    },
    {
      "epoch": 54.18298605671315,
      "grad_norm": 7.853688716888428,
      "learning_rate": 4.847511619405713e-06,
      "loss": 1.6231,
      "step": 691700
    },
    {
      "epoch": 54.190819363935454,
      "grad_norm": 6.38349723815918,
      "learning_rate": 4.840983863387122e-06,
      "loss": 1.631,
      "step": 691800
    },
    {
      "epoch": 54.19865267115776,
      "grad_norm": 7.486296653747559,
      "learning_rate": 4.8344561073685315e-06,
      "loss": 1.5904,
      "step": 691900
    },
    {
      "epoch": 54.20648597838007,
      "grad_norm": 6.483420372009277,
      "learning_rate": 4.827928351349941e-06,
      "loss": 1.5971,
      "step": 692000
    },
    {
      "epoch": 54.21431928560238,
      "grad_norm": 7.165478229522705,
      "learning_rate": 4.821400595331349e-06,
      "loss": 1.592,
      "step": 692100
    },
    {
      "epoch": 54.22215259282469,
      "grad_norm": 5.217750549316406,
      "learning_rate": 4.814872839312758e-06,
      "loss": 1.6663,
      "step": 692200
    },
    {
      "epoch": 54.229985900047,
      "grad_norm": 7.3823676109313965,
      "learning_rate": 4.808345083294167e-06,
      "loss": 1.6901,
      "step": 692300
    },
    {
      "epoch": 54.23781920726931,
      "grad_norm": 5.583439350128174,
      "learning_rate": 4.8018173272755764e-06,
      "loss": 1.6373,
      "step": 692400
    },
    {
      "epoch": 54.24565251449162,
      "grad_norm": 6.067525386810303,
      "learning_rate": 4.795289571256985e-06,
      "loss": 1.5628,
      "step": 692500
    },
    {
      "epoch": 54.25348582171393,
      "grad_norm": 6.187053203582764,
      "learning_rate": 4.788761815238394e-06,
      "loss": 1.5611,
      "step": 692600
    },
    {
      "epoch": 54.261319128936236,
      "grad_norm": 6.860554218292236,
      "learning_rate": 4.782234059219803e-06,
      "loss": 1.5569,
      "step": 692700
    },
    {
      "epoch": 54.26915243615855,
      "grad_norm": 7.263980865478516,
      "learning_rate": 4.775706303201212e-06,
      "loss": 1.5284,
      "step": 692800
    },
    {
      "epoch": 54.276985743380855,
      "grad_norm": 7.123185157775879,
      "learning_rate": 4.7691785471826205e-06,
      "loss": 1.5931,
      "step": 692900
    },
    {
      "epoch": 54.28481905060316,
      "grad_norm": 6.8778510093688965,
      "learning_rate": 4.76265079116403e-06,
      "loss": 1.6028,
      "step": 693000
    },
    {
      "epoch": 54.292652357825474,
      "grad_norm": 11.802155494689941,
      "learning_rate": 4.756123035145439e-06,
      "loss": 1.6301,
      "step": 693100
    },
    {
      "epoch": 54.30048566504778,
      "grad_norm": 5.114302158355713,
      "learning_rate": 4.749595279126848e-06,
      "loss": 1.6131,
      "step": 693200
    },
    {
      "epoch": 54.308318972270094,
      "grad_norm": 7.726003646850586,
      "learning_rate": 4.743067523108256e-06,
      "loss": 1.5432,
      "step": 693300
    },
    {
      "epoch": 54.3161522794924,
      "grad_norm": 6.883106708526611,
      "learning_rate": 4.7365397670896655e-06,
      "loss": 1.5721,
      "step": 693400
    },
    {
      "epoch": 54.32398558671471,
      "grad_norm": 5.2332963943481445,
      "learning_rate": 4.730012011071075e-06,
      "loss": 1.527,
      "step": 693500
    },
    {
      "epoch": 54.33181889393702,
      "grad_norm": 7.2500996589660645,
      "learning_rate": 4.723484255052484e-06,
      "loss": 1.7204,
      "step": 693600
    },
    {
      "epoch": 54.33965220115933,
      "grad_norm": 7.570986270904541,
      "learning_rate": 4.716956499033892e-06,
      "loss": 1.6199,
      "step": 693700
    },
    {
      "epoch": 54.34748550838164,
      "grad_norm": 7.485928535461426,
      "learning_rate": 4.710428743015301e-06,
      "loss": 1.6047,
      "step": 693800
    },
    {
      "epoch": 54.35531881560395,
      "grad_norm": 6.864603042602539,
      "learning_rate": 4.70390098699671e-06,
      "loss": 1.74,
      "step": 693900
    },
    {
      "epoch": 54.36315212282626,
      "grad_norm": 7.1886701583862305,
      "learning_rate": 4.6973732309781196e-06,
      "loss": 1.5992,
      "step": 694000
    },
    {
      "epoch": 54.37098543004856,
      "grad_norm": 8.565812110900879,
      "learning_rate": 4.690845474959528e-06,
      "loss": 1.5759,
      "step": 694100
    },
    {
      "epoch": 54.378818737270876,
      "grad_norm": 7.150340557098389,
      "learning_rate": 4.684317718940937e-06,
      "loss": 1.5777,
      "step": 694200
    },
    {
      "epoch": 54.38665204449318,
      "grad_norm": 7.871878623962402,
      "learning_rate": 4.677789962922346e-06,
      "loss": 1.6156,
      "step": 694300
    },
    {
      "epoch": 54.394485351715495,
      "grad_norm": 6.864487171173096,
      "learning_rate": 4.671262206903755e-06,
      "loss": 1.5457,
      "step": 694400
    },
    {
      "epoch": 54.4023186589378,
      "grad_norm": 6.685492515563965,
      "learning_rate": 4.664734450885164e-06,
      "loss": 1.5571,
      "step": 694500
    },
    {
      "epoch": 54.410151966160115,
      "grad_norm": 7.977350234985352,
      "learning_rate": 4.658206694866573e-06,
      "loss": 1.6349,
      "step": 694600
    },
    {
      "epoch": 54.41798527338242,
      "grad_norm": 5.034354209899902,
      "learning_rate": 4.651678938847982e-06,
      "loss": 1.54,
      "step": 694700
    },
    {
      "epoch": 54.425818580604734,
      "grad_norm": 8.905932426452637,
      "learning_rate": 4.645151182829391e-06,
      "loss": 1.7399,
      "step": 694800
    },
    {
      "epoch": 54.43365188782704,
      "grad_norm": 7.645776748657227,
      "learning_rate": 4.638623426810799e-06,
      "loss": 1.6248,
      "step": 694900
    },
    {
      "epoch": 54.44148519504935,
      "grad_norm": 7.782726287841797,
      "learning_rate": 4.632095670792209e-06,
      "loss": 1.5532,
      "step": 695000
    },
    {
      "epoch": 54.44931850227166,
      "grad_norm": 7.104474067687988,
      "learning_rate": 4.625567914773618e-06,
      "loss": 1.6397,
      "step": 695100
    },
    {
      "epoch": 54.457151809493965,
      "grad_norm": 7.1633172035217285,
      "learning_rate": 4.619040158755027e-06,
      "loss": 1.7111,
      "step": 695200
    },
    {
      "epoch": 54.46498511671628,
      "grad_norm": 5.374105453491211,
      "learning_rate": 4.612512402736435e-06,
      "loss": 1.6877,
      "step": 695300
    },
    {
      "epoch": 54.472818423938584,
      "grad_norm": 6.379464626312256,
      "learning_rate": 4.605984646717844e-06,
      "loss": 1.5744,
      "step": 695400
    },
    {
      "epoch": 54.4806517311609,
      "grad_norm": 5.193099498748779,
      "learning_rate": 4.5994568906992535e-06,
      "loss": 1.5456,
      "step": 695500
    },
    {
      "epoch": 54.4884850383832,
      "grad_norm": 6.885365962982178,
      "learning_rate": 4.592929134680663e-06,
      "loss": 1.622,
      "step": 695600
    },
    {
      "epoch": 54.496318345605516,
      "grad_norm": 8.741067886352539,
      "learning_rate": 4.586401378662071e-06,
      "loss": 1.6189,
      "step": 695700
    },
    {
      "epoch": 54.50415165282782,
      "grad_norm": 7.296619415283203,
      "learning_rate": 4.57987362264348e-06,
      "loss": 1.6874,
      "step": 695800
    },
    {
      "epoch": 54.511984960050135,
      "grad_norm": 5.8778076171875,
      "learning_rate": 4.573345866624889e-06,
      "loss": 1.6682,
      "step": 695900
    },
    {
      "epoch": 54.51981826727244,
      "grad_norm": 6.65753698348999,
      "learning_rate": 4.5668181106062984e-06,
      "loss": 1.6847,
      "step": 696000
    },
    {
      "epoch": 54.527651574494755,
      "grad_norm": 7.842724800109863,
      "learning_rate": 4.560290354587707e-06,
      "loss": 1.7476,
      "step": 696100
    },
    {
      "epoch": 54.53548488171706,
      "grad_norm": 5.377195358276367,
      "learning_rate": 4.553762598569116e-06,
      "loss": 1.6479,
      "step": 696200
    },
    {
      "epoch": 54.543318188939374,
      "grad_norm": 5.511490821838379,
      "learning_rate": 4.547234842550525e-06,
      "loss": 1.6847,
      "step": 696300
    },
    {
      "epoch": 54.55115149616168,
      "grad_norm": 6.249700546264648,
      "learning_rate": 4.540707086531934e-06,
      "loss": 1.6302,
      "step": 696400
    },
    {
      "epoch": 54.558984803383986,
      "grad_norm": 6.149134635925293,
      "learning_rate": 4.534179330513343e-06,
      "loss": 1.6599,
      "step": 696500
    },
    {
      "epoch": 54.5668181106063,
      "grad_norm": 5.946188926696777,
      "learning_rate": 4.527651574494752e-06,
      "loss": 1.6447,
      "step": 696600
    },
    {
      "epoch": 54.574651417828605,
      "grad_norm": 7.231273651123047,
      "learning_rate": 4.521123818476161e-06,
      "loss": 1.6305,
      "step": 696700
    },
    {
      "epoch": 54.58248472505092,
      "grad_norm": 5.430513858795166,
      "learning_rate": 4.51459606245757e-06,
      "loss": 1.5791,
      "step": 696800
    },
    {
      "epoch": 54.590318032273224,
      "grad_norm": 8.34162425994873,
      "learning_rate": 4.508068306438979e-06,
      "loss": 1.6035,
      "step": 696900
    },
    {
      "epoch": 54.59815133949554,
      "grad_norm": 10.199116706848145,
      "learning_rate": 4.5015405504203875e-06,
      "loss": 1.6412,
      "step": 697000
    },
    {
      "epoch": 54.60598464671784,
      "grad_norm": 8.02281379699707,
      "learning_rate": 4.495012794401797e-06,
      "loss": 1.7229,
      "step": 697100
    },
    {
      "epoch": 54.613817953940156,
      "grad_norm": 5.363039970397949,
      "learning_rate": 4.488485038383206e-06,
      "loss": 1.6399,
      "step": 697200
    },
    {
      "epoch": 54.62165126116246,
      "grad_norm": 6.920516014099121,
      "learning_rate": 4.481957282364615e-06,
      "loss": 1.6644,
      "step": 697300
    },
    {
      "epoch": 54.629484568384775,
      "grad_norm": 8.516748428344727,
      "learning_rate": 4.475429526346023e-06,
      "loss": 1.5224,
      "step": 697400
    },
    {
      "epoch": 54.63731787560708,
      "grad_norm": 6.784077167510986,
      "learning_rate": 4.468901770327432e-06,
      "loss": 1.54,
      "step": 697500
    },
    {
      "epoch": 54.64515118282939,
      "grad_norm": 4.512975692749023,
      "learning_rate": 4.4623740143088416e-06,
      "loss": 1.6352,
      "step": 697600
    },
    {
      "epoch": 54.6529844900517,
      "grad_norm": 13.145021438598633,
      "learning_rate": 4.455846258290251e-06,
      "loss": 1.5626,
      "step": 697700
    },
    {
      "epoch": 54.66081779727401,
      "grad_norm": 5.196779251098633,
      "learning_rate": 4.449318502271659e-06,
      "loss": 1.5742,
      "step": 697800
    },
    {
      "epoch": 54.66865110449632,
      "grad_norm": 9.412813186645508,
      "learning_rate": 4.442790746253068e-06,
      "loss": 1.6486,
      "step": 697900
    },
    {
      "epoch": 54.676484411718626,
      "grad_norm": 4.891037464141846,
      "learning_rate": 4.436262990234477e-06,
      "loss": 1.6281,
      "step": 698000
    },
    {
      "epoch": 54.68431771894094,
      "grad_norm": 4.936094284057617,
      "learning_rate": 4.4297352342158865e-06,
      "loss": 1.5279,
      "step": 698100
    },
    {
      "epoch": 54.692151026163245,
      "grad_norm": 5.989481449127197,
      "learning_rate": 4.423207478197295e-06,
      "loss": 1.5774,
      "step": 698200
    },
    {
      "epoch": 54.69998433338556,
      "grad_norm": 10.181092262268066,
      "learning_rate": 4.416679722178704e-06,
      "loss": 1.6495,
      "step": 698300
    },
    {
      "epoch": 54.707817640607864,
      "grad_norm": 6.120795726776123,
      "learning_rate": 4.410151966160113e-06,
      "loss": 1.6784,
      "step": 698400
    },
    {
      "epoch": 54.71565094783018,
      "grad_norm": 5.920156002044678,
      "learning_rate": 4.403624210141522e-06,
      "loss": 1.5935,
      "step": 698500
    },
    {
      "epoch": 54.72348425505248,
      "grad_norm": 5.851497173309326,
      "learning_rate": 4.397096454122931e-06,
      "loss": 1.6327,
      "step": 698600
    },
    {
      "epoch": 54.73131756227479,
      "grad_norm": 5.373587608337402,
      "learning_rate": 4.39056869810434e-06,
      "loss": 1.6703,
      "step": 698700
    },
    {
      "epoch": 54.7391508694971,
      "grad_norm": 5.6524810791015625,
      "learning_rate": 4.384040942085749e-06,
      "loss": 1.5781,
      "step": 698800
    },
    {
      "epoch": 54.74698417671941,
      "grad_norm": 6.907302379608154,
      "learning_rate": 4.377513186067158e-06,
      "loss": 1.6283,
      "step": 698900
    },
    {
      "epoch": 54.75481748394172,
      "grad_norm": 5.987986087799072,
      "learning_rate": 4.370985430048566e-06,
      "loss": 1.5937,
      "step": 699000
    },
    {
      "epoch": 54.76265079116403,
      "grad_norm": 6.610835552215576,
      "learning_rate": 4.3644576740299755e-06,
      "loss": 1.6495,
      "step": 699100
    },
    {
      "epoch": 54.77048409838634,
      "grad_norm": 6.9671311378479,
      "learning_rate": 4.357929918011385e-06,
      "loss": 1.758,
      "step": 699200
    },
    {
      "epoch": 54.77831740560865,
      "grad_norm": 5.634120941162109,
      "learning_rate": 4.351402161992794e-06,
      "loss": 1.6002,
      "step": 699300
    },
    {
      "epoch": 54.78615071283096,
      "grad_norm": 5.899339199066162,
      "learning_rate": 4.344874405974202e-06,
      "loss": 1.6664,
      "step": 699400
    },
    {
      "epoch": 54.793984020053266,
      "grad_norm": 7.003031253814697,
      "learning_rate": 4.338346649955611e-06,
      "loss": 1.5006,
      "step": 699500
    },
    {
      "epoch": 54.80181732727558,
      "grad_norm": 8.643501281738281,
      "learning_rate": 4.3318188939370205e-06,
      "loss": 1.6442,
      "step": 699600
    },
    {
      "epoch": 54.809650634497885,
      "grad_norm": 7.60576057434082,
      "learning_rate": 4.32529113791843e-06,
      "loss": 1.7026,
      "step": 699700
    },
    {
      "epoch": 54.81748394172019,
      "grad_norm": 4.510537147521973,
      "learning_rate": 4.318763381899838e-06,
      "loss": 1.6289,
      "step": 699800
    },
    {
      "epoch": 54.825317248942504,
      "grad_norm": 7.6426472663879395,
      "learning_rate": 4.312235625881247e-06,
      "loss": 1.6017,
      "step": 699900
    },
    {
      "epoch": 54.83315055616481,
      "grad_norm": 6.15557336807251,
      "learning_rate": 4.305707869862656e-06,
      "loss": 1.5687,
      "step": 700000
    },
    {
      "epoch": 54.84098386338712,
      "grad_norm": 7.142772674560547,
      "learning_rate": 4.299180113844065e-06,
      "loss": 1.6555,
      "step": 700100
    },
    {
      "epoch": 54.84881717060943,
      "grad_norm": 5.638321399688721,
      "learning_rate": 4.292652357825474e-06,
      "loss": 1.6947,
      "step": 700200
    },
    {
      "epoch": 54.85665047783174,
      "grad_norm": 4.2386155128479,
      "learning_rate": 4.286124601806883e-06,
      "loss": 1.696,
      "step": 700300
    },
    {
      "epoch": 54.86448378505405,
      "grad_norm": 6.4834442138671875,
      "learning_rate": 4.279596845788292e-06,
      "loss": 1.6922,
      "step": 700400
    },
    {
      "epoch": 54.87231709227636,
      "grad_norm": 6.7708821296691895,
      "learning_rate": 4.273069089769701e-06,
      "loss": 1.584,
      "step": 700500
    },
    {
      "epoch": 54.88015039949867,
      "grad_norm": 7.816046714782715,
      "learning_rate": 4.2665413337511095e-06,
      "loss": 1.5423,
      "step": 700600
    },
    {
      "epoch": 54.88798370672098,
      "grad_norm": 5.801748275756836,
      "learning_rate": 4.260013577732519e-06,
      "loss": 1.6954,
      "step": 700700
    },
    {
      "epoch": 54.89581701394329,
      "grad_norm": 7.381167888641357,
      "learning_rate": 4.253485821713928e-06,
      "loss": 1.6112,
      "step": 700800
    },
    {
      "epoch": 54.90365032116559,
      "grad_norm": 6.093142986297607,
      "learning_rate": 4.246958065695337e-06,
      "loss": 1.5821,
      "step": 700900
    },
    {
      "epoch": 54.911483628387906,
      "grad_norm": 7.2189178466796875,
      "learning_rate": 4.240430309676745e-06,
      "loss": 1.6711,
      "step": 701000
    },
    {
      "epoch": 54.91931693561021,
      "grad_norm": 5.092596054077148,
      "learning_rate": 4.233902553658154e-06,
      "loss": 1.559,
      "step": 701100
    },
    {
      "epoch": 54.927150242832525,
      "grad_norm": 6.589841842651367,
      "learning_rate": 4.2273747976395636e-06,
      "loss": 1.609,
      "step": 701200
    },
    {
      "epoch": 54.93498355005483,
      "grad_norm": 6.231753349304199,
      "learning_rate": 4.220847041620973e-06,
      "loss": 1.7024,
      "step": 701300
    },
    {
      "epoch": 54.942816857277144,
      "grad_norm": 5.718156814575195,
      "learning_rate": 4.214319285602381e-06,
      "loss": 1.4773,
      "step": 701400
    },
    {
      "epoch": 54.95065016449945,
      "grad_norm": 7.213438034057617,
      "learning_rate": 4.20779152958379e-06,
      "loss": 1.5662,
      "step": 701500
    },
    {
      "epoch": 54.95848347172176,
      "grad_norm": 6.147013187408447,
      "learning_rate": 4.201263773565199e-06,
      "loss": 1.6732,
      "step": 701600
    },
    {
      "epoch": 54.96631677894407,
      "grad_norm": 6.732831954956055,
      "learning_rate": 4.1947360175466085e-06,
      "loss": 1.5863,
      "step": 701700
    },
    {
      "epoch": 54.97415008616638,
      "grad_norm": 9.21950912475586,
      "learning_rate": 4.188208261528017e-06,
      "loss": 1.576,
      "step": 701800
    },
    {
      "epoch": 54.98198339338869,
      "grad_norm": 6.011282444000244,
      "learning_rate": 4.181680505509426e-06,
      "loss": 1.6575,
      "step": 701900
    },
    {
      "epoch": 54.989816700611,
      "grad_norm": 9.578598022460938,
      "learning_rate": 4.175152749490835e-06,
      "loss": 1.6351,
      "step": 702000
    },
    {
      "epoch": 54.99765000783331,
      "grad_norm": 6.440369606018066,
      "learning_rate": 4.168624993472244e-06,
      "loss": 1.7059,
      "step": 702100
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.769744634628296,
      "eval_runtime": 2.9135,
      "eval_samples_per_second": 230.654,
      "eval_steps_per_second": 230.654,
      "step": 702130
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.3767582178115845,
      "eval_runtime": 30.3562,
      "eval_samples_per_second": 420.54,
      "eval_steps_per_second": 420.54,
      "step": 702130
    },
    {
      "epoch": 55.00548331505561,
      "grad_norm": 6.025872230529785,
      "learning_rate": 4.162097237453653e-06,
      "loss": 1.6514,
      "step": 702200
    },
    {
      "epoch": 55.01331662227793,
      "grad_norm": 5.825247764587402,
      "learning_rate": 4.155569481435062e-06,
      "loss": 1.6453,
      "step": 702300
    },
    {
      "epoch": 55.02114992950023,
      "grad_norm": 6.483631134033203,
      "learning_rate": 4.149041725416472e-06,
      "loss": 1.673,
      "step": 702400
    },
    {
      "epoch": 55.028983236722546,
      "grad_norm": 5.2372145652771,
      "learning_rate": 4.14251396939788e-06,
      "loss": 1.6264,
      "step": 702500
    },
    {
      "epoch": 55.03681654394485,
      "grad_norm": 5.714906692504883,
      "learning_rate": 4.135986213379289e-06,
      "loss": 1.6413,
      "step": 702600
    },
    {
      "epoch": 55.044649851167165,
      "grad_norm": 6.219970703125,
      "learning_rate": 4.1294584573606975e-06,
      "loss": 1.5287,
      "step": 702700
    },
    {
      "epoch": 55.05248315838947,
      "grad_norm": 6.917080879211426,
      "learning_rate": 4.1229307013421075e-06,
      "loss": 1.5964,
      "step": 702800
    },
    {
      "epoch": 55.060316465611784,
      "grad_norm": 6.853165626525879,
      "learning_rate": 4.116402945323516e-06,
      "loss": 1.5447,
      "step": 702900
    },
    {
      "epoch": 55.06814977283409,
      "grad_norm": 11.929542541503906,
      "learning_rate": 4.109875189304925e-06,
      "loss": 1.6489,
      "step": 703000
    },
    {
      "epoch": 55.0759830800564,
      "grad_norm": 6.610202312469482,
      "learning_rate": 4.103347433286333e-06,
      "loss": 1.6135,
      "step": 703100
    },
    {
      "epoch": 55.08381638727871,
      "grad_norm": 6.608997344970703,
      "learning_rate": 4.096819677267743e-06,
      "loss": 1.5631,
      "step": 703200
    },
    {
      "epoch": 55.091649694501015,
      "grad_norm": 6.1073784828186035,
      "learning_rate": 4.090291921249152e-06,
      "loss": 1.6722,
      "step": 703300
    },
    {
      "epoch": 55.09948300172333,
      "grad_norm": 5.606116771697998,
      "learning_rate": 4.083764165230561e-06,
      "loss": 1.6193,
      "step": 703400
    },
    {
      "epoch": 55.107316308945634,
      "grad_norm": 7.3212809562683105,
      "learning_rate": 4.077236409211969e-06,
      "loss": 1.6732,
      "step": 703500
    },
    {
      "epoch": 55.11514961616795,
      "grad_norm": 7.5079474449157715,
      "learning_rate": 4.070708653193379e-06,
      "loss": 1.5477,
      "step": 703600
    },
    {
      "epoch": 55.12298292339025,
      "grad_norm": 7.389719009399414,
      "learning_rate": 4.064180897174787e-06,
      "loss": 1.6048,
      "step": 703700
    },
    {
      "epoch": 55.13081623061257,
      "grad_norm": 10.241859436035156,
      "learning_rate": 4.0576531411561966e-06,
      "loss": 1.6567,
      "step": 703800
    },
    {
      "epoch": 55.13864953783487,
      "grad_norm": 4.899456024169922,
      "learning_rate": 4.051125385137605e-06,
      "loss": 1.5756,
      "step": 703900
    },
    {
      "epoch": 55.146482845057186,
      "grad_norm": 6.79730749130249,
      "learning_rate": 4.044597629119015e-06,
      "loss": 1.5912,
      "step": 704000
    },
    {
      "epoch": 55.15431615227949,
      "grad_norm": 6.0317840576171875,
      "learning_rate": 4.038069873100423e-06,
      "loss": 1.6984,
      "step": 704100
    },
    {
      "epoch": 55.162149459501805,
      "grad_norm": 5.604363441467285,
      "learning_rate": 4.031542117081832e-06,
      "loss": 1.5774,
      "step": 704200
    },
    {
      "epoch": 55.16998276672411,
      "grad_norm": 6.121724605560303,
      "learning_rate": 4.025014361063241e-06,
      "loss": 1.6727,
      "step": 704300
    },
    {
      "epoch": 55.17781607394642,
      "grad_norm": 6.912920951843262,
      "learning_rate": 4.018486605044651e-06,
      "loss": 1.5687,
      "step": 704400
    },
    {
      "epoch": 55.18564938116873,
      "grad_norm": 6.552274227142334,
      "learning_rate": 4.011958849026059e-06,
      "loss": 1.5914,
      "step": 704500
    },
    {
      "epoch": 55.193482688391036,
      "grad_norm": 6.915497303009033,
      "learning_rate": 4.005431093007468e-06,
      "loss": 1.6892,
      "step": 704600
    },
    {
      "epoch": 55.20131599561335,
      "grad_norm": 6.4447102546691895,
      "learning_rate": 3.9989033369888764e-06,
      "loss": 1.6571,
      "step": 704700
    },
    {
      "epoch": 55.209149302835655,
      "grad_norm": 7.042848587036133,
      "learning_rate": 3.9923755809702864e-06,
      "loss": 1.4785,
      "step": 704800
    },
    {
      "epoch": 55.21698261005797,
      "grad_norm": 6.133045673370361,
      "learning_rate": 3.985847824951695e-06,
      "loss": 1.5754,
      "step": 704900
    },
    {
      "epoch": 55.224815917280274,
      "grad_norm": 5.749240875244141,
      "learning_rate": 3.979320068933104e-06,
      "loss": 1.5987,
      "step": 705000
    },
    {
      "epoch": 55.23264922450259,
      "grad_norm": 4.177453994750977,
      "learning_rate": 3.972792312914512e-06,
      "loss": 1.5661,
      "step": 705100
    },
    {
      "epoch": 55.24048253172489,
      "grad_norm": 8.26611042022705,
      "learning_rate": 3.966264556895922e-06,
      "loss": 1.5801,
      "step": 705200
    },
    {
      "epoch": 55.24831583894721,
      "grad_norm": 8.549894332885742,
      "learning_rate": 3.9597368008773305e-06,
      "loss": 1.6075,
      "step": 705300
    },
    {
      "epoch": 55.25614914616951,
      "grad_norm": 6.6590070724487305,
      "learning_rate": 3.95320904485874e-06,
      "loss": 1.71,
      "step": 705400
    },
    {
      "epoch": 55.26398245339182,
      "grad_norm": 4.973240375518799,
      "learning_rate": 3.946681288840148e-06,
      "loss": 1.665,
      "step": 705500
    },
    {
      "epoch": 55.27181576061413,
      "grad_norm": 6.237765312194824,
      "learning_rate": 3.940153532821558e-06,
      "loss": 1.6,
      "step": 705600
    },
    {
      "epoch": 55.27964906783644,
      "grad_norm": 8.989400863647461,
      "learning_rate": 3.933625776802966e-06,
      "loss": 1.5533,
      "step": 705700
    },
    {
      "epoch": 55.28748237505875,
      "grad_norm": 6.656069278717041,
      "learning_rate": 3.9270980207843755e-06,
      "loss": 1.6318,
      "step": 705800
    },
    {
      "epoch": 55.29531568228106,
      "grad_norm": 8.75481128692627,
      "learning_rate": 3.920570264765784e-06,
      "loss": 1.5766,
      "step": 705900
    },
    {
      "epoch": 55.30314898950337,
      "grad_norm": 4.949740409851074,
      "learning_rate": 3.914042508747194e-06,
      "loss": 1.5498,
      "step": 706000
    },
    {
      "epoch": 55.310982296725676,
      "grad_norm": 7.197430610656738,
      "learning_rate": 3.907514752728602e-06,
      "loss": 1.6478,
      "step": 706100
    },
    {
      "epoch": 55.31881560394799,
      "grad_norm": 6.805253505706787,
      "learning_rate": 3.900986996710011e-06,
      "loss": 1.6453,
      "step": 706200
    },
    {
      "epoch": 55.326648911170295,
      "grad_norm": 5.498593807220459,
      "learning_rate": 3.8944592406914195e-06,
      "loss": 1.6749,
      "step": 706300
    },
    {
      "epoch": 55.33448221839261,
      "grad_norm": 6.591858863830566,
      "learning_rate": 3.8879314846728295e-06,
      "loss": 1.7093,
      "step": 706400
    },
    {
      "epoch": 55.342315525614914,
      "grad_norm": 6.149471282958984,
      "learning_rate": 3.881403728654238e-06,
      "loss": 1.6325,
      "step": 706500
    },
    {
      "epoch": 55.35014883283722,
      "grad_norm": 7.545393466949463,
      "learning_rate": 3.874875972635647e-06,
      "loss": 1.6422,
      "step": 706600
    },
    {
      "epoch": 55.35798214005953,
      "grad_norm": 5.726021766662598,
      "learning_rate": 3.868348216617055e-06,
      "loss": 1.6596,
      "step": 706700
    },
    {
      "epoch": 55.36581544728184,
      "grad_norm": 6.241180896759033,
      "learning_rate": 3.861820460598465e-06,
      "loss": 1.67,
      "step": 706800
    },
    {
      "epoch": 55.37364875450415,
      "grad_norm": 4.651535511016846,
      "learning_rate": 3.855292704579874e-06,
      "loss": 1.5204,
      "step": 706900
    },
    {
      "epoch": 55.38148206172646,
      "grad_norm": 5.722232341766357,
      "learning_rate": 3.848764948561283e-06,
      "loss": 1.6459,
      "step": 707000
    },
    {
      "epoch": 55.38931536894877,
      "grad_norm": 6.401009559631348,
      "learning_rate": 3.842237192542691e-06,
      "loss": 1.6362,
      "step": 707100
    },
    {
      "epoch": 55.39714867617108,
      "grad_norm": 8.615236282348633,
      "learning_rate": 3.835709436524101e-06,
      "loss": 1.592,
      "step": 707200
    },
    {
      "epoch": 55.40498198339339,
      "grad_norm": 6.580412864685059,
      "learning_rate": 3.829181680505509e-06,
      "loss": 1.6006,
      "step": 707300
    },
    {
      "epoch": 55.4128152906157,
      "grad_norm": 7.095913887023926,
      "learning_rate": 3.8226539244869186e-06,
      "loss": 1.672,
      "step": 707400
    },
    {
      "epoch": 55.42064859783801,
      "grad_norm": 6.78807258605957,
      "learning_rate": 3.816126168468327e-06,
      "loss": 1.6407,
      "step": 707500
    },
    {
      "epoch": 55.428481905060316,
      "grad_norm": 5.614809513092041,
      "learning_rate": 3.8095984124497365e-06,
      "loss": 1.5954,
      "step": 707600
    },
    {
      "epoch": 55.43631521228262,
      "grad_norm": 8.08957290649414,
      "learning_rate": 3.803070656431145e-06,
      "loss": 1.633,
      "step": 707700
    },
    {
      "epoch": 55.444148519504935,
      "grad_norm": 6.420277118682861,
      "learning_rate": 3.7965429004125544e-06,
      "loss": 1.7113,
      "step": 707800
    },
    {
      "epoch": 55.45198182672724,
      "grad_norm": 8.539920806884766,
      "learning_rate": 3.790015144393963e-06,
      "loss": 1.6483,
      "step": 707900
    },
    {
      "epoch": 55.459815133949554,
      "grad_norm": 9.879145622253418,
      "learning_rate": 3.7834873883753722e-06,
      "loss": 1.6416,
      "step": 708000
    },
    {
      "epoch": 55.46764844117186,
      "grad_norm": 6.404700756072998,
      "learning_rate": 3.776959632356781e-06,
      "loss": 1.5979,
      "step": 708100
    },
    {
      "epoch": 55.475481748394174,
      "grad_norm": 9.564107894897461,
      "learning_rate": 3.77043187633819e-06,
      "loss": 1.622,
      "step": 708200
    },
    {
      "epoch": 55.48331505561648,
      "grad_norm": 5.024044036865234,
      "learning_rate": 3.763904120319599e-06,
      "loss": 1.5867,
      "step": 708300
    },
    {
      "epoch": 55.49114836283879,
      "grad_norm": 7.149435043334961,
      "learning_rate": 3.757376364301008e-06,
      "loss": 1.6179,
      "step": 708400
    },
    {
      "epoch": 55.4989816700611,
      "grad_norm": 7.383481979370117,
      "learning_rate": 3.750848608282417e-06,
      "loss": 1.6248,
      "step": 708500
    },
    {
      "epoch": 55.50681497728341,
      "grad_norm": 6.433704376220703,
      "learning_rate": 3.744320852263826e-06,
      "loss": 1.6713,
      "step": 708600
    },
    {
      "epoch": 55.51464828450572,
      "grad_norm": 5.870818614959717,
      "learning_rate": 3.737793096245235e-06,
      "loss": 1.6136,
      "step": 708700
    },
    {
      "epoch": 55.52248159172803,
      "grad_norm": 3.5432565212249756,
      "learning_rate": 3.731265340226644e-06,
      "loss": 1.5931,
      "step": 708800
    },
    {
      "epoch": 55.53031489895034,
      "grad_norm": 5.17486572265625,
      "learning_rate": 3.724737584208053e-06,
      "loss": 1.6074,
      "step": 708900
    },
    {
      "epoch": 55.53814820617264,
      "grad_norm": 5.755677700042725,
      "learning_rate": 3.7182098281894617e-06,
      "loss": 1.5962,
      "step": 709000
    },
    {
      "epoch": 55.545981513394956,
      "grad_norm": 5.147977352142334,
      "learning_rate": 3.711682072170871e-06,
      "loss": 1.6046,
      "step": 709100
    },
    {
      "epoch": 55.55381482061726,
      "grad_norm": 6.895458221435547,
      "learning_rate": 3.7051543161522796e-06,
      "loss": 1.7614,
      "step": 709200
    },
    {
      "epoch": 55.561648127839575,
      "grad_norm": 6.290585517883301,
      "learning_rate": 3.6986265601336887e-06,
      "loss": 1.6539,
      "step": 709300
    },
    {
      "epoch": 55.56948143506188,
      "grad_norm": 5.711703300476074,
      "learning_rate": 3.6920988041150975e-06,
      "loss": 1.6222,
      "step": 709400
    },
    {
      "epoch": 55.577314742284194,
      "grad_norm": 5.224559783935547,
      "learning_rate": 3.6855710480965066e-06,
      "loss": 1.7017,
      "step": 709500
    },
    {
      "epoch": 55.5851480495065,
      "grad_norm": 6.144631385803223,
      "learning_rate": 3.6790432920779154e-06,
      "loss": 1.6412,
      "step": 709600
    },
    {
      "epoch": 55.592981356728814,
      "grad_norm": 4.782833099365234,
      "learning_rate": 3.6725155360593245e-06,
      "loss": 1.5833,
      "step": 709700
    },
    {
      "epoch": 55.60081466395112,
      "grad_norm": 5.389855861663818,
      "learning_rate": 3.6659877800407332e-06,
      "loss": 1.6132,
      "step": 709800
    },
    {
      "epoch": 55.60864797117343,
      "grad_norm": 5.6005659103393555,
      "learning_rate": 3.6594600240221424e-06,
      "loss": 1.5709,
      "step": 709900
    },
    {
      "epoch": 55.61648127839574,
      "grad_norm": 6.176398754119873,
      "learning_rate": 3.652932268003551e-06,
      "loss": 1.5937,
      "step": 710000
    },
    {
      "epoch": 55.624314585618045,
      "grad_norm": 7.116921424865723,
      "learning_rate": 3.6464045119849603e-06,
      "loss": 1.636,
      "step": 710100
    },
    {
      "epoch": 55.63214789284036,
      "grad_norm": 8.444345474243164,
      "learning_rate": 3.639876755966369e-06,
      "loss": 1.6643,
      "step": 710200
    },
    {
      "epoch": 55.639981200062664,
      "grad_norm": 6.056286811828613,
      "learning_rate": 3.633348999947778e-06,
      "loss": 1.6432,
      "step": 710300
    },
    {
      "epoch": 55.64781450728498,
      "grad_norm": 7.444211483001709,
      "learning_rate": 3.626821243929187e-06,
      "loss": 1.6485,
      "step": 710400
    },
    {
      "epoch": 55.65564781450728,
      "grad_norm": 9.664283752441406,
      "learning_rate": 3.620293487910596e-06,
      "loss": 1.6513,
      "step": 710500
    },
    {
      "epoch": 55.663481121729596,
      "grad_norm": 7.029632568359375,
      "learning_rate": 3.613765731892005e-06,
      "loss": 1.605,
      "step": 710600
    },
    {
      "epoch": 55.6713144289519,
      "grad_norm": 7.495524883270264,
      "learning_rate": 3.607237975873414e-06,
      "loss": 1.5595,
      "step": 710700
    },
    {
      "epoch": 55.679147736174215,
      "grad_norm": 6.666459560394287,
      "learning_rate": 3.6007102198548227e-06,
      "loss": 1.6627,
      "step": 710800
    },
    {
      "epoch": 55.68698104339652,
      "grad_norm": 5.769407749176025,
      "learning_rate": 3.594182463836232e-06,
      "loss": 1.5794,
      "step": 710900
    },
    {
      "epoch": 55.694814350618834,
      "grad_norm": 10.28681468963623,
      "learning_rate": 3.5876547078176406e-06,
      "loss": 1.673,
      "step": 711000
    },
    {
      "epoch": 55.70264765784114,
      "grad_norm": 6.064692974090576,
      "learning_rate": 3.5811269517990497e-06,
      "loss": 1.5372,
      "step": 711100
    },
    {
      "epoch": 55.71048096506345,
      "grad_norm": 5.328596115112305,
      "learning_rate": 3.5745991957804585e-06,
      "loss": 1.5939,
      "step": 711200
    },
    {
      "epoch": 55.71831427228576,
      "grad_norm": 8.836580276489258,
      "learning_rate": 3.5680714397618676e-06,
      "loss": 1.5776,
      "step": 711300
    },
    {
      "epoch": 55.726147579508066,
      "grad_norm": 5.349210262298584,
      "learning_rate": 3.5615436837432764e-06,
      "loss": 1.7362,
      "step": 711400
    },
    {
      "epoch": 55.73398088673038,
      "grad_norm": 6.366190433502197,
      "learning_rate": 3.5550159277246855e-06,
      "loss": 1.6257,
      "step": 711500
    },
    {
      "epoch": 55.741814193952685,
      "grad_norm": 7.345862865447998,
      "learning_rate": 3.5484881717060943e-06,
      "loss": 1.6553,
      "step": 711600
    },
    {
      "epoch": 55.749647501175,
      "grad_norm": 4.950740814208984,
      "learning_rate": 3.5419604156875034e-06,
      "loss": 1.5699,
      "step": 711700
    },
    {
      "epoch": 55.757480808397304,
      "grad_norm": 7.552945613861084,
      "learning_rate": 3.535432659668912e-06,
      "loss": 1.5789,
      "step": 711800
    },
    {
      "epoch": 55.76531411561962,
      "grad_norm": 6.197149753570557,
      "learning_rate": 3.5289049036503213e-06,
      "loss": 1.6865,
      "step": 711900
    },
    {
      "epoch": 55.77314742284192,
      "grad_norm": 5.771961688995361,
      "learning_rate": 3.52237714763173e-06,
      "loss": 1.6141,
      "step": 712000
    },
    {
      "epoch": 55.780980730064236,
      "grad_norm": 7.141673564910889,
      "learning_rate": 3.515849391613139e-06,
      "loss": 1.6133,
      "step": 712100
    },
    {
      "epoch": 55.78881403728654,
      "grad_norm": 5.587407112121582,
      "learning_rate": 3.509321635594548e-06,
      "loss": 1.6302,
      "step": 712200
    },
    {
      "epoch": 55.79664734450885,
      "grad_norm": 7.604913234710693,
      "learning_rate": 3.502793879575957e-06,
      "loss": 1.6408,
      "step": 712300
    },
    {
      "epoch": 55.80448065173116,
      "grad_norm": 4.587332248687744,
      "learning_rate": 3.496266123557366e-06,
      "loss": 1.5561,
      "step": 712400
    },
    {
      "epoch": 55.81231395895347,
      "grad_norm": 6.478882312774658,
      "learning_rate": 3.489738367538775e-06,
      "loss": 1.6791,
      "step": 712500
    },
    {
      "epoch": 55.82014726617578,
      "grad_norm": 7.419135570526123,
      "learning_rate": 3.4832106115201837e-06,
      "loss": 1.7034,
      "step": 712600
    },
    {
      "epoch": 55.82798057339809,
      "grad_norm": 5.865118503570557,
      "learning_rate": 3.476682855501593e-06,
      "loss": 1.6802,
      "step": 712700
    },
    {
      "epoch": 55.8358138806204,
      "grad_norm": 7.272729396820068,
      "learning_rate": 3.4701550994830016e-06,
      "loss": 1.5632,
      "step": 712800
    },
    {
      "epoch": 55.843647187842706,
      "grad_norm": 7.548064231872559,
      "learning_rate": 3.4636273434644107e-06,
      "loss": 1.7122,
      "step": 712900
    },
    {
      "epoch": 55.85148049506502,
      "grad_norm": 6.385806560516357,
      "learning_rate": 3.4570995874458195e-06,
      "loss": 1.6351,
      "step": 713000
    },
    {
      "epoch": 55.859313802287325,
      "grad_norm": 5.509685039520264,
      "learning_rate": 3.4505718314272286e-06,
      "loss": 1.5911,
      "step": 713100
    },
    {
      "epoch": 55.86714710950964,
      "grad_norm": 6.9323039054870605,
      "learning_rate": 3.4440440754086374e-06,
      "loss": 1.5743,
      "step": 713200
    },
    {
      "epoch": 55.874980416731944,
      "grad_norm": 6.581846237182617,
      "learning_rate": 3.4375163193900465e-06,
      "loss": 1.6567,
      "step": 713300
    },
    {
      "epoch": 55.88281372395426,
      "grad_norm": 7.422293186187744,
      "learning_rate": 3.4309885633714553e-06,
      "loss": 1.5961,
      "step": 713400
    },
    {
      "epoch": 55.89064703117656,
      "grad_norm": 6.8460869789123535,
      "learning_rate": 3.4244608073528644e-06,
      "loss": 1.6276,
      "step": 713500
    },
    {
      "epoch": 55.89848033839887,
      "grad_norm": 7.681219100952148,
      "learning_rate": 3.417933051334273e-06,
      "loss": 1.6937,
      "step": 713600
    },
    {
      "epoch": 55.90631364562118,
      "grad_norm": 5.7399444580078125,
      "learning_rate": 3.4114052953156823e-06,
      "loss": 1.6267,
      "step": 713700
    },
    {
      "epoch": 55.91414695284349,
      "grad_norm": 9.159367561340332,
      "learning_rate": 3.404877539297091e-06,
      "loss": 1.5758,
      "step": 713800
    },
    {
      "epoch": 55.9219802600658,
      "grad_norm": 5.444657802581787,
      "learning_rate": 3.3983497832785e-06,
      "loss": 1.559,
      "step": 713900
    },
    {
      "epoch": 55.92981356728811,
      "grad_norm": 5.584181308746338,
      "learning_rate": 3.391822027259909e-06,
      "loss": 1.6598,
      "step": 714000
    },
    {
      "epoch": 55.93764687451042,
      "grad_norm": 7.734248161315918,
      "learning_rate": 3.385294271241318e-06,
      "loss": 1.5797,
      "step": 714100
    },
    {
      "epoch": 55.94548018173273,
      "grad_norm": 7.681448459625244,
      "learning_rate": 3.378766515222727e-06,
      "loss": 1.666,
      "step": 714200
    },
    {
      "epoch": 55.95331348895504,
      "grad_norm": 7.568289279937744,
      "learning_rate": 3.372238759204136e-06,
      "loss": 1.5408,
      "step": 714300
    },
    {
      "epoch": 55.961146796177346,
      "grad_norm": 5.827922821044922,
      "learning_rate": 3.3657110031855456e-06,
      "loss": 1.616,
      "step": 714400
    },
    {
      "epoch": 55.96898010339966,
      "grad_norm": 6.044751167297363,
      "learning_rate": 3.359183247166954e-06,
      "loss": 1.604,
      "step": 714500
    },
    {
      "epoch": 55.976813410621965,
      "grad_norm": 7.27043342590332,
      "learning_rate": 3.3526554911483634e-06,
      "loss": 1.6829,
      "step": 714600
    },
    {
      "epoch": 55.98464671784427,
      "grad_norm": 6.182494163513184,
      "learning_rate": 3.3461277351297717e-06,
      "loss": 1.6162,
      "step": 714700
    },
    {
      "epoch": 55.992480025066584,
      "grad_norm": 6.273636341094971,
      "learning_rate": 3.3395999791111813e-06,
      "loss": 1.5553,
      "step": 714800
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.7675507068634033,
      "eval_runtime": 1.4962,
      "eval_samples_per_second": 449.139,
      "eval_steps_per_second": 449.139,
      "step": 714896
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.3741308450698853,
      "eval_runtime": 28.6308,
      "eval_samples_per_second": 445.883,
      "eval_steps_per_second": 445.883,
      "step": 714896
    },
    {
      "epoch": 56.00031333228889,
      "grad_norm": 7.192511558532715,
      "learning_rate": 3.3330722230925896e-06,
      "loss": 1.5672,
      "step": 714900
    },
    {
      "epoch": 56.0081466395112,
      "grad_norm": 5.283788681030273,
      "learning_rate": 3.3265444670739992e-06,
      "loss": 1.6266,
      "step": 715000
    },
    {
      "epoch": 56.01597994673351,
      "grad_norm": 6.604536056518555,
      "learning_rate": 3.3200167110554075e-06,
      "loss": 1.6758,
      "step": 715100
    },
    {
      "epoch": 56.02381325395582,
      "grad_norm": 6.654611587524414,
      "learning_rate": 3.313488955036817e-06,
      "loss": 1.6108,
      "step": 715200
    },
    {
      "epoch": 56.03164656117813,
      "grad_norm": 7.044693470001221,
      "learning_rate": 3.3069611990182254e-06,
      "loss": 1.5656,
      "step": 715300
    },
    {
      "epoch": 56.03947986840044,
      "grad_norm": 2.6038734912872314,
      "learning_rate": 3.300433442999635e-06,
      "loss": 1.5922,
      "step": 715400
    },
    {
      "epoch": 56.04731317562275,
      "grad_norm": 7.335946559906006,
      "learning_rate": 3.2939056869810433e-06,
      "loss": 1.6745,
      "step": 715500
    },
    {
      "epoch": 56.05514648284506,
      "grad_norm": 6.503625869750977,
      "learning_rate": 3.287377930962453e-06,
      "loss": 1.6171,
      "step": 715600
    },
    {
      "epoch": 56.06297979006737,
      "grad_norm": 6.958547592163086,
      "learning_rate": 3.280850174943861e-06,
      "loss": 1.7507,
      "step": 715700
    },
    {
      "epoch": 56.07081309728967,
      "grad_norm": 7.0164289474487305,
      "learning_rate": 3.2743224189252708e-06,
      "loss": 1.64,
      "step": 715800
    },
    {
      "epoch": 56.078646404511986,
      "grad_norm": 5.536063194274902,
      "learning_rate": 3.267794662906679e-06,
      "loss": 1.6683,
      "step": 715900
    },
    {
      "epoch": 56.08647971173429,
      "grad_norm": 7.708371639251709,
      "learning_rate": 3.2612669068880887e-06,
      "loss": 1.6001,
      "step": 716000
    },
    {
      "epoch": 56.094313018956605,
      "grad_norm": 5.9977850914001465,
      "learning_rate": 3.254739150869497e-06,
      "loss": 1.6752,
      "step": 716100
    },
    {
      "epoch": 56.10214632617891,
      "grad_norm": 6.623913764953613,
      "learning_rate": 3.2482113948509066e-06,
      "loss": 1.5801,
      "step": 716200
    },
    {
      "epoch": 56.109979633401224,
      "grad_norm": 8.036206245422363,
      "learning_rate": 3.241683638832315e-06,
      "loss": 1.548,
      "step": 716300
    },
    {
      "epoch": 56.11781294062353,
      "grad_norm": 6.742313861846924,
      "learning_rate": 3.2351558828137244e-06,
      "loss": 1.6279,
      "step": 716400
    },
    {
      "epoch": 56.12564624784584,
      "grad_norm": 6.187352180480957,
      "learning_rate": 3.2286281267951328e-06,
      "loss": 1.6983,
      "step": 716500
    },
    {
      "epoch": 56.13347955506815,
      "grad_norm": 6.0311431884765625,
      "learning_rate": 3.2221003707765423e-06,
      "loss": 1.6351,
      "step": 716600
    },
    {
      "epoch": 56.14131286229046,
      "grad_norm": 5.62916898727417,
      "learning_rate": 3.2155726147579506e-06,
      "loss": 1.6246,
      "step": 716700
    },
    {
      "epoch": 56.14914616951277,
      "grad_norm": 6.904988765716553,
      "learning_rate": 3.2090448587393602e-06,
      "loss": 1.6008,
      "step": 716800
    },
    {
      "epoch": 56.156979476735074,
      "grad_norm": 6.256000518798828,
      "learning_rate": 3.2025171027207685e-06,
      "loss": 1.4975,
      "step": 716900
    },
    {
      "epoch": 56.16481278395739,
      "grad_norm": 9.27313232421875,
      "learning_rate": 3.195989346702178e-06,
      "loss": 1.6238,
      "step": 717000
    },
    {
      "epoch": 56.17264609117969,
      "grad_norm": 6.184628009796143,
      "learning_rate": 3.1894615906835864e-06,
      "loss": 1.5196,
      "step": 717100
    },
    {
      "epoch": 56.18047939840201,
      "grad_norm": 7.281494617462158,
      "learning_rate": 3.182933834664996e-06,
      "loss": 1.6074,
      "step": 717200
    },
    {
      "epoch": 56.18831270562431,
      "grad_norm": 5.586573123931885,
      "learning_rate": 3.1764060786464043e-06,
      "loss": 1.6124,
      "step": 717300
    },
    {
      "epoch": 56.196146012846626,
      "grad_norm": 5.886543273925781,
      "learning_rate": 3.169878322627814e-06,
      "loss": 1.5083,
      "step": 717400
    },
    {
      "epoch": 56.20397932006893,
      "grad_norm": 6.935873508453369,
      "learning_rate": 3.163350566609222e-06,
      "loss": 1.6455,
      "step": 717500
    },
    {
      "epoch": 56.211812627291245,
      "grad_norm": 8.02399730682373,
      "learning_rate": 3.1568228105906318e-06,
      "loss": 1.5766,
      "step": 717600
    },
    {
      "epoch": 56.21964593451355,
      "grad_norm": 7.044065952301025,
      "learning_rate": 3.15029505457204e-06,
      "loss": 1.5625,
      "step": 717700
    },
    {
      "epoch": 56.227479241735864,
      "grad_norm": 5.859601974487305,
      "learning_rate": 3.1437672985534497e-06,
      "loss": 1.6372,
      "step": 717800
    },
    {
      "epoch": 56.23531254895817,
      "grad_norm": 8.5557279586792,
      "learning_rate": 3.137239542534858e-06,
      "loss": 1.5434,
      "step": 717900
    },
    {
      "epoch": 56.243145856180476,
      "grad_norm": 6.6094136238098145,
      "learning_rate": 3.1307117865162676e-06,
      "loss": 1.6393,
      "step": 718000
    },
    {
      "epoch": 56.25097916340279,
      "grad_norm": 4.543811321258545,
      "learning_rate": 3.1241840304976763e-06,
      "loss": 1.6312,
      "step": 718100
    },
    {
      "epoch": 56.258812470625095,
      "grad_norm": 5.424569129943848,
      "learning_rate": 3.1176562744790855e-06,
      "loss": 1.5746,
      "step": 718200
    },
    {
      "epoch": 56.26664577784741,
      "grad_norm": 4.48407506942749,
      "learning_rate": 3.111128518460494e-06,
      "loss": 1.5262,
      "step": 718300
    },
    {
      "epoch": 56.274479085069714,
      "grad_norm": 6.724302291870117,
      "learning_rate": 3.1046007624419033e-06,
      "loss": 1.6497,
      "step": 718400
    },
    {
      "epoch": 56.28231239229203,
      "grad_norm": 5.847272872924805,
      "learning_rate": 3.098073006423312e-06,
      "loss": 1.6169,
      "step": 718500
    },
    {
      "epoch": 56.29014569951433,
      "grad_norm": 7.024511814117432,
      "learning_rate": 3.0915452504047212e-06,
      "loss": 1.5752,
      "step": 718600
    },
    {
      "epoch": 56.29797900673665,
      "grad_norm": 6.640041828155518,
      "learning_rate": 3.08501749438613e-06,
      "loss": 1.6137,
      "step": 718700
    },
    {
      "epoch": 56.30581231395895,
      "grad_norm": 6.797670841217041,
      "learning_rate": 3.078489738367539e-06,
      "loss": 1.6534,
      "step": 718800
    },
    {
      "epoch": 56.313645621181266,
      "grad_norm": 6.9449849128723145,
      "learning_rate": 3.071961982348948e-06,
      "loss": 1.7043,
      "step": 718900
    },
    {
      "epoch": 56.32147892840357,
      "grad_norm": 8.265981674194336,
      "learning_rate": 3.065434226330357e-06,
      "loss": 1.5971,
      "step": 719000
    },
    {
      "epoch": 56.32931223562588,
      "grad_norm": 7.366413593292236,
      "learning_rate": 3.0589064703117657e-06,
      "loss": 1.6243,
      "step": 719100
    },
    {
      "epoch": 56.33714554284819,
      "grad_norm": 8.389105796813965,
      "learning_rate": 3.052378714293175e-06,
      "loss": 1.6264,
      "step": 719200
    },
    {
      "epoch": 56.3449788500705,
      "grad_norm": 7.939642906188965,
      "learning_rate": 3.0458509582745836e-06,
      "loss": 1.5966,
      "step": 719300
    },
    {
      "epoch": 56.35281215729281,
      "grad_norm": 7.730679988861084,
      "learning_rate": 3.0393232022559928e-06,
      "loss": 1.716,
      "step": 719400
    },
    {
      "epoch": 56.360645464515116,
      "grad_norm": 6.306567192077637,
      "learning_rate": 3.0327954462374015e-06,
      "loss": 1.6648,
      "step": 719500
    },
    {
      "epoch": 56.36847877173743,
      "grad_norm": 6.281878471374512,
      "learning_rate": 3.0262676902188107e-06,
      "loss": 1.5682,
      "step": 719600
    },
    {
      "epoch": 56.376312078959735,
      "grad_norm": 5.922872543334961,
      "learning_rate": 3.0197399342002194e-06,
      "loss": 1.6877,
      "step": 719700
    },
    {
      "epoch": 56.38414538618205,
      "grad_norm": 8.437997817993164,
      "learning_rate": 3.0132121781816286e-06,
      "loss": 1.6524,
      "step": 719800
    },
    {
      "epoch": 56.391978693404354,
      "grad_norm": 10.54786205291748,
      "learning_rate": 3.0066844221630373e-06,
      "loss": 1.6667,
      "step": 719900
    },
    {
      "epoch": 56.39981200062667,
      "grad_norm": 6.0586395263671875,
      "learning_rate": 3.0001566661444465e-06,
      "loss": 1.625,
      "step": 720000
    },
    {
      "epoch": 56.40764530784897,
      "grad_norm": 5.25422477722168,
      "learning_rate": 2.993628910125855e-06,
      "loss": 1.6756,
      "step": 720100
    },
    {
      "epoch": 56.41547861507129,
      "grad_norm": 6.104376316070557,
      "learning_rate": 2.9871011541072643e-06,
      "loss": 1.5412,
      "step": 720200
    },
    {
      "epoch": 56.42331192229359,
      "grad_norm": 7.366373062133789,
      "learning_rate": 2.980573398088673e-06,
      "loss": 1.72,
      "step": 720300
    },
    {
      "epoch": 56.4311452295159,
      "grad_norm": 7.119471073150635,
      "learning_rate": 2.9740456420700822e-06,
      "loss": 1.6293,
      "step": 720400
    },
    {
      "epoch": 56.43897853673821,
      "grad_norm": 6.630114555358887,
      "learning_rate": 2.967517886051491e-06,
      "loss": 1.6002,
      "step": 720500
    },
    {
      "epoch": 56.44681184396052,
      "grad_norm": 4.895916938781738,
      "learning_rate": 2.9609901300329e-06,
      "loss": 1.6896,
      "step": 720600
    },
    {
      "epoch": 56.45464515118283,
      "grad_norm": 7.696274280548096,
      "learning_rate": 2.954462374014309e-06,
      "loss": 1.6259,
      "step": 720700
    },
    {
      "epoch": 56.46247845840514,
      "grad_norm": 8.20069408416748,
      "learning_rate": 2.947934617995718e-06,
      "loss": 1.6205,
      "step": 720800
    },
    {
      "epoch": 56.47031176562745,
      "grad_norm": 6.845333576202393,
      "learning_rate": 2.9414068619771267e-06,
      "loss": 1.5999,
      "step": 720900
    },
    {
      "epoch": 56.478145072849756,
      "grad_norm": 5.858299255371094,
      "learning_rate": 2.934879105958536e-06,
      "loss": 1.6723,
      "step": 721000
    },
    {
      "epoch": 56.48597838007207,
      "grad_norm": 6.4337053298950195,
      "learning_rate": 2.9283513499399446e-06,
      "loss": 1.64,
      "step": 721100
    },
    {
      "epoch": 56.493811687294375,
      "grad_norm": 5.785645961761475,
      "learning_rate": 2.921823593921354e-06,
      "loss": 1.6165,
      "step": 721200
    },
    {
      "epoch": 56.50164499451669,
      "grad_norm": 5.995100021362305,
      "learning_rate": 2.9152958379027625e-06,
      "loss": 1.6573,
      "step": 721300
    },
    {
      "epoch": 56.509478301738994,
      "grad_norm": 5.850860118865967,
      "learning_rate": 2.9087680818841717e-06,
      "loss": 1.5111,
      "step": 721400
    },
    {
      "epoch": 56.5173116089613,
      "grad_norm": 5.521605968475342,
      "learning_rate": 2.9022403258655804e-06,
      "loss": 1.6211,
      "step": 721500
    },
    {
      "epoch": 56.52514491618361,
      "grad_norm": 6.97835636138916,
      "learning_rate": 2.8957125698469896e-06,
      "loss": 1.6069,
      "step": 721600
    },
    {
      "epoch": 56.53297822340592,
      "grad_norm": 5.375214099884033,
      "learning_rate": 2.8891848138283983e-06,
      "loss": 1.6212,
      "step": 721700
    },
    {
      "epoch": 56.54081153062823,
      "grad_norm": 6.0350565910339355,
      "learning_rate": 2.8826570578098075e-06,
      "loss": 1.6642,
      "step": 721800
    },
    {
      "epoch": 56.54864483785054,
      "grad_norm": 6.968533039093018,
      "learning_rate": 2.876129301791216e-06,
      "loss": 1.5763,
      "step": 721900
    },
    {
      "epoch": 56.55647814507285,
      "grad_norm": 6.690824031829834,
      "learning_rate": 2.8696015457726254e-06,
      "loss": 1.5434,
      "step": 722000
    },
    {
      "epoch": 56.56431145229516,
      "grad_norm": 6.879204273223877,
      "learning_rate": 2.863073789754034e-06,
      "loss": 1.5957,
      "step": 722100
    },
    {
      "epoch": 56.57214475951747,
      "grad_norm": 6.0020833015441895,
      "learning_rate": 2.8565460337354432e-06,
      "loss": 1.5564,
      "step": 722200
    },
    {
      "epoch": 56.57997806673978,
      "grad_norm": 5.2665276527404785,
      "learning_rate": 2.850018277716852e-06,
      "loss": 1.6144,
      "step": 722300
    },
    {
      "epoch": 56.58781137396209,
      "grad_norm": 4.909765243530273,
      "learning_rate": 2.843490521698261e-06,
      "loss": 1.6353,
      "step": 722400
    },
    {
      "epoch": 56.595644681184396,
      "grad_norm": 4.334984302520752,
      "learning_rate": 2.83696276567967e-06,
      "loss": 1.6046,
      "step": 722500
    },
    {
      "epoch": 56.6034779884067,
      "grad_norm": 7.2815423011779785,
      "learning_rate": 2.830435009661079e-06,
      "loss": 1.6124,
      "step": 722600
    },
    {
      "epoch": 56.611311295629015,
      "grad_norm": 5.070094108581543,
      "learning_rate": 2.8239072536424878e-06,
      "loss": 1.5737,
      "step": 722700
    },
    {
      "epoch": 56.61914460285132,
      "grad_norm": 5.220386505126953,
      "learning_rate": 2.817379497623897e-06,
      "loss": 1.615,
      "step": 722800
    },
    {
      "epoch": 56.626977910073634,
      "grad_norm": 7.925285816192627,
      "learning_rate": 2.8108517416053056e-06,
      "loss": 1.588,
      "step": 722900
    },
    {
      "epoch": 56.63481121729594,
      "grad_norm": 4.271951675415039,
      "learning_rate": 2.804323985586715e-06,
      "loss": 1.5566,
      "step": 723000
    },
    {
      "epoch": 56.64264452451825,
      "grad_norm": 7.372254371643066,
      "learning_rate": 2.7977962295681235e-06,
      "loss": 1.7093,
      "step": 723100
    },
    {
      "epoch": 56.65047783174056,
      "grad_norm": 5.537198066711426,
      "learning_rate": 2.7912684735495327e-06,
      "loss": 1.5957,
      "step": 723200
    },
    {
      "epoch": 56.65831113896287,
      "grad_norm": 6.33364200592041,
      "learning_rate": 2.7847407175309414e-06,
      "loss": 1.5035,
      "step": 723300
    },
    {
      "epoch": 56.66614444618518,
      "grad_norm": 7.378651142120361,
      "learning_rate": 2.778212961512351e-06,
      "loss": 1.5862,
      "step": 723400
    },
    {
      "epoch": 56.67397775340749,
      "grad_norm": 4.484988689422607,
      "learning_rate": 2.7716852054937597e-06,
      "loss": 1.721,
      "step": 723500
    },
    {
      "epoch": 56.6818110606298,
      "grad_norm": 5.162400722503662,
      "learning_rate": 2.765157449475169e-06,
      "loss": 1.5157,
      "step": 723600
    },
    {
      "epoch": 56.689644367852104,
      "grad_norm": 5.701492786407471,
      "learning_rate": 2.7586296934565776e-06,
      "loss": 1.6968,
      "step": 723700
    },
    {
      "epoch": 56.69747767507442,
      "grad_norm": 5.4696736335754395,
      "learning_rate": 2.7521019374379868e-06,
      "loss": 1.6881,
      "step": 723800
    },
    {
      "epoch": 56.70531098229672,
      "grad_norm": 6.958911895751953,
      "learning_rate": 2.7455741814193955e-06,
      "loss": 1.6472,
      "step": 723900
    },
    {
      "epoch": 56.713144289519036,
      "grad_norm": 6.083865642547607,
      "learning_rate": 2.7390464254008047e-06,
      "loss": 1.6585,
      "step": 724000
    },
    {
      "epoch": 56.72097759674134,
      "grad_norm": 7.842000961303711,
      "learning_rate": 2.7325186693822134e-06,
      "loss": 1.6082,
      "step": 724100
    },
    {
      "epoch": 56.728810903963655,
      "grad_norm": 4.777082443237305,
      "learning_rate": 2.7259909133636226e-06,
      "loss": 1.6607,
      "step": 724200
    },
    {
      "epoch": 56.73664421118596,
      "grad_norm": 5.600125789642334,
      "learning_rate": 2.7194631573450313e-06,
      "loss": 1.681,
      "step": 724300
    },
    {
      "epoch": 56.744477518408274,
      "grad_norm": 6.381261825561523,
      "learning_rate": 2.7129354013264404e-06,
      "loss": 1.6324,
      "step": 724400
    },
    {
      "epoch": 56.75231082563058,
      "grad_norm": 7.156070232391357,
      "learning_rate": 2.706407645307849e-06,
      "loss": 1.5848,
      "step": 724500
    },
    {
      "epoch": 56.76014413285289,
      "grad_norm": 4.692709922790527,
      "learning_rate": 2.6998798892892583e-06,
      "loss": 1.6242,
      "step": 724600
    },
    {
      "epoch": 56.7679774400752,
      "grad_norm": 7.245505332946777,
      "learning_rate": 2.693352133270667e-06,
      "loss": 1.6527,
      "step": 724700
    },
    {
      "epoch": 56.775810747297506,
      "grad_norm": 7.408173561096191,
      "learning_rate": 2.6868243772520762e-06,
      "loss": 1.7127,
      "step": 724800
    },
    {
      "epoch": 56.78364405451982,
      "grad_norm": 7.359300136566162,
      "learning_rate": 2.680296621233485e-06,
      "loss": 1.5247,
      "step": 724900
    },
    {
      "epoch": 56.791477361742125,
      "grad_norm": 6.582464694976807,
      "learning_rate": 2.673768865214894e-06,
      "loss": 1.6407,
      "step": 725000
    },
    {
      "epoch": 56.79931066896444,
      "grad_norm": 6.889965534210205,
      "learning_rate": 2.667241109196303e-06,
      "loss": 1.6605,
      "step": 725100
    },
    {
      "epoch": 56.807143976186744,
      "grad_norm": 4.820858001708984,
      "learning_rate": 2.660713353177712e-06,
      "loss": 1.6206,
      "step": 725200
    },
    {
      "epoch": 56.81497728340906,
      "grad_norm": 6.098851203918457,
      "learning_rate": 2.6541855971591207e-06,
      "loss": 1.5794,
      "step": 725300
    },
    {
      "epoch": 56.82281059063136,
      "grad_norm": 6.911299228668213,
      "learning_rate": 2.64765784114053e-06,
      "loss": 1.5586,
      "step": 725400
    },
    {
      "epoch": 56.830643897853676,
      "grad_norm": 6.8669514656066895,
      "learning_rate": 2.6411300851219386e-06,
      "loss": 1.5673,
      "step": 725500
    },
    {
      "epoch": 56.83847720507598,
      "grad_norm": 4.955923080444336,
      "learning_rate": 2.6346023291033478e-06,
      "loss": 1.6181,
      "step": 725600
    },
    {
      "epoch": 56.846310512298295,
      "grad_norm": 8.399060249328613,
      "learning_rate": 2.6280745730847565e-06,
      "loss": 1.4903,
      "step": 725700
    },
    {
      "epoch": 56.8541438195206,
      "grad_norm": 8.562704086303711,
      "learning_rate": 2.6215468170661657e-06,
      "loss": 1.6076,
      "step": 725800
    },
    {
      "epoch": 56.861977126742914,
      "grad_norm": 6.482454299926758,
      "learning_rate": 2.6150190610475744e-06,
      "loss": 1.5983,
      "step": 725900
    },
    {
      "epoch": 56.86981043396522,
      "grad_norm": 6.663023948669434,
      "learning_rate": 2.6084913050289836e-06,
      "loss": 1.6857,
      "step": 726000
    },
    {
      "epoch": 56.877643741187526,
      "grad_norm": 7.6800055503845215,
      "learning_rate": 2.6019635490103923e-06,
      "loss": 1.5608,
      "step": 726100
    },
    {
      "epoch": 56.88547704840984,
      "grad_norm": 6.187716007232666,
      "learning_rate": 2.5954357929918015e-06,
      "loss": 1.6134,
      "step": 726200
    },
    {
      "epoch": 56.893310355632146,
      "grad_norm": 7.169971942901611,
      "learning_rate": 2.58890803697321e-06,
      "loss": 1.6373,
      "step": 726300
    },
    {
      "epoch": 56.90114366285446,
      "grad_norm": 6.021287441253662,
      "learning_rate": 2.5823802809546193e-06,
      "loss": 1.5736,
      "step": 726400
    },
    {
      "epoch": 56.908976970076765,
      "grad_norm": 5.653595924377441,
      "learning_rate": 2.575852524936028e-06,
      "loss": 1.5637,
      "step": 726500
    },
    {
      "epoch": 56.91681027729908,
      "grad_norm": 7.016089916229248,
      "learning_rate": 2.5693247689174372e-06,
      "loss": 1.7081,
      "step": 726600
    },
    {
      "epoch": 56.924643584521384,
      "grad_norm": 6.113719463348389,
      "learning_rate": 2.562797012898846e-06,
      "loss": 1.6763,
      "step": 726700
    },
    {
      "epoch": 56.9324768917437,
      "grad_norm": 6.547519207000732,
      "learning_rate": 2.556269256880255e-06,
      "loss": 1.6592,
      "step": 726800
    },
    {
      "epoch": 56.940310198966,
      "grad_norm": 3.643388032913208,
      "learning_rate": 2.549741500861664e-06,
      "loss": 1.6353,
      "step": 726900
    },
    {
      "epoch": 56.948143506188316,
      "grad_norm": 6.708259582519531,
      "learning_rate": 2.543213744843073e-06,
      "loss": 1.6833,
      "step": 727000
    },
    {
      "epoch": 56.95597681341062,
      "grad_norm": 5.8575334548950195,
      "learning_rate": 2.5366859888244817e-06,
      "loss": 1.6036,
      "step": 727100
    },
    {
      "epoch": 56.96381012063293,
      "grad_norm": 7.907059192657471,
      "learning_rate": 2.530158232805891e-06,
      "loss": 1.527,
      "step": 727200
    },
    {
      "epoch": 56.97164342785524,
      "grad_norm": 5.557788848876953,
      "learning_rate": 2.5236304767872996e-06,
      "loss": 1.647,
      "step": 727300
    },
    {
      "epoch": 56.97947673507755,
      "grad_norm": 5.368568420410156,
      "learning_rate": 2.517102720768709e-06,
      "loss": 1.6523,
      "step": 727400
    },
    {
      "epoch": 56.98731004229986,
      "grad_norm": 5.618967056274414,
      "learning_rate": 2.5105749647501175e-06,
      "loss": 1.6448,
      "step": 727500
    },
    {
      "epoch": 56.995143349522166,
      "grad_norm": 6.660874843597412,
      "learning_rate": 2.5040472087315267e-06,
      "loss": 1.5938,
      "step": 727600
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.769554853439331,
      "eval_runtime": 1.5149,
      "eval_samples_per_second": 443.605,
      "eval_steps_per_second": 443.605,
      "step": 727662
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.3744968175888062,
      "eval_runtime": 28.6762,
      "eval_samples_per_second": 445.178,
      "eval_steps_per_second": 445.178,
      "step": 727662
    },
    {
      "epoch": 57.00297665674448,
      "grad_norm": 5.136965274810791,
      "learning_rate": 2.4975194527129354e-06,
      "loss": 1.662,
      "step": 727700
    },
    {
      "epoch": 57.010809963966786,
      "grad_norm": 6.944543361663818,
      "learning_rate": 2.4909916966943446e-06,
      "loss": 1.5909,
      "step": 727800
    },
    {
      "epoch": 57.0186432711891,
      "grad_norm": 8.721120834350586,
      "learning_rate": 2.4844639406757533e-06,
      "loss": 1.6117,
      "step": 727900
    },
    {
      "epoch": 57.026476578411405,
      "grad_norm": 9.906715393066406,
      "learning_rate": 2.4779361846571625e-06,
      "loss": 1.6495,
      "step": 728000
    },
    {
      "epoch": 57.03430988563372,
      "grad_norm": 3.1621673107147217,
      "learning_rate": 2.471408428638571e-06,
      "loss": 1.5668,
      "step": 728100
    },
    {
      "epoch": 57.042143192856024,
      "grad_norm": 6.949521064758301,
      "learning_rate": 2.4648806726199803e-06,
      "loss": 1.6227,
      "step": 728200
    },
    {
      "epoch": 57.04997650007833,
      "grad_norm": 6.270012378692627,
      "learning_rate": 2.458352916601389e-06,
      "loss": 1.6922,
      "step": 728300
    },
    {
      "epoch": 57.05780980730064,
      "grad_norm": 6.385204315185547,
      "learning_rate": 2.4518251605827982e-06,
      "loss": 1.5754,
      "step": 728400
    },
    {
      "epoch": 57.06564311452295,
      "grad_norm": 7.0231523513793945,
      "learning_rate": 2.445297404564207e-06,
      "loss": 1.7055,
      "step": 728500
    },
    {
      "epoch": 57.07347642174526,
      "grad_norm": 7.381348609924316,
      "learning_rate": 2.438769648545616e-06,
      "loss": 1.5646,
      "step": 728600
    },
    {
      "epoch": 57.08130972896757,
      "grad_norm": 13.804730415344238,
      "learning_rate": 2.432241892527025e-06,
      "loss": 1.6265,
      "step": 728700
    },
    {
      "epoch": 57.08914303618988,
      "grad_norm": 4.758538722991943,
      "learning_rate": 2.425714136508434e-06,
      "loss": 1.7257,
      "step": 728800
    },
    {
      "epoch": 57.09697634341219,
      "grad_norm": 6.26002836227417,
      "learning_rate": 2.4191863804898427e-06,
      "loss": 1.5768,
      "step": 728900
    },
    {
      "epoch": 57.1048096506345,
      "grad_norm": 5.600465774536133,
      "learning_rate": 2.412658624471252e-06,
      "loss": 1.6269,
      "step": 729000
    },
    {
      "epoch": 57.112642957856806,
      "grad_norm": 4.823348045349121,
      "learning_rate": 2.4061308684526606e-06,
      "loss": 1.6646,
      "step": 729100
    },
    {
      "epoch": 57.12047626507912,
      "grad_norm": 7.719300270080566,
      "learning_rate": 2.39960311243407e-06,
      "loss": 1.7185,
      "step": 729200
    },
    {
      "epoch": 57.128309572301426,
      "grad_norm": 7.13306188583374,
      "learning_rate": 2.3930753564154785e-06,
      "loss": 1.5753,
      "step": 729300
    },
    {
      "epoch": 57.13614287952373,
      "grad_norm": 5.5393195152282715,
      "learning_rate": 2.3865476003968877e-06,
      "loss": 1.5837,
      "step": 729400
    },
    {
      "epoch": 57.143976186746045,
      "grad_norm": 2.5874087810516357,
      "learning_rate": 2.380019844378297e-06,
      "loss": 1.6045,
      "step": 729500
    },
    {
      "epoch": 57.15180949396835,
      "grad_norm": 6.470973491668701,
      "learning_rate": 2.3734920883597056e-06,
      "loss": 1.6667,
      "step": 729600
    },
    {
      "epoch": 57.159642801190664,
      "grad_norm": 6.909914970397949,
      "learning_rate": 2.3669643323411147e-06,
      "loss": 1.5416,
      "step": 729700
    },
    {
      "epoch": 57.16747610841297,
      "grad_norm": 6.5208282470703125,
      "learning_rate": 2.3604365763225235e-06,
      "loss": 1.6542,
      "step": 729800
    },
    {
      "epoch": 57.17530941563528,
      "grad_norm": 6.7789411544799805,
      "learning_rate": 2.3539088203039326e-06,
      "loss": 1.6798,
      "step": 729900
    },
    {
      "epoch": 57.18314272285759,
      "grad_norm": 7.51836633682251,
      "learning_rate": 2.3473810642853414e-06,
      "loss": 1.5626,
      "step": 730000
    },
    {
      "epoch": 57.1909760300799,
      "grad_norm": 4.917461395263672,
      "learning_rate": 2.3408533082667505e-06,
      "loss": 1.5461,
      "step": 730100
    },
    {
      "epoch": 57.19880933730221,
      "grad_norm": 7.407337188720703,
      "learning_rate": 2.3343255522481592e-06,
      "loss": 1.7829,
      "step": 730200
    },
    {
      "epoch": 57.20664264452452,
      "grad_norm": 5.345488548278809,
      "learning_rate": 2.3277977962295684e-06,
      "loss": 1.6667,
      "step": 730300
    },
    {
      "epoch": 57.21447595174683,
      "grad_norm": 6.576808452606201,
      "learning_rate": 2.321270040210977e-06,
      "loss": 1.5994,
      "step": 730400
    },
    {
      "epoch": 57.22230925896913,
      "grad_norm": 6.795492172241211,
      "learning_rate": 2.3147422841923863e-06,
      "loss": 1.6226,
      "step": 730500
    },
    {
      "epoch": 57.23014256619145,
      "grad_norm": 9.662817001342773,
      "learning_rate": 2.308214528173795e-06,
      "loss": 1.6393,
      "step": 730600
    },
    {
      "epoch": 57.23797587341375,
      "grad_norm": 6.679425239562988,
      "learning_rate": 2.301686772155204e-06,
      "loss": 1.5816,
      "step": 730700
    },
    {
      "epoch": 57.245809180636066,
      "grad_norm": 6.46577262878418,
      "learning_rate": 2.295159016136613e-06,
      "loss": 1.6599,
      "step": 730800
    },
    {
      "epoch": 57.25364248785837,
      "grad_norm": 7.116982936859131,
      "learning_rate": 2.288631260118022e-06,
      "loss": 1.5662,
      "step": 730900
    },
    {
      "epoch": 57.261475795080685,
      "grad_norm": 6.623076438903809,
      "learning_rate": 2.282103504099431e-06,
      "loss": 1.6811,
      "step": 731000
    },
    {
      "epoch": 57.26930910230299,
      "grad_norm": 7.238562107086182,
      "learning_rate": 2.27557574808084e-06,
      "loss": 1.6386,
      "step": 731100
    },
    {
      "epoch": 57.277142409525304,
      "grad_norm": 6.071761608123779,
      "learning_rate": 2.2690479920622487e-06,
      "loss": 1.6962,
      "step": 731200
    },
    {
      "epoch": 57.28497571674761,
      "grad_norm": 6.117508888244629,
      "learning_rate": 2.262520236043658e-06,
      "loss": 1.6124,
      "step": 731300
    },
    {
      "epoch": 57.29280902396992,
      "grad_norm": 6.7366108894348145,
      "learning_rate": 2.2559924800250666e-06,
      "loss": 1.5657,
      "step": 731400
    },
    {
      "epoch": 57.30064233119223,
      "grad_norm": 7.20857048034668,
      "learning_rate": 2.2494647240064757e-06,
      "loss": 1.7404,
      "step": 731500
    },
    {
      "epoch": 57.308475638414535,
      "grad_norm": 5.486095428466797,
      "learning_rate": 2.2429369679878845e-06,
      "loss": 1.6735,
      "step": 731600
    },
    {
      "epoch": 57.31630894563685,
      "grad_norm": 6.485930442810059,
      "learning_rate": 2.2364092119692936e-06,
      "loss": 1.6168,
      "step": 731700
    },
    {
      "epoch": 57.324142252859154,
      "grad_norm": 6.2610392570495605,
      "learning_rate": 2.2298814559507024e-06,
      "loss": 1.6018,
      "step": 731800
    },
    {
      "epoch": 57.33197556008147,
      "grad_norm": 4.716146469116211,
      "learning_rate": 2.2233536999321115e-06,
      "loss": 1.6165,
      "step": 731900
    },
    {
      "epoch": 57.33980886730377,
      "grad_norm": 8.588196754455566,
      "learning_rate": 2.2168259439135202e-06,
      "loss": 1.6678,
      "step": 732000
    },
    {
      "epoch": 57.34764217452609,
      "grad_norm": 5.465202331542969,
      "learning_rate": 2.2102981878949294e-06,
      "loss": 1.5743,
      "step": 732100
    },
    {
      "epoch": 57.35547548174839,
      "grad_norm": 7.727367877960205,
      "learning_rate": 2.203770431876338e-06,
      "loss": 1.6672,
      "step": 732200
    },
    {
      "epoch": 57.363308788970706,
      "grad_norm": 6.355270862579346,
      "learning_rate": 2.1972426758577473e-06,
      "loss": 1.6493,
      "step": 732300
    },
    {
      "epoch": 57.37114209619301,
      "grad_norm": 4.386556148529053,
      "learning_rate": 2.190714919839156e-06,
      "loss": 1.6192,
      "step": 732400
    },
    {
      "epoch": 57.378975403415325,
      "grad_norm": 6.810105800628662,
      "learning_rate": 2.184187163820565e-06,
      "loss": 1.649,
      "step": 732500
    },
    {
      "epoch": 57.38680871063763,
      "grad_norm": 5.237548828125,
      "learning_rate": 2.177659407801974e-06,
      "loss": 1.6359,
      "step": 732600
    },
    {
      "epoch": 57.394642017859944,
      "grad_norm": 6.390378475189209,
      "learning_rate": 2.171131651783383e-06,
      "loss": 1.6447,
      "step": 732700
    },
    {
      "epoch": 57.40247532508225,
      "grad_norm": 8.505611419677734,
      "learning_rate": 2.164603895764792e-06,
      "loss": 1.6005,
      "step": 732800
    },
    {
      "epoch": 57.410308632304556,
      "grad_norm": 8.060585975646973,
      "learning_rate": 2.158076139746201e-06,
      "loss": 1.6366,
      "step": 732900
    },
    {
      "epoch": 57.41814193952687,
      "grad_norm": 6.013392925262451,
      "learning_rate": 2.1515483837276097e-06,
      "loss": 1.6143,
      "step": 733000
    },
    {
      "epoch": 57.425975246749175,
      "grad_norm": 7.22283935546875,
      "learning_rate": 2.145020627709019e-06,
      "loss": 1.6956,
      "step": 733100
    },
    {
      "epoch": 57.43380855397149,
      "grad_norm": 8.129039764404297,
      "learning_rate": 2.1384928716904276e-06,
      "loss": 1.6395,
      "step": 733200
    },
    {
      "epoch": 57.441641861193794,
      "grad_norm": 6.516733646392822,
      "learning_rate": 2.1319651156718367e-06,
      "loss": 1.6544,
      "step": 733300
    },
    {
      "epoch": 57.44947516841611,
      "grad_norm": 6.565813064575195,
      "learning_rate": 2.1254373596532455e-06,
      "loss": 1.6258,
      "step": 733400
    },
    {
      "epoch": 57.45730847563841,
      "grad_norm": 6.002661228179932,
      "learning_rate": 2.1189096036346546e-06,
      "loss": 1.5814,
      "step": 733500
    },
    {
      "epoch": 57.46514178286073,
      "grad_norm": 6.406075477600098,
      "learning_rate": 2.1123818476160634e-06,
      "loss": 1.6192,
      "step": 733600
    },
    {
      "epoch": 57.47297509008303,
      "grad_norm": 7.835973739624023,
      "learning_rate": 2.1058540915974725e-06,
      "loss": 1.6374,
      "step": 733700
    },
    {
      "epoch": 57.480808397305346,
      "grad_norm": 8.146596908569336,
      "learning_rate": 2.0993263355788813e-06,
      "loss": 1.6765,
      "step": 733800
    },
    {
      "epoch": 57.48864170452765,
      "grad_norm": 7.049465179443359,
      "learning_rate": 2.0927985795602904e-06,
      "loss": 1.5005,
      "step": 733900
    },
    {
      "epoch": 57.49647501174996,
      "grad_norm": 6.321768760681152,
      "learning_rate": 2.086270823541699e-06,
      "loss": 1.6576,
      "step": 734000
    },
    {
      "epoch": 57.50430831897227,
      "grad_norm": 7.260645389556885,
      "learning_rate": 2.0797430675231083e-06,
      "loss": 1.5731,
      "step": 734100
    },
    {
      "epoch": 57.51214162619458,
      "grad_norm": 6.35956335067749,
      "learning_rate": 2.073215311504517e-06,
      "loss": 1.6464,
      "step": 734200
    },
    {
      "epoch": 57.51997493341689,
      "grad_norm": 8.434117317199707,
      "learning_rate": 2.066687555485926e-06,
      "loss": 1.6085,
      "step": 734300
    },
    {
      "epoch": 57.527808240639196,
      "grad_norm": 3.192904472351074,
      "learning_rate": 2.060159799467335e-06,
      "loss": 1.6533,
      "step": 734400
    },
    {
      "epoch": 57.53564154786151,
      "grad_norm": 6.510415554046631,
      "learning_rate": 2.053632043448744e-06,
      "loss": 1.7008,
      "step": 734500
    },
    {
      "epoch": 57.543474855083815,
      "grad_norm": 5.5143351554870605,
      "learning_rate": 2.047104287430153e-06,
      "loss": 1.5758,
      "step": 734600
    },
    {
      "epoch": 57.55130816230613,
      "grad_norm": 5.544952392578125,
      "learning_rate": 2.040576531411562e-06,
      "loss": 1.6158,
      "step": 734700
    },
    {
      "epoch": 57.559141469528434,
      "grad_norm": 6.868485927581787,
      "learning_rate": 2.0340487753929707e-06,
      "loss": 1.5621,
      "step": 734800
    },
    {
      "epoch": 57.56697477675075,
      "grad_norm": 5.4295525550842285,
      "learning_rate": 2.02752101937438e-06,
      "loss": 1.5355,
      "step": 734900
    },
    {
      "epoch": 57.57480808397305,
      "grad_norm": 6.877514362335205,
      "learning_rate": 2.0209932633557886e-06,
      "loss": 1.7156,
      "step": 735000
    },
    {
      "epoch": 57.58264139119536,
      "grad_norm": 7.278379917144775,
      "learning_rate": 2.0144655073371977e-06,
      "loss": 1.6232,
      "step": 735100
    },
    {
      "epoch": 57.59047469841767,
      "grad_norm": 5.787582874298096,
      "learning_rate": 2.0079377513186065e-06,
      "loss": 1.595,
      "step": 735200
    },
    {
      "epoch": 57.59830800563998,
      "grad_norm": 6.336916446685791,
      "learning_rate": 2.0014099953000156e-06,
      "loss": 1.654,
      "step": 735300
    },
    {
      "epoch": 57.60614131286229,
      "grad_norm": 6.040771961212158,
      "learning_rate": 1.994882239281425e-06,
      "loss": 1.5913,
      "step": 735400
    },
    {
      "epoch": 57.6139746200846,
      "grad_norm": 6.967398643493652,
      "learning_rate": 1.988354483262834e-06,
      "loss": 1.6109,
      "step": 735500
    },
    {
      "epoch": 57.62180792730691,
      "grad_norm": 6.590910911560059,
      "learning_rate": 1.9818267272442427e-06,
      "loss": 1.5848,
      "step": 735600
    },
    {
      "epoch": 57.62964123452922,
      "grad_norm": 7.432562828063965,
      "learning_rate": 1.975298971225652e-06,
      "loss": 1.6091,
      "step": 735700
    },
    {
      "epoch": 57.63747454175153,
      "grad_norm": 6.536579608917236,
      "learning_rate": 1.9687712152070606e-06,
      "loss": 1.7093,
      "step": 735800
    },
    {
      "epoch": 57.645307848973836,
      "grad_norm": 6.957205772399902,
      "learning_rate": 1.9622434591884697e-06,
      "loss": 1.6815,
      "step": 735900
    },
    {
      "epoch": 57.65314115619615,
      "grad_norm": 7.008903503417969,
      "learning_rate": 1.9557157031698785e-06,
      "loss": 1.5524,
      "step": 736000
    },
    {
      "epoch": 57.660974463418455,
      "grad_norm": 6.324630260467529,
      "learning_rate": 1.9491879471512876e-06,
      "loss": 1.617,
      "step": 736100
    },
    {
      "epoch": 57.66880777064076,
      "grad_norm": 7.058491230010986,
      "learning_rate": 1.9426601911326964e-06,
      "loss": 1.6573,
      "step": 736200
    },
    {
      "epoch": 57.676641077863074,
      "grad_norm": 7.6540985107421875,
      "learning_rate": 1.9361324351141055e-06,
      "loss": 1.5855,
      "step": 736300
    },
    {
      "epoch": 57.68447438508538,
      "grad_norm": 5.672825336456299,
      "learning_rate": 1.9296046790955142e-06,
      "loss": 1.6833,
      "step": 736400
    },
    {
      "epoch": 57.69230769230769,
      "grad_norm": 7.675655841827393,
      "learning_rate": 1.9230769230769234e-06,
      "loss": 1.6029,
      "step": 736500
    },
    {
      "epoch": 57.70014099953,
      "grad_norm": 5.695286273956299,
      "learning_rate": 1.916549167058332e-06,
      "loss": 1.6372,
      "step": 736600
    },
    {
      "epoch": 57.70797430675231,
      "grad_norm": 7.034444332122803,
      "learning_rate": 1.9100214110397413e-06,
      "loss": 1.5713,
      "step": 736700
    },
    {
      "epoch": 57.71580761397462,
      "grad_norm": 6.252984046936035,
      "learning_rate": 1.90349365502115e-06,
      "loss": 1.5599,
      "step": 736800
    },
    {
      "epoch": 57.72364092119693,
      "grad_norm": 5.146773338317871,
      "learning_rate": 1.896965899002559e-06,
      "loss": 1.5982,
      "step": 736900
    },
    {
      "epoch": 57.73147422841924,
      "grad_norm": 6.264477252960205,
      "learning_rate": 1.890438142983968e-06,
      "loss": 1.5692,
      "step": 737000
    },
    {
      "epoch": 57.73930753564155,
      "grad_norm": 7.26755428314209,
      "learning_rate": 1.8839103869653769e-06,
      "loss": 1.5603,
      "step": 737100
    },
    {
      "epoch": 57.74714084286386,
      "grad_norm": 6.475571632385254,
      "learning_rate": 1.8773826309467858e-06,
      "loss": 1.6482,
      "step": 737200
    },
    {
      "epoch": 57.75497415008617,
      "grad_norm": 5.933389186859131,
      "learning_rate": 1.8708548749281947e-06,
      "loss": 1.5684,
      "step": 737300
    },
    {
      "epoch": 57.762807457308476,
      "grad_norm": 5.180096626281738,
      "learning_rate": 1.8643271189096037e-06,
      "loss": 1.5793,
      "step": 737400
    },
    {
      "epoch": 57.77064076453078,
      "grad_norm": 6.73453426361084,
      "learning_rate": 1.8577993628910126e-06,
      "loss": 1.6396,
      "step": 737500
    },
    {
      "epoch": 57.778474071753095,
      "grad_norm": 8.597658157348633,
      "learning_rate": 1.8512716068724216e-06,
      "loss": 1.5727,
      "step": 737600
    },
    {
      "epoch": 57.7863073789754,
      "grad_norm": 5.169195175170898,
      "learning_rate": 1.8447438508538305e-06,
      "loss": 1.607,
      "step": 737700
    },
    {
      "epoch": 57.794140686197714,
      "grad_norm": 10.689943313598633,
      "learning_rate": 1.8382160948352395e-06,
      "loss": 1.6579,
      "step": 737800
    },
    {
      "epoch": 57.80197399342002,
      "grad_norm": 6.373210430145264,
      "learning_rate": 1.8316883388166484e-06,
      "loss": 1.6258,
      "step": 737900
    },
    {
      "epoch": 57.80980730064233,
      "grad_norm": 8.034468650817871,
      "learning_rate": 1.8251605827980574e-06,
      "loss": 1.6148,
      "step": 738000
    },
    {
      "epoch": 57.81764060786464,
      "grad_norm": 5.395201206207275,
      "learning_rate": 1.8186328267794663e-06,
      "loss": 1.6158,
      "step": 738100
    },
    {
      "epoch": 57.82547391508695,
      "grad_norm": 9.033616065979004,
      "learning_rate": 1.8121050707608752e-06,
      "loss": 1.5782,
      "step": 738200
    },
    {
      "epoch": 57.83330722230926,
      "grad_norm": 9.916899681091309,
      "learning_rate": 1.8055773147422842e-06,
      "loss": 1.6549,
      "step": 738300
    },
    {
      "epoch": 57.84114052953157,
      "grad_norm": 7.789247512817383,
      "learning_rate": 1.7990495587236931e-06,
      "loss": 1.6497,
      "step": 738400
    },
    {
      "epoch": 57.84897383675388,
      "grad_norm": 7.0892558097839355,
      "learning_rate": 1.792521802705102e-06,
      "loss": 1.5759,
      "step": 738500
    },
    {
      "epoch": 57.856807143976184,
      "grad_norm": 6.641499996185303,
      "learning_rate": 1.785994046686511e-06,
      "loss": 1.65,
      "step": 738600
    },
    {
      "epoch": 57.8646404511985,
      "grad_norm": 6.2313456535339355,
      "learning_rate": 1.77946629066792e-06,
      "loss": 1.5223,
      "step": 738700
    },
    {
      "epoch": 57.8724737584208,
      "grad_norm": 7.476316452026367,
      "learning_rate": 1.772938534649329e-06,
      "loss": 1.5854,
      "step": 738800
    },
    {
      "epoch": 57.880307065643116,
      "grad_norm": 5.8842692375183105,
      "learning_rate": 1.7664107786307379e-06,
      "loss": 1.6722,
      "step": 738900
    },
    {
      "epoch": 57.88814037286542,
      "grad_norm": 8.659111022949219,
      "learning_rate": 1.7598830226121468e-06,
      "loss": 1.5847,
      "step": 739000
    },
    {
      "epoch": 57.895973680087735,
      "grad_norm": 6.259174346923828,
      "learning_rate": 1.7533552665935557e-06,
      "loss": 1.5717,
      "step": 739100
    },
    {
      "epoch": 57.90380698731004,
      "grad_norm": 5.965080261230469,
      "learning_rate": 1.7468275105749647e-06,
      "loss": 1.5248,
      "step": 739200
    },
    {
      "epoch": 57.911640294532354,
      "grad_norm": 7.316725730895996,
      "learning_rate": 1.7402997545563736e-06,
      "loss": 1.5856,
      "step": 739300
    },
    {
      "epoch": 57.91947360175466,
      "grad_norm": 6.465467929840088,
      "learning_rate": 1.7337719985377826e-06,
      "loss": 1.6001,
      "step": 739400
    },
    {
      "epoch": 57.92730690897697,
      "grad_norm": 8.384865760803223,
      "learning_rate": 1.7272442425191915e-06,
      "loss": 1.5866,
      "step": 739500
    },
    {
      "epoch": 57.93514021619928,
      "grad_norm": 5.707267761230469,
      "learning_rate": 1.7207164865006005e-06,
      "loss": 1.4988,
      "step": 739600
    },
    {
      "epoch": 57.942973523421585,
      "grad_norm": 10.528589248657227,
      "learning_rate": 1.7141887304820094e-06,
      "loss": 1.5551,
      "step": 739700
    },
    {
      "epoch": 57.9508068306439,
      "grad_norm": 6.982769966125488,
      "learning_rate": 1.7076609744634184e-06,
      "loss": 1.5843,
      "step": 739800
    },
    {
      "epoch": 57.958640137866205,
      "grad_norm": 4.472244739532471,
      "learning_rate": 1.7011332184448273e-06,
      "loss": 1.586,
      "step": 739900
    },
    {
      "epoch": 57.96647344508852,
      "grad_norm": 6.226780414581299,
      "learning_rate": 1.6946054624262363e-06,
      "loss": 1.5676,
      "step": 740000
    },
    {
      "epoch": 57.974306752310824,
      "grad_norm": 6.5369873046875,
      "learning_rate": 1.6880777064076452e-06,
      "loss": 1.6826,
      "step": 740100
    },
    {
      "epoch": 57.98214005953314,
      "grad_norm": 4.2210798263549805,
      "learning_rate": 1.6815499503890541e-06,
      "loss": 1.6018,
      "step": 740200
    },
    {
      "epoch": 57.98997336675544,
      "grad_norm": 6.761510848999023,
      "learning_rate": 1.675022194370463e-06,
      "loss": 1.702,
      "step": 740300
    },
    {
      "epoch": 57.997806673977756,
      "grad_norm": 7.988028049468994,
      "learning_rate": 1.668494438351872e-06,
      "loss": 1.5447,
      "step": 740400
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.7700231075286865,
      "eval_runtime": 1.52,
      "eval_samples_per_second": 442.11,
      "eval_steps_per_second": 442.11,
      "step": 740428
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.3733973503112793,
      "eval_runtime": 28.6836,
      "eval_samples_per_second": 445.063,
      "eval_steps_per_second": 445.063,
      "step": 740428
    },
    {
      "epoch": 58.00563998120006,
      "grad_norm": 6.32529878616333,
      "learning_rate": 1.661966682333281e-06,
      "loss": 1.7107,
      "step": 740500
    },
    {
      "epoch": 58.013473288422375,
      "grad_norm": 8.295902252197266,
      "learning_rate": 1.65543892631469e-06,
      "loss": 1.5465,
      "step": 740600
    },
    {
      "epoch": 58.02130659564468,
      "grad_norm": 8.679916381835938,
      "learning_rate": 1.6489111702960989e-06,
      "loss": 1.6182,
      "step": 740700
    },
    {
      "epoch": 58.02913990286699,
      "grad_norm": 5.640059947967529,
      "learning_rate": 1.6423834142775078e-06,
      "loss": 1.6632,
      "step": 740800
    },
    {
      "epoch": 58.0369732100893,
      "grad_norm": 5.859857559204102,
      "learning_rate": 1.6358556582589168e-06,
      "loss": 1.4976,
      "step": 740900
    },
    {
      "epoch": 58.044806517311606,
      "grad_norm": 7.866921424865723,
      "learning_rate": 1.6293279022403257e-06,
      "loss": 1.5692,
      "step": 741000
    },
    {
      "epoch": 58.05263982453392,
      "grad_norm": 10.372262954711914,
      "learning_rate": 1.6228001462217346e-06,
      "loss": 1.5771,
      "step": 741100
    },
    {
      "epoch": 58.060473131756225,
      "grad_norm": 7.030162811279297,
      "learning_rate": 1.6162723902031436e-06,
      "loss": 1.6586,
      "step": 741200
    },
    {
      "epoch": 58.06830643897854,
      "grad_norm": 6.989543437957764,
      "learning_rate": 1.6097446341845525e-06,
      "loss": 1.6729,
      "step": 741300
    },
    {
      "epoch": 58.076139746200845,
      "grad_norm": 6.700727462768555,
      "learning_rate": 1.603216878165962e-06,
      "loss": 1.7122,
      "step": 741400
    },
    {
      "epoch": 58.08397305342316,
      "grad_norm": 5.755996227264404,
      "learning_rate": 1.5966891221473708e-06,
      "loss": 1.5397,
      "step": 741500
    },
    {
      "epoch": 58.091806360645464,
      "grad_norm": 6.756869316101074,
      "learning_rate": 1.5901613661287798e-06,
      "loss": 1.6435,
      "step": 741600
    },
    {
      "epoch": 58.09963966786778,
      "grad_norm": 8.412936210632324,
      "learning_rate": 1.5836336101101887e-06,
      "loss": 1.63,
      "step": 741700
    },
    {
      "epoch": 58.10747297509008,
      "grad_norm": 5.495582103729248,
      "learning_rate": 1.5771058540915977e-06,
      "loss": 1.5565,
      "step": 741800
    },
    {
      "epoch": 58.11530628231239,
      "grad_norm": 5.687201976776123,
      "learning_rate": 1.5705780980730066e-06,
      "loss": 1.64,
      "step": 741900
    },
    {
      "epoch": 58.1231395895347,
      "grad_norm": 6.1091108322143555,
      "learning_rate": 1.5640503420544156e-06,
      "loss": 1.5092,
      "step": 742000
    },
    {
      "epoch": 58.13097289675701,
      "grad_norm": 5.174748420715332,
      "learning_rate": 1.5575225860358243e-06,
      "loss": 1.5727,
      "step": 742100
    },
    {
      "epoch": 58.13880620397932,
      "grad_norm": 6.829691410064697,
      "learning_rate": 1.5509948300172332e-06,
      "loss": 1.6283,
      "step": 742200
    },
    {
      "epoch": 58.14663951120163,
      "grad_norm": 6.390169143676758,
      "learning_rate": 1.5444670739986422e-06,
      "loss": 1.722,
      "step": 742300
    },
    {
      "epoch": 58.15447281842394,
      "grad_norm": 5.6115851402282715,
      "learning_rate": 1.5379393179800511e-06,
      "loss": 1.6301,
      "step": 742400
    },
    {
      "epoch": 58.162306125646246,
      "grad_norm": 6.427196502685547,
      "learning_rate": 1.53141156196146e-06,
      "loss": 1.618,
      "step": 742500
    },
    {
      "epoch": 58.17013943286856,
      "grad_norm": 5.712265491485596,
      "learning_rate": 1.524883805942869e-06,
      "loss": 1.6865,
      "step": 742600
    },
    {
      "epoch": 58.177972740090865,
      "grad_norm": 7.18765115737915,
      "learning_rate": 1.518356049924278e-06,
      "loss": 1.6017,
      "step": 742700
    },
    {
      "epoch": 58.18580604731318,
      "grad_norm": 6.682783126831055,
      "learning_rate": 1.511828293905687e-06,
      "loss": 1.5694,
      "step": 742800
    },
    {
      "epoch": 58.193639354535485,
      "grad_norm": 6.670698165893555,
      "learning_rate": 1.505300537887096e-06,
      "loss": 1.6959,
      "step": 742900
    },
    {
      "epoch": 58.20147266175779,
      "grad_norm": 4.694385051727295,
      "learning_rate": 1.498772781868505e-06,
      "loss": 1.632,
      "step": 743000
    },
    {
      "epoch": 58.209305968980104,
      "grad_norm": 7.2248101234436035,
      "learning_rate": 1.492245025849914e-06,
      "loss": 1.6731,
      "step": 743100
    },
    {
      "epoch": 58.21713927620241,
      "grad_norm": 7.357936859130859,
      "learning_rate": 1.485717269831323e-06,
      "loss": 1.7182,
      "step": 743200
    },
    {
      "epoch": 58.22497258342472,
      "grad_norm": 6.171080589294434,
      "learning_rate": 1.4791895138127319e-06,
      "loss": 1.6245,
      "step": 743300
    },
    {
      "epoch": 58.23280589064703,
      "grad_norm": 7.9077558517456055,
      "learning_rate": 1.4726617577941408e-06,
      "loss": 1.5975,
      "step": 743400
    },
    {
      "epoch": 58.24063919786934,
      "grad_norm": 8.771178245544434,
      "learning_rate": 1.4661340017755497e-06,
      "loss": 1.5682,
      "step": 743500
    },
    {
      "epoch": 58.24847250509165,
      "grad_norm": 5.803225040435791,
      "learning_rate": 1.4596062457569587e-06,
      "loss": 1.5452,
      "step": 743600
    },
    {
      "epoch": 58.25630581231396,
      "grad_norm": 9.207131385803223,
      "learning_rate": 1.4530784897383676e-06,
      "loss": 1.627,
      "step": 743700
    },
    {
      "epoch": 58.26413911953627,
      "grad_norm": 5.929832458496094,
      "learning_rate": 1.4465507337197766e-06,
      "loss": 1.5434,
      "step": 743800
    },
    {
      "epoch": 58.27197242675858,
      "grad_norm": 5.918243885040283,
      "learning_rate": 1.4400229777011855e-06,
      "loss": 1.5731,
      "step": 743900
    },
    {
      "epoch": 58.279805733980886,
      "grad_norm": 5.51373291015625,
      "learning_rate": 1.4334952216825945e-06,
      "loss": 1.6029,
      "step": 744000
    },
    {
      "epoch": 58.2876390412032,
      "grad_norm": 6.595343589782715,
      "learning_rate": 1.4269674656640034e-06,
      "loss": 1.5922,
      "step": 744100
    },
    {
      "epoch": 58.295472348425506,
      "grad_norm": 7.146707057952881,
      "learning_rate": 1.4204397096454124e-06,
      "loss": 1.6053,
      "step": 744200
    },
    {
      "epoch": 58.30330565564781,
      "grad_norm": 4.769878387451172,
      "learning_rate": 1.4139119536268213e-06,
      "loss": 1.6798,
      "step": 744300
    },
    {
      "epoch": 58.311138962870125,
      "grad_norm": 5.558304309844971,
      "learning_rate": 1.4073841976082302e-06,
      "loss": 1.6617,
      "step": 744400
    },
    {
      "epoch": 58.31897227009243,
      "grad_norm": 6.449865818023682,
      "learning_rate": 1.4008564415896392e-06,
      "loss": 1.6036,
      "step": 744500
    },
    {
      "epoch": 58.326805577314744,
      "grad_norm": 8.488080978393555,
      "learning_rate": 1.3943286855710481e-06,
      "loss": 1.6185,
      "step": 744600
    },
    {
      "epoch": 58.33463888453705,
      "grad_norm": 6.723282337188721,
      "learning_rate": 1.387800929552457e-06,
      "loss": 1.7072,
      "step": 744700
    },
    {
      "epoch": 58.34247219175936,
      "grad_norm": 6.178688049316406,
      "learning_rate": 1.381273173533866e-06,
      "loss": 1.494,
      "step": 744800
    },
    {
      "epoch": 58.35030549898167,
      "grad_norm": 5.098343849182129,
      "learning_rate": 1.374745417515275e-06,
      "loss": 1.6002,
      "step": 744900
    },
    {
      "epoch": 58.35813880620398,
      "grad_norm": 6.269369125366211,
      "learning_rate": 1.368217661496684e-06,
      "loss": 1.6905,
      "step": 745000
    },
    {
      "epoch": 58.36597211342629,
      "grad_norm": 9.510878562927246,
      "learning_rate": 1.3616899054780929e-06,
      "loss": 1.648,
      "step": 745100
    },
    {
      "epoch": 58.3738054206486,
      "grad_norm": 7.603990077972412,
      "learning_rate": 1.3551621494595018e-06,
      "loss": 1.6829,
      "step": 745200
    },
    {
      "epoch": 58.38163872787091,
      "grad_norm": 6.1155900955200195,
      "learning_rate": 1.3486343934409107e-06,
      "loss": 1.6172,
      "step": 745300
    },
    {
      "epoch": 58.38947203509321,
      "grad_norm": 3.582465410232544,
      "learning_rate": 1.3421066374223197e-06,
      "loss": 1.6833,
      "step": 745400
    },
    {
      "epoch": 58.397305342315526,
      "grad_norm": 4.83410120010376,
      "learning_rate": 1.3355788814037286e-06,
      "loss": 1.6426,
      "step": 745500
    },
    {
      "epoch": 58.40513864953783,
      "grad_norm": 7.470632076263428,
      "learning_rate": 1.3290511253851376e-06,
      "loss": 1.6747,
      "step": 745600
    },
    {
      "epoch": 58.412971956760146,
      "grad_norm": 5.32043981552124,
      "learning_rate": 1.3225233693665465e-06,
      "loss": 1.6542,
      "step": 745700
    },
    {
      "epoch": 58.42080526398245,
      "grad_norm": 5.0857930183410645,
      "learning_rate": 1.3159956133479555e-06,
      "loss": 1.6357,
      "step": 745800
    },
    {
      "epoch": 58.428638571204765,
      "grad_norm": 4.3812174797058105,
      "learning_rate": 1.3094678573293646e-06,
      "loss": 1.6188,
      "step": 745900
    },
    {
      "epoch": 58.43647187842707,
      "grad_norm": 6.705526351928711,
      "learning_rate": 1.3029401013107736e-06,
      "loss": 1.6928,
      "step": 746000
    },
    {
      "epoch": 58.444305185649384,
      "grad_norm": 5.492679595947266,
      "learning_rate": 1.2964123452921825e-06,
      "loss": 1.5731,
      "step": 746100
    },
    {
      "epoch": 58.45213849287169,
      "grad_norm": 9.353959083557129,
      "learning_rate": 1.2898845892735915e-06,
      "loss": 1.5853,
      "step": 746200
    },
    {
      "epoch": 58.459971800094,
      "grad_norm": 8.19884204864502,
      "learning_rate": 1.2833568332550004e-06,
      "loss": 1.605,
      "step": 746300
    },
    {
      "epoch": 58.46780510731631,
      "grad_norm": 6.107368469238281,
      "learning_rate": 1.2768290772364093e-06,
      "loss": 1.6148,
      "step": 746400
    },
    {
      "epoch": 58.475638414538615,
      "grad_norm": 5.604500770568848,
      "learning_rate": 1.2703013212178183e-06,
      "loss": 1.6466,
      "step": 746500
    },
    {
      "epoch": 58.48347172176093,
      "grad_norm": 6.7792067527771,
      "learning_rate": 1.2637735651992272e-06,
      "loss": 1.5861,
      "step": 746600
    },
    {
      "epoch": 58.491305028983234,
      "grad_norm": 7.31397008895874,
      "learning_rate": 1.2572458091806362e-06,
      "loss": 1.4861,
      "step": 746700
    },
    {
      "epoch": 58.49913833620555,
      "grad_norm": 4.45969295501709,
      "learning_rate": 1.2507180531620451e-06,
      "loss": 1.6436,
      "step": 746800
    },
    {
      "epoch": 58.50697164342785,
      "grad_norm": 4.007171630859375,
      "learning_rate": 1.244190297143454e-06,
      "loss": 1.6068,
      "step": 746900
    },
    {
      "epoch": 58.514804950650166,
      "grad_norm": 6.433829307556152,
      "learning_rate": 1.237662541124863e-06,
      "loss": 1.5448,
      "step": 747000
    },
    {
      "epoch": 58.52263825787247,
      "grad_norm": 5.563141345977783,
      "learning_rate": 1.231134785106272e-06,
      "loss": 1.6266,
      "step": 747100
    },
    {
      "epoch": 58.530471565094786,
      "grad_norm": 6.889705181121826,
      "learning_rate": 1.224607029087681e-06,
      "loss": 1.574,
      "step": 747200
    },
    {
      "epoch": 58.53830487231709,
      "grad_norm": 4.044431209564209,
      "learning_rate": 1.2180792730690899e-06,
      "loss": 1.5859,
      "step": 747300
    },
    {
      "epoch": 58.546138179539405,
      "grad_norm": 6.927507400512695,
      "learning_rate": 1.2115515170504988e-06,
      "loss": 1.6026,
      "step": 747400
    },
    {
      "epoch": 58.55397148676171,
      "grad_norm": 5.482710361480713,
      "learning_rate": 1.2050237610319077e-06,
      "loss": 1.6632,
      "step": 747500
    },
    {
      "epoch": 58.56180479398402,
      "grad_norm": 5.136866569519043,
      "learning_rate": 1.1984960050133167e-06,
      "loss": 1.5815,
      "step": 747600
    },
    {
      "epoch": 58.56963810120633,
      "grad_norm": 6.289796352386475,
      "learning_rate": 1.1919682489947256e-06,
      "loss": 1.5904,
      "step": 747700
    },
    {
      "epoch": 58.577471408428636,
      "grad_norm": 8.21270751953125,
      "learning_rate": 1.1854404929761346e-06,
      "loss": 1.6253,
      "step": 747800
    },
    {
      "epoch": 58.58530471565095,
      "grad_norm": 7.228723526000977,
      "learning_rate": 1.1789127369575435e-06,
      "loss": 1.5812,
      "step": 747900
    },
    {
      "epoch": 58.593138022873255,
      "grad_norm": 6.423294544219971,
      "learning_rate": 1.1723849809389525e-06,
      "loss": 1.6532,
      "step": 748000
    },
    {
      "epoch": 58.60097133009557,
      "grad_norm": 5.486713886260986,
      "learning_rate": 1.1658572249203614e-06,
      "loss": 1.6726,
      "step": 748100
    },
    {
      "epoch": 58.608804637317874,
      "grad_norm": 8.358674049377441,
      "learning_rate": 1.1593294689017704e-06,
      "loss": 1.5865,
      "step": 748200
    },
    {
      "epoch": 58.61663794454019,
      "grad_norm": 8.812862396240234,
      "learning_rate": 1.1528017128831793e-06,
      "loss": 1.6117,
      "step": 748300
    },
    {
      "epoch": 58.62447125176249,
      "grad_norm": 5.821068286895752,
      "learning_rate": 1.1462739568645882e-06,
      "loss": 1.6065,
      "step": 748400
    },
    {
      "epoch": 58.632304558984806,
      "grad_norm": 4.877200126647949,
      "learning_rate": 1.1397462008459972e-06,
      "loss": 1.7248,
      "step": 748500
    },
    {
      "epoch": 58.64013786620711,
      "grad_norm": 8.097618103027344,
      "learning_rate": 1.1332184448274061e-06,
      "loss": 1.577,
      "step": 748600
    },
    {
      "epoch": 58.64797117342942,
      "grad_norm": 7.917606353759766,
      "learning_rate": 1.126690688808815e-06,
      "loss": 1.6801,
      "step": 748700
    },
    {
      "epoch": 58.65580448065173,
      "grad_norm": 6.2027268409729,
      "learning_rate": 1.120162932790224e-06,
      "loss": 1.5436,
      "step": 748800
    },
    {
      "epoch": 58.66363778787404,
      "grad_norm": 6.0522613525390625,
      "learning_rate": 1.1136351767716332e-06,
      "loss": 1.5185,
      "step": 748900
    },
    {
      "epoch": 58.67147109509635,
      "grad_norm": 6.240690231323242,
      "learning_rate": 1.1071074207530421e-06,
      "loss": 1.6641,
      "step": 749000
    },
    {
      "epoch": 58.67930440231866,
      "grad_norm": 7.130448341369629,
      "learning_rate": 1.100579664734451e-06,
      "loss": 1.6057,
      "step": 749100
    },
    {
      "epoch": 58.68713770954097,
      "grad_norm": 5.843691825866699,
      "learning_rate": 1.09405190871586e-06,
      "loss": 1.6434,
      "step": 749200
    },
    {
      "epoch": 58.694971016763276,
      "grad_norm": 5.250845909118652,
      "learning_rate": 1.087524152697269e-06,
      "loss": 1.5349,
      "step": 749300
    },
    {
      "epoch": 58.70280432398559,
      "grad_norm": 6.88082218170166,
      "learning_rate": 1.080996396678678e-06,
      "loss": 1.668,
      "step": 749400
    },
    {
      "epoch": 58.710637631207895,
      "grad_norm": 8.090987205505371,
      "learning_rate": 1.0744686406600868e-06,
      "loss": 1.6141,
      "step": 749500
    },
    {
      "epoch": 58.71847093843021,
      "grad_norm": 7.577231407165527,
      "learning_rate": 1.0679408846414958e-06,
      "loss": 1.5723,
      "step": 749600
    },
    {
      "epoch": 58.726304245652514,
      "grad_norm": 5.946755409240723,
      "learning_rate": 1.0614131286229047e-06,
      "loss": 1.6229,
      "step": 749700
    },
    {
      "epoch": 58.73413755287483,
      "grad_norm": 8.732510566711426,
      "learning_rate": 1.0548853726043137e-06,
      "loss": 1.5984,
      "step": 749800
    },
    {
      "epoch": 58.74197086009713,
      "grad_norm": 6.741604804992676,
      "learning_rate": 1.0483576165857226e-06,
      "loss": 1.6449,
      "step": 749900
    },
    {
      "epoch": 58.74980416731944,
      "grad_norm": 8.745792388916016,
      "learning_rate": 1.0418298605671316e-06,
      "loss": 1.6805,
      "step": 750000
    },
    {
      "epoch": 58.75763747454175,
      "grad_norm": 7.973668575286865,
      "learning_rate": 1.0353021045485405e-06,
      "loss": 1.6228,
      "step": 750100
    },
    {
      "epoch": 58.76547078176406,
      "grad_norm": 6.096862316131592,
      "learning_rate": 1.0287743485299495e-06,
      "loss": 1.5731,
      "step": 750200
    },
    {
      "epoch": 58.77330408898637,
      "grad_norm": 6.12181282043457,
      "learning_rate": 1.0222465925113584e-06,
      "loss": 1.612,
      "step": 750300
    },
    {
      "epoch": 58.78113739620868,
      "grad_norm": 6.504855155944824,
      "learning_rate": 1.0157188364927674e-06,
      "loss": 1.5702,
      "step": 750400
    },
    {
      "epoch": 58.78897070343099,
      "grad_norm": 6.024219512939453,
      "learning_rate": 1.0091910804741763e-06,
      "loss": 1.6733,
      "step": 750500
    },
    {
      "epoch": 58.7968040106533,
      "grad_norm": 5.685523509979248,
      "learning_rate": 1.0026633244555852e-06,
      "loss": 1.6698,
      "step": 750600
    },
    {
      "epoch": 58.80463731787561,
      "grad_norm": 5.367891311645508,
      "learning_rate": 9.961355684369942e-07,
      "loss": 1.5796,
      "step": 750700
    },
    {
      "epoch": 58.812470625097916,
      "grad_norm": 8.836583137512207,
      "learning_rate": 9.896078124184031e-07,
      "loss": 1.6335,
      "step": 750800
    },
    {
      "epoch": 58.82030393232023,
      "grad_norm": 8.244257926940918,
      "learning_rate": 9.83080056399812e-07,
      "loss": 1.6337,
      "step": 750900
    },
    {
      "epoch": 58.828137239542535,
      "grad_norm": 6.2219929695129395,
      "learning_rate": 9.76552300381221e-07,
      "loss": 1.5655,
      "step": 751000
    },
    {
      "epoch": 58.83597054676484,
      "grad_norm": 13.819944381713867,
      "learning_rate": 9.7002454436263e-07,
      "loss": 1.5927,
      "step": 751100
    },
    {
      "epoch": 58.843803853987154,
      "grad_norm": 5.1942138671875,
      "learning_rate": 9.63496788344039e-07,
      "loss": 1.5466,
      "step": 751200
    },
    {
      "epoch": 58.85163716120946,
      "grad_norm": 6.528549671173096,
      "learning_rate": 9.569690323254479e-07,
      "loss": 1.5243,
      "step": 751300
    },
    {
      "epoch": 58.85947046843177,
      "grad_norm": 5.340747356414795,
      "learning_rate": 9.504412763068567e-07,
      "loss": 1.621,
      "step": 751400
    },
    {
      "epoch": 58.86730377565408,
      "grad_norm": 7.257447719573975,
      "learning_rate": 9.439135202882656e-07,
      "loss": 1.6213,
      "step": 751500
    },
    {
      "epoch": 58.87513708287639,
      "grad_norm": 5.837873458862305,
      "learning_rate": 9.373857642696746e-07,
      "loss": 1.606,
      "step": 751600
    },
    {
      "epoch": 58.8829703900987,
      "grad_norm": 8.827235221862793,
      "learning_rate": 9.308580082510835e-07,
      "loss": 1.6632,
      "step": 751700
    },
    {
      "epoch": 58.89080369732101,
      "grad_norm": 7.13429069519043,
      "learning_rate": 9.243302522324925e-07,
      "loss": 1.5944,
      "step": 751800
    },
    {
      "epoch": 58.89863700454332,
      "grad_norm": 9.1155424118042,
      "learning_rate": 9.178024962139016e-07,
      "loss": 1.7193,
      "step": 751900
    },
    {
      "epoch": 58.90647031176563,
      "grad_norm": 5.98245906829834,
      "learning_rate": 9.112747401953106e-07,
      "loss": 1.6296,
      "step": 752000
    },
    {
      "epoch": 58.91430361898794,
      "grad_norm": 5.784422874450684,
      "learning_rate": 9.047469841767195e-07,
      "loss": 1.6111,
      "step": 752100
    },
    {
      "epoch": 58.92213692621024,
      "grad_norm": 6.279618263244629,
      "learning_rate": 8.982192281581285e-07,
      "loss": 1.5563,
      "step": 752200
    },
    {
      "epoch": 58.929970233432556,
      "grad_norm": 5.962766170501709,
      "learning_rate": 8.916914721395374e-07,
      "loss": 1.6774,
      "step": 752300
    },
    {
      "epoch": 58.93780354065486,
      "grad_norm": 6.4848551750183105,
      "learning_rate": 8.851637161209464e-07,
      "loss": 1.6256,
      "step": 752400
    },
    {
      "epoch": 58.945636847877175,
      "grad_norm": 6.180178642272949,
      "learning_rate": 8.786359601023553e-07,
      "loss": 1.6717,
      "step": 752500
    },
    {
      "epoch": 58.95347015509948,
      "grad_norm": 5.097684860229492,
      "learning_rate": 8.721082040837642e-07,
      "loss": 1.6592,
      "step": 752600
    },
    {
      "epoch": 58.961303462321794,
      "grad_norm": 7.201425552368164,
      "learning_rate": 8.655804480651732e-07,
      "loss": 1.6154,
      "step": 752700
    },
    {
      "epoch": 58.9691367695441,
      "grad_norm": 8.004892349243164,
      "learning_rate": 8.590526920465821e-07,
      "loss": 1.613,
      "step": 752800
    },
    {
      "epoch": 58.97697007676641,
      "grad_norm": 6.618892669677734,
      "learning_rate": 8.525249360279911e-07,
      "loss": 1.6306,
      "step": 752900
    },
    {
      "epoch": 58.98480338398872,
      "grad_norm": 5.168543815612793,
      "learning_rate": 8.459971800094e-07,
      "loss": 1.5694,
      "step": 753000
    },
    {
      "epoch": 58.99263669121103,
      "grad_norm": 4.991528034210205,
      "learning_rate": 8.39469423990809e-07,
      "loss": 1.5952,
      "step": 753100
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.7698124647140503,
      "eval_runtime": 1.5584,
      "eval_samples_per_second": 431.22,
      "eval_steps_per_second": 431.22,
      "step": 753194
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.3736052513122559,
      "eval_runtime": 51.7806,
      "eval_samples_per_second": 246.54,
      "eval_steps_per_second": 246.54,
      "step": 753194
    },
    {
      "epoch": 59.00046999843334,
      "grad_norm": 6.822322845458984,
      "learning_rate": 8.329416679722179e-07,
      "loss": 1.6601,
      "step": 753200
    },
    {
      "epoch": 59.008303305655645,
      "grad_norm": 7.877441883087158,
      "learning_rate": 8.264139119536269e-07,
      "loss": 1.6529,
      "step": 753300
    },
    {
      "epoch": 59.01613661287796,
      "grad_norm": 5.328810214996338,
      "learning_rate": 8.198861559350358e-07,
      "loss": 1.5612,
      "step": 753400
    },
    {
      "epoch": 59.023969920100264,
      "grad_norm": 9.097336769104004,
      "learning_rate": 8.133583999164447e-07,
      "loss": 1.653,
      "step": 753500
    },
    {
      "epoch": 59.03180322732258,
      "grad_norm": 6.684463024139404,
      "learning_rate": 8.068306438978537e-07,
      "loss": 1.5527,
      "step": 753600
    },
    {
      "epoch": 59.03963653454488,
      "grad_norm": 5.65624475479126,
      "learning_rate": 8.003028878792626e-07,
      "loss": 1.5974,
      "step": 753700
    },
    {
      "epoch": 59.047469841767196,
      "grad_norm": 7.357165336608887,
      "learning_rate": 7.937751318606716e-07,
      "loss": 1.5611,
      "step": 753800
    },
    {
      "epoch": 59.0553031489895,
      "grad_norm": 7.342175006866455,
      "learning_rate": 7.872473758420805e-07,
      "loss": 1.5916,
      "step": 753900
    },
    {
      "epoch": 59.063136456211815,
      "grad_norm": 6.095142841339111,
      "learning_rate": 7.807196198234895e-07,
      "loss": 1.6226,
      "step": 754000
    },
    {
      "epoch": 59.07096976343412,
      "grad_norm": 6.432136535644531,
      "learning_rate": 7.741918638048984e-07,
      "loss": 1.6615,
      "step": 754100
    },
    {
      "epoch": 59.078803070656434,
      "grad_norm": 8.609246253967285,
      "learning_rate": 7.676641077863075e-07,
      "loss": 1.6083,
      "step": 754200
    },
    {
      "epoch": 59.08663637787874,
      "grad_norm": 6.097626209259033,
      "learning_rate": 7.611363517677164e-07,
      "loss": 1.5863,
      "step": 754300
    },
    {
      "epoch": 59.094469685101046,
      "grad_norm": 6.176827907562256,
      "learning_rate": 7.546085957491254e-07,
      "loss": 1.5769,
      "step": 754400
    },
    {
      "epoch": 59.10230299232336,
      "grad_norm": 6.0266194343566895,
      "learning_rate": 7.480808397305343e-07,
      "loss": 1.5004,
      "step": 754500
    },
    {
      "epoch": 59.110136299545665,
      "grad_norm": 7.243370532989502,
      "learning_rate": 7.415530837119432e-07,
      "loss": 1.5754,
      "step": 754600
    },
    {
      "epoch": 59.11796960676798,
      "grad_norm": 8.222772598266602,
      "learning_rate": 7.350253276933522e-07,
      "loss": 1.5219,
      "step": 754700
    },
    {
      "epoch": 59.125802913990285,
      "grad_norm": 6.312084197998047,
      "learning_rate": 7.284975716747611e-07,
      "loss": 1.591,
      "step": 754800
    },
    {
      "epoch": 59.1336362212126,
      "grad_norm": 6.323251247406006,
      "learning_rate": 7.219698156561701e-07,
      "loss": 1.6062,
      "step": 754900
    },
    {
      "epoch": 59.141469528434904,
      "grad_norm": 4.595057487487793,
      "learning_rate": 7.15442059637579e-07,
      "loss": 1.7604,
      "step": 755000
    },
    {
      "epoch": 59.14930283565722,
      "grad_norm": 7.686472415924072,
      "learning_rate": 7.08914303618988e-07,
      "loss": 1.6462,
      "step": 755100
    },
    {
      "epoch": 59.15713614287952,
      "grad_norm": 6.241962432861328,
      "learning_rate": 7.023865476003969e-07,
      "loss": 1.5479,
      "step": 755200
    },
    {
      "epoch": 59.164969450101836,
      "grad_norm": 8.944079399108887,
      "learning_rate": 6.958587915818059e-07,
      "loss": 1.6272,
      "step": 755300
    },
    {
      "epoch": 59.17280275732414,
      "grad_norm": 7.969149589538574,
      "learning_rate": 6.893310355632148e-07,
      "loss": 1.5997,
      "step": 755400
    },
    {
      "epoch": 59.18063606454645,
      "grad_norm": 6.131226062774658,
      "learning_rate": 6.828032795446237e-07,
      "loss": 1.5821,
      "step": 755500
    },
    {
      "epoch": 59.18846937176876,
      "grad_norm": 3.951324701309204,
      "learning_rate": 6.762755235260327e-07,
      "loss": 1.6625,
      "step": 755600
    },
    {
      "epoch": 59.19630267899107,
      "grad_norm": 8.192845344543457,
      "learning_rate": 6.697477675074417e-07,
      "loss": 1.7159,
      "step": 755700
    },
    {
      "epoch": 59.20413598621338,
      "grad_norm": 6.505942344665527,
      "learning_rate": 6.632200114888507e-07,
      "loss": 1.6809,
      "step": 755800
    },
    {
      "epoch": 59.211969293435686,
      "grad_norm": 7.45244836807251,
      "learning_rate": 6.566922554702596e-07,
      "loss": 1.6767,
      "step": 755900
    },
    {
      "epoch": 59.219802600658,
      "grad_norm": 6.1633076667785645,
      "learning_rate": 6.501644994516686e-07,
      "loss": 1.7347,
      "step": 756000
    },
    {
      "epoch": 59.227635907880305,
      "grad_norm": 6.829871654510498,
      "learning_rate": 6.436367434330775e-07,
      "loss": 1.5834,
      "step": 756100
    },
    {
      "epoch": 59.23546921510262,
      "grad_norm": 7.568211555480957,
      "learning_rate": 6.371089874144865e-07,
      "loss": 1.5529,
      "step": 756200
    },
    {
      "epoch": 59.243302522324925,
      "grad_norm": 7.572887897491455,
      "learning_rate": 6.305812313958954e-07,
      "loss": 1.6781,
      "step": 756300
    },
    {
      "epoch": 59.25113582954724,
      "grad_norm": 8.479804039001465,
      "learning_rate": 6.240534753773044e-07,
      "loss": 1.6289,
      "step": 756400
    },
    {
      "epoch": 59.258969136769544,
      "grad_norm": 7.245270729064941,
      "learning_rate": 6.175257193587133e-07,
      "loss": 1.5829,
      "step": 756500
    },
    {
      "epoch": 59.26680244399186,
      "grad_norm": 7.031589984893799,
      "learning_rate": 6.109979633401222e-07,
      "loss": 1.7039,
      "step": 756600
    },
    {
      "epoch": 59.27463575121416,
      "grad_norm": 6.908185958862305,
      "learning_rate": 6.044702073215312e-07,
      "loss": 1.6129,
      "step": 756700
    },
    {
      "epoch": 59.28246905843647,
      "grad_norm": 8.110635757446289,
      "learning_rate": 5.979424513029401e-07,
      "loss": 1.7009,
      "step": 756800
    },
    {
      "epoch": 59.29030236565878,
      "grad_norm": 6.105732440948486,
      "learning_rate": 5.914146952843491e-07,
      "loss": 1.6018,
      "step": 756900
    },
    {
      "epoch": 59.29813567288109,
      "grad_norm": 5.368101596832275,
      "learning_rate": 5.84886939265758e-07,
      "loss": 1.64,
      "step": 757000
    },
    {
      "epoch": 59.3059689801034,
      "grad_norm": 6.839933395385742,
      "learning_rate": 5.78359183247167e-07,
      "loss": 1.5819,
      "step": 757100
    },
    {
      "epoch": 59.31380228732571,
      "grad_norm": 6.333762168884277,
      "learning_rate": 5.71831427228576e-07,
      "loss": 1.6709,
      "step": 757200
    },
    {
      "epoch": 59.32163559454802,
      "grad_norm": 4.4109110832214355,
      "learning_rate": 5.65303671209985e-07,
      "loss": 1.5833,
      "step": 757300
    },
    {
      "epoch": 59.329468901770326,
      "grad_norm": 5.469234943389893,
      "learning_rate": 5.587759151913939e-07,
      "loss": 1.6779,
      "step": 757400
    },
    {
      "epoch": 59.33730220899264,
      "grad_norm": 8.526432037353516,
      "learning_rate": 5.522481591728029e-07,
      "loss": 1.6576,
      "step": 757500
    },
    {
      "epoch": 59.345135516214945,
      "grad_norm": 8.697284698486328,
      "learning_rate": 5.457204031542118e-07,
      "loss": 1.5968,
      "step": 757600
    },
    {
      "epoch": 59.35296882343726,
      "grad_norm": 5.562967300415039,
      "learning_rate": 5.391926471356207e-07,
      "loss": 1.6227,
      "step": 757700
    },
    {
      "epoch": 59.360802130659565,
      "grad_norm": 5.44148588180542,
      "learning_rate": 5.326648911170297e-07,
      "loss": 1.6428,
      "step": 757800
    },
    {
      "epoch": 59.36863543788187,
      "grad_norm": 9.866989135742188,
      "learning_rate": 5.261371350984386e-07,
      "loss": 1.6339,
      "step": 757900
    },
    {
      "epoch": 59.376468745104184,
      "grad_norm": 8.146477699279785,
      "learning_rate": 5.196093790798476e-07,
      "loss": 1.6052,
      "step": 758000
    },
    {
      "epoch": 59.38430205232649,
      "grad_norm": 6.2995991706848145,
      "learning_rate": 5.130816230612565e-07,
      "loss": 1.5359,
      "step": 758100
    },
    {
      "epoch": 59.3921353595488,
      "grad_norm": 7.725147724151611,
      "learning_rate": 5.065538670426655e-07,
      "loss": 1.5963,
      "step": 758200
    },
    {
      "epoch": 59.39996866677111,
      "grad_norm": 6.575449466705322,
      "learning_rate": 5.000261110240744e-07,
      "loss": 1.5721,
      "step": 758300
    },
    {
      "epoch": 59.40780197399342,
      "grad_norm": 7.331368923187256,
      "learning_rate": 4.934983550054834e-07,
      "loss": 1.6814,
      "step": 758400
    },
    {
      "epoch": 59.41563528121573,
      "grad_norm": 5.620939254760742,
      "learning_rate": 4.869705989868923e-07,
      "loss": 1.5508,
      "step": 758500
    },
    {
      "epoch": 59.42346858843804,
      "grad_norm": 6.9916300773620605,
      "learning_rate": 4.804428429683012e-07,
      "loss": 1.5798,
      "step": 758600
    },
    {
      "epoch": 59.43130189566035,
      "grad_norm": 7.15053129196167,
      "learning_rate": 4.7391508694971024e-07,
      "loss": 1.5858,
      "step": 758700
    },
    {
      "epoch": 59.43913520288266,
      "grad_norm": 8.683083534240723,
      "learning_rate": 4.673873309311192e-07,
      "loss": 1.5769,
      "step": 758800
    },
    {
      "epoch": 59.446968510104966,
      "grad_norm": 5.238961696624756,
      "learning_rate": 4.6085957491252813e-07,
      "loss": 1.5719,
      "step": 758900
    },
    {
      "epoch": 59.45480181732727,
      "grad_norm": 6.193077564239502,
      "learning_rate": 4.543318188939371e-07,
      "loss": 1.6432,
      "step": 759000
    },
    {
      "epoch": 59.462635124549585,
      "grad_norm": 6.290048599243164,
      "learning_rate": 4.47804062875346e-07,
      "loss": 1.6074,
      "step": 759100
    },
    {
      "epoch": 59.47046843177189,
      "grad_norm": 6.422360420227051,
      "learning_rate": 4.4127630685675496e-07,
      "loss": 1.5976,
      "step": 759200
    },
    {
      "epoch": 59.478301738994205,
      "grad_norm": 6.417599201202393,
      "learning_rate": 4.347485508381639e-07,
      "loss": 1.6385,
      "step": 759300
    },
    {
      "epoch": 59.48613504621651,
      "grad_norm": 6.862124919891357,
      "learning_rate": 4.2822079481957285e-07,
      "loss": 1.5796,
      "step": 759400
    },
    {
      "epoch": 59.493968353438824,
      "grad_norm": 5.459568977355957,
      "learning_rate": 4.216930388009818e-07,
      "loss": 1.6376,
      "step": 759500
    },
    {
      "epoch": 59.50180166066113,
      "grad_norm": 6.768130302429199,
      "learning_rate": 4.1516528278239074e-07,
      "loss": 1.6279,
      "step": 759600
    },
    {
      "epoch": 59.50963496788344,
      "grad_norm": 8.545714378356934,
      "learning_rate": 4.086375267637997e-07,
      "loss": 1.6478,
      "step": 759700
    },
    {
      "epoch": 59.51746827510575,
      "grad_norm": 7.135606288909912,
      "learning_rate": 4.0210977074520863e-07,
      "loss": 1.6111,
      "step": 759800
    },
    {
      "epoch": 59.52530158232806,
      "grad_norm": 6.106618404388428,
      "learning_rate": 3.955820147266176e-07,
      "loss": 1.6546,
      "step": 759900
    },
    {
      "epoch": 59.53313488955037,
      "grad_norm": 7.317592620849609,
      "learning_rate": 3.890542587080266e-07,
      "loss": 1.5616,
      "step": 760000
    },
    {
      "epoch": 59.540968196772674,
      "grad_norm": 6.556050777435303,
      "learning_rate": 3.825265026894355e-07,
      "loss": 1.5954,
      "step": 760100
    },
    {
      "epoch": 59.54880150399499,
      "grad_norm": 7.9197516441345215,
      "learning_rate": 3.7599874667084447e-07,
      "loss": 1.6814,
      "step": 760200
    },
    {
      "epoch": 59.55663481121729,
      "grad_norm": 6.379035472869873,
      "learning_rate": 3.694709906522534e-07,
      "loss": 1.5354,
      "step": 760300
    },
    {
      "epoch": 59.564468118439606,
      "grad_norm": 8.157719612121582,
      "learning_rate": 3.6294323463366235e-07,
      "loss": 1.4874,
      "step": 760400
    },
    {
      "epoch": 59.57230142566191,
      "grad_norm": 7.209486961364746,
      "learning_rate": 3.564154786150713e-07,
      "loss": 1.5777,
      "step": 760500
    },
    {
      "epoch": 59.580134732884225,
      "grad_norm": 6.844907760620117,
      "learning_rate": 3.4988772259648024e-07,
      "loss": 1.5977,
      "step": 760600
    },
    {
      "epoch": 59.58796804010653,
      "grad_norm": 5.347789764404297,
      "learning_rate": 3.433599665778892e-07,
      "loss": 1.6692,
      "step": 760700
    },
    {
      "epoch": 59.595801347328845,
      "grad_norm": 7.5457000732421875,
      "learning_rate": 3.3683221055929813e-07,
      "loss": 1.6544,
      "step": 760800
    },
    {
      "epoch": 59.60363465455115,
      "grad_norm": 6.399613857269287,
      "learning_rate": 3.303044545407071e-07,
      "loss": 1.5794,
      "step": 760900
    },
    {
      "epoch": 59.611467961773464,
      "grad_norm": 7.989328384399414,
      "learning_rate": 3.23776698522116e-07,
      "loss": 1.6205,
      "step": 761000
    },
    {
      "epoch": 59.61930126899577,
      "grad_norm": 5.628762245178223,
      "learning_rate": 3.1724894250352497e-07,
      "loss": 1.5925,
      "step": 761100
    },
    {
      "epoch": 59.627134576218076,
      "grad_norm": 5.2133331298828125,
      "learning_rate": 3.107211864849339e-07,
      "loss": 1.5909,
      "step": 761200
    },
    {
      "epoch": 59.63496788344039,
      "grad_norm": 6.590852737426758,
      "learning_rate": 3.041934304663429e-07,
      "loss": 1.6692,
      "step": 761300
    },
    {
      "epoch": 59.642801190662695,
      "grad_norm": 5.6430487632751465,
      "learning_rate": 2.9766567444775186e-07,
      "loss": 1.624,
      "step": 761400
    },
    {
      "epoch": 59.65063449788501,
      "grad_norm": 6.283928871154785,
      "learning_rate": 2.911379184291608e-07,
      "loss": 1.6201,
      "step": 761500
    },
    {
      "epoch": 59.658467805107314,
      "grad_norm": 6.437902450561523,
      "learning_rate": 2.8461016241056974e-07,
      "loss": 1.6556,
      "step": 761600
    },
    {
      "epoch": 59.66630111232963,
      "grad_norm": 5.4804606437683105,
      "learning_rate": 2.780824063919787e-07,
      "loss": 1.6706,
      "step": 761700
    },
    {
      "epoch": 59.67413441955193,
      "grad_norm": 5.011138916015625,
      "learning_rate": 2.7155465037338763e-07,
      "loss": 1.6161,
      "step": 761800
    },
    {
      "epoch": 59.681967726774246,
      "grad_norm": 10.119202613830566,
      "learning_rate": 2.650268943547966e-07,
      "loss": 1.6465,
      "step": 761900
    },
    {
      "epoch": 59.68980103399655,
      "grad_norm": 8.882999420166016,
      "learning_rate": 2.584991383362056e-07,
      "loss": 1.6144,
      "step": 762000
    },
    {
      "epoch": 59.697634341218865,
      "grad_norm": 8.034811973571777,
      "learning_rate": 2.519713823176145e-07,
      "loss": 1.6201,
      "step": 762100
    },
    {
      "epoch": 59.70546764844117,
      "grad_norm": 8.500720977783203,
      "learning_rate": 2.4544362629902347e-07,
      "loss": 1.7009,
      "step": 762200
    },
    {
      "epoch": 59.713300955663485,
      "grad_norm": 7.568216800689697,
      "learning_rate": 2.389158702804324e-07,
      "loss": 1.5472,
      "step": 762300
    },
    {
      "epoch": 59.72113426288579,
      "grad_norm": 5.983696937561035,
      "learning_rate": 2.3238811426184136e-07,
      "loss": 1.4776,
      "step": 762400
    },
    {
      "epoch": 59.7289675701081,
      "grad_norm": 6.402066707611084,
      "learning_rate": 2.258603582432503e-07,
      "loss": 1.5829,
      "step": 762500
    },
    {
      "epoch": 59.73680087733041,
      "grad_norm": 4.588074207305908,
      "learning_rate": 2.1933260222465924e-07,
      "loss": 1.603,
      "step": 762600
    },
    {
      "epoch": 59.744634184552716,
      "grad_norm": 6.563314437866211,
      "learning_rate": 2.128048462060682e-07,
      "loss": 1.4753,
      "step": 762700
    },
    {
      "epoch": 59.75246749177503,
      "grad_norm": 11.338226318359375,
      "learning_rate": 2.062770901874772e-07,
      "loss": 1.6275,
      "step": 762800
    },
    {
      "epoch": 59.760300798997335,
      "grad_norm": 7.020075798034668,
      "learning_rate": 1.9974933416888613e-07,
      "loss": 1.6143,
      "step": 762900
    },
    {
      "epoch": 59.76813410621965,
      "grad_norm": 6.7959370613098145,
      "learning_rate": 1.9322157815029508e-07,
      "loss": 1.5933,
      "step": 763000
    },
    {
      "epoch": 59.775967413441954,
      "grad_norm": 8.368024826049805,
      "learning_rate": 1.8669382213170402e-07,
      "loss": 1.6798,
      "step": 763100
    },
    {
      "epoch": 59.78380072066427,
      "grad_norm": 5.864714622497559,
      "learning_rate": 1.8016606611311297e-07,
      "loss": 1.6418,
      "step": 763200
    },
    {
      "epoch": 59.79163402788657,
      "grad_norm": 5.164127349853516,
      "learning_rate": 1.7363831009452194e-07,
      "loss": 1.6101,
      "step": 763300
    },
    {
      "epoch": 59.799467335108886,
      "grad_norm": 6.2994513511657715,
      "learning_rate": 1.6711055407593088e-07,
      "loss": 1.6927,
      "step": 763400
    },
    {
      "epoch": 59.80730064233119,
      "grad_norm": 7.747951030731201,
      "learning_rate": 1.6058279805733983e-07,
      "loss": 1.6616,
      "step": 763500
    },
    {
      "epoch": 59.8151339495535,
      "grad_norm": 7.766974925994873,
      "learning_rate": 1.5405504203874877e-07,
      "loss": 1.5207,
      "step": 763600
    },
    {
      "epoch": 59.82296725677581,
      "grad_norm": 7.999420166015625,
      "learning_rate": 1.4752728602015772e-07,
      "loss": 1.6982,
      "step": 763700
    },
    {
      "epoch": 59.83080056399812,
      "grad_norm": 6.270042896270752,
      "learning_rate": 1.4099953000156666e-07,
      "loss": 1.6794,
      "step": 763800
    },
    {
      "epoch": 59.83863387122043,
      "grad_norm": 5.969708442687988,
      "learning_rate": 1.344717739829756e-07,
      "loss": 1.6409,
      "step": 763900
    },
    {
      "epoch": 59.84646717844274,
      "grad_norm": 6.3720703125,
      "learning_rate": 1.2794401796438455e-07,
      "loss": 1.759,
      "step": 764000
    },
    {
      "epoch": 59.85430048566505,
      "grad_norm": 6.266080379486084,
      "learning_rate": 1.2141626194579352e-07,
      "loss": 1.6492,
      "step": 764100
    },
    {
      "epoch": 59.862133792887356,
      "grad_norm": 6.432317733764648,
      "learning_rate": 1.1488850592720247e-07,
      "loss": 1.5703,
      "step": 764200
    },
    {
      "epoch": 59.86996710010967,
      "grad_norm": 7.077861785888672,
      "learning_rate": 1.0836074990861141e-07,
      "loss": 1.577,
      "step": 764300
    },
    {
      "epoch": 59.877800407331975,
      "grad_norm": 6.937714099884033,
      "learning_rate": 1.0183299389002036e-07,
      "loss": 1.6617,
      "step": 764400
    },
    {
      "epoch": 59.88563371455429,
      "grad_norm": 10.201576232910156,
      "learning_rate": 9.530523787142931e-08,
      "loss": 1.5808,
      "step": 764500
    },
    {
      "epoch": 59.893467021776594,
      "grad_norm": 5.83322286605835,
      "learning_rate": 8.877748185283827e-08,
      "loss": 1.639,
      "step": 764600
    },
    {
      "epoch": 59.9013003289989,
      "grad_norm": 5.877551078796387,
      "learning_rate": 8.224972583424722e-08,
      "loss": 1.5,
      "step": 764700
    },
    {
      "epoch": 59.90913363622121,
      "grad_norm": 6.75671911239624,
      "learning_rate": 7.572196981565617e-08,
      "loss": 1.6013,
      "step": 764800
    },
    {
      "epoch": 59.91696694344352,
      "grad_norm": 8.453852653503418,
      "learning_rate": 6.919421379706513e-08,
      "loss": 1.4961,
      "step": 764900
    },
    {
      "epoch": 59.92480025066583,
      "grad_norm": 6.159706115722656,
      "learning_rate": 6.266645777847408e-08,
      "loss": 1.6313,
      "step": 765000
    },
    {
      "epoch": 59.93263355788814,
      "grad_norm": 5.3177971839904785,
      "learning_rate": 5.613870175988303e-08,
      "loss": 1.6522,
      "step": 765100
    },
    {
      "epoch": 59.94046686511045,
      "grad_norm": 5.674580097198486,
      "learning_rate": 4.9610945741291974e-08,
      "loss": 1.5935,
      "step": 765200
    },
    {
      "epoch": 59.94830017233276,
      "grad_norm": 7.48637580871582,
      "learning_rate": 4.3083189722700925e-08,
      "loss": 1.6495,
      "step": 765300
    },
    {
      "epoch": 59.95613347955507,
      "grad_norm": 8.66183853149414,
      "learning_rate": 3.6555433704109876e-08,
      "loss": 1.6808,
      "step": 765400
    },
    {
      "epoch": 59.96396678677738,
      "grad_norm": 6.671736717224121,
      "learning_rate": 3.002767768551883e-08,
      "loss": 1.6288,
      "step": 765500
    },
    {
      "epoch": 59.97180009399969,
      "grad_norm": 8.359663963317871,
      "learning_rate": 2.349992166692778e-08,
      "loss": 1.602,
      "step": 765600
    },
    {
      "epoch": 59.979633401221996,
      "grad_norm": 5.419097900390625,
      "learning_rate": 1.6972165648336727e-08,
      "loss": 1.5836,
      "step": 765700
    },
    {
      "epoch": 59.9874667084443,
      "grad_norm": 7.357022762298584,
      "learning_rate": 1.044440962974568e-08,
      "loss": 1.5754,
      "step": 765800
    },
    {
      "epoch": 59.995300015666615,
      "grad_norm": 3.0589332580566406,
      "learning_rate": 3.91665361115463e-09,
      "loss": 1.6631,
      "step": 765900
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.7698549032211304,
      "eval_runtime": 2.9083,
      "eval_samples_per_second": 231.059,
      "eval_steps_per_second": 231.059,
      "step": 765960
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.3733961582183838,
      "eval_runtime": 55.1987,
      "eval_samples_per_second": 231.274,
      "eval_steps_per_second": 231.274,
      "step": 765960
    }
  ],
  "logging_steps": 100,
  "max_steps": 765960,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 473982420787200.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
